Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 126?133,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring annotator agreement in a complex hierarchical dialogue act
annotation scheme
Jeroen Geertzen and Harry Bunt
Language and Information Science
Tilburg University, P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
{j.geertzen,h.bunt}@uvt.nl
Abstract
We present a first analysis of inter-
annotator agreement for the DIT++ tagset
of dialogue acts, a comprehensive, lay-
ered, multidimensional set of 86 tags.
Within a dimension or a layer, subsets of
tags are often hierarchically organised. We
argue that especially for such highly struc-
tured annotation schemes the well-known
kappa statistic is not an adequate measure
of inter-annotator agreement. Instead, we
propose a statistic that takes the structural
properties of the tagset into account, and
we discuss the application of this statistic
in an annotation experiment. The exper-
iment shows promising agreement scores
for most dimensions in the tagset and pro-
vides useful insights into the usability of
the annotation scheme, but also indicates
that several additional factors influence
annotator agreement. We finally suggest
that the proposed approach for measuring
agreement per dimension can be a good
basis for measuring annotator agreement
over the dimensions of a multidimensional
annotation scheme.
1 Introduction
The DIT++ tagset (Bunt, 2005) was designed to
combine in one comprehensive annotation scheme
the communicative functions of dialogue acts dis-
tinguished in Dynamic Interpretation Theory (DIT,
(Bunt, 2000; Bunt and Girard, 2005)), and many
of those in DAMSL (Allen and Core, 1997) and in
other annotation schemes. An important differ-
ence between the DIT++ and DAMSL schemes is the
more elaborate and fine-grained set of functions
for feedback and other aspects of dialogue control
that is available in DIT, partly inspired by the work
of Allwood (Allwood et al, 1993). As it is often
thought that more elaborate and fine-grained anno-
tation schemes are difficult for annotators to apply
consistently, we decided to address this issue in an
annotation experiment on which we report in this
paper. A frequently used way of evaluating hu-
man dialogue act classification is inter-annotator
agreement. Agreement is sometimes measured as
percentage of the cases on which the annotators
agree, but more often expected agreement is taken
into account in using the kappa statistic (Cohen,
1960; Carletta, 1996), which is given by:
? = po ? pe1 ? pe
(1)
where po is the observed proportion of agreement
and pe is the proportion of agreement expected by
chance. Ever since its introduction in general (Co-
hen, 1960) and in computational linguistics (Car-
letta, 1996), many researchers have pointed out
that there are quite some problems in using ? (e.g.
(Di Eugenio and Glass, 2004)), one of which is
the discrepancy between p0 and ? for skewed class
distribution.
Another is that the degree of disagreement is
not taken into account, which is relevant for any
non-nominal scale. To address this problem, a
weighted ? has been proposed (Cohen, 1968) that
penalizes disagreement according to their degree
rather than treating all disagreements equally. It
would be arguable that in a similar way, charac-
teristics of dialogue acts in a particular taxonomy
and possible pragmatic similarity between them
should be taken into account to express annotator
agreement. For dialogue act taxonomies which are
structured in a meaningful way, such as those that
126
express hierarchical relations between concepts in
the taxonomy, the taxonomic structure can be ex-
ploited to express how much annotators disagree
when they choose different concepts that are di-
rectly or indirectly related. Recent work that ac-
counts for some of these aspects is a metric for
automatic dialogue act classification (Lesch et al,
2005) that uses distance in a hierarchical structure
of multidimensional labels.
In the following sections of this paper, we will
first briefly consider the dimensions in the DIT++
scheme and highlight the taxonomic characteris-
tics that will turn out to be relevant in later stage.
We will then introduce a variant of weighted ? for
inter-annotator agreement called ?tw that adopts
a taxonomy-dependent weighting, and discuss its
use.
2 Annotation using DIT
DIT is a context-change (or information-state up-
date) approach to the analysis of dialogue, which
describes utterance meaning in terms of context
update operations called ?dialogue acts?. A dia-
logue act in DIT has two components: (1) the se-
mantic content, being the objects, events, proper-
ties, relations, etc. that are considered; and (2)
the communicative function, that describes how
the addressee is intended to use the semantic con-
tent for updating his context model when he un-
derstands the utterance correctly. DIT takes a mul-
tidimensional view on dialogue in the sense that
speakers may use utterances to address several as-
pects of the communication simultaneously, as re-
flected in the multifunctionality of utterances. One
such aspect is the performance of the task or ac-
tivity for which the dialogue takes place; another
is the monitoring of each other?s attention, under-
standing and uptake through feedback acts; others
include for instance the turn-taking process and
the timing of communicative actions, and finally
yet another aspect is formed by the social obli-
gations that may arise such as greeting, apologis-
ing, or thanking. The various aspects of commu-
nication that can be addressed independently are
called dimensions (Bunt and Girard, 2005; Bunt,
2006). The DIT++ tagset distinguishes 11 dimen-
sions, which all contain a number of communica-
tive functions that are specific to that dimension,
such as TURN GIVING, PAUSING, and APOLOGY.
Besides dimension-specific communicative
functions, DIT also distinguishes a layer of
communicative functions that are not specific to
any particular dimension but that can be used
to address any aspect of communication. These
functions, which include questions, answers,
statements, and commissive as well as directive
acts, are called general purpose functions. A
dialogue act falls within a specific dimension
if it has a communicative function specific for
that dimension or if it has a general-purpose
function and a semantic content relating to that
dimension. Dialogue utterances can in principle
have a function (but never more than one) in each
of the dimensions, so annotators using the DIT++
scheme can assign at most one tag for each of the
11 dimensions to any given utterance.
Both within the set of general-purpose com-
municative function tags and within the sets of
dimension-specific tags, tags can be hierarchically
related in such a way that a label lower in a hier-
archy is more specific than a label higher in the
same hierarchy. Tag F1 is more specific than tag
F2 if F1 defines a context update operation that in-
cludes the update operation corresponding to F2.
For instance, consider a part of the taxonomy for
general purpose functions (Figure 1).
INFO.SEEKING
IND-YNQ
YNQ
CHECK
POSI NEGA
IND-WHQ
WHQ
. . .
Figure 1: Two hierarchies in the information seek-
ing general purpose functions.
For an utterance to be assigned a YN-QUESTION,
we assume the speaker believes that the addressee
knows the truth value of the proposition presented.
For an utterance to be assigned a CHECK, we as-
sume the speaker additionally has a weak be-
lief that the proposition that forms the seman-
tic content is true. And for a POSI-CHECK, there
is the additional assumption that the speaker be-
lieves (weakly) that the hearer also believes that
the proposition is true.1
Similar to the hierarchical relations between
YN-Question, CHECK, and POSI-CHECK, other parts
1For a formal description of each function in the DIT++
tagset see http://ls0143.uvt.nl/dit/
127
of the annotation scheme contain hierarchically re-
lated functions.
The following example illustrates the use of
DIT++ communicative functions for a very simple
translated) dialogue fragment2.
1 S at what time do you want to travel today?
TASK = WH-Q, TURN-MANAGEMENT = GIVE
2 U at ten.
TASK = WH-A, TURN-MANAGEMENT = GIVE
3 S so you want to leave at ten in the morning?
TASK = POSI-CHECK, TURN-MANAGEMENT = GIVE
4 U yes that is right.
TASK = CONFIRM, TURN-MANAGEMENT = GIVE
3 Agreement using ?
3.1 Related work
Inter-annotator agreements have been calculated
with the purpose of qualitatively evaluating tagsets
and individual tags. For DAMSL, the first agree-
ment results were presented in (Core and Allen,
1997), based on the analysis of TRAINS 91-
93 dialogues (Gross et al, 1993; Heeman and
Allen, 1995). In this analysis, 604 utterances
were tagged by mostly two annotators. Follow-
ing the suggestions in (Carletta, 1996), Core et
al. consider kappa scores above 0.67 to indi-
cate significant agreement and scores above 0.8
reliable agreement. Another more recent analy-
sis was performed for 8 dialogues of the MON-
ROE corpus (Stent, 2000), counting 2897 utter-
ances in total, processed by two annotators for 13
DAMSL dimensions. Other analyses apply DAMSL
derived schemes (such as SWITCHBOARD-DAMSL)
to various corpora (e.g. (Di Eugenio et al, 1998;
Shriberg et al, 2004) ). For the comprehensive
DIT++ taxonomy, the work reported here repre-
sents the first investigation of annotator agree-
ment.
3.2 Experiment outline
As noted, existing work on annotator agreement
analysis has mostly involved only two annotators.
It may be argued that especially for annotation of
concepts that are rather complex, an odd number
of annotators is desirable. First, it allows having
majority agreement unless all annotators choose
entirely different. Second, it allows to deal bet-
ter with the undesirable situation that one annota-
tor chooses quite differently from the others. The
2Drawn from the OVIS corpus (Strik et al, 1997):
OVIS2:104/001/001:008-011
agreement scores reported in this paper are all cal-
culated on the basis of the annotations of three
annotators, using the method proposed in (Davies
and Fleiss, 1982).
The dialogues that were annotated are task-
oriented and are all in Dutch. To account for
different complexities of interaction, both human-
machine and human-human dialogues are consid-
ered. Moreover, the dialogues analyzed are drawn
from different corpora: OVIS (Strik et al, 1997),
DIAMOND (Geertzen et al, 2004), and a collec-
tion of Map Task dialogues (Caspers, 2000); see
Table 1, where the number of annotated utterances
is also indicated.
corpus domain type #utt
OVIS TRAINS like interactions H-M 193
on train connections
DIAMOND1 interactions on how to H-M 131
operate a fax device
DIAMOND2 interactions on how to H-H 114
operate a fax device
MAPTASK HCRC Map Task like H-H 120
interaction
558
Table 1: Characteristics of the utterances consid-
ered
Six undergraduate students annotated the se-
lected dialogue material. They had been intro-
duced to the DIT++ annotation scheme and the un-
derlying theory while participating in a course on
pragmatics. During this course they were exposed
to approximately four hours of lecturing and few
small annotation exercises. For all dialogues, the
audio recordings were transcribed and the annota-
tors annotated presegmented utterances for which
full agreement was established on segmentation
level beforehand. During the annotation sessions
the annotators had ? apart from the transcribed
speech ? access to the audio recordings, to the
on-line definitions of the communicative functions
in the scheme and to a very brief, 1-page set of an-
notation guidelines3 . The task was facilitated by
the use of an annotation tool that had been built
for this occasion; this tool allowed the subjects to
assign each utterance one DIT++ tag for each di-
mension without any further constraints. In total
1,674 utterances were annotated.
3.3 Problems with standard ?
If we were to apply the standard ? statistic to
DIT++ annotations, we would not do justice to an
important aspect of the annotation scheme con-
cerning the differences between alternative tags,
3See http://ls0143.uvt.nl/dit
128
and hence the possible differences in the dis-
agreement between annotators using alternative
tags. An aspect in which the DIT++ scheme dif-
fers from other taxonomies for dialogue acts is
that, as noted in Section 2, communicative func-
tions (CFs) within a dimension as well as general-
purpose CFs are often structured into hierarchies
in which a difference in level represents a relation
of specificity. When annotators differ in that they
assign tags which both belong to the same hier-
archy, they may differ in the degree of specificity
that they want to express, but they agree to the ex-
tent that these tags inherit the same elements from
tags higher in the hierarchy. Inter-annotator dis-
agreement is in such a case much less than if they
would choose two unrelated tags. This is for in-
stance obvious in the following example of the an-
notations of two utterances by two annotators:
1 S what do you want to know? WHQ YNQ
2 U can I print now? YNQ CHECK
With utterance 1, the annotators should be said
simply to disagree (in fact, annotator 2 incorrectly
assigns a YNQ function). Concerning utterance 2
the annotators also disagree, but Figure 1 and the
definitions given in Section 2 tell us that the dis-
agreement in this case is quite small, as a CHECK in-
herits the properties of a YNQ. We therefore should
not use a black-and-white measure of agreement,
like the standard ?, but we should have a measure
for partial annotator agreement.
In order to measure partial (dis-)agreement be-
tween annotators in an adequate way, we should
not just take into account whether two tags are hi-
erarchically related or not, but also how far they
are apart in the hierarchy, to reflect that two tags
which are only one level apart are semantically
more closely related than tags that are several lev-
els apart. We will take this additional requirement
into account when designing a weighted disagree-
ment statistic in the next section.
4 Agreement based on structural
taxonomic properties
The agreement coefficient we are looking for
should in the first place be weighted in the sense
that it takes into account the magnitude of dis-
agreement. Two such coefficients are weighted
kappa (?w, (Cohen, 1968)) and alpha (Krippen-
dorff, 1980). For our purposes, we adopt ?w for
its property to take into account a probability dis-
tribution typical for each annotator, generalize it to
the case for multiple annotators by taking the aver-
age over the scores of annotator pairs, and define
a function to be used as distance metric.
4.1 Cohen?s weighted ?
Assuming the case of two annotators, let pij de-
note the proportion of utterances for which the first
and second annotator assigned categories i and j,
respectively. Then Cohen defines ?w in terms of
disagreement rather than agreement where qo =
1 ? po and qe = 1 ? pe such that Equation 1 can
be rewritten to:
? = 1 ? qoqe
(2)
To arrive at ?w, the proportions qo and qe in Equa-
tion 2 are replaced by weighted functions over all
possible category pairs:
?w = 1 ?
? vij ? poij
? vij ? peij
(3)
where vij denotes the disagreement weight. To
calculate this weight we need to specify a distance
function as metric.
4.2 A taxonomic metric
The task of defining a function in order to calcu-
late the difference between a pair of categories re-
quires us to determine semantic-pragmatic related-
ness between the CFs in the taxonomy. For any an-
notation scheme, whether it is hierarchically struc-
tured or not, we could assign for each possible pair
of categories a value that expresses the semantic-
pragmatic relatedness between the two categories
compared to all other possible pairs. However, it
seems quite difficult to find universal characteris-
tics for CFs to be used to express relatedness on a
rational scale. When we consider a taxonomy that
is structured in a meaningful way, in this case one
that expresses hierarchical relations between CF
based on their effect on information states, the tax-
onomic structure can be exploited to express in a
systematic fashion how much annotators disagree
when they choose different concepts that are di-
rectly or indirectly related.
The assignment of different CFs to a specific ut-
terance by two annotators represents full disagree-
ment in the following cases:
1. the two CFs belong to different dimensions;
129
2. one of the two CFs is general-purpose; the
other is dimension-specific;4
3. the two CFs belong to the same dimension
but not to the same hierarchy;
4. the two CFs belong to the same hierarchy
but are not located in the same branch. Two
CFs are said to be located in the same branch
when one of the two CFs is an ancestor of the
other.
If, by contrast, the two CFs take part in a parent-
child relation within a hierarchy (either within a
dimension or among the general-purpose CFs),
then the CFs are related and this assignment repre-
sents partial disagreement. A distance metric that
measures this disagreement, which we denote as
?, should have the following properties:
1. ? should be a real number normalized in the
range [0 . . . 1];
2. Let C be the (unordered) set of CFs.5 For ev-
ery two CFs c1, c2 ? C , ?(c1, c2) = 0 when
c1 and c2 are not related;
3. Let C be the (unordered) set of CFs. For ev-
ery communicative function c ? C , ?(c, c) =
1;
4. Let C be the (unordered) set of CFs. For
every two CFs c1, c2 ? C , ?(c1, c2) =
?(c2, c1).
Furthermore, when c1 and c2 are related, we
should specify how distance between them in the
hierarchy should be expressed in terms of partial
disagreement. For this, we should take the follow-
ing aspects into account:
1. The distance in levels between c1 and c2 in
the hierarchy is proportional to the magnitude
of the disagreement;
4This is in fact a simplification. For instance, an INFORM
act of which the semantic content conveys that the speaker
did not understand the previous utterance forms an act in the
Auto-Feedback dimension (see Note 6), and a tagging to this
effect should perhaps not be considered to express full dis-
agreement with the assignment of the dimension-specific tag
AUTO-FEEDBACK-Int?. See also the next footnote.
5Strictly speaking, in DIT a dialogue act annotation tag is
either (a) the name of a dimension-specific function, or (b) a
pair consisting of the name of a general-purpose function and
the name of a dimension. However, in view of the simplifica-
tion mentioned in the previous note, for the sake of this paper
we may as well consider tags containing a general-purpose
function as simply consisting of that function.
Auto Feedback
Perc?
Int?
Eval?
Exec?
Perc+
Int+
Eval+
Exec+
Figure 2: Hierarchical structures in the auto feed-
back dimension.
2. The magnitude of disagreement between c1
and c2 being located in two different levels of
depths n and n+1 might be considered to be
more different than that between to levels of
depth n + 1 and n + 2. If this would be the
case, the deeper two levels are located in the
tree, the smaller the differences between the
nodes on those levels. For the hierarchies in
DIT, we keep the magnitude of disagreement
linear with the difference in levels, and inde-
pendent of level depth;
Given the considerations above, we propose the
following metric:
?(ci, cj) = a?(ci,cj) ? b?(ci,cj) (4)
where:
? a is a constant for which 0 < a < 1, express-
ing how much distance there is between two
adjacent levels in the hierarchy; a plausible
value for a could be 0.75;
? ? is a function that returns the difference in
depth between the levels of ci and cj;
? b is a constant for which 0 < b ? 1, express-
ing in what rate differences should become
smaller when the depth in the hierarchy gets
larger. If there is no reason to assume that
differences on a higher depth in the hierarchy
are of less magnitude than differences on a
lower depth, then b = 1;
? ?(ci, cj) is a function that returns the mini-
mal depth of ci and cj .
To provide some examples of how ? would be
calculated, let us consider the general purpose
functions in Figure 1. Consider also Figure 2,
that represents two hierarchies of CFs in the auto
130
feedback dimension6, and let us assume the values
of the various parameters those that are suggested
above. We then get the following calculations:
?(IND ? Y NQ,CHECK) = 0.752 ? 1 = 0.563
?(Y NQ,CHECK) = 0.751 ? 1 = 0.75
?(Perc+, P erc+) = 0.750 ? 1 = 1
?(Perc+, Eval+) = 0.752 ? 1 = 0.563
?(Int?, Int+) = 0
?(POSI,NEGA) = 0
To conclude, we can simply take ? to be the
weighting in Cohen?s ?w and come to a coefficient
which we will call taxonomically weighted kappa,
denoted by ?tw:
?tw = 1 ?
?(1 ? ?(i, j)) ? poij
?(1 ? ?(i, j)) ? peij
(5)
4.3 ?tw statistics for DIT
Considering the DIT++ taxonomy, it may be argued
that due to the many hierarchies in the topology
of the general-purpose functions, this is the part
where most is to be gained by employing ?tw.
Table 2 shows the statistics for each dimension,
averaged over all annotation pairs. With anno-
tation pair is understood the pair of assignments
an utterance received by two annotators for a par-
ticular dimension. The figures in the table are
based on those cases in which both annotators as-
signed a function to a specific utterance for a spe-
cific dimension. Cases where either one annotator
does not assign a function while the other does,
or where both annotators do not assign a function,
are not considered. Scores for standard ? and ?tw
can be found in the first two columns. The column
#pairs indicates on how many annotation pairs the
statistics are based. The last column shows the
ap-ratio. This figure indicates which fraction of
all annotated functions in that dimension are rep-
resented by annotation pairs. When #ap denotes
the number of annotation pairs and #pa denotes
the number of partial annotations (annotations in
which one annotator assigned a function and the
other did not), then the ap-ratio is calculated as
#ap/(#pa + #ap). We can observe that due to
the use of the taxonomic weighting both feedback
dimensions and the task dimension gained sub-
stantially in annotator agreement.
6Auto-feedback: feedback on the processing (perception,
understanding, evaluation,..) of previous utterances by the
speaker. DIT also distinguishes allo-feedback, where the
speaker provides or elicits information about the addressee?s
processing.
Dimension ? ?tw #pairs ap-ratio
task 0.47 0.71 848 0.87
task:action discussion 0.61 0.61 91 0.37
auto feedback 0.21 0.57 127 0.34
allo feedback 0.42 0.58 17 0.14
turn management 0.82 0.82 115 0.18
time management 0.58 0.58 68 0.72
contact management 1.00 1.00 8 0.17
topic management nav nav 2 0.08
own com. management 1.00 1.00 2 0.08
partner com. management nav nav 1 0.07
dialogue struct. management 0.74 0.74 15 0.31
social obl. management 1.00 1.00 61 0.80
Table 2: Scores for corrected ? and ?tw per DIT
dimension.
When we look at the agreement statistics and
consider ? scores above 0.67 to be significant
and scores above 0.8 considerably reliable, as is
usual for ? statistics, we can find the dimensions
TURN-MANAGEMENT, CONTACT MANAGEMENT, and
SOCIAL-OBLIGATIONS-MANAGEMENT to be reliable
and DIALOGUE STRUCT. MANAGEMENT to be signif-
icant. For some dimensions, the occurences of
functions in these dimensions in the annotated di-
alogue material were too few to draw conclusions.
When we also take the ap-ratio into account,
only the dimensions TASK, TIME MANAGEMENT,
and SOCIAL-OBLIGATIONS-MANAGEMENT combine
a fair agreement on functions with fair agreement
on whether or not to annotate in these dimensions.
Especially for the other dimensions, the question
should be raised for which cases and for what rea-
sons the ap-ratio is low. This question asks for
further qualitative analysis, which is beyond the
scope of this paper7.
5 Discussion
In the previous sections, we showed how the tax-
onomically weighted ?tw that we proposed can be
more suitable for taxonomies that contain hierar-
chical structures, like the DIT++) taxonomy. How-
ever, there are some specific and general issues
that deserve more attention.
A question that might be raised in using ?tw as
opposed to ordinary ?, is if the assumption that the
interpretations of ? proposed in literature in terms
of reliability is also valid for ?tw statistics. This
is ultimately an empirical issue, to be decided by
which ?tw scores researchers find to correspond to
fair or near agreement between annotators.
Another point of discussion is the arbitrariness
of the values of the parameters that can be cho-
sen in ?. In this paper we proposed a = 0.75 and
? = 0.5. Choosing different values may change
7See (Geertzen, 2006) for more details.
131
the disagreement of two distinct CFs located in the
same hierarchy considerably. Still, we think that
by interpolating smoothly between the intuitively
clear cases at the two extreme ends of the scale,
it is possible to choose reasonable values for the
parameters that scale well, given the average hier-
archy depth.
A more general problem, inherent in almost
any (dialogue act) annotation activity is that when
we consider the possible factors that influence the
agreement scores, we find that they can be nu-
merous. Starting with the tagset, unclear defini-
tions and vague concepts are a major source of
disagreement. Other factors are the quality and ex-
tensiveness of annotation instructions, and the ex-
perience of the annotators. These were kept con-
stant throughout the experiment reported in this
paper, but clearly the use of more experienced or
better trained annotators could have a great influ-
ence. Then there is the influence that the use of an
annotation tool can have. Does the tool gives hints
on annotation consistency (e.g. an ANSWER should
be preceded by a QUESTION), does it enforce con-
sistency, or does it not consider annotation consis-
tency at all? Are the possible choices for anno-
tators presented in such a way that each choice is
equally well visible and accessible? Clearly, when
we do not control these factors sufficiently, we run
the risk that what we measure does not express
what we try to quantify: (dis)agreement among
annotators about the description of what happens
in a dialogue.
6 Conclusion and future work
In this paper we have presented agreement scores
for Cohen?s unweighted ? and claimed that for
annotation schemes with hierarchically related
tags, a weighted ? gives a better indication of
(dis)agreement than unweighted ?. The ? scores
for some dimensions seem not particularly spec-
tacular but become more interesting when look-
ing at semantic-pragmatic differences between di-
alogue acts or CFs. Even though there are some-
what arbitrary aspects in weighting, when parame-
ters are carefully chosen a weighted metric gives a
better representation of the inter-annotator agree-
ments. More generally, we propose that semantic-
pragmatic relatedness between taxonomic con-
cepts should be taken into account when calculat-
ing inter-annotator (dis)agreement. While we used
DIT++ as tagset, the weighting function we pro-
posed can be employed in any taxonomy contain-
ing hierarchically related concepts, since we only
used structural properties of the taxonomy.
We have also quantitatively8 evaluated the
DIT++ tagset per dimension, and obtained an in-
dication of its usability. We focussed on agree-
ment per dimension, but when we desire a global
indication of the difference in semantic-pragmatic
interpretation of a complete utterance it requires
us to consider other aspects. A truly multidimen-
sional study of inter-annotator agreement should
not only take intra-dimensional aspects into ac-
count but also relate the dimensions to each other.
In (Bunt and Girard, 2005; Bunt, 2006) it is argued
that dimensions should be orthogonal, meaning
that an utterance can have a function in one dimen-
sion independent of functions in other dimensions.
This is a somewhat utopical condition, since there
are some functions that show correlations and de-
pendencies with across dimensions. For this rea-
son it makes sense to try to express the effect of the
presence of strong correlations, dependencies and
possible entailments in a multidimensional notion
of (dis)agreement. Additionally, it may be desir-
able to take into account the importance that a CF
can have. It is widely acknowledged that utter-
ances are often multifunctional, but it could be ar-
gued that in many cases an utterance has a primary
function and secondary functions; for instance, if
an utterance has both a task-related function and
one or more other functions, the task-related func-
tion is typically felt to be more important than the
other functions, and disagreement about the task-
related function is therefore felt to be more seri-
ous than disagreement about one of the other func-
tions. This might be taken into account by adding
a weighting function when combining agreement
measures over multiple dimensions.
Other future work we plan is more methodolog-
ical in nature, quantifying the relative effect of the
factors that may have influenced the scores that we
have found. This would create a situation in which
there is more insight in what exactly is evaluated.
As for evaluating the tagset, we for instance plan
to further analyze co-occurence matrices to iden-
tify frequent misannotations, and to have annota-
tors thinking aloud while performing the annota-
tion task.
8Kappa statistics are indicative. To get a full understand-
ing of what the figures represent, qualitative analysis by using
e.g. co-occurence matrices is required, which is beyond the
scope of this paper.
132
Acknowledgements
The authors thank three anonymous reviewers for
their helpful comments on an earlier version of this
paper.
References
James Allen and Mark Core. 1997. Draft of DAMSL:
Dialog act markup in several layers. Unpublished
manuscript.
J. Allwood, J. Nivre, and E. Ahlse?n. 1993. Manual for
coding interaction management. Technical report,
Go?teborg University. Project report: Semantik och
talspra?k.
Harry C. Bunt and Yann Girard. 2005. Designing an
open, multidimensional dialogue act taxonomy. In
Proceedings of the 9th Workshop on the Semantics
and Pragmatics of Dialogue (DIALOR 2005), pages
37?44, Nancy, France, June.
Harry C. Bunt. 2000. Dialogue pragmatics and con-
text specification. In Harry C. Bunt and William
Black, editors, Abduction, Belief and Context in Di-
alogue; Studies in Computational Pragmatics, pages
81?150. John Benjamins, Amsterdam, The Nether-
lands.
Harry C. Bunt. 2005. A framework for dialogue act
specification. In Joint ISO-ACL Workshop on the
Representation and Annotation of Semantic Infor-
mation, Tilburg, The Netherlands, January.
Harry C. Bunt. 2006. Dimensions in dialogue annota-
tion. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genova, Italy, May.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Johanneke Caspers. 2000. Pitch accents, boundary
tones and turn-taking in dutch map task dialogues.
In Proceedings of the 6th International Conference
on Spoken Language Processing (ICSLP 2000), vol-
ume 1, pages 565?568, Beijing, China.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Education and Psychological Mea-
surement, 20:37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70:213?220.
Mark G. Core and James F. Allen. 1997. Coding di-
alogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35, Menlo Park, CA, USA.
American Association for Artificial Intelligence.
Mark Davies and J.L. Fleiss. 1982. Measuring agree-
ment for multinomial data. Biometrics, 38:1047?
1051.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101.
Barbara Di Eugenio, Pamela W. Jordan, Johanna D.
Moore, and Richmond H. Thomason. 1998. An em-
pirical investigation of proposals in collaborative di-
alogues. In Proceedings of the 17th International
Conference on Computational Linguistics and the
36th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL 1998), pages
325?329, Montreal, Canada.
Jeroen Geertzen, Yann Girard, Roser Morante, Ielka
van der Sluis, Hans Van Dam, Barbara Suijkerbuijk,
Rintse van der Werf, and Harry Bunt. 2004. The
diamond project (poster,project description). In The
8th Workshop on the Semantics and Pragmatics of
Dialogue (Catalog?04). Barcelona, Spain.
Jeroen Geertzen. 2006. Inter-annotator agreement
within dit++ dimensions. Technical report, Tilburg
University, Tilburg, The Netherlands.
Derek Gross, James F. Allen, and David R. Traum.
1993. The TRAINS 91 dialogues. Technical Re-
port TN92-1, University of Rochester, Rochester,
NY, USA.
Peter A. Heeman and James F. Allen. 1995. The
TRAINS 93 dialogues. Technical Report TN94-2,
University of Rochester, Rochester, NY, USA.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to its Methodology. Sage Publications,
Beverly Hills, CA, USA.
Stephan Lesch, Thomas Kleinbauer, and Jan Alexan-
dersson. 2005. A new metric for the evaluation
of dialog act classification. In Proceedings of the
9th Workshop on the Semantics and Pragmatics of
Dialogue (DIALOR 2005), pages 143?146, Nancy,
France, june.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings
of the 5th SIGdial Workshop on Discourse and Dia-
logue, pages 97?100, Boston, USA, April-May.
Amanda J. Stent. 2000. The monroe corpus. Techni-
cal Report TR728/TN99-2, University of Rochester,
Rochester, UK.
Helmer Strik, Albert Russel, Henk van den Heuvel, Ca-
tia Cucchiarini, and Lou Boves. 1997. A spoken di-
alog system for the dutch public transport informa-
tion service. International Journal of Speech Tech-
nology, 2(2):119?129.
133
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 176?180, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing by Inference over High-recall Dependency Predictions
Sander Canisius, Toine Bogers,
Antal van den Bosch, Jeroen Geertzen
ILK / Computational Linguistics and AI
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{S.V.M.Canisius,A.M.Bogers,
Antal.vdnBosch,J.Geertzen}@uvt.nl
Erik Tjong Kim Sang
Informatics Institute
University of Amsterdam, Kruislaan 403
NL-1098 SJ Amsterdam, The Netherlands
erikt@science.uva.nl
1 Introduction
As more and more syntactically-annotated corpora
become available for a wide variety of languages,
machine learning approaches to parsing gain inter-
est as a means of developing parsers without having
to repeat some of the labor-intensive and language-
specific activities required for traditional parser de-
velopment, such as manual grammar engineering,
for each new language. The CoNLL-X shared task
on multi-lingual dependency parsing (Buchholz et
al., 2006) aims to evaluate and advance the state-of-
the-art in machine learning-based dependency pars-
ing by providing a standard benchmark set compris-
ing thirteen languages1. In this paper, we describe
two different machine learning approaches to the
CoNLL-X shared task.
Before introducing the two learning-based ap-
proaches, we first describe a number of baselines,
which provide simple reference scores giving some
sense of the difficulty of each language. Next, we
present two machine learning systems: 1) an ap-
proach that directly predicts all dependency relations
in a single run over the input sentence, and 2) a cas-
cade of phrase recognizers. The first approach has
been found to perform best and was selected for sub-
mission to the competition. We conclude this paper
with a detailed error analysis of its output for two of
the thirteen languages, Dutch and Spanish.
1The data sets were extracted from various existing tree-
banks (Hajic? et al, 2004; Simov et al, 2005; Simov and Osen-
ova, 2003; Chen et al, 2003; Bo?hmova? et al, 2003; Kromann,
2003; van der Beek et al, 2002; Brants et al, 2002; Kawata and
Bartels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006; Civit
Torruella and Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer
et al, 2003; Atalay et al, 2003)
2 Baseline approaches
Given the diverse range of languages involved in
the shared task, each having different characteristics
probably requiring different parsing strategies, we
developed four different baseline approaches for as-
signing labeled dependency structures to sentences.
All of the baselines produce strictly projective struc-
tures. While the simple rules implementing these
baselines are insufficient for achieving state-of-the-
art performance, they do serve a useful role in giving
a sense of the difficulty of each of the thirteen lan-
guages. The heuristics for constructing the trees and
labeling the relations used by each of the four base-
lines are described below.
Binary right-branching trees The first baseline
produces right-branching binary trees. The first to-
ken in the sentence is marked as the top node with
HEAD 0 and DEPREL ROOT. For the rest of the
tree, token n ? 1 serves as the HEAD of token n.
Figure 1 shows an example of the kind of tree this
baseline produces.
Binary left-branching trees The binary left-
branching baseline mirrors the previous baseline.
The penultimate token in the sentence is marked as
the top node with HEAD 0 and DEPREL ROOT
since punctuation tokens can never serve as ROOT2.
For the rest of the tree, the HEAD of token n is token
n+1. Figure 2 shows an example of a tree produced
by this baseline.
2We simply assume the final token in the sentence to be
punctuation.
176
Inward-branching trees In this approach, the
first identified verb3 is marked as the ROOT node.
The part of the sentence to the left of the ROOT is
left-branching, the part to the right of the ROOT is
right-branching. Figure 3 shows an example of a
tree produced by this third baseline.
Nearest neighbor-branching trees In our most
complex baseline, the first verb is marked as the
ROOT node and the other verbs (with DEPREL vc)
point to the closest preceding verb. The other to-
kens point in the direction of their nearest neighbor-
ing verb, i.e. the two tokens at a distance of 1 from
a verb have that verb as their HEAD, the two tokens
at a distance of 2 have the tokens at a distance of 1
as their head, and so on until another verb is a closer
neighbor. In the case of ties, i.e. tokens that are
equally distant from two different verbs, the token is
linked to the preceding token. Figure 4 clarifies this
kind of dependency structure in an example tree.
verb verb punct
ROOT
Figure 1: Binary right-branching tree for an example
sentence with two verbs.
verb verb punct
ROOT
Figure 2: Binary left-branching tree for the example
sentence.
verb verb punct
ROOT
Figure 3: Binary inward-branching tree for the ex-
ample sentence.
3We consider a token a verb if its CPOSTAG starts with a
?V?. This is an obviously imperfect, but language-independent
heuristic choice.
ROOT
verb verb punct
Figure 4: Nearest neighbor-branching tree for the
example sentence.
Labeling of identified relations is done using a
three-fold back-off strategy. From the training set,
we collect the most frequent DEPREL tag for each
head-dependent FORM pair, the most frequent DE-
PREL tag for each FORM, and the most frequent
DEPREL tag in the entire training set. The rela-
tions are labeled in this order: first, we look up if the
FORM pair of a token and its head was present in
the training data. If not, then we assign it the most
frequent DEPREL tag in the training data for that
specific token FORM. If all else fails we label the
token with the most frequent DEPREL tag in the en-
tire training set (excluding punct4 and ROOT).
language baseline unlabeled labeled
Arabic left 58.82 39.72
Bulgarian inward 41.29 29.50
Chinese NN 37.18 25.35
Czech NN 34.70 22.28
Danish inward 50.22 36.83
Dutch NN 34.07 26.87
German NN 33.71 26.42
Japanese right 67.18 64.22
Portuguese right 25.67 22.32
Slovene right 24.12 19.42
Spanish inward 32.98 27.47
Swedish NN 34.30 21.47
Turkish right 49.03 31.85
Table 1: The labeled and unlabeled scores for the
best performing baseline for each language (NN =
nearest neighbor-branching).
The best baseline performance (labeled and un-
labeled scores) for each language is listed in Table
1. There was no single baseline that outperformed
the others on all languages. The nearest neighbor
baseline outperformed the other baselines on five
of the thirteen languages. The right-branching and
4Since the evaluation did not score on punctuation.
177
inward-branching baselines were optimal on four
and three languages respectively. The only language
where the left-branching trees provide the best per-
formance is Arabic.
3 Parsing by inference over high-recall
dependency predictions
In our approach to dependency parsing, a machine
learning classifier is trained to predict (directed) la-
beled dependency relations between a head and a de-
pendent. For each token in a sentence, instances are
generated where this token is a potential dependent
of each of the other tokens in the sentence5. The
label that is predicted for each classification case
serves two different purposes at once: 1) it signals
whether the token is a dependent of the designated
head token, and 2) if the instance does in fact corre-
spond to a dependency relation in the resulting parse
of the input sentence, it specifies the type of this re-
lation, as well.
The features we used for encoding instances for
this classification task correspond to a rather simple
description of the head-dependent pair to be clas-
sified. For both the potential head and dependent,
there are features encoding a 2-1-2 window of words
and part-of-speech tags6; in addition, there are two
spatial features: a relative position feature, encoding
whether the dependent is located to the left or to the
right of its potential head, and a distance feature that
expresses the number of tokens between the depen-
dent and its head.
One issue that may arise when considering each
potential dependency relation as a separate classifi-
cation case is that inconsistent trees are produced.
For example, a token may be predicted to be a de-
pendent of more than one head. To recover a valid
dependency tree from the separate dependency pre-
dictions, a simple inference procedure is performed.
Consider a token for which the dependency relation
is to be predicted. For this token, a number of clas-
sification cases have been processed, each of them
5To prevent explosion of the number of classification cases
to be considered for a sentence, we restrict the maximum dis-
tance between a token and its potential head. For each language,
we selected this distance so that, on the training data, 95% of the
dependency relations is covered.
6More specifically, we used the part-of-speech tags from the
POSTAG column of the shared task data files.
indicating whether and if so how the token is related
to one of the other tokens in the sentence. Some of
these predictions may be negative, i.e. the token is
not a dependent of a certain other token in the sen-
tence, others may be positive, suggesting the token
is a dependent of some other token.
If all classifications are negative, the token is as-
sumed to have no head, and consequently no depen-
dency relation is added to the tree for this token; the
node in the dependency tree corresponding to this
token will then be an isolated one. If one of the clas-
sifications is non-negative, suggesting a dependency
relation between this token as a dependent and some
other token as a head, this dependency relation is
added to the tree. Finally, there is the case in which
more than one prediction is non-negative. By defi-
nition, at most one of these predictions can be cor-
rect; therefore, only one dependency relation should
be added to the tree. To select the most-likely can-
didate from the predicted dependency relations, the
candidates are ranked according to the classification
confidence of the base classifier that predicted them,
and the highest-ranked candidate is selected for in-
sertion into the tree.
For our base classifier we used a memory-based
learner as implemented by TiMBL (Daelemans et
al., 2004). In memory-based learning, a machine
learning method based on the nearest-neighbor rule,
the class for a given test instance is predicted by per-
forming weighted voting over the class labels of a
certain number of most-similar training instances.
As a simple measure of confidence for such a pre-
diction, we divide the weight assigned to the major-
ity class by the total weight assigned to all classes.
Though this confidence measure is a rather ad-hoc
one, which should certainly not be confused with
any kind of probability, it tends to work quite well
in practice, and arguably did so in the context of
this study. The parameters of the memory-based
learner have been optimized for accuracy separately
for each language on training and development data
sampled internally from the training set.
The base classifier in our parser is faced with a
classification task with a highly skewed class dis-
tribution, i.e. instances that correspond to a depen-
dency relation are largely outnumbered by those that
do not. In practice, such a huge number of nega-
tive instances usually results in classifiers that tend
178
to predict fairly conservatively, resulting in high pre-
cision, but low recall. In the approach introduced
above, however, it is better to have high recall, even
at the cost of precision, than to have high precision at
the cost of recall. A missed relation by the base clas-
sifier can never be recovered by the inference proce-
dure; however, due to the constraint that each token
can only be a dependent of one head, excessive pre-
diction of dependency relations can still be corrected
by the inference procedure. An effective method for
increasing the recall of a classifier is down-sampling
of the training data. In down-sampling, instances
belonging to the majority class (in this case the neg-
ative class) are removed from the training data, so
as to obtain a more balanced distribution of negative
and non-negative instances.
Figure 5 shows the effect of systematically re-
moving an increasingly larger part of the negative in-
stances from the training data. First of all, the figure
confirms that down-sampling helps to improve re-
call, though it does so at the cost of precision. More
importantly however, it also illustrates that this im-
proved recall is beneficial for the performance of the
dependency parser. The shape of the performance
curve of the dependency parser closely follows that
of the recall. Remarkably, parsing performance con-
tinues to improve with increasingly stronger down-
sampling, even though precision drops considerably
as a result of this. This shows that the confidence
of the classifier for a certain prediction is a suffi-
ciently reliable indication of the quality of that pre-
diction for fixing the over-prediction of dependency
relations. Only when the number of negative train-
ing instances is reduced to equal the number of pos-
itive instances, the performance of the parser is neg-
atively affected. Based on a quick evaluation of var-
ious down-sampling ratios on a 90%-10% train-test
split of the Dutch training data, we decided to down-
sample the training data for all languages with a ratio
of two negative instances for each positive one.
Table 2 lists the unlabeled and labeled attachment
scores of the resulting system for all thirteen lan-
guages.
4 Cascaded dependency parsing
One of the alternative strategies explored by us was
modeling the parsing process as a cascaded pair of
 0
 20
 40
 60
 80
 100
 2 4 6 8 10
Sampling ratio
PrecisionRecallSystem LAS
Figure 5: The effect of down-sampling on precision
and recall of the base classifier, and on labeled ac-
curacy of the dependency parser. The x-axis refers
to the number of negative instances for each posi-
tive instance in the training data. Training and test-
ing was performed on a 90%-10% split of the Dutch
training data.
basic learners. This approach is similar to Yamada
and Matsumoto (2003) but we only use their Left
and Right reduction operators, not Shift. In the first
phase, each learner predicted dependencies between
neighboring words. Dependent words were removed
and the remaining words were sent to the learners for
further rounds of processing until all words but one
had been assigned a head. Whenever crossing links
prevented further assignments of heads to words, the
learner removed the remaining word requiring the
longest dependency link. When the first phase was
finished another learner assigned labels to pairs of
words present in dependency links.
Unlike in related earlier work (Tjong Kim Sang,
2002), we were unable to compare many different
learner configurations. We used two different train-
ing files for the first phase: one for predicting the
dependency links between adjacent words and one
for predicting all other links. As a learner, we used
TiMBL with its default parameters. We evaluated
different feature sets and ended up with using words,
lemmas, POS tags and an extra pair of features with
the POS tags of the children of the focus word. With
this configuration, this cascaded approach achieved
a labeled score of 62.99 on the Dutch test data com-
pared to 74.59 achieved by our main approach.
179
language unlabeled labeled
Arabic 74.59 57.64
Bulgarian 82.51 78.74
Chinese 82.86 78.37
Czech 72.88 60.92
Danish 82.93 77.90
Dutch 77.79 74.59
German 80.01 77.56
Japanese 89.67 87.41
Portuguese 85.61 77.42
Slovene 74.02 59.19
Spanish 71.33 68.32
Swedish 85.08 79.15
Turkish 64.19 51.07
Table 2: The labeled and unlabeled scores for the
submitted system for each of the thirteen languages.
5 Error analysis
We examined the system output for two languages
in more detail: Dutch and Spanish.
5.1 Dutch
With a labeled attachment score of 74.59 and an
unlabeled attachment score of 77.79, our submitted
Dutch system performs somewhat above the average
over all submitted systems (labeled 70.73, unlabeled
75.07). We review the most notable errors made by
our system.
From a part-of-speech (CPOSTAG) perspective,
a remarkable relative amount of head and depen-
dency errors are made on conjunctions. A likely
explanation is that the tag ?Conj? applies to both co-
ordinating and subordinating conjunctions; we did
not use the FEATS information that made this dis-
tinction, which would have likely solved some of
these errors.
Left- and right-directed attachment to heads is
roughly equally successful. Many errors are made
on relations attaching to ROOT; the system appears
to be overgenerating attachments to ROOT, mostly
in cases when it should have generated rightward
attachments. Unsurprisingly, the more distant the
head is, the less accurate the attachment; especially
recall suffers at distances of three and more tokens.
The most frequent attachment error is generat-
ing a ROOT attachment instead of a ?mod? (mod-
ifier) relation, often occurring at the start of a sen-
tence. Many errors relate to ambiguous adverbs such
as bovendien (moreover), tenslotte (after all), and
zo (thus), which tend to occur rather frequently at
the beginning of sentences in the test set, but less
so in the training set. The test set appears to con-
sist largely of formal journalistic texts which typi-
cally tend to use these marked rhetorical words in
sentence-initial position, while the training set is a
more mixed set of texts from different genres plus
a significant set of individual sentences, often man-
ually constructed to provide particular examples of
syntactic constructions.
5.2 Spanish
The Spanish test data set was the only data set on
which the alternative cascaded approach (72.15) out-
performed our main approach (68.32). A detailed
comparison of the output files of the two systems
has revealed two differences. First, the amount of
circular links, a pair of words which have each other
as head, was larger in the analysis of the submitted
system (7%) than in the cascaded analysis (3%) and
the gold data (also 3%). Second, the number of root
words per sentence (always 1 in the gold data) was
more likely to be correct in the cascaded analysis
(70% correct; other sentences had no root) than in
the submitted approach (40% with 20% of the sen-
tences being assigned no roots and 40% more than
one root). Some of these problems might be solvable
with post-processing
Acknowledgements
This research is funded by NWO, the Netherlands
Organization for Scientific Research under the IMIX
programme, and the Dutch Ministry for Economic
Affairs? IOP-MMI programme.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2004. TiMBL: Tilburg memory based learner, ver-
sion 5.1, reference guide. Technical Report ILK 04-02, ILK
Research Group, Tilburg University.
Erik Tjong Kim Sang. 2002. Memory-based shallow parsing.
Journal of Machine Learning Research, 2(Mar):559?594.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 8th In-
ternational Workshop of Parsing Technologies (IWPT2003).
Nancy, France.
180
Proceedings of the 8th International Conference on Computational Semantics, pages 286?290,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Semantic interpretation of Dutch spoken dialogue
Jeroen Geertzen
Dept. of Communication & Information Sciences
Tilburg University, The Netherlands
j.geertzen@uvt.nl
1 Introduction
Semantic interpretation involves the process of ?translating? natural language
to a representation of its meaning. It could be understood as the task of
mapping syntax to semantics, assuming that the syntactic relationships in
an utterance correspond to functional relationships in the meaning repre-
sentation. Relevant work in this area often uses techniques from machine
translation and machine learning in the mapping from natural language to
meaning-representation languages (e.g. [9, 7]). These approaches can be ro-
bust, and thus would be useful in dealing with large quantities of utterances,
but require large amounts of annotated data.
Since the syntax of natural language does not change much from do-
main to domain, an alternative way is to use the output of a wide-coverage
syntactic parser as a basis for single, multiple, or even open-domain lan-
guage processing. To obtain a sufficiently detailed semantic representation,
the phrases in the parses should be linked with domain-specific knowledge
concepts. In this paper, a syntactic parsing based system for the semantic
interpretation of Dutch spoken language is presented and evaluated.
2 Data
Representation of semantic content is often expressed in some form of pred-
icate logic type formula. Examples are varieties of description logics, which
extend semantic frames and networks with a formal logic-based semantics
that uses predicates. In any case, the semantic representation should ideally
be powerful enough to take into account complexities such as negation, quan-
tification, a certain degree of under-specification and (complex) modifiers to
be interesting for use in advanced question answering systems and dialogue
286
systems. Moreover, the logical form should be suitable to support feasi-
ble reasoning, for which also theorem provers, model builders, and model
checkers can be used. Several semantic representations have been proposed
that take these aspects into account, such as for example Quasi Logical
Forms [1] and Dynamic Predicate Logic [6]. For the approach presented
here, a simplified first order logic is used similar to quasi logical forms. The
dialogue data that is used for semantic interpretation consists of recorded
interactions with a help desk on how to operate a fax device. Examples of
resulting utterances and their corresponding semantic content, expressed by
?-expressions of first-order logic, are illustrated in the following table:
utterance semantic content
1 wat moet ik nu doen? ?x . next-step(x)
(what do I have to do now?)
2 druk op een toets ?x . press(x) ? button(x)
(press a button)
3 druk op de groene toets ?x . press(x) ? button(x) ? color(x,?green?)
(press the green button)
4 wat doet de grote knop? ?x . function(x) ? button(x) ? size(x,?big?)
(what does the big button do?)
Three types of predicate groups are distinguished: action predicates,
element predicates, and property predicates. In the domain of operating
a fax device, the predicates and arguments in the logical expressions refer
to entities, properties, events, and tasks in the application domain. The
application domain of the fax device is complex but small: the domain model
consists of 70 entities with at most 10 properties, 72 higher-level actions or
tasks, and 45 different settings.
3 Approach
The semantic representation is obtained in two stages. In the first stage, the
utterances are syntactically parsed. In the second stage, the most probable
derivation obtained in the syntactic parsing is used to construct the semantic
representation.
For the syntactic interpretation of the utterances, the Alpino Parser is
used [3]. This HPSG-based dependency parser aims to accurately provide
287
full parses of unrestricted Dutch text and is publicly available.
1
In the context of spoken dialogue processing, a syntactic parser has to
deal with fragmented input and many syntactically less well-formed utter-
ances in comparison to text parsing. For this reason, the utterances are
additionally parsed with a shallow parser, and the resulting parse is used
in case the Alpino parser fails to provide a full parse. As shallow parser, a
memory based chunk parser trained for spoken Dutch [4] is employed.
To resolve pronouns, a simple pronoun resolution algorithm has been
implemented. This algorithm is similar to the centering algorithm proposed
in [8]. While processing the utterances, each noun phrase identified is placed
on a temporary queue which is pushed on a history stack once the utterance
or turn is closed. Upon encountering a pronoun, the first element on the
queue that meets gender and number agreement is selected as antecedent.
If no candidate is found, the previous queue on the stack is evaluated until
an antecedent is found or all queues on the history stack are evaluated.
The semantic representation is constructed by traversing the dependen-
cies in the parse and by mapping words and phrases to domain concepts.
These domain concepts are events, elements, and domain tasks stored in a
database. This process of semantic interpretation is depicted and exempli-
fied in the following figure:
Alpino
parser
MBSP
parse r
utterance
dependency
graph
mapping
DG to LF
logical
form
DB:events DB:elements DB:tasks
domain model
druk op de groene toets druk  op  de  groene  toets
hd/mod hd/mod
hd/det
hd/obj1
press(X) ^ button(X) ^ color(X,?green?)
anaphora
resolution
dependency
graph
The approach reported here has several aspects in common with that
of Bos [2], who uses a CCG based parser [5] and assigns Discourse Rep-
resentation Structures (DRSs) to the lexical categories used by the parser
1
See: http://www.let.rug.nl/
?
vannoord/alp/Alpino/.
288
after which semantic construction is driven by the syntactic derivation. A
notable difference is that Bos first constructs a DRS representation which
is subsequently translated into first-order logic. Another difference is that
in the approach described in this section, syntactic representations obtained
by the wide-coverage dependency parser are complemented with that of a
chunk parser, which increases robustness when dealing with fragmented in-
put, common in spoken dialogue.
4 Evaluation
The approach for obtaining semantic representations has been tested on a
dataset of 160 utterances and their corresponding semantic content. All ut-
terances are related to the fax domain. The performance on identifying each
of the three types of predicates in the semantic representations is specified
in the following table:
action element property
predicates predicates predicates overall
accuracy (%) 92.2 81.4 94.3 88.1
The results show that identification of element predicates is the least
successful. Where actions and properties are usually mentioned explicitly,
domain elements can be described in various ways. Moreover, in a sub-
stantial number of utterances pronouns are used, which cannot always be
resolved successfully. Nevertheless, an accuracy of 88.1% is achieved on
recovering complete semantic representations.
References
[1] Hiyan Alshawi. Resolving Quasi Logical Forms. Computational Linguistics,
16(3):133?144, 1990.
[2] Johan Bos. Computational semantics in discourse: Underspecification, resolu-
tion, and inference. Journal of Logic, Language and Information, 13(2):139?157,
2004.
[3] Gosse Bouma, Gertjan van Noord, and Robert Malouf. Alpino: Wide-coverage
computational analysis of dutch. In Proc. CLIN-11, pages 45?59. Amsterdam,
2001.
[4] Sander Canisius and Antal van den Bosch. A memory-based shallow parser for
spoken dutch. In Proc. CLIN-14, pages 31?45, Antwerp, 2003.
289
[5] Stephen Clark and James R. Curran. Parsing the WSJ using CCG and log-linear
models. In Proc. ACL 2004, pages 103?110, Barcelona, 2004.
[6] Jeroen Groenendijk and Martin Stokhof. Dynamic Predicate Logic. Linguistics
and Philosophy, 14(1):39?100, 1991.
[7] Rohit J. Kate and Raymond J. Mooney. Using string-kernels for learning se-
mantic parsers. In Proc. ACL-COLING, pages 913?920, Sydney, 2006.
[8] Joel R. Tetreault. A corpus-based evaluation of centering and pronoun resolu-
tion. Computational Linguistics, 27(4):507?520, 2001.
[9] Yuk W. Wong and Raymond Mooney. Learning for semantic parsing with
statistical machine translation. In Proceedings of HLT/NAACL, pages 439?446,
New York, 2006.
290
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 7?15,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Dialogue Act Prediction Using Stochastic Context-Free Grammar
Induction
Jeroen Geertzen
Research Centre for English & Applied Linguistics
University of Cambridge, UK
jg532@cam.ac.uk
Abstract
This paper presents a model-based ap-
proach to dialogue management that is
guided by data-driven dialogue act predic-
tion. The statistical prediction is based on
stochastic context-free grammars that have
been obtained by means of grammatical
inference. The prediction performance of
the method compares favourably to that of
a heuristic baseline and to that of n-gram
language models.
The act prediction is explored both for
dialogue acts without realised semantic
content (consisting only of communicative
functions) and for dialogue acts with re-
alised semantic content.
1 Introduction
Dialogue management is the activity of determin-
ing how to behave as an interlocutor at a specific
moment of time in a conversation: which action
can or should be taken at what state of the dia-
logue. The systematic way in which an interlocu-
tor chooses among the options for continuing a di-
alogue is often called a dialogue strategy.
Coming up with suitable dialogue management
strategies for dialogue systems is not an easy task.
Traditional methods typically involve manually
crafting and tuning frames or hand-crafted rules,
requiring considerable implementation time and
cost. More recently, statistical methods are be-
ing used to semi-automatically obtain models that
can be trained and optimised using dialogue data.1
These methods are usually based on two assump-
tions. First, the training data is assumed to be
representative of the communication that may be
encountered in interaction. Second, it is assumed
that dialogue can be modelled as a Markov De-
cision Process (MDP) (Levin et al, 1998), which
1See e.g. (Young, 2002) for an overview.
implies that dialogue is modelled as a sequential
decision task in which each contribution (action)
results in a transition from one state to another.
The latter assumption allows to assign a reward
for action-state pairs, and to determine the dia-
logue management strategy that results in the max-
imum expected reward by finding for each state
the optimal action by using reinforcement learn-
ing (cf. (Sutton and Barto, 1998)). Reinforce-
ment learning approaches to dialogue manage-
ment have proven to be successful in several task
domains (see for example (Paek, 2006; Lemon et
al., 2006)). In this process there is no supervision,
but what is optimal depends usually on factors that
require human action, such as task completion or
user satisfaction.
The remainder of this paper describes and eval-
uates a model-based approach to dialogue man-
agement in which the decision process of taking
a particular action given a dialogue state is guided
by data-driven dialogue act prediction. The ap-
proach improves over n-gram language models
and can be used in isolation or for user simula-
tion, without yet providing a full alternative to re-
inforcement learning.
2 Using structural properties of
task-oriented dialogue
One of the best known regularities that are ob-
served in dialogue are the two-part structures,
known as adjacency pairs (Schegloff, 1968), like
QUESTION-ANSWER or GREETING-GREETING.
A simple model of predicting a plausible next
dialogue act that deals with such regularities could
be based on bigrams, and to include more context
also higher-order n-grams could be used. For in-
stance, Stolcke et al (2000) explore n-gram mod-
els based on transcribed words and prosodic in-
formation for SWBD-DAMSL dialogue acts in the
Switchboard corpus (Godfrey et al, 1992). After
training back-off n-gram models (Katz, 1987) of
7
different order using frequency smoothing (Witten
and Bell, 1991), it was concluded that trigrams and
higher-order n-grams offer a small gain in predi-
cation performance with respect to bigrams.
Apart from adjacency pairs, there is a variety
of more complex re-occurring interaction patterns.
For instance, the following utterances with cor-
responding dialogue act types illustrate a clarifi-
cation sub-dialogue within an information-request
dialogue:
1 A: How do I do a fax? QUESTION
2 B: Do you want to send QUESTION
or print one?
3 A: I want to print it ANSWER
4 B: Just press the grey button ANSWER
Such structures have received considerable at-
tention and their models are often referred to as
discourse/dialogue grammars (Polanyi and Scha,
1984) or conversational/dialogue games (Levin
and Moore, 1988).
As also remarked by Levin (1999), predict-
ing and recognising dialogue games using n-gram
models is not really successful. There are vari-
ous causes for this. The flat horizontal structure of
n-grams does not allow (hierarchical) grouping of
symbols. This may weaken the predictive power
and reduces the power of the representation since
nested structures such as exemplified above cannot
be represented in a straightforward way.
A better solution would be to express the struc-
ture of dialogue games by a context-free grammar
(CFG) representation in which the terminals are
dialogue acts and the non-terminals denote con-
versational games. Construction of a CFG would
require explicit specification of a discourse gram-
mar, which could be done by hand, but it would be
a great advantage if CFGs could automatically be
induced from the data. An additional advantage
of grammar induction is the possibility to assess
the frequency of typical patterns and a stochastic
context-free grammar (SCFG) may be produced
which can be used for parsing the dialogue data.
3 Sequencing dialogue acts
Both n-gram language models and SCFG based
models work on sequences of symbols. Using
more complex symbols increases data sparsity:
encoding more information increases the number
of unique symbols in the dataset and decreases
the number of reoccurring patterns which could be
used in the prediction.
In compiling the symbols for the prediction ex-
periments, three aspects are important: the identi-
fication of interlocutors, the definition of dialogue
acts, and multifunctionality in dialogue.
The dialogue act taxonomy that is used in the
prediction experiments is that of DIT (Bunt, 2000).
A dialogue act is defined as a pair consisting of a
communicative function (CF) and a semantic con-
tent (SC): a =< CF,SC >. The DIT taxonomy
distinguishes 11 dimensions of communicative
functions, addressing information about the task
domain, feedback, turn management, and other
generic aspects of dialogue (Bunt, 2006). There
are also functions, called the general-purpose
functions, that may occur in any dimension. In
quite some cases, particularly when dialogue con-
trol is addressed and dimension-specific functions
are realised, the SC is empty. General-purpose
functions, by contrast, are always used in combi-
nation with a realised SC. For example:
dialogue act
utterance function semantic content
What to do next? SET-QUESTION next-step(X)
Press the button. SET-ANSWER press(Y) ?
button(Y)
The SC ?if realised? describes objects, prop-
erties, and events in the domain of conversation.
In dialogue act prediction while taking multi-
dimensionality into account, a dialogue D can be
represented as a sequence of events in which an
event is a set of one dialogue act or multiple di-
alogue acts occurring simultaneously. The infor-
mation concerning interlocutor and multifunction-
ality is encoded in a single symbol and denoted by
means of a n-tuple. Assuming that at most three
functions can occur simultaneously, a 4-tuple is
needed2: (interlocutor,da1,da2,da3). An ex-
ample of a bigram of 4-tuples would then look as
follows:
(A,<SET-Q,"next-step(X)">, , ) ,
(B,<SET-A,"press(Y) ? button(Y)">, , )
Two symbols are considered to be identical when
the same speaker is involved and when the sym-
bols both address the same functions. To make
2Ignoring the half percent of occurrences with four simul-
taneous functions.
8
it easy to determine if two symbols are identical,
the order of elements in a tuple is fixed: func-
tions that occur simultaneously are first ordered on
importance of dimension, and subsequently on al-
phabet. The task-related functions are considered
the most important, followed by feedback-related
functions, followed by any other remaining func-
tions. This raises the question how recognition
performance using multifunctional symbols com-
pares against recognition performance using sym-
bols that only encode the primary function
4 N-gram language models
There exists a significant body of work on the use
of language models in relation to dialogue man-
agement. Nagata and Morimoto (1994) describe a
statistical model of discourse based on trigrams of
utterances classified by custom speech act types.
They report 39.7% prediction accuracy for the top
candidate and 61.7% for the top three candidates.
In the context of the dialogue component of the
speech-to-speech translation system VERBMO-
BIL, Reithinger and Maier (1995) use n-gram dia-
logue act probabilities to suggest the most likely
dialogue act. In later work, Alexandersson and
Reithinger (1997) describe an approach which
comes close to the work reported in this paper: Us-
ing grammar induction, plan operators are semi-
automatically derived and combined with a statis-
tical disambiguation component. This system is
claimed to have an accuracy score of around 70%
on turn management classes.
Another study is that of Poesio and Mikheev
(1998), in which prediction based on the previous
dialogue act is compared with prediction based on
the context of dialogue games. Using the Map
Task corpus annotated with ?moves? (dialogue
acts) and ?transactions? (games) they showed that
by using higher dialogue structures it was possi-
ble to perform significantly better than a bigram
model approach. Using bigrams, 38.6% accuracy
was achieved. Additionally taking game structure
into account resulted in 50.6%; adding informa-
tion about speaker change resulted in an accuracy
of 41.8% with bigrams, 54% with game structure.
All studies discussed so far are only concerned
with sequences of communicative functions, and
disregard the semantic content of dialogue acts.
5 Dialogue grammars
To automatically induce patterns from dialogue
data in an unsupervised way, grammatical infer-
ence (GI) techniques can be used. GI is a branch
of unsupervised machine learning that aims to find
structure in symbolic sequential data. In this case,
the input of the GI algorithm will be sequences of
dialogue acts.
5.1 Dialogue Grammars Inducer
For the induction of structure, a GI algorithm has
been implemented that will be referred to as Dia-
logue Grammars Inducer (DGI). This algorithm is
based on distributional clustering and alignment-
based learning (van Zaanen and Adriaans, 2001;
van Zaanen, 2002; Geertzen and van Zaanen,
2004). Alignment-based learning (ABL) is a sym-
bolic grammar inference framework that has suc-
cessfully been applied to several unsupervised ma-
chine learning tasks in natural language process-
ing. The framework accepts sequences with sym-
bols, aligns them with each other, and compares
them to find interchangeable subsequences that
mark structure. As a result, the input sequences
are augmented with the induced structure.
The DGI algorithm takes as input time series of
dialogue acts, and gives as output a set of SCFGs.
The algorithm has five phases:
1. SEGMENTATION: In the first phase of DGI,
the time series are ?if necessary? seg-
mented in smaller sequences based on a spe-
cific time interval in which no communica-
tion takes place. This is a necessary step in
task-oriented conversation in which there is
ample time to discuss (and carry out) several
related tasks, and an interaction often con-
sists of a series of short dialogues.
2. ALIGNMENT LEARNING: In the second
phase a search space of possible structures,
called hypotheses, is generated by compar-
ing all input sequences with each other and
by clustering sub-sequences that share simi-
lar context. To illustrate the alignment learn-
ing, consider the following input sequences:
A:SET-Q, B:PRO-Q, A:PRO-A, B:SET-A.
A:SET-Q, B:PAUSE, B:RESUME, B:SET-A.
A:SET-Q, B:SET-A.
The alignment learning compares all input
sequences with each other, and produces the
9
hypothesised structures depicted below. The
induced structure is represented using brack-
eting.
[i A:SET-Q, [j B:PRO-Q, A:PRO-A, ]j B:SET-A. ]i
[i A:SET-Q, [j B:PAUSE, A:RESUME, ]j B:SET-A. ]i
[i A:SET-Q, [j ]j B:SET-A. ]i
The hypothesis j is generated because of the
similar context (which is underlined). The
hypothesis i, the full span, is introduced by
default, as it might be possible that the se-
quence is in itself a part of a longer sequence.
3. SELECTION LEARNING: The set of hypothe-
ses that is generated during alignment learn-
ing contains hypotheses that are unlikely to
be correct. These hypotheses are filtered out,
overlapping hypotheses are eliminated to as-
sure that it is possible to extract a context-
free grammar, and the remaining hypotheses
are selected and remain in the bracketed out-
put. The decision of which hypotheses to se-
lect and which to discard is based on a Viterbi
beam search (Viterbi, 1967).
4. EXTRACTION: In the fourth phase, SCFG
grammars are extracted from the remaining
hypotheses by means of recursive descent
parsing. Ignoring the stochastic informa-
tion, a CFG of the above-mentioned example
looks in terms of grammar rules as depicted
below:
S ? A:SET-Q J B:SET-A
J ? B:PRO-Q A:PRO-A
J ? B:PAUSE A:RESUME
J ? ?
5. FILTERING: In the last phase, the SCFG
grammars that have small coverage or involve
many non-terminals are filtered out, and the
remaining SCFG grammars are presented as
the output of DGI.
Depending on the mode of working, the DGI
algorithm can generate a SCFG covering the com-
plete input or can generate a set of SCFGs. In the
former mode, the grammar that is generated can be
used for parsing sequences of dialogue acts and by
doing so suggests ways to continue the dialogue.
In the latter mode, by parsing each grammar in the
set of grammars that are expected to represent di-
alogue games in parallel, specific dialogue games
may be recognised, which can in turn be used ben-
eficially in dialogue management.
5.2 A worked example
In testing the algorithm, DGI has been used to
infer a set of SCFGs from a development set of
250 utterances of the DIAMOND corpus (see also
Section 6.1). Already for this small dataset, DGI
produced, using default parameters, 45 ?dialogue
games?. One of the largest produced structures
was the following:
4 S ? A:SET-Q , NTAX , NTBT , B:SET-A
4 NTAX ? B:PRO-Q , NTFJ
3 NTFJ ? A:PRO-A
1 NTFJ ? A:PRO-A , A:CLARIFY
2 NTBT ? B:PRO-Q , A:PRO-A
2 NTBT ? ?
In this figure, each CFG rule has a number in-
dicating how many times the rules has been used.
One of the dialogue fragments that was used to in-
duce this structure is the following excerpt:
utterance dialogue act
A1 how do I do a short code? SET-Q
B1 do you want to program one? PRO-Q
A2 no SET-A
A3 I want to enter a kie* a short code CLARIFY
B2 you want to use a short code? PRO-Q
A4 yes PRO-A
B3 press the VK button SET-A
Unfortunately, many of the 45 induced struc-
tures were very small or involved generalisations
already based on only two input samples. To en-
sure that the grammars produced by DGI gen-
eralise better and are less fragmented, a post-
processing step has been added which traverses
the grammars and eliminates generalisations based
on a low number of samples. In practice, this
means that the post-processing requires the re-
maining grammatical structure to be presented N
times or more in the data.3. The algorithm without
post-processing will be referred to as DGI1; the
algorithm with post-processing as DGI2.
6 Act prediction experiments
To determine how to behave as an interlocutor at
a specific moment of time in a conversation, the
DGI algorithm can be used to infer a SCFG that
models the structure of the interaction. The SCFG
3N = 2 by default, but may increase with the size of the
training data.
10
can then be used to suggest a next dialogue act
to continue the dialogue. In this section, the per-
formance of the proposed SCFG based dialogue
model is compared with the performance of the
well-known n-gram language models, both trained
on intentional level, i.e. on sequences of sets of di-
alogue acts.
6.1 Data
The task-oriented dialogues used in the dialogue
act prediction tasks were drawn from the DIA-
MOND corpus (Geertzen et al, 2004), which con-
tains human-machine and human-human Dutch
dialogues that have an assistance seeking na-
ture. The dataset used in the experiments con-
tains 1, 214 utterances representing 1, 592 func-
tional segments from the human-human part of
corpus. In the domain of the DIAMOND data,
i.e. operating a fax device, the predicates and argu-
ments in the logical expressions of the SC of the
dialogue acts refer to entities, properties, events,
and tasks in the application domain. The appli-
cation domain of the fax device is complex but
small: the domain model consists of 70 entities
with at most 10 properties, 72 higher-level actions
or tasks, and 45 different settings.
Representations of semantic content are often
expressed in some form of predicate logic type
formula. Examples are Quasi Logical Forms (Al-
shawi, 1990), Dynamic Predicate Logic (Groe-
nendijk and Stokhof, 1991), and Underspecified
Discourse Representation Theory (Reyle, 1993).
The SC in the dataset is in a simplified first order
logic similar to quasi logical forms, and is suitable
to support feasible reasoning, for which also theo-
rem provers, model builders, and model checkers
can be used. The following utterances and their
corresponding SC characterise the dataset:
1 wat moet ik nu doen?
(what do I have to do now?)
?x . next-step(x)
2 druk op een toets
(press a button)
?x . press(x) ? button(x)
3 druk op de groene toets
(press the green button)
?x . press(x) ? button(x) ? color(x,?green?)
4 wat zit er boven de starttoets?
(what is located above the starttoets?)
?x . loc-above(x,?button041?)
Three types of predicate groups are distin-
guished: action predicates, element predicates,
and property predicates. These types have a fixed
order. The action predicates appear before element
predicates, which appear in turn before property
predicates. This allows to simplify the semantic
content for the purpose of reducing data sparsity
in act prediction experiments, by stripping away
e.g. property predicates. For instance, if desired
the SC of utterance 3 in the example could be sim-
plified to that of utterance 2, making the semantics
less detailed but still meaningful.
6.2 Methodology and metrics
Evaluation of overall performance in communi-
cation is problematic; there are no generally ac-
cepted criteria as to what constitutes an objective
and sound way of comparative evaluation. An
often-used paradigm for dialogue system evalua-
tion is PARADISE (Walker et al, 2000), in which
the performance metric is derived as a weighted
combination of subjectively rated user satisfac-
tion, task-success measures and dialogue cost.
Evaluating if the predicted dialogue acts are con-
sidered as positive contributions in such a way
would require the model to be embedded in a fully
working dialogue system.
To assess whether the models that are learned
produce human-like behaviour without resorting
to costly user interaction experiments, it is needed
to compare their output with real human responses
given in the same contexts. This will be done by
deriving a model from one part of a dialogue cor-
pus and applying the model on an ?unseen? part
of the corpus, comparing the suggested next dia-
logue act with the observed next dialogue act. To
measure the performance, accuracy is used, which
is defined as the proportion of suggested dialogue
acts that match the observed dialogue acts.
In addition to the accuracy, also perplexity is
used as metric. Perplexity is widely used in re-
lation to speech recognition and language models,
and can in this context be understood as a metric
that measures the number of equiprobable possi-
ble choices that a model faces at a given moment.
Perplexity, being related to entropy is defined as
follows:
Entropy = ?
?
i
p(wi|h) ? log2 p(wi|h)
Perplexity = 2Entropy
11
where h denotes the conditioned part, i.e. wi?1
in the case of bigrams and wi?2, wi?1 in the case
of trigrams, et cetera. In sum, accuracy could be
described as a measure of correctness of the hy-
pothesis and perplexity could be described as how
probable the correct hypothesis is.
For all n-gram language modelling tasks re-
ported, good-turing smoothing was used (Katz,
1987). To reduce the effect of imbalances in the
dialogue data, the results were obtained using 5-
fold cross-validation.
To have an idea how the performance of both
the n-gram language models and the SCFG mod-
els relate to the performance of a simple heuris-
tic, a baseline has been computed which suggests
a majority class label according to the interlocutor
role in the dialogue. The information seeker has
SET-Q and the information provider has SET-A as
majority class label.
6.3 Results for communicative functions
The scores for communicative function prediction
are presented in Table 1. For each of the three
kinds of symbols, accuracy and perplexity are cal-
culated: the first two columns are for the main CF,
the second two columns are for the combination
of speaker identity and main CF, and the third two
columns are for the combination of speaker iden-
tity and all CFs. The scores for the latter two cod-
ings could not be calculated for the 5-gram model,
as the data were too sparse.
As was expected, there is an improvement in
both accuracy (increasing) and perplexity (de-
creasing) for increasing n-gram order. After the
4-gram language model, the scores drop again.
This could well be the result of insufficient train-
ing data, as the more complex symbols could not
be predicted well.
Both language models and SCFG models per-
form better than the baseline, for all three groups.
The two SCFG models, DGI1 and DGI2, clearly
outperform the n-gram language models with a
substantial difference in accuracy. Also the per-
plexity tends to be lower. Furthermore, model
DGI2 performs clearly better than model DGI1,
which indicates that the ?flattening? of non-
terminals which is described in Section 5 results
in better inductions.
When comparing the three groups of sequences,
it can be concluded that providing the speaker
identity combined with the main communicative
function results in better accuracy scores of 5.9%
on average, despite the increase in data sparsity. A
similar effect has also been reported by Stolcke et
al. (2000).
Only for the 5-gram language model, the data
become too sparse to learn reliably a language
model from. There is again an increase in per-
formance when also the last two positions in the
4-tuple are used and all available dialogue act as-
signments are available. It should be noted, how-
ever, that this increase has less impact than adding
the speaker identity. The best performing n-gram
language model achieved 66.4% accuracy; the
best SCFG model achieved 78.9% accuracy.
6.4 Results for dialogue acts
The scores for prediction of dialogue acts, includ-
ing SC, are presented in the left part of Table 2.
The presentation is similar to Table 1: for each of
the three kinds of symbols, accuracy and perplex-
ity were calculated. For dialogue acts that may in-
clude semantic content, computing a useful base-
line is not obvious. The same baseline as for com-
municative functions was used, which results in
lower scores.
The table shows that the attempts to learn to
predict additionally the semantic content of utter-
ances quickly run into data sparsity problems. It
turned out to be impossible to make predictions
from 4-grams and 5-grams, and for 3-grams the
combination of speaker and all dialogue acts could
not be computed. Training the SCFGs, by con-
trast, resulted in fewer problems with data sparsity,
as the models abstract quickly.
As with predicting communicative functions,
the SCFG models show better performance than
the n-gram language models, for which the 2-
grams show slightly better results than the 3-
grams. Where there was a notable performance
difference between DGI1 and DGI2 for CF pre-
diction, for dialogue act prediction there is only a
very little difference, which is insignificant con-
sidering the relatively high standard deviation.
This small difference is explained by the fact that
DGI2 becomes less effective as the size of the
training data decreases.
As with CF prediction, it can be concluded that
providing the speaker identity with the main dia-
logue act results in better scores, but the difference
is less big than observed with CF prediction due to
the increased data sparsity.
12
Table 1: Communicative function prediction scores for n-gram language models and SCFGs in accuracy
(acc, in percent) and perplexity (pp). CFmain denotes the main communicative function, SPK speaker
identity, and CFall all occurring communicative functions.
CFmain SPK + CFmain SPK + CFall
acc pp acc pp acc pp
baseline 39.1?0.23 24.2?0.19 44.6?0.92 22.0?0.25 42.9?1.33 23.7?0.41
2-gram 53.1?0.88 17.9?0.35 58.3?1.84 16.8?0.31 61.1?1.65 16.3?0.59
3-gram 58.6?0.85 17.1?0.47 63.0?1.98 14.5?0.26 65.9?1.92 14.0?0.23
4-gram 60.9?1.12 16.7?0.15 65.4?1.62 15.2?1.07 66.4?2.03 14.2?0.44
5-gram 60.3?0.43 18.6?0.21 - - - -
DGI1 67.4?3.05 18.3?1.28 74.6?1.94 14.8?1.47 76.5?2.13 13.9?0.35
DGI2 71.8?2.67 16.1?1.25 78.3?2.50 14.0?2.39 78.9?1.98 13.6?0.35
Table 2: Dialogue act prediction scores for n-gram language models and SCFGs. DAmain denotes the
dialogue act with the main communicative function, and DAall all occurring dialogue acts.
DAmain SPK + DAmain SPK + DAall
full SC simplified SC
acc pp acc pp acc pp acc pp
baseline 18.5?2.01 31.0?1.64 19.3?1.79 27.6?0.93 18.2?1.93 31.6?1.38 18.2?1.93 31.6?1.38
2-gram 31.2?1.42 28.5?1.03 34.6?1.51 24.7?0.62 35.1?1.30 26.9?0.47 37.5?1.34 26.2?2.37
3-gram 29.0?1.14 34.7?2.82 31.9?1.21 30.5?2.06 - - 29.1?1.28 28.0?2.59
4-gram - - - - - - - -
5-gram - - - - - - - -
DGI1 38.8?3.27 25.1?0.94 42.5?0.96 25.0?1.14 42.9?2.44 27.3?1.98 46.6?2.01 24.6?2.24
DGI2 39.2?2.45 25.0?1.28 42.7?1.03 25.3?0.99 42.4?2.19 28.0?1.57 46.4?1.94 24.7?2.55
The prediction scores of dialogue acts with full
semantic content and simplified semantic content
are presented in the right part of Table 2. For both
cases multifunctionality is taken into account by
including all occurring communicative functions
in each symbol. As can be seen from the table,
the simplification of the semantic content leads to
improvements in the prediction performance for
both types of model. The best n-gram language
model improved with 2.4% accuracy from 35.1%
to 37.5%; the best SCFG-based model improved
with 3.7% from 42.9% to 46.6%.
Moreover, the simplification of the semantic
content reduced the problem of data-sparsity, mak-
ing it also possible to predict based on 3-grams
although the accuracy is considerably lower than
that of the 2-gram model.
7 Discussion
Both n-gram language models and SCFG based
models have their strengths and weaknesses. n-
gram models have the advantage of being very ro-
bust and they can be easily trained. The SCFG
based model can capture regularities that have
gaps, and allow to model long(er) distance rela-
tions. Both algorithms work on sequences and
hence are easily susceptible to data-sparsity when
the symbols in the sequences get more complex.
The SCFG approach, though, has the advantage
that symbols can be clustered in the non-terminals
of the grammar, which allows more flexibility.
The multidimensional nature of the DIT++
functions can be adequately encoded in the sym-
bols of the sequences. Keeping track of the inter-
locutor and including not only the main commu-
nicative function but also other functions that oc-
cur simultaneously results in better performance
even though it decreases the amount of data to
learn from.
The prediction experiments based on main com-
municative functions assume that in case of multi-
functionality, a main function can clearly be iden-
tified. Moreover, it is assumed that task-related
functions are more important than feedback func-
tions or other functions. For most cases, these as-
sumptions are justified, but in some cases they are
13
problematic. For instance, in a heated discussion,
the turn management function could be considered
more important for the dialogue than a simultane-
ously occurring domain specific function. In other
cases, it is impossible to clearly identify a main
function as all functions occurring simultaneously
are equally important to the dialogue.
In general, n-grams of a higher order have a
higher predictability and therefore a lower per-
plexity. However, using high order n-grams is
problematic due to sparsity of training data, which
clearly is the case with 4-grams, and particularly
with 5-grams in combination with complex sym-
bols as for CF prediction.
Considerably more difficult is the prediction of
dialogue acts with realised semantic content, as
is evidenced in the differences in accuracy and
perplexity for all models. Considering that the
data set, with about 1, 600 functional segments,
is rather small, the statistical prediction of logical
expressions increases data sparsity to such a de-
gree that from the n-gram language models, only
2-gram (and 3-grams to some extent) could be
trained. The SCFG models can be trained for both
CF prediction and dialogue act prediction.
As noted in Section 6.2, objective evaluation of
dialogue strategies and behaviour is difficult. The
evaluation approach used here compares the sug-
gested next dialogue act with the next dialogue act
as observed. This is done for each dialogue act in
the test set. This evaluation approach has the ad-
vantage that the evaluation metric can easily be un-
derstood and computed. The approach, however,
is also very strict: in a given dialogue context, con-
tinuations with various types of dialogue acts may
all be equally appropriate. To also take other pos-
sible contributions into account, a rich dataset is
required in which interlocutors act differently in
similar dialogue context with a similar established
common ground. Moreover, such a dataset should
contain for each of these cases with similar dia-
logue context a representative set of samples.
8 Conclusions and future work
An approach to the prediction of communicative
functions and dialogue acts has been presented
that makes use of grammatical inference to auto-
matically extract structure from corpus data. The
algorithm, based on alignment-based learning, has
been tested against a baseline and several n-gram
language models. From the results it can be con-
cluded that the algorithm outperforms the n-gram
models: on the task of predicting the communica-
tive functions, the best performing n-gram model
achieved 66.4% accuracy; the best SCFG model
achieved 78.9% accuracy. Predicting the seman-
tic content in combination with the communica-
tive functions is difficult, as evidenced by moder-
ate scores. Obtaining lower degree n-gram lan-
guage models is feasible, whereas higher degree
models are not trainable. Prediction works better
with the SCFG models, but does not result in con-
vincing scores. As the corpus is small, it is ex-
pected that with scaling up the available training
data, scores will improve for both tasks.
Future work in this direction can go in sev-
eral directions. First, the grammar induction ap-
proach shows potential of learning dialogue game-
like structures unsupervised. The performance on
this task could be tested and measured by applying
the algorithm on corpus data that have been anno-
tated with dialogue games. Second, the models
could also be extended to incorporate more infor-
mation than dialogue acts alone. This could make
comparisons with the performance obtained with
reinforcement learning or with Bayesian networks
interesting. Third, it would be interesting to learn
and apply the same models on other kinds of con-
versation, such as dialogue with more than two in-
terlocutors. Fourth, datasets could be drawn from
a large corpus that covers dialogues on a small
but complex domain. This makes it possible to
evaluate according to the possible continuations
as found in the data for situations with similar di-
alogue context, rather than to evaluate according
to a single possible continuation. Last, the rather
unexplored parameter space of the DGI algorithm
might be worth exploring in optimising the sys-
tem?s performance.
References
Jan Alexandersson and Norbert Reithinger. 1997.
Learning dialogue structures from a corpus. In
Proceedings of Eurospeech 1997, pages 2231?2234,
Rhodes, Greece, September.
Hiyan Alshawi. 1990. Resolving quasi logical forms.
Computational Linguistics, 16(3):133?144.
Harry Bunt. 2000. Dialogue pragmatics and context
specification. In Harry Bunt and William Black, ed-
itors, Abduction, Belief and Context in Dialogue;
Studies in Computational Pragmatics, pages 81?
150. John Benjamins, Amsterdam, The Netherlands.
14
Harry Bunt. 2006. Dimensions in dialogue annota-
tion. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), pages 1444?1449, Genova, Italy, May.
Jeroen Geertzen and Menno M. van Zaanen. 2004.
Grammatical inference using suffix trees. In
Proceedings of the 7th International Colloquium
on Grammatical Inference (ICGI), pages 163?174,
Athens, Greece, October.
Jeroen Geertzen, Yann Girard, and Roser Morante.
2004. The DIAMOND project. Poster at the 8th
Workshop on the Semantics and Pragmatics of Dia-
logue (CATALOG 2004), Barcelona, Spain, July.
John Godfrey, Edward Holliman, and Jane McDaniel.
1992. SWITCHBOARD: Telephone speech corpus
for research and development. In Proceedings of the
ICASSP-92, pages 517?520, San Francisco, USA.
Jeroen Groenendijk and Martin Stokhof. 1991. Dy-
namic predicate logic. Linguistics and Philosophy,
14(1):39?100.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating effectiveness and portabil-
ity of reinforcement learned dialogue strategies with
real users: The talk towninfo evaluation. In Spoken
Language Technology Workshop, pages 178?181.
Joan A. Levin and Johanna A. Moore. 1988. Dialogue-
games: metacommunication structures for natural
language interaction. Distributed Artificial Intelli-
gence, pages 385?397.
Esther Levin, Roberto Pieraccini, and Wieland Eck-
ert. 1998. Using markov decision process for
learning dialogue strategies. In Proceedings of the
ICASSP?98, pages 201?204, Seattle, WA, USA.
Lori Levin, Klaus Ries, Ann Thyme?-Gobbel, and Alon
Lavie. 1999. Tagging of speech acts and dialogue
games in spanish call home. In Proceedings of ACL-
99 Workshop on Discourse Tagging, College Park,
MD, USA.
Masaaki Nagata and Tsuyoshi Morimoto. 1994. First
steps towards statistical modeling of dialogue to pre-
dict the speech act type of the next utterance. Speech
Communication, 15(3-4):193?203.
Tim Paek. 2006. Reinforcement learning for spoken
dialogue systems: Comparing strenghts and weak-
nesses for practical deployment. In Interspeech
Workshop on ?Dialogue on Dialogues?.
Massimo Poesio and Andrei Mikheev. 1998. The
predictive power of game structure in dialogue
act recognition: Experimental results using maxi-
mum entropy estimation. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP-98), Sydney, Australia, December.
Livia Polanyi and Remko Scha. 1984. A syntactic ap-
proach to discourse semantics. In Proceedings of
the 10th international conference on Computational
linguistics, pages 413?419, Stanford, CA, USA.
Norbert Reithinger and Elisabeth Maier. 1995. Uti-
lizing statistical dialogue act processing in VERB-
MOBIL. In Proceedings of the 33rd annual meeting
on the Association for Computational Linguistics
(ACL), pages 116?121, Cambridge, Massachusetts.
Association for Computational Linguistics (ACL).
Uwe Reyle. 1993. Dealing with ambiguities by under-
specification: Construction, representation and de-
duction. Journal of Semantics, 10(2):123?179.
Emanuel A. Schegloff. 1968. Sequencing in con-
versational openings. American Anthropologist,
70:1075?1095.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). MIT Press,
March.
Menno van Zaanen and Pieter W. Adriaans. 2001.
Comparing two unsupervised grammar induction
systems: Alignment-Based Learning vs. EMILE.
Technical Report TR2001.05, University of Leeds,
Leeds, UK, March.
Menno M. van Zaanen. 2002. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds, Leeds, UK, January.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260?269, April.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with paradise. Natural Language Engineering,
6(3-4):363?377.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Steve Young. 2002. The statistical approach to the
design of spoken dialogue systems. Technical Re-
port CUED/F-INFENG/TR.433, Engineering De-
partment, Cambridge University, UK, September.
15
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 218?221,
Paris, October 2009. c?2009 Association for Computational Linguistics
Wide-coverage parsing of speech transcripts
Jeroen Geertzen
Research Centre for English & Applied Linguistics
University of Cambridge, UK
jg532@cam.ac.uk
Abstract
This paper discusses the performance
difference of wide-coverage parsers on
small-domain speech transcripts. Two
parsers (C&C CCG and RASP) are tested
on the speech transcripts of two different
domains (parent-child language, and pic-
ture descriptions).
The performance difference between
the domain-independent parsers and
two domain-trained parsers (MSTParser
and MEGRASP) is substantial, with a
difference of at least 30 percent point
in accuracy. Despite this gap, some of
the grammatical relations can still be
recovered reliably.
1 Introduction
Even though wide-coverage, domain-
independent1 parser systems may perform
sufficiently well for the task at hand, obtaining
highly accurate parses of sentences in a par-
ticular domain usually requires the parser to
be domain-trained. Training a parser requires
a sufficient amount of labelled data (a gold
standard), something that is only available for
very few domains. When accurate parses of
sentences in a new domain are desired, there
are several ways to proceed. Hand labelling all
data in the new domain is a consideration, but is
usually unfeasible as manual annotation is a costly
activity. Another possibility is to minimise the
amount of annotation effort required to achieve
good performance by resorting to semi-automatic
annotation or domain adaptation methods. In
any case, dedicated effort is still required to
obtain highly accurate parses, even with recent
1In this paper, the terms ?wide-coverage? and ?domain-
independent? are used synonymously.
automated domain adaptation methods (Dredze
et al, 2007).
Work that requires parsing in a new domain as
basis of further study or as part of a larger nat-
ural language processing system usually involves
a domain-independent parser with the expectation
that parses are sufficiently accurate for the specific
purpose.2 For instance, Bos and Markert (2005)
use a wide-coverage CCG-parser (Clark and Cur-
ran, 2007) to generate semantic representations for
recognising textual entailment. Geertzen (2009)
uses a HPSG-based dependency parser (Bouma
et al, 2001) to obtain the semantic content of utter-
ances. And in the study of child language acqui-
sition, Buttery and Korhonen (2007) use RASP, a
wide-coverage dependency parser (Briscoe et al,
2006), to look at lexical acquisition.
The goal of this paper is to give an indication
of wide-coverage, domain-independent parser per-
formance on specific domains. Additionally, the
study gives insight into RASP?s performance on
CHILDES, allowing to factor in parsing perfor-
mance in the syntax-based study of Buttery and
Korhonen (2007).
2 Parsing speech transcripts
Parsing performance of two domain-independent
parsers, C&C CCG en RASP, is evaluated on two
speech domains. The first domain, CHILDES, in-
volves parent-child interactions; the second do-
main, CCC, involves a picture description task.
2.1 Parsing systems
Two wide-coverage parser systems are used.
RASP (Briscoe et al, 2006) is a parsing system for
2Without gold standard there is no way of knowing how
well the parser component performs with respect to a de-
sired outcome of syntactic structure. This may not neces-
sarily be a problem, as parsing in such cases is paramount,
and application-based evaluation is preferable. Moreover, it
may be that using linguistically most desired parses does not
result in best application performance.
218
English that utilises a manually-developed gram-
mar and outputs grammatical dependency rela-
tions. The C&C CCG parser (Clark and Cur-
ran, 2007) is a parsing system that is based on
an automatically extracted grammar from CCG-
Bank and uses discriminative training. Both sys-
tems are able to output the exact set of dependency
relations, and in a comparison on a 560-sentence
test set used by Briscoe and Carroll (2006), Clark
and Curran (2007) report a micro-averaged F -
score of 81.14 for the CCG parser, and 76.29 for
RASP. 3 Both parsing systems utilise the Gram-
matical Relations (GR) annotation scheme pro-
posed by Carroll et al (1998). This scheme is in-
tended to cater for parser evaluation, and extends
the dependency structure based method of eval-
uation proposed by Lin (1998). For the parent-
child interaction domain both parsing systems are
compared with two syntactic dependency parsers
that were specifically trained for CHILDES tran-
scripts: MEGRASP (Sagae et al, 2007) and MST-
parser (McDonald et al, 2005).
2.2 Speech phenomena
As CCC and CHILDES transcripts are describ-
ing spoken language, they contain various markers
that encode speech phenomena, particularly dis-
fluencies (e.g. filled pauses, partial words, false
starts, repetitions) and speech repairs (e.g. re-
tractions and corrections). Prior to parser eval-
uation, such disfluencies have been deleted from
the transcripts, which slightly improves parser per-
formance for all systems mentioned. Similar per-
formance improvements are also reported in stud-
ies that address the effect of deletion of repairs
and fillers on parsing (e.g. Charniak and Johnson
(2001); Lease and Johnson (2006)).
2.3 CHILDES data
The major part of the evaluation is based on
the parsing of parent-child interactions from the
CHILDES database (MacWhinney, 2000). A
large portion of CHILDES transcripts was recently
parsed with a domain-specific parser (Sagae et al,
2007), allowing more reliable systematic studies
of syntactic development in child language acqui-
sition. Sagae et al also released their gold stan-
dard data, allowing others to train and evaluate
3It should be remarked that such cross-formalism compar-
isons are difficult in nature. In this case, training data were
different (RASP is not tuned to newspaper text), and CCG
utilises a lexicalised parsing model where RASP does not.
other parser systems.
The gold standard data uses a GR scheme that
is based on that of Carroll et al (1998) but that
differs in two respects: the scheme is extended
to suit the specific need of the child language re-
search community (cf. (Sagae et al, 2004)), and
the scheme does not extensively and explicitly use
the GR hierarchy.
To compare parsing performance, a mapping
from RASP GRs to CHILDES GRs was manu-
ally constructed, containing 75 rules that involve
the label and optional restrictions on the word or
POS-tag of the head or dependent.
3 Parser evaluation
3.1 Measures
System performance is reported with accuracy
measures for labelled and unlabelled dependen-
cies resulting from 15-fold cross-validation.4 The
performance on each grammatical relation is ex-
pressed by precision, recall, and F1-score. Punc-
tuation has been excluded.
3.2 CHILDES
The gold-standard used for evaluation is based on
15 (out of 20) files in the Eve section of the Brown
corpus. The annotations that are available were
made with the CHILDES GR scheme, for which
an inter-annotator percentage agreement of 96.5%
(N = 2) has been reported by Sagae et al (2004).
From all manually annotated utterances initially
available, duplicates, those with less than three to-
kens (about 30% of all), and those with missing
or incomplete parses (1% of all) were removed,
resulting in a set of 14.137 sentences, comprising
93,594 tokens with 4.5 tokens per utterance on av-
erage.
The performance scores that are obtained when
the parsing systems are compared against the gold-
standard are listed in the upper part of Table 1.
As can be seen from the accuracy scores,
MEGRASP and the MSTParser perform with
more than 30 percent point accuracy considerably
better than the domain-independent parsers. How-
ever, the list of performance scores for each of
the grammatical relations in Table 2 shows that
some relations can be recovered with acceptable
4The exception being the MEGRASP, for which because
of computation problems the full gold standard was used (7%
larger than the other training sets), resulting in somewhat
higher scores than expected with cross-validation.
219
Table 1: Parsing accuracy scores.
CHILDES labelled unlabelled
RASP 60.1 69.2
CCG parser 39.1 66.5
MSTParser 93.8 95.4
MEGRASP 90.7 93.5
CCC labelled unlabelled
RASP 66.7 72.3
CCG parser 60.2 68.5
F1-scores, such as auxiliaries, determiners, sub-
jects, and objects of prepositions.5
3.3 CCC
The Cambridge Cookie-theft Corpus (CCC, TO
APPEAR, 2010) contains audio-recorded mono-
logues of 196 subjects that were asked to fully de-
scribe a scene in a picture. As a result, the domain
is small, but at the same time, sentence bound-
aries are difficult to indicate. From this corpus
of 5,628 intonational phrases, a small evaluation
set of 80 phrases has been manually annotated6
with GRs. The performance scores for each of the
parsers is listed in the lower part of Table 1. Accu-
racy scores are higher than those for CHILDES,
and the difference in labelled accuracy between
the domain-independent parsers is less than with
CHILDES. Due to space restrictions it is not pos-
sible to present performance on individual GRs,
but the GRs that are most reliably recovered are
similar to those mentioned in Section 3.2.
4 Considerations
In the work reported here, performance of domain-
independent parsers on narrow domains was cal-
culated for two domains. The availability of
more domain-specific datasets with manually su-
pervised GR annotations would allow a better gen-
eralisation of parser performance. Unfortunately,
datasets with manually verified annotations that
use the same set of syntactic dependencies are
rare.
The CHILDES figures show that the perfor-
mance difference between domain-independent
5MSTParser scores did not fit in the table, but largely
correspond in distributional characteristics, and are available
upon request.
6Not with multiple coders yet, but percentage agreement
for dependency annotation typically varies from 93-98%.
and domain-trained parsers is big. It should be
noted that these results are obtained from speech,
which is usually less syntactically well-formed
than written language. For the speech data anal-
ysed, RASP performs better than the CCG parser,
whereas Clark and Curran (2007) have shown that
the CCG parser outperforms RASP on written text.
To better explain this difference, it would be in-
sightful to compare the confusion matrices of GR
assignments. This would allow assessment on how
the domain-independent parser errors compare to
the domain-trained parser errors.
The mapping from RASP GRs to CHILDES
GRs that was constructed is exhaustive, but there
is still room for fine-tuning and more refined map-
pings, gaining up to about 2% accuracy by esti-
mate.
5 Conclusions and future work
This paper has provided performance scores of
wide-coverage parsers applied to narrow domain
spoken language transcripts to assess the perfor-
mance gap with domain-trained parsers. This gap
appears to be considerable (more than 30 percent
point for CHILDES), but a subset of GRs can still
be recovered with fair accuracy.
We have not yet dealt with comparing
domain-independent and domain-trained parser
errors, which may provide additional insight into
the strengths and weaknesses of wide-coverage
parsers for narrow use.
Acknowledgements
This work is supported by UK EPSRC Grant
EP/F030061/1.
References
Bos, J. and Markert, K. (2005). Recognising tex-
tual entailment with logical inference. In Pro-
ceedings of the HLT and EMNLP conference,
pages 628?635.
Bouma, G., van Noord, G., and Malouf, R. (2001).
Alpino: Wide-coverage computational analysis
of dutch. In Proceedings of the CLIN 2000,
pages 45?59.
Briscoe, T. and Carroll, J. (2006). Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 41?48.
220
Table 2: Performance scores of the parsing systems for major GRs. Some of the relations could not be
reliably be mapped, and are absent for the CCG parser.
RASP CCG parser MEGRASP
relation Prec Rec F1 Prec Rec F1 Prec Rec F1
aux 89.13 69.87 78.33 90.81 62.21 73.84 98.13 96.21 97.16
com 67.80 6.12 11.23 - - - 93.15 88.52 90.78
comp 22.73 64.18 33.57 24.53 53.66 33.67 80.00 84.72 82.29
coord 70.42 64.31 67.23 82.50 30.62 44.66 75.07 83.93 79.26
cpzr 74.67 20.97 32.75 - - - 90.16 85.77 87.91
det 90.34 89.38 89.86 60.88 82.54 70.07 96.38 97.27 96.82
jct 57.85 56.68 57.26 54.71 5.16 9.42 85.14 83.05 84.08
mod 63.04 76.93 69.29 16.89 47.43 24.91 90.00 90.63 90.32
obj 73.34 75.50 74.40 46.09 69.25 55.34 91.93 91.10 91.52
obj2 32.81 55.13 41.13 53.37 39.16 45.18 83.33 74.14 78.47
pobj 88.11 75.51 81.33 - - - 91.94 93.05 92.49
pred 54.77 48.94 51.69 64.60 15.55 25.07 90.21 91.08 90.65
quant 55.87 68.87 61.69 - - - 83.10 91.46 87.08
subj 74.53 67.58 70.89 66.94 66.11 66.52 94.68 95.01 94.84
xcomp 52.17 64.97 57.87 1.62 3.35 2.19 92.11 87.13 89.55
xmod 12.93 15.32 14.02 2.60 24.19 4.69 56.64 65.32 60.67
Briscoe, T., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL on Interactive presen-
tation sessions, pages 77?80.
Buttery, P. and Korhonen, A. (2007). I will
shoot your shopping down and you can shoot
all my tins?automatic lexical acquisition from
the CHILDES database. In Proceedings of the
Workshop on Cognitive Aspects of Computa-
tional Language Acquisition, pages 33?40.
Carroll, J., Briscoe, T., and Sanfilippo, A. (1998).
Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st LREC, pages 447?454.
Charniak, E. and Johnson, M. (2001). Edit de-
tection and parsing for transcribed speech. In
Proceedings of NAACL, pages 118?126.
Clark, S. and Curran, J. R. (2007). Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguis-
tics, 33(4):493?552.
Dredze, M., Blitzer, J., Pratim Talukdar, P.,
Ganchev, K., Graca, J. a., and Pereira, F. (2007).
Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007,
pages 1051?1055.
Geertzen, J. (2009). Semantic interpretation of
Dutch spoken dialogue. In Proceedings of the
Eight IWCS, pages 286?290.
Lease, M. and Johnson, M. (2006). Early deletion
of fillers in processing conversational speech. In
Proceedings of the HLT-NAACL, pages 73?76.
Lin, D. (1998). A dependency-based method
for evaluating broad-coverage parsers. Natural
Language Engineering, 4(2):97?114.
MacWhinney, B. (2000). The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum As-
sociates, Mahwah, NJ, USA, third edition.
McDonald, R., Crammer, K., and Pereira, F.
(2005). Online large-margin training of depen-
dency parsers. In Proceedings of the 43rd An-
nual Meeting on ACL, pages 91?98.
Sagae, K., Davis, E., Lavie, A., MacWhinney,
B., and Wintner, S. (2007). High-accuracy an-
notation and parsing of CHILDES transcripts.
In Proceedings of the ACL-2007 workshop on
Cognitive Aspects of Computational Language
Acquisition.
Sagae, K., MacWhinney, B., and Lavie, A. (2004).
Adding syntactic annotations to transcripts of
parent-child dialogs. In In Proceedings of the
Fourth LREC, pages 1815?1818.
221
