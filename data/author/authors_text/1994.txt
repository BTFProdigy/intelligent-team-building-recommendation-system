Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524?533,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Mining Search Engine Clickthrough Log for  
Matching N-gram Features  
 
 
Huihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang,  
Lei Duan, Belle Tseng 
Yahoo! Inc., Santa Clara, CA 95054 
{huihui,longbin,fanli,ziming,leiduan,belle}@yahoo-inc.com 
 
Abstract 
User clicks on a URL in response to a query are 
extremely useful predictors of the URL?s rele-
vance to that query. Exact match click features 
tend to suffer from severe data sparsity issues in 
web ranking. Such sparsity is particularly pro-
nounced for new URLs or long queries where 
each distinct query-url pair will rarely occur. To 
remedy this, we present a set of straightforward 
yet informative query-url n-gram features that al-
lows for generalization of limited user click data 
to large amounts of unseen query-url pairs. The 
method is motivated by techniques leveraged in 
the NLP community for dealing with unseen 
words. We find that there are interesting regulari-
ties across queries and their preferred destination 
URLs; for example, queries containing ?form? 
tend to lead to clicks on URLs containing ?pdf?. 
We evaluate our set of new query-url features on 
a web search ranking task and obtain improve-
ments that are statistically significant at a p-value 
< 0.0001 level over a strong baseline with exact 
match clickthrough features.   
1 Introduction 
Clickthrough logs record user click behaviors, 
which are a critical source for improving search 
relevance (Bilenko and White, 2008; Radlinski et 
al., 2007; Agichtein and Zheng, 2006; Lu et al 
2006). Previous work (Agichtein et al, 2006) 
demonstrated that clickthrough features (e.g., 
IsNextClicked and IsPreviousClicked) can lead 
to substantial improvements in relevance. Such 
features summarize query-specific user interac-
tions on a search engine. One commonly used 
clickthrough feature is generated based on the 
following observation: if a URL receives a large 
number of first and last clicks across many user 
sessions, then it indicates that this URL might be 
a strongly preferred destination of a query. For 
example, when a user searches for ?yahoo?, they 
tend to only click on the URL www.yahoo.com 
rather than other alternatives. This results in 
www.yahoo.com being the first and last clicked 
URL for the query. We refer to such behavior as 
being navigational clicks (NavClicks). Features 
that use exact query and URL string matches 
(e.g., NavClick, IsNextClicked and IsPrevious-
Clicked)  are referred to as exact match features 
(ExactM) for the remainder of this paper.  
 
The coverage of ExactM features is sparse, espe-
cially for long queries and new URLs. Many 
long queries are either unique or very low fre-
quency. Hence, the improvements from ExactM 
features are limited to the more popular queries. 
In addition, ExactM features tend to be weighted 
heavily in the ranking of results when they are 
available. This introduces a bias where the rank-
ing models tend to strongly favor older URLs 
over new URLs even when the latter otherwise 
appear to be more relevant.  
 
By inspecting the clickthrough logs, we observed 
that unseen query-url pairs are often composed of 
informative previously observed subsequences. 
Specifically, we saw that query n-grams can be 
correlated with sequences of URL n-grams.  For 
example, we find that there are interesting regu-
larities across queries and URLs, such as queries 
containing ?form? tending to lead to clicks on 
URLs containing ?pdf?. This strongly motivates 
the adoption of an approach similar to the Natu-
ral Language Processing (NLP) technique of us-
ing n-grams to deal with unseen words. For ex-
ample, part-of-speech tagging (Brants, 2000) and 
parsing (Klein and Manning, 2003) both require 
dealing with unknown words. By using n-gram 
substrings, novel items can be dealt with using 
any informative substrings they contain that were 
actually observed in the training data.  
 
524
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce our overall me-
thodology. Section 2.1 presents a data mining 
method for building a query-url n-gram diction-
ary, Section 2.2 describes the new ranking fea-
tures in detail. In section 3, we present our ex-
perimental results. Section 4 discusses related 
work, and Section 5 summarizes the contribution 
of this work.  
2 Methodology 
This section describes the detailed methodology 
used in generating the query-url n-gram features. 
Our features require association scores to be pre-
viously calculated, and, hence, we first introduce 
a data mining approach that is used to build an 
association dictionary in Section 2.1. Then, we 
present the procedure used to generate the query-
url n-gram features that use the dictionary in Sec-
tion 2.2. 
 
 
Figure 1: Steps to build a query-url n-gram dic-
tionary 
 
2.1 Data Mining on a Query-URL 
 N-gram Dictionary 
 
The steps involved in building the dictionary are 
shown in Figure 1. We first collect seed query-
url pairs from clickthrough data based on Nav-
Clicks. The queries and URLs from the collected 
pairs are tokenized and converted into a collec-
tion of paired query-url n-grams. For each pair, 
we calculate the mutual information of the query 
n-gram and its corresponding URL n-gram. For 
our experiment, we collect a total of more than 
15M seed pairs and 0.5B query-url n-gram pairs 
using six months of query log data. The details 
are described in the following sections.  
2.1.1 Seed List 
We identify the seed list based on characteristic 
user click behavior. Given a query, we select the 
URL with the most NavClicks as compared to 
other URLs returned. During data collection, the 
rank positions of the top 5 URLs were shuffled to 
avoid the position bias. We aggregate NavClicks 
for a URL occurring in these positions in order to 
both obtain more click data and to avoid the posi-
tion bias issue discussed in Dupret and Piwowar-
ski (2008) and Craswell et al (2008).  
 
For example, in Figure 1, the numbers of Nav-
Clicks for the top three URLs are shown. The 
URL www.irs.gov/pub/irs-pdf/f1040.pdf receives 
the largest number of NavClicks, and, therefore, 
it is used to create the query-url pair: 
 
[irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf] 
 
2.1.2 Query and URL Segmentation  
We segment the seed pairs to n-gram pairs in 
order to increase the coverage beyond that of 
ExactM click features. Within NLP, n-grams are 
typically extracted such that words that are adja-
cent in the original sequence are also adjacent in 
the extracted n-grams. Furthermore, we attempt 
to achieve additional generalization by using skip 
n-grams (Lin and Och, 2004). This means we not 
only extract n-grams for adjacent terms but also 
for sequences that leave out intermediate terms. 
This is motivated by the observation that the se-
mantics of user queries is often preserved even 
when some intermediate terms are removed.  The 
details of the segmentation methods are de-
scribed below. 
2.1.2.1 Query Segmentation  
Prior to query segmentation, we normalize raw 
queries by replacing punctuations with spaces. 
Queries are then segmented into a sequence of 
525
space delimited tokens. From these, we extract 
all possible query n-grams and skip n-grams for 
n smaller than or equal to three (i.e., all unigrams, 
bigrams, and trigrams). For example, given the 
sequence ?irs 1040 form? the adjacent bigrams 
would be ?irs 1040? and ?1040 form?. With skip 
n-grams we also extract ?irs form? as shown in 
Table 1. We do not use n-grams longer than 3 in 
order to avoid problems with overfitting. We will 
refer to this segmentation method as Affix Seg-
mentation.  
 
Table 1: An Example of Affix Segmentation 
N-gram Affix Segmentation 
Unigram irs, 1040, form 
Bigram irs 1040, 1040 form, irs form 
Trigram irs 1040 form 
2.1.2.2 URL Segmentation  
As shown in Table 2, after the queries are seg-
mented, URLs are categorized into four groups: 
domain, URL language, URL region and URL 
path. In general, a URL is delimited by punctua-
tion characters such as ???, ?.?,? ?/?, and ?=?.  
 
Table 2: An Example of URL Segmentation 
URL Groups Example 
Domain irs.gov 
URL language en 
URL region us 
URL path pub, irs, pdf, f1040, pdf 
 
The domain group includes one domain token, 
for example, irs.gov. Although domains could be 
divided into multiple n-grams, we treat them as a 
single unit, with the exception of encoded lan-
guage and region information.  
 
The language and region groups are based on the 
language or region part of the URL n-grams such 
as the suffixes ?.en? and ?.de?. The language and 
region of a URL n-gram are identified by a table 
look-up method. The table is created based on 
the information available at en.wikipedia.org/ 
wiki/List_of_ISO_639-1_codes and en.wikipedia. 
org/wiki/ISO_3166. When there is no clear lan-
guage or region URL n-gram, we use English (en) 
as the default language and United States (us) as 
the default region. 
2.1.3 Calculation of Mutual Information 
After query and URL n-grams are extracted, we 
calculate mutual information (Gale and Church, 
1991) to determine the degree of association be-
tween the n-grams. The definition of query-url n-
gram mutual information (MI) is given in Equa-
tion 1. 
 
)Freq( )Freq(
),Freq(
log2),MI( uq
uq
uq =   (1) 
 
Here q corresponds to a query n-gram and u cor-
responds to a URL n-gram. Freq (q) is the count 
of q in the seed list normalized by the total num-
ber of q. Freq (u) is the count of u normalized by 
the total number of u.  Freq (q, u) is the count of 
q and u that co-occurred in a full query-url pair 
normalized by the total number of q and u. A pair 
will be assigned to a MI score of zero if the items 
occur together no more than expected by chance, 
under the assumption that the two items are sta-
tistically independent. When a pair occurs more 
than is expected by chance, the MI score is posi-
tive. On the other hand, if a pair occurs together 
less than is expected by chance, the mutual in-
formation score is negative. In order to increase 
the confidence of the MI scores, we remove all 
n-grams with less than 3 occurrences in the seed 
list, and assign a zero MI score for any pairs in-
volving these n-grams. No smoothing is applied. 
 
This scoring scheme fits well with the associa-
tion properties we would like to have for our 
query-url n-gram click features. If a query n-
gram cues for a certain URL through one of its n-
grams, the feature will take on a positive value. 
Similarly, if a query n-gram cues against a cer-
tain URL, the feature will take on a negative val-
ue.  
2.1.4 Analysis of Query-URL N-gram 
Association 
By examining our dictionary, we observed a 
number of pairs that are interesting from a rele-
vance ranking perspective. To illustrate, we pre-
sent four examples of n-gram pairs and intui-
tively explore the nature of the n-gram associa-
tions in the dictionary.  
 
Table 3: Examples of MI Scores 
Query n-gram URL n-gram MI score 
?iphone? apple.com 8.7713 
?iphone? amazon.com -0.1555 
?iphone plan? att.com 11.5388 
?iphone plan? apple.com 8.9676 
 
First, let?s examine the association between 
query n-grams and URL n-grams for the queries 
526
?iphone? and ?iphone plan?. Notice that the 
query unigram ?iphone? is strongly associated 
with apple.com, but negatively associated with 
amazon.com. This can be explained by the fact 
that ?iphone? as a product is not only developed 
by Apple but also strongly associated with the 
Apple brand. In contrast, while Amazon.com 
sells iphones, it also sells a large variety of other 
products, thus is not regarded as a very authorita-
tive source of information about the ?iphone?. 
However, by adding additional context, the most 
preferred URL according to MI can change. The 
two examples in the bottom of Table 3 illustrate 
the URL preferences for the query bigram 
?iphone plan?. While apple.com is still a strongly 
preferred destination, there is a much stronger 
preference for att.com. This preference follows 
since apple.com has more product information on 
the ?iphone? while the information provided by 
att.com will be more targeted at visitors who 
want to explore what rate plans are available. 
 
Second, Table 4 shows the association between 
?kimo?, ?.tw? and ?.us?. ?Kimo? was a Taiwan-
ese start-up acquired by Yahoo!. The mutual in-
formation scores accurately reflect the associa-
tion between the query n-gram and region ids.  
 
Table 4: Example of MI Scores 
Query n-gram URL n-gram MI score 
?kimo? tw (taiwan) 12.8303 
?kimo? us (united states) 0.7209 
 
Third, Table 5 shows the association between 
?kanji?, and URLs with Language identification 
of ?Japanese?, ?Chinese? and ?English?. ?Kanji? 
means ?Chinese? in Japanese. Since queries con-
taining ?Kanji? are typically from users inter-
ested in Japanese sites, the mutual information 
shows higher correlation with Japanese than with 
English or Chinese.  
 
Table 5: Example of MI Scores 
Query n-gram URL n-gram MI score 
?kanji? ja (japanese) 11.3862 
?kanji? zh (chinese) 6.2567 
?kanji? en (english) 4.2110 
 
Table 6: Example of MI Score 
Query n-gram URL n-gram MI score 
?form? pdf 4.9067 
?form? htm 1.0916 
?video? watch 5.7192 
?video? htm -1.9079 
 
Fourth, Table 6 shows the association between 
two query n-grams, ?form? and ?video?, that at 
first glance may not actually look very informa-
tive for URL path selection. However, notice that 
the unigram ?form? has a strong preference for 
pdf documents over more standard web pages 
with an html extension. Similarly, queries that 
include ?video? convey a preference for URLs 
containing ?watch?, a characteristic URL n-gram 
for many video sharing websites. 
 
It is reasonable to anticipate that incorporating 
such associations into a search engine?s ranking 
function should help improve both search quality 
and user experience. Take the example where, 
there are two high ranking competing URLs for 
the query ?irs 1040 form?. Let?s also assume 
both documents contain the same query relevant 
keywords, but one is an introduction of the ?irs 
1040 form? as an htm webpage and the other one 
is the real filing form given as a pdf document. 
Since in our dictionary, ?form? is more associ-
ated with pdf than htm, we predict that most us-
ers would prefer the real pdf form directly, so it 
should be placed first in the list of query results. 
While click data for the exact query-url pairs 
confirms this preference, it is reassuring that we 
could identify it without needing to rely on see-
ing the specific query string before. As described 
in detail below, and motivated by this analysis, 
we designed our query-url click features based 
on the contents of the n-gram MI dictionary. 
2.2 Query-URL N-gram Features  
For our feature set, we explored the use of differ-
ent query segmentation approaches (concept and 
affix segmentation) in order to increase the di-
versity of n-grams. In the following section, we 
use an unseen query ?irs 1040 forms? and con-
trast it with the known query ?irs 1040 form? 
from the last section.  
2.2.1 Concept Segmentation Features 
Query concept segmentation is a weighted query 
segmentation approach. Each query is analyti-
cally interpreted as being a main concept and a 
sub concept. We search for the unique segmenta-
tion of the query that maximizes its cumulative 
mutual information score with the URL n-grams. 
Main concepts and sub concepts are n-grams 
from the query that have the strongest association 
with URL n-grams and thus assist in identifying 
relevant landing URL n-grams when the whole 
query or the whole URL has not been seen.  
527
 Algorithm 1: Concept Segmentation 
for U = domain, URL language, URL region, 
URL path do 
    for j = 0... n-1 do 
          M  ?  W0...j 
          S   ?  Wj+1...n 
          for k = 0... m do          
               curr_mi_M ? arg maxk=1...m  MI (M, Uk) 
               curr_mi_S ? arg maxk=1...m  MI (S, Uk) 
                if curr_mi_M + curr_mi_S > curr_best                    
                then 
                    curr_best = curr_mi_M + curr_mi_S 
                    mi_M ? curr_mi_M 
                    mi_S ? curr_mi_S 
                end if 
           end for 
           adding mi_M as a feature 
           adding mi_S as a feature 
      end for 
end for 
 
Pseudo-code for generating query-url n-gram 
features based on the concept segmentation is 
given in Algorithm 1. Each query (Q) is com-
posed of a number of words, w1, w2, w3?,wn. 
Each URL is segmented and categorized to four 
groups: domain, URL language, URL region and 
URL path. Each URL group has m number of 
URL n-grams.  M is the main concept of Q and S 
is the sub concept of Q.  
 
One potential drawback of such concept segmen-
tation is data sparsity. When we look for the 
maximum of cumulative mutual information, we 
may obtain main concepts with very high mutual 
information and sub concepts which do not exist 
in the dictionary. In order to address this problem, 
we implement a second query segmentation me-
thod, affix segmentation, that is discussed in sec-
tion 2.2.2.  
 
Table 7 shows eight concept segmented features. 
?Coverage? is the percentage of query-url pairs 
that have valid feature values. Some of the sam-
ples do not have values because no clicks for the 
pairs were seen in the sample of data used to 
build the dictionary. When a pair does not have a 
value, the default value of zero is assigned. This 
default value is based on the assumption that 
unless we have evidence otherwise, we assume 
all query-url n-grams are statistically independ-
ent and thus provide no preference signal. 
 
Table 7: Eight Features Generated based on 
Concept Segmentation.  
Feature Query N-
gram 
URL N-
gram 
Coverage 
(%) 
MainDS M domain 54.09 
SubDS S domain 30.46 
MainLang M lang. 94.41 
SubLang S lang. 72.40 
MainReg M reg. 90.34 
SubReg S reg. 68.19 
MainPath M path 64.96 
SubPath S path 58.76 
 
Query-URL Domain Features are defined as 
the mutual information of a query n-gram and the 
domain level URL. There are two features in this 
category, one for the query main concept and one 
for the sub concept. They help to identify the 
user preferred host given a query.  
 
Table 8: Example of Selecting Query Segmenta-
tion 
MI(q,u) irs.gov 
?irs? 11.2174 
?1040? 
  
11.6175 
?forms? 7.5049 
11.5550 
Cumulative MI 19.1224 22.7724 
  Seg. 1 Seg.2 
 
To illustrate the concept segmentation features, 
let?s examine the query, ?irs 1040 forms? in the 
context of the domain irs.gov.  The query ?irs 
1040 forms? can be segmented either as ?irs 
1040? and ?forms? or as ?irs? and ?1040 forms?. 
As shown in Table 8, taking the cumulative max-
imum, the second segmentation scores higher 
than the first one. Therefore, the ?irs? and ?1040 
forms? segmentation is preferred. The feature 
value for the main concept is 11.5550, and the 
sub concept is then assigned to be 11.2174. 
 
Query-URL Language and Region Features 
are the mutual information of a query n-gram and 
URL language/region. They are used for provid-
ing language and region information.  
 
Query-URL Path Features are the mutual 
information of a query n-gram and a URL path n-
gram. While there are typically many URL path 
n-grams, only one URL path n-gram is selected 
to be paired with each query n-gram. The se-
lected n-gram is the one that achieves the highest 
528
cumulative maximum MI score. They are used 
for providing association between query n-grams 
and url n-grams such as ?forms? and ?pdf?. 
2.2.2 Affix Segmentation Features 
As previously mentioned, affix segmentation 
addresses sparsity issues associated with concept 
segmentation. Here, we introduce the features 
generated based on affix segmentation. Pseudo-
code for generating the features is given in Algo-
rithm 2. Two query unigrams (w0 and wn) and 
one bigram (w0wn) is used. Each URL is seg-
mented and categorized to four groups: domain, 
URL language, URL region and URL path. Each 
URL group has m number of URL n-grams.   
 
This approach is complementary to the concept 
segmentation for long queries. The affix n-grams 
are in smaller unit, and therefore, are less sparse.  
In addition, the skip bigrams allow for generali-
zations using non-adjacent terms. Table 9 shows 
the coverage of the twelve affix features.  
 
Algorithm 2:  Affix Segmentation  
for U = domain, URL language, URL region, 
URL path do 
     for q = w0, wn, w0wn do 
          for k = 0... m do          
               curr_mi_q ? arg maxk=1...m  MI (q, Uk) 
                if curr_mi_q > curr_best then 
                    curr_best = curr_mi_q  
                end if 
           end for 
           adding curr_mi_q as a feature 
      end for 
end for 
 
Table 9: Twelve Features Generated based on 
Affix Segmentation  
Feature Query N-
gram 
URL N-
gram 
Coverage 
(%) 
PreDS w0 domain 48.09 
SufDS wn domain 47.72 
PresufDS w0wn domain 23.57 
PreLang w0 lang. 55.58 
SufLang wn lang. 58.22 
PresufLang w0wn lang. 24.91 
PreReg w0 reg. 93.82 
SufReg wn reg. 93.59 
PresufReg w0wn reg. 69.29 
PrePath w0 path 98.15 
SufPath wn path 97.80 
PresufPath w0wn path 75.81 
 
Query-url domain affix features has three fea-
tures: MI(w0, domain), MI(wn, domain), and 
MI(w0wn, domain). In the example of ?irs 1040 
forms? and ?irs.gov?, the features are MI(irs, 
irs.gov), MI(forms, irs.gov), and MI(irs forms, 
irs.gov).  
 
Query-url language and region affix features 
has three features respectively: MI(w0, language), 
MI(wn, language), MI(w0wn, language) MI(w0, 
region), MI(wn, region), and MI(w0wn, region). 
In the example of ?irs 1040 forms?, ?en? and 
?us?, the features are MI (irs, en), MI (forms, en), 
MI (irs forms, en), MI (irs, us), MI (forms, us), 
and MI (irs forms,us).  
 
Query-url path affix features has three fea-
tures: MI(w0, path), MI(wn, path), and MI(w0wn, 
path). In the example of ?irs 1040 forms? and 
?www.irs.gov/pub/irs-pdf/f1040.pdf?, there are 
four URL path n-grams, ?pub?, ?irs?, ?pdf?, and 
?f1040?. The URL path n-gram, irs, gets maxi-
mum MI score. Therefore, the query-url path af-
fix features are MI (irs, irs), MI (forms, irs), and 
MI (irs forms, irs).  
 
We demonstrated the procedure to generate 20 
query-url n-gram features, and in Section 3, we 
will present their effectiveness in relevance rank-
ing.   
3 Experiment 
We evaluate the performance of query-url n-
grams features (8 concept and 12 affix features) 
on a ranking application and analyze the results 
from several different perspectives.  
3.1 Datasets 
For all experiments, our training and test data are 
query-url pairs annotated with human judgments.  
In our data, we use five grades to evaluate rele-
vance of a query and URL pair.  
 
The data includes 94K queries for training and 
3.4K queries for evaluation, and each query is 
associated with the top ranked URLs returned 
from a search engine. Totally, there are 916K 
query-url pairs for training and 42K pairs for 
testing. The queries are general and uniformly 
and randomly sampled with replacement, result-
ing in more frequent queries also appearing more 
frequently in our training and test sets. 
529
3.2 Ranking Algorithm 
GBRank is a supervised learning algorithm that 
uses boosted decision trees and incorporates the 
pair-wise information from the training data 
(Zheng et al 2007). It is able to deal with a large 
amount of training data with hundreds of features. 
We use an internal C++ implementation of 
GBRank. 
3.3 Evaluation Metric 
We use Discounted Cumulative Gain (J?rvelin 
and Kek?l?inen, 2002) to evaluate our ranking 
accuracy. Discounted Cumulative Gain (DCG) 
has been widely used in evaluating the quality of 
search engine rankings and is defined as: 
 
?
=
+
=
k
i
i
k i
G
DCG
1 2 )1(log
  (2) 
     
Gi represents the editorial judgment of the i-th 
document. In this paper, we only report normal-
ized DCG5, which is an absolute DCG5 normal-
ized by a baseline, and relative DCG5 im-
provement, which is an improvement normal-
ized by the baseline. Note normalized DCG5 is 
different than NDCG (Normalized Discounted 
Cumulative Gain defined in J?rvelin and 
Kek?l?inen, 2002). We use Wilcoxon signed test 
(Wilcoxon, 1945) to evaluate the significance for 
model comparison. 
3.4 Feature Sets 
Five feature sets are used in our experiments. 
Details are listed in Table 10.  
 
Table 10: Five Feature Sets 
Tag Description  
Base Feature 
Set  
Core Feature Set and ExactM 
click features 
Q-U N-gram 
Feature Set (I) 
Base Feature Set and Q-U N-
gram features 
Core Feature 
Set  
query-based, document-based, 
query-document based fea-
tures 
NavClick Fea-
ture Set 
Core Feature Set and Nav-
Click 
Q-U N-gram 
Feature Set (II) 
Core Feature Set and Q-U N-
gram features 
 
Base Feature Set is a strong baseline feature set 
from a state-of-the-art commercial search engine. 
This set includes NavClick features, and other 
internal ExactM click features. It is used for 
evaluating Query-URL N-gram Feature Set (I) in 
order to know whether query-url n-gram features 
can achieve gains when stacked on top of Ex-
actM features.  
 
Core Feature Set is a weaker variant of the 
baseline system that excludes ExactM click fea-
tures. This system is used for evaluating 
NavClick Feature Set and Query-URL N-gram 
Feature Set (II) independently in order to study 
and contrast the effected queries.  
3.5 Experimental Results 
We compare the query-URL N-gram feature set 
(I) with the base feature set in Section 3.5.1, and 
contrast the NavClick features and the query-
URL N-gram features (II) using the Core Feature 
Set in Section 3.5.2. 
3.5.1 Query-URL N-gram Feature Set (I) 
versus Base Feature Set 
As shown in Figure 2, Query-URL N-gram Fea-
ture Set (I) outperforms Base Feature Set. The 
additional 20 query-url n-gram features achieve 
statistically significant gains at a p-value < 
0.0001 level, suggesting that they are compli-
mentary to ExactM click features. Even though 
the query-url n-gram features are generated from 
the same data as the ExactM features, the gain is 
additive and stackable. The DCG5 impact is 
0.53% relative improvement when running 
GBRank using 2500 trees. Every data point is 
normalized by the DCG5 of the baseline feature 
set using 2500 trees. This is represented in the 
graph as the rightmost point of Base Feature Set 
curve. 
 
0.96
0.97
0.98
0.99
1
1.01
500 1000 1500 2000
Q-U N-gram Features (I)
Base Features
NavClick
Q-U N-gram Features (II)
Core Features
 
Figure 2: Comparison of the five feature sets on 
the normalized DCG5 (Y-axis) against number of 
trees (X-axis).  
 
530
3.5.2 NavClick and Query-URL N-gram 
Feature Set (II) versus Core Fea-
ture Set 
We compare NavClick Feature Set and Query-
URL N-gram Feature Set (II) in the context of 
Core Feature Set, in order to evaluate the two 
independently. As shown in Figure 2, both 
NavClick and Query-URL N-gram Feature Set 
(II) outperform Core Feature Set. It is not sur-
prising that NavClick also outperforms Query-
URL N-gram Feature Set (II) since the n-gram 
features are backoff of NavClick. However, their 
gains are competitive suggesting the query-url n-
gram features are very good relevance indicators. 
The impact of NavClick and Query-URL N-gram 
Feature Set (II) is 0.72% and 0.62% relative 
DCG5 improvement at Tree 2500 respectively. 
3.5.3 Feature Importance 
Using the GBRank model, features are evaluated 
and sequentially selected to build the boosted 
decision trees. The split of each node increases 
the DCG during training. We evaluate a feature?s 
importance by aggregating the DCG impact of 
the feature over all trees (Zheng et al, 2007). 
Here, the feature importance is rescaled so that 
the feature with largest DCG impact is assigned a 
normalized score of 1. Figure 3 illustrates the 
relative influence of each of query-url n-gram 
feature. Of these, n-gram features associated with 
a domain name (i.e., MainDS) rank highest. 
 
 
Figure 3: Feature importance of query-url n-
gram features. The importance (Y axis) is nor-
malized so that the most important feature 
(MainDS)?s importance is 1. 
3.6 Analysis 
We access system performance with respect to 
both query length and frequency using the two 
click features sets in combination with the Core 
Feature Set in order to gain insight into the ef-
fected queries.  
3.6.1 Query Length 
As shown in Table 11, NavClick (NavClick Fea-
ture Set) best improves relevance for two word 
queries. In contrast, Query-url n-gram features in 
isolation (Query-URL N-gram Feature II) are 
able to show sizable improvements on longer 
queries, while slightly degrading performance on 
short 1-word queries. Using both feature sets to-
gether (Query-URL N-gram Feature I) results in 
improvement for queries of all lengths. 
 
These results suggest that the strong signal being 
provided by NavClick for short queries helps to 
compensate for any additional noisy introduced 
by the n-gram features, while allowing the n-
gram features to handle  longer queries that are 
less well covered by NavClick. These longer 
queries are exactly the type of queries our query-
url n-gram features were designed to help with. 
 
Table 11: Relative DCG5 Improvement of 
NavClick, Query-URL N-gram (II), and Query-
URL N-gram Features  (I) vs Core Feature Set 
Length NavClick 
vs Core 
(%) 
QU N-
gram (II)  
vs Core 
(%) 
QU N-
gram (I) 
vs Core 
(%) 
1 word 0.03 -0.04 0.62 
2 words 1.04 1.06 1.58 
3 words 1.00 1.44 2.12 
4+ words 0.4 0.68 1.01 
3.6.2 Query Frequency 
We found that query-url n-gram features improve 
tail queries. Head queries are considered as top 
two million frequent queries in our traffic and 
tail queries include anything outside of that range.  
 
Table 12: Relative DCG5 Improvement of 
NavClick, Query-URL N-gram Features (II) and 
Query-URL N-gram Features (I) vs Core Feature 
Set 
 NavClick vs 
Core (%) 
QU N-gram 
(II) vs Core 
(%) 
QU N-
gram (I) vs 
Core (%) 
Head 0.91 -0.15 1.11 
Tail 0.59 1.11 1.40 
 
As shown in Table 12, query-url n-gram features 
(Query-URL Feature Set II) differ from 
NavClick (NavClick Feature Set) in that they get 
531
more gain from tail queries. Together, they 
(Query-URL Feature Set I) improve both head 
and tail queries. 
3.7 Case Study 
Below we examine queries from the test set and 
analyze the effects of Query-URL N-gram Fea-
ture Set (II) versus Core Feature Set. 
3.7.1 Positive Cases 
1) Animal shelter in va: this query targets a spe-
cific geographic location. Using the baseline fea-
ture set, the root url wvanimalshelter.org is incor-
rectly ranked higher than www.netpets.com/ 
cats/catresc/virginia.htm. Without any addition-
ally ranking information, general URLs (root) 
tend to be ranked more highly than more specific 
URLs (path), as the root pages tend to be more 
popular. However, our new features express a 
preference between ?va? and ?virginia?, and this 
correctly flips the ranking order.  
2) Myspace profile generator: www. myspacgens. 
com/handler.php?gen=profile was incorrectly 
ranked higher than www.profilemods.com/ 
myspace-generators. Our new features convey a 
high user preference association between ?profile 
generator? and the domain profilemods.com, 
which helps to correctly swap the order.   
3.7.2 Negative Cases 
We determined that negative cases where the 
baseline feature set outperforms the new features 
are typically one word navigational queries such 
as ?craigslist?. However, after we combine the 
query-url n-gram features with NavClick, one 
word navigational queries are ranked correctly. 
4 Related Work 
Our work is mainly related to Gao et al (2009) 
and Bilenko and White (2008). Gao et al (2009) 
addressed the sparsity issue by propagating click 
information among similar queries in the same 
cluster. Their idea is based on an observation that 
similar queries go to similar pages. When two 
queries have similar clicked URLs, it is likely 
that they share clicked URLs. In contrast, our 
idea is to utilize NLP techniques to break down 
long, infrequent queries into shorter, frequent 
queries. The two approaches can be mutually 
beneficial. Bilenko and White (2008) expanded 
click data with a search engine by using post-
search user experience collected from toolbars. 
Toolbars keep track of users? click behavior both 
when they are using the search engine directly 
and beyond. Their relevance features are built 
based on whole session clicks extracted from the 
toolbar. In contrast, our n-gram features are built 
on search engine clicks directly. We should be 
able to expand our method to integrate the post-
search clicks with toolbar data.    
 
Other related work can be found in the domain of 
query rewriting. Our n-gram dictionary was orig-
inally designed for query rewriting. Query re-
writing (Xu and Croft, 1996; Salton and Voor-
hees, 1984) reformulates a query to its synonyms 
or related terms automatically. However, the 
coverage of query rewriting is normally small, 
because an inappropriate rewrite can cause sig-
nificant decrease in precision. In contrast, our 
approach can cover a larger number of queries 
without decreasing precision, because it does not 
need to make a binary decision whether a query 
should be reformulated. The association scores 
between queries and rewrites are used as ranking 
features which are trained discriminatively to-
ward search quality.   
5 Conclusion 
In this paper, we presented a set of straightfor-
ward yet informative query-url n-gram features. 
They allow for generalization of limited user 
click data to large amounts of unseen query-url 
pairs. Our experiments showed such features 
gave significant improvement over models with-
out using the features. In addition, we mined an 
interesting dictionary which contains informa-
tive, but not necessarily obvious, query-url syno-
nym pairs such as ?form? and ?pdf?. We are cur-
rently extending our work to a variety of exact 
match features and different sources of click-
through logs.  
Acknowledgement 
Thanks to the anonymous reviewers for detailed 
suggestion and our colleagues: Jon Degenhardt 
and Narayanan Sadagopan for assistance on gen-
erating clickthrough data, Jiang Chen for devel-
oping the decision tree package, Xiangyu Jin for 
a discussion on map/reduce, Beno?t Dumoulin, 
Fuchun Peng, Yumao Lu, and Xing Wei for pro-
ductizing the work, and Rosie Jones, Su-lin Wu, 
Bo Long, Xin Li and Ruiqiang Zhang for com-
ments on an earlier draft.  
 
 
 
532
References  
Agichtein, E., E. Brill, and S. Dumais. 2006. Im-
proving web search ranking by incorporating 
user behavior information. In Proceedings of 
the ACM SIGIR 29. 
Agichtein, Eugene, Zijian Zheng. 2006. Identify-
ing "best bet" web search results by mining 
past user behavior.  In Proceedings of KDD. 
Bilenko, Mikhail and Ryen W. White. 2008.  
Mining the search trails of surfing crowds: 
identifying relevant websites from user activ-
ity. In Proceedings of WWW.  
Brants, T. 2000. Tnt: a statistical part-ofspeech 
tagger. In Proceedings of ANLP 6. 
Craswell, Nick and Martin Szummer. 2007. Ran-
dom walks on the click graph. In Proceedings 
of SIGIR. 
Craswell, Nick, Onno Zoeter, Michael Taylor, 
Bill Ramsey. 2008. An experimental compari-
son of click position-bias models in WSDM. 
Dupret, Georges, Benjamin Piwowarski. 2008. A 
user browsing model to predict search engine 
click data from past observations. In Proceed-
ings of SIGIR 31. 
Gale, William A. and Kenneth W. Church. 1991. 
Identifying word correspondence in parallel 
texts. In Proceedings of HLT 91. 
Gao, Jianfeng, Wei Yuan, Xiao Li, Kefeng Deng, 
and Jian-Yun Nie. 2009. Smoothing Click-
through Data for Web Search Ranking. In Pro-
ceedings of SIGIR 32. 
J?rvelin, K. and J. Kek?l?inen. 2002. Cumulated 
gain-based evaluation of IR techniques, Jour-
nal ACM Transactions on Information Sys-
tems, 20: 422-446. 
Klein, D. and C. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of ACL 41. 
Lin, Chin-Yew and Franz Josef Och. 2004. Au-
tomatic evaluation of machine translation 
quality using longest common subsequence 
and skip-bigram. In In Proceedings of ACL 42.  
Lu, Yumao, Fuchun Peng, Xin Li and Nawaaz 
Ahmed, 2006, Coupling Feature Selection and 
Machine Learning Methods for Navigational 
Query Identification, In Proceeding of CIKM. 
Radlinski, F., Kurup, M. and Joachims, T. 2007. 
Active exploration for learning rankings from 
clickthrough data. In SIGKDD. 
Salton G. and E. Voorhees. 1984. Comparison of 
two methods for Boolean query relevancy 
feedback. Information Processing & Manage-
ment, 20(5).   
Wilcoxon, F. 1945. Individual Comparisons by 
Ranking Methods. Biometrics, 1:80?83. 
Xu Q. and W. Croft. 1996. Query expansion us-
ing local and global document analysis. In 
Proceed of the 19th annual international ACM 
SIGIR. 
Zheng, Z., H. Zha, K. Chen, and G. Sun. 2007. A 
regression framework for learning ranking 
functions using relative relevance judgments. 
In Proceedings of SIGIR 30.  
533
An Integrated Architecture for Example-Based Machine Translation 
Alexander Franz, Keiko Horiguchi, Lei Duan, 
Doris Ecker, Eugene Koontz, and Kazami Uchida 
Spoken Language Technology, Sony US Research Labs 
3300 Zanker P, oad 
San Jose, CA 95134, USA 
{ amf, kei ko, l ei, do ri s,eko ntz, kuchid a } @ sit. sel. so ny.com 
Abstract 
This paper describes a machine translation 
architecture that integrates the use el' 
examples for flexible, idiomatic translations 
with the use o1' linguistic rules for broad 
coverage and grammatical acctu'acy. We 
have implemented a prototype for 
English-to-Japanese translation, and our 
ewfluation shows lhat tile system has good 
translation quality, and only requires 
reasonable computational resources. 
1 Introduction 
Machine translation by analogy lo pairs of 
corresponding expressions in the source and 
target languages, or "example-based transhtlion", 
was firs! proposed by (Nagao 1984). Recent 
work in the example-based I'ramework inchldes 
memory-based translation (Sate & Nagao 1990), 
similarity-driven translation (Watanabe 1992), 
transl'cr-driven lachine translation (Furusc & 
Iida 1996), and patten>based machine 
translation (Watanabe & Takeda 1998). 
The example-based approach promises easy 
translation knowledge acquisition, more flexible 
transfer than brittle rule-based approaches, and 
idiomatic translations. At the same time, the use 
o1' linguistic rules offers a number of important 
benel'its. Detailed linguistic analysis can allow 
an example-based machine translation system to 
handle a wide variety of input, since rules can be 
used to factor out all linguistic wMations that do 
not influence tile exampled)ased transfer. 
Rule-based language generation from detailed 
linguistic representations can lead to higher 
grammatical output quality. Finally, a modular 
system architecture that uses 
domain-independent linguistic regularities in 
separate linguistic modules allows extending the 
system to much broader domains. The 
HARMONY architecture for lkybrid Analogical 
aud rule-based njachine translation of naturally_ 
occurring colloquial hmguagc combines the 
adwmtages of both these approaches. 
2 The Travel Domain 
Our prototype implementation o1' the HARMONY 
architecture was designed to cover the "travel 
domain". This is composed of words, phrases, 
expressions, and sentences related to 
international travel, similar to what is covered 
by typical travel phrase books. 
Two principles guided our detailed definition of 
the translation domain. FirsL the translation 
domain should not be limited to a narrow 
sub-domain, such as appointment scheduling or 
hotel rcscrwttions. Second, the expressions 
considered in the domain should reflect the fact 
that people quickly adapt to limitations in 
human-machiue or machine-mediated 
communication by simplifying the input. For 
example, (Sugaya et al 1999) found that the 
average length el' actual human utterances in a 
hotel resmwation task using speech translation 
was only 6.1 words, much shorter than some o1' 
the data that has been used in previous work on 
speech translation. 
'File current vocabulary o1' 7,500 woMs is 
divided into a group o1' general words, a number 
of extensiblc word groups (such as names el' 
food items or diseases), and a number of 
area-specific woM groups (such as names of 
cities or tourist destinations).The travel domain 
is divided into eight "situations": A general 
situation (including everyday conversation); 
transportation; accommodation; sightseeing; 
1031 
shopping; wining, dining, and nightlife; banking 
and postal; and doctor and pharmacy. 
We created a corpus for this dolnain, and 
divided it into a development set o1' 7,000 
expressions, and a separate, unseen test set of 
5,000 expressions. The development set is 
used for creation and refinelnent of the 
translation knowledge sources, and the test set is 
only used for evaluations. (Each evaluation 
uses a new, random 500-word sample from the 
5,000 word test set.) 
The corpus was balanced to illustrate the widest 
possible variety of types o1' words, phrases, 
syntactic structures, semantic patterns, and 
pragmatic functions. The average length el' the 
expressions in the corpus is 6.5 words. Some 
examples from the development corpus are 
shown below. Even though this dolnain might 
seem rather limited, it still contains inany 
challenges for machine translation. 
? Can I have your last name, please ?
? Is this the bus for  Shinagawa station ?
? 1 would like to make a reservation for two 
people for eight nights. 
? Can you tell us where we can see some 
Buddhist emples ? 
? Most supermarkets sell liquor. 
? Can you recommend a good Chinese 
restaurant in this area ? 
? I 'd like to change 500 Dollars' in traveller's 
checks into Yen. 
? Are there any English..speaking doctolw at 
the hospital? 
3 NLP Infrastructure 
The prototype implementation is coustructed out 
of components that are based on a powerful 
infrastructure for natural language processing 
and language engineering. The three inain 
aspects of this infrastructure are the Grammar 
Programming Language (GPL), the GPL 
compiler, and the GPL runtime environment. 
3.1 The Grammar Programming Language 
The Gralnmar Programming Language (GPL) is 
an imperative programming language for 
feature-structure-based rewrite grammars. GPL 
is a l'ormalism that allows the direct expression 
of linguistic algorithms l'or parsing, transfer, and 
generation. Some ideas in GPL can be traced 
back to Tolnita's pseudo-unification fornmlisln 
(Tomita 1988), and to Lexical-Functional 
Grammar (Dalrymple et al 1995). GPL 
includes variables, simple and complex tests, 
and various manipulation operators. GPL also 
includes control flow statements including 
if-then-else, switch, iteration over 
sub-feature-structures, and other features. An 
example of a silnplified GPL rule for English 
generation is shown in Figure 1. 
Wll SENT --) NP YN_SENT { 
!exist\[Sin VP SUBJ WH\]; 
local-variable WIt_VP = \[$m VP\]; 
local-wlriable WH PHP, ASE; 
$WH PHRASE = find-subfstruct in $WHVP 
where (?exist\[$x WH\]); 
$d I = \[$WH_PHRASE SLOT-VALUE\]; 
\[$WH PHRASE SLOT-VALUE TRACE\] = '+'; 
$d2 = $m; 
/ 
Figure I Example of a GPLGeneration P, ule 
3.2 The GPL Compiler 
GPL grammars are compiled into C code by the 
GPL compiler. The GPL compiler was created 
using the Unix tools lex and yacc (Levine et al 
1990). For each rewrite rule, the GPL compiler 
creates a main action function, which carries out 
most of the tests and manipulatious specified by 
the GPL statements. 
The GPL compiler handles disiunctive feature 
structures in an efficient manner by keeping 
track of sub-feature-structure references within 
each GPL rule, and by generating an expansion 
function that is called once before the action 
function. The coinpiler also tracks variable 
references, and generates and tracks separate test 
functions for nested test expressions. 
3.3 The GPL Run-time Environment 
The result of compiling a GPL grainmar is an 
encapsulated object that can be accessed via a 
public interface function. This interface fnnction 
serves as the link between the compiled GPL 
grammars, and the various 
language-independent a d domain-independent 
software ngines for parsing, transfer, generation, 
and others. This is illustrated in Figure 2. 
1032 
GI)I+ Grammar J Feature Slructure \] 
l)cfinitions 
.,...---- 
GI)L Compiler \] 
1 
hllcrface Function 
Aclion leunctions 
l?xpansion Funclions 
Test Ftmctions 
Compiled GI)L Grammar 
Software Engine 
(parsers, Iransfers, 
gellcrators, ...) 
I"7 
Feature Structtlre Library 
P, el)resentaiion 
Testing code 
Manipulalion code 
Memory Management 
) Figure 2 GI L Run-time Environmenl 
The compiled GPI. grammars use the feature 
structure library, which provides services for 
efficiently representing, testing, manipulating, 
and managing memory for feature structures. A
special-purpose inemory manager maintains 
separate stacks of memory pages for each object 
size. This scheme allows garbage colleclion 
that is so fast that it can be performed after every 
aUempted GPL rule execution. In our 
experiments with Japanese and English parsing, 
we found that l)el'-rule garbage collection 
reduced the overall read/write memory 
requirements by as much as a factor of four to 
six. 
4 Source Language Analysis 
Translation is divided into the steps ot' analysis, 
transfer, and generation. Sourcc-hmguage 
analysis is illustrated in Figure 3. 
English analysis begins with tokenization and 
morphological analysis, which creates a lattice 
that contains lexical feature structures. I)uring 
multi-word matching, expressions from tile 
multi-word lexicon (such as White House or take 
on) are detected in the word lattice, and new arcs 
with the appropriate l xical feature structures are 
added. 
English lnpul 
Morphological Analysis 
I lmtlice with Single-word 
gexical l:cature ,qlructures 
~, 
Mulli-word Matching 
~.~ LaHice with Single and 
Multi-word texical t"eature 
Structures 
I gexical P, educlion Anlbiguity 
P.educed l.exical Feature 
N \[I'UCI.LII'e Lattice 
V I'm'sing 
Senlei|lial Fealure Slruclure 
Thesaurus 
I Compiled 
English 
l'arsing 
Gialnnlar 
Figure 3 Architecture el'the Analysis Module 
Lexical ambiguity reduction reduces the nulnber 
el' arcs in the word lattice. This module carries 
out part-of-speech tagging over the lattice, and 
reduces the lattice to those lexical feature 
structures that arc part of the number of best 
paths that represents tile best speed/accuracy 
trade-off (currently two). This calculation is 
based on the usual lexical and contextual bigram 
probabilities that were estimated from a training 
corpus, but it also takes into account manual 
costs that can be added to lexicon entries, or to 
individual part-of-speech bigrams. 
The resulting reduced lattice with lcxical 
single-word and multi-word feature structures i
parsed using tilt GLR parsing algorithm 
extended to lattice input (Tomita 1986). The 
English parsing grammar consists of 540 GPL 
rules. The output is a sentential feature structure 
that represents the input to the transfer 
component. 
1033 
5 Transfer 
Transfer I?om the source-language sentential 
feature structure to the target-language s ntential 
feature structure is accomplished with a hybrid 
rule-based and example-based method. This is 
illustrated in Figure 4. 
I English Scntcntial ) 
Feature Structure 
\[Compiled l 
\[Tra,,srer / I \[Database I 
1 
1.inguistic ~ Examl)le 
! I Tra,,.<er ~ Matching\[ 
;iiiiii!iiiiiiiiiiii ................... ...... 
Figure 4 Architecture ofthe Transfer Module 
The input feature structure is passed to the 
linguistic transfer procedure. This consists of a 
rule-rewriting software ngine that executes the 
compiled English-to-Japanese transfer grammar. 
The transfer grammar consists of 140 GPL rules, 
and its job is to specify linguistic constraints on 
examples, combine multiple examples, transfer 
informatiou that is beyond the scope of the 
example database, and perl'orm various other 
transformations. The overall effect is to broaden 
the linguistic coverage, and to raise the 
grammatical accuracy far beyond the level of a 
traditional example-based transfer procedure. 
The linguistic transfer procedure operates on the 
input feature structure in a recursive manner, 
and it invokes the example matching procedure 
to find the best translation example for various 
parts of the input. The example matching 
procedure retrieves the best translation examples 
from the example database, which contains 
14,000 example pairs ranging from individual 
words to entire sentences. In an ofl'-line step, the 
example pairs are parsed, disambiguated, and 
indexed for corresponding constituents using a 
Treebanking tool. 
At each invocation of the example matching 
procedure, linguistic constraints fl'om the 
transfer grammar are used to limit the search 
space to appropriate xamples. In an ol'l'-line 
step, these constraints are pre-compiled into a 
complex index that allows a preliminary fast 
match. Examples that survive the fast match are 
matched and aligned with the input feature 
structure (or sub-feature-structure, during 
recursive invocations) using the thesaurus to 
calculate word similarity, and using various 
other constraints and costs for inserting, deleting, 
or altering slots and features. Rather than rely on 
the exact distance in the thesaurus to calculate 
lexical similarity, we use a scheme that is based 
on the information content of thesaurus nodes, 
similar to (Resnik 1995). 
6 Target-language Generation 
The Japanese target-language f ature structure 
l'orms the input to the generation module, which 
is summarized in Figure 5 below. This module 
also consists o1' a rule-rewriting software ngine, 
executing the compiled GPL Japanese 
generation grammar, which consists ol' 200 GPL 
rules. The generator uses the Japanese lexicon to 
create the Japanese target-language expression. 
Feature Slrtlclure Compiled 
Generation 
Granll/lar 
Generation 
Module 
Expression 
Figure 5 Architecture of~.he Generation Module 
7 Evaluation and Conclusions 
We evaluated the trauslation system using a 
random 500-expression sample from the unseen 
test set (see Section 2 above). The translatious 
were manually assigned to one of the following 
categories o1' translatiou quality: 
Failure. Complete translation failure, due to 
lack of coverage of a rule-based component. 
1034 
Wrong. A translation that is COlnpletely wrong, 
or that has major errors in an important part, 
such as in the main clause. 
Major  Problem. A translation that has a 
missing, extra, or incorrect constituent, such as a 
subject, object, or adjectival/prepositional 
predicate. 
Minor Problem. A translation that l-ms a 
missing, extra, or incorrect minor part, such as 
an intensifier, tense, aspect, temporal or locative 
adjunct, adverb, adjective or other prenominal 
modifier, prepositional phrase, verb conjugation 
form, adjective form, or required word or 
constituent order. 
Stylistic Problem. Slylistic problems include 
awkward but tolerable word order, incorrect 
Japanese particles, incorrect idioms, and silnilar. 
Flawless. A translation that does not exhibit any 
of the above problems is considered flawless. 
The results of the ewtluation are shown in Table 
1 below. Overall, 84% of the translations 
convey the meaning in an acceptable manner. 
We also ewtluated the computational resource 
requirements of the system. On a Pentium II1 
running at 500 MHz, the average translation 
speed was 0.44 seconds. The memory 
requirements are summarized in Table 2 below. 
Flawless 60% 
Stylistic Problem 9% 
Minor F'roblcm 14% 
Acceptable with OOV 1% 
Major ProMem 9% 
Wrong Translation 5% 
Translation IVailure 3% 
Table 1 Translalion Quality 
6MB Read-only Memory lbr Code and Data 
Read-only Melnory for l)iclionary, 
Examples, Fast Match Index, etc. 
Read/Write Memory for Feature Slructures 
Read/Write Memory fo," Software Engines 
Table 2 Memory l,Iequirements 
23MB 
14MB 
4MB 
Our plans for further work include extending the 
size of the input w)cabulary, and developing 
mechanisms for closer integration with speech 
recognition and speech synthesis components for 
speech-to-speech translation. We are also 
working on the Japanese-to-English translation 
direction, and we plan to report results on this in 
the future. 
Acknowledgements 
Our thanks go to Robert Bowen, Benjamin 
Hartwell, Chigusa Inaba, Kaori Shibatani, 
Hirono Stonelake, and Kazue Watanabe for their 
language ngineering elTorts, and to Edward tto 
for user interface and application development. 
References 
Dalryml)le, M., R.M. Kaplan, J.T. Maxwell ll, and A. 
Zacncn, cds. (1995) l;ormal Issues in 
Lexical-Functional Grammar. CSLI Lecture Notes 
47, Stanford, CA. 
Furuse, O. and H. Iida (1996) "incremental 
Iranslation utilizing constituent-I~oundary patlcrns", 
in Proceedings of COL1NG-96, pages 412-417. 
Lcvine, J.R., T. Mason, and D. Brown (1990), lex & 
yacc (Second Editio,), O'Rcillcy and Associates, 
Sebastopol, CA. 
Nagao, M. (1984) "A framework of a Machine 
Translation between Japanese and English by 
analogy principle", in Artificial and tlumatt 
Intelligence, A. 17Jithorn and R. Baneiji (eds.), 
North Holland, pages 173--180. 
Resnik, P. (1995) "Using information content to 
evalualc semantic similarity in a taxonomy", in 
Proceedings of LICAI-95. 
Sale, S. and M. Nagao (1990) "Toward 
memo,y-based lratlslalioD", ill Proceedings of 
COLING-90, vol. 3, Helsinki, Finhmd, pages 
247--252. 
Sugaya, F., T. Takczawam A. Yokoo, and S. 
Yamamolo (1999) '%rid-to-end ewtlualion in 
ATP,-Matrix", in Proceedings of Fmrospeech-99, 
Bt, dapest, Hungary, pages 2431--2434. 
Tomita, M. (1988) The Generalized LR 
I'ars'eMCompiler (Version 8.1): User's Guide. 
Technical Memorandum CMU-CMT-88-Memo, 
Cenler for Machine Translation, Carnegie Mellon 
University. 
Tonlila, M., "All efficient word lattice parsing 
algoritlun for conlinuous peech recognition", ill 
l'roceedings of ICASSP-86, Tokyo, Japan, pages 
1569-1572. 
Walanabe, H. (1992) "A similarily-driven transfer 
system", in Proceedings of COLING-92, Nantes, 
France, pages 770-776. 
Walanabe, H. and K. Takeda (1998) "A 
pattern-based Machine Translation syslem 
extended by exanll~le-based processing", in 
Proceedings of ACL-COLING-98, pages 
1369-1373. 
1035 
