Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 290?300, Dublin, Ireland, August 23-29 2014.
Unsupervised Training Set Generation
for Automatic Acquisition of Technical Terminology in Patents
Alex Judea
1
Hinrich Schu?tze
2
So?ren Bru?gmann
3
1
Heidelberg Institute for Theoretical Studies, Heidelberg, Germany
2
Center for Information and Language Processing, University of Munich, Germany
3
Bru?gmann Software GmbH, Papenburg, Germany
Abstract
NLP methods for automatic information access to rich technological knowledge sources like
patents are of great value. One important resource for accessing this knowledge is the tech-
nical terminology of the patent domain. In this paper, we address the problem of automatic
terminology acquisition (ATA), i.e., the problem of automatically identifying all technical terms
in a document. We analyze technical terminology in patents and define the concept of technical
term based on the analysis. We present a novel method for labeling large amounts of high-quality
training data for ATA in an unsupervised fashion. We train two ATA methods on this training
data, a term candidate classifier and a conditional random field (CRF), and investigate the utility
of different types of features. Finally, we show that our method of automatically generating train-
ing data is effective and the two ATA methods successfully generalize, considerably increasing
recall while preserving high precision relative to a state-of-the-art baseline.
1 Introduction
A large part of our technological knowledge is encoded in patents. Methods for automatically finding
information in patents and inferring information from patents are thus of great value. An important
step in getting access to patent information is identification of technical terminology, i.e., finding the
linguistic expressions that denote the technical concepts of a patent: the methods, processes, substances
and objects that are part of the invention or modified by it. In the example ?The present invention
relates to a charging apparatus of a bicycle dynamo?, the bolded compound nouns are the main
content words and refer to specific technological concepts. We call such linguistic expressions (technical)
terms or terms and their totality the (technical) terminology of a document or domain.
We address the task of automatic terminology acquisition (ATA), the task of finding technical terms
in texts without reliance on existing resources that list terms of the domain. In contrast to this stands
automatic terminology recognition (ATR), which we define as finding known terms and their variants
(Jacquemin and Bourigault, 2003). ATA provides input to downstream components like automatic sum-
marization, machine translation, ontology building, information extraction and retrieval. terms ex-
tracted by ATA can be semantically classified or mapped to entries in a semantic database (Krauthammer
and Nenadic, 2004), but we focus on identifying them without further classification in this paper.
Our main contributions are as follows. (i) We present a method for automatically labeling large amounts
of training data for ATA. (ii) We show that two types of statistical classifiers trained on this training
data beat a state-of-the-art baseline, indicating that the automatic labeling is of high quality. (iii) We
study different feature types for ATA and investigate how much they contribute to good performance. We
investigate a semi-supervised setting in which features are selected based on a manually labeled evaluation
set and a completely unsupervised setting where the feature selection is performed on an automatically
produced set. (iv) Finally, we show that performance strongly depends on correct identification of the
boundaries of terms and could be enhanced considerably by improving candidate identification.
The paper is organized as follows. Section 2 gives a definition of technical terminology and provides a
brief analysis of terms in patents. Section 3 presents related work. Section 4 describes the architecture of
our ATA system: preprocessing, linguistic filtering, automatic labeling of training data, feature selection
and postprocessing. Section 5 reports evaluation results and analyzes selected features and errors. Section
6 presents our conclusions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
290
2 Problem Description
Let w
1...k
be a sequence of words w
1
, w
2
, . . . , w
k
and w
k
a head noun. w
1...k
is a term of domain D iff
(i) the head noun w
k
is unmodified (k = 1) or (for k > 1) is modified by sequences of other nouns (?disk
controller?), adjectives (?secondary controller?) or present participles (?writing controller?) and (ii) it
denotes a concept specific to D.
(i) and (ii) describe the syntactic and semantic properties of a term, respectively. Part (i) restricts
terms to parts of noun phrases. This is a reasonable restriction that covers most technical terms
(Daille et al., 1996) and it has been frequently made in the computational terminology literature. We
exclude comparatives and superlatives as modifying adjectives because they are rarely used attributively
in patents and usually modify quantities or qualities of terms(e.g., ?higher shunt currents?); in other
words, only ?positive? (base-form) adjectives are included in our definition. Note that the number of
tokens per term is not restricted by the definition. Our approach aims to find terms of arbitrary length.
Part (ii) of the definition restricts terms to be specific to a domain D. We can set D to a general
domain like ?electricity? and be on a par with many prior definitions (Ananiadou, 1994; Georgantopoulos
and Piperidis, 2000; Zhang and Fang, 2010), but we can also set D to a narrow domain like ?emergency
protective circuit arrangements? (IPC code
1
H02H).
Here, we choose the most general technical domain possible: the domain of all technical subjects. This
is a good setting for many downstream tasks, e.g., information retrieval should benefit from a broad
coverage of D. It also makes annotation easier: Non-experts can carry it out with good agreement
(Section 5.1) because they simply look for all technical expressions.
The syntactic and semantic parts of our definition of term correspond to the concepts of unithood and
termhood , respectively. Unithood is the degree to which a sequence of tokens is a linguistic unit; and
termhood the degree to which a linguistic unit is a term of a domain (Kageura and Umino, 1996). Both
aspects have to be covered by ATA systems.
Terms in Patents
In addition to traditional terms like simple nouns (1, ?voltage?), modified nouns (2, ?secondary arm?)
and nouns modified by prepositional phrases (3, ?trajectory of the lever?), patents provide also coordina-
tions (4, ?constant and variable current?) and complex constructions (5, ?storage device storing a target
temperature value which a battery is intended to reach?).
For ATA, it seems advisable to exclude infrequent and complex nominal expressions from the definition
of term, both from a terminological and a computational point of view. Most nominal expressions that
are generally viewed as terms are single nouns, compound nouns, and nouns with an adjectival modifier
(Daille et al., 1996); our syntactic definition covers these three types. Nominal expressions like (5)
tend to be long; if we were to count such cases as terms, then it would be unclear where the term
ends. When analyzing (5), our first take might be that there is a nucleus (?storage device?) which is
modified by a verbal phrase (?storing a target temperature value?) and that the rest of the phrase is
not part of the term. But it turns out that the whole phrase appears multiple times in its patent; it
is a stable way of denoting a part of the invention. However, the underlying concept is also denoted by
simpler constructions like the nucleus itself, or synonymous terms like ?control circuit?; these simpler
constructions are covered by our definition.
Coordinations like (4) mix multiple concepts (here, ?constant current? and ?variable current?) without
making this explicit on the surface. It is difficult to identify ?constant current? as a potential term
because it is non-contiguous and is only indicated by an adjective. Our treatment of coordinations in
this paper is to only consider sequences satisfying the syntactic definition (i.e., ?variable current?) to be
terms and discard other parts (i.e., ?constant?). Of course, if both conjuncts are complete terms and
satisfy the syntactic definition, both will be identified as terms.
Finally, prepositional phrases like (3) are rather infrequent compared to terms covered by our syntactic
definition. They also tend to be highly ambiguous and the underlying concept is often expressed by terms
covered by our definition (?lever trajectory?).
3 Related Work
Previous work on ATA either employs filtering or sequence models. Filtering combines linguistic and
statistical criteria for (i) extracting a list of candidates (typically word n-grams) based on simple linguistic
criteria, (ii) computing candidate statistics and (iii) using ranking, classification or some other mechanism
for producing a pruned list of terms as output. Because variation of the surface form of terms is limited,
1
wwwcms10.wipo.int/classifications/ipc/en/
291
it makes sense to use word n-grams as the basis for candidate identification ? even though there are cases
that cannot be found this way, e.g., ?constant current? or alternations like ?pressure regulating valve?
vs. ?valve regulating pressure?.
The main difference between ATA methods that rely on filtering is in how they accomplish the rank-
ing/pruning of the candidate list. See Kageura and Umino (1996), Jacquemin (2001) and Pazienza et al.
(2005) for an overview. In this paper, we accomplish this by training a statistical model to classify term
candidates. We also run experiments with a sequence model. Our main innovation is that these models
are trained on automatically labeled training data.
It is difficult to directly compare computational terminology systems because of differences in domain,
language, application and task definition. As an example consider Takeuchi and Collier (2005) who report
an F
1
of .742. However, their task definition includes assigning terms to pre-defined categories such as
DNA and protein as opposed to simply identifying terms. In addition, terminologies in the biomedical
and technological domains are different. In biomedicine, categories like DNA and protein dominate. For
these terms, shape features are informative ? in contrast to terms in patents. Another difference is
that terms in patents tend to be long whereas DNA and proteins are often single-token abbreviations.
3.1 Training Data Collection
One of our main contributions is unsupervised training data generation (Section 4.3). Prior work has used
automatically recognized training data for computational terminology, specifically for ATR (Craven and
Kumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedical
domain. Given large precompiled term lists they search for occurrences of list elements, e.g., genes, in
texts and use the occurrences they find as training examples. This is similar to distant supervision (Mintz
et al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction.
In contrast, our method is applied to ATA for the technological domain and does not rely on precompiled
resources ? we make use of figure references, which are an inherent part of patents. Our method can
be characterized as training data identification: we exploit given conditions in patents for our search of
training data. In contrast, training data recognition methods need precompiled resources as input and
search for instances of resource elements in texts.
3.2 Learning Algorithms and Features
Different learning algorithms and feature sets have been used for computational terminology. Foo and
Merkel (2010) use Ripper (Cohen, 1995) with a variety of features to classify uni- and bigram term
candidates. Hatzivassiloglou et al. (2001) compare C4.5 (Quinlan, 1993) and Naive Bayes (Duda and
Hart, 1973). Zhang et al. (2010) acquire novel terms using CRFs and syntactic features. Takeuchi and
Collier (2005) find that more training data results in higher F scores. Large training sets have the same
positive effect in our experiments. Our approach has the added advantage that the training sets are
generated completely automatically.
4 Approach
As discussed in the introduction, we address the problem of ATA. We use the abbreviation ATAS (au-
tomatic terminology acquisition system) to refer to our approach in general as well as to the specific
implementation we evaluate in this paper.
ATAS consists of three parts: (i) training set generation, (ii) parameter selection and training of the
term candidate classifier (ATAS-TC) and the CRF (ATAS-CRF) and (iii) identification of terminology
in documents.
Processing in step (iii) is document by document because some of our features are document-based.
ATAS takes a document as input and identifies all terms in the document, using the term candidate
classifier or the CRF learned in (ii).
The term candidate classifier (ATAS-TC) decides on entire (multi-token) candidates while the CRF
decides on single tokens. ATAS-TC heavily relies on candidate computation and its decisions are mutually
independent, which is clearly incorrect. In contrast, ATAS-CRF is less dependent on candidate compu-
tation and models dependence of decisions correctly; but it lacks the more ?global? view of ATAS-TC on
entire candidates. We want to investigate which approach is more suited for ATA.
In what follows we describe how we preprocess patents, the linguistic filters used to implement our
syntactic definition of term, automatic labeling of training data (step (i) of ATAS), training of term
candidate classifier and CRF (step (ii) of ATAS), features and feature selection.
292
4.1 Preprocessing
The preprocessing pipeline consists of the ANNIE tokenizer, OpenNLP sentence splitter, Mate POS
tagger (Bohnet (2010), retrained for patents) and Mate lemmatizer. Preprocessing has a big influence
on computational terminology because special domain text poses problems for off-the-shelf components.
For example, patents tend to use common language words in rare functions or meanings, e.g., ?said? as
a de facto determiner in contexts such as ?the structure of said component?. Other problems are the
use of special language words, e.g., substances like ?triphenylphosphine? and acronyms like ?AC?. Such
properties pose serious problems to POS taggers. Patent citations, acronyms and even product names
can include punctuation, confusing sentence splitters. Chemical formulas may confuse tokenizers.
We adapted our POS tagger and sentence splitter for patent language to deal with unusual punctuation
and POS tags ? especially unusual POS tags of common-language words like ?said?. This adaptation
involves training on a manually labeled training set of patent text and some other adjustments; e.g., we
only allow the tag NN for the acronyms ?AC?, ?DC? and ?A/D?.
4.2 Filter
We now describe how we find term candidates that satisfy the syntactic definition; recall that only
(possibly modified) nouns can be terms (Section 2).
In general, candidate identification strategies using linguistic knowledge perform better. There are two
different strategies of this type: (i) parsing the sentence, extracting nominal chunks from the parse and
further processing the nominal chunks and (ii) POS tagging the sentence and extracting word sequences
that satisfy a set of predefined POS patterns. Because many patent sentences are long and difficult to
parse, we adopt the POS pattern approach in this paper. To this end, we define two simple POS-based
rules for finding term candidates.
2
PREMODS. This rule defines a modifier sequence. It matches a sequence of noun pre-modifiers:
(JJ|?/?|VBG|RB|N(N|P))*.3 We include RB because the POS tagger sometimes misclassifies JJ as RB.
We include ?/? because the tokenizer splits abbreviations containing it.
CANDIDATE. This rule defines a term candidate. It matches either a single noun or PREMODS
followed by a noun: (PREMODS N(N|P)). The last noun must be longer than two characters. We
add a flag indicating if the candidate comes before a figure reference. A figure reference consists of an
optional keyword (e.g., ?Figure?, ?Fig.?) and a sequence of numbers and letters, optionally enclosed in
parentheses.
We select the longest match in case of overlapping matches and the first longest match in case of
overlapping matches of the same length.
These simple rules will find all terms ? as well as many non-terms that we will train ATAS to identify ?
with two exceptions. First, due to POS errors some candidates are spurious. Second, unwanted modifiers
may be part of candidates. E.g., the rules will only identify ?same battery? as a candidate and not
?battery?. But only ?battery? is a valid term. To address the latter, we manually compiled a stop list of
67 modifiers, mostly numerals (?first?) and adjectives in anaphoric function (?above-mentioned?). These
modifiers are removed from term candidates.
4.3 Automatic Labeling of Training Data
We view ATA as either a binary classification task where a term candidate classifier decides if a candidate
is a term or not, or as a sequence labeling task where a CRF decides if a token (word) belongs to a
term or not.
Large training sets are needed to train such models. Usually, these sets are produced by expensive
human labeling. We present a method for generating high quality training data in an unsupervised way
without the necessity of precompiled resources. In principle, our method can be used for any language
for which machine-readable patents are available.
Our starting point is that patents typically contain figure references, i.e., pointers to drawings illus-
trating the invention or its parts. Consider the example: ?. . . so that first clamp-holding secondary
arms (1) . . . ? Here, the figure reference (?(1)?) points to the illustration ?Figure 1? and is preceded by
the illustrated term (?clamp-holding secondary arms?). Illustrated terms may be concrete, as in this
example, or abstract, e.g., a diagram illustrating properties of a method.
We call a term candidate that precedes a figure reference a basic figure reference term candidate
(bFRTC). In a manual inspection of bFRTCs in 12 patents we found that almost 95% of bFRTCs were
2
JJ, VBG, and RB are POS tags for positive adjectives, gerunds/present participles, and adverbs, respectively.
3
?*? is the Kleene star, ??? denotes optionality, and ?|? denotes alternation.
293
terms. Thus, bFRTCs can be used as positive training examples because they usually denote technical
concepts; they have the advantage of being identifiable with high precision using simple patterns.
Once the bFRTCs have been identified, there is a simple way to further increase the size of the training
data: we add all extended FRTCs (eFRTCs) to the training set, where we define an eFRTC as a term
candidate whose suffix is a bFRTC. E.g., if we have identified ?shunt current? as a bFRTC, then ?AC
shunt current? is an eFRTC. eFRTCs typically are hyponyms since the modifiers added at the beginning
restrict the bFRTC to a more specific meaning. This kind of hyponymy is a special case of term derivation,
a modification where a base term is further specified by prefixes (Daille et al., 1996). The strategy of
identifying eFRTCs can also be applied to free word order languages because figure references tend to
have a local and fixed occurrence pattern similar to English. We use the term FRTC to refer to both
bFRTCs and eFRTCs.
We identify all FRTCs and add them as positive examples to the training set. We also add the 5%
most frequent candidates as positive examples; most of them are FRTCs, so that this step usually adds
few new training examples.
We label the following candidates as negative training examples: candidates appearing only once in
a patent; patent citations; and measurements. Citations and measurements (?3 cm?) are clear non-
terms. We identify them using regular expressions. Many singletons are non-terms because they denote
common language (i.e., nontechnical) concepts, e.g., ?time?. These heuristics for finding negative training
examples are not applied to a candidate if it has the same head as a positive training example.
We exclude from the training set candidates that do not satisfy any positive or negative criteria.
4.4 Classifiers
We use the L2-regularized logistic regression of LIBLINEAR (Fan et al., 2008) as our term candidate
classifier. We use LIBLINEAR?s default normalization for continuous-valued attributes (normalization
to range [0, 1]) and the default representation for categorical attributes. As LIBLINEAR cannot handle
missing values, we replace them with their means and modes. We set the regularization parameter c = 1.
Our sequence model is CRF++
4
, order 1, with default parameters. The CRF features are adapted from
the ATAS-TC features, e.g., term-level features (e.g., TFIDF) are propagated down to the individual
tokens of the term. We also include word trigrams. We discretize numeric features to three values.
4.5 Features
We developed a set of 74 features for ATA. Some of these features are taken from the literature, some
are specific to our approach and make use of the concept of FRTC and some exploit other properties of
patents (e.g., the importance of the title and the claims in patents). A final group consists of other novel
features that we designed in the course of developing our system. We now provide an overview. c refers
to a term candidate.
Corpus and document statistics. This feature type captures termhood and unithood of c as well as
the position of c?s first occurrence in the document. We use a corpus of technical text C
T
and a general
language corpus C
G
. For every c ? C
T
we collect the number of patents it appears in, its frequency
and its FRTC frequency, i.e., the number of its occurrences that are FRTCs. Features that are intended
to indicate termhood include simple frequencies and distributional characteristics (in C
T
or in a single
patent). Finally, we define a measure of frequency deviation (or ?keywordness?) of h(c), the head of c:
bias(h(c)) =
f
C
G
(h(c))
|C
G
|
|C
T
| ? f
C
T
(h(c))
f
C
G
(resp., f
C
T
) are the frequencies in C
G
(resp., C
T
), |X| is the sum of frequencies of all x ? X.
bias(h(c)) measures the deviation between expected frequency of the head of c (estimated on C
G
) and
its actual frequency. The intuition here is that the frequency of a general language noun like ?time? will
be similar over text types, resulting in a lower bias.
Context. This feature type captures unigrams and bigrams adjacent to c as well as their POS tags.
Part-of-speech. This feature type captures the POS sequence of c.
A patent usually focuses on a narrow technological subdomain. As a result, many of its terms are
semantically related to each other. We would like to include features that directly capture semantic simi-
larity to other terms because a candidate that is semantically similar to several other already recognized
terms is likely to be a term itself.
Our goal in this paper is to address ATA using simple and efficient methods. For this reason, we
approximate semantic similarity using string similarity because a subset of semantically similar terms are
4
crfpp.googlecode.com
294
Tu
tdg
T
l
test
T
l
dev
T
u
sel
patents 365 5 11 25
word tokens 3,422,131 50,007 74,000 152,715
word types 292,994 3711 7391 4141
bFRTCs 119,316 1264 2558 6503
FRTCs 240,240 2371 4942 10,110
candidates 353,238 8836 13,099 27,164
terms 3814 7220
Table 1: Data set statistics
P R F
1
description
1 .704 .797 .748 mean string similarity of c and FRTCs
2 .712 .832 .767 frequency of c as an FRTC in C
T
3 .694 .887 .779 TFIDF of c
4 .703 .888 .784 is c uppercase?
5 .708 .893 .790 is c followed by a figure reference?
6 .710 .896 .792 TFIDF of h(c)
7 .711 .895 .793 frequency of h(c) as an FRTC in C
T
8 .718 .892 .795 bias(h(c))
9 .720 .891 .797 # sentences with FRTCs that c occurs in
10 .720 .893 .797 C-value of c
11 .721 .893 .798 frequency of h(c) in C
G
Table 2: Features selected on T
l
dev
(setting S). c: term candidate. h(c): head of c
also similar on the surface. E.g., the semantic similarity between ?AC power supply source? and ?AC
supply source? also manifests itself as string similarity.
String similarity. When designing a similarity measure, we wanted it to satisfy the following criteria:
(i) more words in common should result in higher scores and (ii) words in common towards the end of the
two strings should be weighted higher than words in common at the beginning. The motivation for (ii)
is that candidates differing only in initial modifiers are often cohyponyms and highly related; conversely,
candidates with different heads are often not related.
To implement this, we represent a candidate c as a vector ~c in |V |-dimensional space where V is the
vocabulary. ~c
i
is set to the position of word w
i
in c if it occurs and 0 otherwise. The string similarity
between c and c
?
is then defined as the cosine of ~c and
~
c
?
. Example: for ?AC power supply source? and
?AC supply source?, we get the vectors (1, 2, 3, 4) and (1, 0, 2, 3) and the cosine .927; comparing the first
string with ?AC power supply? with the vector (1, 2, 3, 0) we get the cosine .683.
Features in our initial set of 74 that make use of this semantic similarity are: maximum similarity of
c to any FRTC, average similarity of c to all FRTCs in the patent and similarity of c to the rightmost
term candidate in the title.
Frantzi and Ananiadou (1997) define C-value(c) as:
C-value(c) = log
2
|c|(f(c)?
1
|T
c
|
?
b?T
c
f(b))
where T
c
is the set of term candidates containing c and f is frequency in C
T
. C-value is high for term
candidates that are frequent and occur as parts of many other term candidates ? this is a good indicator
of termhood.
5 Experiments and Evaluation
5.1 Data Sets
We hired three students with a bachelor degree in computer science to annotate 16 patents. The test
set T
l
test
consists of 5 patents annotated by all three students. We used majority voting to produce the
final gold annotations. The devset T
l
dev
consists of the remaining 11 patents. Each T
l
dev
sentence was
annotated by one student.
Inter-annotator agreement on T
l
test
was .76 (Fleiss? ?). Most disagreements concern modifiers or
common nouns (e.g., the term ?battery? was often not annotated). More extensive training of the
annotators should reduce these problems considerably.
As unlabeled data we randomly selected 390 technology patents. We use 365 as T
u
tdg
for training data
generation and 25 as T
u
sel
for unsupervised feature selection. We made sure the 390 documents are not
295
in T
l
dev
and T
l
test
. We excluded chemical patents because standard preprocessing components often fail
for chemical formulas. Table 1 gives data set statistics.
As our technical corpus C
T
we use T
u
tdg
and as our general corpus C
G
all nouns in the 2000 most
frequent English words from Project Gutenberg
5
. This list contains many general nouns which also
appear in patents (e.g., ?time?) without containing many technical terms (e.g., ?battery?); this way, C
T
and C
G
give us a good contrast between technical and non-technical vocabularies (cf. Section 4.5).
One obstacle to comparing systems for ATA in the technical domain is the lack of publicly available
evaluation benchmarks. We are making our data sets and the annotation guidelines available
6
.
5.2 Baselines
We define the FRTC baseline as the system that labels all FRTCs and only FRTCs as terms. Almost
all FRTCs are terms, but many terms are not FRTCs; thus, the FRTC baseline has high precision and
low recall. Our goal is to preserve high precision while considerably increasing recall, or to generalize
well from FRTCs to other terms.
Our state of the art baseline is Z-CRF, a reimplementation of the CRF described in (Zhang et al.,
2010). Its feature representation includes POS tags, unigrams, bigrams and syntactic information, e.g.,
the number of times a particular token is used in a syntactic function like subject in the training set.
Syntactic information is extracted with Mate (Bohnet, 2010). Z-CRF is trained on T
u
tdg
, just as ATAS.
Our last baseline is the well-known C-value (Frantzi and Ananiadou, 1997). Like our first baseline,
it needs no training data. In contrast to our first baseline, it was specifically designed for terminology
acquisition. It combines observations about statistical and linguistic properties of terms, i.e., a candidate
is preferred as a term if it is long and frequently appears as substring of other candidates. Following
Frantzi and Ananiadou (1997) we regard a candidate as term if its C-value it not zero; unlike them, we
do not restrict the length of terms because the computation of long terms did not pose computational
problems for us.
5.3 Evaluation Setup
We evaluate ATAS using precision, recall and F
1
. Evaluation is based on candidate tokens (as opposed
to candidate types or word tokens); e.g., each instance of a candidate term that is incorrectly classified
as a term is a false positive. Evaluation is strict in the sense that a term is counted as a false positive
if there is a single token that is added or missed.
We evaluate ATAS in two settings. In the system (S) setting, the ATAS pipeline described in Section
4 (ATAS-TC or ATAS-CRF) is used to identify term candidates. This is the real-world setting since
errors in term candidate identification ? misplaced boundaries, missing candidates, etc. ? are a major
source of error in ATA.
We would also like to evaluate candidate classification on gold boundaries (manually verified boundaries
of term candidates); this allows us to quantify by how much performance can be improved if candidate
identification is perfect. However, since gold boundary annotation is expensive, we instead approximated
it: (i) We run automatic term candidate identification. (ii) We remove all term candidates that overlap
with gold (manually annotated) terms. (iii) The set of gold term candidates is then the union of all
remaining automatically identified candidates and the manually annotated terms.
In the gold boundary (G) setting, we provide these gold term candidates to the ATAS pipeline. This
allows us to evaluate the performance of term/non-term classification separately from term candidate
identification.
5.4 Feature Selection
For our feature set of 74, we perform forward feature selection for the term candidate classifier by
selecting the feature in each step that maximizes system F
1
. We perform feature selection (i) on the
manually labeled set T
l
dev
(to gauge performance for an optimal or close-to-optimal feature set) and (ii)
on the automatically labeled set T
u
sel
(to gauge the performance in a completely unsupervised setting).
In the following we explain both settings in more detail.
Table 2 gives the features selected in supervised feature selection, i.e., when features are optimized
on T
l
dev
. Precision remains stable, except for a drop on line 3. Recall rises steadily from .797 to .893. F
1
increases from .748 to .798.
The best feature (line 1) is the mean string similarity of a term candidate c to all FRTCs in a document
(Section 4.5). Together with the next best feature (frequency of c as an FRTC in C
T
) and feature 5 (is
5
en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000
6
h-its.org/english/research/nlp/download/terminology.php
296
ATAS-TC ATAS-CRF Baselines
S-SEL U-SEL S-SEL U-SEL Z-CRF C-value FRTC
S G S G S G S G S G S G S G
T
l d
e
v
1 P .721 .838 .690 .796 .732 .844 .727 .854 .867 .891 .384 .749 .839 1.000
2 R .893 .892 .825 .818 .815 .699 .755 .679 .563 .607 .292 .355 .344 .353
3 F
1
.798 .864 .752 .807 .771 .765 .741 .756 .683
?
.722
?
.314
?
.471
?
.488
?
.522
?
T
l t
e
s
t
4 P .696 .753 .627 .692 .774 .832 .664 .745 .813 .840 .388 .726 .864 1.000
5 R .850 .853 .764 .764 .791 .743 .644 .625 .516 .559 .320 .410 .286 .302
6 F
1
.765 .800 .689 .728 .783 .785 .654 .680 .631
?
.674
?
.350
?
.519
?
.430
?
.465
?
Table 3: System (S) and gold boundary (G) results with supervised (S-SEL) and unsupervised (U-SEL)
feature selection.
?
: significantly lower than corresponding ATAS-TC and ATAS-CRF scores.
c followed by a figure reference?) this supports our intuition for using FRTCs for automatic training set
generation because they are indeed strong indicators for termness. Additionally, feature 9 indicates that
candidates occurring often with FRTCs in sentences are probably terms. Feature 4 (is c uppercase?) is
selected because uppercase term candidates are often abbreviations and terms.
Feature 3 (TFIDF of c) hurts precision, but increases recall, resulting in increased F
1
. This feature
models the hypothesis that a term is frequent in some patents but does not occur in many patents.
Patent writers often invent novel terms rather than using standard ones to make finding a patent hard.
Thus, a term candidate that occurs often in a few patents could be such an obfuscating term.
TFIDF is low for terms with small term frequency. Features 6 (TFIDF of h(c)) and 10 (C-value of c)
can help correctly identify such term candidates as terms.
Features 8 and 11 incorporate information from the general purpose corpus C
G
. Feature 8 contrasts
the frequency of c in C
G
with its frequency in C
T
? frequencies of terms are higher in C
T
, frequencies of
non-terms are similar in both corpora. Feature 11 is complementary to this. It makes it more probable
that c is a non-term if its head appears more often in C
G
. Additionally, string similarity with the patent?s
title is an effective feature.
Unsupervised feature selection, i.e., selection on T
u
sel
, selected seven features that are similar
to those selected by supervised selection and that we will discuss now. The best unsupervised feature
(maximum string similarity, 1) and the best supervised feature (mean string similarity) both capture
partial string overlap of c and FRTCs. For similar reasons, the feature ?string similarity of c and rightmost
NP in patent title? (2) ? which exploits the importance of the title in analogy to the importance of figure
references ? is selected.
Other selected features (relative patent frequency of c and its head (3, 4), number of patent sentences
in which c occurs with FRTCs (5), patent frequency of c = 1?(6)) are also similar to the features selected
in the supervised setting. They capture frequency distributions of c. However, while many features in
the supervised setting capture distributions of c in C
T
, in the unsupervised setting, distributions of c in
the patent are more important. The reason may be that C
T
-based features (which use all technical text
as opposed to the relevant patent in question) are harder to recognize as good predictors if the set used
for selection is automatically labeled and hence noisier.
The last unsupervised feature captures the length of c in tokens (7). Manual inspection revealed that
on average terms have more tokens than non-terms (1.9 vs. 1.3).
5.5 ATAS Results
Table 3 gives evaluation results for ATA on T
l
dev
and T
l
test
. We report results for the ATAS versions
(ATAS-TC, ATAS-CRF) and for the baselines (Z-CRF, C-value, FRTC) as well as for using supervised
(S-SEL) and unsupervised feature selection (U-SEL) in system setting (S) and gold boundary setting
(G).
Differences in F
1
between ATAS and baselines (marked with a ?) are significant at p < .01.
7
If not
stated otherwise, numbers below are for the system setting (S).
We note that F
1
of the ATAS versions is consistently and considerably better than all baselines in
all settings. E.g., line 6 shows system F
1
on T
l
test
of ATAS-TC (.765 for S-SEL, .689 for U-SEL) and
ATAS-CRF (.783 for S-SEL, .654 for U-SEL) compared to Z-CRF (.631), FRTC (.430), and the C-value
baseline (.350) . The better results mainly come from higher recall (except for C-value, which is also
beaten in precision). In general, precision of the baselines is higher, but recall much smaller than for
ATAS. This shows that (i) statistical classifiers can be successfully trained for ATA using our method
7
We use approximate randomization (Yeh, 2000) for all significance tests in this paper.
297
Figure 1: System F
1
as a function of training set size (in percent) in setting G.
for automatically generating training data and (ii) these classifiers beat a state-of-the-art system in both
S-SEL and U-SEL settings.
Comparing S-SEL and U-SEL shows that precision and recall for U-SEL are lower than for S-SEL. For
instance, F
1
of ATAS-TC on T
l
test
is .765 for S-SEL and .689 for U-SEL; F
1
of ATAS-CRF is .783 for
S-SEL and .654 for U-SEL (line 6). In general, we note a bigger drop in recall than in precision, indicating
that U-SEL does not generalize as well as S-SEL. However, the U-SEL numbers are significantly better
than the Z-CRF FRTC, and C-value baselines.
When comparing ATAS-TC with ATAS-CRF we note that ATAS-CRF consistently has higher precision
and lower recall. In most cases, ATAS-TC has considerably higher recall, leading to higher F
1
. This is
not surprising given that feature selection was performed for ATAS-TC. Nevertheless, ATAS-CRF can
compete with ATAS-TC in terms of F
1
. Furthermore, ATAS-CRF produces more stable results because
it shows less variance in F
1
across settings.
Comparing S and G scores shows that knowing exact boundaries has a great impact on results, especially
on precision; looking at S-SEL numbers in line 4 in Table 3, precision for ATAS-TC (resp., ATAS-CRF)
is .696 in S vs. .753 in G (resp., .774 in S vs. .832 in G). Similar differences also hold for U-SEL numbers.
In general, ATAS-TC profits more from knowing exact boundaries than ATAS-CRF. This leads us to the
conclusion that the linguistic filter would greatly benefit from a (statistical) measure of unithood. Note
that this also holds for the baselines; deciding about the termness of gold boundary candidates seems to
be easier, especially for C-value.
All observations hold for T
l
dev
and T
l
test
. However, numbers are higher for T
l
dev
because the ratio of
FRTCs to candidates is higher than for T
l
test
(38% vs. 27%) which improves classification performance
on T
l
dev
? this holds for ATAS as well as for the baselines.
To investigate the quality of the extracted training data, consider Figure 1. It shows F
1
in setting G
as a function of training set size in percent of the total training set T
u
tdg
. For each evaluation point,
we randomly add training examples from the full set. F
1
starts at .834 for 0.1% of training data (344
training examples) and rises to .864 for 100% (353,238 examples), with a small drop at 50%. Note that
1000 examples roughly correspond to one annotated patent. The main results of this experiment are that
(i) a modest amount of automatically labeled training data gives good performance and (ii) the more
automatically labeled data the better. The last point is not a trivial finding, given that training data was
generated automatically. The logarithmic graph shows a nearly linear increase in F
1
for each doubling of
the training data.
To further investigate the quality of the generated training data, we compared automatically and
manually produced training examples. We compare results for 13238 manual and 13238 automatic labels
(setting G, ATAS-TC). We get precision and recall of .811 and .805 for manual and .762 and .850 for
automatic annotations, resulting in similar F1 scores: .808 vs. .804 for manual and automatic annotations,
respectively. We believe that the differences in recall are an artifact of the randomization we performed
before removing automatic training samples. Manual labels are entire patents; in contrast, automatic
labels come from all patents in the training set, leaving us with a more diverse set than the manual
version.
5.6 Error Analysis
We found two major types of false negatives. First, infrequent terms are problematic. It is hard to judge
termness when having limited information about a candidate, especially if it appears only once or twice
in a document. Second, POS errors prevent the system from finding some candidates; e.g., the noun
?current? is frequently mistagged as adjective. Incorrect POS tags also lead to incorrect boundaries.
298
We found four major types of false positives. First, incorrect modifiers lead to partially incorrect
terms. 27% of false positives are of this type. Second, incorrectly recognized figure references cause
incorrect system decisions; e.g., our patterns incorrectly parse an expression like ?value PBA? as a figure
reference even though it is instead a named output of a component. Third, very frequent non-terms are
commonly classified as terms. Almost all frequent candidates are terms, so that the term candidate
classifier has difficulty correctly identifying the exceptions from this pattern.
Finally, if a candidate is a term in one context it may be a non-term in another. A good example
for this are general single token terms like ?apparatus?. Before figure references they are terms, e.g.,
?one preferred form of apparatus 22?. In such cases the figure reference serves as a disambiguator.
However, in other positions they are non-terms, e.g., ?They include braces, collars, splints and other
similar apparatus?.
6 Conclusion and Future Work
This paper introduces a method for ATA with two novel aspects: (i) new powerful features for ATA
and (ii) a procedure for generating an ATA training set in an unsupervised fashion. The training set
generation method produces high quality training data, even when compared to manual annotations. It is
language-independent: It can be applied to patents in any language if the definition of term candidates
is modified for the target language. It is also domain-independent: it can be applied to patents of
any domain. The training data can be successfully used to train ATA models, both term candidate
classification as well as CRF models. Even in a completely unsupervised setting the models outperform a
state-of-the-art baseline. We found that using more automatically labeled training data and using better
term boundaries results in better performance.
In future work, we plan to incorporate term variation patterns (Daille et al., 1996; Jacquemin, 2001)
in the expansion process to decrease the number of FNs and increase recall. We would also like to
improve the terminology identification module because we found that incorrect identified boundaries
affect performance greatly.
Finally, we are planning to extend our approach to languages other than English. Our methods are
language-independent to the extent that a body of patents exists for many common languages. Since
we generate the training set automatically, all we need to do to cover another language is to adapt the
linguistic filters for candidate identification.
Acknowledgments. This work was supported by the European Union (Project Topas, FP7-SME-
2011 286639) and by SPP 1335 Scalable Visual Analytics of Deutsche Forschungsgemeinschaft (DFG
grant SCHU 2246/8-2). We would like to thank the anonymous reviewers for their helpful comments and
suggestions, and Bianca and Luca for their support.
References
Sophia Ananiadou. 1994. A methodology for automatic term recognition. In Proceedings of the 15th
conference on Computational linguistics - Volume 2, COLING ?94, pages 1034?1038.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings
of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing,
China, August.
William W. Cohen. 1995. Fast effective rule induction. In Twelft International Conference on Machine
Learning (ML95), pages 115?123.
Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting infor-
mation from text sources. In Thomas Lengauer, Reinhard Schneider, Peer Bork, Douglas L. Brutlag,
Janice I. Glasgow, Hans-Werner Mewes, and Ralf Zimmer, editors, ISMB, pages 77?86. AAAI.
Be?atrice Daille, Beno??t Habert, Christian Jacquemin, and Jean Royaute?. 1996. Empirical Observation
of Term Variations and Principles for their Description. Terminology, 3(2):197?258.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley &
Sons Inc, 1 edition.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
299
Jody Foo and Magnus Merkel. 2010. Using machine learning to perform automatic term recognition. In
Proceedings of the LREC 2010 Workshop on Methods for automatic acquisition of Language Resources
and their Evaluation Methods, pages 49?54.
Katerina T. Frantzi and Sophia Ananiadou. 1997. Automatic Term Recognition using Contextual Cues.
In Proceedings of 3rd DELOS Workshop, Zurich, Switzerland.
Byron Georgantopoulos and Stelios Piperidis. 2000. Term-based Identification of sentences for Text Sum-
marisation. In Proceedings of Second International Conference on Language Resources and Evaluation
(LREC2000), pages 1067?1070, Athens, Greece.
Vasileios Hatzivassiloglou, Pablo Ariel Dubou, and Andrey Rzhetsky. 2001. Disambiguating proteins,
genes, and rna in text: a machine learning approach. In ISMB (Supplement of Bioinformatics), pages
97?106.
Christian Jacquemin and Didier Bourigault. 2003. Term extraction and automatic indexing. In Ruslan
Mitkov, editor, The Oxford Handbook of Computational Linguistics, chapter 33. Oxford University
Press.
Christian Jacquemin. 2001. Spotting and Discovering Terms Through Natural Language Processing. MIT
Press, April.
Kyo Kageura and Bin Umino. 1996. Methods of automatic term recognition: A review. Terminology,
3(2):259?289.
Michael Krauthammer and Goran Nenadic. 2004. Term identification in the biomedical literature.
Journal of Biomedical Informatics, 37(6):512?526, December.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume
2-Volume 2, pages 1003?1011. Association for Computational Linguistics.
Alex Morgan, Lynette Hirschman, Alexander Yeh, and Marc Colosimo. 2003. Gene Name Extraction
Using FlyBase Resources. In Proceedings of ACL 2003 Workshop on Natural Language Processing in
Biomedicine, pages 1?8, Sapporo, Japan.
Maria Teresa Pazienza, Marco Pennacchiotti, Michele Vindigni, and Fabio Massimo Zanzotto. 2005.
Ai/nlp technologies applied to spacecraft mission design. In Proceedings of the 18th international
conference on Innovations in Applied Artificial Intelligence, IEA/AIE?2005, pages 239?248, London,
UK, UK.
John Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Koichi Takeuchi and Nigel Collier. 2005. Bio-medical entity extraction using support vector machines.
Artificial Intelligence in Medicine, 33(2):125?137, February.
Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational linguistics - Volume 2, COLING ?00, pages 947?953,
Stroudsburg, PA, USA.
Xing Zhang and Alex Chengyu Fang. 2010. An ATE system based on probabilistic relations between
terms and syntactic functions. In 10th International Conference on Statistical Analysis of Textual
Data, pages 1135?1143, Sapienza, Italy, June.
Xing Zhang, Yan Song, and Alex Chengyu Fang. 2010. How well conditional random fields can be used in
novel term recognition. In Proceedings of the 24th Pacific Asia Conference on Language, Information
and Computation, pages 583?592, Tohoku University, Sendai, Japan, November.
300
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 311?321, Dublin, Ireland, August 23-29 2014.
Picking the Amateur?s Mind ? Predicting Chess Player Strength from
Game Annotations
Christian Scheible
Institute for Natural Language Processing
University of Stuttgart, Germany
scheibcn@ims.uni-stuttgart.de
Hinrich Sch?utze
Center for Information
and Language Processing
University of Munich, Germany
Abstract
Results from psychology show a connection between a speaker?s expertise in a task and the lan-
guage he uses to talk about it. In this paper, we present an empirical study on using linguistic
evidence to predict the expertise of a speaker in a task: playing chess. Instructional chess litera-
ture claims that the mindsets of amateur and expert players differ fundamentally (Silman, 1999);
psychological science has empirically arrived at similar results (e.g., Pfau and Murphy (1988)).
We conduct experiments on automatically predicting chess player skill based on their natural lan-
guage game commentary. We make use of annotated chess games, in which players provide their
own interpretation of game in prose. Based on a dataset collected from an online chess forum,
we predict player strength through SVM classification and ranking. We show that using textual
and chess-specific features achieves both high classification accuracy and significant correlation.
Finally, we compare our findings to claims from the chess literature and results from psychology.
1 Introduction
It has been recognized that the language used when describing a certain topic or activity may differ
strongly depending on the speaker?s level of expertise. As shown in empirical experiments in psychology
(e.g., Solomon (1990), Pfau and Murphy (1988)), a speaker?s linguistic choices are influenced by the way
he thinks about the topic. While writer expertise has been addressed previously, we know of no work
that uses linguistic indicators to rank experts.
We present a study on predicting chess expertise from written commentary. Chess is a particularly
interesting task for predicting expertise: First, using data from competitive online chess, we can compare
and rank players within a well-defined ranking system. Second, we can collect textual data for experi-
mental evaluation from web resources, eliminating the need for manual annotation. Third, there is a large
amount of terminology associated with chess, which we can exploit for n-gram based classification.
Chess is difficult for humans because it requires long-term foresight (strategy) as well as the capacity
for internally simulating complicated move sequences (calculation and tactics). For these reasons, the
game for a long time remained challenging even for computers. Players have thus developed general
principles of chess strategy on which many expert players agree. The dominant expert view is that the
understanding of fundamental strategical notions, supplemented by the ability of calculation, is the most
important skill of a chess player. A good player develops a long-term plan for the course of the game.
This view is the foundation of many introductory works to chess (e.g., Capablanca (1921), one of the
earliest works).
Silman (1999) presents games he played with chess students, analyzing their commentary about the
progress of the game. He claims that players who fail to adhere to the aforementioned basic princi-
ples tend to perform worse and argues that the students? thought processes reflect their playing strength
directly. Lack of strategical understanding marks the difference between amateur and expert players.
Experts are mostly concerned with positional aspects, i.e., the optimal placement of pieces that offers a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
311
8rm0ZkZ0s
7opo0lpa0
60Z0ZpZpo
5Z0Z0O0Z0
40Z0Z0ZbZ
3Z0MBZNZ0
2POPZ0OPO
1S0ZQZRJ0
a b c d e f g h
Figure 1: Example chess position, white to play
long-lasting advantage. Amateurs often have tactical aspects in mind, i.e., short-term attacking oppor-
tunities and exploits that potentially lead to loss of material for their opponents. A correlation between
chess strength and verbalization skills has been shown empirically by Pfau and Murphy (1988), who
used experts to assess the quality of the subjects? writing.
In this paper, we investigate the differences between the mindset of amateurs and experts expressed in
written game commentary, also referred to as annotated games. When studying chess, it is best practice
to review one?s own games to further one?s understanding of the game (Heisman, 1995). Students are
encouraged to annotate the games, i.e., writing down their thought process at each move. We address the
problem of predicting the player?s strength from the text of these annotations. Specifically, we want to
predict the rank of the player at the point when a given game was played. In competitive play, the rank
is determined through a numerical rating system ? such as the Elo rating system (Elo, 1978) used in this
paper ? that measures the players? relative strength using pairwise win expectations.
This paper makes the following contributions. First, we introduce a novel training dataset of games
annotated by the players themselves ? collected from online chess forum. We then formulate the task of
playing strength prediction. For each annotated game, each game viewed as a document, we predict the
rating class or overall rank of the player. We show that (i) an SVM model with n-gram features succeeds
at partitioning the players into two rating classes (above and below the mean rating); and (ii) that ranking
SVMs achieve significant correlation between the true and predicted ranking of the players. In addition,
we introduce novel chess-specific features that significantly improve the results. Finally, we compare the
predictions made by our model to claims from instructional chess literature and results from psychology
research.
We next give an overview of basic chess concepts (Section 2). Then, we introduce the dataset (Sec-
tion 3) and task (Section 4). We present our experimental results in Section 5. Section 6 contains an
overview of related work.
2 Basic Chess Concepts
2.1 Chess Terminology
We assume that the reader has basic familiarity with chess, its rules, and the value of individual pieces.
For clarity, we review some basic concepts of chess terminology, particularly elementary concepts related
to tactics and strategy in an example position (Figure 1).
1
From a positional point of view, white is ahead in development: all his minor pieces (bishops and
knights) have moved from their starting point while black?s knight remains on b8. White has also castled
(a move where the rook and king move simultaneously to get the king to a safer spot on either side of the
board) while black has not. White has a space advantage as he occupies the e5-square (which is in black?s
1
Modified from the game Dzindzichashvili ? Yermolinsky (1993) which is the first position discussed in (Silman, 1999)
312
1.e4 e5 2.Nf3 Nc6 3.Bc4 Nh6 4.Nc3 Bd6 Trying to follow basic opening principals, control center,
develop. etc 5.d3 Na5 6.Bb5 Moved bishop not wanting to trade, but realized after the move that my
bishop would be harassed by the pawn on c7 6...c6 7.Ba4 Moved bishop to safety, losing tempo 7...Qf6
8.Bg5 Qg6 9.O-O b5 Realized my bishop was done, might as well get 2 pawns 10.Nxb5 cxb5 11.Bxb5
Ng4 12.Nxe5 Flat out blunder, gave up a knight, at least I had a knight I could capture back 12...Bxe5
13.Qxg4 Bxb2 14.Rab1 Bd4 15.Rfe1 Moved rook to E file hoping to eventually attack the king. 15...h6
16.c3 Poor attempt to move the bishop, I realized it after I made the move 16...Bxc3 17.Rec1 Be5
18.d4 Another crappy attempt to move that bishop 18...Bxd4 19.Rd1 O-O 20.Rxd4 d6 21.Qd1 I don?t
remember why I made this move. 21...Qxg5 22.Rxd6 Bh3 23.Bf1 Protecting g2 23...Nc4 24.Rd5 Qg6
25.Rc1 Qxe4 26.f3 Qe3+ 27.Kh1 Nb2 28.Qc2 Rac8 29.Qe2 Qxc1 30.gxh3 Nc4 31.Qe4 Qxf1#
Figure 2: Example of an annotated game from the dataset (by user aevans410, rated 974)
half of the board) with a pawn. This pawn is potentially weak as it cannot easily be defended by another
pawn. Black has both of his bishops (the bishop pair) which is considered advantageous as bishops
are often superior to knights in open positions. Black?s light-square bishop is bad as it is obstructed by
black?s own pawns (although it is outside the pawn chain and thus flexible). Strategically, black might
want to improve the position of the light-square bishop, make use of his superior dark-square bishop,
and try to exploit the weak e5 pawn. Conversely, white should try create posts for his knights in black?s
territory. Tactically, white has an opportunity to move his knight to b5 (written Nb5 in algebraic chess
notation), from where it would attack the pawn on c7. If the knight could reach c7 (currently defended
by black?s queen), it would fork (double attack) black?s king and rook, which could lead to the trade of
the knight for the rook on the next move (which is referred to as winning the exchange). White?s knight
on f3 is pinned, i.e., the queen would be lost if the knight moved. Black can win a pawn by removing the
defender of e5, the knight on f3, by capturing it with the bishop.
This brief analysis of the position shows the complex theory and terminology that has developed
around chess. The paragraph also shows an example of game annotation (although not every move in the
game will be covered as elaborately in practice in amateur analyses).
2.2 Elo Rating System
Our goal in this paper is to predict the ranking of chess players based on their game annotations. We will
give a brief overview of the Elo system (Elo, 1978) that is commonly used to rank players. Each player
is assigned a score that is changed after each game depending on the expected and actual outcome. On
chess.com, a new player starts with an initial rating of 1200 (an arbitrary number chosen for historical
reasons, which has since become a wide-spread convention in chess). Assuming the current ratings R
a
and R
b
of two players a and b, the expected outcome of the game is defined as
E
a
=
1
1 + 10
?
R
a
?R
b
400
.
E
a
is then used to conduct a (weighted) update of R
a
and R
b
given the actual outcome of the game.
Thus, Elo ratings make pairwise adjustments to the scores. The differences between the ratings of two
players predict the probability of one winning against the other. However, the absolute ratings do not
carry any meaning by themselves.
3 Annotated Chess Game Data
For supervised training, we require a collection of chess games annotated by players of various strengths.
An annotated chess game is a sequence of chess moves with natural language text commentary associated
to specific moves. While many chess game collections are available, some of them containing millions of
games, the majority are unannotated. The small fraction of annotated games mostly features commentary
by masters rather than amateurs, which is not interesting for a contrastive study.
The game analysis forum on chess.com encourages players to post their annotated games for review
through the community. While several games are posted each day, we can only use a small subset of them.
313
Parameter Value
# games 182
# different players 130
mean # moves by game 42
mean # annotated moves by game 16
mean # words by game 114
Table 1: Dataset statistics
0 500 1000 1500 2000 2500
0
.
0
0
0
.
0
5
0
.
1
0
0
.
1
5
Rating (Elo)
%
 
P
l
a
y
e
r
s
? ? ? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ? ? ? ? ?
?
chess.com overall
our dataset
Figure 3: Rating distribution on chess.com and our dataset.
4
Each point shows the percentage of
players in a bin of width 50 around the value. Dotted line: Median on our dataset used for binning.
Many games are posted without annotations, instead soliciting annotation from the community. Others
are missing the rating of the player at the time the game was played ? the user profile shows only the
current rating for the player which may differ strongly from their historical one.
We first downloaded all available games from the forum archive. The games are stored in portable
game notation (PGN, Edwards (1994)). Next, we manually removed games where the annotation had
been conducted automatically by a chess program. We also removed games that had annotations at fewer
than three moves. The final dataset consists of 182 games with annotations in English and known player
rating.
2
We reproduce an example game from the data in Figure 2. This game is typical as the first
couple of moves are not commented (as opening moves are typically well-known). Then, the annotator
comments on select moves that he believes are key to the progress of the game. Table 1 shows some
statistics about the dataset.
The distribution of the ratings in our dataset is shown in Figure 3 in comparison to the overall standard
chess rating distribution on chess.com.
3
Elo ratings assume a normal distribution of players. We see
that overall, the distributions are quite similar, although we have a higher peak and our sample mean is
shifted towards higher ratings (1347 overall vs 1462 on our dataset). It is more common for mid-level
players to request annotation advice than it is for low-rated players (who might not know about this
practice) or high-rated players (who do not look for support by the lower-rated community).
The dataset is still somewhat noisy as players may obtain different ratings depending on the type of
venue (over-the-board tournament vs online chess) or the amount of time the players had available (time
control). Differences in these parameters lead to different rating distributions.
4
For this reason, the total
ordering given through the ratings may be difficult to predict. Thus, we will conduct experiments both
2
Available at http://www.ims.uni-stuttgart.de/data/chess
3
Data from http://www.chess.com/echess/players
4
cf. http://www.chess.com/article/view/chesscom-rating-comparisons
314
on ranking and on classification where the rating range is binned into two rating classes.
4 Predicting Chess Strength from Annotations
4.1 Classification and Ranking
The task addressed in this paper is prediction on the game level, i.e., predicting the strength of the player
of each game at the time when the game was played. We view a game as a document ? the concatenation
of the annotations at each move ? and extract feature vectors as described in Section 4.2. We pursue two
different machine learning approaches based on support vector machines (SVMs) to predicting chess
strength: classification and ranking.
The simplest way to approach the problem is classification. For this purpose, we divide the range of
observed rating into two evenly spaced rating classes at the median of the overall rating range (henceforth
amateur and expert). The classification view has obvious disadvantages. At the boundaries of the bins,
the distinction between them becomes difficult.
To predict a total ordering of all players, we use a ranking SVM (Herbrich et al., 1999). This model
casts ranking as learning a binary classification function that decides whether rank(x
1
) > rank(x
2
) over
all possible pairs of example feature vectors x
1
and x
2
with differing rank.
Note that since Elo ratings are continuous real numbers, it would be conceivable to fit a regression
model. However, Elo is designed as a pairwise ranking measure. While a relative difference in Elo
represents the probability of one player beating the other, the absolute Elo rating is not directly inter-
pretable.
5
4.2 Features
We extract unigrams (UG) and bigrams (BG) from the texts. In addition, we propose the following two
chess-specific feature sets derived from the text:
6
Notation (NOT). We introduce two indicators for whether the annotations contain certain types of
formal chess notation. The feature SQUARE is added if the annotation contains a reference to a specific
square on the chess board (e.g., d4). If the annotation contains a move in algebraic notation (e.g., Nxb4+,
meaning that a knight moved to b4, captured a piece there and put the enemy king in check), the feature
MOVE is added.
Similarity to master annotations (MS). This feature is intended to compensate for the lack of training
data. We used a master-annotated database consisting of 500 games annotated by chess masters which
is available online.
7
As we do not know the exact rating of the annotators, and to avoid strong class
imbalances, we cannot make use of the games directly through supervision. Instead, we calculate the
cosine similarity between the centroid
8
of the n-gram feature vectors of the master games and each game
in the chess.com dataset. The cosine similarity between each game and the master centroid is added
as a numerical feature.
Additionally, the master similarity scores can be used on their own to rank the games. This can be
viewed distant supervision as strength is learned from an external database. We will evaluate this ranking
in comparison with our trained models.
5 Experiments
This section, contains experimental results on classifying and ranking chess players. We first present
quantitative evaluation of the classification and ranking models and discuss the effect of chess-specific
5
Preliminary experiments with SVM regression showed little improvements over a baseline of assigning the mean rating to
all games. This suggests that the distribution of rankings is difficult to model ? possibly due to the low number of annotated
games on which the model can be trained.
6
We also tried using the length of the annotation as well as the number of annotated moves as a feature, which did not
contribute any improvements.
7
http://www.angelfire.com/games3/smartbridge/famous_games.zip
8
We also tried a k-NN approach where we computed the mean similarity of a game from our dataset to its k nearest
neighbors among the master games (k ? 1, 2, 5,?), but found that this approach performed worse.
315
Model Features F
(?)
1
F
(?)
1
F
(?)
1
1 Majority BL 67.2 0.0 33.6
2 SVM (linear) UG 73.4 71.6 72.5
3 SVM (linear) UG, BG 74.1 72.0 73.1
4 SVM (linear) UG, BG, NOT 75.7 74.9 75.3
5 SVM (linear) UG, BG, NOT, MS 74.2 73.0 73.6
(a) Results (F
1
in %)
1 2 3 4 5
1
2 **
3 **
4 ** ?
5 **
(b) Statistical significance of
differences in F
1
. **: p < 0.01,
*: p < 0.05, ?: p < 0.1
Table 2: Classification results
Class Features
Amateur (?) bishop, d4, opening, instead, trying, should, did, where, do, even, rook, get, good, he, coming, point i,
exchange, thought, did not, his, clock, too, or, on clock, knight for
Expert (?) this, game, can, will, winning, NOT:move, time, draw, because, white, back, black, mate, that, but, moves,
can?t, very, on, won, really, so, i know, now, only
Table 3: Top 25 features with most negative (amateur) and positive (expert) weights (mean over all folds)
in the best setup (UG, BG, NOT)
features. Second, we qualitatively compare the predictions of our models with findings and claims from
the literature about the connection between a player?s mindset and strength.
5.1 Experimental Setup
To generate feature vectors, we first concatenate all the annotations for a game, tokenize and lowercase
the texts, and remove punctuation as well as a small number of stopwords. We exclude rare words
to avoid overfitting: We remove all n-grams that occur fewer than 5 times, and add the chess-specific
features proposed above. Finally, we L
2
-normalize each vector.
We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM (Chang and Lin,
2011). We run all experiments in a 10-fold cross-validation setup.
Wemeasure macro-averaged F
1
for our classification results. We evaluate the ranking model using two
measures: pairwise ranking accuracy (Acc
r
), i.e., the accuracy over the binary ranking decision for each
player pair; and Spearman?s rank correlation coefficient ? for the overall ranking. To test whether differ-
ences between results are statistical significant, we apply approximate randomization (Noreen, 1989) for
F
1
, and the test by Steiger (1980) for correlations, which is applicable to ?.
5.2 Classification
We first investigate the classification case, i.e., whether we can distinguish players below and above the
rating mean. Table 2 shows the results for this experiment. We show F
1
scores for the lower and higher
half of the players (F
(?)
1
and F
(?)
1
, respectively), and the macro average of these two scores (F
(?)
1
). We
first note that all SVM classifiers (lines 2?5) score significantly higher than the majority baseline (line 1).
When adding bigrams (line 3) and chess-specific notation features (line 4), F
1
increases. However, these
improvements are not statistically significant. The master similarity feature (line 5) leads to a drop in
F
1
from the previous line. The relatively low rank correlation between the master similarity scores and
the two classes (? = 0.334) leads to this effect. The low correlation itself may occur because the master
games were annotated by a third party (instead of the players), leading to strong differences in style.
There are several reasons for misclassification. Many errors occur in the dense region around the
class boundary. Also, shorter game annotations are more difficult to classify than longer ones. For
detailed error analysis, we first examine the most positively and negatively weighted features of the
trained models (Table 3). We will provide a more detailed look into the features in Section 5.4. We
316
Model Features Acc
r
? sig
1 MS (standalone) ? ? 0.279 ?
2 SVM (linear) UG 58.7 0.266 ?
3 SVM (linear) UG, BG 58.8 0.286 ?
4 SVM (linear) UG, BG, NOT 60.0 0.307 ?
5 SVM (linear) UG, BG, NOT, MS 59.8 0.310 ?
6 SVM (RBF) UG 64.0 0.389 ?
7 SVM (RBF) UG, BG 63.9 0.395 ?
8 SVM (RBF) UG, BG, NOT 63.8 0.400 ?
9 SVM (RBF) UG, BG, NOT, MS 63.5 0.397 ?
(a) Ranking results (accuracy in % and ?)
1 2 3 4 5 6 7 8 9
1
2
3
4 *
5 *
6 ? ?
7 ? * ?
8 ? * * ? ?
9 ? ? ?
(b) Statistical significance of differences in ?.
**: p < 0.01, *: p < 0.05, ?: p < 0.1
Table 4: Ranking results for standalone master similarity and SVM (linear and RBF kernel). Check in
sig column denote significance of correlation with true ranking (p < 0.05). Numbers in sigdiff column
denote a significant improvement (p < 0.05) in ? over the respective line.
find that there are noticeable differences in the writing styles of amateurs and experts. According to
the model, one of the most prominent distinctions is that amateurs tend to refer to the opponent as he,
whereas experts use white and black more frequently. However, it is of course not universally true, which
leads to the misclassification of some experts as amateurs. Another difference in style is that amateur
players tend to write about the game in the past tense. This is a manifestation of an important distinction:
Amateurs often state the obvious developments of the game (e.g., Flat out blunder, gave up a knight
in Figure 2) or speculate about options (e.g., hoping to eventually attack), while experts provide more
thorough positional analysis at key points.
5.3 Ranking
We now turn to ranking experiments (Table 4). We first evaluate the ranking produced by ordering the
games by their similarity to the master centroid (line 1). We find that the resulting rank correlation is low
but significant.
The results for the linear SVM ranker are shown in lines 2?5. Total ranking is considerably more diffi-
cult than binary classification of rating classes. Using a linear SVM, we again achieve low but significant
correlations. The linear classifiers (lines 2?5) do not significantly outperform the standalone master sim-
ilarity (MS) baseline (line 1). Chess-specific features (lines 4 and 5) boost the results, outperforming the
bigram models (line 3) significantly. The improvement from adding the MS centroid score feature is not
significant.
We again perform error analysis by examining the feature weights (Table 5). We find an overall picture
similar to the classification setup (cf. Table 3). The notation feature serves as a good indicator for the
upper rating range (cf. Table 3) as experienced players find it easier to express themselves through
notation. We observed that lower players tend to express moves in words (e.g., ?move my knight to d5?)
rather than through notation (Nd5), which could serve as an explanation for why pieces (bishop, knight,
rook) appear among the top features for amateur players.
However, some features change signs between the two experiments (e.g., king, square). This effect
may indicate that the binary ranking problem is not linearly separable, which is plausible; mid-rated
players may use terms that neither low-rated nor high-rated players use. Examining correlations at dif-
ferent ranking ranges confirms this suggestion. In top and bottom thirds of the rating scale, the true and
predicted ranks are not correlated significantly. This means that the ranking SVM only succeeds at rank-
ing players in middle third of the rating scale. To introduce non-linearity, we conduct further experiments
with an SVM with a radial basis function (RBF) kernel.
The results of this experiment are shown in lines 6?9 of Table 4. All RBF models perform better than
317
Class Features
Weaker instead, king, thinking, one my, fight, d4, even, should, should i, bishop, decided, did, i didn?t, opening, feel,
put, defense, knight on, black king, been, with my, where, get, cover, pin
Stronger NOT:move, moves, game, time, won, i know, already, will, stop, way, winning, line, can?t, can, black has, this,
MS, king side, computer, threaten, first, back, any way, my knight, win pawn, d
Table 5: Top 25 features with most negative (lower rating) and positive (higher rating) weights, mean
over all folds (rank(x
1
) > rank(x
2
) or vice versa) in the best ranking setup (linear SVM, UG, BG, NOT)
Feature Coefficient
capture -0.29
take -0.21
bishop -1.06
knight -0.19
rook -0.54
king 0.19
queen 0.08
pawn 0.44
pin -0.26
fork -0.27
Feature Coefficient
threat 0.13
danger 0.25
stop 0.50
weakness 0.34
light 0.21
dark 0.37
variation 0.41
winning 0.87
losing 0.08
like -0.16
hate -0.05
good -0.27
bad 0.52
Feature Coefficient
white 0.74
black 0.71
he -0.51
fight -0.17
know 0.41
will 0.88
thinking -0.44
believe -0.02
maybe -0.19
hoping -0.30
Feature Coefficient
time 0.81
clock -0.47
time pressure -0.12
blunder -0.31
tempo -0.36
checkmate -0.24
mate 0.69
opening -0.63
castle -0.33
fall -0.22
eat -0.28
Table 6: Selected SVM weights in the best 2-class setup, mean over all folds
the unigram and bigram linear models; all except for the unigram model (lines 7?9) also yield weakly
significant improvements over the MS baseline. Adding the notation features (line 8 improves the results
and leads to improvements with stronger significance. The RBF kernel makes feature weight analysis
impossible, so we cannot perform further error analysis.
5.4 Comparing the Learned Models and Strength Indicators from the Chess Literature
There are many conjectures from instructional chess literature and results from psychological research
about various aspects of player behavior. In this section, we compare these to the predictions made by
our supervised expertise model. In Table 6, we list selected weights from the best classification model
(line 3 in Table 2). We opt for analzying the classifier rather than the ranker as we find the former more
directly interpretable.
Long-Term vs Short-Term Planning. The SVM model reflect the short-term nature of the amateurs?
thoughts in several ways: (i) Amateurs focus on specific moves rather than long-term plans, and thus,
terms like capture and take are deemed predictive for lower ratings. (ii) Amateurs often think piece-
specific (Silman, 1999), particularly about moves with minor pieces (bishop or knight), and these terms
receive high negative weights, pointing to lower ratings. Related to this, Reynolds (1982) observed that
amateurs often focus on the current location of a piece, whereas experts mostly consider possible future
locations. The SVM model learns this by weighting bigrams of the form * on, where * is a piece, as
indicators for low ratings. (iii) Many terms related to elementary tactics (e.g., pin, fork) indicate lower-
rated players, whereas terms relating to tactical foresight (e.g., threat, danger, stop) as well as positional
terms (e.g., weakness, light and dark squares, variation) indicate higher-rated players.
Emotions. A popular and wide-spread claim is that weaker chess players often lose because they are
too emotionally invested in the game and thus get carried away (e.g., Cleveland (1907), Silman (1999)).
We experimented with a sentiment feature, counting polar terms in the annotations using a polarity
lexicon (Wilson et al., 2005). However, this feature did not improve our results.
Manual examination of features expressing sentiment reveals that both amateurs and experts use sub-
jective terms. We note that the vocabulary of subjective expressions is very constrained for stronger
318
players while it is open for weaker ones. Expert players tend to assess positions as winning or losing
for a side, whereas weaker players tend to use terms such as like and hate. Both terms are identified as
indicators of the respective strength class in our models. Other subjective assessments (e.g., good and
bad) are divided among the classes. Emotional tendencies of amateurs can also be observed through
objective indicators. As discussed above, stronger players talk about the game with a more distanced
view, often referring to their opponent by their color (white or black) rather than using the pronoun he.
Lower-rated players appear to use terms indicating competitions more frequently, such as fight.
Confidence. Silman (1999) argues that weaker players lack confidence, which leads to them losing
track of their own plans and to eventually follow their opponent?s will (often called losing the initiative).
This process is indeed captured by our trained models. Terms of high confidence (such as know, will) are
weighted towards the stronger class, whereas terms with higher uncertainty (such as thinking, believe,
maybe, hoping) indicate the weaker class. This observation is in line with findings on self-assigned
confidence judgments of chess players (Reynolds, 1992). The sets of terms expressing certainty and
uncertainty, respectively, are small in our dataset, so weights for most terms can be learned directly on
the n-grams.
Time Management. It has been suggested that deficiencies in time management are responsible for
many losses at the amateur level, particularly in fast games (e.g., blitz chess, where each player has 5
minutes to complete the game), for example due to poor pattern recognition skills of beginners (Calder-
wood et al., 1988). In the trained models, we see that the term time itself is actually considered a good
indicator for stronger players. Time is often used to signify number of moves. So, when used on its own,
time is referring to efficient play, which is indicative of strong players. Conversely, the terms clock and
time pressure are deemed good features to identify weaker players.
Chess Terminology. As shown in Section 2.1 and throughout this paper, there is a vast amount of chess
terminology. We observe that frequent usage of such terms (e.g., blunder ? a grave mistake, tempo, check-
mate ? experts use mate, opening, castle) actually indicate a weaker player. This seems counterintuitive
at first, as we may expect lower-rated players to be less familiar with such terms. However, it appears
that they are frequently overused by weaker players. This also holds for metaphorical terms, such as fall
or eat instead of capture.
6 Related Work
The treatment of writer expertise in extralinguistic tasks in NLP has mostly focused on two problems:
(i) retrieval of experts for specific areas ? i.e., predicting the area of expertise of a writer (e.g., Tu et al.
(2010; Kivim?aki et al. (2013)); and (ii) using expert status in different downstream applications such as
sentiment analysis (e.g., Liu et al. (2008)) or dialog systems (e.g., Komatani et al. (2003)). Conversely,
our work is concerned with predicting a ranking by expertise within a single task.
Several publications have dealt with natural language processing related to games. Chen and Mooney
(2008) investigate grounded language learning where commentary describing the specific course of a
game is automatically generated. Commentator expertise is not taken into account in this study. Branavan
et al. (2012) introduced a model for using game manuals to increase the strength of a computer playing
the strategy video game Civilization II. Cadilhac et al. (2013) investigated the prediction of player actions
in the strategy board game The Settlers of Catan. Our approach differs conceptually from theirs as their
main focus lies on modeling concrete actions in the game (either predicting or learning them); our goal
is to predict player strength, i.e., to learn to compare players among each other. Rather than explicitly
modeling the game, commentary analysis aims to provide insight into specific thought processes.
Work in psychology research by Pfau and Murphy (1988) showed the quality of chess players? verbal-
ization about positions is correlated significantly with their rating. While they use manual assessments
by chess masters to determine the quality of a player?s writing, our approach is to learn this distinction
is automatically given the ratings.
319
7 Conclusion
In this paper, we presented experiments on predicting the expertise of speakers in a task using linguistic
evidence. We introduced a classification and a ranking task for automatically ranking chess players by
playing strength using their natural language commentary. SVM models succeed at predicting either a
rating class or an overall ranking. In the ranking case, we could significantly boost the results by using
chess-specific features extracted from the text. Finally, we compared the predictions of the SVM with
popular claims from instructional chess literature as well as results from psychology research. We found
that many of the traditional findings are reflected in the features learned by our models.
Acknowledgements
We thank Daniel Quernheim for providing his chess expertise, Kyle Richardson and Jason Utt for helpful
suggestions, and the anonymous reviewers for their comments.
References
SRK Branavan, David Silver, and Regina Barzilay. 2012. Learning to win by reading manuals in a monte-carlo
framework. Journal of Artificial Intelligence Research, 43(1):661?704.
Anais Cadilhac, Nicholas Asher, Farah Benamara, and Alex Lascarides. 2013. Grounding strategic conversation:
Using negotiation dialogues to predict trades in a win-lose game. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 357?368.
Roberta Calderwood, Gary A Klein, and Beth W Crandall. 1988. Time pressure, skill, and move quality in chess.
The American Journal of Psychology, 101(4):481?493.
Jos?e R Capablanca. 1921. Chess Fundamentals. Harcourt.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology (ACM TIST), 2(3):1?27.
David L Chen and Raymond J Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In
Proceedings of the 25th International Conference on Machine learning (ICML), pages 128?135.
Alfred A Cleveland. 1907. The psychology of chess and of learning to play it. The American Journal of Psychol-
ogy, 18(3):269?308.
Steven J Edwards. 1994. Portable game notation specification and implementation guide.
Arpad E Elo. 1978. The Rating of Chessplayers, Past and Present. Batsford.
Dan Heisman. 1995. The Improving Annotator ? From Beginner to Master. Chess Enterprises.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Large margin rank boundaries for ordinal regression.
Advances in Neural Information Processing Systems (NIPS), pages 115?132.
Ilkka Kivim?aki, Alexander Panchenko, Adrien Dessy, Dries Verdegem, Pascal Francq, Hugues Bersini, and Marco
Saerens. 2013. A graph-based approach to skill extraction from text. In Proceedings of TextGraphs-8, pages
79?87.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, and Hiroshi G. Okuno. 2003. Flexible guidance genera-
tion using user model in spoken dialogue systems. In Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 256?263.
Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online
reviews. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM), pages
443?452.
Eric W Noreen. 1989. Computer Intensive Methods for Hypothesis Testing: An Introduction. Wiley.
H Douglas Pfau and Martin D Murphy. 1988. Role of verbal knowledge in chess skill. The American Journal of
Psychology, 101(1):73?86.
320
Robert I Reynolds. 1982. Search heuristics of chess players of different calibers. The American journal of
psychology, 95(3):383?392.
Robert I Reynolds. 1992. Recognition of expertise in chess players. The American journal of psychology,
105(3):409?415.
Jeremy Silman. 1999. The Amateur?s Mind: Turning Chess Misconceptions into Chess Mastery. Siles Press.
Gregg E A Solomon. 1990. Psychology of novice and expert wine talk. The American Journal of Psychology,
103(4):495?517.
James H Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245.
Yuancheng Tu, Nikhil Johri, Dan Roth, and Julia Hockenmaier. 2010. Citation author topic model in expert search.
In Proceedings of the 2010 Conference on Computational Linguistics (Coling): Posters, pages 1265?1273.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the Conference on Human Language Technology (HLT) and Empirical
Methods in Natural Language Processing (EMNLP), pages 347?354.
321
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1210?1215,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Fine-Grained Contextual Predictions for Hard Sentiment Words
Sebastian Ebert and Hinrich Sch?utze
Center for Information and Language Processing
University of Munich, Germany
ebert@cis.lmu.de, inquiries@cislmu.org
Abstract
We put forward the hypothesis that high-
accuracy sentiment analysis is only pos-
sible if word senses with different polar-
ity are accurately recognized. We pro-
vide evidence for this hypothesis in a case
study for the adjective ?hard? and propose
contextually enhanced sentiment lexicons
that contain the information necessary for
sentiment-relevant sense disambiguation.
An experimental evaluation demonstrates
that senses with different polarity can be
distinguished well using a combination of
standard and novel features.
1 Introduction
This paper deals with fine-grained sentiment anal-
ysis. We aim to make three contributions. First,
based on a detailed linguistic analysis of contexts
of the word ?hard? (Section 3), we give evidence
that highly accurate sentiment analysis is only pos-
sible if senses with different polarity are accu-
rately recognized.
Second, based on this analysis, we propose to
return to a lexicon-based approach to sentiment
analysis that supports identifying sense distinc-
tions relevant to sentiment. Currently available
sentiment lexicons give the polarity for each word
or each sense, but this is of limited utility if senses
cannot be automatically identified in context. We
extend the lexicon-based approach by introducing
the concept of a contextually enhanced sentiment
lexicon (CESL). The lexicon entry of a word w in
CESL has three components: (i) the senses of w;
(ii) a sentiment annotation of each sense; (iii) a
data structure that, given a context in which w oc-
curs, allows to identify the sense of w used in that
context.
As we will see in Section 3, the CESL sense
inventory ? (i) above ? should be optimized for
sentiment analysis: closely related senses with the
same sentiment should be merged whereas subtle
semantic distinctions that give rise to different po-
larities should be distinguished.
The data structure in (iii) is a statistical classi-
fication model in the simplest case. We will give
one other example for (iii) below: it can also be a
set of centroids of context vector representations,
with a mapping of these centroids to the senses.
If sentiment-relevant sense disambiguation is
the first step in sentiment analysis, then power-
ful contextual features are necessary to support
making fine-grained distinctions. Our third con-
tribution is that we experiment with deep learn-
ing as a source of such features. We look at
two types of deep learning features: word em-
beddings and neural network language model pre-
dictions (Section 4). We show that deep learn-
ing features significantly improve the accuracy
of context-dependent polarity classification (Sec-
tion 5).
2 Related work
Initial work on sentiment analysis was either based
on sentiment lexicons that listed words as posi-
tive or negative sentiment indicators (e.g., Turney
(2002), Yu and Hatzivassiloglou (2003)), on statis-
tical classification approaches that represent doc-
uments as ngrams (e.g., Pang et al. (2002)) or on
a combination of both (e.g., Riloff et al. (2003),
Whitelaw et al. (2005)). The underlying assump-
tion of lexicon-based sentiment analysis is that
a word always has the same sentiment. This is
clearly wrong because words can have senses with
different polarity, e.g., ?hard wood? (neutral) vs.
?hard memory? (negative).
Ngram approaches are also limited because
ngram representations are not a good basis for
relevant generalizations. For example, the neu-
tral adverbial sense ?intense? of ?hard? (?laugh
hard?, ?try hard?) vs. the negative adjectival mean-
1210
Cobuild syntax meaning example patterns sent. # train # test
1 FIRM 1 ADJ firm, stiff hard floor neu 78 5
2 DIFFICULT 2, 4, 9,
10, 11
ADJ difficult hard question hard for,
hard on,
hard to V
neg 2561 120
3 ADVERB 3a, 5,
6, 7
ADV intensely work hard neu 425 19
4 INTENSE 3b ADJ intense hard look be hard at
it
neu 24 7
5 HARD-MAN 8 ADJ unkind hard man neg 15 0
6 HARD-TRUTH 12 attributive
ADJ
definitely
true
hard truth neu 5 6
7 MUSIC ADJ hard-rock-
type music
hard beats neu 347 15
8 CONTRAST ADJ opposite of
soft transi-
tion
hard edge neu 3 1
9 NEGATIVE-P 13, 15 phrases neg 36 2
10 NEUTRAL-P 14, 16 phrases neu 375 27
Table 1: Sense inventory of ?hard?.
ing ?difficult? (?hard life?, ?hard memory?) cannot
be easily distinguished based on an ngram repre-
sentation. Moreover, although ngram approaches
could learn the polarity of these phrases they do
not generalize to new phrases.
More recent compositional approaches to senti-
ment analysis can outperform lexicon and ngram-
based methods (e.g., Socher et al. (2011), Socher
et al. (2013)). However, these approaches conflate
two different types of contextual effects: differ-
ences in sense or lexical meaning (?hard memory?
vs. ?hard wood?) on the one hand and meaning
composition like negation on the other hand. From
the point of view of linguistic theory, these are dif-
ferent types of contextual effects that should not
be conflated. Recognizing that ?hard? occurs in
the scope of negation is of no use if the basic po-
larity of the contextually evoked sense of ?hard?
(e.g., negative in ?no hard memories? vs. neutral
in ?no hard wood?) is not recognized.
Wilson et al. (2009) present an approach to clas-
sify contextual polarity building on a two-step pro-
cess. First, they classify if a sentiment word is po-
lar in a phrase and if so, second, they classify its
polarity. Our approach can be seen as an exten-
sion of this approach; the main difference is that
we will show in our analysis of ?hard? that the
polarity of phrases depends on the senses of the
words that are used. This is evidence that high-
accuracy polarity classification depends on sense
disambiguation.
There has been previous work on assigning po-
larity values to senses of words taken from Word-
Net (e.g., Baccianella et al. (2010), Wiebe and Mi-
halcea (2006)). However, these approaches are not
able to disambiguate the sense of a word given its
context.
Previous work on representation learning for
sentiment analysis includes (Maas and Ng, 2010)
and (Maas et al., 2011). Their models learn word
embeddings that capture semantic similarities and
word sentiment at the same time. Their approach
focuses on sentiment of entire sentences or docu-
ments and does not consider each sentiment word
instance at a local level.
We present experiments with one supervised
and one semisupervised approach to word sense
disambiguation (WSD) in this paper. Other
WSD approaches, e.g., thesaurus-based WSD
(Yarowsky, 1992), could also be used for CESL.
3 Linguistic analysis of sentiment
contexts of ?hard?
We took a random sample of 5000 contexts of
?hard? in the Amazon Product Review Data (Jin-
dal and Liu, 2008). We use 200 as a test set and set
aside 200 for future use. We analyzed the remain-
ing 4600 contexts using a tool we designed for this
study, which provides functionality for selecting
and sorting contexts, including a keyword in con-
text display. If a reliable pattern has been identi-
fied (e.g., the phrase ?die hard?), then all contexts
matching the pattern can be labeled automatically.
Our goal is to identify the different uses of
?hard? that are relevant for sentiment. The basis
for our inventory is the Cobuild (Sinclair, 1987)
1211
lexicon entry for ?hard?. We use Cobuild because
it was compiled based on an empirical analysis of
corpus data and is therefore more likely to satisfy
the requirements of NLP applications than a tradi-
tional dictionary.
Cobuild lists 16 senses. One of these senses
(3) is split into two to distinguish the adverbial
(?to accelerate hard?) and adjectival (?hard accel-
eration?) uses of ?hard? in the meaning ?intense?.
We conflated five senses (2, 4, 9, 10, 11) refer-
ring to different types of difficulty: ?hard ques-
tion? (2), ?hard work? (4), ?hard life? (11) and
two variants of ?hard on?: ?hard on someone?
(9), ?hard on something? (10); and four differ-
ent senses (3a, 5, 6, 7) referring to different types
of intensity: ?to work hard? (3a), ?to look hard?
(5), ?to kick hard? (6), ?to laugh hard? (7). Fur-
thermore, we identified a number of noncompo-
sitional meanings or phrases (lists NEGATIVE-P
and NEUTRAL-P in the supplementary material
1
)
in addition to the four listed by Cobuild (13, 14,
15, 16). In addition, new senses for ?hard? are in-
troduced for opposites of senses of ?soft?: the op-
posite of ?quiet/gentle voice/sound? (7: MUSIC;
e.g., ?hard beat?, ?not too hard of a song?) and
the opposite of ?smooth surface/texture? (8: CON-
TRAST; e.g., ?hard line?, ?hard edge?).
Table 1 lists the 10 different uses that are the re-
sult of our analysis. For each use, we give the cor-
responding Cobuild sense numbers, syntactic in-
formation, meaning, an example, typical patterns,
polarity, and number of occurrences in training
and test sets.
7 uses are neutral and 3 are negative. As
?hard?s? polarity in most sentiment lexicons is
negative, but only 3 out of 7 senses are negative,
?hard? provides evidence for our hypothesis that
senses need to be disambiguated to allow for fine-
grained and accurate polarity recognition.
We hired two PhD students to label each of the
200 contexts in the test set with one of the 10 la-
bels in Table 1 (? = .78). Disagreement was re-
solved by a third person.
We have published the labeled data set of
4600+200 contexts as supplementary material.
4 Deep learning features
We use two types of deep learning features to be
able to make the fine-grained distinctions neces-
1
All supplementary material is available at http://
www.cis.lmu.de/ebert .
sary for sense disambiguation. First, we use word
embeddings similar to other recent work (see be-
low). Second, we use a deep learning language
model (LM) to predict the distribution of words for
the position at which the word of interest occurs.
For example, an LM will predict that words like
?granite? and ?concrete? are likely in the context
?a * countertop? and that words like ?serious? and
?difficult? are likely in the context ?a * problem?.
This is then the basis for distinguishing contexts
in which ?hard? is neutral (in the meaning ?firm,
solid?) from contexts in which it is a sentiment in-
dicator (in the meaning ?difficult?). We will use
the term predicted context distribution or PCD to
refer to the distribution predicted by the LM.
We use the vectorized log-bilinear language
model (vLBL) (Mnih and Kavukcuoglu, 2013)
because it has three appealing features. (i) It
learns state of the art word embeddings (Mnih and
Kavukcuoglu, 2013). (ii) The model is a language
model and can be used to calculate PCDs. (iii) As
a linear model, vLBL can be trained much faster
than other models (e.g., Bengio et al. (2003)).
The vLBL trains one set of word embeddings
for the input space (R) and one for the target space
(Q). We denote the input representation of word
w as r
w
and the target representation as q
w
. For a
given context c = w
1
, . . . , w
n
the model predicts
a target representation q? by linearly combining the
context word representations with position depen-
dent weights:
q?(c) =
n
?
i=1
d
i
 r
w
i
where d
i
? D is the weight vector associated
with position i in the context and  is point-
wise multiplication. Given the model parameters
? = {R,Q,D, b} the similarity between q? and the
correct target word embedding is computed by the
similarity function
s
?
(w, c) = q?(c)
T
q
w
+ b
w
where b
w
is a bias term.
We train the model with stochastic gradient
descent on mini-batches of size 100, following
the noise-contrastive estimation training proce-
dure of Mnih and Kavukcuoglu (2013). We use
AdaGrad (Duchi et al., 2011) with the initial learn-
ing rate set to ? = 0.5. The embeddings size is set
to 100.
1212
n
g
r
a
m
P
C
D
e
m
b
e
d
acc prec rec F
1
d
e
v
e
l
o
p
m
e
n
t
bl 1 .62 .62 1.00 .76
f
u
l
l
y
2 + .90 .91 .94 .92
3 + .90 .91 .92 .92
4 + .87 .87 .92 .90
5 + + .92 .92 .94 .93
6 + + .91 .90 .95 .92
7 + + .86 .83 .96 .89
8 + + + .92 .93 .95 .94
s
e
m
i
9 + .85 .87 .89 .88
10 + .85 .87 .89 .88
11 + .76 .73 .98 .83
12 + + .85 .87 .89 .88
13 + + .85 .87 .89 .88
14 + + .85 .89 .87 .88
15 + + + .86 .87 .90 .89
t
e
s
t
bl 16 .66 .66 1.00 .80
fully 17 + + + .90 .89 .96 .92
semi 18 + + + .85 .85 .91 .88
Table 2: Classification results; bl: baseline
During training we do not need to normalize the
similarity explicitly, because the normalization is
implicitly learned by the model. However, nor-
malization is still necessary for prediction. The
normalized PCD for a context c of word w is com-
puted using the softmax function:
P
c
?
(w) =
exp(s
?
(w, c))
?
w
?
exp(s
?
(w
?
, c))
We use a window size ofws = 7 for training the
model. We found that the model did not capture
enough contextual phenomena forws = 3 and that
results for ws = 11 did not have better quality
than ws = 7, but had a negative impact on the
training time. Using a vocabulary of the 100,000
most frequent words, we train the vLBL model for
4 epochs on 1.3 billion 7-grams randomly selected
from the English Wikipedia.
5 Experiments
The lexicon entry of ?hard? in CESL consists of (i)
the senses, (ii) the polarity annotations (neutral or
negative) and (iii) the sense disambiguation data
structure. Components (i) and (ii) are shown in
Table 1. In this section, we evaluate two different
options for (iii) on the task of sentiment classifica-
tion.
1 2 3 4 5 6 7 8
1
2 ?
3 ?
4 ? ? ?
5 ? ?
6 ? ?
7 ? ? * ? ?
8 ? * * ? * ?
Table 3: Significant differences of lines 1?8 in Ta-
ble 2; ?: p=0.01, *: p=0.05, ?: p=0.1
The first approach is to use a statistical classi-
fication model as the sense disambiguation struc-
ture. We use liblinear (Fan et al., 2008) with stan-
dard parameters for classification based on three
different feature types: ngrams, embeddings (em-
bed) and PCDs. Ngram features are all n-grams
for n ? {1, 2, 3}. As embedding features we
use (i) the mean of the input space (R) embed-
dings and (ii) the mean of the target space (Q) em-
beddings of the words in the context (see Blacoe
and Lapata (2012) for justification of using simple
mean). As PCD features we use the PCD predicted
by vLBL for the sentiment word of interest, in our
case ?hard?.
We split the set of 4600 contexts introduced in
Section 3 into a training set of 4000 and a devel-
opment set of 600.
Table 2 (lines 1?8) shows the classification re-
sults on the development set for all feature type
combinations. Significant differences between re-
sults ? computed using the approximate random-
ization test (Pad?o, 2006) ? are given in Table 3.
The majority baseline (bl), which assigns a nega-
tive label to all examples, reaches F
1
= .76. The
classifier is significantly better than the baseline
for all feature combinations with F
1
ranging from
.89 to .94. We obtain the best classification result
(.94) when all three feature types are combined
(significantly better than all other feature combi-
nations except for 5).
Manually labeling all occurrences of a word
is expensive. As an alternative we investigate
clustering of the contexts of the word of interest.
Therefore, we represent each of the 4000 con-
texts of ?hard? in the training set as its PCD
2
, use
2
To transform vectors into a format that is more appropri-
ate for the underlying Gaussian model of kmeans, we take the
square root of each probability in the PCD vectors.
1213
kmeans clustering with k = 100 and then label
each cluster. This decreases the cost of labeling
by an order of magnitude since only 100 clusters
have to be labeled instead of 4000 training set con-
texts.
Table 2 (lines 9?15) shows results for this
semisupervised approach to classification, using
the same classifier and the same feature types, but
the cluster-based labels instead of manual labels.
For most feature combinations, F
1
drops com-
pared to fully supervised classification. The best
performing model for supervised classification
(ngram+PCD+embed) loses 5%.
This is not a large drop considering the savings
in manual labeling effort. All results are signifi-
cantly better than the baseline. There are no signif-
icant differences between the different feature sets
(lines 9?15) with the exception of embed, which
is significantly worse than the other 6 sets.
The centroids of the 100 clusters can serve as an
alternative sense disambiguation structure for the
lexicon entry of ?hard? in CESL.
3
Each sense s is
associated with the centroids of the clusters whose
majority sense is s.
As final experiment (lines 16?18 in Table 2),
we evaluate performance for the baseline and for
PCD+ngram+embed ? the best feature set ? on the
test set. On the test set, baseline performance is
.80 (.04 higher than .76 on line 1, Table 2); F
1
of
PCD+ngram+embed is .92 (.02 less than develop-
ment set) for supervised classification and is .88
(.01 less) for semisupervised classification (com-
paring to lines 8 and 15 in Table 2). Both results
(.92 and .88) are significantly higher than the base-
line (.80).
6 Conclusion
The sentiment of a sentence or document is the
output of a causal chain that involves complex lin-
guistic processes like contextual modification and
negation. Our hypothesis in this paper was that
for high-accuracy sentiment analysis, we need to
model the root causes of this causal chain: the
meanings of individual words. This is in contrast
to other work in sentiment analysis that conflates
different linguistic phenomena (word sense ambi-
guity, contextual effects, negation) and attempts to
address all of them with a single model.
For sense disambiguation, the first step in the
causal chain of generating sentiment, we proposed
3
Included in supplementary material.
CESL, a contextually enhanced sentiment lexi-
con that for each word w holds the inventory of
senses of w, polarity annotations of these senses
and a data structure for assigning contexts of w
to the senses. We introduced new features for
sentiment analysis to be able to perform the fine-
grained modeling of context needed for CESL. In
a case study for the word ?hard?, we showed that
high accuracy in sentiment disambiguation can be
achieved using our approach. In future work, we
would like to show that our findings generalize
from the case of ?hard? to the entire sentiment lex-
icon.
Acknowledgments
This work was supported by DFG (grant SCHU
2246/10). We thank Lucia Krisnawati and Sascha
Rothe for their help with annotation.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In International Conference on Language Resources
and Evaluation, pages 2200?2204.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 546?
556. Association for Computational Linguistics.
John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In International Conference on Web
Search and Web Data Mining, pages 219?230.
Andrew L. Maas and Andrew Y. Ng. 2010. A proba-
bilistic model for semantic word vectors. In Annual
Conference on Advances in Neural Information Pro-
cessing Systems: Deep Learning and Unsupervised
Feature Learning Workshop.
1214
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Annual Meeting of the Association for Computa-
tional Linguistics, pages 142?150.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Annual Conference on Advances
in Neural Information Processing Systems, pages
2265?2273.
Sebastian Pad?o, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 79?86.
Ellen Riloff, Janyce Wiebe, and Theresa Ann Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In Conference on Natural
Language Learning, volume 4, pages 25?32.
John Sinclair. 1987. Looking Up: Account of the
Cobuild Project in Lexical Computing. Collins
CoBUILD.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 151?161.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 417?
424.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In International Conference on Informa-
tion and Knowledge Management, pages 625?631.
ACM.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1065?
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
David Yarowsky. 1992. Word-sense disambiguation
using statistical models of Roget?s categories trained
on large corpora. In International Conference on
Computational Linguistics, pages 454?460.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Conference on Empirical Methods in
Natural Language Processing, pages 129?136.
1215
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447?1452,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Mined Coreference Chains as a Resource for a Semantic Task
Heike Adel and Hinrich Sch?utze
Center for Information and Language Processing
University of Munich
Germany
heike.adel@cis.lmu.de
Abstract
We propose to use coreference chains ex-
tracted from a large corpus as a resource
for semantic tasks. We extract three mil-
lion coreference chains and train word
embeddings on them. Then, we com-
pare these embeddings to word vectors de-
rived from raw text data and show that
coreference-based word embeddings im-
prove F
1
on the task of antonym classifi-
cation by up to .09.
1 Introduction
After more than a decade of work on coreference
resolution, coreference resolution systems have
reached a certain level of maturity (e.g., Recasens
et al. (2010)). While accuracy is far from perfect
and many phenomena such as bridging still pose
difficult research problems, the quality of the out-
put of these systems is high enough to be useful
for many applications.
In this paper, we propose to run coreference res-
olution systems on large corpora, to collect the
coreference chains found and to use them as a re-
source for solving semantic tasks. This amounts
to using mined coreference chains as an automat-
ically compiled resource similar to the way cooc-
currence statistics, dependency pairs and aligned
parallel corpora are used in many applications in
NLP. Coreference chains have interesting comple-
mentary properties compared to these other re-
sources. For example, it is difficult to distinguish
true semantic similarity (e.g., ?cows? ? ?cattle?)
from mere associational relatedness (e.g., ?cows?
? ?milk?) based on cooccurrence statistics. In con-
trast, coreference chains should be able to make
that distinction since only ?cows? and ?cattle? can
occur in the same coreference chain, not ?cows?
and ?milk?.
As a proof of concept we compile a resource
of mined coreference chains from the Gigaword
corpus and apply it to the task of identifying
antonyms. We induce distributed representations
for words based on (i) cooccurrence statistics and
(ii) mined coreference chains and show that a com-
bination of both outperforms cooccurrence statis-
tics on antonym identification.
In summary, we make two contributions. First,
we propose to use coreference chains mined from
large corpora as a resource in NLP and publish the
first such resource. Second, in a proof of concept
study, we show that they can be used to solve a se-
mantic task ? antonym identification ? better than
is possible with existing resources.
We focus on the task of finding antonyms in this
paper since antonyms usually are distributionally
similar but semantically dissimilar words. Hence,
it is often not possible to distinguish them from
synonyms with distributional models only. In con-
trast, we expect that the coreference-based repre-
sentations can provide useful complementary in-
formation to this task. In general, coreference-
based similarity can however be used as an addi-
tional feature for any task that distributional simi-
larity is useful for. Thus, our coreference resource
can be applied to a variety of NLP tasks, e.g. find-
ing alternative names for entities (in a way similar
to Wikipedia anchors) for tasks in the context of
knowledge base population.
The remainder of the paper is organized as fol-
lows. In Section 2, we describe how we create
word embeddings and how our antonym classi-
fier works. The word embeddings are then eval-
uated qualitatively, quantitatively and for the task
of antonym detection (Section 3). Section 4 dis-
cusses related work and Section 5 concludes.
2 System description
2.1 Coreference-based embeddings
Standard word embeddings derived from text data
may not be able to distinguish between semantic
1447
text-based coref.-based
his my, their, her, your, our he, him, himself, zechariah, ancestor
woman man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia
Table 1: Nearest neighbors of ?his? / ?woman? for text-based & coreference-based embeddings
association and true synonymy. As a result, syn-
onyms and antonyms may be mapped to similar
word vectors (Yih et al., 2012). For many NLP
tasks, however, information about true synonymy
or antonymy may be important.
In this paper, we develop two different word
embeddings: embeddings calculated on raw text
data and embeddings derived from automatically
extracted coreference chains. For the calcula-
tion of the vector representations, the word2vec
toolkit
1
by Mikolov et al. (2013) is applied. We
use the skip-gram model for our experiments be-
cause its results for semantic similarity are better
according to Mikolov et al. (2013). We train a
first model on a subset of English Gigaword data.
2
In the following sections, we call the resulting
embeddings text-based. To improve the seman-
tic similarities of the vectors, we prepare another
training text consisting of coreference chains. We
use CoreNLP (Lee et al., 2011) to extract coref-
erence chains from the Gigaword corpus. Then
we build a skip-gram model on these coreference
chains. The extracted coreference chains are pro-
vided as an additional resource to this paper
3
. Al-
though they have been developed using only a
publicly available toolkit, we expect this resource
to be helpful for other researchers since the pro-
cess to extract the coreference chains of such a
large text corpus takes several weeks on multi-core
machines. In total, we extracted 3.1M coreference
chains. 2.7M of them consist of at least two differ-
ent markables. The median (mean) length of the
chains is 3 (4.0) and the median (mean) length of
a markable is 1 (2.7). To train word embeddings,
the markables of each coreference chain are con-
catenated to one text line. These lines are used as
input sentences for word2vec. We refer to the re-
sulting embeddings as coreference-based.
2.2 Antonym detection
In the following experiments, we use word em-
beddings to discriminate antonyms from non-
antonyms. We formalize this as a supervised clas-
1
https://code.google.com/p/word2vec
2
LDC2012T21, Agence France-Presse 2010
3
https://code.google.com/p/cistern
sification task and apply SVMs (Chang and Lin,
2011).
The following features are used to represent a
pair of two words w and v:
1. cosine similarity of the text-based embed-
dings of w and v;
2. inverse rank of v in the nearest text-based
neighbors of w;
3. cosine similarity of the coreference-based
embeddings of w and v;
4. inverse rank of v in the nearest coreference-
based neighbors of w;
5. difference of (1) and (3);
6. difference of (2) and (4).
We experiment with three different subsets of
these features: text-based (1 and 2), coreference-
based (3 and 4) and all features.
3 Experiments and results
3.1 Qualitative analysis of word vectors
Table 1 lists the five nearest neighbors based on
cosine similarity of text-based and coreference-
based word vectors for ?his? and ?woman?.
We see that the two types of embeddings cap-
ture different notions of similarity. Unlike the text-
based neighbors, the coreference-based neighbors
have the same gender. The text-based neighbors
are mutually substitutable words, but substitution
seems to change the meaning more than for the
coreference-based neighbors.
In Figure 1, we illustrate the vectors for some
antonyms (connected by lines).
For reducing the dimensionality of the vector
space to 2D, we applied the t-SNE toolkit
4
. It uses
stochastic neighbor embedding with a Student?s
t-distribution to map high dimensional vectors
into a lower dimensional space (Van der Maaten
and Hinton, 2008). The Figure shows that the
coreference-based word embeddings are able to
4
http://homepage.tudelft.nl/19j49/t-SNE.html
1448
1.5 1.0 0.5 0.0 0.5 1.01.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
willingness
innocence
guiltliteracy
unwillingness
toughness
illiteracy
frailty
1.5 1.0 0.5 0.0 0.5 1.01.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
willingness
innocence
guilt
unwillingness
frailty
toughness literacy
illiteracy
Figure 1: 2D-positions of words in the text-based (top) and coreference-based embeddings (bottom)
enlarge the distance between antonyms (especially
for guilt vs. innocence and toughness vs. frailty)
compared to text-based word vectors.
3.2 Quantitative analysis of word vectors
To verify that coreference-based embeddings bet-
ter represent semantic components relevant to
coreference, we split our coreference resource into
two parts (about 85% and 15% of the data), trained
embeddings on the first part and computed the co-
sine similarity ? both text-based and coreference-
based ? for each pair of words occurring in the
same coreference chain in the second part. The
statistics in Table 2 confirm that coreference-based
vectors have higher similarity within chains than
text-based vectors.
3.3 Experimental setup
We formalize antonym detection as a binary classi-
fication task. Given a target word w and one of its
nearest neighbors v, the classifier decides whether
v is an antonym of w. Our data set is a set of pairs,
each consisting of a target word w and a candi-
date v. For all word types of our vocabulary, we
search for antonyms using the online dictionary
Merriam Webster.
5
The resulting list is provided
as an additional resource
6
. It contains 6225 words
with antonyms. Positive training examples are col-
lected by checking if the 500 nearest text-based
neighbors of w contain one of the antonyms listed
by Webster. Negative training examples are cre-
ated by replacing the antonym with a random word
from the 500 nearest neighbors that is not listed as
5
http://www.merriam-webster.com
6
https://code.google.com/p/cistern
an antonym. By selecting both the positive and
the negative examples from the nearest neighbors
of the word vectors, we intend to develop a task
which is hard to solve: The classifier has to find
the small portion of semantically dissimilar words
(i.e., antonyms) among distributionally very simi-
lar words. The total number of positive and nega-
tive examples is 2337 each. The data are split into
training (80%), development (10%) and test (10%)
sets.
In initial experiments, we found only a small
difference in antonym classification performance
between text-based and coreference-based fea-
tures. When analyzing the errors, we realized that
our rationale for using coreference-based embed-
dings only applies to nouns, not to other parts of
speech. This will be discussed in detail below. We
therefore run our experiments in two modes: all
word classification (all pairs are considered) and
noun classification (only pairs are considered for
which the target word is a noun). We use the Stan-
ford part-of-speech tagger (Toutanova et al., 2003)
to determine whether a word is a noun or not.
Our classifier is a radial basis function (rbf) sup-
port vector machine (SVM). The rbf kernel per-
formed better than a linear kernel in initial exper-
iments. The SVM parameters C and ? are opti-
mized on the development set. The representation
of target-candidate pairs consists of the features
described in Section 2.
3.4 Experimental results and discussion
We perform the experiments with the three differ-
ent feature sets described in Section 2: text-based,
coreference-based and all features. Table 3 shows
1449
all word classification noun classification
development set test set development set test set
feature set P R F
1
P R F
1
P R F
1
P R F
1
text-based .83 .66 .74 .74 .55 .63 .91 .61 .73 .74 .51 .60
coreference-based .67 .42 .51 .65 .43 .52 .86 .47 .61 .77 .45 .57
text+coref .79 .65 .72 .75 .58 .66 .88 .70 .78 .79 .61 .69
Table 3: Results for different feature sets. Best result in each column in bold.
minimum maximum median
text-based vectors -0.350 0.998 0.156
coref.-based vectors -0.318 0.999 0.161
Table 2: Cosine similarity of words in the same
coreference chain
results for development and test sets.
For all word classification, coreference-based
features do not improve performance on the de-
velopment set (e.g., F
1
is .74 for text-based vs .72
for text+coref). On the test set, however, the com-
bination of all features (text+coref) has better per-
formance than text-based alone: .66 vs .63.
For noun classification, using coreference-
based features in addition to text-based features
improves results on development set (F
1
is .78 vs
.73) and test set (.69 vs .60).
These results show that mined coreference
chains are a useful resource and provide infor-
mation that is complementary to other methods.
Even though adding coreference-based embed-
dings improves performance on antonym classi-
fication, the experiments also show that using
only coreference-based embeddings is almost al-
ways worse than using only text-based embed-
dings. This is not surprising given that the amount
of training data for the word embeddings is differ-
ent in the two cases. Coreference chains provide
only a small subset of the word-word relations that
are given to the word2vec skip-gram model when
applied to raw text. If the sizes of the training data
sets were similar in the two cases, we would ex-
pect performance to be comparable.
In the beginning, our hypothesis was that coref-
erence information should be helpful for antonym
classification in general. When we performed an
error analysis for our initial results, we realized
that this hypothesis only holds for nouns. Other
types of words cooccurring in coreference chains
are not more likely to be synonyms than words
cooccurring in text windows. Two contexts that
illustrate this point are ?bright sides, but also dif-
ficult and dark ones? and ?a series of black and
white shots? (elements of coreference chains in
italics). Thus, adjectives with opposite meanings
can cooccur in coreference chains just as they can
cooccur in window-based contexts. For nouns, it
is much less likely that the same coreference chain
will contain both a noun and its antonym since ?
by definition ? markables in a coreference chain
refer to the same identical entity.
4 Related work
Traditionally, words have been represented by
vectors of the size of the vocabulary with a one at
the word index and zeros otherwise (one-hot vec-
tors). However, this approach cannot handle un-
known words (Turian et al., 2010) and similari-
ties among words cannot be represented (Mikolov
et al., 2013). Therefore, distributed word repre-
sentations (embeddings) become more and more
popular. They are low-dimensional, real-valued
vectors. Mikolov et al. (2013) have published
word2vec, a toolkit that provides different possi-
bilities to estimate word embeddings (cbow model
and skip-gram model). They show that the re-
sulting word vectors capture semantic and syntac-
tic relationships of words. Baroni et al. (2014)
show that word embeddings are able to outper-
form count based word vectors on a variety of
NLP tasks. Recently, Levy and Goldberg (2014)
have generalized the skip-gram model to include
not only linear but arbitrary contexts like contexts
derived from dependency parse trees. Andreas and
Klein (2014) investigate the amount of additional
information continuous word embeddings could
add to a constituency parser and find that most
of their information is redundant to what can be
learned from labeled parse trees. In (Yih et al.,
2012), the vector space representation of words is
modified so that high positive similarities are as-
signed to synonyms and high negative similarities
to antonyms. For this, latent semantic analysis is
applied to a matrix of thesaurus entries. The val-
1450
ues representing antonyms are negated.
There has been a great deal of work on apply-
ing the vector space model and cosine similarity
to find synonyms or antonyms. Hagiwara et al.
(2006) represent each word as a vector with cooc-
currence frequencies of words and contexts as el-
ements, normalized by the inverse document fre-
quency. The authors investigate three types of con-
textual information (dependency, sentence cooc-
currence and proximity) and find that a combi-
nation of them leads to the most stable results.
Schulte im Walde and K?oper (2013) build a vector
space model on lexico-syntactic patterns and ap-
ply a Rocchio classifier to distinguish synonyms
from antonyms, among other tasks. Van der Plas
and Tiedemann (2006) use automatically aligned
translations of the same text in different languages
to build context vectors. Based on these vectors,
they detect synonyms.
In contrast, there are also studies using linguis-
tic knowledge from external resources: Senellart
and Blondel (2008) propose a method for syn-
onym detection based on graph similarity in a
graph generated using the definitions of a mono-
lingual dictionary. Harabagiu et al. (2006) rec-
ognize antonymy by generating antonymy chains
based on WordNet relations. Mohammad et al.
(2008) look for the word with the highest degree of
antonymy to a given target word among five candi-
dates. For this task, they use thesaurus information
and the similarity of the contexts of two contrast-
ing words. Lin et al. (2003) use Hearst patterns
to distiguish synonyms from antonyms. Work by
Turney (2008) is similar except that the patterns
are learned.
Except for the publicly available coreference
resolution system, our approach does not need ex-
ternal resources such as dictionaries or bilingual
corpora and no human labor is required. Thus,
it can be easily applied to any corpus in any lan-
guage as long as there exists a coreference resolu-
tion system in this language. The pattern-based
approach (Lin et al., 2003; Turney, 2008) dis-
cussed above also needs few resources. In contrast
to our work, it relies on patterns and might there-
fore restrict the number of recognizable synonyms
and antonyms to those appearing in the context of
the pre-defined patterns. On the other hand, pat-
terns could explicitely distinguish contexts typical
for synonyms from contexts for antonyms. Hence,
we plan to combine our coreference-based method
with pattern-based methods in the future.
5 Conclusion
In this paper, we showed that mined corefer-
ence chains can be used for creating word em-
beddings that capture a type of semantic sim-
ilarity that is different from the one captured
by standard text-based embeddings. We showed
that coreference-based embeddings improve per-
formance of antonym classification by .09 F
1
compared to using only text-based embeddings.
We achieved precision values of up to .79, recall
values of up to .61 and F
1
scores of up to .69.
Acknowledgments
This work was supported by DFG (grant SCHU
2246/4-2).
References
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In ACL,
pages 822?827.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238?247.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2006. Selection of effective contextual
information for automatic synonym acquisition. In
COLING/ACL, pages 353?360.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In AAAI, volume 6, pages 755?762.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In CoNLL: Shared Task, pages 28?34.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL, pages 302?308.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In IJCAI, pages 1492?1493.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Workshop at ICLR.
1451
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982?991.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In 5th International Workshop
on Semantic Evaluation, pages 1?8.
Sabine Schulte im Walde and Maximilian K?oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web, pages 184?
198. Springer.
Pierre Senellart and Vincent D Blondel. 2008. Auto-
matic discovery of similar words. In Survey of Text
Mining II, pages 25?44. Springer.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT, pages 252?259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL, pages 384?
394.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
COLING, pages 905?912.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(11):2579?2605.
Lonneke Van der Plas and J?org Tiedemann. 2006.
Finding synonyms using automatic word alignment
and measures of distributional similarity. In COL-
ING/ACL, pages 866?873.
Wen-tau Yih, Geoffrey Zweig, and John C Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP/CoNLL, pages 1212?1222.
1452
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 200?204,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multi-Domain Sentiment Relevance Classification with Automatic
Representation Learning
Christian Scheible
Institut f?ur Maschinelle Sprachverarbeitung
University of Stuttgart
scheibcn@ims.uni-stuttgart.de
Hinrich Sch?utze
Center for Information
and Language Processing
University of Munich
Abstract
Sentiment relevance (SR) aims at identify-
ing content that does not contribute to sen-
timent analysis. Previously, automatic SR
classification has been studied in a limited
scope, using a single domain and feature
augmentation techniques that require large
hand-crafted databases. In this paper, we
present experiments on SR classification
with automatically learned feature repre-
sentations on multiple domains. We show
that a combination of transfer learning and
in-task supervision using features learned
unsupervisedly by the stacked denoising
autoencoder significantly outperforms a
bag-of-words baseline for in-domain and
cross-domain classification.
1 Introduction
Many approaches to sentiment analysis rely on
term-based clues to detect the polarity of sentences
or documents, using the bag-of-words (BoW)
model (Wang and Manning, 2012). One drawback
of this approach is that the polarity of a clue is
often treated as fixed, which can be problematic
when content is not intended to contribute to the
polarity of the entity but contains a term with a
known lexical non-neutral polarity.
For example, movie reviews often have plot
summaries which contain subjective descriptions,
e.g., ?April loves her new home and friends.?, con-
taining ?loves?, commonly a subjective positive
term. Other domains contain different types of
nonrelevant content: Music reviews may contain
track listings, product reviews on retail platforms
contain complaints that do not concern the prod-
uct, e.g., about shipping and handling. Filtering
such nonrelevant content can help to improve sen-
timent analysis (Pang and Lee, 2004). Sentiment
relevance (Scheible and Sch?utze, 2013; Taboada
et al., 2009; T?ackstr?om and McDonald, 2011) for-
malizes this distinction: Content that contributes
to the overall sentiment of a document is said to
be sentiment relevant (SR), other content is senti-
ment nonrelevant (SNR).
The main bottleneck in automatic SR classifi-
cation is the lack of annotated data. On the sen-
tence level, it has been attempted for the movie
review domain (Scheible and Sch?utze, 2013) on
a manually annotated dataset that covers around
3,500 sentences. The sentiment analysis data by
T?ackstr?om and McDonald (2011) contains SR an-
notations for five product review domains, four of
which have fewer than 1,000 annotated examples.
As the amount of labeled data is low, we adopt
transfer learning (TL, (Thrun, 1995)), which has
been used before for SR classification. In this
setup, we train a classifier on a different task, using
subjectivity-labeled data ? for which a large num-
ber of annotated examples is available ? and ap-
ply it for SR classification. To enable knowledge
transfer between the tasks, feature space augmen-
tation has been proposed. For this purpose, we em-
ploy automatic representation learning, using the
stacked denoising autoencoder (SDA, (Vincent et
al., 2010)) which has been applied successfully to
other domain adaptation problems such as cross-
domain sentiment analysis (Glorot et al., 2011).
In this paper, we present experiments on both
multi-domain and cross-domain SR classification.
We show that compared to the in-domain base-
line, TL with SDA features increases F
1
by 6.8%
on average. We find that domain adaptation using
TL with the SDA compensates for strong domain
shifts, reducing the average classification transfer
loss by 12.7%.
2 Stacked Denoising Autoencoders
The stacked denoising autoencoder (SDA, (Vin-
cent et al., 2010)) is a neural network (NN) model
for unsupervised feature representation learning.
200
An autoencoder takes an input vector x, uses an
NN layer with a (possibly) nonlinear activation
function to generate a hidden feature representa-
tion h. A second NN layer reconstructs x at the
output, minimizing the error.
Denoising autoencoders reconstruct x from a
corrupted version of the input, x?. As the model
learns to be robust to noise, the representations are
expected to generalize better. For discrete data,
masking noise is a natural choice, where each in-
put unit is randomly set to 0 with probability p.
Autoencoders can be stacked by using the h
i
produced by the i
th
autoencoder as the input to
the (i+1)
th
one, yielding the representation h
i+1
.
The h of the topmost autoencoder is the final rep-
resentation output by the SDA. We let k-SDA de-
note a stack of k denoising autoencoders.
Chen et al. (2012) introduced a marginalized
closed-form version, the mSDA. We opt for this
version as it is faster to train and allows us to use
the full feature space which would be inefficient
with iterative backpropagation training.
3 Task and Experimental Setup
The task in this paper is multi- and cross-domain
SR classification. Two aspects motivate our work:
First, we need to address the sparse data situa-
tion. Second, we are interested in how cross-
domain effects influence SR classification. We
classify SR in three different setups: in-domain
(ID), in which we take the training and test data
from the same domain; domain adaptation (DA),
where training and test data are from different do-
mains; and transfer learning (TL), where we use a
much larger amount of data from a different but re-
lated task. To improve the generalization capabili-
ties of the models, we use representations learned
by the SDA. We will next describe our classifica-
tion setup in more detail.
Data We use the following datasets for our ex-
periments. Table 1 shows statistics on the datasets.
CINEMA: The movie SR data (CINEMA)
by Scheible and Sch?utze (2013) contains SR-
annotated sentences for the movie review domain.
Ambiguous sentences are marked as unknown; we
exclude them.
PRODUCTS: The multi-domain product data
(PRODUCTS) by T?ackstr?om and McDonald (2011)
contains labeled sentences from five Amazon.com
product review domains: BOOKS, DVDS, electron-
ics (EL), MUSIC, and video games (VG). This
Dataset #doc #sent #SR #SNR
CINEMA 125 3,487 2,759 728
PRODUCTS 294 3,836 2,689 1,147
?BOOKS 59 739 424 315
?DVDS 59 799 524 275
?ELECTRONICS 57 628 491 137
?MUSIC 59 638 448 190
?VIDEOGAMES 60 1032 802 230
P&L ? 10,000 5,000 5,000
UNLAB 7,500 68,927 ? ?
Table 1: Dataset statistics
dataset differs from CINEMA firstly in the product
domains (except obviously for DVDS which also
covers movies). Secondly, the data was collected
from a retail site, which introduces further facets
of sentiment nonrelevance, as discussed above.
Thirdly, the annotation style has no unknown cat-
egory: ambiguous examples are marked as SR.
P&L: The subjectivity data (P&L) by Pang and
Lee (2004) serves as our cross-task training data
for transfer learning. The dataset was heuristically
created for subjectivity detection on the movie do-
main by sampling snippets from Rotten Tomatoes
as subjective and sentences from IMDb plot sum-
maries as objective examples.
UNLAB: To improve generalization on PROD-
UCTS, we use additional unlabeled sentences
(UNLAB) for SDA training. We extract the sen-
tences of 1,500 randomly selected documents for
each of the five domains from the Amazon.com
review data by Jindal and Liu (2008).
SDA setup We train the SDA with 10,000 hid-
den units and tanh nonlinearity on the BoW fea-
tures of all available data as the input. We opti-
mize the noise level p with 2-fold cross-validation
on the in-domain training folds.
Classification setup We perform SR classifica-
tion with a linear support vector machine (SVM)
using LIBLINEAR (Chang and Lin, 2011). We
perform 2-fold cross-validation for all training
data but P&L. We report overall macro-averaged
F
1
over both folds. The feature representation for
the SVM is either bag of words (BoW) or the k-
SDA output. Unlike Chen et al. (2012), we do not
use concatenations of BoW and SDA vectors as
we found them to perform worse.
Evaluation As class distributions are heavily
skewed, we use macro-averaged F
1
(s, t) (train-
ing on s and evaluating on t) as the basic eval-
uation measure. We evaluate DA with transfer
loss, the difference in F
1
of a classifier CL with
201
Features Setup CINEMA BOOKS DVDS EL MUSIC VG ?
1 Majority BL ? 39.6 28.9 32.6 39.2 35.1 39.0 35.7
2 BoW ID 74.0 57.5 49.8 55.1 55.5 55.0 58.4
3 1-SDA ID 73.6 55.3 48.4 43.8 41.8 44.1 52.6
4 2-SDA ID 76.0 54.5 52.5 43.9 41.2 46.7 53.6
5 BoW TL 71.5 60.7 60.2 50.3 55.1 53.2 59.6
6 1-SDA TL 73.3 62.9 60.6 59.0 59.9 57.0 63.1
7 2-SDA TL 76.2 62.9 65.8 59.7 59.9 60.5 64.9
8 BoW ID+TL 76.6 63.5 61.7 52.4 56.7 57.0 62.3
9 1-SDA ID+TL 79.0 62.7 62.1 57.7 57.8 57.4 63.9
10 2-SDA ID+TL 80.4 62.7 65.2 59.0 58.7 58.9 65.2
Table 2: Macro-averaged F
1
(%) evaluating on each test domain on both folds. ? = row mean. Bold:
best result in each column and results in that column not significantly different from it.
respect to the in-domain baseline BL: L(s, t) =
F
(BL)
1
(t, t)?F
(CL)
1
(s, t). L is negative if the clas-
sifier surpasses the baseline. As a statistical sig-
nificance test (indicated by
?
in the text), we use
approximate randomization (Noreen, 1989) with
10,000 iterations at p < 0.05.
4 Experiments
In-Domain Classification (ID) Table 2 shows
macro-averaged F
1
for different SR models. We
first turn to fully supervised SR classification with
bag-of-words (BoW) features using ID training
(line 2). While the results for CINEMA are high,
on par with the reported results in related work,
they are low for the PRODUCTS data. This is not
surprising as the SVM is trained with fewer than
600 examples on each domain. Also, no unknown
category exists in the latter dataset. While am-
biguous examples on CINEMA are annotated as un-
known, they receive an SR label on PRODUCTS.
Thus, many examples are ambiguous and thus dif-
ficult to classify. SDA features worsen results
significantly
?
(lines 3?4) on all domains except
CINEMA and DVDS due to data sparsity. They are
the two most homogeneous domains where plot
descriptions make up a large part of the SNR con-
tent. On many domains, there is no single proto-
typical type of SNR which could be learned from
a small amount of training data.
Transfer Learning (TL) TL with training on
P&L and evaluation on CINEMA/PRODUCTS with
BoW features (line 5) performs slightly worse than
ID classification, except on BOOKS and DVDS
where we see strong improvements. This result
is easy to explain: Both BOOKS and DVDS contain
SNR descriptions of narratives, which are covered
well in P&L. This distinction is less helpful on
domains like EL where SNR content is different,
so we achieve worse results even with the much
larger P&L data.
We find that 1-SDA (line 6) already performs
significantly
?
better than the ID baseline on all do-
mains except CINEMA which has a much larger
amount of ID training data available than the other
domains (approx. 1700 sentences vs. fewer than
600). Using stacking, 2-SDA (line 7) improves
the results on three domains significantly,
?
and
performs on par with the ID classifier on CIN-
EMA. We found that stack depths of k > 2 do
not significantly
?
increase performance.
Finally, we try a combination of ID and TL
(ID+TL), training on both P&L and the respective
ID training fold of CINEMA/PRODUCTS. The re-
sults for this experiment are shown in lines 8?10
in Table 2. Comparing BoW models, we beat both
ID and TL across all domains (lines 2 and 5). With
SDA features, we are able to beat ID for CINEMA.
The results on the other domains are comparable
to plain TL. This is a promising result, showing
that with SDA features, ID+TL performs as well
as or better than plain TL. This property could be
exploited for domains where labeled data is not
available. We will show below that SDA features
become important when we apply ID+TL to do-
main adaptation.
We also conducted experiments using only the
5,000 most frequent features but found that the
SDA does not generalize well from this input rep-
resentation, particularly on EL and MUSIC. This
confirms that in SR, rare features make an im-
portant contribution (such as named entities in the
movie domain).
Domain Adaptation (DA) We now evaluate the
task in a DA setting, comparing the ID and ID+TL
202
CC BC DC EC MC VC CB BB DB EB MB VB CD BD DD ED MD VD CE BE DE EE ME VE CM BM DM EM MM VM C
V BV DV EV MV VV
BoW ID 2?SDA ID BOW ID+TL 2?SDA ID+TL
Tra
ns
fer
 Lo
ss
 (%
)
?15
?10
?5
0
5
10
15
20
25
Figure 1: Transfer losses (%) for DA. Training-test pairs grouped by target domain and abbreviated by
first letter (e.g., CD: training on CINEMA, evaluating on DVDS). In-domain results shown for comparison
to Table 2.
setups with BoW and 2-SDA features. We mea-
sure the transfer losses we suffer from training on
one domain and evaluating on another (Figure 1).
The overall picture is the same as above: ID+TL
2-SDA models perform best. In the baseline BoW
ID setup, domain shifts have a strong influence on
the results. The combination of out-of-domain and
out-of-task data in ID+TL keeps losses uniformly
low. 2-SDA features lower almost all losses fur-
ther. On average, 2-SDA ID+TL reduces trans-
fer loss by 12.7 points compared to the baseline
(Table 3). As expected, pairings of thematically
strongly related domains (e.g., BOOKS and DVDS)
have lower losses in all setups.
The biggest challenge is the strong domain shift
between the CINEMA and PRODUCTS domains
(concerning mainly the retail aspects). With BoW
ID, losses on CINEMA reach up to 25 points, and
using CINEMA for training causes high losses for
PRODUCTS in most cases. Our key result is that
the ID+TL 2-SDA setup successfully compensates
for these problems, reducing the losses below 0.
Losses across the PRODUCTS domains are less
pronounced. The DVDS baseline classifier has the
lowest F
1
(cf. Table 2) and shows the highest im-
provements in domain adaptation: BoW models of
other domains perform better than the in-domain
classifier. Analyzing the DVDS model shows over-
fitting to specific movie terms which occur fre-
quently across each review in the training data.
SNR content in movies is mostly concerned with
named entity types which cannot easily be learned
from BoW representations. Out-of-domain mod-
els are less specialized and perform better than in-
BoW 2-SDA
ID 6.7 8.9
ID+TL -1.8 -6.0
Table 3: Mean transfer losses (%) for the different
training data and feature representation setups. In-
domain results not included.
domain models. TL and SDA increase the cover-
age of movie terms and provide better generaliza-
tion, which improves performance further.
BOOKS is the most challenging domain in all se-
tups. It is particularly heterogeneous, containing
both fiction and non-fiction reviews which feature
different SNR aspects. Both results illustrate that
domain effects depend on how diverse SNR con-
tent is within the domain.
Overall, the results show that ID+TL leads to a
successful compensation of cross-domain effects.
SDA features improve the results significantly
?
for
ID+TL. In particular, we find that the SDA suc-
cessfully compensates for the strong domain shift
between CINEMA and PRODUCTS.
5 Conclusion
We presented experiments on multi- and cross-
domain sentiment relevance classification. We
showed that transfer learning (TL) using stacked
denoising autoencoder (SDA) representations sig-
nificantly increases performance by 6.8% F
1
for
in-domain classification. Moreover, the average
transfer loss in domain adaptation is reduced by
12.7 percentage points where the SDA features
compensate for strong domain shifts.
203
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(ACM TIST), 2(3):1?27.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the
29th International Conference on Machine Learning
(ICML).
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML), pages 513?520.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the International
Conference on Web Search and Web Data Mining
(WSDM), pages 219?230.
Eric W. Noreen. 1989. Computer Intensive Methods
for Hypothesis Testing: An Introduction. Wiley.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL), pages 271?278.
Christian Scheible and Hinrich Sch?utze. 2013. Senti-
ment relevance. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 954?963.
Maite Taboada, Julian Brooke, and Manfred Stede.
2009. Genre-based paragraph classification for sen-
timent analysis. In Proceedings of the SIGDIAL
2009 Conference, pages 62?70.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 569?574.
Sebastian Thrun. 1995. Is learning the n-th thing any
easier than learning the first? In Proceedings of Ad-
vances in Neural Information Processing Systems 8
(NIPS), pages 640?646.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research (JMLR), 11:3371?3408.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 90?94.
204
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1392?1402,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
CoSimRank: A Flexible & Efficient Graph-Theoretic Similarity Measure
Sascha Rothe and Hinrich Sch?utze
Center for Information & Language Processing
University of Munich
sascha@cis.lmu.de
Abstract
We present CoSimRank, a graph-theoretic
similarity measure that is efficient because
it can compute a single node similarity
without having to compute the similarities
of the entire graph. We present equivalent
formalizations that show CoSimRank?s
close relationship to Personalized Page-
Rank and SimRank and also show how
we can take advantage of fast matrix mul-
tiplication algorithms to compute CoSim-
Rank. Another advantage of CoSimRank
is that it can be flexibly extended from ba-
sic node-node similarity to several other
graph-theoretic similarity measures. In an
experimental evaluation on the tasks of
synonym extraction and bilingual lexicon
extraction, CoSimRank is faster or more
accurate than previous approaches.
1 Introduction
Graph-theoretic algorithms have been successfully
applied to many problems in NLP (Mihalcea and
Radev, 2011). These algorithms are often based on
PageRank (Brin and Page, 1998) and other central-
ity measures (e.g., (Erkan and Radev, 2004)). An
alternative for tasks involving similarity is Sim-
Rank (Jeh and Widom, 2002). SimRank is based
on the simple intuition that nodes in a graph should
be considered as similar to the extent that their
neighbors are similar. Unfortunately, SimRank
has time complexity O(n
3
) (where n is the num-
ber of nodes in the graph) and therefore does not
scale to the large graphs that are typical of NLP.
This paper introduces CoSimRank,
1
a new
graph-theoretic algorithm for computing node
similarity that combines features of SimRank and
PageRank. Our key observation is that to compute
the similarity of two nodes, we need not consider
1
Code available at code.google.com/p/cistern
all other nodes in the graph as SimRank does; in-
stead, CoSimRank starts random walks from the
two nodes and computes their similarity at each
time step. This offers large savings in computa-
tion time if we only need the similarities of a small
subset of all n
2
node similarities.
These two cases ? computing a few similari-
ties and computing many similarities ? correspond
to two different representations we can compute
CoSimRank on: a vector representation, which is
fast for only a few similarities, and a matrix repre-
sentation, which can take advantage of fast matrix
multiplication algorithms.
CoSimRank can be used to compute many vari-
ations of basic node similarity ? including similar-
ity for graphs with weighted and typed edges and
similarity for sets of nodes. Thus, CoSimRank has
the added advantage of being a flexible tool for dif-
ferent types of applications.
The extension of CoSimRank to similarity
across graphs is important for the application of
bilingual lexicon extraction: given a set of corre-
spondences between nodes in two graphsA andB
(corresponding to two different languages), a pair
of nodes (a ? A, b ? B) is a good candidate for a
translation pair if their node similarity is high. In
an experimental evaluation, we show that CoSim-
Rank is more efficient and more accurate than both
SimRank and PageRank-based algorithms.
This paper is structured as follows. Section 2
discusses related work. Section 3 introduces
CoSimRank. In Section 4, we compare CoSim-
Rank and SimRank. By providing some useful
extensions, we demonstrate the great flexibility of
CoSimRank (Section 5). We perform an exper-
imental evaluation of CoSimRank in Section 6.
Section 7 summarizes the paper.
2 Related Work
Our work is unsupervised. We therefore do not
review graph-based methods that make extensive
1392
use of supervised learning (e.g., de Melo and
Weikum (2012)).
Since the original version of SimRank (Jeh and
Widom, 2002) has complexity O(n
4
), many ex-
tensions have been proposed to speed up its calcu-
lation. A Monte Carlo algorithm, which is scalable
to the whole web, was suggested by Fogaras and
R?acz (2005). However, in an evaluation of this al-
gorithm we found that it does not give competitive
results (see Section 6). A matrix representation of
SimRank called SimFusion (Xi et al, 2005) im-
proves the computational complexity from O(n
4
)
to O(n
3
). Lizorkin et al (2010) also reduce com-
plexity to O(n
3
) by selecting essential node pairs
and using partial sums. They also give a useful
overview of SimRank, SimFusion and the Monte
Carlo methods of Fogaras and R?acz (2005). A
non-iterative computation for SimRank was intro-
duced by Li et al (2010). This is especially useful
for dynamic graphs. However, all of these meth-
ods have to run SimRank on the entire graph and
are not efficient enough for very large graphs. We
are interested in applications that only need a frac-
tion of all O(n
2
) pairwise similarities. The algo-
rithm we propose below is an order of magnitude
faster in such applications because it is based on a
local formulation of the similarity measure.
2
Apart from SimRank, many other similarity
measures have been proposed. Leicht et al (2006)
introduce a similarity measure that is also based on
the idea that nodes are similar when their neigh-
bors are, but that is designed for bipartite graphs.
However, most graphs in NLP are not bipartite and
Jeh and Widom (2002) also proposed a SimRank
variant for bipartite graphs.
Another important similarity measure is cosine
similarity of Personalized PageRank (PPR) vec-
tors. We will refer to this measure as PPR+cos.
Hughes and Ramage (2007) find that PPR+cos
has high correlation with human similarity judg-
ments on WordNet-based graphs. Agirre et al
(2009) use PPR+cos for WordNet and for cross-
lingual studies. Like CoSimRank, PPR+cos is
efficient when computing single node pair simi-
larities; we therefore use it as one of our base-
lines below. This method is also used by Chang
et al (2013) for semantic relatedness. They also
experimented with Euclidean distance and KL-
2
A reviewer suggests that CoSimRank is an efficient ver-
sion of SimRank in a way analogous to SALSA?s (Lempel
and Moran, 2000) relationship to HITS (Kleinberg, 1999) in
that different aspects of similarity are decoupled.
divergence. Interestingly, a simpler method per-
formed best when comparing with human simi-
larity judgments. In this method only the entries
corresponding to the compared nodes are used for
a similarity score. Rao et al (2008) compared
PPR+cos to other graph based similarity mea-
sures like shortest-path and bounded-length ran-
dom walks. PPR+cos performed best except for
a new similarity measure based on commute time.
We do not compare against this new measure as it
uses the graph Laplacian and so cannot be com-
puted for a single node pair.
One reason CoSimRank is efficient is that we
need only compute a few iterations of the random
walk. This is often true of this type of algorithm;
cf. (Sch?utze and Walsh, 2008).
LexRank (Erkan and Radev, 2004) is similar to
PPR+cos in that it combines PageRank and cosine;
it initializes the sentence similarity matrix of a
document using cosine and then applies PageRank
to compute lexical centrality. Despite this superfi-
cial relatedness, applications like lexicon extrac-
tion that look for similar entities and applications
that look for central entities are quite different.
In addition to faster versions of SimRank, there
has also been work on extensions of SimRank.
Dorow et al (2009) and Laws et al (2010) ex-
tend SimRank to edge weights, edge labels and
multiple graphs. We use their Multi-Edge Extrac-
tion (MEE) algorithm as one of our baselines be-
low. A similar graph of dependency structures was
built by Minkov and Cohen (2008). They applied
different similarity measures, e.g., cosine of de-
pendency vectors or a new algorithm called path-
constrained graph walk, on synonym extraction
(Minkov and Cohen, 2012). We compare CoSim-
Rank with their results in our experiments (see
Section 6).
Some other applications of SimRank or other
graph based similarity measures in NLP include
work on document similarity (Li et al, 2009),
the transfer of sentiment information between lan-
guages (Scheible et al, 2010) and named entity
disambiguation (Han and Zhao, 2010). Hoang and
Kan (2010) use SimRank for related work sum-
marization. Muthukrishnan et al (2010) combine
link based similarity and content based similarity
for document clustering and classification.
These approaches use at least one of cosine sim-
ilarity, PageRank and SimRank. CoSimRank can
either be interpreted as an efficient version of Sim-
1393
Rank or as a version of Personalized PageRank
for similarity measurement. The novelty is that
we compute similarity for vectors that are induced
using a new algorithm, so that the similarity mea-
surement is much more efficient when an applica-
tion only needs a fraction of all O(n
2
) pairwise
similarities.
3 CoSimRank
We first first give an intuitive introduction of
CoSimRank as a Personalized PageRank (PPR)
derivative. Later on, we will give a matrix formu-
lation to compare CoSimRank with SimRank.
3.1 Personalized PageRank
Haveliwala (2002) introduced Personalized Page-
Rank ? or topic-sensitive PageRank ? based on the
idea that the uniform damping vector p
(0)
can be
replaced by a personalized vector, which depends
on node i. We usually set p
(0)
(i) = e
i
, with e
i
be-
ing a vector of the standard basis, i.e., the i
th
entry
is 1 and all other entries are 0. The PPR vector of
node i is given by:
p
(k)
(i) = dAp
(k?1)
(i) + (1? d)p
(0)
(i) (1)
where A is the stochastic matrix of the Markov
chain, i.e., the row normalized adjacency matrix.
The damping factor d ? (0, 1) ensures that the
computation converges. The PPR vector after k
iterations is given by p
(k)
.
To visualize this formula, one can imagine a
random surfer starting at node i and following one
of the links with probability d or jumping back to
the starting node i with probability (1? d). Entry
i of the converged PPR vector represents the prob-
ability that the random surfer is on node i after an
unlimited number of steps.
To simulate the behavior of SimRank we will
simplify this equation and set the damping factor
d = 1. We will re-add a damping factor later in
the calculation.
p
(k)
= Ap
(k?1)
(2)
Note that the personalization vector p
(0)
was elim-
inated, but is still present as the starting vector of
the iteration.
3.2 Similarity of vectors
Let p(i) be the PPR vector of node i. The cosine
of two vectors u and v is computed by dividing
Figure 1: Graph motivating CoSimRank algo-
rithm. Whereas PPR gives relatively high similar-
ity to the pair (law,suit), CoSimRank assigns the
pair similarity 0.
the inner product ?u, v? by the lengths of the vec-
tors. The cosine of two PPR vectors can be used as
a similarity measure for the corresponding nodes
(Hughes and Ramage, 2007; Agirre et al, 2009):
s(i, j) =
?p(i), p(j)?
|p(i)||p(j)|
(3)
This measure s(i, j) looks at the probability that
a random walker is on a certain edge after an un-
limited number of steps. This is potentially prob-
lematic as the example in Figure 1 shows. The
PPR vectors of suit and dress will have some
weight on tailor, which is good. However, the
PPR vector of law will also have a non-zero weight
for tailor. So law and dress are similar because of
the node tailor. This is undesirable.
We can prevent this type of spurious similarity
by taking into account the path the random surfer
took to get to a particular node. We formalize this
by defining CoSimRank s(i, j) as follows:
s(i, j) =
?
?
k=0
c
k
?p
(k)
(i), p
(k)
(j)? (4)
where p
(k)
(i) is the PPR vector of node i from
Eq. 2 after k iterations. We compare the PPR vec-
tors at each time step k. The sum of all similarities
is the value of CoSimRank, i.e., the final similar-
ity. We add a damping factor c, so that early meet-
ings are more valuable than later meetings.
To compute the similarity of two vectors u and
v we use the inner product ??, ?? in Eq. 4 for two
reasons:
1. This is similar to cosine similarity except that
the 1-norm is used instead of the 2-norm.
Since our vectors are probability vectors, we
have
?p(i), p(j)?
|p(i)||p(j)|
= ?p(i), p(j)?
1394
for the 1-norm.
3
2. Without expensive normalization, we can
give a simple matrix formalization of CoSim-
Rank and compute it efficiently using fast
matrix multiplication algorithms.
Later on, the following iterative computation of
CoSimRank will prove useful:
s
(k)
(i, j) = c
k
?p
(k)
(i), p
(k)
(j)?+ s
(k?1)
(i, j)
(5)
3.3 Matrix formulation
The matrix formulation of CoSimRank is:
S
(0)
= E
S
(1)
= cAA
T
+ S
(0)
S
(2)
= c
2
A
2
(A
T
)
2
+ S
(1)
. . .
S
(k)
= c
k
A
k
(A
T
)
k
+ S
(k?1)
(6)
We will see in Section 5 that this formulation is the
basis for a very efficient version of CoSimRank.
3.4 Convergence properties
As the PPR vectors have only positive values, we
can easily see in Eq. 4 that the CoSimRank of
one node pair is monotonically non-decreasing.
For the dot product of two vectors, the Cauchy-
Schwarz inequality gives the upper bound:
?u, v? ? ?u? ?v?
where ?x? is the norm of x. From Eq. 2 we get
?
?
p
(k)
?
?
1
= 1, where ???
1
is the 1-norm. We also
know from elementary functional analysis that the
1-norm is the biggest of all p-norms and so one
has
?
?
p
(k)
?
?
? 1. It follows that CoSimRank grows
more slowly than a geometric series and converges
if |c| < 1:
s(i, j) ?
?
?
k=0
c
k
=
1
1? c
If an upper bound of 1 is desired for s(i, j) (in-
stead of 1/(1? c)), then we can use s
?
:
s
?
(i, j) = (1? c)s(i, j)
3
This type of similarity measure has also been used and
investigated by
?
O S?eaghdha and Copestake (2008), Cha
(2007), Jebara et al (2004) (probability product kernel) and
(Jaakkola et al, 1999) (Fisher kernel) among others.
4 Comparison to SimRank
The original SimRank equation can be written as
follows (Jeh and Widom, 2002):
r(i, j) =
?
?
?
?
?
1, if i = j
c
|N(i)||N(j)|
?
k?N(i)
l?N(j)
r(k, l), else
where N(i) denotes the nodes connected to i.
SimRank is computed iteratively. With A be-
ing the normalized adjacency matrix we can write
SimRank in matrix formulation:
R
(0)
= E
R
(k)
= max{cAR
(k?1)
A
T
, R
(0)
} (7)
where the maximum of two matrices refers to the
element-wise maximum. We will now prove by in-
duction that the matrix formulation of CoSimRank
(Eq. 6) is equivalent to:
S
?(k)
= cAS
?(k?1)
A
T
+ S
(0)
(8)
and thus very similar to SimRank (Eq. 7).
The base case S
(1)
= S
?(1)
is trivial. Inductive
step:
S
?(k)
(8)
= cAS
?(k?1)
A
T
+ S
(0)
= cA(c
k?1
A
k?1
(A
T
)
k?1
+ S
(k?2)
)A
T
+ S
(0)
= c
k
A
k
(A
T
)
k
+ cAS
(k?2)
A
T
+ S
(0)
= c
k
A
k
(A
T
)
k
+ S
(k?1)
(6)
= S
(k)
Comparing Eqs. 7 and 8, we see that SimRank
and CoSimRank are very similar except that they
initialize the similarities on the diagonal differ-
ently. Whereas SimRank sets each of these en-
tries back to one at each iteration, CoSimRank
adds one. Thus, when computing the two similar-
ity measures iteratively, the diagonal element (i, i)
will be set to 1 by both methods for those initial it-
erations for which this entry is 0 for cAS
(k?1)
A
T
(i.e., before applying either max or add). The
methods diverge when the entry is 6= 0 for the first
time.
Complexity of computing all n
2
similarities.
The matrix formulas of both SimRank (Eq. 7)
and CoSimRank (Eq. 8) have time complexity
O(n
3
) or ? if we want to take the higher efficiency
of computation for sparse graphs into account ?
O(dn
2
) where n is the number of nodes and d the
1395
average degree. Space complexity is O(n
2
) for
both algorithms.
Complexity of computing k
2
 n
2
similar-
ities. In most cases, we only want to compute
k
2
similarities for k nodes. For CoSimRank, we
compute the k PPR vectors inO(kdn) (Eq. 2) and
compute the k
2
similarities in O(k
2
n) (Eq. 5). If
d < k, then the time complexity of CoSimRank
is O(k
2
n). If we only compute a single similar-
ity, then the complexity is O(dn). In contrast, the
complexity of SimRank is the same as in the all-
similarities case: O(dn
2
). It is not obvious how to
design a lower-complexity version of SimRank for
this case. Thus, we have reduced SimRank?s cu-
bic time complexity to a quadratic time complex-
ity for CoSimRank or ? assuming that the aver-
age degree d does not depend on n ? SimRank?s
quadratic time complexity to linear time complex-
ity for the case of computing few similarities.
Space complexity for computing k
2
similarities
is O(kn) since we need only store k vectors, not
the complete similarity matrix. This complexity
can be exploited even for the all similarities appli-
cation: If the matrix formulation cannot be used
because the O(n
2
) similarity matrix is too big for
available memory, then we can compute all sim-
ilarities in batches ? and if desired in parallel ?
whose size is chosen such that the vectors of each
batch still fit in memory.
In summary, CoSimRank and SimRank have
similar space and time complexities for comput-
ing all n
2
similarities. For the more typical case
that we only want to compute a fraction of all sim-
ilarities, we have recast the global SimRank for-
mulation as a local CoSimRank formulation. As a
result, time and space complexities are much im-
proved. In Section 6, we will show that this is also
true in practice.
5 Extensions
We will show now that the basic CoSimRank algo-
rithm can be extended in a number of ways and is
thus a flexible tool for different NLP applications.
5.1 Weighted edges
The use of weighted edges was first proposed in
the PageRank patent. It is straightforward and
easy to implement by replacing the row normal-
ized adjacency matrixA with an arbitrary stochas-
tic matrix P . We can use this edge weighted Page-
Rank for CoSimRank.
5.2 CoSimRank across graphs
We often want to compute the similarity of nodes
in two different graphs with a known node-node
correspondence; this is the scenario we are faced
with in the lexicon extraction task (see Section 6).
A variant of SimRank for this task was presented
by Dorow et al (2009). We will now present an
equivalent method for CoSimRank. We denote the
number of nodes in the two graphs U and V by
|U | and |V |, respectively. We compute PPR vec-
tors p ? R
|U |
and q ? R
|V |
for each graph. Let
S
(0)
? R
|U |?|V |
be the known node-node corre-
spondences. The analog of CoSimRank (Eq. 4)
for two graphs is then:
s(i, j) =
?
?
k=0
c
k
?
(u,v)?S
(0)
p
(k)
u
(i)q
(k)
v
(j) (9)
The matrix formulation (cf. Eq. 6) is:
S
(k)
= c
k
A
k
S
(0)
(B
T
)
k
+ S
(k?1)
(10)
whereA andB are row-normalized adjacency ma-
trices. We can interpret S
(0)
as a change of basis.
A similar approach for word embeddings was pub-
lished by Mikolov et al (2013). They call S
(0)
the
translation matrix.
5.3 Typed edges
To be able to directly compare to prior work in our
experiments, we also present a method to integrate
a set of typed edges T in the CoSimRank calcula-
tion. For this we will compute a similarity matrix
for each edge type ? and merge them into one ma-
trix for the next iteration:
S
(k)
=
(
c
|T |
?
??T
A
?
S
(k?1)
B
T
?
)
+ S
(0)
(11)
This formula is identical to the random surfer
model where two surfers only meet iff they are
on the same node and used the same edge type to
get there. A more strict claim would be to use the
same edge type at any time of their journey:
S
(k)
=
c
k
|T |
k
?
??T
k
(
k
?
i=1
A
?
i
)
S
(0)
(
k?1
?
i=0
B
T
?
k?i
)
+ S
(k?1)
(12)
We will not use Eq. 12 due to its space complexity.
1396
5.4 Similarity of sets of nodes
CoSimRank can also be used to compute the sim-
ilarity s(V,W ) of two sets V and W of nodes,
e.g., short text snippets. We are not including this
method in our experiments, but we will give the
equation here, as traditional document similarity
measures (e.g., cosine similarity) perform poorly
on this task although there also are known alter-
natives with good results (Sahami and Heilman,
2006). For a set V , the initial PPR vector is given
by:
p
(0)
i
(V ) =
{
1
|V |
, if i ? V
0, else
We then reuse Eq. 4 to compute s(V,W ):
s(V,W ) =
?
?
k=0
c
k
?p
(k)
(V ), p
(k)
(W )?
In summary, modifications proposed for Sim-
Rank (weighted and typed edges, similarity across
graphs) as well as modifications proposed for
PageRank (sets of nodes) can also be applied to
CoSimRank. This makes CoSimRank a very flex-
ible similarity measure.
We will test the first three extensions experi-
mentally in the next section and leave similarity
of node sets for future work.
6 Experiments
We evaluate CoSimRank for the tasks of syn-
onym extraction and bilingual lexicon extraction.
We use the basic version of CoSimRank (Eq. 4)
for synonym extraction and the two-graph version
(Eq. 9) for lexicon extraction, both with weighted
edges. Our motivation for this application is that
two words that are synonyms of each other should
have similar lexical neighbors and that two words
that are translations of each other should have
neighbors that correspond to each other; thus, in
each case the nodes should be similar in the graph-
theoretic sense and CoSimRank should be able to
identify this similarity.
We use the English and German graphs pub-
lished by Laws et al (2010), including edge
weighting and normalization. Nodes are nouns,
adjectives and verbs occurring in Wikipedia.
There are three types of edges, corresponding to
three types of syntactic configurations extracted
from the parsed Wikipedias: adjective-noun, verb-
object and noun-noun coordination. Table 1 gives
examples and number of nodes and edges.
Edge types
relation entities description example
amod a, v adjective-noun a fast car
dobj v, n verb-object drive a car
ncrd n, n noun-noun cars and busses
Graph statistics
nodes nouns adjectives verbs
de 34,544 10,067 2,828
en 22,258 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,905
en 288,878 686,069 510,351
Table 1: Edge types (above) and number of nodes
and edges (below)
6.1 Baselines
We propose CoSimRank as an efficient algorithm
for computing the similarity of nodes in a graph.
Consequently, we compare against the two main
methods for this task in NLP: SimRank and exten-
sions of PageRank.
We also compare against the MEE (Multi-Edge
Extraction) variant of SimRank (Dorow et al,
2009), which handles labeled edges more effi-
ciently than SimRank:
S
?(k)
=
c
|T |
?
??T
A
?
S
(k?1)
B
T
?
S
(k)
= max{S
?(k)
, S
(0)
}
where A
?
is the row-normalized adjacency matrix
for edge type ? (see edge types in Table 1).
Apart from SimRank, extensions of PageRank
are the main methods for computing the similar-
ity of nodes in graphs in NLP (e.g., Hughes and
Ramage (2007), Agirre et al (2009) and other pa-
pers discussed in related work). Generally, these
methods compute the Personalized PageRank for
each node (see Eq. 1). When the computation has
converged, the similarity of two nodes is given by
the cosine similarity of the Personalized PageRank
vectors. We implemented this method for our ex-
periments and call it PPR+cos.
6.2 Synonym Extraction
We use TS68, a test set of 68 synonym pairs pub-
lished by Minkov and Cohen (2012) for evalua-
tion. This gold standard lists a single word as the
1397
P@1 P@10 MRR
one-synonym
PPR+cos 20.6% 52.9% 0.32
SimRank 25.0% 61.8% 0.37
CoSimRank 25.0% 61.8% 0.37
Typed CoSimRank 23.5% 63.2% 0.37
extended
PPR+cos 32.6% 73.5% 0.48
SimRank 45.6% 83.8% 0.59
CoSimRank 45.6% 83.8% 0.59
Typed CoSimRank 44.1% 83.8% 0.59
Table 2: Results for synonym extraction on TS68.
Best result in each column in bold.
correct synonym even if there are several equally
acceptable near-synonyms (see Table 3 for exam-
ples). We call this the one-synonym evaluation.
Three native English speakers were asked to mark
synonyms, that were proposed by a baseline or by
CoSimRank, i.e. ranked in the top 10. If all three
of them agreed on one word as being a synonym
in at least one meaning, we added this as a correct
answer to the test set. We call this the ?extended?
evaluation (see Table 2).
Synonym extraction is run on the English graph.
To calculate PPR+cos, we computed 20 iterations
with a decay factor of 0.8 and used the cosine sim-
ilarity with the 2-norm in the denominator to com-
pare two vectors. For the other three methods, we
also used a decay factor of 0.8 and computed 5 it-
erations. Recall that CoSimRank uses the simple
inner product ??, ?? to compare vectors.
Our evaluation measures are proportion of
words correctly translated by word in the top
position (P@1), proportion of words correctly
translated by a word in one of the top 10 posi-
tions (P@10) and Mean Reciprocal Rank (MRR).
CoSimRank?s MRR scores of 0.37 (one-synonym)
and 0.59 (extended) are the same or better than all
baselines (see Table 2). CoSimRank and SimRank
have the same P@1 and P@10 accuracy (although
they differed on some decisions). CoSimRank is
better than PPR+cos on both evaluations, but as
this test set is very small, the results are not signif-
icant. Table 3 shows a sample of synonyms pro-
posed by CoSimRank.
Minkov and Cohen (2012) tested cosine and
random-walk measures on grammatical relation-
keyword expected extracted
movie film film
modern contemporary contemporary
demonstrate protest show
attractive appealing beautiful
economic profitable financial
close shut open
Table 3: Examples for extracted synonyms. Cor-
rect synonyms according to extended evaluation in
bold.
ships (similar to our setup) as well as on cooccur-
rence statistics. The MRR scores for these meth-
ods range from 0.29 to 0.59. (MRR is equivalent
to MAP as reported by Minkov and Cohen (2012)
when there is only one correct answer.) Their
best number (0.59) is better than our one-synonym
result; however, they performed manual postpro-
cessing of results ? e.g., discarding words that are
morphologically or semantically related to other
words in the list ? so our fully automatic results
cannot be directly compared.
6.3 Lexicon Extraction
We evaluate lexicon extraction on TS1000, a test
set of 1000 items, (Laws et al, 2010) each con-
sisting of an English word and its German transla-
tions. For lexicon extraction, we use the same pa-
rameters as in the synonym extraction task for all
four similarity measures. We use a seed dictionary
of 12,630 word pairs to establish node-node corre-
spondences between the two graphs. We remove
a search keyword from the seed dictionary before
calculating similarities for it, something that the
architecture of CoSimRank makes easy because
we can use a different seed dictionary S
(0)
for ev-
ery keyword.
Both CoSimRank methods outperform Sim-
Rank significantly (see Table 4). The differ-
ence between CoSimRank with and without typed
edges is not significant. (This observation was also
made for SimRank on a smaller graph and test set
(Laws et al, 2010).)
PPR+cos?s performance at 14.8% correct trans-
lations is much lower than SimRank and CoSim-
Rank. The disadvantage of this similarity mea-
sure is significant and even more visible on bilin-
gual lexicon extraction than on synonym extrac-
tion (see Table 2). The reason might be that we
are not comparing the whole PPR vector anymore,
1398
P@1 P@10
PPR+cos 14.8%
?
45.7%
?
SimRank MEE 48.0%
?
76.0%
?
CoSimRank 61.1% 84.0%
Typed CoSimRank 61.4% 83.9%
Table 4: Results for bilingual lexicon extraction
(TS1000 EN ? DE). Best result in each column
in bold.
but only entries which occur in the seed dictionary
(see Eq. 9). As the seed dictionary contains 12,630
word pairs, this means that only every fourth entry
of the PPR vector (the German graph has 47,439
nodes) is used for similarity calculation. This is
also true for CoSimRank, but it seems that CoSim-
Rank is more stable because we compare more
than one vector.
?
We also experimented with the method of Fog-
aras and R?acz (2005). We tried a number of differ-
ent ways of modifying it for weighted graphs: (i)
running the random walks with the weighted ad-
jacency matrix as Markov matrix, (ii) storing the
weight (product of each edge weight) of a random
walk and using it as a factor if two walks meet
and (iii) a combination of both. We needed about
10,000 random walks in all three conditions. As a
result, the computational time was approximately
30 minutes per test word, so this method is even
slower than SimRank for our application. The ac-
curacies P@1 and P@10 were worse in all experi-
ments than those of CoSimRank.
6.4 Run time performance
Table 5 compares the run time performance of
CoSimRank with the baselines. We ran all exper-
iments on a 64-bit Linux machine with 64 Intel
Xenon X7560 2.27Ghz CPUs and 1TB RAM. The
calculated time is the sum of the time spent in user
mode and the time spent in kernel mode. The ac-
tual wall clock time was significantly lower as we
used up to 64 CPUs.
Compared to SimRank, CoSimRank is more
than 40 times faster on synonym extraction and six
times faster on lexicon extraction. SimRank is at
a disadvantage because it computes all similarities
in the graph regardless of the size of the test set;
it is particularly inefficient on synonym extraction
because the English graph contains a large number
?
significantly worse than CoSimRank (? = 0.05, one-
tailed Z-Test)
synonym extraction lexicon extraction
(68 word pairs) (1000 word pairs)
PPR+cos 2,228 2,195
SimRank 23,423 14,418
CoSimRank 524 2,342
Typed CoSimRank 615 6,108
Table 5: Execution times in minutes for CoSim-
Rank and the baselines. Best result in each column
in bold.
of edges (see Table 1).
Compared to PPR+cos, CoSimRank is roughly
four times faster on synonym extraction and has
comparable performance on lexicon extraction.
We compute 20 iterations of PPR+cos to reach
convergence and then calculate a single cosine
similarity. For CoSimRank, we need only com-
pute five iterations to reach convergence, but we
have to compute a vector similarity in each itera-
tion. The counteracting effects of fewer iterations
and more vector similarity computations can give
either CoSimRank or PPR+cos an advantage, as
is the case for synonym extraction and lexicon ex-
traction, respectively.
CoSimRank should generally be three times
faster than typed CoSimRank since the typed ver-
sion has to repeat the computation for each of
the three types. This effect is only visible on the
larger test set (lexicon extraction) because the gen-
eral computation overhead is about the same on a
smaller test set.
6.5 Comparison with WINTIAN
Here we address inducing a bilingual lexicon from
a seed set based on grammatical relations found
by a parser. An alternative approach is to in-
duce a bilingual lexicon from Wikipedia?s inter-
wiki links (Rapp et al, 2012). These two ap-
proaches have different strengths and weaknesses;
e.g., the interwiki-link-based approach does not
require a seed set, but it can only be applied to
comparable corpora that consist of corresponding
? although not necessarily ?parallel? ? documents.
Despite these differences it is still interesting to
compare the two algorithms. Rapp et al (2012)
kindly provided their test set to us. It contains
1000 English words and a single correct German
translation for each. We evaluate on a subset we
call TS774 that consists of the 774 test word pairs
that are in the intersection of words covered by the
1399
P@1 P@10
Wintian 43.8% 55.4%
?
CoSimRank 43.0% 73.6%
Table 6: Results for bilingual lexicon extraction
(TS774 DE? EN). Best result in each column in
bold.
WINTIAN Wikipedia data (Rapp et al, 2012) and
words covered by our data. Most of the 226 miss-
ing word pairs are adverbs, prepositions and plural
forms that are not covered by our graphs due to the
construction algorithm we use: lemmatization, re-
striction to adjectives, nouns and verbs etc.
Table 6 shows that CoSimRank is slightly, but
not significantly worse than WINTIAN on P@1
(43.0 vs 43.8), but significantly better on P@10
(73.6 vs 55.4).
4
The reason could be that CoSim-
Rank is a more effective algorithm than WIN-
TIAN; but the different initializations (seed set vs
interwiki links) or the different linguistic represen-
tations (grammatical relations vs bag-of-words)
could also be responsible.
6.6 Error Analysis
The results on TS774 can be considered conserva-
tive since only one translation is accepted as being
correct. In reality other translations might also be
acceptable (e.g., both street and road for Stra?e).
In contrast, TS1000 accepts more than one cor-
rect translation. Additionally, TS774 was created
by translating English words into German (using
Google translate). We are now testing the reverse
direction. So we are doomed to fail if the original
English word is a less common translation of an
ambiguous German word. For example, the En-
glish word gulf was translated by Google to Golf,
but the most common sense of Golf is the sport.
Hence our algorithm will incorrectly translate it
back to golf.
As we can see in Table 7, we also face the prob-
lems discussed by Laws et al (2010): the algo-
rithm sometimes picks cohyponyms (which can
still be seen as reasonable) and antonyms (which
are clear errors).
Contrary to our intuition, the edge-typed vari-
ant of CoSimRank did not perform significantly
better than the non-edge-typed version. Looking
4
We achieved better results for CoSimRank by optimizing
the damping factor, but in this paper, we only present results
for a fixed damping factor of 0.8.
keyword gold standard CoSimRank
arm poor impoverished
erreichen reach achieve
gehen go walk
direkt directly direct
weit far further
breit wide narrow
reduzieren reduce increase
Stunde hour second
Westen west southwest
Junge boy child
Table 7: Examples for CoSimRank translation er-
rors on TS774. We counted translations as incor-
rect if they were not listed in the gold standard
even if they were correct translations according to
www.dict.cc (in bold).
at Table 1, we see that there is only one edge type
connecting adjectives. The same is true for verbs.
The random surfer only has a real choice between
different edge types when she is on a noun node.
Combined with the fact that only the last edge type
is important this has absolutely no effect for a ran-
dom surfer meeting at adjectives or verbs.
Two possible solutions would be (i) to use more
fine-grained edge types, (ii) to apply Eq. 12, in
which the edge type of each step is important.
However, this will increase the memory needed for
calculation.
7 Summary
We have presented CoSimRank, a new similar-
ity measure that can be computed for a single
node pair without relying on the similarities in the
whole graph. We gave two different formaliza-
tions of CoSimRank: (i) a derivation from Person-
alized PageRank and (ii) a matrix representation
that can take advantage of fast matrix multipli-
cation algorithms. We also presented extensions
of CoSimRank for a number of applications, thus
demonstrating the flexibility of CoSimRank as a
similarity measure.
We showed that CoSimRank is superior to
SimRank in time and space complexity; and
we demonstrated that CoSimRank performs bet-
ter than PPR+cos on two similarity computation
tasks.
Acknowledgments. This work was supported
by DFG (SCHU 2246/2-2).
1400
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
WWW, pages 107?117.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. Mathematical Models and Methods
in Applied Sciences, 1(4):300?307.
Ching-Yun Chang, Stephen Clark, and Brian Harring-
ton. 2013. Getting creative with semantic similarity.
In Semantic Computing (ICSC), 2013 IEEE Seventh
International Conference on, pages 330?333.
Gerard de Melo and Gerhard Weikum. 2012. Uwn: A
large multilingual lexical knowledge base. In ACL
(System Demonstrations), pages 151?156.
Beate Dorow, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, GEMS ?09, pages 91?95.
G?unes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Intell. Res. (JAIR), 22:457?
479.
D?aniel Fogaras and Bal?azs R?acz. 2005. Scaling
link-based similarity search. In Proceedings of the
14th international conference on World Wide Web,
WWW ?05, pages 641?650.
Xianpei Han and Jun Zhao. 2010. Structural semantic
relatedness: a knowledge-based method to named
entity disambiguation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 50?59.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th international conference
on World Wide Web, WWW ?02, pages 517?526.
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 427?435.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
EMNLP-CoNLL, pages 581?589.
Tommi Jaakkola, David Haussler, et al 1999. Exploit-
ing generative models in discriminative classifiers.
Advances in neural information processing systems,
pages 487?493.
Tony Jebara, Risi Kondor, and Andrew Howard. 2004.
Probability product kernels. The Journal of Machine
Learning Research, 5:819?844.
Glen Jeh and Jennifer Widom. 2002. Simrank: a
measure of structural-context similarity. In Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ?02, pages 538?543.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
Florian Laws, Lukas ichelbacher, Beate Dorow, Chris-
tian Scheible, Ulrich Heid, and Hinrich Sch?utze.
2010. A linguistically grounded graph model
for bilingual lexicon extraction. In Coling 2010:
Posters, pages 614?622.
Elizabeth Leicht, Petter Holme, and Mark Newman.
2006. Vertex similarity in networks. Physical Re-
view E, 73(2):026120.
Ronny Lempel and Shlomo Moran. 2000. The
stochastic approach for link-structure analysis
(salsa) and the tkc effect. Computer Networks,
33(1):387?401.
Pei Li, Zhixu Li, Hongyan Liu, Jun He, and Xiaoy-
ong Du. 2009. Using link-based content analy-
sis to measure document similarity effectively. In
Proceedings of the Joint International Conferences
on Advances in Data and Web Management, AP-
Web/WAIM ?09, pages 455?467.
Cuiping Li, Jiawei Han, Guoming He, Xin Jin, Yizhou
Sun, Yintao Yu, and Tianyi Wu. 2010. Fast com-
putation of simrank for static and dynamic informa-
tion networks. In Proceedings of the 13th Interna-
tional Conference on Extending Database Technol-
ogy, EDBT ?10, pages 465?476.
Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, and
Denis Turdakov. 2010. Accuracy estimate and op-
timization techniques for simrank computation. The
VLDB Journal?The International Journal on Very
Large Data Bases, 19(1):45?66.
Rada Mihalcea and Dragomir Radev. 2011. Graph-
based natural language processing and information
retrieval. Cambridge University Press.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.
Einat Minkov and William W. Cohen. 2008. Learn-
ing graph walk based similarity measures for parsed
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 907?916.
1401
Einat Minkov and William W. Cohen. 2012. Graph
based similarity measures for synonym extraction
from parsed text. In Workshop Proceedings of
TextGraphs-7 on Graph-based Methods for Natural
Language Processing, TextGraphs-7 ?12, pages 20?
24.
Pradeep Muthukrishnan, Dragomir Radev, and
Qiaozhu Mei. 2010. Edge weight regularization
over multiple graphs for similarity learning. In Data
Mining (ICDM), 2010 IEEE 10th International
Conference on, pages 374?383.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 649?
656.
Delip Rao, David Yarowsky, and Chris Callison-Burch.
2008. Affinity measures based on the graph Lapla-
cian. In Proceedings of the 3rd Textgraphs Work-
shop on Graph-Based Algorithms for Natural Lan-
guage Processing, TextGraphs-3, pages 41?48.
Reinhard Rapp, Serge Sharoff, and Bogdan Babych.
2012. Identifying word translations from compa-
rable documents without a seed lexicon. In LREC,
pages 460?466.
Mehran Sahami and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the simi-
larity of short text snippets. In Proceedings of the
15th international conference on World Wide Web,
WWW ?06, pages 377?386.
Christian Scheible, Florian Laws, Lukas Michelbacher,
and Hinrich Sch?utze. 2010. Sentiment transla-
tion through multi-edge graphs. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1104?1112.
Hinrich Sch?utze and Michael Walsh. 2008. A graph-
theoretic model of lexical syntactic acquisition. In
EMNLP, pages 917?926.
Wensi Xi, Edward A. Fox, Weiguo Fan, Benyu Zhang,
Zheng Chen, Jun Yan, and Dong Zhuang. 2005.
Simfusion: measuring similarity using unified re-
lationship matrix. In Proceedings of the 28th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?05, pages 130?137.
1402
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 42?48,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Citation Polarity Classification with Product Reviews
Charles Jochim
?
IBM Research ? Ireland
charlesj@ie.ibm.com
Hinrich Sch?utze
Center for Information & Language Processing
University of Munich
Abstract
Recent work classifying citations in scien-
tific literature has shown that it is possi-
ble to improve classification results with
extensive feature engineering. While this
result confirms that citation classification
is feasible, there are two drawbacks to
this approach: (i) it requires a large anno-
tated corpus for supervised classification,
which in the case of scientific literature
is quite expensive; and (ii) feature engi-
neering that is too specific to one area of
scientific literature may not be portable to
other domains, even within scientific liter-
ature. In this paper we address these two
drawbacks. First, we frame citation clas-
sification as a domain adaptation task and
leverage the abundant labeled data avail-
able in other domains. Then, to avoid
over-engineering specific citation features
for a particular scientific domain, we ex-
plore a deep learning neural network ap-
proach that has shown to generalize well
across domains using unigram and bigram
features. We achieve better citation clas-
sification results with this cross-domain
approach than using in-domain classifica-
tion.
1 Introduction
Citations have been categorized and studied for
a half-century (Garfield, 1955) to better under-
stand when and how citations are used, and
to record and measure how information is ex-
changed (e.g., networks of co-cited papers or au-
thors (Small and Griffith, 1974)). Recently, the
value of this information has been shown in practi-
cal applications such as information retrieval (IR)
?
This work was primarily conducted at the IMS ? Uni-
versity of Stuttgart.
(Ritchie et al, 2008), summarization (Qazvinian
and Radev, 2008), and even identifying scientific
breakthroughs (Small and Klavans, 2011). We ex-
pect that by identifying and labeling the function
of citations we can improve the effectiveness of
these applications.
There has been no consensus on what aspects
or functions of a citation should be annotated and
how. Early citation classification focused more on
citation motivation (Garfield, 1964), while later
classification considered more the citation func-
tion (Chubin and Moitra, 1975). Recent stud-
ies using automatic classification have continued
this tradition of introducing a new classification
scheme with each new investigation into the use
of citations (Nanba and Okumura, 1999; Teufel
et al, 2006a; Dong and Sch?afer, 2011; Abu-Jbara
et al, 2013). One distinction that has been more
consistently annotated across recent citation clas-
sification studies is between positive and negative
citations (Athar, 2011; Athar and Teufel, 2012;
Abu-Jbara et al, 2013).
1
The popularity of this
distinction likely owes to the prominence of sen-
timent analysis in NLP (Liu, 2010). We follow
much of the recent work on citation classification
and concentrate on citation polarity.
2 Domain Adaptation
By concentrating on citation polarity we are able
to compare our classification to previous citation
polarity work. This choice also allows us to access
the wealth of existing data containing polarity an-
notation and then frame the task as a domain adap-
tation problem. Of course the risk in approaching
the problem as domain adaptation is that the do-
mains are so different that the representation of
a positive instance of a movie or product review,
for example, will not coincide with that of a posi-
1
Dong and Sch?afer (2011) also annotate polarity, which
can be found in their dataset (described later), but this is not
discussed in their paper.
42
tive scientific citation. On the other hand, because
there is a limited amount of annotated citation data
available, by leveraging large amounts of anno-
tated polarity data we could potentially even im-
prove citation classification.
We treat citation polarity classification as a sen-
timent analysis domain adaptation task and there-
fore must be careful not to define features that are
too domain specific. Previous work in citation po-
larity classification focuses on finding new cita-
tion features to improve classification, borrowing
a few from text classification in general (e.g., n-
grams), and perhaps others from sentiment analy-
sis problems (e.g., the polarity lexicon from Wil-
son et al (2005)). We would like to do as little
feature engineering as possible to ensure that the
features we use are meaningful across domains.
However, we do still want features that somehow
capture the inherent positivity or negativity of our
labeled instances, i.e., citations or Amazon prod-
uct reviews. Currently a popular approach for ac-
complishing this is to use deep learning neural net-
works (Bengio, 2009), which have been shown
to perform well on a variety of NLP tasks us-
ing only bag-of-word features (Collobert et al,
2011). More specifically related to our work, deep
learning neural networks have been successfully
employed for sentiment analysis (Socher et al,
2011) and for sentiment domain adaptation (Glo-
rot et al, 2011). In this paper we examine one
of these approaches, marginalized stacked denois-
ing autoencoders (mSDA) from Chen et al (2012),
which has been successful in classifying the po-
larity of Amazon product reviews across product
domains. Since mSDA achieved state-of-the-art
performance in Amazon product domain adapta-
tion, we are hopeful it will also be effective when
switching to a more distant domain like scientific
citations.
3 Experimental Setup
3.1 Corpora
We are interested in domain adaptation for citation
classification and therefore need a target dataset of
citations and a non-citation source dataset. There
are two corpora available that contain citation
function annotation, the DFKI Citation Corpus
(Dong and Sch?afer, 2011) and the IMS Citation
Corpus (Jochim and Sch?utze, 2012). Both corpora
have only about 2000 instances; unfortunately,
there are no larger corpora available with citation
annotation and this task would benefit from more
annotated data. Due to the infrequent use of neg-
ative citations, a substantial annotation effort (an-
notating over 5 times more data) would be nec-
essary to reach 1000 negative citation instances,
which is the number of negative instances in a sin-
gle domain in the multi-domain corpus described
below.
The DFKI Citation Corpus
2
has been used for
classifying citation function (Dong and Sch?afer,
2011), but the dataset alo includes polarity an-
notation. The dataset has 1768 citation sentences
with polarity annotation: 190 are labeled as pos-
itive, 57 as negative, and the vast majority, 1521,
are left neutral. The second citation corpus, the
IMS Citation Corpus
3
contains 2008 annotated ci-
tations: 1836 are labeled positive and 172 are la-
beled negative. Jochim and Sch?utze (2012) use
annotation labels from Moravcsik and Murugesan
(1975) where positive instances are labeled confir-
mative, negative instances are labeled negational,
and there is no neutral class. Because each of
the citation corpora is of modest size we combine
them to form one citation dataset, which we will
refer to as CITD. The two citation corpora com-
prising CITD both come from the ACL Anthol-
ogy (Bird et al, 2008): the IMS corpus uses the
ACL proceedings from 2004 and the DFKI corpus
uses parts of the proceedings from 2007 and 2008.
Since mSDA also makes use of large amounts of
unlabeled data, we extend our CITD corpus with
citations from the proceedings of the remaining
years of the ACL, 1979?2003, 2005?2006, and
2009.
There are a number of non-citation corpora
available that contain polarity annotation. For
these experiments we use the Multi-Domain Senti-
ment Dataset
4
(henceforth MDSD), introduced by
Blitzer et al (2007). We use the version of the
MDSD that includes positive and negative labels
for product reviews taken from Amazon.com in
the following domains: books, dvd, electronics,
and kitchen. For each domain there are 1000 pos-
itive reviews and 1000 negative reviews that com-
prise the ?labeled? data, and then roughly 4000
more reviews in the ?unlabeled?
5
data. Reviews
2
https://aclbib.opendfki.de/repos/
trunk/citation_classification_dataset/
3
http://www.ims.uni-stuttgart.de/
?
jochimcs/citation-classification/
4
http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/
5
It is usually treated as unlabeled data even though it ac-
43
Corpus Instances Pos. Neg. Neut.
DFKI 1768 190 57 1521
IMS 2008 1836 172 ?
MDSD 27,677 13,882 13,795 ?
Table 1: Polarity corpora.
were preprocessed so that for each review you find
a list of unigrams and bigrams with their frequency
within the review. Unigrams from a stop list of 55
stop words are removed, but stop words in bigrams
remain.
Table 1 shows the distribution of polarity labels
in the corpora we use for our experiments. We
combine the DFKI and IMS corpora into the CITD
corpus. We omit the citations labeled neutral from
the DFKI corpus because the IMS corpus does not
contain neutral annotation nor does the MDSD. It
is the case in many sentiment analysis corpora that
only positive and negative instances are included,
e.g., (Pang et al, 2002).
The citation corpora presented above are both
unbalanced and both have a highly skewed distri-
bution. The MDSD on the other hand is evenly
balanced and an effort was even made to keep
the data treated as ?unlabeled? rather balanced.
For this reason, in line with previous work us-
ing MDSD, we balance the labeled portion of the
CITD corpus. This is done by taking 179 unique
negative sentences in the DFKI and IMS corpora
and randomly selecting an equal number of posi-
tive sentences. The IMS corpus can have multiple
labeled citations per sentence: there are 122 sen-
tences containing the 172 negative citations from
Table 1. The final CITD corpus comprises this
balanced corpus of 358 labeled citation sentences
plus another 22,093 unlabeled citation sentences.
3.2 Features
In our experiments, we restrict our features to un-
igrams and bigrams from the product review or
citation context (i.e., the sentence containing the
citation). This follows previous studies in do-
main adaptation (Blitzer et al, 2007; Glorot et al,
2011). Chen et al (2012) achieve state-of-the-art
results on MDSD by testing the 5000 and 30,000
most frequent unigram and bigram features.
Previous work in citation classification has
largely focused on identifying new features for
tually contains positive and negative labels, which have been
used, e.g., in (Chen et al, 2012).
improving classification accuracy. A significant
amount of effort goes into engineering new fea-
tures, in particular for identifying cue phrases,
e.g., (Teufel et al, 2006b; Dong and Sch?afer,
2011). However, there seems to be little consen-
sus on which features help most for this task. For
example, Abu-Jbara et al (2013) and Jochim and
Sch?utze (2012) find the list of polar words from
Wilson et al (2005) to be useful, and neither study
lists dependency relations as significant features.
Athar (2011) on the other hand reported significant
improvement using dependency relation features
and found that the same list of polar words slightly
hurt classification accuracy. The classifiers and
implementation of features varies between these
studies, but the problem remains that there seems
to be no clear set of features for citation polarity
classification.
The lack of consensus on the most useful cita-
tion polarity features coupled with the recent suc-
cess of deep learning neural networks (Collobert et
al., 2011) further motivate our choice to limit our
features to the n-grams available in the product re-
view or citation context and not rely on external
resources or tools for additional features.
3.3 Classification with mSDA
For classification we use marginalized stacked de-
noising autoencoders (mSDA) from Chen et al
(2012)
6
plus a linear SVM. mSDA takes the con-
cept of denoising ? introducing noise to make the
autoencoder more robust ? from Vincent et al
(2008), but does the optimization in closed form,
thereby avoiding iterating over the input vector to
stochastically introduce noise. The result of this
is faster run times and currently state-of-the-art
performance on MDSD, which makes it a good
choice for our domain adaptation task. The mSDA
implementation comes with LIBSVM, which we
replace with LIBLINEAR (Fan et al, 2008) for
faster run times with no decrease in accuracy. LIB-
LINEAR, with default settings, also serves as our
baseline.
3.4 Outline of Experiments
Our initial experiments simply extend those of
Chen et al (2012) (and others who have used
MDSD) by adding another domain, citations. We
train on each of the domains from the MDSD ?
6
We use their MATLAB implementation available at
http://www.cse.wustl.edu/
?
mchen/code/
mSDA.tar.
44
books dvd electronics kitchen
0.0
0.1
0.2
0.3
0.4
0.5
0.6
SVMmSDAIn?domain F1
Figure 1: Cross domain macro-F
1
results train-
ing on Multi-Domain Sentiment Dataset and test-
ing on citation dataset (CITD). The horizontal line
indicates macro-F
1
for in-domain citation classifi-
cation.
books, dvd, electronics, and kitchen ? and test on
the citation data. We split the labeled data 80/20
following Blitzer et al (2007) (cf. Chen et al
(2012) train on all ?labeled? data and test on the
?unlabeled? data). These experiments should help
answer two questions: does a larger amount of
training data, even if out of domain, improve ci-
tation classification; and how well do the differ-
ent product domains generalize to citations (i.e.,
which domains are most similar to citations)?
In contrast to previous work using MDSD, a lot
of the work in domain adaptation also leverages a
small amount of labeled target data. In our second
set of experiments, we follow the domain adap-
tation approaches described in (Daum?e III, 2007)
and train on product review and citation data be-
fore testing on citations.
4 Results and Discussion
4.1 Citation mSDA
Our initial results show that using mSDA for do-
main adaptation to citations actually outperforms
in-domain classification. In Figure 1 we com-
pare citation classification with mSDA to the SVM
baseline. Each pair of vertical bars represents
training on a domain from MDSD (e.g., books)
and testing on CITD. The dark gray bar indicates
the F
1
scores for the SVM baseline using the
30,000 features and the lighter gray bar shows the
mSDA results. The black horizontal line indicates
the F
1
score for in-domain citation classification,
which sometimes represents the goal for domain
adaptation. We can see that using a larger dataset,
even if out of domain, does improve citation clas-
sification. For books, dvd, and electronics, even
the SVM baseline improves on in-domain classifi-
cation. mSDA does better than the baseline for all
domains except dvd. Using a larger training set,
along with mSDA, which makes use of the un-
labeled data, leads to the best results for citation
classification.
In domain adaptation we would expect the do-
mains most similar to the target to lead to the
highest results. Like Dai et al (2007), we mea-
sure the Kullback-Leibler divergence between the
source and target domains? distributions. Accord-
ing to this measure, citations are most similar to
the books domain. Therefore, it is not surprising
that training on books performs well on citations,
and intuitively, among the domains in the Amazon
dataset, a book review is most similar to a scien-
tific citation. This makes the good mSDA results
for electronics a bit more surprising.
4.2 Easy Domain Adaptation
The results in Section 4.1 are for semi-supervised
domain adaptation: the case where we have some
large annotated corpus (Amazon product reviews)
and a large unannotated corpus (citations). There
have been a number of other successful attempts at
fully supervised domain adaptation, where it is as-
sumed that some small amount of data is annotated
in the target domain (Chelba and Acero, 2004;
Daum?e III, 2007; Jiang and Zhai, 2007). To see
how mSDA compares to supervised domain adap-
tation we take the various approaches presented by
Daum?e III (2007). The results of this comparison
can be seen in Table 2. Briefly, ?All? trains on
source and target data; ?Weight? is the same as
?All? except that instances may be weighted dif-
ferently based on their domain (weights are chosen
on a development set); ?Pred? trains on the source
data, makes predictions on the target data, and
then trains on the target data with the predictions;
?LinInt? linearly interpolates predictions using the
source-only and target-only models (the interpola-
tion parameter is chosen on a development set);
?Augment? uses a larger feature set with source-
specific and target-specific copies of features; see
45
Domain Baseline All Weight Pred LinInt Augment mSDA
books 54.5 54.8 52.0 51.9 53.4 53.4 57.1
dvd 53.2 50.9 56.0 53.4 51.9 47.5 51.6
electronics 53.4 49.0 50.5 53.4 54.8 51.9 59.2
kitchen 47.9 48.8 50.7 53.4 52.6 49.2 50.1
citations 51.9 ? ? ? ? ? 54.9
Table 2: Macro-F
1
results on CITD using different domain adaptation approaches.
(Daum?e III, 2007) for further details.
We are only interested in citations as the tar-
get domain. Daum?e?s source-only baseline cor-
responds to the ?Baseline? column for domains:
books, dvd, electronics, and kitchen; while his
target-only baseline can be seen for citations in the
last row of the ?Baseline? column in Table 2.
The semi-supervised mSDA performs quite
well with respect to the fully supervised ap-
proaches, obtaining the best results for books and
electronics, which are also the highest scores over-
all. Weight and Pred have the highest F
1
scores for
dvd and kitchen respectively. Daum?e III (2007)
noted that the ?Augment? algorithm performed
best when the target-only results were better than
the source-only results. When this was not the
case in his experiments, i.e., for the treebank
chunking task, both Weight and Pred were among
the best approaches. In our experiments, training
on source-only outperforms target-only, with the
exception of the kitchen domain.
We have included the line for citations to see the
results training only on the target data (F
1
= 51.9)
and to see the improvement when using all of the
unlabeled data with mSDA (F
1
= 54.9).
4.3 Discussion
These results are very promising. Although they
are not quite as high as other published results
for citation polarity (Abu-Jbara et al, 2013)
7
, we
have shown that you can improve citation polarity
classification by leveraging large amounts of an-
notated data from other domains and using a sim-
ple set of features.
mSDA and fully supervised approaches can also
be straightforwardly combined. We do not present
those results here due to space constraints. The
7
Their work included a CRF model to identify the citation
context that gave them an increase of 9.2 percent F
1
over a
single sentence citation context. Our approach achieves sim-
ilar macro-F
1
on only the citation sentence, but using a dif-
ferent corpus.
combination led to mixed results: adding mSDA
to the supervised approaches tended to improve F
1
over those approaches but results never exceeded
the top mSDA numbers in Table 2.
5 Related Work
Teufel et al (2006b) introduced automatic citation
function classification, with classes that could be
grouped as positive, negative, and neutral. They
relied in part on a manually compiled list of cue
phrases that cannot easily be transferred to other
classification schemes or other scientific domains.
Athar (2011) followed this and was the first to
specifically target polarity classification on scien-
tific citations. He found that dependency tuples
contributed the most significant improvement in
results. Abu-Jbara et al (2013) also looks at both
citation function and citation polarity. A big con-
tribution of this work is that they also train a CRF
sequence tagger to find the citation context, which
significantly improves results over using only the
citing sentence. Their feature analysis indicates
that lexicons for negation, speculation, and po-
larity were most important for improving polarity
classification.
6 Conclusion
Robust citation classification has been hindered by
the relative lack of annotated data. In this pa-
per we successfully use a large, out-of-domain,
annotated corpus to improve the citation polarity
classification. Our approach uses a deep learning
neural network for domain adaptation with labeled
out-of-domain data and unlabeled in-domain data.
This semi-supervised domain adaptation approach
outperforms the in-domain citation polarity classi-
fication and other fully supervised domain adapta-
tion approaches.
Acknowledgments. We thank the DFG for
funding this work (SPP 1335 Scalable Visual An-
alytics).
46
References
Amjad Abu-Jbara, Jefferson Ezra, and Dragomir
Radev. 2013. Purpose and polarity of citation: To-
wards NLP-based bibliometrics. In Proceedings of
NAACL-HLT, pages 596?606.
Awais Athar and Simone Teufel. 2012. Context-
enhanced citation sentiment detection. In Proceed-
ings of NAACL-HLT, pages 597?601.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of ACL Student Session, pages 81?87.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational
linguistics. In Proceedings of LREC, pages 1755?
1759.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL, pages 440?447.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proceedings of EMNLP, pages 285?292.
Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-
berger, and Fei Sha. 2012. Marginalized denoising
autoencoders for domain adaptation. In Proceedings
of ICML, pages 767?774.
Daryl E. Chubin and Soumyo D. Moitra. 1975. Con-
tent analysis of references: Adjunct or alternative to
citation counting? Social Studies of Science, 5:423?
441.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive bayes classifiers for
text classification. In AAAI, pages 540?545.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256?263.
Cailing Dong and Ulrich Sch?afer. 2011. Ensemble-
style self-training on citation classification. In Pro-
ceedings of IJCNLP, pages 623?631.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Eugene Garfield. 1955. Citation indexes to science:
A new dimension in documentation through associ-
ation of ideas. Science, 122:108?111.
Eugene Garfield. 1964. Can citation indexing be au-
tomated? In Statistical Association Methods for
Mechanized Documentation, Symposium Proceed-
ings, pages 189?192.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of ICML, pages 513?520.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL,
pages 264?271.
Charles Jochim and Hinrich Sch?utze. 2012. Towards
a generic and flexible citation classifier based on
a faceted classification scheme. In Proceedings of
COLING, pages 1343?1358.
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Nitin Indurkhya and Fred J. Damerau, editors,
Handbook of Natural Language Processing, Second
Edition. CRC Press, Taylor and Francis Group.
Michael J. Moravcsik and Poovanalingam Murugesan.
1975. Some results on the function and quality of
citations. Social Studies of Science, 5:86?92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of IJCAI, pages 926?
931.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of COLING, pages 689?
696.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proceedings of CIKM, pages 213?222.
Henry G. Small and Belver C. Griffith. 1974. The
structure of scientific literatures I: Identifying and
graphing specialties. Science Studies, 4(1):17?40.
Henry Small and Richard Klavans. 2011. Identifying
scientific breakthroughs by combining co-citation
analysis and citation context. In Proceedings of In-
ternational Society for Scientometrics and Informet-
rics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
47
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006a. An annotation scheme for citation function.
In Proceedings of SIGdial Workshop on Discourse
and Dialogue, pages 80?87.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006b. Automatic classification of citation function.
In Proceedings of EMNLP, pages 103?110.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of ICML, pages 1096?1103.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347?354.
48
Proceedings of the ACL 2014 Student Research Workshop, pages 41?47,
Baltimore, Maryland USA, June 22-27 2014.
c?2014 Association for Computational Linguistics
An Exploration of Embeddings for Generalized Phrases
Wenpeng Yin and Hinrich Sch?utze
Center for Information and Language Processing
University of Munich, Germany
wenpeng@cis.lmu.de
Abstract
Deep learning embeddings have been suc-
cessfully used for many natural language
processing problems. Embeddings are
mostly computed for word forms although
lots of recent papers have extended this to
other linguistic units like morphemes and
word sequences. In this paper, we define
the concept of generalized phrase that in-
cludes conventional linguistic phrases as
well as skip-bigrams. We compute em-
beddings for generalized phrases and show
in experimental evaluations on corefer-
ence resolution and paraphrase identifica-
tion that such embeddings perform better
than word form embeddings.
1 Motivation
One advantage of recent work in deep learning on
natural language processing (NLP) is that linguis-
tic units are represented by rich and informative
embeddings. These embeddings support better
performance on a variety of NLP tasks (Collobert
et al, 2011) than symbolic linguistic representa-
tions that do not directly represent information
about similarity and other linguistic properties.
Embeddings are mostly derived for word forms al-
though a number of recent papers have extended
this to other linguistic units like morphemes (Lu-
ong et al, 2013), phrases and word sequences
(Socher et al, 2010; Mikolov et al, 2013).
1
Thus,
an important question is: what are the basic lin-
guistic units that should be represented by embed-
dings in a deep learning NLP system? Building
on the prior work in (Socher et al, 2010; Mikolov
et al, 2013), we generalize the notion of phrase to
include skip-bigrams (SkipBs) and lexicon entries,
1
Socher et al use the term ?word sequence?. Mikolov et
al. use the term ?phrase? for word sequences that are mostly
frequent continuous collocations.
where lexicon entries can be both ?continuous?
and ?noncontinuous? linguistic phrases. Exam-
ples of skip-bigrams at distance 2 in the sentence
?this tea helped me to relax? are: ?this*helped?,
?tea*me?, ?helped*to? . . . Examples of linguistic
phrases listed in a typical lexicon are continuous
phrases like ?cold cuts? and ?White House? that
only occur without intervening words and discon-
tinous phrases like ?take over? and ?turn off? that
can occur with intervening words. We consider
it promising to compute embeddings for these
phrases because many phrases, including the four
examples we just gave, are noncompositional or
weakly compositional, i.e., it is difficult to com-
pute the meaning of the phrase from the meaning
of its parts. We write gaps as ?*? for SkipBs and
? ? for phrases.
We can approach the question of what basic
linguistic units should have representations from
a practical as well as from a cognitive point of
view. In practical terms, we want representations
to be optimized for good generalization. There
are many situations where a particular task involv-
ing a word cannot be solved based on the word
itself, but it can be solved by analyzing the con-
text of the word. For example, if a coreference
resolution system needs to determine whether the
unknown word ?Xiulan? (a Chinese first name)
in ?he helped Xiulan to find a flat? refers to an
animate or an inanimate entity, then the SkipB
?helped*to? is a good indicator for the animacy of
the unknown word ? whereas the unknown word
itself provides no clue.
From a cognitive point of view, it can be argued
that many basic units that the human cognitive sys-
tem uses have multiple words. Particularly con-
vincing examples for such units are phrasal verbs
in English, which often have a non-compositional
meaning. It is implausible to suppose that we
retrieve atomic representations for, say, ?keep?,
?up?, ?on? and ?from? and then combine them to
41
form the meanings of the expressions ?keep your
head up,? ?keep the pressure on,? ?keep him from
laughing?. Rather, it is more plausible that we rec-
ognize ?keep up?, ?keep on? and ?keep from? as
relevant basic linguistic units in these contexts and
that the human cognitive systems represents them
as units.
We can view SkipBs and discontinuous phrases
as extreme cases of treating two words that do not
occur next to each other as a unit. SkipBs are de-
fined purely statistically and we will consider any
pair of words as a potential SkipB in our exper-
iments below. In contrast, discontinuous phrases
are well motivated. It is clear that the words
?picked? and ?up? in the sentences ?I picked it
up? belong together and form a unit very similar to
the word ?collected? in ?I collected it?. The most
useful definition of discontinuous units probably
lies in between SkipBs and phrases: we definitely
want to include all phrases, but also some (but not
all) statistical SkipBs. The initial work presented
in this paper may help in finding a good ?compro-
mise? definition.
This paper contributes to a preliminary inves-
tigation of generalized phrase embeddings and
shows that they are better suited than word em-
bedding for a coreference resolution classification
task and for paraphrase identification. Another
contribution lies in that the phrase embeddings we
release
2
could be a valuable resource for others.
The remainder of this paper is organized as fol-
lows. Section 2 and Section 3 introduce how to
learn embeddings for SkipBs and phrases, respec-
tively. Experiments are provided in Section 4.
Subsequently, we analyze related work in Section
5, and conclude our work in Section 6.
2 Embedding learning for SkipBs
With English Gigaword Corpus (Parker et al,
2009), we use the skip-gram model as imple-
mented in word2vec
3
(Mikolov et al, 2013) to in-
duce embeddings. Word2vec skip-gram scheme is
a neural network language model, using a given
word to predict its context words within a window
size. To be able to use word2vec directly with-
out code changes, we represent the corpus as a
sequence of sentences, each consisting of two to-
kens: a SkipB and a word that occurs between the
2
http://www.cis.lmu.de/pub/
phraseEmbedding.txt.bz2
3
https://code.google.com/p/word2vec/
two enclosing words of the SkipB. The distance
k between the two enclosing words can be var-
ied. In our experiments, we use either distance
k = 2 or distance 2 ? k ? 3. For example, for
k = 2, the trigramw
i?1
w
i
w
i+1
generates the sin-
gle sentence ?w
i?1
*w
i+1
w
i
?; and for 2 ? k ? 3,
the fourgram w
i?2
w
i?1
w
i
w
i+1
generates the
four sentences ?w
i?2
*w
i
w
i?1
?, ?w
i?1
*w
i+1
w
i
?,
?w
i?2
*w
i+1
w
i?1
? and ?w
i?2
*w
i+1
w
i
?.
In this setup, the middle context of SkipBs are
kept (i.e., the second token in the new sentences),
and the surrounding context of words of original
sentences are also kept (i.e., the SkipB in the new
sentences). We can run word2vec without any
changes on the reformatted corpus to learn embed-
dings for SkipBs. As a baseline, we run word2vec
on the original corpus to compute embeddings for
words. Embedding size is set to 200.
3 Embedding learning for phrases
3.1 Phrase collection
Phrases defined by a lexicon have not been deeply
investigated before in deep learning. To collect
canonical phrase set, we extract two-word phrases
defined in Wiktionary
4
, and two-word phrases de-
fined in Wordnet (Miller and Fellbaum, 1998) to
form a collection of size 95218. This collection
contains phrases whose parts always occur next to
each other (e.g., ?cold cuts?) and phrases whose
parts more often occur separated from each other
(e.g., ?take (something) apart?).
3.2 Identification of phrase continuity
Wiktionary and WordNet do not categorize
phrases as continuous or discontinous. So we need
a heuristic for determining this automatically.
For each phrase ?A B?, we compute
[c
1
, c
2
, c
3
, c
4
, c
5
] where c
i
, 1 ? i ? 5, indi-
cates there are c
i
occurrences of A and B in that
order with a distance of i. We compute these
statistics for a corpus consisting of Gigaword
and Wikipedia. We set the maximal distance
to 5 because discontinuous phrases are rarely
separated by more than 5 tokens.
If c
1
is 10 times higher than (c
2
+c
3
+c
4
+c
5
)/4,
we classify ?A B? as continuous, otherwise as dis-
continuous. Taking phrase ?pick off? as an ex-
ample, it gets vector [1121, 632, 337, 348, 4052],
c
1
(1121) is smaller than the average 1342.25, so
4
http://en.wiktionary.org/wiki/
Wiktionary:Main_Page
42
?pick off? is set as ?discontinuous?. Further con-
sider ?Cornell University? which gets [14831, 16,
177, 331, 3471], satisfying above condition, hence
it is treated as a continuous phrase.
3.3 Sentence reformatting
Given the continuity information of phrases,
sentence ?? ? ?A ? ? ?B ? ? ? ? is reformated into
?? ? ?A B ? ? ?A B ? ? ? ? if ?A B? is a discontinu-
ous phrase and is separated by maximal 4 words,
and sentence ?? ? ?AB ? ? ? ? into ?? ? ?A B ? ? ? ? if
?A B? is a continuous phrase.
In the first case, we use phrase ?A B? to replace
each of its component words for the purpose of
making the context of both constituents available
to the phrase in learning. For the second situation,
it is natural to combine the two words directly to
form an independent semantic unit.
Word2vec is run on the reformatted corpus to
learn embeddings for both words and phrases.
Embedding size is also set to 200.
3.4 Examples of phrase neighbors
Usually, compositional methods for learning rep-
resentations of multi-word text suffer from the dif-
ficulty in integrating word form representations,
like word embeddings. To our knowledge, there is
no released embeddings which can directly facil-
itate measuring the semantic affinity between lin-
guistic units of arbitrary lengths. Table 1 attempts
to provide some nearest neighbors for given typ-
ical phrases to show the promising perspective
of our work. Note that discontinuous phrases
like ?turn off? have plausible single word nearest
neighbors like ?unplug?.
4 Experiments
Our motivation for generalized phrases in Sec-
tion 1 was that they can be used to infer the at-
tributes of the context they enclose and that they
can capture non-compositional semantics. Our hy-
pothesis was that they are more suitable for this
than word embeddings. In this section we carry
out two experiments to test this hypothesis.
4.1 Animacy classification for markables
A markable in coreference resolution is a linguis-
tic expression that refers to an entity in the real
world or another linguistic expression. Examples
of markables include noun phrases (?the man?),
named entities (?Peter?) and nested nominal ex-
pressions (?their?). We address the task of ani-
macy classification of markables: classifying them
as animate/inanimate. This feature is useful for
coreference resolution systems because only ani-
mate markables can be referred to using masculine
and feminine pronouns in English like ?him? and
?she?. Thus, this is an important clue for automat-
ically clustering the markables of a document into
correct coreference chains.
To create training and test sets, we extract all
39,689 coreference chains from the CoNLL2012
OntoNotes corpus.
5
We label chains that con-
tain an animate pronoun markable (?she?, ?her?,
?he?, ?him? or ?his?) and no inanimate pronoun
markable (?it? or ?its?) as animate; and chains
that contain an inanimate pronoun markable and
no animate pronoun markable as inanimate. Other
chains are discarded.
We extract 39,942 markables and their contexts
from the 10,361 animate and inanimate chains.
The context of a markable is represented as a
SkipB: it is simply the pair of the two words occur-
ring to the left and right of the markable. The gold
label of a markable and its SkipB is the animacy
status of its chain: either animate or inanimate. We
divide all SkipBs having received an embedding in
the embedding learning phase into a training set of
11,301 (8097 animate, 3204 inanimate) and a bal-
anced test set of 4036.
We use LIBLINEAR (Fan et al, 2008) for clas-
sification, with penalty factors 3 and 1 for inan-
imate and animate classes, respectively, because
the training data are unbalanced.
4.1.1 Experimental results
We compare the following representations for an-
imacy classification of markables. (i) Phrase em-
bedding: Skip-bigram embeddings with skip dis-
tance k = 2 and 2 ? k ? 3; (ii) Word em-
bedding: concatenation of the embeddings of the
two enclosing words where the embeddings are
either standard word2vec embeddings (see Sec-
tion 2) or the embeddings published by (Collobert
et al, 2011);
6
(iii) the one-hot vector representa-
tion of a SkipB: the concatentation of two one-hot
vectors of dimensionality V where V is the size
of the vocabulary. The first (resp. second) vector
5
http://conll.cemantix.org/2012/data.
html
6
http://metaoptimize.com/projects/
wordreprs/
43
turn off caught up take over macular degeneration telephone interview
switch off mixed up take charge eye disease statement
unplug entangled replace diabetic retinopathy interview
turning off involved take control cataracts conference call
shut off enmeshed stay on periodontal disease teleconference
block out tangled retire epilepsy telephone call
turned off mired succeed glaucoma told
fiddle with engaged step down skin cancer said
Table 1: Phrases and their nearest neighbors
is the one-hot vector for the left (resp. right) word
of the SkipB. Experimental results are shown in
Table 2.
representation accuracy
phrase embedding
k = 2 0.703
2 ? k ? 3 0.700
word embedding
word2vec 0.668*
?
Collobert et al 0.662*
?
one-hot vectors 0.638*
?
Table 2: Classification accuracy. Mark ?*? means
significantly lower than ?phrase embedding?, k =
2; ??? means significantly lower than ?phrase em-
bedding?, 2 ? k ? 3. As significance test, we use
the test of equal proportion, p < .05, throughout.
The results show that phrase embeddings have
an obvious advantage in this classification task,
both for k = 2 and 2 ? k ? 3. This validates
our hypothesis that learning embeddings for dis-
continuous linguistic units is promising.
In our error analysis, we found two types of
frequent errors. (i) Unspecific SkipBs. Many
SkipBs are equally appropriate for animate and
inanimate markables. Examples of such SkipBs
include ?take*in? and ?then*goes?. (ii) Untypical
use of specific SkipBs. Even SkipBs that are spe-
cific with respect to what type of markable they
enclose sometimes occur with the ?wrong? type
of markable. For example, most markables oc-
curring in the SkipB ?of*whose? are animate be-
cause ?whose? usually refers to an animate mark-
able. However, in the context ?. . . the southeast-
ern area of Fujian whose economy is the most ac-
tive? the enclosed markable is Fujian, a province
of China. This example shows that ?whose? occa-
sionally refers to an inanimate entity even though
these cases are infrequent.
4.1.2 Nearest neighbors of SkipBs
Table 3 shows some SkipBs and their nearest
neighbors in descending order, where similarity is
computed with cosine measure.
A general phenomenon is that phrase embed-
dings capture high degree of consistency in infer-
ring the attributes of enclosed words. Considering
the neighbor list in the first column, we can esti-
mate that a verb probably appears as the middle
token. Furthermore, noun, pronoun, adjective and
adverb can roughly be inferred for the remaining
columns, respectively.
7
4.2 Paraphrase identification task
Paraphrase identification depends on semantic
analysis. Standard approaches are unlikely to as-
sign a high similarity score to the two sentences
?he started the machine? and ?he turned the ma-
chine on?. In our approach, embedding of the
phrase ?turned on? can greatly help us to infer cor-
rectly that the sentences are paraphrases. Hence,
phrase embeddings and in particular embeddings
of discontinuous phrases seem promising in para-
phrase detection task.
We use theMicrosoft Paraphrase Corpus (Dolan
et al, 2004) for evaluation. It consists of a training
set with 2753 true paraphrase pairs and 1323 false
paraphrase pairs, along with a test set with 1147
true and 578 false pairs. After discarding pairs
in which neither sentence contains phrases, 3027
training pairs (2123 true vs. 904 false) and 1273
test pairs (871 true vs. 402 false) remain.
7
A reviewer points out that this is only a suggestive anal-
ysis and that corpus statistics about these contexts would be
required to establish that phrase embeddings can predict part-
of-speech with high accuracy.
44
who*afghanistan, some*told women*have with*responsibility he*worried
had*afghanistan other*told men*have of*responsibility she*worried
he*afghanistan two*told children*have and*responsibility was*worried
who*iraq ?*told girls*have ?*responsibility is*worried
have*afghanistan but*told parents*have that*responsibility said*worried
fighters*afghanistan one*told students*have ?s*responsibility that*worried
who*kosovo because*told young*have the* responsibility they*worried
was*afghanistan and*told people*have for*responsibility ?s*worried
Table 3: SkipBs and their nearest neighbors
We tackle the paraphrase identification task via
supervised binary classification. Sentence repre-
sentation equals to the addition over all the to-
ken embeddings (words as well as phrases). A
slight difference is that when dealing with a sen-
tence like ?? ? ?A B ? ? ?A B ? ? ? ? we only consider
?A B? embedding once. The system ?word em-
bedding? is based on the embeddings of single
words only. Subsequently, pair representation is
derived by concatenating the two sentence vectors.
This concatentation is then classified by LIBLIN-
EAR as ?paraphrase? or ?no paraphrase?.
4.2.1 Experimental results and analysis
Table 4 shows the performance of two methods.
Phrase embeddings are apparently better. Most
work on paraphrase detection has devised intri-
cate features and achieves performance numbers
higher than what we report here (Ji and Eisenstein,
2013; Madnani et al, 2012; Blacoe and Lapata,
2012). Our objective is only to demonstrate the
superiority of considering phrase embedding over
merely word embedding in this standard task.
We are interested in how phrase embeddings
make an impact on this task. To that end, we per-
form an analysis on test examples where word em-
beddings are better than phrase embeddings and
vice versa.
Table 5 shows four pairs, of which ?phrase em-
bedding? outperforms ?word embedding? in the
Methods Accuracy F1
baseline 0.684 0.803
word embedding 0.695 0.805
phrase embedding 0.713 0.812
Table 4: Paraphrase task results.
first two examples, ?word embedding? defeats
?phrase embedding? in the last two examples. In
the first pair, successful phrase detection enables
to split sentences into better units, thus the gener-
ated representation can convey the sentence mean-
ing more exactly.
The meaning difference in the second pair orig-
inates from the synonym substitution between
?take over as chief financial officer? and ?fill
the position?. The embedding of the phrase
?take over? matches the embedding of the single
word ?fill? in this context.
?Phrase embedding? in the third pair suffers
from wrong phrase detection. Actually, ?in? and
?on? can not be treated as a sound phrase in that
situation even though ?in on? is defined by Wik-
tionary. Indeed, this failure, to some extent, re-
sults from the shortcomings of our method in dis-
covering true phrases. Furthermore, figuring out
whether two words are a phrase might need to
analyse syntactic structure in depth. This work is
directly based on naive intuitive knowledge, acting
as an initial exploration. Profound investigation is
left as future work.
Our implementation discovers the contained
phrases in the fourth pair perfectly. Yet, ?word em-
bedding? defeats ?phrase embedding? still. The
pair is not a paraphrase partly because the numbers
are different; e.g., there is a big difference between
?5.8 basis points? and ?50 basis points?. Only a
method that can correctly treat numerical informa-
tion can succeed here. However, the appearance of
phrases ?central bank?, ?interest rates? and ?ba-
sis points? makes the non-numerical parts more
expressive and informative, leading to less dom-
inant for digital quantifications. On the contrary,
though ?word embedding? fails to split the sen-
45
GWP sentence 1 sentence 2
1 0 1 Common side effects include
nasal congestion, runny nose, sore throat
and cough, the FDA said .
The most common side effects after get-
ting the nasal spray were nasal congestion,
runny nose, sore throat and cough .
1 0 1 Douglas Robinson, a senior vice president
of finance, will take over as chief financial
officer on an interim basis .
Douglas Robinson, CA senior
vice president, finance, will fill the
position in the interim .
1 1 0 They were being held Sunday in the Camden
County Jail on $ 100,000 bail each .
The Jacksons remained in on Camden
County jail $ 100,000 bail .
0 0 1 The interest rate sensitive two year Schatz
yield was down 5.8 basis points at 1.99 per-
cent .
The Swedish central bank cut inter-
est rates by 50 basis points to 3.0 percent
.
Table 5: Four typical sentence pairs in which the predictions of word embedding system and phrase
embedding system differ. G = gold annotation, W = prediction of word embedding system, P = prediction
of phrase embedding system. The formatting used by the system is shown. The original word order of
sentence 2 of the third pair is ?? ? ? in Camden County jail on $ 100,000 bail?.
tences into better units, it weakens unexpectedly
the expressiveness of subordinate context. This
example demonstrates the difficulty of paraphrase
identification. Differing from simple similarity
tasks, two sentences are often not paraphrases
even though they may contain very similar words.
5 Related work
To date, approaches to extend embedding (or
more generally ?representation?) beyond individ-
ual words are either compositional or holistic
(Turney, 2012).
The best known work along the first line is by
(Socher et al, 2010; Socher et al, 2011; Socher
et al, 2012; Blacoe and Lapata, 2012), in which
distributed representations of phrases or even sen-
tences are calculated from the distributed repre-
sentations of their parts. This approach is only
plausible for units that are compositional, i.e.,
whose properties are systematically predictable
from their parts. As well, how to develop a ro-
bust composition function still faces big hurdles;
cf. Table 5.1 in (Mitchell and Lapata, 2010). Our
approach (as well as similar work on continuous
phrases) makes more sense for noncompositional
units.
Phrase representations can also be derived by
methods other than deep learning of embed-
dings, e.g., as vector space representations (Tur-
ney, 2012; Turney, 2013; Dinu et al, 2013). The
main point of this paper ? generalizing phrases to
discontinuous phrases and computing representa-
tions for them ? is orthogonal to this issue. It
would be interesting to evaluate other types of rep-
resentations for generalized phrases.
6 Conclusion and Future Work
We have argued that generalized phrases are part
of the inventory of linguistic units that we should
compute embeddings for and we have shown that
such embeddings are superior to word form em-
beddings in a coreference resolution task and stan-
dard paraphrase identification task.
In this paper we have presented initial work on
several problems that we plan to continue in the
future: (i) How should the inventory of continu-
ous and discontinous phrases be determined? We
used a purely statistical definition on the one hand
and dictionaries on the other. A combination of
the two methods would be desirable. (ii) How can
we distinguish between phrases that only occur in
continuous form and phrases that must or can oc-
cur discontinuously? (iii) Given a sentence that
contains the parts of a discontinuous phrase in cor-
rect order, how do we determine that the cooccur-
rence of the two parts constitutes an instance of
the discontinuous phrase? (iv) Which tasks benefit
most significantly from the introduction of gener-
alized phrases?
Acknowledgments
This work was funded by DFG (grant SCHU
2246/4). We thank Google for a travel grant to
support the presentation of this paper.
46
References
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Workshop on Continuous Vector Space
Models and their Compositionality, pages 50?58.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th international conference
on Computational Linguistics, page 350. Associa-
tion for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimi-
native improvements to distributional sentence sim-
ilarity. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 891?896.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Conference on Computa-
tional Natural Language Learning, pages 104?113.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182?190. Asso-
ciation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. arXiv preprint arXiv:1310.4546.
George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388?1429.
Robert Parker, Linguistic Data Consortium, et al
2009. English gigaword fourth edition. Linguistic
Data Consortium.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1?9.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211.
Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Peter D Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. Transactions of the Association for
Computational Linguistics, 1:353?366.
47
