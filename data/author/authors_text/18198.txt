Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 424?433,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Dependency Tree Abstraction for Long-Distance Reordering in
Statistical Machine Translation
Chenchen Ding
Department of Computer Science
University of Tsukuba
1-1-1 Tennodai, Tsukuba, Ibaraki , Japan
tei@mibel.cs.tsukuba.ac.jp
Yuki Arase
Microsoft Research
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
yukiar@microsoft.com
Abstract
Word reordering is a crucial technique
in statistical machine translation in which
syntactic information plays an important
role. Synchronous context-free gram-
mar has typically been used for this pur-
pose with various modifications for adding
flexibilities to its synchronized tree gen-
eration. We permit further flexibilities
in the synchronous context-free grammar
in order to translate between languages
with drastically different word order. Our
method pre-processes a parallel corpus by
abstracting source-side dependency trees,
and performs long-distance reordering on
top of an off-the-shelf phrase-based sys-
tem. Experimental results show that our
method significantly outperforms previous
phrase-based and syntax-based models for
translation between English and Japanese.
1 Introduction
Since the inception of statistical machine trans-
lation (SMT), long-distance word reordering has
been a notable challenge, particularly when trans-
lating between languages with drastically different
word orders, such as subject-verb-object (SVO)
and subject-object-verb (SOV) languages like En-
glish and Japanese, respectively. Phrase-based
models (Koehn et al., 2003; Och and Ney, 2004;
Xiong et al., 2006) have been strong in local
translation and reordering. However, phrase-based
models cannot effectively conduct long-distance
reordering because they are based purely on statis-
tics of syntax-independent phrases. As a comple-
mentary approach to phrase-based models, some
researchers have incorporated syntactic informa-
tion into an SMT framework (Wu, 1997; Yamada
and Knight, 2001; Liu et al., 2006) using syn-
chronous context-free grammar (SCFG) (Aho and
when the fluid pressure cylinder 31 is used, fluid is gradually applied.
,
is
gradually
applied. fluid
when 
is
used
[N]
the 
fluid
pressure
cylinder
31
[N]
[X]
[X]
(root)
Figure 1: English abstraction tree example
Ullman, 1972). The original SCFG assumes that
the syntactic trees of the source and target lan-
guages can be derived synchronously. However,
this assumption is too strict for handling paral-
lel sentences that are often comparable rather than
parallel. For alleviating this assumption, some re-
searchers have added flexibilities in synchronized
tree generation (Wu, 1997; Burkett et al., 2010).
In addition, in the SMT framework, there is an
approach that alleviates the assumption by only
generating the source-side syntactic tree and pro-
jecting it to the target-side sentence (Yamada and
Knight, 2001; Liu et al., 2006).
In practice, these existing methods are not flex-
ible enough to handle parallel sentence pairs, es-
pecially those of SVO and SOV languages. There-
fore, we permit further flexibility in SCFG aiming
to effectively conduct long-distance reordering.
We design our method as a pre-processing proce-
dure so that we can use a well-developed phrase-
based system without adding heavy computational
complexity to the system. Specifically, we propose
an abstraction tree that is a shallow and nested
representation, i.e., abstraction of the dependency
tree as Fig. 1 depicts. Our method pre-processes a
parallel corpus by generating source-side abstrac-
tion trees and projecting the trees onto the target-
side sentences. It then decomposes the corpus
by collecting corresponding node pairs as a new
corpus, and finally trains the phrase-based model.
In this manner, the source-side grammar is deter-
mined on the fly for each sentence based on a de-
424
pendency parse of the source sentence. The target
side of each production in the grammar is deter-
mined by running the phrase-based decoder.
We empirically show effectiveness of our
method for English-to-Japanese and Japanese-to-
English translations by comparing it to phrase-
based and syntax-based models. Experimental re-
sults show that our method significantly outper-
forms the previous methods with respect to the
BLEU (Papineni et al., 2002) metric.
2 Related Work
For adding flexibilities to SCFG under an SMT
scenario, previous studies generate only a source-
side syntactic tree and project it to the target-side
sentence regardless of the true target-side syntactic
structure. Liu et al. (2006) propose a tree-to-string
model using a source-side constituency tree to ex-
tract correspondences between the source-side tree
and the target-side sentence. Quirk et al. (2005)
and Xie et al. (2011) use a dependency tree for the
same purpose. Since these methods project a fine-
grained source-side syntax tree, an accurate pro-
jection is possible only when the target-side sen-
tence has a syntactic structure that is similar to
the source-side. Zhu and Xiao (2011) and Huang
and Pendus (2013) generalize rules obtained by
the tree-to-string model to increase the chance of
rule matching at decoding. Despite their merits,
none of these methods resolves the problem of tree
projection to the target-side.
The hierarchical phrase-based model (HIERO)
proposed by Chiang (2007) is independent of any
syntactic information and generates SCFG rules
only from parallel sentence pairs. Li et al. (2012)
and Feng et al. (2012) incorporate syntactic infor-
mation into HIERO as soft constraints. Since these
methods are bound by the original HIERO rules
that are independent of syntactic information, their
rules cannot represent the global syntactic struc-
ture of a sentence.
There are also pre-reordering methods for long-
distance reordering in SVO-to-SOV translations
using heuristics designed based on source-side
syntactic structures (Xu et al., 2009; Isozaki et al.,
2010; Isozaki et al., 2012). They are fine-tuned to
handle only specific reordering problems in a pre-
determined language pair. Another approach is to
statistically learn pre-reordering rules from a cor-
pus; however, this requires a highly parallel train-
ing corpus consisting of literal translations to learn
Algorithm 1 CKY-style decoding
Input: Input sentence u and its dependency tree r
u
, transla-
tion model TM , block-LM bLM , sentence-LM sLM ,
size of m-best m
1: ?
u
? generate abstraction tree of u using r
u
2: NodeTrans[][]? ?
3: for all node in ?
u
do
4: m-best? Decode(node, TM, bLM,m)
5: (start, end)? start and end indices of node in u
6: NodeTrans[start][end]? m-best
7: end for
8: for start := 1 to |u| do
9: for span := 0 to |u| ? 1 do
10: end? start+ span
11: ChildTrans[]? ?
12: for all (i, j) such that start ? i ? j ? end do
13: if NodeTrans[i][j] 6= ? then
14: add NodeTrans[i][j] to ChildTrans
15: end if
16: end for
17: CubePruning(NodeTrans[start][end],
ChildTrans, sLM,m)
18: end for
19: end for
effective rules (Neubig et al., 2012; Navratil et al.,
2012). Such a training dataset is not widely avail-
able in many languages.
3 Overview of the Proposed Method
Our method pre-processes sentences in a paral-
lel corpus based on source-side abstraction trees.
It first generates an abstraction tree ?
s
of a source-
side sentence s by abstracting its dependency tree
r
s
: (s, r
s
) ? ?
s
. It then projects the tree to the
target-side sentence t for generating a target-side
abstraction tree ?
t
that has exactly the same struc-
ture to ?
s
, i.e., (?
s
, t) ? ?
t
. The abstraction-
tree generation process can be adapted to translat-
ing languages by specifying source-side part-of-
speeches (POSs) as input. Abstraction tree struc-
tures also depend on the dependency grammar that
a parser uses. In this study, we assume commonly
used Stanford typed dependency (de Marneffe et
al., 2006) for English and the chunk-based depen-
dency with ipadic (Asahara and Matsumoto, 2003)
for Japanese. Investigation of effects of different
dependency grammars is our future work.
We decompose the sentence pair into node pairs
according to correspondences between the source
and target abstraction trees, and generate a new
corpus referred to as a block-corpus (Fig. 6). Us-
ing the block-corpus and the original corpus, we
train a phrase-based model. Its translation model
is trained with the block-corpus, and two target-
side language models (LMs) are trained with the
block-corpus (referred to as block-LM) and the
425
a pressure cylinder usedhigh ishot water
Figure 2: [N] node detection example
original corpus (referred to as sentence-LM), re-
spectively. Thus the sentence-LM can be trained
using a larger-scale monolingual corpus. Com-
pared to previous methods that also decompose
sentence pairs (Xu et al., 2005; Sudoh et al., 2010),
our method is more syntax-oriented.
In decoding, we adopt the parsing algorithm
with cube-pruning (Huang and Chiang, 2005) into
a phrase-based decoder to translate the abstrac-
tion tree of an input sentence efficiently. As
Algorithm 1 shows, our decoder first generates
the abstraction tree of the input sentence (line
1), and independently translates each node us-
ing the block-LM that models ordering among
non-terminals and lexical words (line 3?7). It
then combines the m-best translation hypotheses
of each node to construct a sentence-level trans-
lation (line 8?19). Specifically, we insert sets of
the m-best translation hypotheses of child nodes
into the m-best hypotheses of their parent node
by replacing the corresponding non-terminals us-
ing the cube-pruning (line 17). The ordering of
these child nodes has been determined in their
parent node by the phrase-based model that re-
gards non-terminals only as single words. By
doing so, long-distance reordering is solved con-
sidering the global syntactic structure and con-
texts (lexical strings) preserved in the node. In
cube-pruning, we use the sentence-LM to com-
pose fluent sentence-level translation. The block-
LM and sentence-LM scores are treated as inde-
pendent features.
The computational complexity of our decoder
(line 3?19) is O(|N |C), where |N | is the number
of nodes in the abstraction-tree and C is a con-
stant representing the complexity of phrase-based
decoder and cube-pruning. Since combinations of
hypotheses in cube-pruning are determined by the
abstraction-tree in our method, the computational
cost is significantly smaller than HIERO?s case.
4 Abstraction Tree Generation
In this section, we provide the formal definition of
an abstraction tree and the generation method.
4.1 Definition of Abstraction Tree
We define an abstraction tree as ? = {N , E},
where N is a set of nodes and E is a set of
when is gradually applied .used ,is[N] [N]
Figure 3: [X] and [P] node detection. Since the word
?used? is a head, it and its governing span are detached from
the root ?applied? as a child node.
edges. For conducting abstraction based on syn-
tactic structures, we merge a span governed by a
dependency head as a node and represent it by a
non-terminal in a parent node. As a result, the i-th
nodeN
i
consists of a sequence of lexical words w
and non-terminals L that replace spans governed
by heads in the corresponding child nodes:
N
i
= {?|?
1
, . . . , ?
|N
i
|
}, ?
k
? {w,L}.
The edge E
ij
between a parent node N
i
and
its child node N
j
corresponds to a governor-
dependent relationship from the head in N
i
to its
dependent w
x
in N
j
. w
x
is another head and gov-
erns other words in N
j
. The span covered by N
j
is replaced by a non-terminal in N
i
.
We use three kinds of labels to represent L for
explicitly using syntactic information that is use-
ful for long-distance reordering; [N], [P], and [X]
according to the head in the corresponding node.
We label a child node [N] when its head word is
a noun and forms a base noun phrase, [P] when
its head word is an adposition
1
, and [X] for others
like verb phrases, conjunctive phrases, and rela-
tive phrases. These nodes play different roles in a
sentence. An [N] node, i.e., a base noun phrase,
adds context to a sentence. A [P] node depends
on other phrases and generally appears relatively
freely in a sentence. Thus, we assume that the [P]
node requires special reordering.
The abstraction tree depicted in Fig. 1 has a par-
ent [X] node ?when [N] is used? and its child [N]
node ?the fluid pressure cylinder 31.? The word
?used? governs ?cylinder? in the [N] node, and the
[N] node folds the context in the [X] node.
4.2 Tree Construction
We begin with detecting [N] nodes, then pro-
ceed to [P] and [X] nodes. These processes require
to specify source-side POSs as input for adapting
to translating languages. We finally flatten frag-
mented nodes.
For detecting [N] nodes, we take a POS of noun
as input and identify each noun and its governing
span, i.e., a string of all governed words, using the
source-side dependency tree as Fig. 2 shows. We
1
A preposition in English and a postposition in Japanese.
426
Algorithm 2 [P] and [X] node detection
Input: Source-side sentence s and its dependency tree r
s
,
POS list of adpositions PPos, [N] node list N -Nodes
Output: [P] and [X] nodes P -Nodes, X-Nodes
1: P -Nodes[]? ?, X-Nodes[]? ?
2: HeadList[]? root of r
s
3: repeat
4: head? pop node from HeadList
5: ChildList[]? all dependents of head
6: for all child in ChildList do
7: if child /? N -Nodes and child has dependents
then
8: add child to HeadList
9: remove child from ChildList
10: end if
11: end for
12: start? smallest start index of nodes in ChildList
13: end? largest end index of nodes in ChildList
14: if POS of head ? PPos then
15: add span [start, end] of s to P -Nodes
16: else
17: add span [start, end] of s to X-Nodes
18: end if
19: remove head from HeadList
20: until HeadList = ?
regard descendant dependents as being governed
by the noun for detecting a noun phrase of a com-
plete form. We extract the span as a node and re-
place it by an [N] label in the sentence.
Next, we identify [P] and [X] nodes given a list
of source-side POSs of adpositions as input. As
Algorithm 2 shows, after [N] node detection, we
trace the dependency tree from its root to leaves
(line 3?20). We find all the dependents to the root,
then check if each dependent is a head. If a de-
pendent of the root is a head and governs other
words, we detach the dependent to process later
(line 6?11). We then find the smallest start index
and largest end index of dependent words and set
the corresponding span as a node (if a dependent
is in an [N] node, we use the start and end indices
of the [N] node). Each node is labeled accord-
ing to the POS of its head as [P] or [X] (line 14?
18). We then take the detached dependent as a new
root and repeat the process until no more detach-
ment is possible. The computational complexity
is O(|s|
2
). Through this process, a span with di-
rect dependencies is extracted as a node, and other
spans with descendant dependencies become de-
scendant nodes, replaced by non-terminals in their
parent node as shown in Fig. 3.
4.3 Handling Complex Noun Phrase
As described in Sec. 4.2, we detect a noun phrase
as an [N] node. However, an [N] node be-
comes more complex than a base noun phrase
when the head governs a clause, such as a relative
the cyli n der that isusesmachine ??
[N] [X]original [N]
Figure 4: Handling a complex noun phrase
clause. Such a complex node may require long-
distance reordering of an inside clause when trans-
lating. Therefore, we separate the noun phrase and
clause. We take a POS list whose word can be a
head of [P] and [X] nodes (preposition, verb, to,
Wh-determiner/pronoun/adverb, and coordinating
conjunction for English) as input. If the POS of a
noun?s dependent is in the list, we detach the de-
pendency arc, and then re-attach the dependency
arc to the head of the noun. As a result, the base
noun phrase becomes an [N] node and its clause
becomes a [P] or [X] node that is transformed to a
sibling of the [N] node.
In Fig. 4, the word ?cylinder? in the original [N]
node has a relative clause and governs ?is.? We de-
tach the dependency arc and re-attach it to ?uses?
(the head of ?cylinder?), so that the noun phrase
and the clause become sibling [N] and [X] nodes.
4.4 Flattening Fragmented Nodes
The above processes are independent of the size
of each node, meaning they produce fragmented
nodes of only a few words. Such fragmented
nodes make the tree projection to the target-side
difficult. To solve this problem, we flatten the ab-
straction tree as shown in Algorithm 3. We pro-
cess an internal node in ?
s
from bottom to top. If
the covering span of an internal node is less than a
threshold ? ? N, its child nodes are merged (line 3
and 4, Algorithm 3). Specifically, we reinsert the
child nodes by replacing the corresponding non-
terminals with lexical strings that the child nodes
have been covered by. The computational cost is
O(|N |). We investigate the effect of ? in the fol-
lowing evaluation section (Table 2).
5 Abstraction Tree Projection
In this section, we describe a method for project-
ing the obtained source-side abstraction tree onto
the target-side sentence.
5.1 Tree Structure Projection
We use word alignment results for tree structure
projection. However, accurate word alignment
is challenging when handling language pairs in
which long-distance reordering is needed, and the
alignment noise propagates to the tree projection.
427
Algorithm 3 Tree flattening
Input: Abstraction tree ?
s
, threshold ?
Output: Flattened tree ?
?
s
1: for all internal node in ?
s
, from bottom to top do
2: (start, end)? start and end indices of node
3: if end? start+ 1 < ? then
4: ?
?
s
?MergeChildNodes(node, ?
s
)
5: end if
6: end for
To avoid this problem, we first omit alignment
links of function words whose alignment quality
tends to be lower than that of the content words.
We then complement the quality of word align-
ment by adapting the syntactic cohesion assump-
tion (Yamada and Knight, 2001) that assumes a
word string covered by a sub-tree of the source-
side syntactic tree corresponds to a string of con-
tiguous words in the target-side sentence. Follow-
ing the assumption, we project the k-th node of
the source-side abstraction tree N
(s)
k
to a string of
contiguous words in the target-side:
N
(s)
k
7? t
i
, . . . , t
j
, s.t. 1 ? i ? j ? |t|,
where t
i
is the i-th word in the target-side sentence
and |t| is the number of words in the sentence.
For each node of the source-side abstraction
tree, we first obtain its covering span. We then
define a vector c ? {0, 1}
n
whose elements repre-
sent word alignment links in a binary manner. If
and only if the i-th target word is aligned to a word
in the span, the i-th element of c becomes 1, oth-
erwise it is 0. Since the original word alignment
represented by the vector c may be noisy, we find
a vector c
?
? {0, 1}
n
that maximizes the syntac-
tic cohesion assumption. In c
?
, only consecutive
elements between two indices i and j are 1, and
others are 0. We derive such c
?
as follows:
C
min
(c) = {c
?
| argmin
c
?
?c
?
? c?}, (1)
s.t. ? i, j, 1 ? i ? j ? n and
c
?
k
=
{
1 i ? k ? j,
0 otherwise,
c
?
= argmax
c
?
?C
min
(c)
?c
?
?. (2)
The operator ? ? ? computes the Euclidean norm
of a vector and c
?
k
is the k-th element of a vector
c
?
. Finally, c
?
represents the best possible word
links that maximize the syntactic cohesion as-
sumption, i.e., the longest contiguous word string
in the target-side, and that are closest to the orig-
inal word alignment. Specifically, Eq. (1) deter-
mines vectors that have the smallest distance to
Algorithm 4 Tree projection
Input: Source-side abstraction tree ?
s
, target-side sentence
t, word alignmentAw between s and t
Output: Target-side abstraction tree ?
t
1: ?
t
[]? ?
2: remove links of function words inAw
3: for span := |s| ? 1 to 0 do
4: for start := 1 to |s| do
5: end? start+ span
6: if span [start, end] ? ?
s
then
7: c? GenerateV ector([start, end],Aw)
8: c? ? Solve(c)  Eq. (1) and Eq. (2)
9: (i, j)? start and end indices of c?
10: add span [i, j] of t as a node into ?
t
11: Aw ? UpdateWordAlignment(c, c?)
12: end if
13: end for
14: end for
the original vector c while satisfying the hard con-
straint, and Eq. (2) selects the one whose norm is
largest, i.e., a vector that has longest contiguous
word links to the target-side. For computational
efficiency, we use the greedy-search so that the
computational cost is O(|t|). When Eq. (2) has
multiple solutions, word links in these solutions
are equally likely, and thus we merge them into a
unique solution. Specifically, we take the union
of the solutions and find the smallest index i
l
and
largest index i
r
whose elements are 1. We then set
all elements between i
l
and i
r
to 1.
As Algorithm 4 shows, we conduct this pro-
cess in a top-down manner throughout the abstrac-
tion tree (line 3?14). When processing each node,
word alignment links are updated by overwriting
links in c with the ones in c
?
(line 11). The com-
putational cost isO(|N
(s)
||t|), where |N
(s)
| is the
number of nodes in ?
s
. Figure 5 shows a projection
example of a node. A node of ?when [N] is used?
covers a span of ?when the fluid pressure cylin-
der 31 is used.? The words in the span are aligned
to the 1st, 2nd, and 5th target words (chunks)
2
;
however, the link to the 5th target word (chunk)
is a mis-alignment. With the alignment vector c
of [1, 1, 0, 0,1, 0, 0], we can remove this misalign-
ment and derive c
?
of [1, 1, 0, 0,0, 0, 0].
5.2 Fixed-Expression Recovery
The abstraction tree generation and projection are
based on a dependency tree, and thus may over-
segment a fixed-expression, such as idioms and
multi-word expressions. Since a fixed-expression
composes a complete meaning using a contiguous
word string, splitting it into different nodes results
2
In Japanese, a unit of dependency is a chunk in general,
and thus we conduct chunking before projection.
428
Node:
Target 
sentence:
Span: when the pressure cylinder 31 usedfluid is
?? ? ???? 31 ? ?? ? ?? ? ?? ? ?? ? ??
?? ? ?? ?
usediswhen [N]
? = 1, 1, 0, 0, 1, 0, 0
Figure 5: Abstraction tree projection
in poor translation. To avoid this issue, we gener-
ate a list of fixed-expressions using conventional
methods (Evert, 2008) and force them to remain in
one node. On both the source and target abstrac-
tion trees, we recursively reinsert nodes to their
parent node when such a fixed-expression is over-
segmented and spread over multiple nodes.
5.3 Block-Corpus Construction
After tree structure projection, we extract cor-
responding node pairs as a block-corpus. Each
node pair has a form ??
s
,?
t
,AL?, where ?s ?
{?
s
, L}
n
represents the source-side node of
length n. It consists of a sequence of lexical
words in the source-side vocabulary ?
s
and non-
terminals L. ?
t
? {?
t
, L}
m
similarly represents
the target-side node of length m. AL preserves
correspondences between the non-terminals in the
source and target nodes.
Specifically, we extract a pair of leaf nodes as
a pair of lexical strings. As for internal nodes,
we use the same non-terminal labels appearing
in the source-side node at the target-side node.
Namely, the span covered by child nodes are re-
placed by corresponding non-terminal labels in the
source-side node. At the same time, we record the
correspondence between the non-terminals. Fig-
ure 6 shows an example of the block-corpus, in
which the boxed indices indicate correspondences
of non-terminals in the source and target nodes.
6 Evaluation
We evaluate our method in English-to-Japanese
(EJ) and Japanese-to-English (JE) translation
tasks, since long-distance reordering is a serious
problem in this language pair.
6.1 Experiment Corpus
We use NTCIR-7 PATMT (Fujii et al., 2008), a
publicly available standard evaluation dataset, for
EJ and JE machine translation. The dataset is con-
structed using English and Japanese patents and
consists of 1.8 million parallel sentence pairs for
training, 915 sentence pairs for development, and
1, 381 sentence pairs for testing. The development
Input parallel corpus
When the fluid pressure cylinder 31 
is used , fluid is gradually applied .
?? ? ???? 31 ? ?? ?
?? ? ?? ? ?? ? ??
?? ? ?? ?
Block-Corpus
[X] , [N]   is gradually applied . [X] [N]    ? ?? ? ?? ??? ?? ? ?? ?
when [N] is used [N]    ? ?? ?
the fluid pressure cylinder 31 ?? ? ???? 31
fluid ??
00 0 0
1 1
Figure 6: Block-corpus example. Boxed indices link non-
terminals in the source and target exemplars.
and test sets have one reference per sentence. This
dataset is bidirectional and can be used for both EJ
and JE translation evaluation.
6.2 Implementation of Proposed Method
We implement our method for EJ and JE
translation tasks. In both cases, we use an
in-house implementation of English POS tag-
ger (Collins, 2002) and a Japanese morpholog-
ical analyzer (Kudo et al., 2004) for tokeniza-
tion and POS tagging. As for EJ translation,
we use the Stanford parser (de Marneffe et al.,
2006) to obtain English abstraction trees. We
also use an in-house implementation of a Japanese
chunker (Kudo and Matsumoto, 2002) to obtain
chunks in Japanese sentences. We apply the chun-
ker just before tree projection for using a chunk
as a projection unit, since a chunk is the basic
unit in Japanese. As for JE translation, we use
a popular Japanese dependency parser (Kudo and
Matsumoto, 2002) to obtain Japanese abstraction
trees. We convert Japanese chunk-level depen-
dency tree to a word-level using a simple heuris-
tic. We use GIZA++ (Och and Ney, 2003) with the
grow-diag-final-and heuristic for word alignment.
We use an in-house implementation of
the bracketing transduction grammar (BTG)
model (Xiong et al., 2006) as the phrase-based
model that our method relies on for translation.
Non-terminals in our block-corpus are regarded
as a single word, and their alignments AL deter-
mined in the block-corpus are exclusively used to
align them. We set the maximum phrase length to
5 when training the translation model, since we
find that the performance is stable even setting
larger values as in (Koehn et al., 2003). We then
train the sentence-LM and block-LM using the
original corpus and the obtained block-corpus,
respectively. We ignore a sentence-end tag (</s>)
in the block-LM. With each corpus, we train a
5-gram LM using the SRI toolkit (Stolcke, 2002).
429
6.3 Comparison Method
Since our method pre-processes the parallel cor-
pus based on SCFG with increased flexibility and
trains a BTG model using the processed corpus,
we compare our method to another BTG model
trained only with the original corpus (simply re-
ferred to as the BTG model). We also com-
pare to the tree-to-string model and HIERO using
state-of-the-art implementations available in the
Moses system (Koehn et al., 2007), since they are
based on SCFG. The tree-to-string model requires
source-side constituency trees. For EJ transla-
tion, we use a state-of-the-art English constituency
parser (Miyao and Tsujii, 2005; Miyao and Tsujii,
2008). For JE translation, we transform a Japanese
dependency tree into a constituency tree using a
simple heuristic because there is no publicly avail-
able constituency parser. During the translation
model training, we use the same setting as our
method. In addition, we set the maximum span
of rule extraction to infinity for the tree-to-string
model and 10 for HIERO following Moses? de-
fault. We use the sentence-LM in these models as
they assume.
In addition, we compare our method to Head-
Finalization (Isozaki et al., 2010; Isozaki et al.,
2012) because it has achieved the best BLEU score
in EJ translation by handling long-distance re-
ordering. It is a specialized method to EJ trans-
lation, where a syntactic head in an English sen-
tence is reordered behind its constituents for com-
plying with the head-final nature of the Japanese
language. We pre-process the parallel corpus us-
ing the Head-Finalization and train a BTG model
using the same setting with our method to observe
the effect of different pre-processing methods.
During decoding, we set the translation table
size to 10 for each source string, and the stack
and beam sizes in the cube pruning to 100 for our
method (i.e., m-best = 100) and all other mod-
els. The maximum reordering span in the tree-to-
string model and HIERO is the same as the rule
extraction setting (infinity and 10, respectively).
We set the word reordering limit to infinity for our
method and the BTG model, while we set it to 3
for Head-Finalization as their papers report.
We tune feature weights by the minimum error
rate training (Och, 2003) to maximize the BLEU
score using the development set. As an evaluation
metric, we compute the BLEU score using the test
set, and all the scores discussed in Sec. 6.4 are the
Method EJ JE
Proposed method (? = 10) 31.78 28.55
BTG 28.82
??
26.98
??
HIERO 29.27
??
27.96
?
Tree-to-string 30.97
??
26.28
??
Head-Finalization 29.52
??
NA
Table 1: Test-set BLEU scores. The symbol
??
represents
a significant difference at the p < .01 level and
?
indicates a
significant difference at the p < .05 level against our method.
test-set BLEU scores. Significance tests are con-
ducted using bootstrap sampling (Koehn, 2004).
6.4 Result and Discussion
In this section, we present experimental results
and discuss them in detail.
Overall Performance Table 1 shows the BLEU
scores, in which our method significantly outper-
forms all other models for both EJ and JE transla-
tion tasks. These results indicate that our method
effectively incorporates syntactic information into
the phrase-based model and improves the transla-
tion quality.
For EJ translation, our method outperforms
the BTG model by 2.96, the HIERO by 2.51,
the tree-to-string model by 0.81, and the Head-
Finalization
3
by 2.26 in terms of BLEU score.
When we compare our method to the Head-
Finalization, both of them improve the BTG model
by pre-processing the parallel corpus. Moreover,
our method outperforms the Head-Finalization us-
ing richer syntactic information.
For JE translation, our method outperforms the
BTG model by 1.57, the HIERO by 0.59, and the
tree-to-string model by 2.27 in terms of BLEU
score. Our method and the tree-to-string model,
which depend on syntactic information, largely
outperform the BTG model and HIERO in EJ
translation. While the BTG model and HIERO,
which are independent of syntactic information,
outperform the tree-to-string model in JE trans-
lation. One reason for this phenomenon is that
English is a strongly configurational language that
has rigid word order while Japanese is an agglu-
tinative language that has relatively free word or-
der. A rigid syntactic structure provides solid clues
for word reordering when translated into a flexible
language, while a flexible structure provides weak
clues for fitting it to a rigid structure.
3
The BLEU score reported in this experiment differs from
their papers. This may be because they use a phrase-based
model in the Moses system, while we use the BTG model.
430
? EJ JE
BLEU height BLEU height
0 31.15 4.1 (1.5) 28.41 4.2 (1.4)
3 30.88 3.8 (1.7) 28.34 3.9 (1.6)
5 31.21 3.7 (1.5) 28.39 3.8 (1.5)
8 31.61 3.4 (1.4) 28.52 3.4 (1.4)
10 31.78 3.1 (1.3) 28.55 3.2 (1.3)
12 31.76 2.9 (1.3) 28.54 3.0 (1.3)
15 31.25 2.6 (1.2) 28.21 2.7 (1.2)
? 28.82 1.0 (?) 26.98 1.0 (?)
Table 2: Effect of threshold ?
Effect of Flattening Threshold Table 2 shows
BLEU scores when changing the flattening thresh-
old ? in our method, and averages and standard
deviations of the abstraction tree heights (? = ?
is equal to the BTG model). The performance im-
proves as we increase the threshold, i.e., increas-
ing the level of abstraction. Our method achieves
the best BLEU score when ? = 10 for both EJ and
JE translation, with the performance degrading as
we further increase the threshold.
This trend shows the trade-off between phrase-
based and syntax-based approaches. When the
threshold is too small, an abstraction tree be-
comes closer to the dependency tree and the tree-
projection becomes difficult. In addition, con-
text information becomes unavailable when con-
ducting long-distance reordering with a deep tree.
On the other hand, when setting the threshold too
large, the abstraction tree becomes too abstracted
and syntactic structures useful for long-distance
word reordering are lost. We need to balance these
effects by setting an appropriate threshold.
Effect of Non-Terminals and Fixed-Expressions
We change the kinds of non-terminal labels in an
abstraction tree to investigate their effect on the
translation quality. When we merge the [P] label
to the [X] label, i.e., use only [N] and [X] labels,
the BLEU score drops 0.40 in EJ translation while
the score is unaffected in JE translation. This is
because flexible Japanese syntax does not differ-
entiate postpositional phrases with others, while
English syntax prohibits such a flexibility.
When we merge all labels and only use the [X]
label, the BLEU score drops 0.57 in EJ transla-
tion and 0.43 in JE translation. This result sup-
ports our design of the abstraction tree that distin-
guishes non-terminals according to their different
functionalities in a sentence.
We also evaluate the effect of fixed-expressions
as described in Sec. 5.2. Results show a significant
change when over-splitting fixed-expressions; the
BLEU score drops 1.13 for EJ and 0.36 for JE
translation without reinserting fixed-expressions.
Method acceptable ? global ? local ?
Proposed 52 30 4
BTG 34 38 7
Tree-to-string 47 32 7
Table 3: Error distribution in 100 samples of EJ translation
Error Analysis We randomly sample 100 trans-
lation outputs per our method (? = 10), BTG, and
tree-to-string models for each EJ and JE transla-
tion tasks, and manually categorize errors based
on (Vilar et al., 2006). We focus primarily on
reordering errors and exclusively categorize the
samples into acceptable translations, translations
with only global or local reordering errors, as well
as others that are complicated combinations of var-
ious errors. An acceptable translation correctly
conveys the information in a source sentence even
if it contains minor grammatical errors.
Table 3 shows the distribution of acceptable
translations and those with global/local reordering
errors in the EJ task (results of JE task are omitted
due to the severe space limitation, but their trend
is similar). It confirms that our method reduces re-
ordering errors, not only for long-distance but for
local reordering, and increases the ratio of accept-
able translations compared to the BTG and tree-
to-string models. We also find that long-distance
reordering was attempted in 85, 66, and 70 sen-
tences by our method, BTG, and tree-to-string, re-
spectively, among these translations. The results
show that our method performs long-distance re-
ordering more frequently than others.
When we compare translations performed by
our method to those performed by the tree-to-
string model, we observe that their effectiveness
depends on a range of reordering. Our method is
effective in long-distance reordering like those of
clauses, while the tree-to-string model performs
middle-range reordering well. This is due to the
trade-off regarding the level of abstraction as dis-
cussed in the flattening threshold experiment.
7 Conclusion and Future Work
We have proposed an abstraction tree for effec-
tively conducting long-distance reordering using
an off-the-shelf phrase-based model. Evaluation
results show that our method outperforms conven-
tional phrase-based and syntax-based models.
We plan to investigate the effect of translating
language pairs and dependency grammars in ab-
straction tree generation. In addition, we will ap-
ply a structure-aware word aligner (Neubig et al.,
2011) to improve the tree projection.
431
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The the-
ory of parsing, translation, and compiling. Prentice-
Hall Inc.
Masayuki Asahara and Yuji Matsumoto. 2003.
ipadic version 2.7.0 user?s manual. http:
//sourceforge.jp/projects/ipadic/
docs/ipadic-2.7.0-manual-en.pdf.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchro-
nized grammars. In Proceedings of Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (NAACL-HLT 2010), pages 127?135.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1?8.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of International Conference on Lan-
guage Resources and Evaluation (LREC 2006),
pages 449?454.
Stefan Evert. 2008. Corpora and collocations. In Anke
L?udeling and Merja Kyt?o, editors, Corpus Linguis-
tics. An International Handbook, volume 2, chap-
ter 58. Mouton de Gruyter.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou,
and Qun Liu. 2012. Hierarchical chunk-to-string
translation. In Proceedings of Annual Meeting of
the Association for Computational Linguistics (ACL
2012), pages 950?958.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Pro-
ceedings of NTCIR-7 Workshop Meeting (NTCIR),
pages 389?400.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of International Workshop
on Parsing Technology (IWPT 2005), pages 53?64.
Fei Huang and Cezar Pendus. 2013. Generalized re-
ordering rules for improved SMT. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2013), pages 387?392.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings
of Joint Workshop on Statistical Machine Transla-
tion and Metrics MATR (WMT-MetricsMATR 2010),
pages 244?251.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2012. HPSG-based preprocessing
for English-to-Japanese translation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 11(3):8:1?8:16.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of Conference of the North American Chapter of
the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT 2003),
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2007),
pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of Conference on Natural Language Learn-
ing (CoNLL 2002), pages 1?7.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 230?237.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Head-driven hierarchical phrase-
based translation. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 33?37.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of International Con-
ference on Computational Linguistics and Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 609?616.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of Annual Meeting on Asso-
ciation for Computational Linguistics (ACL 2005),
pages 83?90.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Jiri Navratil, Karthik Visweswariah, and Ananthakrish-
nan Ramanathan. 2012. A comparison of syntac-
tic reordering methods for English-German machine
432
translation. In Proceedings of International Confer-
ence on Computational Linguistics (COLING 2012),
pages 2043?2058.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011.
An unsupervised model for joint phrase alignment
and extraction. In Proceedings of Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL-HLT 2011),
pages 632?641.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), pages
843?853.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
Annual Meeting on Association for Computational
Linguistics (ACL 2003), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005), pages 271?279.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP 2002), pages 901?904.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsu-
tomu Hirao, and Masaaki Nagata. 2010. Divide
and translate: Improving long distance reordering
in statistical machine translation. In Proceedings
of Joint Workshop on Statistical Machine Transla-
tion and Metrics MATR (WMT-MetricsMATR 2010),
pages 418?427.
David Vilar, Jia Xu, Luis Fernando d?Haro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 697?702.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2011), pages 216?226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of In-
ternational Conference on Computational Linguis-
tics and Annual Meeting on Association for Com-
putational Linguistics (COLING-ACL 2006), pages
521?528.
Jia Xu, Richard Zens, and Hermann Ney. 2005.
Sentence segmentation using IBM word alignment
model 1. In Proceedings of Annual Conference of
the European Association for Machine Translation
(EAMT 2005), pages 280?287.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT 2009),
pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of Annual Meeting on Association for Computa-
tional Linguistics (ACL 2001), pages 523?530.
Jingbo Zhu and Tong Xiao. 2011. Improving decoding
generalization for tree-to-string translation. In Pro-
ceedings of Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2001), pages 418?423.
433
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1597?1607,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Machine Translation Detection from Monolingual Web-Text
Yuki Arase
Microsoft Research Asia
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
yukiar@microsoft.com
Ming Zhou
Microsoft Research Asia
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
mingzhou@microsoft.com
Abstract
We propose a method for automatically
detecting low-quality Web-text translated
by statistical machine translation (SMT)
systems. We focus on the phrase salad
phenomenon that is observed in existing
SMT results and propose a set of computa-
tionally inexpensive features to effectively
detect such machine-translated sentences
from a large-scale Web-mined text. Un-
like previous approaches that require bilin-
gual data, our method uses only monolin-
gual text as input; therefore it is applicable
for refining data produced by a variety of
Web-mining activities. Evaluation results
show that the proposed method achieves
an accuracy of 95.8% for sentences and
80.6% for text in noisy Web pages.
1 Introduction
The Web provides an extremely large volume
of textual content on diverse topics and areas.
Such data is beneficial for constructing a large
scale monolingual (Microsoft Web N-gram Ser-
vices, 2010; Google N-gram Corpus, 2006) and
bilingual (Nie et al, 1999; Shi et al, 2006;
Ishisaka et al, 2009; Jiang et al, 2009) corpus
that can be used for training statistical models for
NLP tools, as well as for building a large-scale
knowledge-base (Suchanek et al, 2007; Zhu et al,
2009; Fader et al, 2011; Nakashole et al, 2012).
With recent advances in statistical machine trans-
lation (SMT) systems and their wide adoption in
Web services through APIs (Microsoft Translator,
2009; Google Translate, 2006), a large amount
of text in Web pages is translated by SMT sys-
tems. According to Rarrick et al (2011), their
Web crawler finds that more than 15% of English-
Japanese parallel documents are machine transla-
tion. Machine-translated sentences are useful if
they are of sufficient quality and indistinguish-
able from human-generated sentences; however,
the quality of these machine-translated sentences
is generally much lower than sentences generated
by native speakers and professional translators.
Therefore, a method to detect and filter such SMT
results is desired to best make use of Web-mined
data.
To solve this problem, we propose a method
for automatically detecting Web-text translated by
SMT systems1. We especially target machine-
translated text produced through the Web APIs
that is rapidly increasing. We focus on the phrase
salad phenomenon (Lopez, 2008), which char-
acterizes translations by existing SMT systems,
i.e., each phrase in a sentence is semantically
and syntactically correct but becomes incorrect
when combined with other phrases in the sentence.
Based on this trait, we propose features for eval-
uating the likelihood of machine-translated sen-
tences and use a classifier to determine whether
the sentence is generated by the SMT systems.
The primary contributions of the proposed
method are threefold. First, unlike previous stud-
ies that use parallel text and bilingual features,
such as (Rarrick et al, 2011), our method only
requires monolingual text as input. Therefore,
our method can be used in monolingual Web data
mining where bilingual information is unavailable.
Second, the proposed features are designed to be
computationally light so that the method is suit-
able for handling a large-scale Web-mined data.
Our method determines if an input sentence con-
tains phrase salads using a simple yet effective fea-
tures, i.e., language models (LMs) and automati-
cally obtained non-contiguous phrases that are fre-
quently used by people but difficult for SMT sys-
tems to generate. Third, our method computes fea-
tures using both human-generated text and SMT
1In this paper, the term machine-translated is used for in-
dicating translation by SMT systems.
1597
results to capture a phrase salad by contrasting
these features, which significantly improves detec-
tion accuracy.
We evaluate our method using Japanese and En-
glish datasets, including a human evaluation to as-
sess its performance. The results show that our
method achieves an accuracy of 95.8% for sen-
tences and 80.6% for noisy Web-text.
2 Related Work
Previous methods for detecting machine-
translated text are mostly designed for bilingual
corpus construction. Antonova and Misyurev
(2011) design a phrase-based decoder for
detecting machine-translated documents in
Russian-English Web data. By evaluating the
BLEU score (Papineni et al, 2002) of trans-
lated documents (by their decoder) against the
target-side documents, machine translation (MT)
results are detected. Rarrick et al (2011) extract a
variety of features, such as the number of tokens
and character types, from sentences in both the
source and target languages to capture words that
are mis-translated by MT systems. With these
features, the likelihood of a bilingual sentence
pair being machine-translated can be determined.
Confidence estimation of MT results is also
a related area. These studies aim to precisely
replicate human judgment in terms of the qual-
ity of machine-translated sentences based on fea-
tures extracted using a syntactic parser (Corston-
Oliver et al, 2001; Gamon et al, 2005; Avramidis
et al, 2011) or essay scoring system (Parton
et al, 2011), assuming that their input is al-
ways machine-translated. In contrast, our method
aims at making a binary judgment to distin-
guish machine-translated sentences from a mix-
ture of machine-translated and human-generated
sentences. In addition, although methods for
confidence estimation can assume sentences of a
known source language and reference translations
as inputs, these are unavailable in our problem set-
ting.
Another related area is automatic grammatical
error detection for English as a second language
(ESL) learners (Leacock et al, 2010). We use
common features that are also used in this area.
They target specific error types commonly made
by ESL learners, such as errors in prepositions and
subject-verb agreement. In contrast, our method
does not specify error types and aims to de-
tect machine-translated sentences focusing on the
phrase salad phenomenon produced by SMT sys-
tems. In addition, errors generated by ESL learn-
ers and SMT systems are different. ESL learners
make spelling and grammar mistakes at the word
level but their sentence are generally structured
while SMT results are unstructured due to phrase
salads. Works on translationese detection (Baroni
and Bernardini, 2005; Kurokawa et al, 2009; Ilisei
et al, 2010) aim to automatically identify human-
translated text by professionals using text gener-
ated by native speakers. These are related, but our
work focuses on machine-translated text.
The closest to our approach is the method pro-
posed by Moore and Lewis (2010). It automat-
ically selects data for creating a domain-specific
LM. Specifically, the method constructs LMs us-
ing corpora of target and non-target domains and
computes a cross-entropy score of an input sen-
tence for estimating the likelihood that the input
sentence belongs to the target or non-target do-
mains. While the context is different, our work
uses a similar idea of data selection for the pur-
pose of detecting low-quality sentences translated
by SMT systems.
3 Proposed Method
When APIs of SMT services are used for machine-
translating an Web page, they typically insert
specific tags into the HTML source. Utilizing
such tags makes MT detection trivial. How-
ever, the actual situation is more complicated in
real Web data. When people manually copy and
paste machine-translated sentences, such tags are
lost. In addition, human-generated and machine-
translated sentences are often mixed together even
in a single paragraph. To observe the distribu-
tion of machine-translated sentences in such diffi-
cult cases, we examine 3K sentences collected by
our in-house Web crawler. Among them, exclud-
ing the pages with the tags of MT APIs, 6.7% of
them are found to be clearly machine translation.
Our goal is to automatically identify these sen-
tences that cannot be simply detected by the tags,
except when the sentences are of sufficient qual-
ity to be indistinguishable from human-generated
sentences.
3.1 Phrase Salad Phenomenon
Fig. 1 illustrates the phrase salad phenomenon that
characterizes a sentence translated by an existing
1598
| Of surprise | was up | foreigners flocked | overseas | as well, | they publicized not only | Japan, | saw an article from the news. |
Natural English: The news was broadcasted not only in Japan but also overseas, and it surprised foreigners who read the article.
Unnatural phrase sequence
Natural phrase|       |
Missing combinational word
Figure 1: The phrase salad phenomenon in a sentence translated by an SMT system; each (segmented) phrase is correct and
fluent, but dotted arcs show unnatural sequences of phrases and the boxed phrase shows an incomplete non-contiguous phrase.
SMT system. Each phrase, a sequence of con-
secutive words, is fluent and grammatically cor-
rect; however, the fluency and grammar correct-
ness are both poor in inter-phrases. In addition, a
phrase salad becomes obvious by observing dis-
tant phrases. For example, the boxed phrase in
Fig. 1 is a part of the non-contiguous phrase ?not
only ? but also2;? however, it lacks the latter part
of the phrase (?but also?) that is also necessary
for composing a meaning. Such non-contiguous
phrases are difficult for most SMT systems to gen-
erate, since these phrases require insertion of sub-
phrases in distant parts of the sentence.
Based on the observation of these characteris-
tics, we define features to capture a phrase salad
by examining local and distant phrases. These
features evaluate (1) fluency (Sec. 3.2), (2) gram-
maticality (Sec. 3.3), and (3) completeness of
non-contiguous phrases in a sentence (Sec. 3.4).
Furthermore, humans can distinguish machine-
translated text because they have prior knowledge
of how a human-generated sentence would look
like, which has been accumulated by observing a
lot of examples through their life. This knowl-
edge makes phrase-salads, e.g., missing objects
and influent sequence of words, obvious for hu-
mans since they rarely appear on human-generated
sentences. Based on this assumption, we ex-
tract these features using both human-generated
and machine-translated text. Features extracted
from human-generated text represent the similar-
ity to human-generated text. Likewise, features
extracted from machine-translated text depict the
similarity to machine-translated text. By contrast-
ing these feature weights, we can effectively cap-
ture phrase salads in the sentence.
3.2 Fluency Feature
In a machine-translated sentence, fluency becomes
poor among phrases where a phrase salad occurs.
We capture this influency using two independent
LM scores; fw,H and fw,MT . The former LM is
2We use the symbol ? to represent a gap in which any
word or phrase can be placed.
trained with human-generated sentences and the
latter one is trained with machine-translated sen-
tences. We input a sentence into both of the LMs
and use the scores as the fluency features.
3.3 Grammaticality Feature
In a sentence with phrase salads, its grammatical-
ity is poor because tense and voice become in-
consistent among phrases. We capture this using
LMs trained with part-of-speech (POS) sequences
of human-generated and machine-translated sen-
tences, and the features of fpos,H and fpos,MT are
respectively computed. In a similar manner with a
word-based LM, such grammatical inconsistency
among phrases is detectable when computing a
POS LM score, since the score becomes worse
when an N -gram covers inter-phrases where a
phrase salad occurs. This approach achieves com-
putational efficiency since it only requires a POS
tagger.
Since a phrase salad may occur among distant
phrases of a sentence, it is also effective to evalu-
ate combinations of phrases that cannot be cov-
ered by the span of N -gram. For this purpose,
we make use of function words that sparsely ap-
pear in a sentence where their combinations are
syntactically constrained. For example, the same
preposition rarely appears many times in a human-
generated sentence, while it does in a machine-
translated sentence due to the phrase salad. Simi-
lar to the POS LM, we first analyze sentences gen-
erated by human or SMT by a POS tagger, extract
sequences of function words, and finally train LMs
with the sequences. We use these LMs to obtain
scores that are used as features ffw,H and ffw,MT .
3.4 Gappy-Phrase Feature
There are a lot of common non-contiguous phrases
that consist of sub-phrases (contiguous word
string) and gaps, which we refer to as gappy-
phrases (Bansal et al, 2011). We specifically use
gappy-phrases of 2-tuple, i.e., phrases consisting
of two sub-phrases and one gap in the middle.
Let us take an English example ?not only ? but
1599
Sequences
World population not only grows , but grows old .
A press release not only informs but also teases .
Hazelnuts are not only for food , but also fuel .
The coalition must not only listen but also act .
Table 1: Example of a sequence database
also.? When a sentence contains the phrase ?not
only,? the phrase ?but also? is likely to appear in
human-generated setences. Such a gappy-phrase
is difficult for SMT systems to correctly generate
and causes a phrase salad. Therefore, we define a
feature to evaluate how likely a sentence contains
gappy-phrases in a complete form without missing
sub-phrases. This feature is effective to comple-
ment LMs that capture characteristics inN -grams.
Sequential Pattern Mining It is costly to man-
ually collect a lot of such gappy-phrases. There-
fore, we regard the task as sequential pattern min-
ing and apply PrefixSpan proposed by Pei et al
(2001), which is a widely used sequential pattern
mining method3.
Given a set of sequences and a user-specified
min support ? N threshold, the sequential pattern
mining finds all frequent subsequences whose oc-
currence frequency is no less than min support.
For example, given a sequence database like Ta-
ble 1, the sequential pattern mining finds all fre-
quent subsequences, e.g., ?not only,? ?not only ?
but also,? ?not ? but ?,? and etc.
To capture a phrase salad by contrasting appear-
ance of gappy-phrases in human-generated and
machine-translated text, we independently extract
gappy-phrases from each of them using PrefixS-
pan. We then compute features fg,H and fg,MT
using the obtained phrases.
Observation of Extracted Gappy-Phrases
Based on a preliminary experiment, we set
the parameter min support of PrefixSpan to
100 for computational efficiency. We extract
gappy-phrases (of 2-tuple) from our develop-
ment dataset described in Sec. 4.1 that includes
254K human-generated and 134K machine-
translated sentences in Japanese, and 210K
human-generated and 159K machine-translated
sentences in English.
Regarding the Japanese dataset, we obtain
about 104K and 64K gappy-phrases from human-
3Due to the severe space limitation, readers are referred to
that paper.
generated and machine-translated sentences, re-
spectively. According to our observation of the
extracted phrases, 21K phrases commonly ap-
pear in human-generated and machine-translated
sentences. Many of these common phrases are
incomplete forms of gappy-phrases that lack se-
mantic meaning to humans, such as ?not only ?
the? and ?not only ? and.? On the other hand,
complete forms of gappy-phrases that preserve se-
mantic meaning exclusively appear in phrases ex-
tracted from human-generated sentences. We also
obtain about 74K and 42K phrases from human-
generated and machine-translated sentences in the
English dataset (21K of them are common).
Phrase Selection As a result of sequential
pattern mining, we can gather a huge num-
ber of gappy-phrases from human-generated and
machine-translated text, but as we described
above, many of them are common. In addition,
it is computationally expensive to use all of them.
Therefore, our method selects useful phrases for
detecting machine-translated sentences.
Although there are several approaches for fea-
ture selection, e.g., (Sebastiani, 2002), we use a
method that is suitable for handling a large num-
ber of feature candidates. Specifically, we evaluate
gappy-phrases based on the information gain that
measures the amount of information in bits ob-
tained for class prediction when knowing the pres-
ence or absence of a phrase and the corresponding
class distribution. This corresponds to measuring
an expected reduction in entropy, i.e., uncertainty
associated with a random factor. The information
gain G ? R for a gappy-phrase g is defined as
G(g) .= H(C)? P (X1g )H(C|X1g )
?P (X0g )H(C|X0g ),
where H(C) represents the entropy of the classifi-
cation, C is a stochastic variable taking a class,Xg
is a stochastic variable representing the presence
(X1g ) or absence (X0g ) of the phrase g, P (Xg) rep-
resents the probability of presence or absence of
the phrase g, and H(C|Xg) is the conditional en-
tropy due to the phrase g. We use top-k phrases
based on the information gain G. Specifically, we
use the top 40% of phrases to compute the feature
values. Table 2 shows examples of gappy-phrases
extracted from human-generated and machine-
translated text in our development dataset and re-
main after feature selection.
1600
in the early ? period after ? after the
known as ? to and also ? and
Human more ? than MT and ? but the
not only ? but also no ? not
with ? as well as not ? not
Table 2: Example of gappy-phrases extracted from human-
generated and machine-translated text; phrases preserving se-
mantic meaning are extracted only from human-generated
text.
The gappy-phrases depend on each other, and
the more phrases extracted from human-generated
(machine-translated) text are found in a sentence,
the more likely the sentence is human-generated
(machine-translated). Therefore, we compute the
feature as
fc(s) =
?
i?k
wi?(i, s),
where wi is a weight of the i-th phrase, and ?(i, s)
is a Kronecker?s delta function that takes 1 if the
sentence s includes the i-th phrase and takes 0 oth-
erwise. We may set the weight wi according to the
importance of the phrase, such as the information
gain. In this work, we set wi to 1 for simplicity.
3.5 Classification
Table 3 summarizes the features employed in
our method. In addition to the discussed fea-
tures, we use the length of a sentence as a fea-
ture flen to avoid the bias of LM-based fea-
tures that favor shorter sentences. The proposed
method takes a monolingual sentence from Web
data as input and computes a feature vector of
f = (fw,H , . . . , flen) ? R9. Each feature is fi-
nally normalized to have a zero-mean and unit
variance distribution. In the feature space, a
support vector machine (SVM) classifier (Vap-
nik, 1995) is used to determine the likelihoods
of machine-translated and human-generated sen-
tences.
4 Experiments
We evaluate our method using both Japanese and
English datasets from various aspects and investi-
gate its characteristics. In this section, we describe
our experiment settings.
4.1 Data Preparation
For the purpose of evaluation, we use human-
generated and machine-translated sentences for
Feature Notation
Fluency fw,H , fw,MT
Grammaticality fpos,H , fpos,MT
ffw,H , ffw,MT
Gappy-phrase fg,H , fg,MT
Length flen
Table 3: List of proposed features and their notations
constructing LMs, extracting gappy-phrases, and
training a classifier. These sentences should
be ensured to be human-generated or machine-
translated, and the human-generated and machine-
translated sentences express the same content for
fairness of evaluation to avoid effects due to vo-
cabulary difference.
As a dataset that meets these requirements, we
use parallel text in public websites (this is for fair
evaluation and our method can be trained using
nonparallel text on an actual deployment). Eight
popular sites having Japanese and English paral-
lel pages are crawled, whose text is manually veri-
fied to be human-generated. The main textual con-
tent of these 131K parallel pages are extracted,
and the sentences are aligned using (Ma, 2006).
As illustrated in Fig. 2, the text in one language
is fed to the Bing translator, Google Translate,
and an in-house SMT system4 implemented based
on (Chiang, 2005) by ourselves for obtaining sen-
tences translated by SMT systems. Due to a severe
limitation on the number of requests to the APIs,
we randomly subsample sentences before sending
them to these SMT systems. We use text in the
other language as human-generated sentences5.
In this manner, we prepare 508K human-
generated and 268K machine-translated sentences
as a Japanese dataset, and 420K human-generated
and 318K machine-translated sentences as an En-
glish dataset. We split each of them into two even
datasets and use one for development and the other
for evaluation.
4.2 Experiment Setting
For the fluency and grammaticality features, we
train 4-gram LMs using the development dataset
with the SRI toolkit (Stolcke, 2002). To obtain
the POS information, we use Mecab (Kudo et al,
2004) for Japanese and a POS tagger developed by
Toutanova et al (2003) for English. We evaluate
4A preliminary evaluation of the in-house SMT system
shows that it has comparable quality with Bing translator.
5These are a mixture of sentences generated by native
speakers and professional translators/editors.
1601
Parallel sentences  
MT systems Machine 
translated 
sentences 
Human -
generated 
sentences 
Human -
generated 
sentences 
Human -
generated 
sentences 
? 
English 
Japanese 
Japanese 
Japanese 
? 
? 
? 
Figure 2: Experimental data preparation; text in one lan-
guage is fed to SMT systems and the other is used as human-
generated sentences.
the effect of the sizes of N -grams and develop-
ment dataset in the experiments.
Using the proposed features, we train an SVM
classifier for detecting machine-translated sen-
tences. We use an implementation of LIB-
SVM (Chang and Lin, 2011) with a radial basis
function kernel due to the relatively small number
of features in the proposed method. We set appro-
priate parameters by grid search in a preliminary
experiment.
We evaluate the performance of MT detection
based on accuracy6 that is a broadly used evalua-
tion metric for classification problems:
accuracy = nTP + nTNn ,
where nTP and nTN are the numbers of true-
positives and true-negatives, respectively, and n
is the total number of exemplars. The accuracy
scores that we report in Sec. 5 are all based on 10-
fold cross validation.
4.3 Comparison Method
We compare our method with the method
of (Moore and Lewis, 2010) (Cross-Entropy). Al-
though the Cross-Entropy method is designed for
the task of domain adaptation of an LM, our prob-
lem is a variant of their original problem and
thus their method is directly relevant. In our
context, the method computes the cross-entropy
scores IMT (s) and IH(s) of an input sentence
s against LMs trained on machine-translated and
human-generated sentences. Cross-entropy and
perplexity are monotonically related, as perplex-
ity of s according to an LM M is simply ob-
6Although we also examine precision and recall of clas-
sification results, they are similar to accuracy reported in this
paper.
Method Accuracy
Cross-Entropy 90.7
Lexical Feature 87.8
Proposed feature Word LMs 94.1
POS LMs 91.3
FW LMs 82.7
GPs 85.7
Table 4: Accuracy (%) of individual features and compari-
son methods
tained by bIM (s) where IM (s) is cross-entropy
score and b is a base with regard to which the
cross-entropy is measured. The method scores
the sentence according to the cross-entropy differ-
ence, i.e., IMT (s)? IH(s), and decides that the
sentence is machine-translated when the score is
lower than a predefined threshold. The classifica-
tion is performed by 10-fold cross validation. We
find the best performing threshold on a training set
and evaluate the accuracy with a test set using the
determined threshold.
Additionally, we compare our method to a
method that uses a feature indicating presence or
absence of unigrams, which we call Lexical Fea-
ture. This feature is commonly used for transla-
tionese detection and shows the best performance
as a single feature in (Baroni and Bernardini,
2005). It is also used by Rarrick et al (2011) and
shows the best performance by itself in detecting
machine-translated sentences in English-Japanese
translation in the setting of bilingual input. We
implement the feature and use it against a mono-
lingual input to fit our problem setting.
5 Results and Discussions
In this section, we analyze and discuss the experi-
ment results in detail.
5.1 Accuracy on Japanese Dataset
We evaluate the sentence-level and document-
level accuracy of our method using the Japanese
dataset. Specifically, we evaluate effects of indi-
vidual features and their combinations, compare
with human annotations, and assess performance
variations across different sentence lengths and
various settings on LM training.
Effect of Individual Feature Table 4 shows the
accuracy scores of individual features and com-
parison methods. We refer to features for flu-
ency (fw,H , fw,MT ) as Word LMs, grammatical-
ity using POS LMs (fpos,H , fpos,MT ) as POS LMs
1602
Method Accuracy
Word LMs + GPs 94.7
Word LMs + POS LMs 95.1
Word LMs + POS LMs + GPs 95.4
Word LMs + POS LMs + FW LMs 95.5
All 95.8
Table 5: Accuracy (%) of feature combinations; there are
significant differences (p  .01) against the accuracy score
of Word LMs.
and function word LMs (ffw,H , ffw,MT ) as FW
LMs, respectively, and for completeness of gappy-
phrases (fg,H , fg,MT ) as GPs. The Word LMs
show the best accuracy that outperforms Cross-
Entropy by 3.4% and Lexical Feature by 6.3%.
This high accuracy is achieved by contrasting flu-
ency in human-generated and machine-translated
text to capture the phrase salad phenomenon. The
accuracy of Word LM trained only on human-
generated sentences is limited to 65.5%. On the
other hand, the accuracy of Word LM trained on
machine-translated sentences shows a better per-
formance (84.4%). By combining these into a
single feature vector f = (fw,H , fw,MT , flen), the
accuracy is largely improved.
It is interesting that Lexical Feature achieves
a high accuracy of 87.8% despite its simplicity.
Since Lexical Feature is a bag-of-words model,
it can consider distant words in a sentence. This
is effective for capturing a phrase salad that oc-
curs among distant phrases, which N -gram can-
not cover. As for Cross-Entropy, a simple sub-
traction of cross-entropy scores cannot well con-
trast the fluency in human-generated and machine-
translated text and results in poorer accuracy than
Word LMs.
The accuracy of POS LMs (91.3%) is slightly
lower than that of Word LMs due to the limited
vocabulary, i.e., the number of POSs. The accu-
racy of FW LMs and GPs are even lower. This
is convincing since these features cannot have rea-
sonable values when a sentence does not include a
function word and gappy-phrase. However, these
features are complementary to Word LMs as we
will see in the next paragraph.
Effect of Feature Combination Table 5 shows
the accuracy when combining features. Sign tests
show that the accuracy scores of these feature
combinations are significantly different (p .01)
against the accuracy of Word LMs. The results
show that the features complement each other. The
Error Ratio Accuracy
(%) Word
LMs
All
Has wrong content words 37.8 93.1 95.0
Misses content words 12.2 91.8 96.5
Has wrong function words 19.7 92.7 97.1
Misses function words 13.0 93.3 95.6
Has wrong inflections 10.8 97.3 98.7
Table 6: Distribution (%) of machine translation errors and
accuracy (%) of proposed method on the different errors
combination of all features reaches an accuracy
of 95.8%, which improves the accuracy of Word
LMs by 1.7%. This result supports that FW LMs
and GPs are effective to capture a phrase salad oc-
curring in distant phrases and complement the ev-
idence in N -grams that is captured by LMs. This
effect becomes more obvious in the human evalu-
ation.
We also evaluate the accuracy of the proposed
method at a document level. Due to the high accu-
racy at the sentence-level, we use a voting method
to judge a document, i.e., deciding if the docu-
ment is machine-translated when ?% of its sen-
tences are judged as machine-translated. We use
all features and find that our method achieves 99%
precision and recall with ? = 50.
Human Evaluation To further investigate the
characteristics of our method, we conduct a human
evaluation. We sample Japanese sentences and ask
three native speakers to 1) judge whether a sen-
tence is human-generated or machine-translated
and 2) list errors that the sentence contains. Re-
garding the task 1), we allow the annotators to as-
sign ?hard to determine? for difficult cases. We al-
locate about 230 sentences for each annotator (in
total 700 sentences) without overlapping annota-
tion sets.
The accuracy of annotations is found to be
88.2%, which shows that our method is even su-
perior to native speakers. Agreement between the
annotators and our method (with all features) is
85.1%. As we interview the annotators, we find
that human annotations are strongly affected by
the annotators? domain knowledge. For example,
technical sentences are more often misclassified
by the annotators.
Table 6 shows the distribution of errors on
machine-translated sentences found by the anno-
tators (on sentences that they correctly classified)
with the accuracy of Word LMs and all features on
1603
02
4
6
8
10
70
75
80
85
90
95
100
6 10 14 18 22 26 30 34 38 42 46 50 54 58 62 66 70 74 78
Ratio
(%)
Accu
rac
y (%)
Num. of words in a sentence
Proposed MethodCross-EntropyLexical FeatureHumanLength distribution
Figure 3: Accuracy (%) across different sentence lengths
(the primary axis) and distribution (%) of sentence lengths in
the evaluation dataset (the secondly axis)
these sentences (a sentence may contain multiple
errors). It indicates that the accuracy of Word LMs
is improved by feature combination; from 1.4% on
sentences of ?Has wrong inflections? to 4.7% on
sentences of ?Misses content words?.
Effect of Sentence Length The accuracy of the
proposed method is significantly affected by sen-
tence length (the number of words in a sentence).
Fig. 3 shows the accuracy of the proposed method
(with all features) and comparison methods w.r.t.
sentence lengths (with the primary axis), as well
as the distribution of sentence lengths in the eval-
uation dataset (with the secondly axis). We ag-
gregate the classification results on each cross-
validation (test results). It also shows the accu-
racy of human annotations w.r.t. sentence lengths,
which we obtain for the 700 sentences in the hu-
man evaluation. The accuracy drops on all meth-
ods when sentences are short; the accuracy of our
method is 91.6% when a sentence contains less
than or equal to 10 words. The proposed method
shows the similar trend with the human annota-
tions, and even the accuracy of human annota-
tions significantly drops on such short sentences.
This result indicates that SMT results on short
sentences tend to be of sufficient quality and in-
distinguishable from human-generated sentences.
Since such high-quality machine-translations do
not harm the quality of Web-mined data, we do
not need to detect them.
Effect of Setting on LM Training We evalu-
ate the performance variation w.r.t. the sizes of
N -grams and development dataset. Fig. 4 shows
the accuracy of the LM based features and feature
combination when changing sizes of N -grams.
The performance of Word LMs is stabilized after
78
83
88
93
98
1 2 3 4
Accu
rac
y (%)
N- gram
Word LMsPOS LMsFW LMsAL L
Figure 4: Effect of the sizes of N -grams on MT detection
accuracy (%)
3-gram while that of POS LMs is still improved
at 4-gram. This is because POS LMs need more
evidence to compensate for their limited vocabu-
lary. FW LMs become stable at 3-gram because
the possible number of function words in a sen-
tence should be small.
When we change the size of the development
dataset with 10% increments, the accuracy curve is
stabilized when the size is 40% of all set. Consid-
ering the fact that the overall development dataset
is small, it shows that our method is deployable
with a small dataset.
5.2 Accuracy on English Dataset
To investigate the applicability of our method to
other languages, we apply the same method to
the English dataset. Because English is a config-
urational language, function words are less flex-
ible than case markers in Japanese. Therefore,
SMT systems may better handle English function
words, which potentially decreases the effect of
FW LMs in our method. In addition, because En-
glish is a morphologically poor language, the ef-
fect of POS LMs may be reduced.
Nevertheless, in our experiment, all features
are shown to be effective even with the English
dataset. The combination of all features achieves
the best performance, with an accuracy of 93.1%,
which outperforms Cross-Entropy by 1.9%, and
Lexical Feature by 8.5%. Even though improve-
ments by POS LMs and FW LMs are smaller than
Japanese case, their effects are still positive. We
also find that GPs stably contribute to the accu-
racy. These results show the applicability of our
method to other languages.
5.3 Accuracy on Raw Web Pages
To avoid unmodeled factors affecting the evalua-
tion, we have carefully removed noise from our
experiment datasets. However, real Web pages are
1604
more complex; there are often instances of sen-
tence fragments, such as captions and navigational
link text. To evaluate the accuracy of our method
on real Web pages, we conduct experiments using
the dataset generated by Rarrick et al (2011) that
contains randomly crawled Web pages annotated
by two annotators to judge if a page is human-
generated or machine-translated. We use Japanese
sentences extracted from 69 pages (43 human-
generated and 26 machine-translated pages) where
the annotators? judgments agree; 3, 312 sentences
consisting of 1, 399 machine-translated and 1, 913
human-generated sentences. To replicate the sit-
uation in real Web pages, we conduct a minimal
preprocessing, i.e., simply removing HTML tags,
and then feed all the remaining text to our method.
An SVM classifier is trained with features ob-
tained by the LMs and gappy-phrases computed
from the data described in Sec. 4.1. Our method
shows 80.6% accuracy at a sentence level and
82.4% accuracy at a document level using the vot-
ing method. One factor for this performance dif-
ference is again sentence lengths, as SMT results
of short phrases in Web pages can be of high-
quality. Another factor is the noise in Web pages.
We find that experimental pages contain lots of
non-sentences, such as fragments of scripts and
product codes. The results show that we need a
preprocessing to remove typical noise in Web text
before SMT detection to handle noisy Web pages.
5.4 Quality of Cleaned Data
Finally, we briefly demonstrate the effect of
machine-translation filtering in an end-to-end sce-
nario, taking LM construction as an example.
We construct LMs reusing the Japanese evalua-
tion dataset described in Sec. 4.1 where machine-
translated sentences are removed by the pro-
posed method (LM-Proposed), Lexical Feature
(LM-LF), and Cross-Entropy (LM-CE), as well
as an LM with all sentences, i.e., with machine-
translated sentences (LM-All). As a result of 5-
fold cross-validation, LM-Proposed has 17.8%,
17.1%, and 16.3% lower perplexities on average
compared to LM-All, LM-LF, and LM-CE, re-
spectively. These results show that our method
is useful for improving the quality of Web-mined
data.
6 Conclusion
We propose a method for detecting machine-
translated sentences from monolingual Web-text
focusing on the phrase salad phenomenon pro-
duced by existing SMT systems. The experimen-
tal results show that our method achieves an accu-
racy of 95.8% for sentences and 80.6% for noisy
Web text.
We plan to extend our method to detect
machine-translated sentences produced by differ-
ent MT systems, e.g., a rule-based system, and
develop a unified framework for cleaning various
types of noise in Web-mined data. In addition, we
will investigate the effect of source and target lan-
guages on translation in terms of MT detection. As
Lopez (2008) describes, a phrase-salad is a com-
mon phenomenon that characterizes current SMT
results. Therefore, we expect that our method is
basically effective on different language pairs. We
will conduct experiments to evaluate performance
difference using various language pairs.
Acknowledgments
We sincerely appreciate Spencer Rarrick and Will
Lewis for active discussion and sharing the exper-
imental data with us. We thank Junichi Tsujii for
his valuable feedback to improve our work.
References
Alexandra Antonova and Alexey Misyurev. 2011.
Building a web-based parallel corpus and filtering
out machine translated text. In Proceedings of the
Workshop on Building and Using Comparable Cor-
pora, pages 136?144.
Eleftherios Avramidis, Maja Popovic, David Vilar Tor-
res, and Aljoscha Burchardt. 2011. Evaluate with
confidence estimation: Machine ranking of trans-
lation outputs using grammatical features. In Pro-
ceedings of the Workshop on Statistical Machine
Translation (WMT 2011), pages 65?70.
Mohit Bansal, Chris Quirk, and Robert C. Moore.
2011. Gappy phrasal alignment by agreement. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 1308?
1317.
Marco Baroni and Silvia Bernardini. 2005. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. Literary and Linguistic Computing,
21(3):259?274.
1605
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM : a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2005), pages
263?270.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to the
automatic evaluation of machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2001), pages
148?155.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011), pages 1535?1545.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of European Association for Machine
Translation (EAMT 2005).
Google N-gram Corpus. 2006. http://www.ldc.
upenn.edu/Catalog/CatalogEntry.
jsp?catalogId=LDC2006T13.
Google Translate. 2006. http://code.google.
com/apis/language/.
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2010. Identification of transla-
tionese: A machine learning approach. In Proceed-
ings of the International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing 2010), pages 503?511.
Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, and
Kazuhide Yamamoto. 2009. Development of a
Japanese-English software manual parallel corpus.
In Proceedings of the Machine Translation Summit
(MT Summit XII).
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In
Proceedings of the Joint Conference of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP
2009), pages 870?878.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), pages 230?
237.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and its
impact on machine translation. In Proceedings of
the Machine Translation Summit (MT-Summit XII).
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan
and Claypool Publishers.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Xiaoyi Ma. 2006. Champollion: a robust parallel text
sentence aligner. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC 2006), pages 489?492.
Microsoft Translator. 2009. http://www.
microsofttranslator.com/dev/.
Microsoft Web N-gram Services. 2010. http://
research.microsoft.com/web-ngram.
Robert Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
220?224.
Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A taxonomy
of relational patterns with semantic types. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL 2012), pages 1135?1145.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings
of the Annual International ACM SIGIR Conference
(SIGIR 1999), pages 74?81.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311?318.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation (WMT 2011), pages 108?115.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen
Pinto, Qiming Chen, Umeshwar Dayal, and Mei-
Chun Hsu. 2001. PrefixSpan: Mining sequen-
tial patterns efficiently by prefix-projected pattern
growth. In Proceedings of the International Con-
ference on Data Engineering (ICDE 2001), pages
215?224.
1606
Spencer Rarrick, Chris Quirk, and Will Lewis. 2011.
MT detection in web-scraped parallel corpora. In
Proceedings of the Machine Translation Summit
(MT Summit XIII).
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A DOM tree alignment model for mining par-
allel data from the web. In Proceedings of the Inter-
national Conference on Computational Linguistics
and the Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL 2006), pages
489?496.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901?904.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of International Conference
on World Wide Web (WWW 2007), pages 697?706.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003), pages 252?259.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical
approach to extracting entity relationships. In Pro-
ceedings of International Conference on World Wide
Web (WWW 2009), pages 101?110.
1607
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative Approach to Fill-in-the-Blank Quiz Generation for
Language Learners
Keisuke Sakaguchi1? Yuki Arase2 Mamoru Komachi1?
1Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
2Microsoft Research Asia
Bldg.2, No. 5 Danling St., Haidian Dist., Beijing, P. R. China
{keisuke-sa, komachi}@is.naist.jp, yukiar@microsoft.com
Abstract
We propose discriminative methods to
generate semantic distractors of fill-in-the-
blank quiz for language learners using a
large-scale language learners? corpus. Un-
like previous studies, the proposed meth-
ods aim at satisfying both reliability and
validity of generated distractors; distrac-
tors should be exclusive against answers
to avoid multiple answers in one quiz,
and distractors should discriminate learn-
ers? proficiency. Detailed user evaluation
with 3 native and 23 non-native speakers
of English shows that our methods achieve
better reliability and validity than previous
methods.
1 Introduction
Fill-in-the-blank is a popular style used for eval-
uating proficiency of language learners, from
homework to official tests, such as TOEIC1 and
TOEFL2. As shown in Figure 1, a quiz is com-
posed of 4 parts; (1) sentence, (2) blank to fill in,
(3) correct answer, and (4) distractors (incorrect
options). However, it is not easy to come up with
appropriate distractors without rich experience in
language education. There are two major require-
ments that distractors should satisfy: reliability
and validity (Alderson et al, 1995). First, distrac-
tors should be reliable; they are exclusive against
the answer and none of distractors can replace the
answer to avoid allowing multiple correct answers
in one quiz. Second, distractors should be valid;
they discriminate learners? proficiency adequately.
?This work has been done when the author was visiting
Microsoft Research Asia.
? Now at Tokyo Metropolitan University (Email: ko-
machi@tmu.ac.jp).
1http://www.ets.org/toeic
2http://www.ets.org/toefl
Each side, government and opposition, is _____   
the other for the political crisis, and for the violence.  
 
(a) blaming   (b) accusing   (c) BOTH 	

Figure 1: Example of a fill-in-the-blank quiz,
where (a) blaming is the answer and (b) accusing
is a distractor.
There are previous studies on distractor gener-
ation for automatic fill-in-the-blank quiz genera-
tion (Mitkov et al, 2006). Hoshino and Nakagawa
(2005) randomly selected distractors from words
in the same document. Sumita et al (2005) used
an English thesaurus to generate distractors. Liu et
al. (2005) collected distractor candidates that are
close to the answer in terms of word-frequency,
and ranked them by an association/collocation
measure between the candidate and surrounding
words in a given context. Dahlmeier and Ng
(2011) generated candidates for collocation er-
ror correction for English as a Second Language
(ESL) writing using paraphrasing with native lan-
guage (L1) pivoting technique. This method takes
an sentence containing a collocation error as in-
put and translates it into L1, and then translate it
back to English to generate correction candidates.
Although the purpose is different, the technique is
also applicable for distractor generation. To our
best knowledge, there have not been studies that
fully employed actual errors made by ESL learn-
ers for distractor generation.
In this paper, we propose automated distrac-
tor generation methods using a large-scale ESL
corpus with a discriminative model. We focus
on semantically confusing distractors that measure
learners? competence to distinguish word-sense
and select an appropriate word. We especially tar-
get verbs, because verbs are difficult for language
learners to use correctly (Leacock et al, 2010).
Our proposed methods use discriminative models238
Orig.	 I	 stop	 company	 on	 today	 .	Corr.	 I	 quit	 a	 company	 today	 .	Type	 NA	 #REP#	 #DEL#	 NA	 #INS#	 NA	 NA	
Figure 2: Example of a sentence correction pair
and error tags (Replacement, Deletion and Inser-
tion).
trained on error patterns extracted from an ESL
corpus, and can generate exclusive distractors with
taking context of a given sentence into considera-
tion.
We conduct human evaluation using 3 native
and 23 non-native speakers of English. The result
shows that 98.3% of distractors generated by our
methods are reliable. Furthermore, the non-native
speakers? performance on quiz generated by our
method has about 0.76 of correlation coefficient
with their TOEIC scores, which shows that dis-
tractors generated by our methods satisfy validity.
Contributions of this paper are twofold; (1) we
present methods for generating reliable and valid
distractors, (2) we also demonstrate the effective-
ness of ESL corpus and discriminative models on
distractor generation.
2 Proposed Method
To generate distractors, we first need to decide
which word to be blanked. We then generate can-
didates of distractors and rank them based on a
certain criterion to select distractors to output.
In this section, we propose our methods for ex-
tracting target words from ESL corpus and select-
ing distractors by a discriminative model that con-
siders long-distance context of a given sentence.
2.1 Error-Correction Pair Extraction
We use the Lang-8 Corpus of Learner English3 as
a large-scale ESL corpus, which consists of 1.2M
sentence correction pairs. For generating semantic
distractors, we regard a correction as a target and
the misused word as one of the distractor candi-
dates.
In the Lang-8 corpus, there is no clue to align
the original and corrected words. In addition,
words may be deleted and inserted in the corrected
sentence, which makes the alignment difficult.
Therefore, we detect word deletion, insertion, and
replacement by dynamic programming4. We com-
3http://cl.naist.jp/nldata/lang-8/
4The implementation is available at https:
//github.com/tkyf/epair
Feature Example
Word[i-2] ,
Word[i-1] is
Word[i+1] the
Word[i+2] other
Dep[i] child nsubj side, aux is, dobj other, prep for
Class accuse
Table 1: Example of features and class label ex-
tracted from a sentence: Each side, government
and opposition, is *accusing/blaming the other for
the political crisis, and for the violence.
pare a corrected sentence against its original sen-
tence, and when word insertion and deletion er-
rors are identified, we put a spaceholder (Figure
2). We then extract error-correction (i.e. replace-
ment) pairs by comparing trigrams around the re-
placement in the original and corrected sentences,
for considering surrounding context of the target.
These error-correction pairs are a mixture of gram-
matical mistakes, spelling errors, and semantic
confusions. Therefore, we identify pairs due to se-
mantic confusion; we exclude grammatical error
corrections by eliminating pairs whose error and
correction have different part-of-speech (POS)5,
and exclude spelling error corrections based on
edit-distance. As a result, we extract 689 unique
verbs (lemma) and 3,885 correction pairs in total.
Using the error-correction pairs, we calculate
conditional probabilities P (we|wc), which repre-
sent how probable that ESL learners misuse the
word wc as we. Based on the probabilities, we
compute a confusion matrix. The confusion ma-
trix can generate distractors reflecting error pat-
terns of ESL learners. Given a sentence, we iden-
tify verbs appearing in the confusion matrix and
make them blank, then outputs distractor candi-
dates that have high confusion probability. We
rank the candidates by a generative model to
consider the surrounding context (e.g. N-gram).
We refer to this generative method as Confusion-
matrix Method (CFM).
2.2 Discriminative Model for Distractor
Generation and Selection
To generate distractors that considers long-
distance context and reflects detailed syntactic in-
formation of the sentence, we train multiple clas-
sifiers for each target word using error-correction
pairs extracted from ESL corpus. A classifier for
5Because the Lang-8 corpus does not have POS tags, we
assign POS by the NLTK (http://nltk.org/) toolkit.239
a target word takes a sentence (in which the tar-
get word appears) as an input and outputs a verb
as the best distractor given the context using fol-
lowing features: 5-gram (?1 and ?2 words of the
target) lemmas and dependency type with the tar-
get child (lemma). The dependent is normalized
when it is a pronoun, date, time, or number (e.g. he
? #PRP#) to avoid making feature space sparse.
Table 1 shows an example of features and a class
label for the classifier of a target verb (blame).
These classifiers are based on a discriminative
model: Support Vector Machine (SVM)6 (Vapnik,
1995). We propose two methods for training the
classifiers.
First, we directly use the corrected sentences in
the Lang-8 corpus. As shown in Table 1, we use
the 5-gram and dependency features7, and use the
original word (misused word by ESL learners) as
a class. We refer to this method as DiscESL.
Second, we train classifiers with an ESL-
simulated native corpus, because (1) the number
of sentences containing a certain error-correction
pair is still limited in the ESL corpus and (2)
corrected sentences are still difficult to parse cor-
rectly due to inherent noise in the Lang-8 corpus.
Specifically, we use articles collected from Voice
of America (VOA) Learning English8, which con-
sist of 270k sentences. For each target in a given
sentence, we artificially change the target into an
incorrect word according to the error probabilities
obtained from the learners confusion matrix ex-
plained in Section 2.2. In order to collect a suf-
ficient amount of training data, we generate 100
samples for each training sentence in which the
target word is replaced into an erroneous word.
We refer to this method as DiscSimESL9.
3 Evaluation with Native-Speakers
In this experiment, we evaluate the reliability of
generated distractors. The authors asked the help
of 3 native speakers of English (1 male and 2 fe-
males, majoring computer science) from an au-
thor?s graduate school. We provide each partici-
pant a gift card of $30 as a compensation when
completing the task.
6We use Linear SVM with default settings in the scikit-
learn toolkit 0.13.1. http://scikit-learn.org
7We use the Stanford CoreNLP 1.3.4 http://nlp.
stanford.edu/software/corenlp.shtml
8http://learningenglish.voanews.com/
9The implementation is available at https:
//github.com/keisks/disc-sim-esl
Method Corpus Model
Proposed
CFM ESL Generative
DiscESL ESL Discriminative
DiscSimESL Pseudo-ESL Discriminative
Baseline
THM Native Generative
RTM Native Generative
Table 2: Summary of proposed methods (CFM:
Confusion Matrix Method, DiscESL: Discrimina-
tive model with ESL corpus, DiscSimESL: Dis-
criminative model with simulated ESL corpus)
and baseline (THM: Thesaurus Method, RTM:
Roundtrip Method).
In order to compare distractors generated by dif-
ferent methods, we ask participants to solve the
generated fill-in-the-blank quiz presented in Fig-
ure 1. Each quiz has 3 options: (a) only word A
is correct, (b) only word B is correct, (c) both are
correct. The source sentences to generate a quiz
are collected from VOA, which are not included in
the training dataset of the DiscSimESL. We gen-
erate 50 quizzes using different sentences per each
method to avoid showing the same sentence mul-
tiple times to participants. We randomly ordered
the quizzes generated by different methods for fair
comparison.
We compare the proposed methods to two base-
lines implementing previous studies: Thesaurus-
based Method (THM) and Roundtrip Translation
Method (RTM). Table 2 shows a summary of each
method. The THM is based on (Sumita et al,
2005) and extract distractor candidates from syn-
onyms of the target extracted from WordNet10.
The RTM is based on (Dahlmeier and Ng, 2011)
and extracts distractor candidates from roundtrip
(pivoting) translation lexicon constructed from the
WIT3 corpus (Cettolo et al, 2012)11, which cov-
ers a wide variety of topics. We build English-
Japanese and Japanese-English word-based trans-
lation tables using GIZA++ (IBM Model4). In
this dictionary, the target word is translated into
Japanese words and they are translated back to En-
glish as distractor candidates. To consider (local)
context, the candidates generated by the THM,
RTM, and CFM are re-ranked by 5-gram language
10WordNet 3.0 http://wordnet.princeton.
edu/wordnet/
11Available at http://wit3.fbk.eu240
Method RAD (%) ?
Proposed
CFM 94.5 (93.1 - 96.0) 0.55
DiscESL 95.0 (93.6 - 96.3) 0.73
DiscSimESL 98.3 (97.5 - 99.1) 0.69
Baseline
THM 89.3 (87.4 - 91.3) 0.57
RTM 93.6 (92.1 - 95.1) 0.53
Table 3: Ratio of appropriate distractors (RAD)
with a 95% confidence interval and inter-rater
agreement statistics ?.
model score trained on Google 1T Web Corpus
(Brants and Franz, 2006) with IRSTLM toolkit12.
As an evaluation metric, we compute the ratio
of appropriate distractors (RAD) by the following
equation: RAD = NAD/NALL, where NALL is
the total number of quizzes and NAD is the num-
ber of quizzes on which more than or equal to 2
participants agree by selecting the correct answer.
When at least 2 participants select the option (c)
(both options are correct), we determine the dis-
tractor as inappropriate. We also compute the av-
erage of inter-rater agreement ? among all partici-
pants for each method.
Table 3 shows the results of the first experiment;
RAD with a 95% confidence interval and inter-
rater agreement ?. All of our proposed methods
outperform baselines regarding RAD with high
inter-rater agreement. In particular, DiscSimESL
achieves 9.0% and 4.7% higher RAD than THM
and RTM, respectively. These results show that
the effectiveness of using ESL corpus to gener-
ate reliable distractors. With respect to ?, our
discriminative models achieve from 0.12 to 0.2
higher agreement than baselines, indicating that
the discriminative models can generate sound dis-
tractors more effectively than generative models.
The lower ? on generative models may be because
the distractors are semantically too close to the tar-
get (correct answer) as following examples:
The coalition has *published/issued a
report saying that ... .
As a result, the quiz from generative models is not
reliable since both published and issued are cor-
rect.
4 Evaluation with ESL Learners
In this experiment, we evaluate the validity of gen-
erated distractors regarding ESL learners? profi-
12The irstlm toolkit 5.80 http://sourceforge.
net/projects/irstlm/files/irstlm/
Method r Corr Dist Both Std
Proposed
CFM 0.71 56.7 29.6 13.5 11.5
DiscESL 0.48 62.4 27.9 10.4 12.8
DiscSimESL 0.76 64.0 20.7 15.1 13.4
Baseline
THM 0.68 57.2 28.1 14.6 10.7
RTM 0.67 63.4 26.9 9.5 13.2
Table 4: (1) Correlation coefficient r against par-
ticipants? TOEIC scores, (2) the average percent-
age of correct answer (Corr), incorrect answer of
distractor (Dist), and incorrect answer that both
are correct (Both) chosen by participants, and (3)
standard deviation (Std) of Corr.
300 400 500 600 700 800 900 1000TOEIC Score20
30
40
50
60
70
80
90
100
Accu
racy
(%)
DiscSimESLThesaurus (THM)
Figure 3: Correlation between the participants?
TOEIC scores and accuracy on THM and Disc-
SimESL.
ciency. Twenty-three Japanese native speakers (15
males and 8 females) are participated. All the par-
ticipants, who have taken at least 8 years of En-
glish education, self-report proficiency levels as
the TOEIC scores from 380 to 99013. All the par-
ticipants are graduate students majoring in science
related courses. We call for participants by e-
mailing to a graduate school. We provide each
participant a gift card of $10 as a compensation
when completing the task. We ask participants
to solve 20 quizzes per each method in the same
manner as Section 3. To evaluate validity of dis-
tractors, we use only reliable quizzes accepted in
Section 3. Namely, we exclude quizzes whose op-
tions are both correct. We evaluate correlation be-
tween learners? accuracy for the generated quizzes
and the TOEIC score.
Table 4 represents the results; the highest corre-
13The official score range of the TOEIC is from 10 to 990.241
lation coefficient r and standard deviation on Disc-
SimESL shows that its distractors achieve best va-
lidity. Figure 3 depicts the correlations between
the participants? TOEIC scores and accuracy (i.e.
Corr.) on THM and DiscSimESL. It illustrates that
DiscSimESL achieves higher level of positive cor-
relation than THM. Table 4 also shows high per-
centage of choosing ?(c) both are correct? on Disc-
SimESL, which indicates that distractors gener-
ated from DiscSimESL are difficult to distinguish
for ESL learners but not for native speakers as a
following example:
..., she found herself on stage ...
*playing/performing a number one hit.
A relatively lower correlation coefficient on
DiscESLmay be caused by inherent noise on pars-
ing the Lang-8 corpus and domain difference from
quiz sentences (VOA).
5 Conclusion
We have presented methods that automatically
generate semantic distractors of a fill-in-the-blank
quiz for ESL learners. The proposed methods em-
ploy discriminative models trained using error pat-
terns extracted from ESL corpus and can gener-
ate reliable distractors by taking context of a given
sentence into consideration. The human evalua-
tion shows that 98.3% of distractors are reliable
when generated by our method (DiscSimESL).
The results also demonstrate 0.76 of correlation
coefficient to their TOEIC scores, indicating that
the distractors have better validity than previous
methods. As future work, we plan to extend
our methods for other POS, such as adjective and
noun. Moreover, we will take ESL learners? pro-
ficiency into account for generating distractors of
appropriate levels for different learners.
Acknowledgments
This work was supported by the Microsoft Re-
search Collaborative Research (CORE) Projects.
We are grateful to Yangyang Xi for granting per-
mission to use text from Lang-8 and Takuya Fu-
jino for his error pair extraction algorithm. We
would also thank anonymous reviewers for valu-
able comments and suggestions.
References
Charles Alderson, Caroline Clapham, and DianneWall.
1995. Language Test Construction and Evaluation.
Cambridge University Press.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1. Technical report, Google
Research.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3 : Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Associattion for Ma-
chine Translation (EAMT), pages 261?268, Trent,
Italy, May.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with l1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117, Edinburgh, Scotland, UK.,
July.
Ayako Hoshino and Hiroshi Nakagawa. 2005. A Real-
TimeMultiple-Choice Question Generation for Lan-
guage Testing ? A Preliminary Study ?. In Pro-
ceedings of the 2nd Workshop on Building Educa-
tional Applications Using NLP, pages 17?20, Ann
Arbor, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammat-
ical Error Detection for Language Learners. Syn-
thesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao,
and Shang-Ming Huang. 2005. Applications of
Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items. In Proceedings of the
2ndWorkshop on Building Educational Applications
Using NLP, pages 1?8, Ann Arbor, June.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A Computer-Aided Environment for Generat-
ing Multiple-Choice Test Items. Natural Language
Engineering, 12:177?194, 5.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speak-
ers? Proficiency of English by Using a Test with
Automatically-Generated Fill-in-the-Blank Ques-
tions. In Proceedings of the 2nd Workshop on Build-
ing Educational Applications Using NLP, pages 61?
68, Ann Arbor, June.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
242
