Proceedings of NAACL-HLT 2013, pages 1110?1119,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Labeling the Languages of Words in Mixed-Language Documents using
Weakly Supervised Methods
Ben King
Department of EECS
University of Michigan
Ann Arbor, MI
benking@umich.edu
Steven Abney
Department of Linguistics
University of Michigan
Ann Arbor, MI
abney@umich.edu
Abstract
In this paper we consider the problem of label-
ing the languages of words in mixed-language
documents. This problem is approached in a
weakly supervised fashion, as a sequence la-
beling problem with monolingual text sam-
ples for training data. Among the approaches
evaluated, a conditional random field model
trained with generalized expectation criteria
was the most accurate and performed consis-
tently as the amount of training data was var-
ied.
1 Introduction
Language identification is a well-studied problem
(Hughes et al, 2006), but it is typically only studied
in its canonical text-classification formulation, iden-
tifying a document?s language given sample texts
from a few different languages. But there are sev-
eral other interesting and useful formulations of the
problem that have received relatively little attention.
Here, we focus on the problem of labeling the lan-
guages of individual words within a multilingual
document. To our knowledge, this is the first paper
to specifically address this problem.
Our own motivation for studying this problem
stems from issues encountered while attempting to
build language resources for minority languages. In
trying to extend parts of Kevin Scannell?s Cru?bada?n
project (Scannell, 2007), which automatically builds
minority language corpora from the Web, we found
that the majority of webpages that contain text in
a minority language also contain text in other lan-
guages. Since Scannell?s method builds these cor-
pora by bootstrapping from the pages that were re-
trieved, the corpus-building process can go disas-
trously wrong without accounting for this problem.
And any resources, such as a lexicon, created from
the corpus will also be incorrect.
In this paper, we explore techniques for per-
forming language identification at the word level in
mixed language documents. Our results show that
one can do better than independent word language
classification, as there are clues in a word?s context:
words of one language are frequently surrounded by
words in the same language, and many documents
have patterns that may be marked by the presence of
certain words or punctuation. The methods in this
paper also outperform sentence-level language iden-
tification, which is too coarse to capture most of the
shifts between language.
To evaluate our methods, we collected and man-
ually annotated a corpus of over 250,000 words
of bilingual (though mostly non-parallel) text from
the web. After running several different weakly-
supervised learning methods, we found that a condi-
tional random field model trained with generalized
expectation criteria is the most accurate and per-
forms quite consistently as the amount of training
data is varied.
In section 2, we review the related work. In sec-
tion 3, we define the task and describe the data and
its annotation. Because the task of language identi-
fication for individual words has not been explicitly
studied in the literature, and because of its impor-
tance to the overall task, we examine the features
and methods that work best for independent word
language identification in section 4. We begin to ex-
1110
amine the larger problem of labeling the language
of words in context in section 5 by describing our
methods. In section 6, we describe the evaluation
and present the results. We present our error analy-
sis in section 7 and conclude in section 8.
2 Related Work
Language identification is one of the older NLP
problems (Beesley, 1988), especially in regards to
spoken language (House and Neuburg, 1977), and
has received a fair share of attention through the
years (Hughes et al, 2006). In its standard formu-
lation, language identification assumes monolingual
documents and attempts to classify each document
according to its language from some closed set of
known languages.
Many approaches have been proposed, such as
Markov models (Dunning, 1994), Monte Carlo
methods (Poutsma, 2002), and more recently sup-
port vector machines with string kernels, but nearly
all approaches use the n-gram features first sug-
gested by (Cavnar and Trenkle, 1994). Performance
of language identification is generally very high with
large documents, usually in excess of 99% accuracy,
but Xia et al (2009) mention that current methods
still can perform quite poorly when the class of po-
tential languages is very large or the texts to be clas-
sified are very short.
This paper attempts to address three of the on-
going issues specifically mentioned by Hughes et
al. (2006) in their survey of textual language iden-
tification: supporting minority languages, sparse or
impoverished training data, and multilingual docu-
ments.
A number of methods have been proposed in re-
cent years to apply to the problems of unsuper-
vised and weakly-supervised learning. Excluding
self- and co-training methods, these methods can
be categorized into two broad classes: those which
bootstrap from a small number of tokens (some-
times called prototypes) (Collins and Singer, 1999;
Haghighi and Klein, 2006), and those which impose
constraints on the underlying unsupervised learning
problem (Chang et al, 2007; Bellare et al, 2009;
Druck et al, 2008; Ganchev et al, 2010).
Constraint-based weakly supervised learning has
been applied to some sequence labeling problems,
through such methods as contrastive estimation
(Smith and Eisner, 2005), generalized expectation
criteria (Mann and McCallum, 2008), alternating
projections (Singh et al, 2010), and posterior reg-
ularization (Ganchev et al, 2010).
Perhaps the work that is most similar to this work
is the study of code-switching within NLP literature.
Most of the work done has been on automatically
identifying code-switch points (Joshi, 1982; Solorio
and Liu, 2008). The problem of identifying lan-
guage in the presence of code-switching has seen
the most attention in the realm of speech process-
ing (Chu et al, 2007; Lyu and Lyu, 2008), among
many others. Though code-switching has been well-
studied linguistically, it is only one possible rea-
son to explain why a document contains multiple
languages, and is actually one of the less common
causes observed in our corpus. For that reason, we
approach this problem more generally, assuming no
specific generative process behind multilingual text.
3 Task Definition
The task we describe in this paper is a sequence
labeling problem, labeling a word in running text
according to the language to which it belongs. In
the interest of being able to produce reliable hu-
man annotations, we limit ourselves to texts with
exactly two languages represented, though the tech-
niques developed in this paper would certainly be
applicable to documents with more than two lan-
guages. The two languages represented in the paper
are known a priori by the labeler and the only train-
ing data available to the labeler is a small amount
of sample text in each of the two languages repre-
sented.
In most NLP sequence labeling problems, the re-
searchers can safely assume that each sequence (but
not each item in the sequence) is independent and
identically distributed (iid) according to some un-
derlying distribution common to all the documents.
For example, it is safe to assume that a sentence
drawn from WSJ section 23 can be labeled by a
model trained on the other sections. With the task
of this paper we cannot assume that sequences from
different documents are iid, (e.g. One document
may have 90% of its words in Basque, while another
only has 20%), but we do make the simplifying as-
1111
sumption that sequences within the same document
are iid.
Because of this difference, the labeler is presented
each document separately and must label its words
independently of any other document. And the train-
ing data for this task is not in the form of labeled
sequences. Rather, the models in this task are given
two monolingual example texts which are used only
to learn a model for individual instances. Any se-
quential dependencies between words must be boot-
strapped from the document. It is this aspect of
the problem that makes it well-suited for weakly-
supervised learning.
It is worth considering whether this problem is
best approached at the word level, or if perhaps
sentence- or paragraph-level language identification
would suffice for this task. In those cases, we could
easily segment the text at the sentence or paragraph
level and feed those segments to an existing lan-
guage identifier. To answer this question we seg-
mented our corpus into sentences by splitting at ev-
ery period, exclamation point, or question mark (an
overly agressive approximation of sentence segmen-
tation). Even if every sentence was given the cor-
rect majority label under this sentence segmentation,
the maximum possible word-level accuracy that a
sentence-level classifier could achieve is 85.8%, and
even though this number reflects quite optimistic
conditions, it is still much lower than the methods
of this paper are able to achieve.
3.1 Evaluation Data
To build a corpus of mixed language documents, we
used the BootCat tool (Baroni and Bernardini, 2004)
seeded with words from a minority language. Boot-
Cat is designed to automatically collect webpages
on a specific topic by repeatedly searching for key-
words from a topic-specific set of seed words. We
found that this method works equally well for lan-
guages as for topics, when seeded with words from
a specific language. Once BootCat returned a col-
lection of documents, we manually identified docu-
ments from the set that contained text in both the tar-
get language and in English, but did not contain text
in any other languages. Since the problem becomes
trivial when the languages do not share a character
set, we limited ourselves to languages with a Latin
orthography.
Language # words Language # words
Azerbaijani 4114 Lingala 1359
Banjar 10485 Lombard 18512
Basque 5488 Malagasy 6779
Cebuano 17994 Nahuatl 1133
Chippewa 15721 Ojibwa 24974
Cornish 2284 Oromo 28636
Croatian 17318 Pular 3648
Czech 886 Serbian 2457
Faroese 8307 Slovak 8403
Fulfulde 458 Somali 11613
Hausa 2899 Sotho 8198
Hungarian 9598 Tswana 879
Igbo 11828 Uzbek 43
Kiribati 2187 Yoruba 4845
Kurdish 531 Zulu 20783
Table 1: Languages present in the corpus and their
number of words before separating out English text.
We found that there was an important balance to
be struck concerning the popularity of a language. If
a language is not spoken widely enough, then there
is little chance of finding any text in that language on
the Web. Conversely if a language is too widely spo-
ken, then it is difficult to find mixed-language pages
for it. The list of languages present in the corpus
and the number of words in each language reflects
this balance as seen in Table 1.
For researchers who wish to make use this data,
the set of annotations used in this paper is available
from the first author?s website1.
3.2 Annotation
Before the human annotators were presented with
the mixed-language documents fetched by Boot-
Cat, the documents were first stripped of all HTML
markup, converted to Unicode, and had HTML es-
cape sequences replaced with the proper Unicode
characters. Documents that had any encoding er-
rors (e.g. original page used a mixture of encodings)
were excluded from the corpus.
1http://www-personal.umich.edu/?benking/
resources/mixed-language-annotations-
release-v1.0.tgz
1112
ENG: because of LUTARU.Thank you ntate T.T! Sevice...
SOT: Retselisitsoemonethi ekare jwale hotla sebetswa ...
ENG: Lesotho is heading 4 development #big-ups Mr ...
SOT: Basotho bare monoana hao its?upe.
ENG: Just do the job and lets see what you are made ...
SOT: Malerato Mokoena Ntate Thabane, molimo ...
ENG: It is God who reigns and if God is seen in your ...
SOT: Mathabo Letsie http://www.facebook.com/taole. ...
ENG: As Zuma did he should introduce a way of we can ...
SOT: Msekhotho Matona a rona ha a hlomamisoe, re ...
Table 2: An example of text from an annotated
English-Sotho web page.
Since there are many different reasons that the
language in a document may change (e.g. code-
switching, change of authors, borrowing) and many
variations thereof, we attempted to create a broad
set of annotation rules that would cover many cases,
rather than writing a large number of very specific
rules. In cases when the language use was ambigu-
ous, the annotators were instructed simply to make
their best guess. Table 2 shows an example of an
annotated document.
Generally, only well-digested English loanwords
and borrowings were to be marked as belonging to
the foreign language. If a word appeared in the con-
text of both languages, it was permissible for that
word to receive different labels at different times,
depending on its context.
Ordinary proper names (like ?John Williams? or
?Chicago?) were to be marked as belonging to the
language of the context in which they appear. This
rule also applied to abbreviations (like ?FIFA? or
?BBC?). The exception to this rule was proper
names composed of common nouns (like ?Stairway
to Heaven? or ?American Red Cross?) and to abbre-
viations that spelled out English words, which were
to be marked as belonging to the language of the
words they were composed of.
The annotators were instructed not to assign la-
bels to numbers or punctuation, but they were al-
lowed to use numbers as punctuation as clues for as-
signing other labels.
3.3 Human Agreement
To verify that the annotation rules were reasonable
and led to a problem that could potentially be solved
by a computer, we had each of the annotators mark
Language # words Language # words
Azerbaijani 211 Lingala 1816
Banjar 450 Lombard 2955
Basque 1378 Malagasy 4038
Cebuano 1898 Nahuatl 3544
Chippewa 92 Ojibwa 167
Cornish 2096 Oromo 1443
Croatian 1505 Pular 1285
Czech 1503 Serbian 1515
English 16469 Slovak 1504
Faroese 1585 Somali 1871
Fulfulde 1097 Sotho 2154
Hausa 2677 Tswana 2191
Hungarian 1541 Uzbek 1533
Igbo 2079 Yoruba 2454
Kiribati 1891 Zulu 1075
Kurdish 1674
Table 3: Number of total words of training data for
each language.
up a small shared set of a few hundred words from
each of eight documents, in order to measure the
inter-annotator agreement.
The average actual agreement was 0.988, with 0.5
agreement expected by chance for a kappa of 0.975.
3.4 Training Data
Following Scannell (2007), we collected small
monolingual samples of 643 languages from four
sources: the Universal Declaration of Human
Rights2, non-English Wikipedias3, the Jehovah?s
Witnesses website4, and the Rosetta project (Lands-
bergen, 1989).
Only 30 of these languages ended up being used
in experiments. Table 3 shows the sizes of the mono-
lingual samples of the languages used in this paper.
2The Universal Declaration of Human Rights is a document
created by the United Nations and translated into many lan-
guages. As of February 2011 there were 365 versions available
from http://www.unicode.org/udhr/
3As of February 2011, there were 113 Wikipedias in differ-
ent languages. Current versions of Wikipedia can be accessed
from http://meta.wikimedia.org/wiki/List of
Wikipedias
4As of February 2011, there were 310 versions of the site
available at http://www.watchtower.org
1113
They range from 92 for Chippewa to 16469 for En-
glish. Most of the languages have between 1300 and
1600 words in their example text. To attempt to mit-
igate variation caused by the sizes of these language
samples, we sample an equal number of words with
replacement from each of English and a second lan-
guage to create the training data.
4 Word-level Language Classification
We shift our attention momentarily to a subproblem
of the overall task: independent word-level language
classification. While the task of language identifica-
tion has been studied extensively at the document,
sentence, and query level, little or no work has been
done at the level of an individual word. For this rea-
son, we feel it is prudent to formally evaluate the fea-
tures and classifiers which perform most effectively
at the task of word language classification (ignoring
any sequential dependencies at this point).
4.1 Features
We used a logistic regression classifier to experiment
with combinations of the following features: charac-
ter unigrams, bigrams, trigrams, 4-grams, 5-grams,
and the full word. For these experiments, the train-
ing data consisted of 1000 words sampled uniformly
with replacement from the sample text in the appro-
priate languages. Table 4 shows the accuracies that
the classifier achieved when using different sets of
features averaged over 10 independent runs.
Features Accuracy
Unigrams 0.8056
Bigrams 0.8783
Trigrams 0.8491
4-grams 0.7846
5-grams 0.6977
{1,2,3,4,5}-grams 0.8817
{1,2,3,4,5}-grams, word 0.8819
Table 4: Logistic regression accuracy when trained
using varying features.
The use of all available features seems to be the
best option, and we use the full set of features in
all proceeding experiments. This result also concurs
with the findigs of (Cavnar and Trenkle, 1994), who
0 200 400 600 800 1,000
0.7
0.8
0.9
Sampled Words
A
cc
u
ra
cy
logistic regression
na??ve Bayes
decision tree
winnow2
Figure 1: Learning curves for logistic regression,
na??ve Bayes, decision tree, and Winnow2 on the in-
dependent word classification problem as the num-
ber of sampled words in each training example
changes from 10 to 1000.
found 1-5-grams to be most effective for document
language classification.
4.2 Classifiers
Using all available features, we compare four MAL-
LET (McCallum, 2002) classifiers: logistic regres-
sion, na??ve Bayes, decision tree, and Winnow2. Fig-
ure 1 shows the learning curves for each classifier as
the number of sampled words comprising each train-
ing example is varied from 10 to 1000.
Since a na??ve Bayes classifier gave the best per-
formance in most experiments, we use na??ve Bayes
as a representative word classifier for the rest of the
paper.
5 Methods
Moving onto the main task of this paper, labeling
sequences of words in documents according to their
languages, we use this section to describe our meth-
ods.
Since training data for this task is limited and is
of a different type than the evaluation data (labeled
instances from monolingual example texts vs. la-
beled sequences from the multilingual document),
we approach the problem with weakly- and semi-
supervised methods.
1114
The sequence labeling methods are presented
with a few new sequence-relevant features, which
are not applicable to independent word classification
(since these features do not appear in the training
data):
? a feature for the presence of each possible non-
word character (punctuation or digit) between
the previous and the current words
? a feature for the presence of each possible non-
word character between the current and next
words
In addition to independent word classification,
which was covered in section 4, we also imple-
ment a conditional random field model trained with
generalized expectation criteria, a hidden Markov
model (HMM) trained with expectation maximiza-
tion (EM), and a logistic regression model trained
with generalized expectation criteria.
We had also considered that a semi-Markov CRF
(Sarawagi and Cohen, 2004) could be useful if
we could model segment lengths (a non-Markovian
feature), but we found that gold-standard segment
lengths did not seem to be distributed according to
any canonical distribution, and we did not have a re-
liable way to estimate these segment lengths.
5.1 Conditional Random Field Model trained
with Generalized Expectation
Generalized expectation (GE) criteria (Druck et al,
2008) are terms added to the objective function of
a learning algorithm which specify preferences for
the learned model. When the model is a linear
chain conditional random field (CRF) model, we can
straightforwardly express these criteria in the objec-
tive function with a KL-divergence term between the
expected values of the current model p? and the pre-
ferred model p? (Mann and McCallum, 2008).
O(?;D,U) =
?
d
log p?(y
(d)|x(d))?
?
k ?k
2?2
? ?D(p?||p??)
Practically, to compute these expectations, we
produce the smoothed MLE on the output label dis-
tribution for every feature observed in the training
data. For example, the trigram ?ter? may occur
27 times in the English sample text and 34 times
in the other sample text, leading to an MLE of
p?(eng|ter) ? 0.44.
Because we do not expect the true marginal label
distribution to be uniform (i.e. the document may
not have equal numbers of words in each language),
we first estimate the expected marginal label distri-
bution by classifying each word in the document in-
dependently using na??ve Bayes and taking the result-
ing counts of labels produced by the classifier as an
MLE estimate for it: p?(eng) and p?(non).
We use these terms to bias the expected label dis-
tributions over each feature. Let Feng and Fnon re-
spectively be the collections of all training data fea-
tures with the two labels. For every label l ? L =
{eng,non} and every feature f ? Feng?Fnon, we
calculate
p(l|f) =
count(f,Fl) + ?
count(f,
?
iFi) + ?|L|
?
p?(l)
puniform(l)
,
the biased maximum likelihood expected output
label distribution. To avoid having p(l|f) = 0,
which can cause the KL-divergence to be undefined,
we perform additive smoothing with ? = 0.5 on the
counts before multiplying with the biasing term.
We use the implementation of CRF with GE cri-
teria from MALLET (McCallum, 2002), which uses
a gradient descent algorithm to optimize the objec-
tive function. (Mann and McCallum, 2008; Druck,
2011)
5.2 Hidden Markov Model trained with
Expectation Maximization
A second method we used was a hidden Markov
model (HMM) trained iteratively using the Expec-
tation Maximization algorithm (Dempster et al,
1977). Here an HMM is preferable to a CRF be-
cause it is a generative model and therefore uses pa-
rameters with simple interpretations. In the case of
an HMM, it is easy to estimate emission and transi-
tion probabilities using an external method and then
set these directly.
To initialize the HMM, we use a uniform distri-
bution for transition probabilities, and produce the
emission probabilities by using a na??ve Bayes clas-
sifier trained over the two small language samples.
1115
In the expectation step, we simply pass the docu-
ment through the HMM and record the labels it pro-
duces for each word in the document.
In the maximization step, we produce maximum-
likelihood estimates for transition probabilities from
the transitions between the labels produced. To
estimate emission probabilities, we retrain a na??ve
Bayes classifier on the small language samples along
the set of words from the document that were labeled
as being in the respective language. We iterated this
process until convergence, which usually took fewer
than 10 iterations.
We additionally experimented with a na??ve Bayes
classifier trained by EM in the same fashion, except
that it had no transition probabilities to update. This
classifier?s performance was almost identical to that
of the GE-trained MaxEnt method mentioned in the
following section, so we omit it from the results and
analysis for that reason.
5.3 Logistic Regression trained with
Generalized Expectation
GE criteria can also be straightforwardly applied to
the weakly supervised training of logistic regression
models. The special case where the constraints spec-
ified are over marginal label distributions, is called
label regularization.
As with the CRF constraint creation, here we first
use an ordinary supervised na??ve Bayes classifier in
order to estimate the marginal label distributions for
the document, which can be used to create more ac-
curate output label expectations that are biased to
the marginal label distributions over all words in the
document.
We use the MALLET implementation of a GE-
trained logistic regression classifier, which opti-
mizes the objective function using a gradient descent
algorithm.
5.4 Word-level Classification
Our fourth method served as a baseline and did
not involve any sequence labeling, only independent
classification of words. Since na??ve Bayes was the
best performer among word classification methods,
we use that the representative of independent word
classification methods. The implementation of the
na??ve Bayes classifier is from MALLET.
0 200 400 600 800 1,000
0.7
0.75
0.8
0.85
0.9
0.95
Sampled Words
A
cc
u
ra
cy
na??ve Bayes
GE-trained logistic regression
EM-trained HMM
GE-trained CRF
Figure 2: Learning curves for na??ve Bayes, logistic
regression trained with GE, HMM trained with EM,
and CRF trained with GE as the number of sampled
words in each training example changes from 10 to
1000.
We also implemented a self-trained CRF, initially
trained on the output of this na??ve Bayes classifier,
and trained on its own output in subsequent itera-
tions. This method was not able to consistently out-
perform the na??ve Bayes classifier after any number
of iterations.
6 Evaluation and Results
We evaluated each method using simple token-level
accuracy, i.e. whether the correct label was assigned
to a word in the document. Word boundaries were
defined by punctuation or whitespace, and no tokens
containing a digit were included. Figure 2 displays
the accuracy for each method as the number of sam-
pled words from each language example is varied
from 10 to 1000.
In all the cases we tested, CRF trained with GE
is clearly the most accurate option among the meth-
ods examined, though the EM-trained HMM seemed
to be approaching a similar accuracy with large
amounts of training data. With a slight edge in ef-
ficiency also in its favor, we think the GE+CRF ap-
proach, rather than EM+HMM, is the best approach
for this problem because of its consistent perfor-
mance across a wide range of training data sizes.
In its favor, the EM+HMM approach has a slightly
1116
lower variance in its performance across different
files, though not at a statistically significant level.
Contrary to most of the results in (Mann and Mc-
Callum, 2010), a logistic regression classifier trained
with GE did not outperform a standard supervised
na??ve Bayes classifier. We suspect that this is due
to the different nature of this problem as compared
to most other sequence labeling problems, with the
classifier bootstrapping over a single document only.
In the problems studied by Mann and McCallum, the
GE-trained classifier was able to train over the entire
training set, which was on average about 50,000 in-
stances, far more than the number of words in the
average document in this set (2,500).
7 Error Analysis
In order to analyze the types of mistakes that the
models made we performed an error analysis on ten
randomly selected files, looking at each mislabeled
word and classifying the error according to its type.
The results of this analysis are in Table 5. The three
classes of errors are (1) named entity errors, when
a named entity is given a label that does not match
the label it was given in the original annotation, (2)
shared word errors, when a word that could belong
to either language is classified incorrectly, and (3)
other, a case that covers all other types of errors.
Method NE SW Other
GE+CRF 41% 10% 49%
EM+HMM 50% 14% 35%
GE+MaxEnt 37% 12% 51%
Na??ve Bayes 42% 17% 40%
Table 5: Types of errors and their proportions among
the different methods. NE stands for Named Entity,
SW stands for Shared Word, and Other covers all
other types of errors.
Our annotation rules for named entities specified
that named entities should be given a label match-
ing their context, but this was rather arbitrary, and
not explicitly followed by any of the methods, which
treat a named entity as if it was any other token. This
was the one of most frequent types of error made by
each of the methods and in our conclusion in sec-
tion 8, we discuss ways to improve it.
In a regression analysis to determine which fac-
tors had the greatest correlations with the GE-
trained CRF performance, the estimated proportion
of named entities in the document had by far the
greatest correlation with CRF accuracy of anything
we measured. Following that in decreasing order of
correlation strength were the cosine similarity be-
tween English and the document?s second language,
the number of words in the monolingual example
text (even though we sampled from it), and the aver-
age length of gold-standard monolingual sequences
in the document.
The learning curve for GE-trained CRF in Fig-
ure 2 is somewhat atypical as far as most machine
learning methods are concerned: performance is
typically non-decreasing as more training data is
made available.
We believe that the model is becoming over-
constrained as more words are used to create the
constraints. The GE method does not have a way
to specify that some of the soft constraints (for the
labels observed most frequently in the sample text)
should be more important than other constraints
(those observed less frequently). When we mea-
sure the KL-divergence between the label distribu-
tions predicted by the constraints and the true la-
bel distribution, we find that this divergence seems
to reach its minimum value between 600 and 800
words, which is where the GE+CRF also seems to
reach its maximum performance.
The step with a na??ve Bayes classifier estimating
the marginal label distribution ended up being quite
important overall. Without it, the accuracy dropped
by more than a full percentage point absolute. But
the problem of inaccurate constraint estimation is
one that needs further consideration. Some possible
ways to address it may be to prune the constraints
according to their frequency or perhaps according to
a metric like entropy, or to vary the GE-criteria coef-
ficient in the objective function in order to penalize
the model less for varying from the expected model.
8 Conclusion
This paper addresses three of the ongoing issues
specifically mentioned by Hughes et al (2006) in
their survey of textual language identification. Our
approach is able to support minority languages; in
1117
fact, almost all of the languages we tested on would
be considered minority languages. We also address
the issue of sparse or impoverished training data.
Because we use weakly-supervised methods, we are
able to successfully learn to recognize a language
with as few as 10 words of training data5. The last
and most obvious point we address is that of multi-
lingual documents, which is the focus of the paper.
We present a weakly-supervised system for iden-
tifying the languages of individual words in mixed-
language documents. We found that across a broad
range of training data sizes, a CRF model trained
with GE criteria is an accurate sequence classifier
and is preferable to other methods for several rea-
sons.
One major issue to be improved upon in future
work is how named entities are handled. A straight-
forward way to approach this may be to create an-
other label for named entities, which (for the pur-
poses of evaluation) would be considered not to be-
long to any of the languages in the document. We
could simply choose not to evaluate a system on the
named entity tokens in a document. Alternatively,
the problem of language-independent named entity
recognition has received some attention in the past
(Tjong Kim Sang and De Meulder, 2003), and it may
be beneficial to incorporate such a system in a robust
word-level language identification system.
Going forward, an issue that needs to be ad-
dressed with this method is its dependence on know-
ing the set of possible languages a priori. Because
we don?t see an easy way to adapt this method to ac-
curately label words in documents from a possible
set of thousands of languages when the document
itself may only contain two or three languages, we
would propose the following future work.
We propose a two-step approach to general word-
level language identification. The first step would be
to examine a multilingual document, and with high
accuracy, list the languages that are present in the
document. The second step would be identical to the
approach described in this paper (but with the two-
language restriction lifted), and would be responsi-
ble for labeling the languages of individual words,
using the set of languages provided by the first step.
5With only 10 words of each language as training data, the
CRF approach correctly labels 88% of words
References
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of the Fourth International Conference
on Langauge Resources and Evaluation (LREC 2004),
volume 4, pages 1313?1316, Lisbon, Portugal.
Kenneth R. Beesley. 1988. Language identifier: A com-
puter program for automatic natural-language identifi-
cation of on-line text. In Proceedings of the 29th An-
nual Conference of the American Translators Associa-
tion, volume 47, page 54.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
pages 43?50. AUAI Press.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Annual Symposium on Document Analysis and
Information (SDAIR 94), pages 161?175, Las Vegas,
Nevada.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280?287, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chyng-Leei Chu, Dau-cheng Lyu, and Ren-yuan Lyu.
2007. Language identification on code-switching
speech. In Proceedings of ROCLING.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 100?110.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1?38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR 2008), pages 595?602. ACM.
Gregory Druck. 2011. Generalized Expectation Criteria
for Lightly Supervised Learning. Ph.D. thesis, Univer-
sity of Massachusetts Amherst.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical report.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001?2049.
1118
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
A.S. House and E.P. Neuburg. 1977. Toward automatic
identification of the language of an utterance. i. pre-
liminary methodological considerations. The Journal
of the Acoustical Society of America, 62:708.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. International Conference on Lan-
guage Resources and Evaluation, pages 485?488.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of the
9th conference on Computational linguistics-Volume
1, pages 145?150. Academia Praha.
Jan Landsbergen. 1989. The rosetta project. pages 82?
87, Munich, Germany.
Dau-Cheng Lyu and Ren-Yuan Lyu. 2008. Language
identification on code-switching utterances using mul-
tiple cues. In Ninth Annual Conference of the Interna-
tional Speech Communication Association.
Gideon S. Mann and Andrew McCallum. 2008. Gener-
alized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings of
ACL-08: HLT, pages 870?878, Columbus, Ohio, June.
Association for Computational Linguistics.
Gideon S. Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. The Journal of Machine
Learning Research, pages 955?984.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45(1):179?189.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems (NIPS 2004), 17:1185?1192.
Kevin P. Scannell. 2007. The cru?bada?n project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, volume 4, pages 5?15,
Louvain-la-Neuve, Belgium.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 73?81, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceed-
ings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, pages 142?147.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data
off the web. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 870?878, Athens, Greece, March. Association
for Computational Linguistics.
1119
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 249?254,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Random Walk Factoid Annotation for Collective Discourse
Ben King Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
benking@umich.edu
rahuljha@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Robert Mankoff ?
The New Yorker Magazine
New York, NY
bob mankoff
@newyorker.com
Abstract
In this paper, we study the problem of au-
tomatically annotating the factoids present
in collective discourse. Factoids are in-
formation units that are shared between
instances of collective discourse and may
have many different ways of being realized
in words. Our approach divides this prob-
lem into two steps, using a graph-based
approach for each step: (1) factoid dis-
covery, finding groups of words that corre-
spond to the same factoid, and (2) factoid
assignment, using these groups of words
to mark collective discourse units that con-
tain the respective factoids. We study this
on two novel data sets: the New Yorker
caption contest data set, and the crossword
clues data set.
1 Introduction
Collective discourse tends to contain relatively
few factoids, or information units about which the
author speaks, but many nuggets, different ways
to speak about or refer to a factoid (Qazvinian and
Radev, 2011). Many natural language applications
could be improved with good factoid annotation.
Our approach in this paper divides this problem
into two subtasks: discovery of factoids, and as-
signment of factoids. We take a graph-based ap-
proach to the problem, clustering a word graph to
discover factoids and using random walks to as-
sign factoids to discourse units.
We also introduce two new datasets in this pa-
per, covered in more detail in section 3. The
New Yorker cartoon caption dataset, provided
by Robert Mankoff, the cartoon editor at The
New Yorker magazine, is composed of reader-
submitted captions for a cartoon published in the
magazine. The crossword clue dataset consists
?Cartoon Editor, The New Yorker magazine
Figure 1: The cartoon used for the New Yorker
caption contest #331.
of word-clue pairs used in major American cross-
word puzzles, with most words having several
hundred different clues published for it.
The term ?factoid? is used as in (Van Halteren
and Teufel, 2003), but in a slightly more abstract
sense in this paper, denoting a set of related words
that should ideally refer to a real-world entity, but
may not for some of the less coherent factoids.
The factoids discovered using this method don?t
necessarily correspond to the factoids that might
be chosen by annotators.
For example, given two user-submitted cartoon
captions
? ?When they said, ?Take us to your leader,? I
don?t think they meant your mother?s house,?
? and ?You?d better call your mother and tell
her to set a few extra place settings,?
a human may say that they share the factoid called
?mother.? The automatic methods however, might
say that these captions share factoid3, which is
identified by the words ?mother,? ?in-laws,? ?fam-
ily,? ?house,? etc.
The layout of this paper is as follows: we review
related work in section 2, we introduce the datasets
249
in detail in section 3, we describe our methods in
section 4, and report results in section 5.
2 Related Work
The distribution of factoids present in text collec-
tions is important for several NLP tasks such as
summarization. The Pyramid Evaluation method
(Nenkova and Passonneau, 2004) for automatic
summary evaluation depends on finding and an-
notating factoids in input sentences. Qazvinian
and Radev (2011) also studied the properties of
factoids present in collective human datasets and
used it to create a summarization system. Hennig
et al (2010) describe an approach for automati-
cally learning factoids for pyramid evaluation us-
ing a topic modeling approach.
Our random-walk annotation technique is sim-
ilar to the one used in (Hassan and Radev, 2010)
to identify the semantic polarity of words. Das
and Petrov (2011) also introduced a graph-based
method for part-of-speech tagging in which edge
weights are based on feature vectors similarity,
which is like the corpus-based lexical similarity
graph that we construct.
3 Data Sets
We introduce two new data sets in this paper, the
New Yorker caption contest data set, and the cross-
word clues data set. Though these two data sets are
quite different, they share a few important char-
acteristics. First, the discourse units tend to be
short, approximately ten words for cartoon cap-
tions and approximately three words for crossword
clues. Second, though the authors act indepen-
dently, they tend to produce surprisingly similar
text, making the same sorts of jokes, or referring
to words in the same sorts of ways. Thirdly, the
authors often try to be non-obvious: obvious jokes
are often not funny, and obvious crossword clues
make a puzzle less challenging.
3.1 New Yorker Caption Contest Data Set
The New Yorker magazine holds a weekly con-
test1 in which they publish a cartoon without
a caption and solicit caption suggestions from
their readers. The three funniest captions are se-
lected by the editor and published in the follow-
ing weeks. Figure 1 shows an example of such
a cartoon, while Table 1 shows examples of cap-
tions, including its winning captions. As part of
1http://www.newyorker.com/humor/caption
I don?t care what planet they are from, they can pass on the
left like everyone else.
I don?t care what planet they?re from, they should have the
common courtesy to dim their lights.
I don?t care where he?s from, you pass on the left.
If he wants to pass, he can use the right lane like everyone
else.
When they said, ?Take us to your leader,? I don?t think they
meant your mother?s house.
They may be disappointed when they learn that ?our leader?
is your mother.
You?d better call your mother and tell her to set a few extra
place settings.
If they ask for our leader, is it Obama or your mother?
Which finger do I use for aliens?
I guess the middle finger means the same thing to them.
I sense somehow that flipping the bird was lost on them.
What?s the Klingon gesture for ?Go around us, jerk??
Table 1: Captions for contest #331. Finalists are
listed in italics.
this research project, we have acquired five car-
toons along with all of the captions submitted in
the corresponding contest.
While the task of automatically identifying the
funny captions would be quite useful, it is well be-
yond the current state of the art in NLP. A much
more manageable task, and one that is quite impor-
tant for the contest?s editor is to annotate captions
according to their factoids. This allows the orga-
nizers of the contest to find the most frequently
mentioned factoids and select representative cap-
tions for each factoid.
On average, each cartoon has 5,400 submitted
captions, but for each of five cartoons, we sam-
pled 500 captions for annotation. The annotators
were instructed to mark factoids by identifying
and grouping events, objects, and themes present
in the captions, creating a unique name for each
factoid, and marking the captions that contain each
factoid. One caption could be given many differ-
ent labels. For example, in cartoon #331, such fac-
toids may be ?bad directions?, ?police?, ?take me
to your leader?, ?racism?, or ?headlights?. After
annotating, each set of captions contained about
60 factoids on average. On average a caption was
annotated with 0.90 factoids, with approximately
80% of the discourse units having at least one fac-
toid, 20% having at least two, and only 2% hav-
ing more than two. Inter-annotator agreement was
moderate, with an F1-score (described more in
section 5) of 0.6 between annotators.
As van Halteren and Teufel (2003) also found
250
0 20 40 600
20
40
60
(a)
0 5 10 15 20 250
50
100
150
(b)
Figure 2: Average factoid frequency distributions
for cartoon captions (a) and crossword clues (b).
0 100 200 300 400 5000
20
40
60
(a)
0 100 200 300 400 5000
5
10
(b)
Figure 3: Growth of the number of unique factoids
as the size of the corpus grows for cartoon captions
(a) and crossword clues (b).
when examining factoid distributions in human-
produced summaries, we found that the distribu-
tion of factoids in the caption set for each car-
toon seems to follow a power law. Figure 2 shows
the average frequencies of factoids, when ordered
from most- to least-frequent. We also found a
Heap?s law-type effect in the number of unique
factoids compared to the size of the corpus, as in
Figure 3.
3.2 Crossword Clues Data Set
Clues in crossword puzzles are typically obscure,
requiring the reader to recognize double mean-
ings or puns, which leads to a great deal of diver-
sity. These clues can also refer to one or more
of many different senses of the word. Table 2
shows examples of many different clues for the
word ?tea?. This table clearly illustrates the differ-
ence between factoids (the senses being referred
to) and nuggets (the realization of the factoids).
The website crosswordtracker.com col-
lects a large number of clues that appear in dif-
ferent published crossword puzzles and aggregates
them according to their answer. From this site, we
collected 200 sets of clues for common crossword
answers.
We manually annotated 20 sets of crossword
clues according to their factoids in the same fash-
ion as described in section 3.1. On average each
set of clues contains 283 clues and 15 different
factoids. Inter-annotator agreement on this dataset
was quite high with an F1-score of 0.96.
Clue Sense
Major Indian export drink
Leaves for a break? drink
Darjeeling, e.g. drink
Afternoon social event
4:00 gathering event
Sympathy partner film
Mythical Irish queen person
Party movement political movement
Word with rose or garden plant and place
Table 2: Examples of crossword clues and their
different senses for the word ?tea?.
4 Methods
4.1 Random Walk Method
We take a graph-based approach to the discovery
of factoids, clustering a word similarity graph and
taking the resulting clusters to be the factoids. Two
different graphs, a word co-occurrence graph and
a lexical similarity graph learned from the corpus,
are compared. We also compare the graph-based
methods against baselines of clustering and topic
modeling.
4.1.1 Word Co-occurrence Graph
To create the word co-occurrence graph, we create
a link between every pair of words with an edge
weight proportional to the number of times they
both occur in the same discourse unit.
4.1.2 Corpus-based Lexical Similarity Graph
To build the lexical similarity graph, a lexical sim-
ilarity function is learned from the corpus, that
is, from one set of captions or clues. We do this
by computing feature vectors for each lemma and
using the cosine similarity between these feature
vectors as a lexical similarity function. We con-
struct a word graph with edge weights propor-
tional to the learned similarity of the respective
word pairs.
We use three types of features in these feature
vectors: context word features, context part-of-
speech features, and spelling features. Context
features are the presence of each word in a win-
dow of five words (two words on each side plus the
word in question). Context part-of-speech features
are the part-of-speech labels given by the Stan-
ford POS tagger (Toutanova et al, 2003) within
the same window. Spelling features are the counts
of all character trigrams present in the word.
Table 3 shows examples of similar word pairs
from the set of crossword clues for ?tea?. From
251
Figure 4: Example of natural clusters in a subsection of the word co-occurrence graph for the crossword
clue ?astro?.
Word pair Sim.
(white-gloves, white-glove) 0.74
(may, can) 0.57
(midafternoon, mid-afternoon) 0.55
(company, co.) 0.46
(supermarket, market) 0.53
(pick-me-up, perk-me-up) 0.44
(green, black) 0.44
(lady, earl) 0.39
(kenyan, indian) 0.38
Table 3: Examples of similar pairs of words as cal-
culated on the set of crossword clues for ?tea?.
this table, we can see that this method is able
to successfully identify several similar word pairs
that would be missed by most lexical databases:
minor lexical variations, such as ?pick-me-up? vs.
?perk-me-up?; abbreviations, such as ?company?
and ?co.?; and words that are similar only in this
context, such as ?lady? and ?earl? (referring to
Lady Grey and Earl Grey tea).
4.1.3 Graph Clustering
To cluster the word similarity graph, we use the
Louvain graph clustering method (Blondel et al,
2008), a hierarchical method that optimizes graph
modularity. This method produces several hierar-
chical cluster levels. We use the highest level, cor-
responding to the fewest number of clusters.
Figure 4 shows an example of clusters found
in the word graph for the crossword clue ?as-
tro?. There are three obvious clusters, one for the
Houston Astros baseball team, one for the dog in
the Jetsons cartoon, and one for the lexical prefix
?astro-?. In this example, two of the clusters are
connected by a clue that mentions multiple senses,
?Houston ballplayer or Jetson dog?.
4.1.4 Random Walk Factoid Assignment
After discovering factoids, the remaining task is
to annotate captions according to the factoids they
contain. We approach this problem by taking ran-
dom walks on the word graph constructed in the
previous sections, starting the random walks from
words in the caption and measuring the hitting
times to different clusters.
For each discourse unit, we repeatedly sam-
ple words from it and take Markov random walks
starting from the nodes corresponding to the se-
lected and lasting 10 steps (which is enough to en-
sure that every node in the graph can be reached).
After 1000 random walks, we measure the aver-
age hitting time to each cluster, where a cluster is
considered to be reached by the random walk the
first time a node in that cluster is reached. Heuris-
tically, 1000 random walks was more than enough
to ensure that the factoid distribution had stabi-
lized in development data.
The labels that are applied to a caption are the
labels of the clusters that have a sufficiently low
hitting time. We perform five-fold cross valida-
tion on each caption or set of clues and tune the
threshold on the hitting time such that the aver-
age number of labels per unit produced matches
the average number of labels per unit in the gold
annotation of the held-out portion.
For example, a certain caption may have the fol-
lowing hitting times to the different factoid clus-
ters:
factoid1 0.11
factoid2 0.75
factoid3 1.14
factoid4 2.41
If the held-out portion has 1.2 factoids per cap-
tion, it may be determined that the optimal thresh-
252
old on the hitting times is 0.8, that is, a threshold
of 0.8 produces 1.2 factoids per caption in the test-
set on average. In this case factoid1 and factoid2
would be marked for this caption, since the hitting
times fall below the threshold.
4.2 Clustering
A simple baseline that can act as a surrogate for
factoid annotation is clustering of discourse units,
which is equivalent to assigning exactly one fac-
toid (the name of its cluster) to each discourse
unit. As our clustering method, we use C-Lexrank
(Qazvinian and Radev, 2008), a method that has
been well-tested on collective discourse.
4.3 Topic Model
Topic modeling is a natural way to approach the
problem of factoid annotation, if we consider the
topics to be factoids. We use the Mallet (McCal-
lum, 2002) implementation of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003). As with the ran-
dom walk method, we perform five-fold cross val-
idation, tuning the threshold for the average num-
ber of labels per discourse unit to match the aver-
age number of labels in the held-out portion. Be-
cause LDA needs to know the number of topics
a priori, we set the number of topics to be equal
to the true number of factoids. We also use the
average number of unique factoids in the held-out
portion as the number of LDA topics.
5 Evaluation and Results
We evaluate this task in a way similar to pairwise
clustering evaluation methods, where every pair of
discourse units that should share at least one fac-
toid and does is a true positive instance, every pair
that should share a factoid and does not is a false
negative, etc. From this we are able to calculate
precision, recall, and F1-score. This is a reason-
able evaluation method, since the average number
of factoids per discourse unit is close to one. Be-
cause the factoids discovered by this method don?t
necessarily match the factoids chosen by the an-
notators, it doesn?t make sense to try to measure
whether two discourse units share the ?correct?
factoid.
Tables 4 and 5 show the results of the various
methods on the cartoon captions and crossword
clues datasets, respectively. On the crossword
clues datasets, the random-walk-based methods
are clearly superior to the other methods tested,
whereas simple clustering is more effective on the
Method Prec. Rec. F1
LDA 0.318 0.070 0.115
C-Lexrank 0.131 0.347 0.183
Word co-occurrence graph 0.115 0.348 0.166
Word similarity graph 0.093 0.669 0.162
Table 4: Performance of various methods annotat-
ing factoids for cartoon captions.
Method Prec. Rec. F1
LDA 0.315 0.067 0.106
C-Lexrank 0.702 0.251 0.336
Word co-occurrence graph 0.649 0.257 0.347
Word similarity graph 0.575 0.397 0.447
Table 5: Performance of various methods annotat-
ing factoids for crossword clues.
cartoon captions dataset.
In some sense, the two datasets in this paper
both represent difficult domains, ones in which
authors are intentionally obscure. The good re-
sults acheived on the crossword clues dataset in-
dicate that this obscurity can be overcome when
discourse units are short. Future work in this
vein includes applying these methods to domains,
such as newswire, that are more typical for sum-
marization, and if necessary, investigating how
these methods can best be applied to domains with
longer sentences.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 395?403. As-
sociation for Computational Linguistics.
Leonhard Hennig, Ernesto William De Luca, and Sahin
Albayrak. 2010. Learning summary content units
with topic modeling. In Proceedings of the 23rd
253
International Conference on Computational Lin-
guistics: Posters, COLING ?10, pages 391?399,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method.
Vahed Qazvinian and Dragomir R Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 689?696. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R Radev. 2011.
Learning from collective human behavior to intro-
duce diversity in lexical choice. In Proceedings of
the 49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies, pages 1098?1108.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Hans Van Halteren and Simone Teufel. 2003. Exam-
ining the consensus between human summaries: ini-
tial experiments with factoid analysis. In Proceed-
ings of the HLT-NAACL 03 on Text summarization
workshop-Volume 5, pages 57?64. Association for
Computational Linguistics.
254
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 829?835,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying Opinion Subgroups in Arabic Online Discussions
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Department of Computer Science
George Washington University
Washington DC, USA
mtdiab@gwu.edu
Ben King
Department of EECS
University of Michigan
Ann Arbor, MI, USA
benking@umich.edu
Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we use Arabic natural lan-
guage processing techniques to analyze
Arabic debates. The goal is to identify
how the participants in a discussion split
into subgroups with contrasting opinions.
The members of each subgroup share the
same opinion with respect to the discus-
sion topic and an opposing opinion to
the members of other subgroups. We
use opinion mining techniques to identify
opinion expressions and determine their
polarities and their targets. We opinion
predictions to represent the discussion in
one of two formal representations: signed
attitude network or a space of attitude vec-
tors. We identify opinion subgroups by
partitioning the signed network represen-
tation or by clustering the vector space
representation. We evaluate the system us-
ing a data set of labeled discussions and
show that it achieves good results.
1 Introduction
Arabic is one of the fastest growing languages
on the internet. The number of internet users in
the Arab region grew by 2500% over the past 10
years. As of January 2012, the number of Arabic-
speaking internet users was 86 millions. The re-
cent political and civic movements in the Arab
World resulted in a revolutionary growth in the
number of Arabic users on social networking sites.
For example, Arabic is the fastest growing lan-
guage in Twitter history 1.
This growth in the presence of Arab users on
social networks and all the interactions and dis-
cussions that happen among them led to a huge
amount of opinion-rich Arabic text being avail-
able. Analyzing this text could reveal the different
viewpoints of Arab users with respect to the topics
that they discuss online.
When a controversial topic is discussed, it is
normal for the discussants to adopt different view-
points towards it. This usually causes rifts in dis-
cussion groups and leads to the split of the dis-
cussants into subgroups with contrasting opinions.
Our goal in this paper is to use natural language
processing techniques to detect opinion subgroups
in Arabic discussions. Our approach starts by
identifying opinionated (subjective) text and deter-
mining its polarity (positive, negative, or neutral).
Next, we determine the target of each opinion ex-
pression. The target of opinion can be a named
entity mentioned in the discussion or an aspect of
the discussed topic. We use the identified opinion-
target relations to represent the discussion in one
of two formal representations. In the first repre-
sentation, each discussant is represented by a vec-
tor that encodes all his or her opinion information
towards the discussion topic. In the second repre-
sentation, each discussant is represented by a node
in a signed graph. A positive edge connects two
discussants if they have similar opinion towards
the topic, otherwise the sign of the edge is nega-
1http://semiocast.com/publications/
2011_11_24_Arabic_highest_growth_on_
Twitter
829
tive. To identify opinion subgroups, we cluster the
vector space (the first representation) or partition
the signed network (the second representation).
We evaluate this system using a data set of Ara-
bic discussions collected from an Arabic debating
site. We experiment with several variations of the
system. The results show that the clustering the
vector space representation achieves better results
than partitioning the signed network representa-
tion.
2 Previous Work
Our work is related to a large body of research on
opinion mining and sentiment analysis. Pang &
Lee (2008) and Liu & Zhang (2012) wrote two re-
cent comprehensive surveys about sentiment anal-
ysis and opinion mining techniques and applica-
tions.
Previous work has proposed methods for iden-
tifying subjective text that expresses opinion
and distinguishing it from objective text that
presents factual information (Wiebe, 2000; Hatzi-
vassiloglou and Wiebe, 2000a; Banea et al, 2008;
Riloff and Wiebe, 2003).
Subjective text may express positive, negative,
or neutral opinion. Previous work addressed the
problem of identifying the polarity of subjective
text (Hatzivassiloglou and Wiebe, 2000b; Hassan
et al, 2010; Riloff et al, 2006). Many of the pro-
posed methods for text polarity identification de-
pend on the availability of polarity lexicons (i.e.
lists of positive and negative words). Several ap-
proaches have been devised for building such lex-
icons (Turney and Littman, 2003; Kanayama and
Nasukawa, 2006; Takamura et al, 2005; Hassan
and Radev, 2010). Other research efforts focused
on identifying the holders and the targets of opin-
ion (Zhai et al, 2010; Popescu and Etzioni, 2007;
Bethard et al, 2004).
Opinion mining and sentiment analysis tech-
niques have been used in various applications.
One example of such applications is identifying
perspectives (Grefenstette et al, 2004; Lin et al,
2006) which is most similar to the topic of this
paper. For example, in (Lin et al, 2006), the au-
thors experiment with several supervised and sta-
tistical models to capture how perspectives are ex-
pressed at the document and the sentence levels.
Laver et al (2003) proposed a method for extract-
ing perspectives from political texts. They used
their method to estimate the policy positions of po-
litical parties in Britain and Ireland, on both eco-
nomic and social policy dimensions.
Somasundaran and Wiebe (2009) present an un-
supervised opinion analysis method for debate-
side classification. They mine the web to learn
associations that are indicative of opinion stances
in debates and combine this knowledge with dis-
course information. Anand et al (2011) present a
supervised method for stance classification. They
use a number of linguistic and structural fea-
tures such as unigrams, bigrams, cue words, re-
peated punctuation, and opinion dependencies to
build a stance classification model. In previous
work, we proposed a method that uses participant-
to-participant and participant-to-topic attitudes to
identify subgroups in ideological discussions us-
ing attitude vector space clustering (Abu-Jbara and
Radev, 2012). In this paper, we extend this method
by adding latent similarity features to the attitude
vectors and applying it to Arabic discussions. In
another previous work, our group proposed a su-
pervised method for extracting signed social net-
works from text (Hassan et al, 2012a). The
signed networks constructed using this method
were based only on participant-to-participant at-
titudes that are expressed explicitly in discussions.
We used this method to extract signed networks
from discussions and used a partitioning algo-
rithm to detect opinion subgroups (Hassan et al,
2012b). In this paper, we extend this method by
using participant-to-topic attitudes to construct the
signed network.
Unfortunately, not much work has been done
on Arabic sentiment analysis and opinion min-
ing. Abbasi et al (2008) applies sentiment anal-
ysis techniques to identify and classify document-
level opinions in text crawled from English and
Arabic web forums. Hassan et al (2011) pro-
posed a method for identifying the polarity of non-
English words using multilingual semantic graphs.
They applied their method to Arabic and Hindi.
Abdul-Mageed and Diab (2011) annotated a cor-
pus of Modern Standard Arabic (MSA) news text
for subjectivity at the sentence level. In a later
work (2012a), they expanded their corpus by la-
830
beling data from more genres using Amazon Me-
chanical Turk. Abdul-Mageed et al (2012a) de-
veloped SAMAR, a system for subjectivity and
Sentiment Analysis for Arabic social media gen-
res. We use this system as a component in our
approach.
3 Approach
In this section, we present our approach to de-
tecting opinion subgroups in Arabic discussions.
We propose a pipeline that consists of five com-
ponents. The input to the pipeline is a discussion
thread in Arabic language crawled from a discus-
sion forum. The output is the list of participants
in the discussion and the subgroup membership of
each discussant. We describe the components of
the pipeline in the following subsections.
3.1 Preprocessing
The input to this component is a discussion thread
in HTML format. We parse the HTML file to iden-
tify the posts, the discussants, and the thread struc-
ture. We transform the Arabic content of the posts
and the discussant names that are written in Arabic
to the Buckwalter encoding (Buckwalter, 2004).
We use AMIRAN (Diab, 2009), a system for pro-
cessing Arabic text, to tokenize the text and iden-
tify noun phrases.
3.2 Identifying Opinionated Text
To identify opinion-bearing text, we start from the
word level. We identify the polarized words that
appear in text by looking each word up in a lexicon
of Arabic polarized words. In our experiments, we
use Sifat (Abdul-Mageed and Diab, 2012b), a lex-
icon of 3982 Arabic adjectives labeled as positive,
negative, or neutral.
The polarity of a word may be dependant on
its context (Wilson et al, 2005). For example,
a positive word that appears in a negated context
should be treated as expressing negative opinion
rather than positive. To identify the polarity of
a word given the sentence it appears in, we use
SAMAR (Abdul-Mageed et al, 2012b), a system
for Subjectivity and Sentiment Analysis for Ara-
bic social media genres. SAMAR labels a sen-
tence that contains an opinion expression as pos-
itive, negative, or neutral taking into account the
context of the opinion expression. The reported
accuracy of SAMAR on different data sets ranges
between 84% and 95% for subjectivity classifica-
tion and 65% and 81% for polarity classification.
3.3 Identifying Opinion Targets
In this step, we determine the targets that the opin-
ion is expressed towards. We treat as an opin-
ion target any noun phrase (NP) that appears in
a sentence that SAMAR labeled as polarized (pos-
itive or negative) in the previous step. To avoid
the noise that may result from including all noun
phrases, we limit what we consider as an opinion
target, to the ones that appear in at least two posts
written by two different participants. Since, the
sentence may contain multiple possible targets for
every opinion expression, we associate each opin-
ion expression with the target that is closest to it in
the sentence. For each discussant, we keep track
of the targets mentioned in his/her posts and the
number of times each target was mentioned in a
positive/negative context.
3.4 Latent Textual Similarity
If two participants share the same opinion, they
tend to focus on similar aspects of the discus-
sion topic and emphasize similar points that sup-
port their opinion. To capture this, we follow
previous work (Guo and Diab, 2012; Dasigi et
al., 2012) and apply Latent Dirichelet Allocation
(LDA) topic models to the text written by the dif-
ferent participants. We use an LDA model with
100 topics. So, we represent all the text written
in the discussion by each participant as a vector
of 100 dimensions. The vector of each participant
contains the topic distribution of the participant, as
produced by the LDA model.
3.5 Subgroup Detection
At this point, we have for every discussant the tar-
gets towards which he/she expressed explicit opin-
ion and a 100-dimensions vector representing the
LDA distribution of the text written by him/her.
We use this information to represent the discussion
in two representations. In the first representation,
each discussant is represented by a vector. For ev-
ery target identified in steps 3 of the pipeline, we
add three entries in the vector. The first entry holds
the total number of times the target was mentioned
by the discussant. The second entry holds the
831
 ??? ???? ???? ?? ?????? ????? ???? ?????? ???? ???? ?????
?????? ??????? ?????? ?????? ?? ????? ???? ??? ???? ?? ??????? 
 ?????? ???? ??? ???? ?????? ??????? ????? ??? ???? ?????? ???????
 ?? ?? ?????? ???? ???? ???? ???? ??????? ???? ??????? ??????
 ????? ??? ?????? ????? ?? ????? ???? ??  ??????? ?? ?????
???? ?? ????? ?????? ?????? ??? ??? ?? ???? ?? ???? ??? ???? 
 ???? ??? ??? ???  ?????????? ???? ???? ??? ??? ?? ????
 ???????? ?????? ?? ?????? ??????? ?? ?? ???? ???????
 ?????? ???????? ???????? ??? ????? ?????? ?? ?????
????? ??? ???? ??? ???? ???????? ??? 
(a) 
(c) (b) 
Figure 1: An example debate taken from our dataset. (a) is the discussion topic. (b) and (c) are two posts
expressing contrasting viewpoints with respect to the topic.
number of times the target was mentioned in a pos-
itive context. The third entry holds the number of
target mentions in a negative context. We also add
to this vector the 100 topic entries from the LDA
vector of that discussant. So, if the number of tar-
gets identified in step 3 of the pipeline is t then
the number of entries in the discussant vector is
3 ? t+ 100.
To identify opinion subgroups, we cluster
the vector space. We experiment with several
clustering algorithms including K-means (Mac-
Queen, 1967), Farthest First (FF) (Hochbaum and
Shmoys, 1985; Dasgupta, 2002), and Expectation
Maximization (EM) (Dempster et al, 1977).
The second representation is a signed network
representation. In this representation, each dis-
cussant is represented by a node in a graph. Two
discussants are connected by an edge if they both
mention at least one common target in their posts.
If a discussant mentions a target multiple times in
different contexts with different polarities, the ma-
jority polarity is assumed as the opinion of this
discussant with respect to this target. A positive
sign is assigned to the edge connecting two discus-
sants if the number of targets that they have simi-
lar opinion towards is greater than the targets that
they have opposing opinion towards, otherwise a
negative sign is assigned to the edge.
To identify subgroups, we use a signed net-
work partitioning algorithm to partition the net-
work. Each resulting partition constitutes a sub-
group. Following (Hassan et al, 2012b), we use
the Dorian-Mrvar (1996) algorithm to partition the
signed network. The optimization criterion aims
to have dense positive links within groups and
dense negative links between groups.
The optimization function is as follows:
F (C) = ?? |NEG|+ (1? ?)? |POS| (1)
where C is the clustering under evaluation,
|NEG| is the number of negative links between
nodes in the same subgroup, |POS| is the number
of positive links between nodes in different sub-
groups, and ? is a parameter that specifies impor-
tance of the two terms. We set ? to 0.5 in all our
experiments.
Clusters are selected such that P (C) is mini-
mum. The best clustering that minimizes P (C) is
found by moving nodes around clusters in a greedy
way starting with a random clustering. To han-
dle the possibility of finding a local minima, the
whole process is repeated k times with random
restarts and the clustering with the minimum value
of P (C) is returned. We set k to 3 in all our ex-
periments.
4 Data
We use data from an Arabic discussion forum
called Naqeshny.2. Naqeshny is a platform for
two-sided debates. The debate starts when a per-
son asks a question (e.g. which political party do
you support?) and gives two possible answers or
positions. The registered members of the web-
site who are interested in the topic participate in
the debate by selecting a position and then post-
ing text to support that position and dispute the
2www.Naqeshny.com
832
opposing position. This means that the data set is
self-labeled for subgroup membership. Since the
tools used in our system are trained on Modern
Standard Arabic (MSA) text, we selected debates
that are mostly MSA. The data set consists of 36
debates comprising a total of 711 posts written by
326 users. The average number of posts per dis-
cussion is 19.75 and the average number of partic-
ipants per discussion is 13.08. Figure 1 shows an
example from the data.
5 Experiments and Results
We use three metrics to evaluate the resulting sub-
groups: Purity (Manning et al, 2008), Entropy,
and F-measure. We ran several variations of the
system on the data set described in the previous
section. In one variation, we use the signed net-
work partitioning approach to detect subgroups.
In the other variations, we use the vector space
clustering approach. We experiment with differ-
ent clustering algorithms. We also run two experi-
ments to evaluate the contribution of both opinion-
target counts and latent similarity features on the
clustering accuracy. In one run, we use target-
opinion counts only. In the other run, we use latent
similarity features only. EM was used as the clus-
tering algorithm in these two runs. Table 1 shows
the results. All the results have been tested for sta-
tistical significance using a 2-tailed paired t-test.
The differences between the results of the different
methods shown in the table are statistically signif-
icant at the 0.05 level. The results show that the
clustering approach achieves better results than the
signed network partitioning approach. This can be
explained by the fact that the vector representa-
tion is a richer representation and encodes all the
discussants? opinion information explicitly. The
results also show that Expectation Maximization
achieves significantly better results than the other
clustering algorithms that we experimented with.
The results also show that both latent text similar-
ity and opinion-target features are important and
contribute to the performance.
6 Conclusion
In this paper, we presented a system for iden-
tifying opinion subgroups in Arabic online dis-
cussions. The system uses opinion and text sim-
System Purity F-Measure Entropy
Signed Network 0.71 0.67 0.68
Clustering - K-means 0.72 0.70 0.67
Clustering - EM 0.77 0.76 0.50
Clustering - FF 0.72 0.69 0.70
Opinion-Target Only 0.67 0.65 0.72
Text Similarity Only 0.64 0.65 0.74
Table 1: Comparison of the different variations of
the proposed approach
ilarity features to encode discussants? opinions.
Two approaches were explored for detecting sub-
groups. The first approach clusters a space of dis-
cussant opinion vectors. The second approach par-
titions a signed network representation of the dis-
cussion. Our experiments showed that the former
approach achieves better results. Our experiments
also showed that both opinion and similarity fea-
tures are important.
Acknowledgements
This research was funded in part by the Office of
the Director of National Intelligence, Intelligence
Advanced Research Projects Activity. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the of?cial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
The authors would like to thank Basma Siam for
her help with collecting the data.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34,
June.
Muhammad Abdul-Mageed and Mona Diab. 2011.
Subjectivity and sentiment annotation of modern
standard arabic newswire. In Proceedings of the
5th Linguistic Annotation Workshop, pages 110?
118, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Muhammad Abdul-Mageed and Mona Diab. 2012a.
Awatif: A multi-genre corpus for modern standard
arabic subjectivity and sentiment analysis. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Ugur Dogan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
833
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Muhammad Abdul-Mageed and Mona Diab. 2012b.
Toward building a large-scale arabic sentiment lexi-
con. In Proceedings of the 6th International Global
Word-Net Conference, Matsue, Japan.
Muhammad Abdul-Mageed, Sandra Ku?bler, and Mona
Diab. 2012a. Samar: a system for subjectivity
and sentiment analysis of arabic social media. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ?12, pages 19?28, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Muhammad Abdul-Mageed, Sandra Kuebler, and
Mona Diab. 2012b. Samar: A system for subjec-
tivity and sentiment analysis of arabic social me-
dia. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 19?28, Jeju, Korea, July. Associa-
tion for Computational Linguistics.
Amjad Abu-Jbara and Dragomir Radev. 2012. Sub-
group detection in ideological discussions. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Jeju, Korea, July. The Associa-
tion for Computational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2.011), pages 1?9,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC?08.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text, page 2224.
Tim Buckwalter. 2004. Issues in arabic orthography
and morphology analysis. In Proceedings of the
Workshop on Computational Approaches to Arabic
Script-based Languages, Semitic ?04, pages 31?34,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude us-
ing textual latent semantics. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 65?69, Jeju Island, Korea, July. Association
for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1?38.
Mona Diab. 2009. Second generation tools (amira
2.0): Fast and robust tokenization, pos tagging, and
base phrase chunking. In Khalid Choukri and Bente
Maegaard, editors, Proceedings of the Second Inter-
national Conference on Arabic Language Resources
and Tools, Cairo, Egypt, April. The MEDAR Con-
sortium.
Patrick Doreian and Andrej Mrvar. 1996. A partition-
ing approach to structural balance. Social Networks,
18(2):149?168.
Gregory Grefenstette, Yan Qu, James G Shanahan, and
David A Evans. 2004. Coupling niche browsers
and affect analysis for an opinion mining applica-
tion. In Proceedings of RIAO, volume 4, pages 186?
194. Citeseer.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
864?872, Jeju Island, Korea, July. Association for
Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: identi-
fying sentences with attitude in online discussions.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1245?1255.
Ahmed Hassan, Amjad Abu-Jbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, HLT ?11, pages 592?
597, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012a. Extracting signed social networks
from text. In Workshop Proceedings of TextGraphs-
7: Graph-based Methods for Natural Language Pro-
cessing, pages 6?14, Jeju, Republic of Korea, July.
Association for Computational Linguistics.
834
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012b. Signed attitude networks: Predict-
ing positive and negative links using linguistic anal-
ysis. In Submitted to the Conference on Emprical
Methods in Natural Language Processing, Jeju, Ko-
rea, July. The Association for Computational Lin-
guistics.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000a.
Effects of adjective orientation and gradability on
sentence subjectivity. In COLING, pages 299?305.
Vasileios Hatzivassiloglou and Janyce M Wiebe.
2000b. Effects of adjective orientation and grad-
ability on sentence subjectivity. In Proceedings of
the 18th conference on Computational linguistics-
Volume 1, pages 299?305. Association for Compu-
tational Linguistics.
Hochbaum and Shmoys. 1985. A best possible heuris-
tic for the k-center problem. Mathematics of Oper-
ations Research, 10(2):180?184.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In EMNLP?06, pages
355?363.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(02):311?331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 109?116. Association for Computational Lin-
guistics.
Bing Liu and Lei Zhang. 2012. A survey of opin-
ion mining and sentiment analysis. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 415?463. Springer US.
J. B. MacQueen. 1967. Some methods for classifi-
cation and analysis of multivariate observations. In
L. M. Le Cam and J. Neyman, editors, Proc. of the
fifth Berkeley Symposium on Mathematical Statistics
and Probability, volume 1, pages 281?297. Univer-
sity of California Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Orena Etzioni. 2007. Extract-
ing product features and opinions from reviews. In
Natural language processing and text mining, pages
9?28. Springer.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
440?448. Association for Computational Linguis-
tics.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226?234, Suntec, Singapore, August.
Association for Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Van-
couver, Canada.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of the
23rd International Conference on Computational
Linguistics, pages 1272?1280. Association for Com-
putational Linguistics.
835
Heterogeneous Networks and Their Applications: Scientometrics, Name
Disambiguation, and Topic Modeling
Ben King, Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
{benking,rahuljha}@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present heterogeneous networks as a way to
unify lexical networks with relational data. We
build a unified ACL Anthology network, tying
together the citation, author collaboration, and
term-cooccurence networks with affiliation and
venue relations. This representation proves to
be convenient and allows problems such as name
disambiguation, topic modeling, and the mea-
surement of scientific impact to be easily solved
using only this network and off-the-shelf graph
algorithms.
1 Introduction
Graph-based methods have been used to great ef-
fect in NLP, on problems such as word sense disam-
biguation (Mihalcea, 2005), summarization (Erkan
and Radev, 2004), and dependency parsing (McDon-
ald et al., 2005). Most previous studies of networks
consider networks with only a single type of node,
and in some cases using a network with a single type
of node can be an oversimplified view if it ignores
other types of relationships.
In this paper we will demonstrate heterogeneous
networks, networks with multiple different types of
nodes and edges, along with several applications of
them. The applications in this paper are not pre-
sented so much as robust attempts to out-perform the
current state-of-the-art, but rather attempts at being
competitive against top methods with little effort be-
yond the construction of the heterogeneous network.
Throughout this paper, we will use the data from
the ACL Anthology Network (AAN) (Bird et al.,
2008; Radev et al., 2013), which contains additional
metadata relationships not found in the ACL Anthol-
ogy, as a typical heterogeneous network. The results
in this paper should be generally applicable to other
heterogeneous networks.
1.1 Heterogeneous AAN schema
We build a heterogeneous graph G(V,E) from
AAN, where V is the set of vertices and E is the
set of edges connecting vertices. A vertex can be
one of five semantic types: {paper, author, venue,
institution, term}. An edge can also be one of five
types, each connecting different types of vertices:
? author ? [writes] ? paper
? paper ? [cites] ? paper
? paper ? [published in] ? venue1
? author ? [affiliated with] ? institution2
? paper ? [contains] ? term
All of this data, except for the terms, is available
for all papers in the 2009 release of AAN. Terms are
extracted from titles by running TextRank (Mihal-
cea and Tarau, 2004) on NP-chunks from titles and
manually filtering out bad terms.
We show the usefulness of this representation
in several applications: the measurement of scien-
tific impact (Section 2), name disambiguation (Sec-
tion 3), and topic modeling (Section 4). The hetero-
geneous network representation provides a simple
framework for combining lexical networks (like the
term co-occurence network) with metadata relations
from a source like AAN and allows us to begin to
develop NLP-aware methods for problems like sci-
entometrics and name disambiguation, which are not
usually framed in an NLP perspective.
1For a joint meeting of venues A and B publishing a paper
x, two edges (x,A) and (x,B) are created.
2Author-affiliation edges are weighted according to the
number of papers an author has published from an institution.
1
Transactions of the Association for Computational Linguistics, 2 (2014) 1?14. Action Editor: Lillian Lee.
Submitted 3/2013; Revised 6/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
2 Scientific Impact Measurement
The study of scientometrics, which attempts to
quantify the scientific impact of papers, authors, etc.
has received much attention recently, even within
the NLP community. In the past few years, there
have been many proposed measures of scientific im-
pact based on relationships between entities. Intu-
itively, a model that can take into account many dif-
ferent types of relationships between entities should
be able to measure scientific impact more accu-
rately than simpler measures like citation counts or
h-index.
We propose using Pagerank on the heterogeneous
AAN (Page et al., 1999) to measure scientific impact.
Since changes in the network schema can affect the
relative rankings between different types of entities,
this method is probably not appropriate for compar-
ing entities of two different types against each other.
But between nodes of the same type, this measure is
an appropriate (and as we will show, accurate) way
to compare impacts.
We see this method as a first logical step in the
direction of heterogeneous network-based sciento-
metrics. This method could easily be extended to
use a directed schema (Kurland and Lee, 2005) or a
schema that is aware of the lexical content of citation
sentences, such as sentiment-based signed networks
(Hassan et al., 2012).
Determining the intrinsic quality of scientific im-
pact measures can be difficult since there is no
way to collect gold standard measurements for real-
world entities. Previous studies have attempted to
show that their measures give high scores to a few
known high-impact entities, e.g. Nobel prize win-
ners (Hirsch, 2005), or have performed a statistical
component analysis to find the most important mea-
sures in a group of related statistics (Bollen et al.,
2009). Our approach, instead, is to generate real-
istic data from synthetic entities whose impacts are
known.
We had considered alternative formulations that
did not rely on synthetic data, but each of them
presented problems. When we attempted manual
prominence annotation for AAN data, the inter-
judge agreement (measured by Spearman correla-
tion) in our experiments ranged from decent (0.9
in the case of institutions) to poor (0.3 for authors)
to nearly random (0.03 for terms), far too low to
use in most cases. We also considered evaluating
prominence measures by their ability to predict fu-
ture citations to an entity. Citations are often used
as a proxy for impact, but our measurements have
found that correlation between past citations and fu-
ture citations is too high for citation prediction to be
a meaningful evaluation3.
2.1 Creating a synthetic AAN
In network theory, a common technique for testing
network algorithms when judgments of real-world
data are expensive or impossible to obtain is to test
the algorithm on a synthetic network. To create such
a synthetic network, the authors define a simple, but
realistic generative process by which the real-world
networks of interest may arise. The properties of
the network are measured to ensure that it replicates
certain observable behaviors of the real-world net-
work. They can then test network algorithms to see
how well they are able to recover the hidden param-
eters that generated the synthetic network. (Pastor-
Satorras and Vespignani, 2001; Clauset et al., 2009;
Karrer and Newman, 2011)
We take a two-step approach to generating this
synthetic data, first generating entities with known
impacts, and second, linking these entities together
according to their latent impacts. Our heuristic is
that high impact entities should be linked to other
high impact entities and vice-versa. As in the net-
work theory literature, we must show that this data
reflects important properties observed in the true
AAN.
One such property is that the number of citations
per paper follows a power law distribution (Redner,
1998). We observe this behavior in AAN along with
several other small-world behaviors, such as a small
diameter, a small average shortest path length, and a
high clustering coefficient in the coauthorship graph.
We strive to replicate these properties in our syn-
thetic data.
3Most existing impact measurements require access to at
least one year?s worth of citation information. The Spearman
correlation between the number of citations received after one
year and after five years is 0.79 with correlation between suc-
cessive years as high as 0.99. Practically this means that the
measures that best correlate with citations after five years are
exactly those that best correlate with citations after one year.
2
Since scientific impact measures attempt to quan-
tify the true impact of entities, we can use these mea-
sures to help understand how the true impact mea-
sures are distributed across different entities. In fact,
citation counts, being a good estimate of impact, can
be used to generate these latent impact variables for
each entity. For each type of entity (papers, authors,
institutions, venues, and terms), we create a latent
impact by sampling from the appropriate citation
count distribution. After sampling, all the impacts
are normalized to fall in the [0, 1] interval, with the
highest-impact entity of each type having a latent
impact of 1. Additive smoothing is used to avoid
having an impact of 0.
Once we have created the entities, our method
for placing edges is most similar to the Erdo?s-
Re?yni method for creating random graphs (Erdo?s
and Re?nyi, 1960), in which edges are distributed
uniformly at random between pairs of vertices. In-
stead of distributing links uniformly, links between
entities are sampled proportionally to I(a)I(b)(1 ?
(I(a) ? I(b))2), where I(x) is the latent impact of
entity x.
We tried several other formulae that failed to
replicate the properties of the real AAN. The
I(a)I(b) part of the formula above reflects a pref-
erence for nodes of any type to connect with high-
impact entities (e.g., major conferences receive
many submissions even though most submissions
will be rejected), but the 1 ? (I(a) ? I(b))2 part
also reflects the reality that entities of similar promi-
nence are most likely to attach to each other (e.g.,
well-known authors publish in major conferences,
while less well-known authors may publish mostly
in lesser-known workshops).
Using this distribution, we randomly sample links
between papers and authors; authors and institu-
tions; papers and venues; and papers and terms. The
only exception to this was paper-to-paper citation
links, for which we did not expect this same be-
havior to apply, as low-impact papers regularly cite
high-impact papers, but not vice-versa. To model ci-
tations, we selected citing papers uniformly at ran-
dom and cited papers in proportion to their impacts.
(Albert and Baraba?si, 2002)
Finally, we generated a network equal in size to
AAN, that is, with the exact same numbers of pa-
pers, authors, etc. and the exact same number of
Relationship True value Synth. value
Paper-citations power
law coeff. 1.82 2.12
Diameter 9 8
Avg. shortest path 4.27 4.05
Collaboration network
clustering coeff. 0.34 0.26
Table 1: Network properties of the synthetic AAN
compared with the true AAN.
paper-author links, paper-venue links, etc. Table 1
compares the observed properties of the true AAN
with the observed properties of this synthetic version
of AAN. None of the statistics are exact matches, but
when building random graphs, it is not uncommon
for measures to differ by many orders of magnitude,
so a model that has measures that are on the same
order of magnitude as the observed data is generally
considered to be a decent model (Newman and Park,
2003).
2.2 Measuring impact on the synthetic AAN
This random network is, of course, still imperfect
in some regards. First of all, it has no time aspect,
so it is not possible for impact to change over time,
which means we cannot test against some impact
measures that have a time component like CiteR-
ank (Maslov and Redner, 2008). Second, there are
some constraints present in the real world that are
not enforced here. Because the edges are randomly
selected, some papers have no venues, while others
have multiple venues. There is also nothing to en-
force certain consistencies, such as authors publish-
ing many papers from relatively few institutions, or
repeatedly collaborating with the same authors.
We had also considered using existing random
graph models such as the Baraba?si-Albert model
(Baraba?si and Albert, 1999), which are known to
produce graphs that exhibit power law behavior.
These models, however, do not provide a way to re-
spect the latent impacts of the entities, as they add
links in proportion only to the number of existing
links a node has.
We measure the quality of impact measures by
comparing ranked lists: the ordering of the entities
3
Paper measure Agreement
Heterogeneous network Pagerank 0.773
Citation network Pagerank 0.558
Citation count 0.642
Author measure Agreement
Heterogeneous network Pagerank 0.461
Coauthorship network Pagerank 0.244
h-index (Hirsch, 2005) 0.292
Aggregated citation count 0.236
i10-index 0.235
Institution measure Agreement
Heterogeneous network Pagerank 0.373
h-index (Mitra, 2006) 0.334
Aggregated citation count 0.327
Venue measure Agreement
Heterogeneous network Pagerank 0.449
h-index (Braun et al., 2006) 0.425
Aggregated citation count 0.370
Impact factor 0.092
Venue citation network Pagerank (Bollen
et al., 2006) 0.366
Table 2: Agreement of various impact measures
with the true latent impact.
by their true (but hidden) impact against their order-
ing according to the impact measure. The agree-
ment between these lists is measured by Kendall?s
Tau. Table 2 compares several well-known impact
measures with our impact measure, Pagerank cen-
trality on the heterogeneous AAN network. We find
that some popular methods, such as h-index (Hirsch,
2005) are too coarse to accurately capture much
of the underlying variation. There is a version of
Kendall?s Tau that accounts for ties, and while this
metric slightly helps the coarser measures, Pagerank
on the heterogeneous network is still the clear win-
ner.
When comparing different ordering methods, it
is natural to wonder which of entities the orderings
disagree on. In general, non-heterogeneous mea-
sures like h-index or collaboration network Pager-
ank, which only focus on one type of relationship
can suffer when the entity in question has an impor-
tant relationship of another type. For example, if an
author is highly cited, but mostly works alone, his
1985 1990 1995 2000 2005 2010
20
40
60
80
100
120
Re
lat
ive
Pa
ge
ran
k
ACL
EMNLP
COLING
NAACL
Figure 1: Evolution of conference impacts. The y-
axis measures relative Pagerank, the entity?s Pager-
ank relative to the average Pagerank in that year.
contribution would be undervalued in the collabo-
ration network, but would be more accurate in the
heterogeneous network.
The majority of the differences between the im-
pact measures, though, tend to be in how they han-
dle entities of low prominence. It seems that, for the
most part, there is relatively little disagreement in
the orderings of high-impact entities between differ-
ent impact measures. That is, most highly prominent
entities tend to be highly rated by most measures.
But when an author or a paper, for example, only has
one or two citations, it can be advantageous to look
at more types of relationships than just citations.
The paper may be written by an otherwise prominent
author, or published at a well-known venue, and hav-
ing many types of relations at its disposal can help a
method like heterogeneous network Pagerank better
distinguish between two low-prominence entities.
2.3 Top-ranked entities according to
heterogeneous network PageRank
Table 3 shows the papers, authors, institutions,
venues, and terms that received the highest Pager-
ank in the heterogeneous AAN. It is obvious that the
top-ranked entities in this network are not simply the
most highly cited entities.
This ranking also does not have any time bias
toward the entities that are currently prominent, as
some of the top authors were more prolific in previ-
ous decades than at the current time. We also see
this effect with COLING, which for many of the
early years, is the only venue in the ACL Anthology.
4
Top Papers Top Authors Top Institutions Top Venues TopTerms
? Building A Large Annotated Corpus OfEnglish: The Penn Treebank 4 15 Jun?ichi Tsujii 4 8
Carnegie Mellon
University 4 1 COLING ? translation
? The Mathematics Of Statistical MachineTranslation: Parameter Estimation 4 7
Aravind K.
Joshi 4 1
University of
Edinburgh 5 1 ACL 4 3 speech
? Attention, Intentions, And The Structure OfDiscourse 4 18
Ralph
Grishman 5 2
University of
Pennsylvania 4 2 HLT 5 1 parsing
? A Maximum Entropy Approach To NaturalLanguage Processing 4 75 Hitoshi Isahara 5 2
Massachusetts
Institute of
Technology
4 4 EACL 5 1 machinetranslation
? BLEU: a Method for Automatic Evaluationof Machine Translation 4 20
Yuji
Matsumoto 4 12
Saarland
University 4 7 LREC 4 3 generation
? A Maximum-Entropy-Inspired Parser 4 7 Kathleen R.McKeown 5 2
IBM T.J. Watson
Research Center ? NAACL 4 3 evaluation
4 2 A Stochastic Parts Program And NounPhrase Parser For Unrestricted Text 4 13 Eduard Hovy 4 39 CNRS 5 3 EMNLP 4 6 grammar
5 1 A Systematic Comparison of VariousStatistical Alignment Models 4 10
Christopher D.
Manning 4 26
University of
Tokyo 5 5
Computational
Linguistics 4 16 dialogue
4 4
Transformation-Based Error-Driven
Learning and Natural Language Processing:
a Case Study in Part-of-Speech Tagging
4 93 Yorick Wilks 5 4 StanfordUniversity 4 4 IJCNLP 4 10
knowl-
edge
4 1 A Maximum Entropy Model forPart-of-Speech Tagging 5 9 Hermann Ney 4 3 BBN Technologies 4 1
Workshop on
Speech and
Natural
Language
4 1 discourse
Table 3: The entities of each type receiving the highest scores from the heterogeneous network Pagerank
impact measure along with their respective changes in ranking when compared to a simple citation count
measure.
One possible way to address this is to use a narrower
time window when creating the graph, such as only
including edges from the previous five years. We
apply this technique in the following section.
2.4 Entity impact evolution
The heterogeneous graph formalism also provides a
natural way to study the evolution of impact over
time, as in (Hall et al., 2008), but at a much finer
granularity. Hall et al. measured the year-by-year
prominence of statistical topics, but we can measure
year-by-year prominence for any entity in the graph.
To measure the evolution of impacts over the
years, we iteratively create year-by-year versions of
the heterogeneous AAN. Each of these graphs con-
tains all entities along with all edges occurring in a
five year window. Due to space, we cannot com-
prehensively exhibit this technique and the data it
produces, but as a brief example, in Figure 1, we
show how the impacts of some major NLP confer-
ences changes over time.
The graph shows that NAACL and EMNLP have
been steadily gaining prominence since their intro-
ductions, but also shows that ACL has had to make
up a lot of ground since 1990 to surpass COLING.
We also notice that all the major conferences have
grown in impact since 2005, and believe that as the
field continues to grow, the major conferences will
continue to become more and more important.
3 Name Disambiguation
We frame network name disambiguation in a link
prediction setting (Taskar et al., 2003; Liben-Nowell
and Kleinberg, 2007). The problems of name dis-
ambiguation and link prediction share many char-
acteristics, and we have found that if two ambigu-
ous name nodes are close enough to be selected by a
link-prediction method, then they likely correspond
to the same real-world author.
We intend to show that the heterogeneous biblio-
graphic network can be used to better disambiguate
author names than the author collaboration network.
The heterogeneous network for this problem con-
tains papers, authors, terms, venues, and institutions.
We compare several well-known network similarity
measures from link prediction by transforming the
5
Network Distance Measure Precision Recall F1-score Rand index Purity NMI
Heterogeneous Truncated Commute Time 0.59 0.78 0.63 0.63 0.71 0.43
Heterogeneous Shortest Path 0.90 0.79 0.83 0.87 0.94 0.76
Heterogeneous PropFlow 0.89 0.83 0.84 0.87 0.93 0.77
Coauthorship Truncated Commute Time 0.47 0.80 0.54 0.47 0.60 0.18
Coauthorship Shortest Path 0.54 0.73 0.60 0.61 0.67 0.31
Coauthorship PropFlow 0.57 0.76 0.64 0.66 0.71 0.43
Coauthorship GHOST 0.89 0.60 0.69 0.81 0.94 0.63
Table 4: Performance of different networks and distance measures on the author name disambiguation task.
The performance measures are averaged over the sets of two, three, and four authors. Rand index is from
(Rand, 1971) and NMI is an abbreviation for normalized mutual information (Strehl and Ghosh, 2003)
similarities to distances and inducing clusters of au-
thors based on these distances.
We compare three distance measures: shortest
path, truncated commute time (Sarkar et al., 2008),
and PropFlow (Lichtenwalter et al., 2010). Short-
est path distance can be a useful metric for author
disambiguation because it is small when two am-
biguous nodes are neighbors in the graph or share
a neighbor. Its downside is that it only considers one
path between nodes, the shortest, and cannot take
advantage of the fact that there may be many short
paths between two nodes.
Truncated commute time is a variant of commute
time where all paths longer than some threshold are
truncated. The truncation threshold l should be set
such that no semantically meaningful path is trun-
cated. We use a value of ten for l in the heteroge-
neous graph and three in the coauthorship graph4.
The advantage of truncated commute time over or-
dinary commute time is simpler calculation, as no
paths longer than l need be considered. The down-
side of this method is that large branching factors
tend to lead to less agreement between commute
time and truncated commute time.
PropFlow is a quantity that measures the proba-
bility that a non-intersecting random walk starting at
node a reaches node b in l steps or fewer, where l is
again a threshold. As before, l should be a bound on
the length of semantically meaningful paths, so we
use the same values for l as with truncated commute
time. Of course, PropFlow is not a metric, which is
4This is a standard coauthorship graph with the edge weights
equal to the number of publications shared between authors.
The heterogeneous network does not have author-to-author
links, as authors are linked by paper nodes.
required for some clustering methods. We use the
following equation to transform PropFlow to a met-
ric: d(a, b) = 1PropF low(a,b) ? 1.
With each of the distance measures, we apply
the same clustering method: partitioning around
medoids, with the number of clusters automatically
determined using the gap statistic method (Tibshi-
rani et al., 2001). We create the null distribution
needed for the gap statistic method by many itera-
tions of randomly sampling distances from the com-
plete distance matrix between all nodes in the graph.
The gap statistic method automatically selects the
number of clusters from two, three, or four author
clusters.
We compare our methods against GHOST (Fan et
al., 2011), a high-performance author disambigua-
tion method based on the coauthorship graph.
3.1 Data
To generate name disambiguation data, we use the
pseudoword method of (Gale et al., 1992). Specif-
ically, we choose two or more completely random
authors and conflate them by giving all instances
of both authors the same name. We let each paper
written by this pseudoauthor be an instance to be
clustered. The clusters produced by any author dis-
ambiguation method can then be compared against
the papers actually written by each of the two au-
thors. This method, of course, relies on having all of
the underlying authors completely disambiguated,
which AAN provides.
This method is used to create 100 distambiguation
sets with two authors, 100 for three authors, and 100
for four authors.
6
3.2 Results
Table 4 shows the performance of author name dis-
ambiguation with different networks and distance
metrics. F1-score is the measure that is most of-
ten used to compare author disambiguation methods.
Both PropFlow and shortest path similarity on the
heterogeneous network perform quite well accord-
ing this measure, as well as the other reported mea-
sures. While comparable recall can be achieved us-
ing only the coauthorship graph, the heterogeneous
graph allows for much higher precision.
4 Random walk topic model
Here we present a topic model based entirely on
graph random walks. This method is not truly a
statistical model as there are no statistical parame-
ters being learned, but rather a topic-discovery and
-assignment method, attempting to solve the same
problem as statistical topic models such as proba-
bilistic latent semantic analysis (pLSA) (Hofmann,
1999) or latent Dirichlet allocation (LDA) (Blei et
al., 2003). In the absence of better terminology, we
use the name random walk topic model.
While this method does not have the robust math-
ematical foundation that statistical topic models pos-
sess, in its favor it has modularity, simplicity, and
interpretability. This language model is modular as
it completely separates the discovery of topics from
the association of topics with entities. It is sim-
ple because it requires only a clustering algorithm
and random walk algorithms, instead of complex in-
ference algorithms. The method also does not re-
quire any modification if the topology of the net-
work changes, whereas statistical models may need
an entirely different inference procedure if, e.g., au-
thor topics are desired in addition to paper topics.
Thirdly this method is easily interpretable with top-
ics provided by clustering in the word-relatedness
graph and topic association based on random walks
from entities to topics.
4.1 Topics from word graph clustering
From the set of ACL anthology titles, we create
two graphs: (1) a word relatedness graph by cre-
ating a weighted link between each pair of words
corresponding to the PropFlow (Lichtenwalter et al.,
2010) measure between them on the full heteroge-
neous graph and (2) a word co-occurence graph by
creating a weighted link between each pair of words
corresponding to the number of titles in which both
words occur.
Both of these graphs are then clustered using
Graph Factorization Clustering (GFC). GFC is a soft
clustering algorithm for graphs that models graph
edges as a mixture of latent node-cluster association
variables. (Yu et al., 2006)
Given a word graph G with vertices V and ad-
jacency matrix [w]ij , GFC attempts to fit a bipar-
tite graph K(V,U) with adjacency matrix [b]ij onto
this data, with the m nodes of U representing the
clusters. Whereas in G, similarity between two
words i and j can be measured with wij , we can
similarly measure their similarity in K with w?ij =?m
p=1
bipbjp
?p where ?p =
?n
i=1 bip is the degree of
vertex p ? U .
Essentially the bipartite graph attempts to approx-
imate the transition probability between i and j inG
with the sum of transition probabilities from i to j
through any of the m nodes in U . Yu, et al. (2006)
present an algorithm for minimizing the divergence
distance `(X,Y) =?ij(xijlog xijyij ? xij + yij) be-
tween [w]ij and [w?]ij .
We run GFC with this distance metric and m =
100 clusters on the word graph until convergence
(change in log-likelihood < 0.1%). After conver-
gence, the nodes in U become the clusters and the
weights bip (constrained to sum to 1 for each clus-
ter) become the topic-word association scores.
Examples of some topics found by this method
are shown in Table 5. From manual inspection of
these topics, we found them to be very much like
topics created by statistical topic models. We find
instances of all the types of topics listed in (Mimno
et al., 2011): chained, intruded, random, and unbal-
anced. For an evaluation of these topics see Sec-
tion 4.3.1.
4.2 Entity-topic association
To associate entities with topics, we first create
the heterogeneous network as in previous sections,
adding links between papers and their title words,
along with links between words and the topics that
were discovered in the previous section. Word-topic
links are also weighted according to the weights
7
Word sense induction sense disambiguation word induction unsupervised clustering senses based similarity chinese
CRFs + their applications entity named recognition random conditional fields chinese entities biomedical segmentation
Dependency parsing parsing dependency projective probabilistic incremental deterministic algorithm data syntactic trees
Tagging models tagging model latent markov conditional random parsing unsupervised segmentation
Multi-doc summarization summarization multi document text topic based query extractive focused summaries
Chinese word segmentation word segmentation chinese based alignment character tagging bakeoff model crf
Lexical semantics lexical semantic distributional similarity wordnet resources lexicon acquistion semantics representation
Cross-lingual IR cross lingual retrieval document language linguistic multi person multilingual coreference
Generation for summar. sentence based compression text summarization ordering approach ranking generation
Spoken language speech recognition automatic prosodic tagging spontaneous news broadcast understanding conversational
French function words de la du des le automatique analyse une en pour
Question answering question answering system answer domain retrieval web based open systems
Unsupervised learning unsupervised discovery learning induction knowledge graph acquisition concept clustering pattern
SVMs for NLP support vector machines errors space classification correcting word parsing detecting
MaxEnt models entropy maximum approach based attachment model models phrase prepositional disambiguation
Dialogue systems dialogue spoken systems human conversational multi interaction dialogues utterances multimodal
Semantic role-labeling semantic role labeling parsing syntactic features ill dependency formed framenet
SMT based translation machine statistical phrase english approach learning reordering model
Coreference resolution resolution coreference anaphora reference pronoun ellipsis ambiguity resolving approach pronominal
Semi- and weak-supervision learning supervised semi classification active data clustering approach graph weakly
Information retrieval based retrieval similarity models semantic space model distance measures document
Discourse discourse relations structure rhetorical coherence temporal representation text connectives theory
CFG parsing context free grammars parsing linear probabilistic rewriting grammar systems optimal
Min. risk train. and decod. minimum efficient training error rate translation risk bayes decoding statistical
Phonology phoneme conversion letter phonological grapheme rules applying transliteration syllable sound
Sentiment sentiment opinion reviews classification mining polarity analysis predicting product features
Neural net speech recog. speech robust recognition real network time neural networks language environments
Finite state methods state finite transducers automata weighted translation parsing incremental minimal construction
Mechanical Turk mechanical turk automatic evaluation amazon techniques data articles image scientific
Table 5: Top 10 words for several topics created by the co-occurence random walk topic model. The left
column is a manual label.
Topic 59 Topic 82
translation 0.1953 parsing 0.1715
machine 0.1802 dependency 0.1192
statistical 0.0784 projective 0.0138
Machine Translation 0.0018 K-best Spanning Tree Parsing 0.0025
Better Hypothesis Testing for Statistical
Machine Translation: Controlling for
Optimizer Instability
0.0016 Pseudo-Projective Dependency Parsing 0.0024
Filtering Antonymous, Trend- Contrasting, and
Polarity-Dissimilar Distributional Paraphrases
for Improving Statistical Machine Translation
0.0015 Shift-Reduce Dependency DAG Parsing 0.0017
Knight, Kevin 0.0083 Nivre, Joakim 0.0120
Koehn, Philipp 0.0074 Johnson, Mark 0.0085
Ney, Hermann 0.0072 Nederhof, Mark-Jan 0.0064
RWTH Aachen University 0.0212 Vaxjo University 0.0113
Carnegie Mellon University 0.0183 Brown University 0.0107
University of Southern California 0.0177 University of Amsterdam 0.0094
Workshop on Statistical Machine Translation 0.0590 ACL 0.0512
EMNLP 0.0270 EMNLP 0.0259
COLING 0.0173 CoNLL 0.0223
Table 6: Examples of entities associated with selected topics.
8
determined by GCF. We then simply take random
walks from topics to entities and measure the pro-
portion at which the random walk arrives at each en-
tity of interest. These proportions become the entity-
topic association scores.
For example, if we wanted to find the authors
most associated with topic 12, we would take a num-
ber of random walks (say 50,000) starting at topic
12 and terminating as soon as the random walk first
reaches an author node. Measuring the proportion
at which random walks arrive at each allows us to
compute an association score between topic 12 and
each author.
A common problem in random walks on large
graphs is that the walk can easily get ?lost? between
two nodes that should be very near by taking a just
a few steps in the wrong direction. To keep the ran-
dom walks from taking these wrong steps, we adjust
the topology of the network using directed links to
keep the random walks moving in the ?right? direc-
tion. We design the graph such that if we desire a
random walk from nodes of type s to nodes of type t,
the random walk will never be able to follow an out-
going link that does not decrease its distance from
the nodes of t.
As shown in section 2.3, there are certain nodes at
which a random walk (like Pagerank) arrives at more
often than others simply because of their positions in
the graph. This suggests that there may be stationary
random walk distributions over entities, which we
would need to adjust for in order to find the most
significant entities for a topic.
Indeed this is what we do find. As an example, if
we sample topics uniformly and take random walks
to author nodes, by chance we end up at Jun?ichi
Tsujii on 0.3% of random walks, Eduard Hovy on
0.2% of walks, etc. These values are about 1000
times greater than would be expected at random.
To adjust for this effect, when we take a random
walk from a topic x to an entity type t, we subtract
out this stationary distribution for t, which corre-
sponds to the proportion of random walks that end
at any particular entity of type t by chance, and not
by virtue of the fact that the walk started at topic x.
The resulting distribution yields the entities of t that
are most significantly associated with topic x. Ta-
ble 6 gives examples of the most significant entities
for a couple of topics.
?200 ?150 ?100 ?50
RW-cooc
RW-sim
RTM
LDA
Coherence
Figure 2: Distribution of topic coherences for the
four topic models.
4.3 Topic Model Evaluation
We provide two separate evaluations in this section,
one of the topics alone, and one extrinstic evaluation
of the entire paper-topic model. The variants of ran-
dom walk topic models are compared against LDA
and the relational topic model (RTM), each with 100
topics (Chang and Blei, 2010). As RTM allows only
a single type of relationship between documents, we
use citations as the inter-document relationships.
4.3.1 Topic Coherence
The coherence of a topic is evaluated using the co-
herence metric introduced in (Mimno et al., 2011).
Given the top M words V (t) = (v(t)1 , ..., v(t)M ) for a
topic t, the coherence of that topic can be calculated
with the following formula:
C(t;V (t)) =
M?
m=2
m?1?
l=1
log
(
D(v(t)m , v(t)l ) + 1
D(v(t)l )
)
,
where D(v) is the number of documents contain-
ing v and D(v, v?) is the number of documents con-
taining both v and v?.
This measure of coherence is highly correlated
with manual annotations of topic quality, with a
higher coherence score corresponding to a more co-
herent, higher quality topic. After calculating the co-
herence for each of the 100 topics for RTM and the
random-walk topic model, the average coherence for
RTM topics was -135.2 and the average coherence
for word-similarity random walk topics was -122.2,
with statistical significance at p < 0.01. Figure 2
demonstrates this, showing that the word similarity-
based random walk method generates several highly
coherent topics. The average coherence for the LDA
and the co-occurence random walk model were sig-
nificantly lower.
9
4.3.2 Extrinsic Evaluation
One difficulty in evaluating this random-walk
topic model intrinsically against a statistical topic
model like RTM is that existing evaluation measures
assume certain statistical properties of the topic, for
example, that the topics are generated according to a
Dirichlet prior. Because of this, we choose instead to
evaluate this topic model extrinsically with a down-
stream application. We choose an information re-
trieval application, returning a ranked list of similar
documents, given a reference document.
We evaluate five different methods: citation-
RTM, LDA, the two versions of the random-walk
topic model, and a simple word vector similarity
baseline. Similarity between documents with the
topic models are determined by cosine similarity be-
tween the topic vectors of the two documents. Word
vector similarity determines the similarity between
documents by taking the cosine similarity of their
word vectors. From these similarity scores, a ranked
list is produced.
The document set for this task is the set of all pa-
pers appearing at ACL between 2000 and 2011. The
top 10 results returned by each method are pooled
and manually evaluated with a relevance score be-
tween 1 and 10. Thirty such result sets were manu-
ally annotated. We then evaluate each method ac-
cording to its discounted cumulative gain (DCG)
(Ja?rvelin and Keka?la?inen, 2000).
Performance of these methods is summarized in
Table 7. The co-occurence-based random walk topic
model performed comparably with the best per-
former at this task, LDA, and there was no signifi-
cant difference between the two at p < 0.05.
Going forward, an important problem is to rec-
oncile the co-occurence- and word-similarity-based
formulations of this topic model, as the two formu-
lations perform very differently in our two evalua-
tions. Heuristically, the co-occurence model seems
to create good human-readable topics, while the
word-similarity model creates topics that are more
mathematically-coherent, but less human-readable.
5 Related Work
Heterogeneous networks have been studied in a
number of different fields, such as biology (Sio-
son, 2005), transportation networks (Lozano and
Method DCG
Word vector 1.345 ? 0.007
LDA 3.302 ? 0.008
RTM 3.058 ? 0.011
Random-walk (cooc) 3.295 ? 0.006
Random-walk (sim) 2.761 ? 0.007
Table 7: DCG Performance of the various topic
models and baselines on the related document find-
ing task. A 95% confidence interval is provided.
Storchi, 2002), social networks (Lambiotte and Aus-
loos, 2006), and bibliographic networks (Sun et al.,
2011). These networks are also sometimes known
by the name complex networks or multimodal net-
works, but both these terms have other connotations.
We prefer ?heterogeneous networks? as used by Sun
et al. (2009).
There has also been some study of these networks
in general, in community detection (Murata, 2010),
clustering (Long et al., 2008; Sun et al., 2012), and
data mining (Muthukrishnan et al., 2010), but there
has not yet been any comprehensive study. Recently,
NLP has seen several uses of heterogeneous net-
works (though not by that name) for use with label
propagation algorithms (Das and Petrov, 2011; Spe-
riosu et al., 2011) and random walks (Toutanova et
al., 2004; Kok and Brockett, 2010).
Several authors have proposed the idea of using
network centrality measures to rank the impacts of
journals, authors, papers, etc. (Bollen et al., 2006;
Bergstrom et al., 2008; Chen et al., 2007; Liu et al.,
2005), and it has even been proposed that central-
ity can be applicable in bipartite networks (Zhou et
al., 2007). We propose that Pagerank on any gen-
eral heterogeneous network is appropriate for creat-
ing ranked lists for each type of entity. Most previ-
ous papers also lack a robust evaluation, demonstrat-
ing agreement with previous methods or with some
external awards or recognitions. We use a random
graph that replicates the properties of the real-world
network to show that Pagerank on the heterogeneous
network outperforms other methods.
Name disambiguation has been studied in a num-
ber of different settings, including graph-based set-
tings. It is common to use the coauthorship graph
(Kang et al., 2009; Fan et al., 2011), but authors
10
have also used lexical similarity graphs (On and Lee,
2007), citation graphs (McRae-Spencer and Shad-
bolt, 2006), or social networks (Malin, 2005). Al-
most all graph methods are unsupervised.
There have been some topic models developed
specifically for relational data (Wang et al., 2006;
Airoldi et al., 2008), but both of these models have
limitations in the types of relational data they are
able to model. The group topic model described in
(Wang et al., 2006) is able to create stronger topics
by considering associations between words, events,
and entities, but is very coarse in the way it han-
dles the behavior of entities, and does not generalize
to multiple different types of entities. The stochas-
tic blockmodel of (Airoldi et al., 2008) can create
blocks of similar entities in a graph and is general
in the types of graphs it can handle, but produces
less meaningful results on graphs that have specific
schemas.
6 Conclusion and Future Directions
In this paper, we present a heterogeneous net-
work treatment of the ACL Anthology Network and
demonstrate several applications of it. Using only
off-the-shelf graph algorithms with a single data rep-
resentation, the heterogeneous AAN, we are able to
very easily build a scientific impact measure that is
more accurate than existing measures, an author dis-
ambiguation system better than existing graph-based
author disambiguation systems, and a random-walk-
based topic model that is competitive with statistical
topic models.
While there are many other tasks, such as citation-
based summarization, that could likely be ap-
proached using this framework with the appropri-
ate addition of new types of nodes into the hetero-
geneous AAN network, there are even some poten-
tial synergies between the tasks described in this pa-
per that have yet to be explored. For example, we
may consider that the methods of the author disam-
biguation or topic modeling tasks could be to find
the highest-impact papers associated with a term (for
survey generation, perhaps) or high-impact authors
associated with a workshop?s topic (to select good
reviewers for it). We believe that heterogeneous
graphs are a flexible framework that will allow re-
searchers to find simple, flexible solutions for a va-
riety of problems.
Acknowledgments
This research is supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of
Interior National Business Center (DoI/NBC) contract
number D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions con-
tained herein are those of the authors and should not be
interpreted as necessarily representing the official poli-
cies or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg,
and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. The Journal of Machine Learning Re-
search, 9:1981?2014.
Re?ka Albert and Albert-La?szlo? Baraba?si. 2002. Statisti-
cal mechanics of complex networks. Reviews of mod-
ern physics, 74(1):47.
A.L. Baraba?si and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512.
Carl T. Bergstrom, Jevin D. West, and Marc A. Wiseman.
2008. The eigenfactor metrics. The Journal of Neuro-
science, 28(45):11433?11434.
Steven Bird, Robert Dale, Bonnie J Dorr, Bryan Gib-
son, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir R Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational lin-
guistics. In Proc. of the 6th International Conference
on Language Resources and Evaluation Conference
(LREC08), pages 1755?1759.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993?1022.
Johan Bollen, Marko A. Rodriguez, and Herbert Van
de Sompel. 2006. Journal status. CoRR,
abs/cs/0601030.
Johan Bollen, Herbert Van de Sompel, Aric Hagberg, and
Ryan Chute. 2009. A principal component analysis of
39 scientific impact measures. PloS one, 4(6):e6022.
Tibor Braun, Wolfgang Gla?nzel, and Andra?s Schubert.
2006. A hirsch-type index for journals. Scientomet-
rics, 69(1):169?173.
Jonathan Chang and David M Blei. 2010. Hierarchical
relational models for document networks. The Annals
of Applied Statistics, 4(1):124?150.
11
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with googles pagerank
algorithm. Journal of Informetrics, 1(1):8?15.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ
Newman. 2009. Power-law distributions in empirical
data. SIAM review, 51(4):661?703.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
Paul Erdo?s and Alfre?d Re?nyi. 1960. On the evolution of
random graphs. Magyar Tud. Akad. Mat. Kutato? Int.
Ko?zl, 5:17?61.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Xiaoming Fan, Jianyong Wang, Xu Pu, Lizhu Zhou, and
Bing Lv. 2011. On graph-based name disambigua-
tion. J. Data and Information Quality, 2(2):10:1?
10:23, February.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In Working Notes of the AAAI
Fall Symposium on Probabilistic Approaches to Natu-
ral Language, volume 54, page 60.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
363?371. ACL.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Extracting signed social networks from text.
TextGraphs-7, page 6.
Jorge E. Hirsch. 2005. An index to quantify an indi-
vidual?s scientific research output. Proceedings of the
National Academy of Sciences of the United states of
America, 102(46):16569.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 50?57. ACM.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2000. IR evalua-
tion methods for retrieving highly relevant documents.
In Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 41?48. ACM.
In-Su Kang, Seung-Hoon Na, Seungwoo Lee, Hanmin
Jung, Pyung Kim, Won-Kyung Sung, and Jong-Hyeok
Lee. 2009. On co-authorship for author disam-
biguation. Information Processing & Management,
45(1):84?97.
Brian Karrer and Mark EJ Newman. 2011. Stochas-
tic blockmodels and community structure in networks.
Physical Review E, 83(1):016107.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 145?153. ACL.
Oren Kurland and Lillian Lee. 2005. Pagerank without
hyperlinks: Structural reranking using links induced
by language models. In SIGIR ?05.
Renaud Lambiotte and Marcel Ausloos. 2006. Collabo-
rative tagging as a tripartite network. Computational
Science?ICCS 2006, pages 1114?1117.
David Liben-Nowell and Jon Kleinberg. 2007. The link-
prediction problem for social networks. Journal of the
American society for information science and technol-
ogy, 58(7):1019?1031.
R.N. Lichtenwalter, J.T. Lussier, and N.V. Chawla. 2010.
New perspectives and methods in link prediction. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 243?252. ACM.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. Infor-
mation processing & management, 41(6):1462?1480.
Bo Long, Zhongfei Zhang, and Tianbing Xu. 2008.
Clustering on complex graphs. In Proc. the 23rd Conf.
AAAI 2008.
Angelica Lozano and Giovanni Storchi. 2002. Shortest
viable hyperpath in multimodal networks. Transporta-
tion Research Part B: Methodological, 36(10):853?
874.
Bradley Malin. 2005. Unsupervised name disambigua-
tion via social network similarity. In Workshop on
Link Analysis, Counterterrorism, and Security, vol-
ume 1401, pages 93?102.
Sergei Maslov and Sidney Redner. 2008. Promise
and pitfalls of extending google?s pagerank algorithm
to citation networks. The Journal of Neuroscience,
28(44):11103?11105.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Duncan M. McRae-Spencer and Nigel R. Shadbolt.
2006. Also by the same author: Aktiveauthor, a cita-
tion graph approach to name disambiguation. In Pro-
ceedings of the 6th ACM/IEEE-CS joint conference on
Digital libraries, pages 53?54. ACM.
12
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT-EMNLP, pages 411?418. ACL.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 262?272. ACL.
Panchanan Mitra. 2006. Hirsch-type indices for rank-
ing institutions scientific research output. Current Sci-
ence, 91(11):1439.
Tsuyoshi Murata. 2010. Detecting communities from
tripartite networks. In Proceedings of the 19th inter-
national conference on World wide web, pages 1159?
1160. ACM.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over mul-
tiple graphs for similarity learning. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 374?383. IEEE.
Mark E.J. Newman and Juyong Park. 2003. Why social
networks are different from other types of networks.
Physical Review E, 68(3):036122.
Byung-Won On and Dongwon Lee. 2007. Scalable name
disambiguation using multi-level graph partition. In
Proceedings of the 7th SIAM international conference
on data mining, pages 575?580.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web.
Romualdo Pastor-Satorras and Alessandro Vespignani.
2001. Epidemic spreading in scale-free networks.
Physical review letters, 86(14):3200?3203.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The ACL
anthology network corpus. Language Resources and
Evaluation, pages 1?26.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846?850.
S. Redner. 1998. How popular is your paper? an empir-
ical study of the citation distribution. The European
Physical Journal B-Condensed Matter and Complex
Systems, 4(2):131?134.
P. Sarkar, A.W. Moore, and A. Prakash. 2008. Fast incre-
mental proximity search in large graphs. In Proceed-
ings of the 25th international conference on Machine
learning, pages 896?903. ACM.
Allan A. Sioson. 2005. Multimodal networks in biology.
Ph.D. thesis, Virginia Polytechnic Institute and State
University.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 53?63, Edin-
burgh, Scotland, July. ACL.
Alexander Strehl and Joydeep Ghosh. 2003. Cluster
ensembles?a knowledge reuse framework for com-
bining multiple partitions. The Journal of Machine
Learning Research, 3:583?617.
Yizhou Sun, Jiawei Han, Peixiang Zhao, Zhijun Yin,
Hong Cheng, and Tianyi Wu. 2009. Rankclus: inte-
grating clustering with ranking for heterogeneous in-
formation network analysis. In Proceedings of the
12th International Conference on Extending Database
Technology: Advances in Database Technology, pages
565?576. ACM.
Yizhou Sun, Rick Barber, Manish Gupta, and Jiawei Han.
2011. Co-author relationship prediction in heteroge-
neous bibliographic networks.
Yizhou Sun, Charu C. Aggarwal, and Jiawei Han. 2012.
Relation strength-aware clustering of heterogeneous
information networks with incomplete attributes. Pro-
ceedings of the VLDB Endowment, 5(5):394?405.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Neural Information Processing Systems, volume 15.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
2001. Estimating the number of clusters in a data
set via the gap statistic. Journal of the Royal Sta-
tistical Society: Series B (Statistical Methodology),
63(2):411?423.
Kristina Toutanova, Christopher D Manning, and An-
drew Y Ng. 2004. Learning random walk models
for inducing word dependency distributions. In Pro-
ceedings of the twenty-first international conference
on Machine learning, page 103. ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2006. Group and topic discovery from relations and
their attributes. Technical report, DTIC Document.
Kai Yu, Shipeng Yu, and Volker Tresp. 2006. Soft
clustering on graphs. Advances in Neural Information
Processing Systems, 18:1553.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Data Mining,
2007. ICDM 2007. Seventh IEEE International Con-
ference on, pages 739?744. IEEE.
13
14
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146?154,
Dublin, Ireland, August 23 2014.
Experiments in Sentence Language Identification with Groups of Similar
Languages
Ben King
Department of EECS
University of Michigan
Ann Arbor
benking@umich.edu
Dragomir Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor
radev@umich.edu
Steven Abney
Department of Linguistics
University of Michigan
Ann Arbor
abney@umich.edu
Abstract
Language identification is a simple problem that becomes much more difficult when its usual
assumptions are broken. In this paper we consider the task of classifying short segments of text in
closely-related languages for the Discriminating Similar Languages shared task, which is broken
into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech
and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish,
and (F) American and British English. We consider a number of different methods to boost
classification performance, such as feature selection and data filtering, but we ultimately find that
a simple na??ve Bayes classifier using character and word n-gram features is a strong baseline that
is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.
1 Introduction
Language identification constitutes the first stage of many NLP pipelines. Before applying tools trained
on specific languages, one must determine the language of the text. It is also is often considered to be a
solved task because of the high accuracy of language identification methods in the canonical formulation
of the problem with long monolingual documents and a set of mostly dissimilar languages to choose
from. We consider a different setting with much shorter text in the form of single sentences drawn from
very similar languages or dialects.
This paper describes experiments related to and our submissions to the Discriminating Similar Lan-
guages (DSL) shared task. This shared task has six subtasks, each a classification task in which a sentence
must be labeled as belonging to a small set of related languages:
? Task A: Bosnian vs. Croatian vs. Serbian
? Task B: Indonesian vs. Malay
? Task C: Czech vs. Slovak
? Task D: Brazilian vs. European Portuguese
? Task E: Argentinian vs. Peninsular Spanish
? Task F: American vs. British English
The first three tasks involve classes that could be rightly called separate languages or dialects. The
classes of each of the final three tasks have high mutual intelligibility and are so similar that some
linguists may not even classify them as separate dialects. We will use the term ?language variant? to
refer to such classes.
In this paper we experiment with several types of methods aimed at improving the classification ac-
curacy of these tasks: machine learning methods, data pre-processing, feature selection, and additional
training data. We find that a simple na??ve Bayes classifier using character and word n-gram features is
a strong baseline that is difficult to improve on. Because this paper covers so many different types of
methods, its format eschews the standard ?Results? section, instead providing comparisons of methods
as they are presented.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
146
2 Related Work
Recent directions in language identification have included finer-grained language identification (King
and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs
(Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely
related languages.
Language identification for closely related languages has been considered by several researchers,
though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguish-
ing Croatian from Serbian and Slovenian is explored by Ljube?si?c et al. (2007), who used a list of most
frequent words along with a Markov model and a word blacklist, a list of words that are not allowed
to appear in a certain language. A similar approach was later used by Tiedemann and Ljube?si?c (2012)
to distinguish Bosnian, Croatian, and Serbian. They further develop the idea of a blacklist classifier,
loosening the binary restriction of the earlier work?s blacklist and considering the frequencies of words
rather than their absolute counts. This blacklist classifier is able to outperform a na??ve Bayes classifier
with large amounts of training data. They also find training on parallel data to be important, as it al-
lows the machine learning methods to pick out features relating to the differences between the languages
themselves, rather than learning differences in domain.
Zampieri et al. consider classes that would be most often classified as language varieties rather than
separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al.,
2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and
Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity
between a document and a corpus according to the size of the intersection between the sets of types in
each.
A similar, but somewhat different problem of automatically identifying lexical variants between
closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they
are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch.
3 Data
This paper?s training data and evaluation data both come from the DSL corpus collection (DSLCC)
(Tan et al., 2014). We use the training section of this data for training and the development section for
evaluation. The training section consists of 18,000 labeled instances per class, while the development
section has 2,000 labeled instances per class.
In order to try to increase classifier accuracy (and to avoid the problems with the task F training
data), we decided to collect additional training data for each open-class task. For each task, we collected
newspaper text from the appropriate websites for each of the 2?3 languages. We used regular expressions
to split the text into sentences, and created a set of rules to filter out strings that were unlikely to be good
sentences. Because the pages on the newspaper websites tended to have some boilerplate text, we collated
all the sentences and only kept one copy of each sentence.
Task Language/Dialect Newspaper Sentences Words
A
Bosnian Nezavisne Novine 175,741 3,250,648
Croatian Novi List 231,271 4,591,318
Serbian Ve?cernje Novosti 239,390 5,213,507
B
Indonesian Kompas 114,785 1,896,138
Malay Berita Harian 36,144 695,597
C
Czech Den??k 160,972 2,432,393
Slovak Denn??k SME 62,908 970,913
D
Brazilian Portuguese O Estado de S. Paulo 558,169 11,199,168
European Portuguese Correio da Manh?a 148,745 2,979,904
E
Argentinian Spanish La Naci?on 333,246 7,769,941
Peninsular Spanish El Pa??s 195,897 4,329,480
F
American English The New York Times 473,350 10,491,641
British English The Guardian 971,097 20,288,294
Table 1: Sources and amounts of training data collected for the open track for each task.
147
In order to create balanced training data, for each task we downsampled the number of sentences of
the larger collection(s) to match the number of sentences in the smaller collection. For example, we
downsampled the British English collection to 473,350 sentences and combined it with the American
English sentences to create the training data for English. Figure 1 shows results of training using this
external data.
3.1 Features
We use many types of features that have been found to be useful in previous language identification
work: word unigrams, word bigrams, and character n-grams (2 ? n ? 6). Character n-grams are simply
substrings of the sentence and may include in addition to letters, whitespace, punctuation, digits, and
anything else that might be in the sentence. Words, for the purpose of word unigrams and bigrams, are
simply maximal tokens not containing any punctuation, digit, or whitespace.
When instances are encoded into feature vectors, each feature has a value equal to the number of times
it occured in the corresponding sentence, so the majority of features have a value of 0 for any given
instance, but it is possible for a feature to occur multiple times in a sentence and have a value greater
than 1.0 in the feature vector. Table 2 below compares the performance of a na??ve Bayes classifier using
each of the different feature groups below.
Word Character
Task All 1 2 2 3 4 5 6
Bosnian/Croatian/Serbian 0.9348 0.9290 0.8183 0.7720 0.8808 0.9412 0.9338 0.9323
Indonesian/Malay 0.9918 0.9943 0.9885 0.8545 0.9518 0.9833 0.9908 0.9930
Czech/Slovak 0.9998 1.0000 0.9985 0.9980 0.9998 0.9998 1.0000 1.0000
Portuguese 0.9535 0.9468 0.9493 0.7935 0.8888 0.9318 0.9468 0.9570
Spanish 0.8623 0.8738 0.8625 0.7673 0.8273 0.8513 0.8610 0.8660
English 0.4970 0.4948 0.5005 0.4825 0.4988 0.5010 0.5048 0.4993
Average 0.8732 0.8731 0.8529 0.7780 0.8412 0.8681 0.8729 0.8746
Table 2: Accuracies compared for different sets of features compared. The classifier used here is na??ve
Bayes.
4 Methods
Our baseline method against which we compare all other models is a na??ve Bayes classifier using word
unigram features trained on the DSL-provided training data. The methods we compare to it can be
broken into three classes: other machine learning methods, feature selection methods, and data filtering
methods.
The classification pipeline used here has the following stages: (1) data filtering, (2) feature extraction,
(3) feature selection, (4) training, and (5) classification.
4.1 Machine Learning Methods
We will use the following notation throughout this section. An instance x, that is, a sentence to be
classified, with a corresponding class label y is encoded into a feature vector f(x), where each entry
is an integer denoting how many times the feature corresponding to that entry?s index occurred in the
sentence. The class label here is a language and it?s drawn from a small set y ? Y .
In addition to the na??ve Bayes classifier, we also experiment with two versions of logistic regression
and a support vector machine classifier. The MALLET machine learning library implementations are
used for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ).
Na??ve Bayes A na??ve Bayes classifier models the class label as an independent combination of input
features.
148
P (y|f(x)) =
1
P (f(x))
P (y)
n
?
i=1
P (f(x)
i
|y) (1)
As na??ve Bayes is a generative classifier, it has been shown to be able to outperform discriminative
classifiers when the number of training instances is small compared to the number of features (Ng and
Jordan, 2002). This classifier is additionally advantageous in that it has a simple closed-form solution
for maximizing its log likelihood.
Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are
encoded in a vector ?. The conditional probability of a class label over an instance (x, y) is modeled as
follows:
P (y|x; ?) =
1
Z(x; ?)
exp {f(x, y) ? ?} ; Z(x, ?) =
?
y?Y
exp {f(x, y) ? ?} (2)
The parameter vector ? is commonly estimated by maximizing the log-likelihood of this function over
the set of training instances (x, y) ? T in the following way:
? = argmax
?
?
(x,y)?T
logP (y
i
|x
i
; ?)? ?R(?) (3)
The term R(?) above is a regularization term. It is common for such a classifier to overfit the pa-
rameters to the training data. To keep this from happening, a regularization term can be added which
keeps the parameters in ? from growing too large. Two common choices for this function are L2 and L1
normalization:
R
L2
= ||?||
2
2
=
n
?
i=1
?
2
i
, R
L1
= ||?||
1
=
n
?
i=1
|?
i
| (4)
L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior
on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the
parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is
greater than the number of training examples, which we expect to be the case with many of the tasks in
this paper (Ng, 2004).
Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts
to find a boundary that linearly separates the training data with the maximum possible margin. SVMs
have been shown to be a very efficient and high accuracy method to classify data across a wide variety
of different types of tasks (Tsochantaridis et al., 2004).
Table 3 below compares these machine learning methods. Because of its consistently good perfor-
mance across tasks, we use a na??ve Bayes classifier throughout the rest of the paper.
4.2 Feature Selection Methods
We expect that the majority of features are not relevant to the classification task, and so we experimented
with several methods of feature selection, both manual and automatic.
Information Gain As a fully automatic method of feature extraction, we used information gain to
score features according to their expected usefulness. Information gain (IG) is an information theoretic
concept that (colloquially) measures the amount of knowledge about the class label that is gained by
having access to a specific feature. If f is the occurence an individual feature and
?
f the non-occurence
of a feature, we measure its information gain by the following formula:
G(f) = P (f)
?
?
?
y?Y
P (y|f)logP (y|f)
?
?
+ P (
?
f)
?
?
?
y?Y
logP (y|
?
f)logP (y|
?
f)
?
?
(5)
149
Task
Logistic
Regression
(L2-norm)
Logistic
Regression
(L1-norm)
Na??ve Bayes SVM
Bosnian/Croatian/Serbian 0.9138 0.9135 0.9290 0.9100
Indonesian/Malay 0.9878 0.9810 0.9943 0.9873
Czech/Slovak 0.9983 0.9958 1.0000 0.9985
Portuguese 0.9383 0.9368 0.9468 0.9325
Spanish 0.8843 0.8770 0.8738 0.8768
English 0.5000 0.4945 0.4948 0.4958
Average 0.8704 0.8648 0.8731 0.8668
Table 3: Comparison of different machine learning methods using word unigram features on the six
tasks.
To reduce the number of features being used in classification (and to hopefully remove irrelevant
features), we choose the 10,000 features with the highest IG scores. IG considers each feature indepen-
dently, so it is possible that redundant feature sets could be chosen. For example, it might happen that
both the quadrigram ther and the trigram the score highly according to IG and are both selected, even
though they are highly correlated with one another.
Parallel Text Feature Selection Because IG feature selection often seemed to choose features more
related to differences in domain than to differences in language (see Table 7), we wanted to try to isolate
features that are specific to language differences. It has been shown in previous work that training on
parallel text can help to isolate language differences since the domains of the languages are identical
(Tiedemann and Ljube?si?c, 2012). For each of the tasks,
1
we use translations of the complete Bible as a
parallel corpus, running IG feature selection exactly as above. Table 4 below gives more details about
the texts used.
Task Language/Dialect Bible
B
Indonesian Alkitab dalam Bahasa Indonesia Masa Kini
Malay 2001 Today?s Malay Version
C
Czech Cesk?y studijn?? preklad
Slovak Slovensk?y Ekumenick?y Biblia
D
Brazilian Portuguese a B
?
IBLIA para todos
European Portuguese Almeida Revista e Corrigida (Portugal)
E
Argentinian Spanish La Palabra (versi?on hispanoamericana)
Peninsular Spanish La Palabra (versi?on espa?nola)
F
American English New International Version
British English New International Version Anglicized
Table 4: Bibles used as parallel corpora for feature selection.
Manual Feature Selection We also used manual feature selection, selecting features to use in the clas-
sifiers from lists published on Wikipedia comparing the two languages. Of course some of the features in
lists like these are features that are quite difficult to detect using NLP (especially before the language has
been identified) such as characteristic passive or genitive constructions. But there are many features that
we are able to detect and use in a list of manually selected features, such as character n-grams relating
to morphology and spelling and word n-grams relating to vocabulary differences.
Table 5 below compares these feature selection methods on each task. Since the manual feature selec-
tion suggested all types of features, including character n-gram and word unigram and bigram features,
the experiments in this section use all features described in Section 3.1. The results show that any type
of feature selection consistently hurts performance, though IG hurts the least, and it should be noted
that in certain cases with other machine learning methods, IG feature selection actually yielded better
1
excluding Task A, for which we were unable to find a Bible in Latin-script Serbian or any Bible in Bosnian
150
performance than all features. That the feature selection methods designed to isolate language-specific
features performed so poorly is one indicator that the labeled data has additional differences that are not
tied to the languages themselves. We discuss this idea further in Section 5.
Task No feature selection IG Parallel Manual
Bosnian/Croatian/Serbian 0.9348 0.9300 ? 0.6328
Indonesian/Malay 0.9918 0.9768 0.8093 0.8485
Czech/Slovak 0.9998 0.9995 0.9940 0.8118
Portuguese 0.9535 0.9193 0.7215 0.6888
Spanish 0.8623 0.8310 0.5210 0.7023
English 0.4970 0.4978 0.5020 0.5053
Average 0.8732 0.8590 ? 0.6982
Table 5: Comparison of manual and automatic feature selection methods. IG and parallel feature selec-
tion both use the 10,000 features with the highest IG scores.
4.3 Data Filtering Methods
English Word Removal In looking through the training data for the non-English tasks, we observed
that it was not uncommon for sentences in these languages to contain English words and phrases. Be-
cause foreign words should be independent of the language/dialect used, English words included in the
sentences for other tasks should just be noise that, if removed will improve classification performance.
For each of the non-English tasks (A, B, C, D, and E), we create a new training set for identifying
English/non-English words by mixing together 1,000 random English words with 10,000 random task-
language words. The imbalance in the classes is a compromise, approximating the actual proportions in
the test without leading to a degenerate classifier. Because English and the other classes are so dissimilar,
the performance of the English word classifier is very insensitive to the actual ratio. From this data, we
train a na??ve Bayes classifier using character 3-grams, 4-grams, and 5-grams.
We manually labeled the words of 150 sentences from the five non-English tasks in order to evaluate
the English word classifier. Across the five tasks, the precision was 0.76 and the recall was 0.66, leading
to an F1-score of 0.70. Any words labeled as English by the classifier were removed from the sentence
and it was passed on to the feature extraction, classification, and training stages.
Named Entity Removal We also observed another common class of word that could potentially act
as a noise source: named entities. Across all the languages listed studied here, it is common for named
entities to begin with a capital letter. Lacking named entity recognizers for all the languages here, we
instead used the property of having an initial capital letter as a surrogate for recognizing a word as a
named entity. Because all the languaes studied here also have the convention of capitalizing the first
word of a sentence, we remove all words beginning with a capital letter except for the first and pass this
abridged sentence on to the feature extraction, classification, and training stages.
Task No data filtering
English Word
Removal
Named Entity
Removal
Bosnian/Croatian/Serbian 0.9138 0.9105 0.9003
Indonesian/Malay 0.9878 0.9885 0.9778
Czech/Slovak 0.9983 0.9980 0.9973
Portuguese 0.9383 0.9365 0.9068
Spanish 0.8843 0.8835 0.8555
English 0.5000 0.5000 0.5050
Average 0.8704 0.8695 0.8571
Table 6: Comparison of data filtering methods using word unigram features on the six tasks.
151
(A)
0 0.2 0.4 0.6 0.8 1
?10
5
0.4
0.6
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(B)
0 0.5 1 1.5 2 2.5
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(C)
0 0.5 1 1.5
?10
5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(D)
0 0.2 0.4 0.6 0.8 1
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(E)
0 0.5 1 1.5
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(F)
0 0.5 1 1.5 2
?10
5
0.5
0.6
0.7
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
Figure 1: Learning curves for the six tasks as the number of training instances per language is varied.
The line marked ?DSL? is the learning curve for the DSL-provided training data evaluated against the
developement data. The line marked ?external? is our external newspaper training data evaluated against
the development data. The line marked ?external (CV)? is our external training data evaluated using
10-fold cross-validation.
152
Bosnian/Croatian/Serbian Indonesian/Malay Czech/Slovak Portuguese Spanish English
da bisa sa Portugal the I
kako berkata se R Rosario you
sa kerana aj euros han The
kazao karena ako Brasil euros said
takode daripada ve cento Argentina Obama
rekao saat pre governo PP your
evra dari pro Lusa Fe If
tijekom beliau ktor?e PSD Rajoy that
posle selepas s?u Ele Espa?na but
posto bahwa ktor?y Governo Madrid It
Table 7: The ten word-unigram features given the highest weight by information gain feature selection
for each of the six tasks.
5 Discussion
Across many of the tasks, there was evidence that performance was tied more strongly to domain-specific
features of the two classes rather than to language- (or language-variant-) specific features. For example,
Table 7 shows the best word-unigram features selected by information gain feature selection for each of
the tasks. The Portuguese, Spanish, and English tasks specifically have as many of their most important
features named entities and other non-language specific features.
It seems that for many of the tasks, it is easier to distinguish the subject matter written about than it is to
distinguish the languages/dialects themselves. With Portuguese, for example, Brazilian dialect speakers
were much more likely to discuss places in Brazil and mention Brazilian reais (currency, abbreviated
as R), while European speakers mentioned euros, places in Portugal, and discussed Portuguese politics.
While there are definite linguistic differences between Brazilian and European Portuguese, these seem
to be less pronounced than the superficial differences in subject matter.
Practically, this is not necessarily a bad thing for this shared task, as the domain information gives extra
clues that allow the task to be completed with higher accuracy than would otherwise be possible. This
would become problematic if one wanted to apply a classifier trained on this data to general domains,
where the classifier may not be able to rely on the speaker talking about a certain subject matter. To
address this, the classifier would either need to focus on features specific to the language pair itself or
would need to be trained on data that spanned many domains.
Further evidence of domain overfitting comes from the fact that the larger training sets drawn from
newspaper text were not able to improve performance on the development set over the provided training
data, which is presumably drawn from the same collection as the development data. Figure 1 shows
learning curves for each of the six tasks. Though all the external text is self-consistent (cross-validation
results in high accuracy), in none of the cases does training on a large amount of external data allow the
classifier to exceed the accuracy achieved by training on the DSL data.
6 Conclusion
In this paper we experimented with several methods for classification of sentences in closely-related lan-
guages for the DSL shared task. Our analysis showed that, when dealing with closely related languages,
the task of classifying text according to its language was difficult to untie from the taks of classifying
other text characteristics, such as the domain. Across all our types of methods, we found that a na??ve
Bayes classifier using character n-gram, word unigram, and word bigram features was a strong baseline.
In future work, we would like to try to improve on these results by incorporating features that try to
capture syntactic relationships. Certainly some of the pairs of languages considered here are close enough
that they could be chunked, tagged, or parsed before knowing exactly which variety they belong to. This
would allow for the inclusion of features related to transitivity, agreement, complementation, etc. For
example, in British English, the verb ?provide? is monotransitive, but ditransitive in American English. It
is unclear how much features like these would improve accuracy, but it is likely that they would ultimately
be necessary to improve classification of similar languages to human levels of performance.
153
References
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identi-
fication for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language
in Social Media, pages 65?74. Association for Computational Linguistics.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195?215.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on
top-bag-of-word similarity. pages 404?410.
Thorsten Joachims. Svmlight: Support vector machine. http://svmlight. joachims. org/.
Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL-HLT, pages 1110?1119.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identication: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces, pages
541?546.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27?40.
Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic
regression and naive bayes. Advances in neural information processing systems, 2:841?848.
Andrew Y Ng. 2004. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the
twenty-first international conference on Machine learning, page 78. ACM.
Dong-Phuong Nguyen and A Seza Dogruoz. 2013. Word level language identification in online multilingual
communication. Association for Computational Linguistics.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16(4):469?491.
Jason Rennie. 2004. On l2-norm regularization and the gaussian prior.
http://people.csail.mit.edu/jrennie/writing.
Liling Tan, Marcos Zampieri, Nikola Ljube?sic, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The dsl corpus collection. In Proceedings of The 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
COLING, pages 2619?2634.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector ma-
chine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international
conference on Machine learning, page 104. ACM.
Marcos Zampieri and Binyam Gebrekidan. 2012. Automatic identification of language varieties: The case of
portuguese. In Proceedings of KONVENS, pages 233?237.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2012. Classifying pluricentric languages:
Extending the monolingual model. In Proceedings of the Fourth Swedish Language Technlogy Conference
(SLTC2012), pages 79?80.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos
distribution for the identification of spanish varieties. Proceedings of TALN2013, Sable dOlonne, France, pages
580?587.
154
