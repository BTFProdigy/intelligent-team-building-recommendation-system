Unsupervised Type and Token Identification
of Idiomatic Expressions
Afsaneh Fazly?
University of Toronto
Paul Cook??
University of Toronto
Suzanne Stevenson?
University of Toronto
Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it
is not clear exactly how people learn and understand them. They are of special interest to
linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic
idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the
properties of idioms in the linguistics literature, there is not much agreement on which properties
are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have
mostly been overlooked by researchers in computational linguistics. In this article, we look
into the usefulness of some of the identified linguistic properties of idioms for their automatic
recognition. Specifically, we develop statistical measures that each model a specific property
of idiomatic expressions by looking at their actual usage patterns in text. We use these sta-
tistical measures in a type-based classification task where we automatically separate idiomatic
expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface
literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of
the measures in a token identification task where we distinguish idiomatic and literal usages of
potentially idiomatic expressions in context.
1. Introduction
Idioms form a heterogeneous class, with prototypical examples such as by and large, kick
the bucket, and let the cat out of the bag. It is hard to find a single agreed-upon definition
that covers all members of this class (Glucksberg 1993; Cacciari 1993; Nunberg, Sag,
and Wasow 1994), but they are often defined as sequences of words involving some de-
gree of semantic idiosyncrasy or non-compositionality. That is, an idiom has a different
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: afsaneh@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: pcook@cs.toronto.edu.
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: suzanne@cs.toronto.edu.
Submission received: 12 September 2007; revised submission received: 29 February 2008; accepted for
publication: 6 May 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
meaning from the simple composition of the meaning of its component words. Idioms
are widely and creatively used by speakers of a language to express ideas cleverly, eco-
nomically, or implicitly, and thus appear in all languages and in all text genres (Sag et al
2002). Many expressions acquire an idiomatic meaning over time (Cacciari 1993); conse-
quently, new idioms come into existence on a daily basis (Cowie, Mackin, and McCaig
1983; Seaton and Macaulay 2002). Automatic tools are therefore necessary for assisting
lexicographers in keeping lexical resources up to date, as well as for creating and ex-
tending computational lexicons for use in natural language processing (NLP) systems.
Though completely frozen idioms, such as by and large, can be represented as
words with spaces (Sag et al 2002), most idioms are syntactically well-formed phrases
that allow some variability in expression, such as shoot the breeze and hold fire (Gibbs
and Nayak 1989; d?Arcais 1993; Fellbaum 2007). Such idioms allow a varying degree
of morphosyntactic flexibility?for example, held fire and hold one?s fire allow for an
idiomatic reading, whereas typically only a literal interpretation is available for fire was
held and held fires. Clearly, a words-with-spaces approach does not work for phrasal
idioms. Hence, in addition to requiring NLP tools for recognizing idiomatic expressions
(types) to include in a lexicon, methods for determining the allowable and preferred
usages (a.k.a. canonical forms) of such expressions are also needed. Moreover, in many
situations, an NLP system will need to distinguish a usage (token) of a potentially
idiomatic expression as either idiomatic or literal in order to handle a given sequence of
words appropriately. For example, a machine translation system must translate held fire
differently in The army held their fire and The worshippers held the fire up to the idol.
Previous studies focusing on the automatic identification of idiom types have often
recognized the importance of drawing on their linguistic properties, such as their se-
mantic idiosyncrasy or their restricted flexibility, pointed out earlier. Some researchers
have relied on a manual encoding of idiom-specific knowledge in a lexicon (Copestake
et al 2002; Odijk 2004; Villavicencio et al 2004), whereas others have presented ap-
proaches for the automatic acquisition of more general (hence less distinctive) knowl-
edge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work
that looks into the acquisition of the distinctive properties of idioms has been limited,
both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid,
and Spranger 2004). Our goal is to develop unsupervised means for the automatic
acquisition of lexical, syntactic, and semantic knowledge about a broadly documented
class of idiomatic expressions.
Specifically, we focus on a cross-linguistically prominent class of phrasal idioms
which are commonly and productively formed from the combination of a frequent verb
and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg,
Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and
push one?s luck. We refer to these as verb+noun idiomatic combinations or VNICs.1
We present a comprehensive analysis of the distinctive linguistic properties of phrasal
idioms, including VNICs (Section 2), and propose statistical measures that capture each
property (Section 3). We provide a multi-faceted evaluation of the measures (Section 4),
showing their effectiveness in the recognition of idiomatic expressions (types)?that is,
separating them from similar-on-the-surface literal phrases?as well as their superiority
to existing state-of-the-art techniques. Drawing on these statistical measures, we also
propose an unsupervised method for the automatic acquisition of an idiom?s canonical
1 We use the abbreviation VNIC and the term expression to refer to a verb+noun type with a potential
idiomatic meaning. We use the terms instance and usage to refer to a token occurrence of an expression.
62
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
forms (e.g., shoot the breeze as opposed to shoot a breeze), and show that it can successfully
accomplish the task (Section 5).
It is possible for a single VNIC to have both idiomatic and non-idiomatic (literal)
meanings. For example, make a face is ambiguous between an idiom, as in The little girl
made a funny face at her mother, and a literal combination, as in She made a face on the
snowman using a carrot and two buttons. Despite the common perception that phrases
that can be idioms are mainly used in their idiomatic sense, our analysis of 60 idioms
has shown otherwise. We found that close to half of these also have a clear literal
meaning; and of those with a literal meaning, on average around 40% of their usages
are literal. Distinguishing token phrases as idiomatic or literal combinations of words is
thus essential for NLP tasks, such as semantic parsing and machine translation, which
require the identification of multiword semantic units.
Most recent studies focusing on the identification of idiomatic and non-idiomatic
tokens either assume the existence of manually annotated data for a supervised clas-
sification (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually
encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005;
Hashimoto, Sato, and Utsuro 2006), or even ignore the specific properties of non-
literal language and rely mainly on general purpose methods for the task (Birke and
Sarkar 2006). We propose unsupervised methods that rely on automatically acquired
knowledge about idiom types to identify their token occurrences as idiomatic or literal
(Section 6). More specifically, we explore the hypothesis that the type-based knowledge
we automatically acquire about an idiomatic expression can be used to determine
whether an instance of the expression is used literally or idiomatically (token-based
knowledge). Our experimental results show that the performance of the token-based
idiom identificationmethods proposed here is comparable to that of existing supervised
techniques (Section 7).
2. Idiomaticity, Semantic Analyzability, and Flexibility
Although syntactically well-formed, phrasal idioms (including VNICs) involve a certain
degree of semantic idiosyncrasy. This means that phrasal idioms are to some extent
nontransparent; that is, even knowing the meaning of the individual component words,
the meaning of the idiom is hard to determine without special context or previous ex-
posure. There is much evidence in the linguistics literature that idiomatic combinations
also have idiosyncratic lexical and syntactic behavior. Here, we first define semantic
analyzability and elaborate on its relation to semantic idiosyncrasy or idiomaticity. We
then expound on the lexical and syntactic behavior of VNICs, pointing out a suggestive
relation between the degree of idiomaticity of a VNIC and the degree of its lexicosyn-
tactic flexibility.
2.1 Semantic Analyzability
Idioms have been traditionally believed to be completely non-compositional (Fraser
1970; Katz 1973). This means that unlike compositional combinations, the meaning
of an idiom cannot be solely predicted from the meaning of its parts. Nonetheless,
many linguists and psycholinguists argue against such a view, providing evidence
from idioms that show some degree of semantic compositionality (Nunberg, Sag, and
Wasow 1994; Gibbs 1995). The alternative view suggests that many idioms in fact do
63
Computational Linguistics Volume 35, Number 1
have internal semantic structure, while recognizing that they are not compositional in a
simplistic or traditional sense. To explain the semantic behavior of idioms, researchers
who take this alternative view thus use new terms such as semantic decomposability
and/or semantic analyzability in place of compositionality.
To say that an idiom is semantically analyzable to some extent means that the
constituents contribute some sort of independent meaning?not necessarily their literal
semantics?to the overall idiomatic interpretation. Generally, the more semantically
analyzable an idiom is, the easier it is to map the idiom constituents onto their cor-
responding idiomatic referents. In other words, the more semantically analyzable an
idiom is, the easier it is to make predictions about the idiomatic meaning from the
meaning of the idiom parts. Semantic analyzability is thus inversely related to semantic
idiosyncrasy.
Many linguists and psycholinguists conclude that idioms clearly form a heteroge-
neous class, not all of them being truly non-compositional or unanalyzable (Abeille?
1995; Moon 1998; Grant 2005). Rather, semantic analyzability in idioms is a matter of
degree. For example, the meaning of shoot the breeze (?to chat idly?), a highly idiomatic
expression, has nothing to do with either shoot or breeze. A less idiomatic expression,
such as spill the beans (?to reveal a secret?), may be analyzed as spill metaphorically
corresponding to ?reveal? and beans referring to ?secret(s).? An idiom such as pop the
question is even less idiomatic because the relations between the idiom parts and their
idiomatic referents are more directly established, namely, pop corresponds to ?suddenly
ask? and question refers to ?marriage proposal.? As we will explain in the following
section, there is evidence that the difference in the degree of semantic analyzability of
idiomatic expressions is also reflected in their lexical and syntactic behavior.
2.2 Lexical and Syntactic Flexibility
Most idioms are known to be lexically fixed, meaning that the substitution of a near syn-
onym (or a closely related word) for a constituent part does not preserve the idiomatic
meaning of the expression. For example, neither shoot the wind nor hit the breeze are valid
variations of the idiom shoot the breeze. Similarly, spill the beans has an idiomatic meaning,
while spill the peas and spread the beans have only literal interpretations. There are, how-
ever, idiomatic expressions that have one (or more) lexical variants. For example, blow
one?s own trumpet and toot one?s own horn have the same idiomatic interpretation (Cowie,
Mackin, and McCaig 1983); also keep one?s cool and lose one?s cool have closely related
meanings (Nunberg, Sag, and Wasow 1994). Nonetheless, it is not the norm for idioms
to have lexical variants; when they do, there are usually unpredictable restrictions on
the substitutions they allow.
Idiomatic combinations are also syntactically distinct from compositional combi-
nations. Many VNICs cannot undergo syntactic variations and at the same time retain
their idiomatic interpretations. It is important, however, to note that VNICs differ with
respect to the extent to which they can tolerate syntactic operations, that is, the degree
of syntactic flexibility they exhibit. Some are syntactically inflexible for the most part,
whereas others are more versatile, as illustrated in the sentences in Examples (1) and (2):
1. (a) Sam and Azin shot the breeze.
(b) ?? Sam and Azin shot a breeze.
(c) ?? Sam and Azin shot the breezes.
(d) ?? Sam and Azin shot the casual breeze.
64
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(e) ?? The breeze was shot by Sam and Azin.
(f) ?? The breeze that Sam and Azin shot was quite refreshing.
(g) ?? Which breeze did Sam and Azin shoot?
2. (a) Azin spilled the beans.
(b) ? Azin spilled some beans.
(c) ?? Azin spilled the bean.
(d) Azin spilled the Enron beans.
(e) The beans were spilled by Azin.
(f) The beans that Azin spilled caused Sam a lot of trouble.
(g) Which beans did Azin spill?
Linguists have often explained the lexical and syntactic flexibility of idiomatic
combinations in terms of their semantic analyzability (Fellbaum 1993; Gibbs 1993;
Glucksberg 1993; Nunberg, Sag, and Wasow 1994; Schenk 1995). The common belief
is that because the constituents of a semantically analyzable idiom can be mapped onto
their corresponding referents in the idiomatic interpretation, analyzable (less idiomatic)
expressions are often more open to lexical substitution and syntactic variation. Psy-
cholinguistic studies also support this hypothesis: Gibbs and Nayak (1989) and Gibbs
et al (1989), through a series of psychological experiments, demonstrate that there is
variation in the degree of lexicosyntactic flexibility of idiomatic combinations. (Both
studies narrow their focus to verb phrase idiomatic combinations, mainly of the form
verb+noun.) Moreover, their findings provide evidence that the lexical and syntactic
flexibility of VNICs is not arbitrary, but rather correlates with the semantic analyzability
of these idioms as perceived by the speakers participating in the experiments.
Corpus-based studies such as those by Moon (1998), Riehemann (2001), and Grant
(2005) conclude that idioms are not as fixed as most have assumed. These claims are
often based on observing certain idiomatic combinations in a form other than their so-
called canonical forms. For example, Moon mentions that she has observed both kick
the pail and kick the can as variations of kick the bucket. Also, Grant finds evidence of
variations such as eat one?s heart (out) and eat one?s hearts (out) in the BNC. Riehemann
concludes that in contrast to non-idiomatic combinations of words, ?idioms have a
strongly preferred canonical form, but at the same time the occurrence of lexical and
syntactic variations of idioms is too common to be ignored? (page 67). Our understand-
ing of such findings is that idiomatic combinations are not inherently frozen and that it
is possible for them to appear in forms other than their agreed-upon canonical forms.
However, it is important to note that most such observed variations are constrained,
often with unpredictable restrictions.
We are well aware that semantic analyzability is neither a necessary nor a sufficient
condition for an idiomatic combination to be lexically or syntactically flexible. Other
factors, such as communicative intentions and pragmatic constraints, can motivate a
speaker to use a variant in place of a canonical form (Glucksberg 1993). For exam-
ple, journalism is well known for manipulating idiomatic expressions for humor or
cleverness (Grant 2005). The age and the degree of familiarity of an idiom have also
been shown to be important factors that affect its flexibility (Gibbs and Nayak 1989).
Nonetheless, linguists often use observations about lexical and syntactic flexibility of
VNICs in order to make judgments about their degree of idiomaticity (Kyto? 1999;
Tanabe 1999). We thus conclude that lexicosyntactic behavior of a VNIC, although
affected by historical and pragmatic factors, can be at least partially explained in terms
of semantic analyzability or idiomaticity.
65
Computational Linguistics Volume 35, Number 1
3. Automatic Acquisition of Type-Based Knowledge about VNICs
We use the observed connection between idiomaticity and (in)flexibility to devise sta-
tistical measures for automatically distinguishing idiomatic verb+noun combinations
(types) from literal phrases. More specifically, we aim to identify verb?noun pairs such
as ?keep, word? as having an associated idiomatic expression (keep one?s word), and
also distinguish these from verb?noun pairs such as ?keep, fish? which do not have
an idiomatic interpretation. Although VNICs vary in their degree of flexibility (cf.
Examples (1) and (2)), on the whole they contrast with fully compositional phrases,
which are more lexically productive and appear in a wider range of syntactic forms. We
thus propose to use the degree of lexical and syntactic flexibility of a given verb+noun
combination to determine the level of idiomaticity of the expression.
Note that our assumption here is in line with corpus-linguistic studies on idioms:
we do not claim that it is inherently impossible for VNICs to undergo lexical sub-
stitution or syntactic variation. In fact, for each given idiomatic combination, it may
well be possible to find a specific situation in which a lexical or a syntactic variant of
the canonical form is perfectly plausible. However, the main point of the assumption
here is that VNICs are more likely to appear in fixed forms (known as their canonical
forms), more so than non-idiomatic phrases. Therefore, the overall distribution of a
VNIC in different lexical and syntactic forms is expected to be notably different from
the corresponding distribution of a typical verb+noun combination.
The following subsections describe our proposed statistical measures for idiomatic-
ity, which quantify the degree of lexical, syntactic, and overall fixedness of a given
verb+noun combination (represented as a verb?noun pair).
3.1 Measuring Lexical Fixedness
A VNIC is lexically fixed if the replacement of any of its constituents by a semantically
(and syntactically) similar word does not generally result in another VNIC, but in
an invalid or a literal expression. One way of measuring lexical fixedness of a given
verb+noun combination is thus to examine the idiomaticity of its variants, that is,
expressions generated by replacing one of the constituents by a similar word. This
approach has twomain challenges: (i) it requires prior knowledge about the idiomaticity
of expressions (which is what we are developing our measure to determine); (ii) it can
only measure the lexical fixedness of idiomatic combinations, and so could not apply to
literal combinations. We thus interpret this property statistically in the following way:
We expect a lexically fixed verb+noun combination to appear much more frequently
than its variants in general.
Specifically, we examine the strength of association between the verb and the
noun constituent of a combination (the target expression or its lexical variants) as
an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the
automatically built thesaurus of Lin (1998) to find words similar to each constituent,
in order to automatically generate variants.2 Variants are generated by replacing either
2 We also replicated our experiments with an automatically built thesaurus created from the British
National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results
were similar, hence we do not report them here.
66
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
the noun or the verb constituent of a pair with a semantically (and syntactically) similar
word.3
Examples of automatically generated variants for the pair ?spill, bean? are ?pour,
bean?, ?stream, bean?, ?spill, corn?, and ?spill, rice?.
Let Ssim(v) = {vi | 1 ? i ? Kv} be the set of the Kv most similar verbs to the verb v
of the target pair ?v,n?, and Ssim(n) =
{
nj | 1 ? j ? Kn
}
be the set of the Kn most similar
nouns to the noun n (according to Lin?s thesaurus). The set of variants for the target pair
is thus:
Ssim(v,n) = {?vi,n?| 1 ? i ? Kv} ?
{
?v,nj?| 1 ? j ? Kn
}
.
We calculate the association strength for the target pair and for each of its variants
using an information-theoretic measure called pointwise mutual information or PMI
(Church et al 1991):
PMI(vr, nt) = log
P(vr, nt)
P(vr)P(nt)
= log
Nv+n f (vr, nt)
f (vr, ?) f (?, nt)
(1)
where ?vr, nt? ? {?v, n?} ? Ssim(v,n); Nv+n is the total number of verb?object pairs in the
corpus; f (vr, nt) is the frequency of vr and nt co-occurring as a verb?object pair; f (vr, ?)
is the total frequency of the target (transitive) verb with any noun as its direct object;
and f (?, nt) is the total frequency of the noun nt in the direct object position of any verb
in the corpus.
In his work, Lin (1999) assumes that a target expression is non-compositional if and
only if its PMI value is significantly different from that of all the variants. Instead, we
propose a novel technique that brings together the association strengths (PMI values)
of the target and the variant expressions into a single measure reflecting the degree of
lexical fixedness for the target pair. We assume that the target pair is lexically fixed to
the extent that its PMI deviates from the average PMI of its variants. By ourmeasure, the
target pair is considered lexically fixed (i.e., is given a high fixedness score) only if the
difference between its PMI value and that of most of its variants?not necessarily all, as
in themethod of Lin (1999)?is high.4 Ourmeasure calculates this deviation, normalized
using the sample?s standard deviation:
Fixednesslex(v, n)
.
=
PMI(v, n)? PMI
s (2)
3 In an early version of this work (Fazly and Stevenson 2006), only the noun constituent was varied
because we expected replacing the verb constituent with a related verb to be more likely to yield another
VNIC, as in keep/lose one?s cool, give/get the bird, crack/break the ice (Nunberg, Sag, and Wasow 1994; Grant
2005). Later experiments on the development data showed that variants generated by replacing both
constituents, one at a time, produce better results.
4 This way, even if an idiom has a few frequently used variants (e.g., break the ice and crack the ice), it may
still be assigned a high fixedness score if most other variants are uncommon. Note also that it is possible
that some variants of a given idiom are frequently used literal expressions (e.g., make biscuit for take
biscuit). It is thus important to use a flexible formulation that relies on the collective evidence (e.g.,
average PMI) and hence is less sensitive to individual cases.
67
Computational Linguistics Volume 35, Number 1
where PMI is the mean and s the standard deviation of the following sample:
{
PMI(vr, nt) | ?vr, nt? ? {?v, n?} ? Ssim(v,n)
}
PMI can be negative, zero, or positive; thus Fixednesslex(v, n) ? [??,+?], where high
positive values indicate higher degrees of lexical fixedness.
3.2 Measuring Syntactic Fixedness
Compared to literal (non-idiomatic) verb+noun combinations, VNICs are expected to
appear in more restricted syntactic forms. To quantify the syntactic fixedness of a target
verb?noun pair, we thus need to: (i) identify relevant syntactic patterns, namely, those
that help distinguish VNICs from literal verb+noun combinations; and (ii) translate the
frequency distribution of the target pair in the identified patterns into a measure of
syntactic fixedness.
3.2.1 Identifying Relevant Patterns. Determining a unique set of syntactic patterns appro-
priate for the recognition of all idiomatic combinations is difficult indeed: Exactly which
forms an idiomatic combination can occur in is not entirely predictable (Sag et al 2002).
Nonetheless, there are hypotheses about the difference in behavior of VNICs and literal
verb+noun combinations with respect to particular syntactic variations (Nunberg, Sag,
and Wasow 1994). Linguists note that semantic analyzability of VNICs is related to
the referential status of the noun constituent (i.e., the process of idiomatization of a
verb+noun combination is believed to be accompanied by a change from concreteness
to abstractness for the noun). The referential status of the noun is in turn assumed to
be related to the participation of the combination in certain morpho-syntactic forms.
In what follows, we describe three types of syntactic variation that are assumed to be
mostly tolerated by literal combinations, but less tolerated by many VNICs.
Passivization. There is much evidence in the linguistics literature that VNICs often do
not undergo passivization. Linguists mainly attribute this to the fact that in most cases,
only referential nouns appear as the surface subject of a passive construction (Gibbs
and Nayak 1989). Due to the non-referential status of the noun constituent in most
VNICs, we expect that they do not undergo passivization as often as literal verb+noun
combinations do. Another explanation for this assumption is that passives are mainly
used to put focus on the object of a clause or sentence. For most VNICs, no such
communicative purpose can be served by topicalizing the noun constituent through
passivization (Jackendoff 1997). The passive construction is thus considered as one of
the syntactic patterns relevant to measuring syntactic flexibility.5
Determiner type. A strong correlation has been observed between the flexibility of the
determiner preceding the noun in a verb+noun combination and the overall flexibility
of the phrase (Fellbaum 1993; Kearns 2002; Desbiens and Simon 2003). It is however
5 Note that there are idioms that appear primarily in a passivized form, for example, the die is cast (?the
decision is made and will not change?). Our measure can in principle recognize such idioms because we
do not require that an idiom appears mainly in active form; rather, we include voice (passive or active) as
an important part of the syntactic pattern of an idiomatic combination.
68
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
important to note that the nature of the determiner is also affected by other factors,
such as the semantic properties of the noun. For this reason, determiner flexibility is
sometimes argued not to be a good predictor of the overall syntactic flexibility of an ex-
pression. Nonetheless, many researchers consider it as an important part in the process
of idiomatization of a verb+noun combination (Akimoto 1999; Kyto? 1999; Tanabe 1999).
We thus expect a VNIC to mainly appear with one type of determiner.
Pluralization. Although the verb constituent of a VNIC is morphologically flexible, the
morphological flexibility of the noun relates to its referential status (Grant 2005). Again,
one should note that the use of a singular or plural noun in a VNICmay also be affected
by the semantic properties of the noun. Recall that during the idiomatization process,
the noun constituent may become more abstract in meaning. In this process, the noun
may lose some of its nominal features, including number (Akimoto 1999). The non-
referential noun constituent of a VNIC is thus expected to mainly appear in just one of
the singular or plural forms.
Merging the three types of variation results in a pattern set, P , of 11 distinct syntac-
tic patterns that are displayed in Table 1 along with examples for each pattern. When
developing this set of patterns, we have taken into account the linguistic theories about
the syntactic constraints on idiomatic expressions; for example, our choice of patterns
is consistent with the idiom typology developed by Nicolas (1995). Note that we merge
some of the individual patterns into one; for example, we include only one passive
pattern independently of the choice of the determiner or the number of the noun. The
motivation here is to merge low frequency patterns (i.e., those that are expected to
be less common) in order to acquire more reliable evidence on the distribution of a
particular verb?noun pair over the resulting pattern set. In principle, however, the set
can be expanded to include more patterns; it can also be modified to contain different
patterns for different classes of idiomatic combinations.
3.2.2 Devising a Statistical Measure. The second step is to devise a statistical measure
that quantifies the degree of syntactic fixedness of a verb?noun pair, with respect to
Table 1
Patterns used in the syntactic fixedness measure, along with examples for each. A pattern
signature is composed of a verb v in active (vact) or passive (vpass) voice; a determiner (det) that
can be NULL, indefinite (a/an), definite (the), demonstrative (DEM), or possessive (POSS); and a
noun n that can be singular (nsg) or plural (npl).
Pattern No. Pattern Signature Example
1 vact det:NULL nsg give money
2 vact det:a/an nsg give a book
3 vact det:the nsg give the book
4 vact det:DEM nsg give this book
5 vact det:POSS nsg give my book
6 vact det:NULL npl give books
7 vact det:the npl give the books
8 vact det:DEM npl give those books
9 vact det:POSS npl give my books
10 vact det:OTHER nsg,pl give many books
11 vpass det:ANY nsg,pl a/the/this/my book/books was/were given
69
Computational Linguistics Volume 35, Number 1
the selected set of patterns, P . We propose a measure that compares the syntactic
behavior of the target pair with that of a ?typical? verb?noun pair. Syntactic behav-
ior of a typical pair is defined as the prior probability distribution over the patterns in
P . The maximum likelihood estimate for the prior probability of an individual pattern
pt ? P is calculated as
P(pt) =
?
vi?V
?
nj?N
f (vi, nj, pt)
?
vi?V
?
nj?N
?
ptk?P
f (vi, nj, ptk)
=
f (?, ?, pt)
f (?, ?, ?)
(3)
where V is the set of all instances of transitive verbs in the corpus, andN is the set of all
instances of nouns appearing as the direct object of some verb.
The syntactic behavior of the target verb?noun pair ?v,n? is defined as the posterior
probability distribution over the patterns, given the particular pair. The maximum like-
lihood estimate for the posterior probability of an individual pattern pt is calculated as
P(pt | v, n) =
f (v, n, pt)
?
ptk?P
f (v, n, ptk)
=
f (v, n, pt)
f (v, n, ?)
. (4)
The degree of syntactic fixedness of the target verb?noun pair is estimated as
the divergence of its syntactic behavior (the posterior distribution over the patterns)
from the typical syntactic behavior (the prior distribution). The divergence of the two
probability distributions is calculated using a standard information-theoretic measure,
the Kullback Leibler (KL-) divergence (Cover and Thomas 1991):
Fixednesssyn (v, n)
.
= D(P(pt | v,n) ||P(pt))
=
?
ptk?P
P(ptk | v, n) log
P(ptk | v, n)
P(ptk)
(5)
KL-divergence has proven useful in many NLP applications (Resnik 1999; Dagan,
Pereira, and Lee 1994). KL-divergence is always non-negative and is zero if and only
if the two distributions are exactly the same. Thus, Fixednesssyn(v, n) ? [0,+?], where
large values indicate higher degrees of syntactic fixedness.
3.3 A Unified Measure of Fixedness
VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed
than literal verb+noun combinations (see Section 2). We thus propose a new measure
70
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity to be a measure of the overall fixedness of a given pair. We define
Fixednessoverall (v, n) as a weighted combination of Fixednesslex and Fixednesssyn:
Fixednessoverall (v, n)
.
= ? Fixednesssyn (v, n) + (1? ?) Fixednesslex (v, n) (6)
where ?weights the relative contribution of the measures in predicting idiomaticity.
Recall that Fixednesslex(v, n) ? [??,+?], and Fixednesssyn(v, n) ? [0,+?]. To
combine them in the overall fixedness measure, we rescale them, so that they fall in
the range [0, 1]. Thus, Fixednessoverall(v, n) ? [0, 1], where values closer to 1 indicate a
higher degree of overall fixedness.
4. VNIC Type Recognition: Evaluation
To evaluate our proposed fixedness measures, we analyze their appropriateness for
determining the degree of idiomaticity of a set of experimental expressions (in the form
of verb?noun pairs, extracted as described in Section 4.1). More specifically, we first use
each measure to assign scores to the experimental pairs. We then use the scores assigned
by each measure to perform two different tasks, and assess the overall goodness of the
measure by looking at its performance in both.
First, we look into the classification performance of each measure by using the
scores to separate idiomatic verb?noun pairs from literal ones in a mixed list. This is
done by setting a threshold, here the median score, where all pairs with scores higher
than the threshold are labeled as idiomatic and the rest as literal.6 For classification, we
report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random
(chance) baseline, referred to as Rand. Second, we examine the retrieval performance
of our fixedness measures by using the scores to rank verb?noun pairs according to
their degree of idiomaticity. For retrieval, we present the precision?recall curves, as
well as the interpolated three-point average precision or IAP?that is, the average of
the interpolated precisions at the recall levels of 20%, 50%, and 80%. The interpolated
average precision and precision?recall curves are commonly used for the evaluation of
information retrieval systems (Manning and Schu?tze 1999), and reflect the goodness of
a measure in placing the relevant items (here, idioms) before the irrelevant ones (here,
literals).
Idioms are often assumed to exhibit collocational behavior to some extent, that is,
the components of an idiom are expected to appear together more often than expected
by chance. Hence, someNLP systems have used collocational measures to identify them
(Smadja 1993; Evert and Krenn 2001). However, as discussed in Section 2, idioms have
distinctive syntactic and semantic properties that separate them from simple colloca-
tions. For example, although collocations involve some degree of semantic idiosyncrasy
(strong tea vs. ?powerful tea), compared to idioms, they typically have a more transparent
meaning, and their syntactic behavior is more similar to that of literal expressions. We
thus expect our fixedness measures that draw on the distinctive linguistic properties
of idioms to be more appropriate than measures of collocation for the identification of
idioms. To verify this hypothesis, in both the classification and retrieval tasks, we com-
pare the performance of the fixedness measures with that of two collocation extraction
measures: an informed baseline, PMI, and a position-based fixedness measure proposed
6 We adopt the median for this particular (balanced) data set, understanding that in practice a suitable
threshold would need to be determined, e.g., based on development data.
71
Computational Linguistics Volume 35, Number 1
by Smadja (1993), which we refer to as Smadja. Next, we provide more details on PMI
and Smadja.
PMI is a widely used measure for extracting statistically significant combinations
of words or collocations. It has also been used for the recognition of idioms (Evert and
Krenn 2001), warranting its use as an informed baseline here for comparison.7 As in
Equation (1), our calculation of PMI here restricts the counts of the verb?noun pair to
the direct object relation. Smadja (1993) proposes a collocation extraction method which
measures the fixedness of a word sequence (e.g., a verb?noun pair) by examining the
relative position of the component words across their occurrences together. We replicate
Smadja?s method, where we measure fixedness of a target verb?noun pair as the spread
(variance) of the co-occurrence frequency of the verb and the noun over 10 relative
positions within a five-word window.8
Recall from Section 3.1 that our Fixednesslex measure is intended as an improve-
ment over the non-compositionalitymeasure of Lin (1999). For the sake of completeness,
we also compare the classification performance of our Fixednesslex with that of Lin?s
(1999) measure, which we refer to as Lin.9
We first elaborate on the methodological aspects of our experiments in Section 4.1,
and then present a discussion of the experimental results in Section 4.2.
4.1 Experimental Setup
4.1.1 Corpus and Data Extraction. We use the British National Corpus (BNC; Burnard
2000); to extract verb?noun pairs, along with information on the syntactic patterns they
appear in. We automatically parse the BNC using the Collins parser (Collins 1999), and
augment it with information about verb and noun lemmas, automatically generated
using WordNet (Fellbaum 1998). We further process the corpus using TGrep2 (Rohde
2004) in order to extract syntactic dependencies. For each instance of a transitive verb,
we use heuristics to extract the noun phrase (NP) in either the direct object position
(if the sentence is active), or the subject position (if the sentence is passive). We then
automatically find the head noun of the extracted NP, its number (singular or plural),
and the determiner introducing it.
4.1.2 Experimental Expressions. We select our development and test expressions from
verb?noun pairs that involve a member of a predefined list of transitive verbs, referred
to as basic verbs. Basic verbs, in their literal use, refer to states or acts that are central
to human experience. They are thus frequent, highly polysemous, and tend to combine
with other words to form idiomatic combinations (Cacciari 1993; Claridge 2000; Gentner
and France 2004). An initial list of such verbs was selected from several linguistic and
psycholinguistic studies on basic vocabulary (Ogden 1968; Clark 1978; Nunberg, Sag,
andWasow 1994; Goldberg 1995; Pauwels 2000; Claridge 2000; Newman and Rice 2004).
We further augmented this initial list with verbs that are semantically related to another
7 PMI has been shown to perform better than or comparable to many other association measures (Inkpen
2003; Mohammad and Hirst, submitted). In our experiments, we also found that PMI consistently
performs better than two other association measures, the Dice coefficient and the log-likelihood measure.
Experiments by Krenn and Evert (2001) showed contradicting results for PMI; however, these
experiments were performed on small-sized corpora, and on data which contained items with very low
frequency.
8 We implement the method as explained in Smadja (1993), taking into account the part-of-speech tags of
the target component words.
9 We implement the method as explained in Lin (1999), using 95% confidence intervals. We thus need to
ignore variants with frequency lower than 4 for which no confidence interval can be formed.
72
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
verb already in the list; for example, lose is added in analogy with find. Here is the final
list of the 28 verbs in alphabetical order:
blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move,
place, pull, push, put, see, set, shoot, smell, take, throw, touch
From the corpus, we extract all the verb?noun pairs (lemmas) that contain any
of these listed basic verbs, and that appear at least 10 times in the corpus in a direct
object relation (irrespective of any intervening determiners or adjectives). From these,
we select a subset that are idiomatic, and another subset that are literal, as follows: A
verb?noun pair is considered idiomatic if it appears in an idiom listed in a credible
dictionary such as the Oxford Dictionary of Current Idiomatic English (ODCIE; Cowie,
Mackin, and McCaig 1983), or the Collins COBUILD Idioms Dictionary (CCID; Seaton
and Macaulay 2002).10 To decide whether a verb?noun pair has appeared in an idiom,
we look for all idioms containing the verb and the noun in a direct-object relation,
irrespective of any intervening determiners or adjectives, and/or any other arguments.
The pair is considered literal if it involves a physical act or state (i.e., the basic semantics
of the verb) and does not appear in any of the mentioned dictionaries as an idiom (or
part of an idiom). From the set of idiomatic pairs, we then randomly pull out 80 de-
velopment pairs and 100 test pairs, ensuring that we have items of both low and high
frequency. We then double the size of each data set (development and test) by adding
equal numbers of literal pairs, with similar frequency distributions. Some of the idioms
corresponding to the experimental idiomatic pairs are: kick the habit,move mountains, lose
face, and keep one?s word. Examples of literal pairs include: move carriage, lose ticket, and
keep fish.
Development expressions are used in devising the fixedness measures, as well as
in determining the values of their parameters as explained in the next subsection. Test
expressions are saved as unseen data for the final evaluation.
4.1.3 Parameter Settings. Our lexical fixedness measure in Equation (2) involves two
parameters, Kv and Kn, which determine the number of lexical variants considered in
measuring the lexical fixedness of a given verb?noun pair. We make the least-biased
assumption on the proportion of variants generated by replacing the verb (Kv) and
those generated by replacing the noun (Kn)?that is, we assume Kv = Kn.
11 We perform
experiments on the development data, where we set the total number of variants (i.e.,
Kv + Kn) from 10 to 100 by steps of 10. (For simplicity, we refer to the total number
of variants as K). Figure 1(a) shows the change in performance of Fixednesslex as a
function of K. Recall that Acc is the classification accuracy, and IAP reflects the average
precision of a measure in ranking idiomatic pairs before non-idiomatic ones. According
to these results, there is not much variation in the performance of the measure for
10 Our development data also contains items from several other dictionaries, such as Chambers Idioms
(Kirkpatrick and Schwarz 1982). However, our test data, which is also used in the token-based
experiments, however, only contains idioms from the two dictionaries ODCIE and CCID. Results
reported in this article are all on test pairs; development pairs are mainly used for the development of the
methods.
11 We also performed experiments on the development data in which we did not restrict the number of
variants, and hence did not enforce the condition Kv = Kn. Instead, we tried using a variety of thresholds
on the similarity scores (from the thesaurus) in order to find the set of most similar words to a given verb
or noun. We found that fixing the number of most similar words is more effective than using a similarity
threshold, perhaps because the actual scores can be very different for different words.
73
Computational Linguistics Volume 35, Number 1
Figure 1
%IAP and %Acc of Fixednesslex and Fixednessoverall over development data.
K ? 20. We thus choose an intermediate value for K that yields the highest accuracy
and a reasonably high precision; specifically, we set K to 50.
The overall fixedness measure defined in Equation (6) also uses a parameter, ?,
which determines the relative weights given to the individual fixedness measures in
the linear combination. We experiment on the development data with different values
of ? ranging from 0 to 1 by steps of .02; results are shown in Figure 1(b). As can be seen
in the figure, the accuracy of Fixednessoverall is not affected much by the change in the
value of ?. The average precision (IAP), however, shows that the combined measure
performs best when somewhat equal weights are given to the two individual measures,
and performs worst when the lexical fixedness component is completely ignored (i.e.,
? is close to 1). These results also reinforce that a complete evaluation of our fixedness
measures should include both metrics, accuracy, and average precision, as they reveal
different aspects of performance. Here, for example, Fixednesssyn (? = 1) has compa-
rable accuracy to Fixednesslex (? = 0), reflecting that the two measures generally give
higher scores to idioms. However, the ranking precision of the latter is much higher
than that of the former, showing that Fixednesslex ranks many of the idioms at the very
top of the list. In all our experiments reported here, we set ? to .6, a value for which
Fixednessoverall shows reasonably good performance according to both Acc and IAP.
4.2 Experimental Results and Analysis
In this section, we report the results of evaluating our measures on unseen test expres-
sions, with parameters set to the values determined in Section 4.1.3. (Results on devel-
opment data have similar trends to those on test data.) We analyze the classification
performance of the individual lexical and syntactic fixedness measures in Section 4.2.1,
and discuss their effectiveness for retrieval in Section 4.2.2. Section 4.2.3 then looks into
the performance of the overall fixedness measure, and Section 4.2.4 presents a summary
and discussion of the results.
4.2.1 Classification Performance. Here, we look into the performance of the individual
fixedness measures, Fixednesslex and Fixednesssyn, in classifying a mixed set of verb?
noun pairs into idiomatic and literal classes. We compare their performance against the
74
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 2
Accuracy and relative error reduction for the two fixedness measures, the two baseline
measures, and Smadja, over all test pairs (TESTall), and test pairs divided by frequency
(TESTflow and TESTfhigh ).
TESTall TESTflow
TESTfhigh
Measure %Acc (%ERR) %Acc (%ERR) %Acc (%ERR)
Rand 50 50 50
PMI 63 (26) 56 (12) 70 (40)
Smadja 54 (8) 64 (28) 62 (24)
Fixednesslex 68 (36) 70 (40) 70 (40)
Fixednesssyn 71 (42) 72 (44) 82 (64)
two baselines, Rand and PMI, as well as the two state-of-the-art methods, Smadja and
Lin. For analytical purposes, we further divide the set of all test expressions, TESTall,
into two sets corresponding to two frequency bands: TESTflow contains 50 idiomatic
and 50 literal pairs, each with total frequency (across all syntactic patterns under
consideration) between 10 and 40; TESTfhigh consists of 50 idiomatic and 50 literal pairs,
each with total frequency of 40 or greater. Classification performances of all measures
except Lin are given in Table 2. Lin does not assign scores to the test verb?noun pairs,
hence we cannot calculate its classification accuracy the same way we do for the other
methods (i.e., using median as the threshold). A separate comparison between Lin and
Fixednesslex is provided at the end of this section.
As can be seen in the first two columns of Table 2, the informed baseline, PMI, shows
a large improvement over the random baseline (26% error reduction) on TESTall. This
shows that many VNICs have turned into institutionalized (i.e., statistically significant)
co-occurrences. Hence, one can get relatively good performance by treating verb+noun
idiomatic combinations as collocations. Fixednesslex performs considerably better than
the informed baseline (36% vs. 26% error reduction on TESTall). Fixednesssyn has the best
performance (shown in boldface), with 42% error reduction over the random baseline,
and 21.6% error reduction over PMI. These results demonstrate that lexical and syntactic
fixedness are good indicators of idiomaticity, better than a simple measure of colloca-
tion such as PMI. On TESTall, Smadja performs only slightly better than the random
baseline (8% error reduction), reflecting that a position-based fixedness measure is not
sufficient for identifying idiomatic combinations. These results suggest that looking into
deep linguistic properties of VNICs is necessary for the appropriate treatment of these
expressions.12
PMI is known to perform poorly on low frequency items. To examine the effect of
frequency on the measures, we analyze their performance on the two divisions of the
12 Performing the ?2 test of statistical significance, we find that the differences between Smadja and our
lexical and syntactic fixedness measures are statistically significant at p < 0.05. However, the differences
in performance between fixedness measures and PMI are not statistically significant. Note that this does
not imply that the differences are not substantial, rather that there is not enough evidence in the observed
data to reject the null hypothesis (that two methods perform the same in general) with high confidence.
Moreover, ?2 is a non-parametric (distribution free) test and hence it has less power to reject a null
hypothesis. Later, when we take into account the actual scores assigned by the measures, we find that all
differences are statistically significant (see Sections 4.2.2?4.2.3 for more details). All significance tests are
performed using the R (2004) package.
75
Computational Linguistics Volume 35, Number 1
test data, corresponding to the two frequency bands, TESTflow and TESTfhigh . Results are
given in the four rightmost columns of Table 2, with the best performance shown in
boldface. As expected, the performance of PMI drops substantially for low frequency
items. Interestingly, although it is a PMI-based measure, Fixednesslex has comparable
performance on all data sets. The performance of Fixednesssyn improves quite a bit
when it is applied to high frequency items, while maintaining similar performance on
the low frequency items. These results show that the lexical and syntactic fixedness
measures perform reasonably well on both low and high frequency items.13 Hence they
can be used with a higher degree of confidence, especially when applied to data that is
heterogeneous with regard to frequency. This is important because, while some VNICs
are very common, others have very low frequency, as noted by Grant (2005). Smadja
shows a notable improvement in performance when data is divided by frequency. This
effect is likely due to the fact that fixedness is measured as the spread of the position-
based (raw) co-occurrence frequencies. Nonetheless, on both data sets the performance
of Smadja remains substantially worse than that of our two fixedness measures (the
differences are statistically significant in three out of the four comparisons at p < .05).
Collectively, these results show that our linguisticallymotivated fixednessmeasures
are particularly suited for identifying idiomatic combinations, especially in comparison
with more general collocation extraction techniques, such as PMI or the position-based
fixedness measure of Smadja (1993). Especially, our measures tend to perform well on
low frequency items, perhaps due to their reliance on distinctive linguistic properties of
idioms.
We now compare the classification performance of Fixednesslex to that of Lin. Unlike
Fixednesslex, Lin does not assign continuous scores to the verb?noun pairs, but rather
classifies them as idiomatic or non-idiomatic. Thus, we cannot use the same threshold
(e.g., median) for the two methods to calculate their classification accuracies in a com-
parable way. Recall also from Section 3.1 that the performance of both these methods
depends on the value of K (the number of variants). We thus measure the classification
precision of the methods at equivalent levels of recall, using the same number of
variants K at each recall level for the two measures. Varying K from 2 to 100 by steps of
4, Lin and Fixednesslex achieve an average classification precision of 81.5% and 85.8%,
respectively. Performing a t-test on the precisions of the two methods confirms that
the difference between the two is statistically significant at p < .001. In addition, our
method has the advantage of assigning a score to a target verb?noun reflecting its degree
of lexical fixedness. Such information can help a lexicographer decide whether a given
verb?noun should be placed in a lexicon.
4.2.2 Retrieval Performance. The classification results suggest that the individual fixed-
ness measures are overall better than a simple measure of collocation at separating
idiomatic pairs from literal ones. Here, we have a closer look at their performance
by examining their goodness in ranking verb?noun pairs according to their degree
of idiomaticity. Recall that the fixedness measures are devised to reflect the degree of
fixedness and hence the degree of idiomaticity of a target verb?noun pair. Thus, the
result of applying eachmeasure to a list of mixed pairs is a list that is ranked in the order
13 In fact, the results show that the performance of both fixedness measures is better when data is divided
by frequency. Although we expect better performance over high frequency items, more investigation is
needed to verify whether the improvement in performance over low frequency items is a meaningful
effect or merely an accident of the data at hand.
76
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity. For a measure to be considered good at retrieval, we expect idiomatic
pairs to be very frequent near the top of the ranked list, and to become less frequent
towards the bottom. Precision?recall curves are very indicative of this trend: The ideal
measure will have a precision of 100% for all values of recall, namely, themeasure places
all idiomatic pairs at the very top of the ranked list. In reality, although the precision
drops as recall increases, we expect a good measure to keep high precision at most
levels of recall.
Figure 2 depicts the interpolated precision?recall curves for PMI and Smadja, and
for the lexical, syntactic, and overall fixedness measures, over TESTall. Note that the
minimum interpolated precision is 50% due to the equal number of idiomatic and literal
pairs in the test data. In this section, we discuss the retrieval performance of the two
individual fixedness measures; the next section analyzes the performance of the overall
fixedness measure.
The precision?recall curves of Smadja and PMI are nearly flat (with PMI consis-
tently higher than Smadja), showing that the distribution of idiomatic pairs in the lists
ranked by these two measures is only slightly better than random. A close look at the
precision?recall curve of Fixednesslex reveals that, up to the recall level of 50%, the
precision of this measure is substantially higher than that of PMI. This means that,
compared to PMI, Fixednesslex places more idiomatic pairs at the very top of the list.
At higher recall levels (50% and higher), Fixednesslex still consistently outperforms PMI.
Nonetheless, at these recall values, the twomeasures have relatively low precision (com-
pared to the other measures), suggesting that both measures also put many idiomatic
pairs near the bottom of the list. In contrast, the precision?recall curve of Fixednesssyn
shows that its performance is consistently much better than that of PMI: Even at the
recall level of 90%, its precision is close to 70% (cf. 55% precision of PMI).
A comparison of the precision?recall curves of the two individual fixedness mea-
sures reveals their complementary nature. Compared to Fixednesslex, Fixednesssyn
maintains higher precision at very high levels of recall, suggesting that the syntactic
fixedness measure places fewer idiomatic pairs at the bottom of the ranked list. In con-
trast, Fixednesslex has notably higher precision than Fixednesssyn at recall levels of up to
40%, suggesting that the former puts more idiomatic pairs at the top of the ranked list.
Statistical significance tests confirm these observations: Using the Wilcoxon Signed
Rank test (1945), we find that both Fixednesslex and Fixednesssyn produce significantly
different rankings from PMI and Smadja (p  .001). Also, the rankings of the items
Figure 2
Precision?recall curves for PMI, Smadja, and for the fixedness measures, over TESTall.
77
Computational Linguistics Volume 35, Number 1
Table 3
Classification and retrieval performance of the overall fixedness measure over TESTall.
Measure %Acc (%ERR) %IAP
PMI 63 (26) 63.5
Smadja 54 (8) 57.2
Fixednesslex 68 (36) 75.3
Fixednesssyn 71 (42) 75.9
Fixednessoverall 74 (48) 84.7
produced by the two individual fixedness measures are found to be significantly differ-
ent at p < .01.
4.2.3 Performance of the Overall Fixedness Measure. We now look at the classification
and retrieval performance of the overall fixedness measure. Table 3 presents %Acc,
%ERR, and %IAP of Fixednessoverall, repeating that of PMI, Smadja, Fixednesslex, and
Fixednesssyn, for comparison. Here again the error reductions are relative to the random
baseline of 50%. Looking at classification performance (expressed in terms of %Acc
and %ERR), we can see that Fixednessoverall notably outperforms all other measures,
including lexical and syntactic fixedness (18.8% error reduction relative to Fixednesslex,
and 10% error reduction relative to Fixednesssyn). According to the classification
results, each of the lexical and syntactic fixedness measures are good at separating
idiomatic from literal combinations, with syntactic fixedness performing better. Here
we demonstrate that combining them into a single measure of fixedness, while giving
more weight to the better measure, results in a more effective classifier.14 The overall
behavior of this measure as a function of ? is displayed in Figure 3.
As can be seen in Table 3, Fixednesslex and Fixednesssyn have comparable IAP:
75.3% and 75.9%, respectively. In comparison, Fixednessoverall has a much higher IAP
of 84.7%, reinforcing the claim that combining evidence from both lexical and syntac-
tic fixedness is beneficial. Recall from Section 4.2.2 that the two individual fixedness
measures exhibit complementary behavior, as observed in their precision?recall curves
shown in Figure 2. The precision?recall curve of the overall fixedness measure shows
that this measure in fact combines advantages of the two individual measures: At most
recall levels, Fixednessoverall has a higher precision than both individual measures. Sta-
tistical significance tests that look at the actual scores assigned by the measures confirm
that the observed differences in performance are significant. The Wilcoxon Signed Rank
test shows that the Fixednessoverall measure produces a ranking that is significantly
different from those of the individual fixedness measures, the baseline PMI, and Smadja
(at p .001).
4.2.4 Summary and Discussion. Overall, the worst performance belongs to the two collo-
cation extraction methods, PMI and Smadja, both in classifying test pairs as idiomatic or
14 Using a ?2 test, we find a statistically significant difference between the classification performance of
Fixednessoverall and that of Smadja (p < 0.01), and also a marginally significant difference between the
performance of Fixednessoverall and that of PMI (p < .1). Recall from footnote 12 (page 15) that none
of the individual measures? performances significantly differed from that of PMI. Nonetheless, no
significant differences are found between the classification performance of Fixednessoverall and that
of the individual fixedness measures.
78
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Figure 3
Classification performance of Fixednessoverall on test data as a function of ?.
literal, and in ranking the pairs according to their degree of idiomaticity. This suggests
that although some VNICs are institutionalized, many do not appear with markedly
high frequency, and hence only looking at their frequency is not sufficient for their
recognition. Moreover, a position-based fixedness measure does not seem to sufficiently
capture the syntactic fixedness of VNICs in contrast to the flexibility of literal phrases.
Fixednessoverall is the best performer of all, supporting the hypothesis that many VNICs
are both lexically and syntactically fixed, more so than literal verb+noun combinations.
In addition, these results demonstrate that incorporating such linguistic properties into
statistical measures is beneficial for the recognition of VNICs.
Althoughwe focus on experimental expressionswith frequency higher than 10, PMI
still shows great sensitivity to frequency differences, performing especially poorly on
items with frequency between 10 and 40. In contrast, none of the fixedness measures
are as sensitive to such frequency differences. Especially interesting is the consistent
performance of Fixednesslex, which is a PMI-based measure, on low and high frequency
items. These observations put further emphasis on the importance of devising new
methods for extracting multiword expressions with particular syntactic and semantic
properties, such as VNICs.
To further analyze the performance of the fixedness measures, we look at the top
and bottom 20 pairs (10%) in the lists ranked by each fixedness measure. Interestingly,
the list ranked by Fixednessoverall contains no false positives ( fp) in the top 20 items,
and no false negatives ( fn) in the bottom 20 items, once again reinforcing the usefulness
of combining evidence from the individual lexical and syntactic fixedness measures.
False positive and false negative errors found in the top and bottom 20 ranked pairs,
respectively, for the syntactic and lexical fixedness measures are given in Table 4. (Note
that fp errors are the non-idiomatic pairs ranked at the top, whereas fn errors are the
idiomatic pairs ranked at the bottom.)
We first look at the errors made by Fixednesssyn. The first fp error, throw hat, is
an interesting one: even though the pair is not an idiomatic expression on its own,
it is part of the larger idiomatic phrase throw one?s hat in the ring, and hence exhibits
syntactic fixedness. This shows that our methods can be easily extended to identify
other types of verb phrase idiomatic combinations which exhibit syntactic behavior
similar to VNICs. Looking at the frequency distribution of the occurrence of the other
two fp errors, touch finger and lose home, in the 11 patterns from Table 1, we observe that
both pairs tend to appear mainly in the patterns ?vact det:POSS nsg? (touch one?s finger,
lose one?s home) and/or ?vact det:POSS npl? (touch one?s fingers). These examples show
79
Computational Linguistics Volume 35, Number 1
Table 4
Errors found in the top and bottom 20 pairs in the lists ranked by the two individual fixedness
measures; fp stands for false positive, fn stands for false negative.
Measure: Fixednesssyn Fixednesslex
Error Type: fp fn fp fn
throw hat make pile push barrow have moment
touch finger keep secret blow bridge give way
lose home keep hand
that syntactic fixedness is not a sufficient condition for idiomaticity. In other words,
it is possible for non-idiomatic expressions to be syntactically fixed for reasons other
than semantic idiosyncrasy. In these examples, the nouns finger and home tend to be
introduced by a possessive determiner, because they often belong to someone. It is also
important to note that these two patterns have a low prior (i.e., verb?noun pairs do
not typically appear in these patterns). Hence, an expression with a strong tendency to
appear in such patterns will be given a high syntactic fixedness score.
The frequency distribution of the two fn errors for Fixednesssyn reveals that they are
given low scores mainly because their distributions are similar to the prior. Even though
make pile preferably appears in the two patterns ?vact det:a/an nsg? and ?vact det:NULL
npl,? both patterns have reasonably high prior probabilities. Moreover, because of the
low frequency of make pile (< 40), the evidence is not sufficient to distinguish it from a
typical verb?noun pair. The pair keep secret has a high frequency, but its occurrences are
scattered across all 11 patterns, closely matching the prior distribution. The latter exam-
ple shows that syntactic fixedness is not a necessary condition for idiomaticity either.15
Analyzing the errors made by Fixednesslex is more difficult as many factors may
affect scores given by this measure. Most important is the quality of the automatically
generated variants. We find that in one case, push barrow, the first 25 distributionally
similar nouns (taken from the automatically built thesaurus) are proper nouns, perhaps
because Barrow is a common last name. In general, it seems that the similar verbs and
nouns for a target verb?noun pair are not necessarily related to the same sense of the
target word. Another possible source of error is that in this measure we use PMI as an
indirect clue to idiomaticity. In the case of give way and keep hand, many of the variants
are plausible combinations with very high frequency of occurrence, for example, give
opportunity, give order, find way for the former, and hold hand, put hand, keep eye for the
latter. Whereas some of these high-frequency variants are literal (e.g., hold hand) or
idiomatic (e.g., keep eye), many have metaphorical interpretations (e.g., give opportunity,
find way). In our ongoing work, we use lexical and syntactic fixedness measures, in com-
bination with other linguistically motivated features, to distinguish such metaphori-
cal combinations from both literal and idiomatic expressions (Fazly and Stevenson,
to appear).
One way to decrease the likelihood of making any of these errors is to combine
evidence from the lexical and syntactic fixedness of idioms. As can be seen in Table 4, the
two fixedness measures make different errors, and combining them results in a measure
15 One might argue that keep secret is more semantically analyzable and hence less idiomatic than an
expression such as shoot the breeze. Nonetheless, it is still semantically more idiosyncratic than a fully
literal combination such as keep a pen, and hence should not be ranked at the very bottom of the list.
80
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(the overall fixedness) that makes fewer errors. In the future, we intend to also look into
other properties of idioms, such as their semantic non-compositionality, as extra sources
of information.
5. Determining the Canonical Forms of VNICs
Our evaluation of the fixedness measures demonstrates their usefulness for the au-
tomatic recognition of VNICs. Recall from Section 2 that idioms appear in restricted
syntactic forms, often referred to as their canonical forms (Glucksberg 1993; Riehemann
2001; Grant 2005). For example, the idiom pull one?s weight mainly appears in this form
(when used idiomatically). The lexical representation of an idiomatic combination thus
must contain information about its canonical forms. Such information is necessary both
for automatically generating appropriate forms (e.g., in a natural language generation
system or a machine translation system), and for inclusion in dictionaries for learners
(e.g., in the context of computational lexicography).
Because VNICs are syntactically fixed, they are mostly expected to have a small
number of canonical forms. For example, shoot the breeze is listed in many idiom dictio-
naries as the canonical form for ?shoot, breeze?. Also, hold fire and hold one?s fire are listed
in CCID as canonical forms for ?hold, fire?. We expect a VNIC to occur in its canonical
form(s) with substantially higher frequency than in any other syntactic patterns. We
thus devise an unsupervised method that discovers the canonical form(s) of a given
idiomatic verb?noun pair by examining its frequency of occurrence in each syntactic
pattern under consideration. Specifically, the set of the canonical form(s) of the target
pair ?v, n? is defined as
C(v, n) = {ptk ? P | z(v, n, ptk) > Tz} (7)
Here, P is the set of patterns (see Table 1), and the condition z(v, n, ptk) > Tz determines
whether the frequency of the target pair ?v,n? in ptk is substantially higher than its
frequency in other patterns; z(v, n, ptk) is calculated using the statistic z-score as in
Equation (8), and Tz is a predefined threshold.
z(v, n, ptk) =
f (v, n, ptk)? f
s (8)
where f is the sample mean and s the sample standard deviation.
The statistic z(v, n, ptk) indicates how far and in which direction the frequency of
occurrence of the target pair ?v, n? in a particular pattern ptk deviates from the sample
mean, expressed in units of the sample standard deviation. To decide whether ptk is a
canonical pattern for the target pair, we check whether its z-score, z(v, n, ptk), is greater
than a threshold Tz. Here, we set Tz to 1, based on the distribution of z and through
examining the development data.
We evaluate our unsupervised canonical form identification method by verifying
its predicted forms against ODCIE and CCID. Specifically, for each of the 100 idiomatic
pairs in TESTall, we calculate the precision and recall of its predicted canonical forms
(those whose z-scores are above Tz), compared to the canonical forms listed in the two
dictionaries. The average precision across the 100 test pairs is 81.2%, and the average
recall is 88% (with 68 of the pairs having 100% precision and 100% recall). Moreover, we
81
Computational Linguistics Volume 35, Number 1
find that for the overwhelming majority of the pairs, 86%, the predicted canonical form
with the highest z-score appears in the dictionary entry of the pair.
According to the entries in ODCIE and CCID, 93 out of the 100 idiomatic pairs in
TESTall have one canonical form. Our canonical form extractionmethod on average finds
1.2 canonical forms for these 100 pairs (one canonical form for 79 of them, two for 18,
and three for 3 of these). Generally, our method tends to extract more canonical forms
than listed in the dictionaries. This is a desired property, because idiom dictionaries
often do not exhaustively list all canonical forms, but themost dominant ones. Examples
of such cases include: see the sights for which our method also finds see sights as a canon-
ical form, and catch one?s attention for which our method also finds catch the attention.
There are also cases where our method finds canonical forms for a given expression due
to noise resulting from the use of the expression in a non-idiomatic sense. For example,
for hold one?s horses, our method also finds hold the horse and hold the horses as canonical
forms. Similarly, for get the bird, our method also finds get a bird.
In a few cases (4 out of 100), our method finds fewer canonical forms than listed
in the dictionaries. These are catch the/one?s imagination, have a/one?s fling, make a/one?s
mark, and have a/the nerve. For the first two of these, the z-score of the missed pattern
is only slightly lower than our predefined threshold. In other cases (8 out of 100), none
of the canonical forms extracted by our method match those in a dictionary. Some of
these expressions also have a non-idiomatic sense which might be more dominant than
the idiomatic usage. For example, for give the push and give the flick, our method finds
give a push and give a flick, respectively, perhaps due to the common use of the latter
forms as light verb constructions. Formake one?s peace, our method finds a different form,
make peace, which seems a plausible canonical form; and moreover, the canonical form
listed in the dictionaries (make one?s peace) has a z-score which is only slightly lower
than our threshold. There is also one case where our method finds a canonical form
that corresponds to a different idiom using the same verb+noun: we find lose touch as
a canonical form, whereas the dictionaries list an idiom with a different canonical form
(lose one?s touch) as the idiom with lose and touch.
In general, canonical forms extracted by our method are reasonably accurate, but
may need to be further analyzed by a lexicographer to filter out incorrectly found
patterns. Moreover, our method extracts new canonical forms for some expressions,
which could be used to augment dictionaries.
6. Automatic Identification of VNIC Tokens
In previous sections, we have provided an analysis of the lexical and syntactic behavior
of idiomatic expressions. We have shown that our proposed techniques that draw on
such properties can successfully distinguish an idiomatic verb+noun combination (a
VNIC type) such as get the sack from a non-idiomatic (literal) one such as get the bag. It is
important, however, to note that a potentially idiomatic expression such as get the sack
can also have a literal interpretation in a given context, as in Joe got the sack from the top
shelf . This is true of many potential idioms, although the relative proportion of literal
usages may differ from one expression to another. For example, an expression such as
see stars is much more likely to have a literal interpretation than get the sack (according to
our findings in the BNC). Identification of idiomatic tokens in context is thus necessary
for a full understanding of text, and this will be the focus of Sections 6 and 7.
Recent studies addressing token identification for idiomatic expressions mainly
perform the task as one of word sense disambiguation, and draw on the local context of
82
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a token to disambiguate it. Such techniques either do not use any information regarding
the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the
property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make
use of deep linguistic information often handcode the knowledge into the systems
(Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006). Our goal is
to develop techniques that draw on the specific linguistic properties of idioms for their
identification, without the need for handcoded knowledge or manually labelled train-
ing data. Such unsupervised techniques can also help provide automatically labelled
(noisy) training data to bootstrap (semi-)supervised methods.
In Sections 3 and 4, we showed that the lexical and syntactic fixedness of idioms
is especially relevant to their type-based recognition. We expect such properties to also
be relevant for their token identification. Moreover, we have shown that it is possible to
learn about the fixedness of idioms in an unsupervised manner. Here, we propose unsu-
pervised techniques that draw on the syntactic fixedness of idioms to classify individual
tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward a
classification technique that combines such information (in the form of noisy training
data) with evidence from the local context of usages of an expression. In Section 6.1,
we elaborate on the underlying assumptions of our token identification techniques.
Section 6.2 then describes our proposed methods that draw on these assumptions to
perform the task.
6.1 Underlying Assumptions
Although there may be fine-grained differences in meaning across the idiomatic us-
ages of an expression, as well as across its literal usages, we assume that the idiomatic
and literal usages correspond to two coarse-grained senses of the expression. We will
refer then to each of the literal and idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that each may have multiple fine-grained
senses.
Recall from Section 2 that idioms tend to be somewhat fixed with respect to the
syntactic configurations in which they occur. For example, pull one?s weight tends to
mainly appear in this form when used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used with a literal meaning. In other words,
an idiom tends to have one (or a small number of) canonical form(s), which are its most
preferred syntactic patterns.16 Here we assume that, in most cases, idiomatic usages of
an expression tend to occur in its canonical form(s). We also assume that, in contrast,
the literal usages of an expression are less syntactically restricted, and are expressed
in a greater variety of patterns. Because of their relative unrestrictedness, literal usages
may occur in a canonical form for that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative syntactic patterns for the expression,
which we refer to as the non-canonical forms of the expression, are more likely to be
literal.
Drawing on these assumptions, we develop unsupervised methods that deter-
mine, for each verb+noun token in context, whether it has an idiomatic or a literal
16 As noted previously, 93 out of the 100 idiomatic pairs in TESTall have one canonical form, according to the
entries in ODCIE and CCID. Also, our canonical form extraction method on average finds 1.2 canonical
forms for the 100 test idioms.
83
Computational Linguistics Volume 35, Number 1
interpretation. Clearly, the success of our methods depends on the extent to which these
assumptions hold (we will return to these assumptions in Section 7.2.3).
6.2 Proposed Methods
This section elaborates on our proposed methods for identifying the idiomatic and
literal usages of a verb+noun combination: the CFORM method that uses knowledge
of canonical forms only, and the CONTEXT method that also incorporates distributional
evidence about the local context of a token. Both methods draw on our assumptions
described herein, that usages in the canonical form(s) for a potential idiom are more
likely to be idiomatic, and those in other forms are more likely to be literal. Because
our methods need information about canonical forms of an expression, we use the
unsupervisedmethod described in Section 5 to find these automatically. In the following
discussion, we describe each method in more detail.
CFORM. This method classifies an instance (token) of an expression as idiomatic if it
occurs in one of the automatically determined canonical form(s) for that expression
(e.g., pull one?s weight), and as literal otherwise (e.g., pull a weight, pull the weights). The
underlying assumption of this method is that information about the canonical form(s) of
an idiom type can provide a reasonably accurate classification of its individual instances
as literal or idiomatic.
CONTEXT. Recall our assumption that the idiomatic and literal usages of an idiom corre-
spond to two coarse-grained meanings of the expression. It is natural to further assume
that the literal and idiomatic usages have more in common semantically within each
group than between the two groups. Adopting a distributional approach to meaning?
where the meaning of an expression is approximated by the words with which it co-
occurs (Firth 1957)?we would expect the literal and idiomatic usages of an expression
to typically occur with different sets of words.
Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local
context of an idiom usage is useful in identifying its sense. Inspired by this work, we
propose an unsupervisedmethod that incorporates distributional information about the
local context of the usages of an idiom, in addition to the (syntactic) knowledge about
its canonical forms, in order to determine if its token usages are literal or idiomatic.
To achieve this, the method compares the context surrounding a test instance of an
expression to ?gold-standard? contexts for the idiomatic and literal usages of the expres-
sion, which are taken from noisy training data automatically labelled using canonical
forms.17
For each test instance of an expression, the CONTEXT method thus compares its
co-occurring words to two sets of gold-standard co-occurring words: one typical of
idiomatic usages and one typical of literal usages of the expression (we will shortly
explain precisely how we find these). If the test token is determined to be (on aver-
age) more similar to the idiomatic usages, then it is labelled as idiomatic. Other-
wise, it is labelled as literal. To measure similarity between two sets of words, we use
17 The two CONTEXT methods in our earlier work (Cook, Fazly, and Stevenson 2007) were biased because
they used information about the canonical form of a test token (in addition to context information).
We found that when the bias was removed, the similarity measure used in those techniques was not
as effective, and hence we have developed a different method here.
84
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a standard distributional similarity measure, Jaccard, defined subsequently.18 In the
following equation A and B represent the two sets of words to be compared:
Jaccard(A,B) = A ? B
A ? B (9)
Nowwe explain how the CONTEXT method finds typically co-occurring words for each
of the idiomatic and literal meanings of an expression. Note that unlike in a supervised
setting, here we do not assume access to manually annotated training data. We thus use
knowledge of automatically acquired canonical forms to find these.
The CONTEXT method labels usages of an expression in a leave-one-out strategy,
where each test token is labelled by using the other tokens as noisy training (gold-
standard) data. Specifically, to provide gold-standard data for each instance of an
expression, we first divide the other instances (of the same expression) into likely-
idiomatic and likely-literal groups, where the former group contains usages in canonical
form(s) and the latter contains usages in non-canonical form(s). We then pick represen-
tative usages from each group by selecting the K instances that are most similar to the
instance being labelled (the test token) according to the Jaccard similarity score.
Recall that we assume canonical form(s) are predictive of the idiomatic usages and
non-canonical form(s) are indicative of the literal usages of an expression. We thus
expect the co-occurrence sets of the selected canonical and non-canonical instances to
reflect the idiomatic and literal meanings of the expression, respectively. We take the
average similarity of the test token to the K nearest canonical instances (likely idiomatic)
and the K nearest non-canonical instances (likely literal), and label the test token accord-
ingly.19 In the event that there are less than K canonical or non-canonical form usages
of an expression, we take the average similarity over however many instances there are
of this form. If we have no instances of one of these forms, we classify each token as
idiomatic, the label we expect to be more frequent.
7. VNIC Token Identification: Evaluation
To evaluate the performance of our proposed token identification methods, we use
each in a classification task, in which the method indicates for each instance of a given
expression whether it has an idiomatic or a literal interpretation. Section 7.1 explains
the details of our experimental setup. Section 7.2 then presents the experimental results
as well as some discussion and analysis.
7.1 Experimental Setup
7.1.1 Experimental Expressions and Annotation. In our token classification experiments,
we use a subset of the 180 idiomatic expressions in the development and test data sets
used in the type-based experiments of Section 4. From the original 180 expressions, we
discard those whose frequency in the BNC is lower than 20, to increase the likelihood
that there are both literal and idiomatic usages of each expression. We also discard any
18 It is possible to incorporate extra knowledge sources, such as WordNet, for measuring similarity
between two sets of words. However, our intention is to focus on purely unsupervised, knowledge-lean
approaches.
19 We also tried using the average similarity of the test token to all instances in each group. However,
we found that focusing on the most similar instances from each group performs better.
85
Computational Linguistics Volume 35, Number 1
expression that is not from the two dictionaries ODCIE and CCID (see Section 4.1.2
for more details on the original data sets). This process results in the selection of
60 candidate verb?noun pairs.
For each of the selected pairs, 100 sentences containing its usage were randomly ex-
tracted from the automatically parsed BNC, using themethod described in Section 4.1.1.
For a pair which occurs less than 100 times in the BNC, all of its usages were extracted.
Two judges were asked to independently label each use of each candidate expression as
literal, idiomatic, or unknown. When annotating a token, the judges had access to only
the sentence in which it occurred, and not the surrounding sentences. If this context was
insufficient to determine the class of the expression, the judge assigned the unknown
label. In an effort to assure high agreement between the judges? annotations, the judges
were also provided with the dictionary definitions of the idiomatic meanings of the
expressions.
Idiomaticity is not a binary property; rather it is known to fall on a continuum
from completely semantically transparent, or literal, to entirely opaque, or idiomatic.
The human annotators were required to pick the label, literal or idiomatic, that best fit
the usage in their judgment; they were not to use the unknown label for intermediate
cases. Figurative extensions of literal meanings were classified as literal if their overall
meaning was judged to be fairly transparent, as in You turn right when we hit the road
at the end of this track (taken from the BNC). Sometimes an idiomatic usage, such as have
word in At the moment they only had the word of Nicola?s husband for what had happened
(also taken from the BNC), is somewhat directly related to its literal meaning, which
is not the case for more semantically opaque idioms such as hit the roof. This sentence
was classified as idiomatic because the idiomatic meaning is muchmore salient than the
literal meaning.
First, our primary judge, a native English speaker and an author of this paper,
annotated each use of each candidate expression. Based on this judge?s annotations, we
removed the 25 expressions with fewer than 5 instances of either of their literal or idi-
omatic meanings, leaving 28 expressions.20 (We will revisit the 25 removed expressions
in Section 7.2.4.) The remaining expressions were then split into development (DEV) and
test (TEST) sets of 14 expressions each. The data was divided such that DEV and TEST
would be approximately equal with respect to the frequency of their expressions, as
well as their proportion of idiomatic-to-literal usages (according to the primary judge?s
annotations). At this stage, DEV and TEST contained a total of 813 and 743 tokens,
respectively.
Our second judge, also a native English-speaking author of this paper, then anno-
tated DEV and TEST sentences. The observed agreement and unweighted kappa score
(Cohen 1960) on TEST were 76% and 0.62, respectively. The judges discussed tokens on
which they disagreed to achieve a consensus annotation. Final annotations were gener-
ated by removing tokens that received the unknown label as the consensus annotation,
leaving DEV and TEST with a total of 573 and 607 tokens, and an average of 41 and 43 to-
kens per expression, respectively. Table 5 shows the DEV and the TEST verb?noun pairs
used in our experiments. The table also contains information on the number of tokens
considered for each pair, as well as the percentage of its usages which are idiomatic.
20 From the original set of 60 expressions, seven were excluded because our primary annotator did not
provide any annotations for them. These include catch one?s breath, cut one?s losses, and push one?s luck (for
which our annotator did not have access to a literal interpretation); and blow one?s (own) horn, pull one?s
hair, give a lift, and get the bird (for which our annotator did not have access to an idiomatic meaning).
86
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 5
Experimental DEV and TEST verb?noun pairs, their token frequency (FRQ), and the percentage of
their usages that are idiomatic (%IDM), ordered in decreasing %IDM.
DEV TEST
verb?noun FRQ %IDM verb?noun FRQ %IDM
find foot 52 90 have word 89 90
make face 30 90 lose thread 20 90
get nod 26 89 get sack 50 86
pull weight 33 82 make mark 85 85
kick heel 38 79 cut figure 43 84
hit road 31 77 pull punch 22 82
take heart 79 73 blow top 28 82
pull plug 65 69 make scene 48 58
blow trumpet 29 66 make hay 17 53
hit roof 17 65 get wind 29 45
lose head 38 55 make hit 14 36
make pile 25 32 blow whistle 78 35
pull leg 51 22 hold fire 23 30
see star 61 8 hit wall 61 11
7.1.2 Baselines, Parameters, and Performance Measures. We compare the performance of
our proposed methods, CFORM and CONTEXT, with the baseline of always predicting
an idiomatic interpretation, the most frequent meaning in our development data. We
also compare the unsupervised methods against a supervised method, SUP, which is
similar to CONTEXT, except that it forms the idiomatic and literal co-occurrence sets
from manually annotated data (instead of automatically labelled data using canonical
forms). Like CONTEXT, SUP also classifies tokens in a leave-one-out methodology using
the K idiomatic and literal instances which are most similar to a test token. For both
CONTEXT and SUP, we set the value of K (the number of similar instances used as
gold-standard) to 5, since experiments on DEV indicated that performance did not vary
substantially using a range of values of K.
For all methods, we report the accuracy macro-averaged over all expressions in
TEST. We use the individual accuracies (accuracies for the individual expressions) to
perform t-tests for verifying whether different methods have significantly different
performance. To further analyze the performance of the methods, we also report their
recall and precision on identifying usages from each of the idiomatic and literal classes.
7.2 Experimental Results and Analysis
We first discuss the overall performance of our proposed unsupervised methods in
Section 7.2.1. Results reported in Section 7.2.1 are on TEST (results on DEV have similar
trends, unless noted otherwise). Next, we look into the performance of our methods
on expressions with different proportions of idiomatic-to-literal usages in Section 7.2.2,
which presents results on TEST and DEV combined, as explained subsequently. Sec-
tion 7.2.3 provides an analysis of the errors made because of using canonical forms, and
identifies some possible directions for future work. In Section 7.2.4, we present results
on a new data set containing expressions with highly skewed proportion of idiomatic-
to-literal usages.
87
Computational Linguistics Volume 35, Number 1
Table 6
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on TEST expressions.
Method %Acc (%ERR)
Baseline 61.9
Unsupervised CONTEXT 65.8 (10.2)
CFORM 72.4 (27.6)
Supervised SUP 82.7 (54.6)
7.2.1 Overall Performance. Table 6 shows the macro-averaged accuracy on TEST of our
two unsupervised methods, as well as that of the baseline and the supervised method
for comparison. The best unsupervised performance is indicated in boldface.
As the table shows, both of our unsupervised methods as well as the supervised
method outperform the baseline, confirming that the canonical forms of an expression,
and local context, are both informative in distinguishing literal and idiomatic instances
of the expression.21 Moreover, CFORM outperforms CONTEXT (difference is marginally
significant at p < .06), which is somewhat unexpected, as CONTEXT was proposed
as an improvement over CFORM in that it combines contextual information along
with the syntactic information provided by CFORM. We return to these results later
(Section 7.2.3) to offer some reasons as to why this might be the case. However, the
results using CFORM confirm our hypothesis that canonical forms?which reflect the
overall behavior of a verb+noun type?are strongly informative about the class of a
token. Importantly, this is the case even though the canonical forms that we use are
imperfect knowledge obtained automatically through an unsupervised method.
Comparing CFORM with SUP, we observe that even though on average the latter
outperforms the former, the difference is not statistically significant (p > .1). A close
look at the performance of these methods on the individual expressions reveals that
neither consistently outperforms the other on all (or even most) expressions. Moreover,
as we will see in Section 7.2.2, SUP seems to gain most of its advantage over CFORM on
expressions with a low proportion of idiomatic usages, for which canonical forms tend
to have less predictive value (see Section 7.2.3 for details).
Recall that both CONTEXT and SUP label each token by comparing its local context
to those of its K nearest ?idiomatic? and its K nearest ?literal? usages. The difference is
that CONTEXT uses noisy (automatically) labelled data to identify these nearest usages
for each token, whereas SUP uses manually labelled data. One possible direction for fu-
ture work is thus to investigate whether providing substantially larger amounts of data
alleviates the effect of noise, as is often found to be the case by researchers in the field.
7.2.2 Performance Based on Class Distribution. Recall from Section 6 that both of our un-
supervised techniques for token identification depend on how accurately the canonical
forms of an expression can be acquired. The canonical form acquisition technique which
we use here works well if the idiomatic meaning of an expression is sufficiently frequent
compared to its literal usage. In this section, we thus examine the performance of the
21 Performing a paired t-test, we find that the difference between the baseline and CFORM is marginally
significant, p < .06, whereas the difference between baseline and CONTEXT is not statistically significant.
The difference between the baseline and SUP is significant at p < .01. The trend on DEV is somewhat
similar: baseline and CFORM are significantly different at p < .05; SUP is marginally different from
baseline at p < .06.
88
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 7
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 28 expressions
in DT (DEV and TEST combined), divided according to the proportion of idiomatic-to-literal
usages (high and low).
DTIhigh DTIlow
Method %Acc (%ERR) %Acc (%ERR)
Baseline 81.4 35.0
Unsupervised CONTEXT 80.6 (?4.3) 44.6 (14.8)
CFORM 84.7 (17.7) 53.4 (28.3)
Supervised SUP 84.4 (16.1) 76.8 (64.3)
token identification methods for expressions with different proportions of idiomatic-to-
literal usages.
We merge DEV and TEST (referring to the new set as DT), and then divide the re-
sulting set of 28 expressions according to their proportion of idiomatic-to-literal usages
(as determined by the human annotations) as follows.22 Looking at the proportion of
idiomatic usages of our expressions in Table 5, we can see that there are gaps between
55% and 65% in DEV, and between 58% and 82% in TEST, in terms of proportion
of idiomatic usages. The value of 65% thus serves as a natural lower bound for dominant
idiomatic usage, and the value of 58% as a natural upper bound for non-dominant
idiomatic usage. We therefore split DT into two sets: DTIhigh contains 17 expressions with
65?90% of their usages being idiomatic (i.e., their idiomatic usage is dominant), whereas
DTIlow contains 11 expressions with 8?58% of their occurrences being idiomatic (i.e., their
idiomatic usage is not dominant).
Table 7 shows the average accuracy of all the methods on these two groups of
expressions, with the best performance on each group shown in boldface. We first look
at the performance of our methods on DTIhigh . On these expressions, CFORM outperforms
both the baseline (difference is not statistically significant) and CONTEXT (difference is
statistically significant at p < .05). CFORM also has a comparable performance to the su-
pervised method, reinforcing that for these expressions accurate canonical forms can be
acquired and that such knowledge can be used with high confidence for distinguishing
idiomatic and literal usages in context.
We now look into the performance on expressions in DTIlow . On these, both CFORM
and CONTEXT outperform the baseline, showing that even for expressions whose idi-
omatic meaning is not dominant, automatically acquired canonical forms can help with
their token classification. Nonetheless, both these methods perform substantially worse
than the supervised method, reinforcing that the automatically acquired canonical
forms are noisier, and hence less predictive, than they are for expressions in DTIhigh .
The poor performance of the unsupervised methods on expressions in DTIlow (com-
pared to the supervised performance) is likely to be mostly due to the less predictive
canonical forms extracted for these expressions. In general, we can conclude that when
canonical forms can be extracted with a high accuracy, the performance of the CFORM
method is comparable to that of a supervised method. One possible way of improving
the performance of unsupervised methods is thus to develop more accurate techniques
for the automatic acquisition of canonical forms.
22 We combine the two sets in order to have a sufficient number of expressions in each group after division.
89
Computational Linguistics Volume 35, Number 1
Table 8
Confusion matrix for CFORM on expression blow trumpet. idm = idiomatic class; lit = literal class;
tp = true positive; fp = false positive; fn = false negative; tn = true negative.
True Class
idm lit
Predicted idm 17 = tp 6 = fp
Class lit 2 = fn 4 = tn
Table 9
Formulas for calculating Sens and PPV (recall and precision for the idiomatic class), and Spec
and NPV (recall and precision for the literal class) from a confusion matrix.
recall (R) precision (P)
idm Sens =
tp
tp+ fn
PPV =
tp
tp+ fp
lit Spec = tn
tn+ fp
NPV = tn
tn+ fn
Accuracy is often not a sufficient measure for the evaluation of a binary (two-class)
classifier, especially when the number of items in the two classes (here, idiomatic and
literal) differ. Instead, one can have a closer look at the performance of a classifier by
examining its confusion matrix, which compares the labels predicted by the classifier
for each item with its true label. As an example, the confusion matrix of the CFORM
method for the expression blow trumpet is given in Table 8.
Note that the choice of idiomatic as the positive class (and literal as the negative
class) is arbitrary; however, because our ultimate goal is to identify idiomatic usages,
there is a natural reason for this choice. To summarize a confusion matrix, four standard
measures are often used, which are calculated from the cells in the matrix. The measures
are sensitivity (Sens), positive predictive value (PPV), specificity (Spec), and negative
predictive value (NPV), and are calculated as in Table 9. As stated in the table, Sens
and PPV are equivalents of recall and precision for the positive (idiomatic) class, also
referred to as Ridm and Pidm later in the article. Similarly, Spec and NPV are equivalents
of recall and precision for the negative (literal) class, also referred to as Rlit and Plit.
23
Table 10 gives the trimmed mean values of these four performance measures over
expressions in DTIhigh and DTIlow for the baseline, the two unsupervised methods, and the
supervised method.24 (The performance measures on individual expressions are given
in Tables 12, 13, and 14 in the Appendix.) Table 10 shows that, as expected, the baseline
has very high Sens (100% recall on identifying idiomatic usages), but very low Spec (0%
23 We mainly refer to these measures using their standard names in the literature: Sens, PPV, Spec, and
NPV. Alongside the standard names, we use the more expressive names Ridm, Pidm, Rlit, and Plit, to
remind the reader about the semantics of the measures.
24 When averaging interdependent measures, such as precision and recall, one needs to make sure that
the observed trend in the averages is consistent with that in the individual values. Trimmed mean is a
standard statistic used in such cases, which is equivalent to the mean after discarding a percentage (often
between 5 and 25) of the sample data at the high and low ends. Here, we report a 14%-trimmed mean,
which involves removing two data points from each end. The analysis presented here is based on the
trimmed means, as well as the individual values of the performance measures.
90
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 10
Detailed classification performance of all methods over DTIhigh and DTIlow . Performance is given
using four measures: Sens or Ridm, PPV or Pidm, Spec or Rlit, and NPV or Plit, macro-averaged
using 14%-trimmed mean.
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .82 0.00 0.00
DTIhigh CONTEXT .97 .84 .11 .18
CFORM .95 .92 .61 .71
SUP .99 .86 .22 .53
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .36 0.00 0.00
DTIlow CONTEXT .89 .37 .22 .63
CFORM .86 .43 .36 .86
SUP .44 .62 .88 .80
recall on identifying literal usages). We thus expect a well-performing method to have
lower Sens than the baseline, but higher Spec and also higher PPV and NPV (i.e., higher
precision on both idiomatic and literal usages).
Looking at performance on DTIhigh , we find that all three methods have reasonably
high Sens and PPV, revealing that the methods are good at labeling idiomatic usages.
Performance on literal usages, however, differs across the three methods. CONTEXT has
very low Spec andNPV, showing that it tends to label most tokens?including the literal
ones?as idiomatic. A close look at the performance of this method on the individual
expressions also confirms this tendency: on many expressions (10 out of 17) the Spec
and NPV of CONTEXT are both zero (see Table 13 in the Appendix). As we will see in
Section 7.2.3, this tendency is partly due to the distribution of the idiomatic and literal
usages in canonical and non-canonical forms; because literal usages can also appear in
a canonical form, for many expressions there are often not many non-canonical form
instances. (Recall that, for training, CONTEXT uses instances in canonical form as being
idiomatic and those in non-canonical form as being literal.) Thus, in many cases, it
is a priori more likely that a token is more similar to the K most similar canonical
form instances. Interestingly, CFORM is the method with the highest Spec and NPV,
even higher than those of the supervised method. Nonetheless, even CFORM is overall
much better at identifying idiomatic tokens than literal ones (see Section 7.2.3 for more
discussion on this).
We now turn to performance on DTIlow . CFORM has a high Sens, but a low PPV,
indicating that most idiomatic usages are identified correctly, but many literal usages
are also misclassified as idiomatic (hence a low Spec). CONTEXT shows the same trend
as CFORM, though overall it has poorer performance. Performance of SUP varies across
the expressions in this group: SUP is very good at identifying literal usages of these
expressions (high Spec and NPV for all expressions). Nonetheless, SUP has a low recall
in identifying idiomatic usages (low Sens) for many of these expressions.
7.2.3 Discussion and Error Analysis. In this section, we examine twomain issues. First, we
look into the plausibility of our original assumptions regarding the predictive value of
canonical forms (and non-canonical forms). Second, we investigate the appropriateness
of our automatically extracted canonical forms.
91
Computational Linguistics Volume 35, Number 1
To learn more about the predictive value of canonical forms, we examine the per-
formance of CFORM on the 28 expressions under study. More specifically, we look at
the values of Sens, PPV, Spec, and NPV on these expressions, as shown in Table 12
in the Appendix. On expressions in DTIhigh , CFORM has both high Sens and high PPV.
The formulas in Table 9 indicate that if both Sens and PPV are high, then tp fn and
tp fp. Thus, most idiomatic usages of expressions in DTIhigh appear in a canonical form,
and most usages in a canonical form are idiomatic. The values of Spec and NPV on the
same expressions are in general lower (compared to Sens and PPV), showing that tn is
not much higher than fp or fn.
On expressions in DTIlow , CFORM generally has high Sens but low-to-medium PPV.
This indicates that for these expressions, most idiomatic usages appear in a canonical
form, but not all usages in a canonical form are idiomatic. On these expressions, CFORM
has generally high NPV, but mostly low Spec. These indicate that tn fn, that is, most
usages in a non-canonical form are literal, and that tn is often lower than fp, that is, many
literal usages also appear in a canonical form. For example, almost all usages of hit wall
in a non-canonical form are literal, but most of its literal usages appear in a canonical
form.
Generally, it seems that, as we expected, literal usages are less restricted in terms
of the syntactic form they appear in; they can appear in both canonical form(s) and
in non-canonical form(s). For an expression with a low proportion of literal usages,
we can thus acquire canonical forms that are both accurate and have high predictive
value for identifying idiomatic usages in context. On the contrary, for expressions
with a relatively high proportion of literal usages, automatically acquired canonical
forms are less accurate and also have low predictive value (i.e., they are not specific
to idiomatic usages). We expected that using contextual information would help in
such cases. However, our CONTEXT method relies on noisy training data automatically
labelled using information about canonical forms. Given these findings, it is not sur-
prising that this method performs substantially worse than a corresponding supervised
method that uses similar contextual information, but manually labelled training data. It
remains to be tested in the future whether providing more noisy data will help. Another
possible future direction is to develop context methods that can better exploit noisy
labelled data.
Now we look at a few cases where our automatically extracted canonical forms are
not sufficiently accurate. For a verb+noun such as make pile (i.e., make a pile of money),
we correctly identify only some of the canonical forms. The automatically determined
canonical forms for make pile are make a pile and make piles. However, we find that idi-
omatic usages of this expression are sometimes of the formmake one?s pile. Furthermore,
we find that the frequency of this form is much higher than that of the non-canonical
forms, and not substantially lower than the frequency cut-off for selection as a canonical
form. This indicates that our heuristic for selecting patterns as canonical forms could be
fine-tuned to yield an improvement in performance.
For the expression pull plug, we identify its canonical form as pull the plug, but find a
mixture of literal and idiomatic usages in this form. However, many of the literal usages
are verb-particle constructions using out (pull the plug out), while many of the idiomatic
usages occur with a prepositional phrase headed by on (pull the plug on). This indi-
cates that incorporating information about particles and prepositions could improve
the quality of the canonical forms. Other syntactic categories, such as adjectives, may
also be informative in determining canonical forms for expressions which are typically
used idiomatically with words of a particular syntactic category, as in blow one?s own
trumpet.
92
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 11
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 23 expressions
in SKEWED-IDM and on the 37 expressions in the combination of TEST and SKEWED-IDM (ALL).
SKEWED-IDM ALL
Method %Acc (%ERR) %Acc (%ERR)
Baseline 97.9 84.3
Unsupervised CONTEXT 94.2 (?176.2) 83.3 (?6.4)
CFORM 86.7 (?533.3) 81.3 (?19.1)
Supervised SUP 97.9 (0.0) 92.1 (49.7)
7.2.4 Performance on Expressions with Skewed Distribution. Recall from Section 7.1.1 that,
from the original set of 60 candidate expressions, we excluded those that had fewer than
5 instances of either of their literal or idiomatic meanings. It is nonetheless important to
see how well our methods perform on such expressions. In this section, we thus report
the performance of our measures on the set of 23 expressions with mostly idiomatic
usages, referred to as SKEWED-IDM. Table 11 presents the macro-averaged accuracy of
our methods on these expressions. This table also shows the accuracy on all unseen test
expressions, that is, the combination of SKEWED-IDM and TEST, referred to as ALL, for
comparison.25
On SKEWED-IDM, the supervised method performs as well as the baseline, whereas
both unsupervised methods perform worse.26 Note that for 19 out of the 23 expressions
in SKEWED-IDM, all instances are idiomatic, and the baseline accuracy is thus 100%. On
these, SUP also has 100% accuracy because no literal instances are available, and thus
SUP labels every token as idiomatic (same as the baseline). As for the unsupervised
methods, we can see that, unlike on TEST, the CONTEXT method outperforms CFORM
(the difference is statistically significant at p < .001). We saw previously that CONTEXT
tends to label usages as idiomatic. This bias might be partially responsible for the
better performance of CONTEXT on this data set. Moreover, we find that many of these
expressions tend to appear in a highly frequent canonical form, but also in less frequent
syntactic forms which we (perhaps incorrectly) consider as non-canonical forms. When
considering the performance on all unseen test expressions (ALL), neither unsupervised
method performs as well as the baseline, but the supervised method offers a substantial
improvement over the baseline.27
Our annotators pointed out that for many of the expressions in SKEWED-IDM,
either a literal interpretation was almost impossible (as for catch one?s imagination),
or extremely implausible (as for kick the habit). Hence, the annotators could predict
beforehand that the expression would be mainly used with an idiomatic meaning. A
semi-supervised approach that combines expert human knowledge with automatically
extracted corpus-drawn information can thus be beneficial for the task of identifying
25 The results obtained on the two excluded expressions which are predominantly used literally in terms
of percent accuracy using the various methods are as follows. Baseline: 4.2, Unsupervised CONTEXT: 6.5,
Unsupervised CFORM: 16.2, Supervised: 43.5. However, because there are only two such expressions,
it is difficult to draw conclusions from these results, and we do not further consider these expressions.
26 According to a paired t-test, on SKEWED-IDM, all the observed differences are statistically significant at
p < .05.
27 According to a paired t-test, on ALL, the differences between the supervised method and the three other
methods are statistically significant at p < .01; none of the other differences are statistically significant.
93
Computational Linguistics Volume 35, Number 1
idiomatic expressions in context. A human expert (e.g., a lexicographer) could first
filter out expressions for which a literal interpretation is highly unlikely. For the rest
of the expressions, a simple unsupervised method such as CFORM?that relies only on
automatically extracted information?can be used with reasonable accuracy.
8. Related Work
8.1 Type-Based Recognition of Idioms and Other Multiword Expressions
Our work relates to previous studies on determining the compositionality (the inverse
of idiomaticity) of idioms and other multiword expressions (MWEs). Most previous
work on the compositionality of MWEs either treats them as collocations (Smadja 1993),
or examines the distributional similarity between the expression and its constituents
(Baldwin et al 2003; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Others have identified MWEs by looking into specific linguistic cues,
such as the lexical fixedness of non-compositional MWEs (Lin 1999; Wermter and Hahn
2005), or the lexical flexibility of productive noun compounds (Lapata and Lascarides
2003). Venkatapathy and Joshi (2005) combine aspects of this work, by incorporating
lexical fixedness, distributional similarity, and collocation-based measures into a
set of features which are used to rank verb+noun combinations according to their
compositionality. Our work differs from such studies in that it considers various kinds
of fixedness as surface behaviors that are tightly related to the underlying semantic
idiosyncrasy (idiomaticity) of expressions. Accordingly, we propose novel methods
for measuring the degree of lexical, syntactic, and overall fixedness of verb+noun
combinations, and use these as indirect ways of measuring degree of idiomaticity.
Earlier research on the lexical encoding of idiom types mainly relied on the exis-
tence of human annotations, especially for detecting which syntactic variations (e.g.,
passivization) an idiom can undergo (Odijk 2004; Villavicencio et al 2004). Evert, Heid,
and Spranger (2004) and Ritz and Heid (2006) propose methods for automatically
determining morphosyntactic preferences of idiomatic expressions. However, they treat
individual morphosyntactic markers (e.g., the number of the noun in a verb+noun
combination) as independent features, and rely mainly on the relative frequency of
each possible value for a feature (e.g., plural for number) as an indicator of a preference
for that value. If the relative frequency of a particular value of a feature for a given
combination (or the lower bound of the confidence interval, in the case of Evert, Heid,
and Spranger?s approach) is higher than a certain threshold, then the expression is
said to have a preference for that value. These studies recognize that morphosyntactic
preferences can be employed as clues to the identification of idiomatic combinations;
however, none proposes a systematic approach for such a task. Moreover, only subjec-
tive evaluations of the proposed methods are presented.
Others have also drawn on the notion of syntactic fixedness for the detection
of idioms and other MWEs. Widdows and Dorow (2005), for example, look into the
fixedness of a highly constrained type of idiom, namely, those of the form ?X conj X?
where X is a noun or an adjective, and conj is a conjunction such as and, or, but. Smadja
(1993) also notes the importance of syntactic fixedness in identifying strongly associated
multiword sequences, including collocations and idioms. Nonetheless, in both these
studies, the notion of syntactic fixedness is limited to the relative position of words
within the sequence. Such a general notion of fixedness does not take into account some
of the important syntactic properties of idioms (e.g., the choice of the determiner), and
hence cannot distinguish among different subtypes of MWEs which may differ on such
94
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
grounds. Our syntactic fixedness measure looks into a set of linguistically informed
patterns associated with a coherent, though large, class of idiomatic expressions. Results
presented in this article show that the fixedness measures can successfully separate
idioms from literal phrases. Corpus analysis of the measures proves that they can also
be used to distinguish idioms from other MWEs, such as light verb constructions and
collocations (Fazly and Stevenson 2007; Fazly and Stevenson, to appear). Bannard (2007)
proposes an extension of our syntactic fixedness measure?which first appeared in
Fazly and Stevenson (2006)?where he uses different prior distributions for different
syntactic variations.
Work on the identification of MWE types has also looked at evidence from another
language. For example, Melamed (1997a) assumes that non-compositional compounds
(NCCs) are usually not translated word-for-word to another language. He thus pro-
poses to discover NCCs by maximizing the information-theoretic predictive value of
a translation model between two languages. The sample extracted NCCs reveal an
important drawback of the proposed method: It relies on a translation model only,
without taking into account any prior linguistic knowledge about possible NCCswithin
a language. Nonetheless, such a technique is capable of identifying many NCCs that are
relevant for a translation task. Villada Moiro?n and Tiedemann (2006) propose measures
for distinguishing idiomatic expressions from literal ones (in Dutch), by examining
their automatically generated translations into a second language, such as English or
Spanish. Their approach is based on the assumptions that idiomatic expressions tend
to have fewer predictable translations and fewer compositional meanings, compared
to the literal ones. The first property is measured as the diversity in the translations
for the expression, estimated using an entropy-based measure proposed by Melamed
(1997b). The non-compositionality of an expression is measured as the overlap between
the meaning of an expression (i.e., its translations) and those of its component words.
General approaches (such as those explained in the previous paragraph) may be
more easily extended to different domains and languages. Our measures incorporate
language-specific information about idiomatic expressions, thus extra work may be
required to extend and apply them to other languages and other expressions. (Though
see Van de Cruys and Villada Moiro?n [2007] for an extension of our measures to Dutch
idioms of the form verb plus prepositional phrase.) Nonetheless, because our measures
capture deep linguistic information, they are also expected to acquire more detailed
knowledge?for example, they can be used for identifying other classes of MWEs (Fazly
and Stevenson 2007).
8.2 Token-Based Identification of Idioms and Other Multiword Expressions
A handful of studies have focused on identifying idiomatic and non-idiomatic usages
(tokens) of words or MWEs. Birke and Sarkar (2006) propose a minimally supervised
algorithm for distinguishing between literal and non-literal usages of verbs in context.
Their algorithm uses seed sets of literal and non-literal usages that are automatically
extracted from online resources such as WordNet. The similarity between the context of
a target token and that of each seed set determines the class of the token. The approach is
general in that it uses a slightly modified version of an existing word sense disambigua-
tion algorithm. This is both an advantage and a drawback: The algorithm can be easily
extended to other parts of speech and other languages; however, such a general method
ignores the specific properties of non-literal (metaphorical and/or idiomatic) language.
Similarly, the supervised token classification method of Katz and Giesbrecht (2006)
relies primarily on the local context of a token, and fails to exploit specific linguistic
95
Computational Linguistics Volume 35, Number 1
properties of non-literal language. Our results suggest that such properties are often
more informative than the local context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher (2005) distinguishes between com-
positional and non-compositional usages of English verb-particle constructions. Their
classifier incorporates linguistically motivated features, such as the degree of separation
between the verb and particle. Here, we focus on a different class of English MWEs,
namely, the class of idiomatic verb+noun combinations. Moreover, by making a more
direct use of their syntactic behavior, we develop unsupervised token classification
methods that perform well. The unsupervised token classifier of Hashimoto, Sato, and
Utsuro (2006) uses manually encoded information about allowable and non-allowable
syntactic transformations of Japanese idioms, which are roughly equivalent to our
notions of canonical and non-canonical forms. The rule-based classifier of Uchiyama,
Baldwin, and Ishizaki (2005) incorporates syntactic information about Japanese com-
pound verbs (JCVs), a type of MWE composed of two verbs. In both cases, although the
classifiers incorporate syntactic information about MWEs, their manual development
limits the scalability of the approaches.
Uchiyama, Baldwin, and Ishizaki (2005) also propose a statistical token classifica-
tion method for JCVs. This method is similar to ours, in that it also uses type-based
knowledge to determine the class of each token in context. However, their method is
supervised, whereas our methods are unsupervised. Moreover, Uchiyama, Baldwin,
and Ishizaki only evaluate their methods on a set of JCVs that are mostly monosemous.
Our main focus here is on MWEs that are harder to disambiguate, that is, those that
have two clear idiomatic and literal meanings, and that are frequently used with either
meaning.
9. Conclusions
The significance of the role idioms play in language has long been recognized; however,
due to their peculiar behavior, they have been mostly overlooked by researchers in
computational linguistics. In this work, we focus on a broadly documented and cross-
linguistically frequent class of idiomatic MWEs: those that involve the combination
of a verb and a noun in its direct object position, which we refer to as verb+noun
idiomatic combinations or VNICs. Although a great deal of research has focused on
non-compositionality of MWEs, less attention has been paid to other properties relevant
to their semantic idiosyncrasy, such as lexical and syntactic fixedness. Drawing on such
properties, we have developed techniques for the automatic recognition of VNIC types,
as well as methods for their token identification in context.
We propose techniques for the automatic acquisition and encoding of knowledge
about the lexicosyntactic behavior of idiomatic combinations. More specifically, we
propose novel statistical measures that quantify the degree of lexical, syntactic, and
overall fixedness of a verb+noun combination. We demonstrate that these measures
can be successfully applied to the task of automatically distinguishing idiomatic ex-
pressions (types) from non-idiomatic ones. Our results show that the syntactic and
overall fixedness measures substantially outperform existing measures of collocation
extraction, even when they incorporate some syntactic information. We put forward
an unsupervised means for automatically discovering the set of syntactic variations
that are preferred by a VNIC type (its canonical forms) and that should be included
in its lexical representation. In addition, we show that the canonical form extraction
method can effectively be used in identifying idiomatic and literal usages (tokens) of an
expression in context.
96
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
We have annotated a total of 2, 465 tokens for 51 VNIC types according to whether
they are a literal or idiomatic usage. We found that for 28 expressions (1, 180 tokens),
approximately 40% of the usages were literal. For the remaining 23 expressions (1, 285
tokens), almost all usages were idiomatic. These figures indicate that automatically
determining whether a particular instance of an expression is used idiomatically or lit-
erally is of great importance for NLP applications. We have proposed two unsupervised
methods that perform such a task.
Our proposed methods incorporate automatically acquired knowledge about the
overall syntactic behavior of a VNIC type, in order to do token classification. More
specifically, our methods draw on the syntactic fixedness of VNICs?a property which
has been largely ignored in previous studies of MWE tokens. Our results confirm the
usefulness of this property as incorporated into our methods. On the 23 expressions
whose usages are predominantly idiomatic, because the baseline is very high none
of the methods outperform it. Nonetheless, as pointed out by our human annotators,
for many of these expressions it can be predicted beforehand that they are mainly
idiomatic and that a literal interpretation is impossible or highly implausible. On the
28 expressions with frequent literal usages, all our methods outperform the baseline of
always predicting themost dominant class (idiomatic). Moreover, on these, the accuracy
of our best unsupervised method is not substantially lower than the accuracy of a
standard supervised approach.
Appendix: Performance on the Individual Expressions
This Appendix contains the values of the four performance measures, Sens, PPV, Spec,
and NPV, for our two unsupervised methods (i.e., CFORM and CONTEXT) as well as for
the supervised method, SUP, on individual expressions in DTIhigh and DTIlow . Expressions
(verb?noun pairs) in each data set are ordered alphabetically.
Table 12
Performance of CFORM on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.92 0.60 1.00
blow trumpet 0.89 0.89 0.80 0.80
cut figure 0.97 0.97 0.86 0.86
find foot 0.98 0.92 0.20 0.50
get nod 0.96 1.00 1.00 0.75
get sack 1.00 0.96 0.71 1.00
have word 0.56 0.96 0.78 0.17
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 1.00 0.81 0.12 1.00
lose thread 0.94 0.94 0.50 0.50
make face 0.74 0.95 0.67 0.22
make mark 0.85 1.00 1.00 0.54
pull plug 0.89 0.77 0.40 0.62
pull punch 0.83 0.94 0.75 0.50
pull weight 1.00 0.93 0.67 1.00
take heart 1.00 0.97 0.88 1.00
97
Computational Linguistics Volume 35, Number 1
Table 12
(continued)
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow whistle 0.93 0.44 0.37 0.90
get wind 0.85 0.73 0.75 0.86
hit wall 0.86 0.11 0.09 0.83
hold fire 1.00 0.37 0.25 1.00
lose head 0.76 0.62 0.41 0.58
DTIlow make hay 1.00 0.56 0.12 1.00
make hit 1.00 0.71 0.78 1.00
make pile 0.25 0.14 0.29 0.45
make scene 0.82 0.68 0.45 0.64
pull leg 0.64 0.23 0.40 0.80
see star 0.80 0.10 0.38 0.95
Table 13
Performance of CONTEXT on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.89 0.74 0.40 0.67
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 1.00 0.88 0.00 0.00
get sack 1.00 0.86 0.00 0.00
have word 0.70 0.95 0.67 0.20
hit road 1.00 0.77 0.00 0.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.90 0.00 0.00
make face 0.85 0.88 0.00 0.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.96 0.69 0.05 0.33
pull punch 0.94 0.89 0.50 0.67
pull weight 1.00 0.82 0.00 0.00
take heart 0.90 0.85 0.38 0.50
blow whistle 0.89 0.36 0.18 0.75
get wind 0.85 0.65 0.62 0.83
hit wall 1.00 0.11 0.00 0.00
hold fire 1.00 0.30 0.00 0.00
lose head 0.90 0.56 0.12 0.50
DTIlow make hay 0.78 0.50 0.12 0.33
make hit 0.60 0.38 0.44 0.67
make pile 0.50 0.25 0.29 0.56
make scene 0.96 0.66 0.30 0.86
pull leg 0.82 0.22 0.20 0.80
see star 1.00 0.12 0.32 1.00
98
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 14
Performance of SUP on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.95 0.72 0.30 0.75
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 0.91 0.91 0.33 0.33
get sack 1.00 0.86 0.00 0.00
have word 1.00 0.90 0.00 0.00
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 0.82 0.64 0.17 0.33
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.95 0.50 1.00
make face 1.00 0.96 0.67 1.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.98 0.90 0.75 0.94
pull punch 1.00 0.90 0.50 1.00
pull weight 1.00 0.82 0.00 0.00
take heart 0.93 0.83 0.25 0.50
blow whistle 0.52 0.78 0.92 0.78
get wind 0.77 0.71 0.75 0.80
hit wall 0.00 0.00 1.00 0.89
hold fire 0.00 0.00 0.88 0.67
lose head 0.48 0.62 0.65 0.50
DTIlow make hay 0.89 0.80 0.75 0.86
make hit 0.40 1.00 1.00 0.75
make pile 0.38 0.75 0.94 0.76
make scene 0.89 0.69 0.45 0.75
pull leg 0.55 0.75 0.95 0.88
see star 0.00 0.00 1.00 0.92
99
Computational Linguistics Volume 35, Number 1
Acknowledgments
This article is an extended and updated
combination of two papers that appeared,
respectively, in the proceedings of EACL
2006 and the proceedings of the ACL 2007
Workshop on A Broader Perspective on
Multiword Expressions. We wish to thank
the anonymous reviewers of those papers
for their helpful recommendations. We also
thank the anonymous reviewers of this
article for their insightful comments which
we believe have helped us improve the
quality of the work. We are grateful to Eric
Joanis for providing us with the NP-head
extraction software, and to Afra Alishahi
and Vivian Tsang for proofreading the
manuscript. Our work is financially
supported by the Natural Sciences and
Engineering Research Council of Canada,
the Ontario Graduate Scholarship program,
and the University of Toronto.
References
Abeille?, Anne. 1995. The flexibility of French
idioms: A representation with lexicalized
Tree Adjoining Grammar. In Everaert
et al, editors, Idioms: Structural and
Psychological Perspectives. LEA, Mahwah,
NJ, pages 15?42.
Akimoto, Minoji. 1999. Collocations and
idioms in Late Modern English. In L. J.
Brinton and M. Akimoto. Collocational and
Idiomatic Aspects of Composite Predicates in
the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 207?238.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003. An
empirical model of multiword expression
decomposability. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 89?96, Sapporo.
Bannard, Colin. 2007. A measure of syntactic
flexibility for automatically identifying
multiword expressions in corpora. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 1?8, Prague.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
approach to the semantics of
verb-particles. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 65?72, Sapporo.
Birke, Julia and Anoop Sarkar. 2006. A
clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL?06), pages 329?336, Trento.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition),
second edition. Available at www.natcorp.
ox.ac.uk.
Cacciari, Cristina. 1993. The place of idioms
in a literal and metaphorical world. In C.
Cacciari and P. Tabossi, Idioms: Processing,
Structure, and Interpretation. LEA, Mahwah,
NJ, pages 27?53.
Church, Kenneth, William Gale, Patrick
Hanks, and Donald Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik,
editor, Lexical Acquisition: Exploiting
On-Line Resources to Build a Lexicon. LEA,
Mahwah, NJ, pages 115?164.
Claridge, Claudia. 2000.Multi-word Verbs in
Early Modern English: A Corpus-based Study.
Editions Rodopi B. V., Amsterdam.
Clark, Eve V. 1978. Discovering what words
can do. Papers from the Parasession on the
Lexicon, 14:34?57.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Cook, Paul, Afsaneh Fazly, and Suzanne
Stevenson. 2007. Pulling their weight:
Exploiting syntactic forms for the
automatic identification of idiomatic
expressions in context. In Proceedings of the
ACL?07 Workshop on a Broader Perspective on
Multiword Expressions, pages 41?48,
Prague.
Copestake, Ann, Fabre Lambeau, Aline
Villavicencio, Francis Bond, Timothy
Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic
precision and reusability. In Proceedings of
the 4th International Conference on Language
Resources and Evaluation (LREC?02),
pages 1941?47, Las Palmas.
Cover, Thomas M. and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley
and Sons, Inc., New York.
Cowie, Anthony P., Ronald Mackin, and
Isabel R. McCaig. 1983. Oxford Dictionary of
Current Idiomatic English, volume 2. Oxford
University Press.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word co-occurrence probabilities. In
Proceedings of the 32nd Anuual Meeting of the
100
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Association for Computational Linguistics
(ACL?94), pages 272?278, Las Cruces, NM.
d?Arcais, Giovanni B. Flores. 1993. The
comprehension and semantic
interpretation of idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 79?98.
Desbiens, Marguerite Champagne and Mara
Simon. 2003. De?terminants et locutions
verbales. Manuscript. Available at
www.er.uqam.ca/nobel/scilang/cesla02/
mara margue.pdf.
Evert, Stefan, Ulrich Heid, and Kristina
Spranger. 2004. Identifying
morphosyntactic preferences in
collocations. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC?04),
pages 907?910, Lisbon.
Evert, Stefan and Brigitte Krenn. 2001.
Methods for the qualitative evaluation of
lexical association measures. In Proceedings
of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL?01),
pages 188?195, Toulouse.
Fazly, Afsaneh and Suzanne Stevenson. 2006.
Automatically constructing a lexicon of
verb phrase idiomatic combinations. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?06),
pages 337?344, Trento.
Fazly, Afsaneh and Suzanne Stevenson. 2007.
Distinguishing subtypes of multiword
expressions using linguistically-motivated
statistical measures. In Proceedings of the
ACL?07 Workshop on a Broader Perspective
on Multiword Expressions, pages 9?16,
Prague.
Fazly, Afsaneh and Suzanne Stevenson. A
distributional account of the semantics of
multiword expressions. To appear in the
Italian Journal of Linguistics.
Fellbaum, Christiane. 1993. The determiner
in English idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure,
and Interpretation. LEA, Mahwah, NJ,
pages 271?295.
Fellbaum, Christiane, editor. 1998.WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fellbaum, Christiane. 2002. VP idioms in the
lexicon: Topics for research using a very
large corpus. In Proceedings of the
KONVENS 2002 Conference, pages 7?11,
Saarbruecken, Germany.
Fellbaum, Christiane. 2007. The ontological
loneliness of idioms. In Andrea Schalley
and Dietmar Zaefferer, editors,
Ontolinguistics. Mouton de Gruyter, Berlin,
pages 419?434.
Firth, John R. 1957. A synopsis of linguistic
theory 1930?1955. In Studies in Linguistic
Analysis (special volume of the Philological
Society). The Philological Society, Oxford,
pages 1?32.
Fraser, Bruce. 1970. Idioms within a
transformational grammar. Foundations of
Language, 6:22?42.
Gentner, Dedre and Ilene M. France. 2004.
The verb mutability effect: Studies of the
combinatorial semantics of nouns and
verbs. In Steven L. Small, Garrison W.
Cottrell, and Michael K. Tanenhaus,
editors, Lexical Ambiguity Resolution:
Perspectives from Psycholinguistics,
Neuropsychology, and Artificial Intelligence.
Kaufmann, San Mateo, CA, pages 343?382.
Gibbs, Raymond W. Jr. 1993. Why idioms are
not dead metaphors. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 57?77.
Gibbs, Raymond W. Jr. 1995. Idiomaticity
and human cognition. In Everaert et al,
editors, Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 97?116.
Gibbs, Raymond W. Jr. and Nandini P.
Nayak. 1989. Psychololinguistic studies on
the syntactic behavior of idioms. Cognitive
Psychology, 21:100?138.
Gibbs, Raymond W. Jr., Nandini P. Nayak,
J. Bolton, and M. Keppel. 1989. Speaker?s
assumptions about the lexical flexibility
of idioms.Memory and Cognition,
17:58?68.
Glucksberg, Sam. 1993. Idiom meanings and
allusional content. In C. Cacciari and P.
Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 3?26.
Goldberg, Adele E. 1995. Constructions: A
Construction Grammar Approach to
Argument Structure. The University of
Chicago Press.
Grant, Lynn E. 2005. Frequency of ?core
idioms? in the British National Corpus
(BNC). International Journal of Corpus
Linguistics, 10(4):429?451.
Hashimoto, Chikara, Satoshi Sato, and
Takehito Utsuro. 2006. Japanese idiom
recognition: Drawing a line between
literal and idiomatic meanings. In
Proceedings of the 17th International
Conference on Computational Linguistics
and the 36th Annual Meeting of the
101
Computational Linguistics Volume 35, Number 1
Association for Computational Linguistics
(COLING-ACL?06), pages 353?360, Sydney.
Inkpen, Diana. 2003. Building a Lexical
Knowledge-Base of Near-Synonym Differences.
Ph.D. thesis, University of Toronto.
Jackendoff, Ray. 1997. The Architecture of the
Language Faculty. MIT Press, Cambridge,
MA.
Katz, Graham and Eugenie Giesbrecht. 2006.
Automatic identification of
non-compositional multi-word
expressions using Latent Semantic
Analysis. In Proceedings of the ACL?06
Workshop on Multiword Expressions:
Identifying and Exploiting Underlying
Properties, pages 12?19, Sydney.
Katz, Jerrold J. 1973. Compositionality,
idiomaticity, and lexical substitution. In
S. Anderson and P. Kiparsky, editors, A
Festschrift for Morris Halle. Holt, Rinehart
and Winston, New York, pages 357?376.
Kearns, Kate. 2002. Light verbs in English.
Manuscript. Available at www.ling.
canterbury.ac.nz/people/kearns.html.
Kirkpatrick, E. M. and C. M. Schwarz,
editors. 1982. Chambers Idioms. W & R
Chambers Ltd, Edinburgh.
Krenn, Brigitte and Stefan Evert. 2001. Can
we do better than frequency? A case study
on extracting PP-verb collocations. In
Proceedings of the ACL?01 Workshop on
Collocations, pages 39?46, Toulouse.
Kyto?, Merja. 1999. Collocational and
idiomatic aspects of verbs in Early Modern
English. In L. J. Brinton and M. Akimoto.
Collocational and Idiomatic Aspects of
Composite Predicates in the History of
English. John Benjamins Publishing
Company, Amsterdam, pages 167?206.
Lapata, Mirella and Alex Lascarides. 2003.
Detecting novel compounds: The role of
distributional evidence. In Proceedings of
the 11th Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 235?242, Budapest.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics and the 36th
Annual Meeting of the Association for
Computational Linguistics
(COLING-ACL?98), pages 768?774,
Montreal.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL?99), pages 317?324, College Park,
Maryland.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum
of compositionality in phrasal verbs.
In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 73?80, Sapporo.
Melamed, I. Dan. 1997a. Automatic
discovery of non-compositional
compounds in parallel data. In Proceedings
of the 2nd Conference on Empirical Methods in
Natural Language Processing (EMNLP?97),
pages 97?108, Providence, RI.
Melamed, I. Dan. 1997b. Measuring semantic
entropy. In Proceedings of the ACL-SIGLEX
Workshop on Tagging Text with Lexical
Semantics: Why, What and How,
pages 41?46, Washington, DC.
Mohammad, Saif and Graeme Hirst.
Distributional measures as proxies for
semantic relatedness. Submitted.
Moon, Rosamund. 1998. Fixed Expressions and
Idioms in English: A Corpus-Based Approach.
Oxford University Press.
Newman, John and Sally Rice. 2004. Patterns
of usage for English SIT, STAND, and LIE:
A cognitively inspired exploration in
corpus linguistics. Cognitive Linguistics,
15(3):351?396.
Nicolas, Tim. 1995. Semantics of idiom
modification. In Everaert et al, editors,
Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 233?252.
Nunberg, Geoffrey, Ivan A. Sag, and Thomas
Wasow. 1994. Idioms. Language,
70(3):491?538.
Odijk, Jan. 2004. A proposed standard for the
lexical representations of idioms. In
Proceedings of Euralex?04, pages 153?164,
Lorient.
Ogden, Charles Kay. 1968. Basic English,
International Second Language. Harcourt,
Brace, and World, New York.
Patrick, Jon and Jeremy Fletcher. 2005.
Classifying verb-particle constructions
by verb arguments. In Proceedings of
the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 200?209,
Colcheter.
Pauwels, Paul. 2000. Put, Set, Lay and Place: A
Cognitive Linguistic Approach to Verbal
Meaning. LINCOM EUROPA, Munich.
102
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
R 2004. Notes on R: A Programming
Environment for Data Analysis and Graphics.
Available at www.r-project.org.
Resnik, Philip. 1999. Semantic similarity in a
taxonomy: An information-based measure
and its application to problems of
ambiguity in natural language. Journal of
Artificial Intelligence Research (JAIR),
(11):95?130.
Riehemann, Susanne. 2001. A Constructional
Approach to Idioms and Word Formation.
Ph.D. thesis, Stanford University.
Ritz, Julia and Ulrich Heid. 2006. Extraction
tools for collocations and their
morphosyntactic specificities. In
Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC?06), pages 1925?30,
Genoa.
Rohde, Douglas L. T. 2004. TGrep2 User
Manual. Available at http://tedlab.mit.
edu/?dr/Tgrep2.
Sag, Ivan A., Timothy Baldwin, Francis
Bond, Ann Copestake, and Dan Flickinger.
2002. Multiword expressions: A pain in the
neck for NLP. In Proceedings of the 3rd
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing?02), pages 1?15, Mexico City.
Schenk, Andre?. 1995. The syntactic behavior
of idioms. In Everaert et al, editors, Idioms:
Structural and Psychological Perspectives.
LEA, Mahwah, NJ, chapter 10,
pages 253?271.
Seaton, Maggie and Alison Macaulay,
editors. 2002. Collins COBUILD Idioms
Dictionary. HarperCollins Publishers,
second edition, New York.
Smadja, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143?177.
Tanabe, Harumi. 1999. Composite predicates
and phrasal verbs in The Paston Letters. In
L. J. Brinton and M. Akimoto. Collocational
and Idiomatic Aspects of Composite Predicates
in the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 97?132.
Uchiyama, Kiyoko, Timothy Baldwin, and
Shun Ishizaki. 2005. Disambiguating
Japanese compound verbs. Computer
Speech and Language, 19:497?512.
Van de Cruys, Tim and Begon?a
Villada Moiro?n. 2007. Semantics-based
multiword expression extraction. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 25?32, Prague.
Venkatapathy, Sriram and Aravid Joshi. 2005.
Measuring the relative compositionality of
verb-noun (V-N) collocations by
integrating features. In Proceedings of Joint
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing (HLT-EMNLP?05),
pages 899?906, Vancouver.
Villada Moiro?n, Begon?a and Jo?rg Tiedemann.
2006. Identifying idiomatic expressions
using automatic word-alignment. In
Proceedings of the EACL?06 Workshop on
Multiword Expressions in a Multilingual
Context, pages 33?40, Trento.
Villavicencio, Aline, Ann Copestake,
Benjamin Waldron, and Fabre Lambeau.
2004. Lexical encoding of multiword
expressions. In Proceedings of the 2nd ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 80?87,
Barcelona.
Wermter, Joachim and Udo Hahn. 2005.
Paradigmatic modifiability statistics for
the extraction of complex multi-word
terms. In Proceedings of Joint Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing
(HLT-EMNLP?05), pages 843?850,
Vancouver.
Widdows, Dominic and Beate Dorow. 2005.
Automatic extraction of idioms using
graph analysis and asymmetric
lexicosyntactic patterns. In Proceedings of
ACL?05 Workshop on Deep Lexical
Acquisition, pages 48?56, Ann Arbor, MI.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1(6):80?83.
103

Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45?53,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Classifying Particle Semantics in English Verb-Particle Constructions
Paul Cook
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
suzanne@cs.toronto.edu
Abstract
Previous computational work on learning
the semantic properties of verb-particle
constructions (VPCs) has focused on their
compositionality, and has left unaddressed
the issue of which meaning of the compo-
nent words is being used in a given VPC.
We develop a feature space for use in clas-
sification of the sense contributed by the
particle in a VPC, and test this on VPCs
using the particle up. The features that
capture linguistic properties of VPCs that
are relevant to the semantics of the par-
ticle outperform linguistically uninformed
word co-occurrence features in our exper-
iments on unseen test VPCs.
1 Introduction
A challenge in learning the semantics of mul-
tiword expressions (MWEs) is their varying de-
grees of compositionality?the contribution of
each component word to the overall semantics
of the expression. MWEs fall on a range from
fully compositional (i.e., each component con-
tributes its meaning, as in frying pan) to non-
compositional or idiomatic (as in hit the roof ). Be-
cause of this variation, researchers have explored
automatic methods for learning whether, or the de-
gree to which, an MWE is compositional (e.g.,
Lin, 1999; Bannard et al, 2003; McCarthy et al,
2003; Fazly et al, 2005).
However, such work leaves unaddressed the ba-
sic issue of which of the possible meanings of a
component word is contributed when the MWE is
(at least partly) compositional. Words are notori-
ously ambiguous, so that even if it can be deter-
mined that an MWE is compositional, its meaning
is still unknown, since the actual semantic contri-
bution of the components is yet to be determined.
We address this problem in the domain of verb-
particle constructions (VPCs) in English, a rich
source of MWEs.
VPCs combine a verb with any of a finite set
of particles, as in jump up, figure out, or give in.
Particles such as up, out, or in, with their literal
meaning based in physical spatial relations, show
a variety of metaphorical and aspectual meaning
extensions, as exemplified here for the particle up:
(1a) The sun just came up. [vertical spatial movement]
(1b) She walked up to him. [movement toward a goal]
(1c) Drink up your juice! [completion]
(1d) He curled up into a ball. [reflexive movement]
Cognitive linguistic analysis, as in Lindner (1981),
can provide the basis for elaborating this type of
semantic variation.
Given such a sense inventory for a particle,
our goal is to automatically determine its mean-
ing when used with a given verb in a VPC. We
classify VPCs according to their particle sense,
using statistical features that capture the seman-
tic and syntactic properties of verbs and particles.
We contrast these with simple word co-occurrence
features, which are often used to indicate the se-
mantics of a target word. In our experiments, we
focus on VPCs using the particle up because it is
highly frequent and has a wide range of meanings.
However, it is worth emphasizing that our feature
space draws on general properties of VPCs, and is
not specific to this particle.
A VPC may be ambiguous, with its particle oc-
curring in more than one sense; in contrast to (1a),
come up may use up in a goal-oriented sense as in
45
The deadline is coming up. While our long-term
goal is token classification (disambiguation) of a
VPC in context, following other work on VPCs
(e.g., Bannard et al, 2003; McCarthy et al, 2003),
we begin here with the task of type classification.
Given our use of features which capture the statis-
tical behaviour relevant to a VPC across a corpus,
we assume that the outcome of type classification
yields the predominant sense of the particle in the
VPC. Predominant sense identification is a useful
component of sense disambiguation of word to-
kens (McCarthy et al, 2004), and we presume our
VPC type classification work will form the basis
for later token disambiguation.
Section 2 continues the paper with a discussion
of the features we developed for particle sense
classification. Section 3 first presents some brief
cognitive linguistic background, followed by the
sense classes of up used in our experiments. Sec-
tions 4 and 5 discuss our experimental set-up and
results, Section 6 related work, and Section 7 our
conclusions.
2 Features Used in Classification
The following subsections describe the two sets of
features we investigated. The linguistic features
are motivated by specific semantic and syntactic
properties of verbs and VPCs, while the word co-
occurrence features are more general.
2.1 Linguistically Motivated Features
2.1.1 Slot Features
We hypothesize that the semantic contribution
of a particle when combined with a given verb is
related to the semantics of that verb. That is, the
particle contributes the same meaning when com-
bining with any of a semantic class of verbs.1 For
example, the VPCs drink up, eat up and gobble up
all draw on the completion sense of up; the VPCs
puff out, spread out and stretch out all draw on the
extension sense of out. The prevalence of these
patterns suggests that features which have been
shown to be effective for the semantic classifica-
tion of verbs may be useful for our task.
We adopt simple syntactic ?slot? features which
have been successfully used in automatic seman-
tic classification of verbs (Joanis and Stevenson,
1Villavicencio (2005) observes that verbs from a seman-
tic class will form VPCs with similar sets of particles. Here
we are hypothesizing further that VPCs formed from verbs
of a semantic class draw on the same meaning of the given
particle.
2003). The features are motivated by the fact
that semantic properties of a verb are reflected
in the syntactic expression of the participants in
the event the verb describes. The slot features
encode the relative frequencies of the syntactic
slots?subject, direct and indirect object, object of
a preposition?that the arguments and adjuncts of
a verb appear in. We calculate the slot features
over three contexts: all uses of a verb; all uses of
the verb in a VPC with the target particle (up in our
experiments); all uses of the verb in a VPC with
any of a set of high frequency particles (to capture
its semantics when used in VPCs in general).
2.1.2 Particle Features
Two types of features are motivated by proper-
ties specific to the semantics and syntax of par-
ticles and VPCs. First, Wurmbrand (2000) notes
that compositional particle verbs in German (a
somewhat related phenomenon to English VPCs)
allow the replacement of their particle with seman-
tically similar particles. We extend this idea, hy-
pothesizing that when a verb combines with a par-
ticle such as up in a particular sense, the pattern
of usage of that verb in VPCs using all other par-
ticles may be indicative of the sense of the target
particle (in this case up) when combined with that
verb. To reflect this observation, we count the rel-
ative frequency of any occurrence of the verb used
in a VPC with each of a set of high frequency par-
ticles.
Second, one of the striking syntactic properties
of VPCs is that they can often occur in either the
joined configuration (2a) or the split configuration
(2b):
(2a) Drink up your milk! He walked out quickly.
(2b) Drink your milk up! He walked quickly out.
Bolinger (1971) notes that the joined construction
may be more favoured when the sense of the par-
ticle is not literal. To encode this, we calculate the
relative frequency of the verb co-occurring with
the particle up with each of   ?  words between
the verb and up, reflecting varying degrees of verb-
particle separation.
2.2 Word Co-occurrence Features
We also explore the use of general context fea-
tures, in the form of word co-occurrence frequency
vectors, which have been used in numerous ap-
proaches to determining the semantics of a target
46
word. Note, however, that unlike the task of word
sense disambiguation, which examines the context
of a target word token to be disambiguated, here
we are looking at aggregate contexts across all in-
stances of a target VPC, in order to perform type
classification.
We adopt very simple word co-occurrence fea-
tures (WCFs), calculated as the frequency of any
(non-stoplist) word within a certain window left
and right of the target. We noted above that the
target particle semantics is related both to the se-
mantics of the verb it co-occurs with, and to the
occurrence of the verb across VPCs with different
particles. Thus we not only calculate the WCFs of
the target VPC (a given verb used with the parti-
cle up), but also the WCFs of the verb itself, and
the verb used in a VPC with any of the high fre-
quency particles. These WCFs give us a very gen-
eral means for determining semantics, whose per-
formance we can contrast with our linguistic fea-
tures.
3 Particle Semantics and Sense Classes
We give some brief background on cognitive
grammar and its relation to particle semantics, and
then turn to the semantic analysis of up that we
draw on as the basis for the sense classes in our
experiments.
3.1 Cognitive Grammar and Schemas
Some linguistic studies consider many VPCs to be
idiomatic, but do not give a detailed account of
the semantic similarities between them (Bolinger,
1971; Fraser, 1976; Jackendoff, 2002). In con-
trast, work in cognitive linguistics has claimed that
many so-called idiomatic expressions draw on the
compositional contribution of (at least some of)
their components (Lindner, 1981; Morgan, 1997;
Hampe, 2000). In cognitive grammar (Langacker,
1987), non-spatial concepts are represented as spa-
tial relations. Key terms from this framework are:
Trajector (TR) The object which is conceptually
foregrounded.
Landmark (LM) The object against which the
TR is foregrounded.
Schema An abstract conceptualization of an ex-
perience. Here we focus on schemas depict-
ing a TR, LM and their relationship in both
the initial configuration and the final config-
uration communicated by some expression.
TR
TR
LM LM
Initial Final
Figure 1: Schema for Vertical up.
The semantic contribution of a particle in a VPC
corresponds to a schema. For example, in sen-
tence (3), the TR is the balloon and the LM is the
ground the balloon is moving away from.
(3) The balloon floated up.
The schema describing the semantic contribution
of the particle in the above sentence is shown
in Figure 1, which illustrates the relationship be-
tween the TR and LM in the initial and final con-
figurations.
3.2 The Senses of up
Lindner (1981) identifies a set of schemas for each
of the particles up and out, and groups VPCs ac-
cording to which schema is contributed by their
particle. Here we describe the four senses of up
identified by Lindner.
3.2.1 Vertical up (Vert-up)
In this schema (shown above in Figure 1), the
TR moves away from the LM in the direction of
increase along a vertically oriented axis. This in-
cludes prototypical spatial upward movement such
as that in sentence (3), as well as upward move-
ment along an abstract vertical axis as in sen-
tence (4).
(4) The price of gas jumped up.
In Lindner?s analysis, this sense also includes ex-
tensions of upward movement where a vertical
path or posture is still salient. Note that in some of
these senses, the notion of verticality is metaphor-
ical; the contribution of such senses to a VPC may
not be considered compositional in a traditional
analysis. Some of the most common sense exten-
sions are given below, with a brief justification as
to why verticality is still salient.
47
Initial
TR
LM = goal LM = goal
TR
Final
Figure 2: Schema for Goal-Oriented up.
Up as a path into perceptual field. Spatially
high objects are generally easier to perceive.
Examples: show up, spring up, whip up.
Up as a path into mental field. Here up encodes
a path for mental as opposed to physical objects.
Examples: dream up, dredge up, think up.
Up as a path into a state of activity. Activity is
prototypically associated with an erect posture.
Examples: get up, set up, start up.
3.2.2 Goal-Oriented up (Goal-up)
Here the TR approaches a goal LM; movement
is not necessarily vertical (see Figure 2). Proto-
typical examples are walk up and march up. This
category also includes extensions into the social
domain (kiss up and suck up), as well as exten-
sions into the domain of time (come up and move
up), as in:
(5a) The intern kissed up to his boss.
(5b) The deadline is coming up quickly.
3.2.3 Completive up (Cmpl-up)
Cmpl-up is a sub-sense of Goal-up in which the
goal represents an action being done to comple-
tion. This sense shares its schema with Goal-up
(Figure 2), but it is considered as a separate sense
since it corresponds to uses of up as an aspectual
marker. Examples of Cmpl-up are: clean up, drink
up, eat up, finish up and study up.
3.2.4 Reflexive up (Refl-up)
Reflexive up is a sub-sense of Goal-up in which
the sub-parts of the TR are approaching each other.
The schema for Refl-up is shown in Figure 3; it is
unique in that the TR and LM are the same object.
Examples of Refl-up are: bottle up, connect up,
couple up, curl up and roll up.
LM = TR LM = TR
Initial Final
Figure 3: Schema for Reflexive up.
Vertical up Goal-Oriented up
Completive up
Reflexive up
Figure 4: Simplified schematic network for up.
3.3 The Sense Classes for Our Study
Adopting a cognitive linguistic perspective, we as-
sume that all uses of a particle make some compo-
sitional contribution of meaning to a VPC. In this
work, we classify target VPCs according to which
of the above senses of up is contributed to the ex-
pression. For example, the expressions jump up
and pick up are designated as being in the class
Vert-up since up in these VPCs has the vertical
sense, while clean up and drink up are designated
as being in the class Cmpl-up since up here has
the completive sense. The relations among the
senses of up can be shown in a ?schematic net-
work? (Langacker, 1987). Figure 4 shows a sim-
plification of such a network in which we connect
more similar senses with shorter edges. This type
of analysis allows us to alter the granularity of our
classification in a linguistically motivated fashion
by combining closely related senses. Thus we can
explore the effect of different sense granularities
on classification.
4 Materials and Methods
4.1 Experimental Expressions
We created a list of English VPCs using up, based
on a list of VPCs made available by McIntyre
(2001) and a list of VPCs compiled by two human
judges. The judges then filtered this list to include
only VPCs which they both agreed were valid, re-
sulting in a final list of 389 VPCs. From this list,
training, verification and test sets of sixty VPCs
each are randomly selected. Note that the expense
of manually annotating the data (as described be-
low) prevents us from using larger datasets in this
initial investigation. The experimental sets are
48
chosen such that each includes the same propor-
tion of verbs across three frequency bands, so that
the sets do not differ in frequency distribution of
the verbs. (We use frequency of the verbs, rather
than the VPCs, since many of our features are
based on the verb of the expression, and moreover,
VPC frequency is approximate.) The verification
data is used in exploration of the feature space and
selection of final features to use in testing; the test
set is held out for final testing of the classifiers.
Each VPC in each dataset is annotated by the
two human judges according to which of the four
senses of up identified in Section 3.2 is contributed
to the VPC. As noted in Section 1, VPCs may
be ambiguous with respect to their particle sense.
Since our task here is type classification, the
judges identify the particle sense of a VPC in its
predominant usage, in their assessment. The ob-
served inter-annotator agreement is      for each
dataset. The unweighted observed kappa scores
are
  
,
  
	
and      , for the training, verifica-
tion and test sets respectively.
4.2 Calculation of the Features
We extract our features from the 100M word
British National Corpus (BNC, Burnard, 2000).
VPCs are identified using a simple heuristic based
on part-of-speech tags, similar to one technique
used by Baldwin (2005). A use of a verb is con-
sidered a VPC if it occurs with a particle (tagged
AVP) within a six word window to the right. Over
a random sample of 113 VPCs thus extracted, we
found 88% to be true VPCs, somewhat below the
performance of Baldwin?s (2005) best extraction
method, indicating potential room for improve-
ment.
The slot and particle features are calculated us-
ing a modified version of the ExtractVerb software
provided by Joanis and Stevenson (2003), which
runs over the BNC pre-processed using Abney?s
(1991) Cass chunker.
To compute the word co-occurrence features
(WCFs), we first determine the relative frequency
of all words which occur within a five word win-
dow left and right of any of the target expressions
in the training data. From this list we eliminate
the most frequent 1% of words as a stoplist and
then use the next  most frequent words as ?fea-
ture words?. For each ?feature word?, we then cal-
culate its relative frequency of occurrence within
the same five word window of the target expres-
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up 1 1 3
Cmpl-up 20 23 22
Refl-up 15 3 8
Table 1: Frequency of items in each sense class.
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up  21 24 25
Cmpl-up
Refl-up 15 3 8
Table 2: Frequency of items in each class for the
3-way task.
sions in all datasets. We use      and      
to create feature sets WCF  and WCF  respec-
tively.
4.3 Experimental Classes
Table 1 shows the distribution of senses in each
dataset. Each of the training and verification sets
has only one VPC corresponding to Goal-up. Re-
call that Goal-up shares a schema with Cmpl-up,
and is therefore very close to it in meaning, as in-
dicated spatially in Figure 4. We therefore merge
Goal-up and Cmpl-up into a single sense, to pro-
vide more balanced classes.
Since we want to see how our features per-
form on differing granularities of sense classes, we
run each experiment as both a 3-way and 2-way
classification task. In the 3-way task, the sense
classes correspond to the meanings Vert-up, Goal-
up merged with Cmpl-up (as noted above), and
Refl-up, as shown in Table 2. In the 2-way task, we
further merge the classes corresponding to Goal-
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up  36 27 33
Cmpl-up 
Refl-up
Table 3: Frequency of items in each class for the
2-way task.
49
up/Cmpl-up with that of Refl-up, as shown in Ta-
ble 3. We choose to merge these classes because
(as illustrated in Figure 4) Refl-up is a sub-sense of
Goal-up, and moreover, all three of these senses
contrast with Vert-up, in which increase along a
vertical axis is the salient property. It is worth em-
phasizing that the 2-way task is not simply a clas-
sification between literal and non-literal up?Vert-
up includes extensions of up in which the increase
along a vertical axis is metaphorical.
4.4 Evaluation Metrics and Classifier
Software
The variation in the frequency of the sense classes
of up across the datasets makes the true distri-
bution of the classes difficult to estimate. Fur-
thermore, there is no obvious informed baseline
for this task. Therefore, we make the assumption
that the true distribution of the classes is uniform,
and use the chance accuracy   as the baseline
(where  is the number of classes?in our exper-
iments, either  or  ). Accordingly, our measure
of classification accuracy should weight each class
evenly. Therefore, we report the average per class
accuracy, which gives equal weight to each class.
For classification we use LIBSVM (Chang and
Lin, 2001), an implementation of a support-vector
machine. We set the input parameters, cost
and gamma, using 10-fold cross-validation on the
training data. In addition, we assign a weight of
 
	 

 
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Pulling their Weight: Exploiting Syntactic Forms for the Automatic
Identification of Idiomatic Expressions in Context
Paul Cook and Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
fpcook,afsaneh,suzanneg@cs.toronto.edu
Abstract
Much work on idioms has focused on type
identification, i.e., determining whether a se-
quence of words can form an idiomatic ex-
pression. Since an idiom type often has a
literal interpretation as well, token classifi-
cation of potential idioms in context is criti-
cal for NLP. We explore the use of informa-
tive prior knowledge about the overall syn-
tactic behaviour of a potentially-idiomatic
expression (type-based knowledge) to de-
termine whether an instance of the expres-
sion is used idiomatically or literally (token-
based knowledge). We develop unsuper-
vised methods for the task, and show that
their performance is comparable to that of
state-of-the-art supervised techniques.
1 Introduction
Identification of multiword expressions (MWEs),
such as car park, make a decision, and kick the
bucket, is extremely important for accurate natural
language processing (NLP) (Sag et al, 2002). Most
MWEs need to be treated as single units of mean-
ing, e.g., make a decision roughly means ?decide?.
Nonetheless, the components of an MWE can be
separated, making it hard for an NLP system to iden-
tify the expression as a whole. Many researchers
have recently developed methods for the automatic
acquisition of various properties of MWEs from cor-
pora (Lin, 1999; Krenn and Evert, 2001; Baldwin et
al., 2003; McCarthy et al, 2003; Venkatapathy and
Joshi, 2005; Villada Moiro?n and Tiedemann, 2006;
Fazly and Stevenson, 2006). These studies look
into properties, such as the collocational behaviour
of MWEs, their semantic non-compositionality, and
their lexicosyntactic fixedness, in order to distin-
guish them from similar-on-the-surface literal com-
binations.
Most of these methods have been aimed at rec-
ognizing MWE types; less attention has been paid
to the identification of instances (tokens) of MWEs
in context. For example, most such techniques (if
successful) would identify make a face as a poten-
tial MWE. This expression is, however, ambiguous
between an idiom, as in The little girl made a funny
face at her mother, and a literal combination, as in
She made a face on the snowman using a carrot and
two buttons. Despite the common perception that
phrases that can be idioms are mainly used in their
idiomatic sense, our analysis of 60 idioms has shown
otherwise. We found that close to half of these id-
ioms also have a clear literal meaning; and of the ex-
pressions with a literal meaning, on average around
40% of their usages are literal. Distinguishing token
phrases as MWEs or literal combinations of words is
thus essential for NLP applications that require the
identification of multiword semantic units, such as
semantic parsing and machine translation.
Recent studies addressing MWE token classifi-
cation mainly perform the task as one of word
sense disambiguation, and draw on the local con-
text of an expression to disambiguate it. Such
techniques either do not use any information re-
garding the linguistic properties of MWEs (Birke
and Sarkar, 2006), or mainly focus on their non-
compositionality (Katz and Giesbrecht, 2006). Pre-
41
vious work on the identification of MWE types,
however, has found other properties of MWEs, such
as their syntactic fixedness, to be relevant to their
identification (Evert et al, 2004; Fazly and Steven-
son, 2006). In this paper, we propose techniques that
draw on this property to classify individual tokens of
a potentially idiomatic phrase as literal or idiomatic.
We also put forward classification techniques that
combine such information with evidence from the
local context of an MWE.
We explore the hypothesis that informative prior
knowledge about the overall syntactic behaviour of
an idiomatic expression (type-based knowledge) can
be used to determine whether an instance of the
expression is used literally or idiomatically (token-
based knowledge). Based on this hypothesis, we de-
velop unsupervised methods for token classification,
and show that their performance is comparable to
that of a standard supervised method.
Many verbs can be combined with one or more of
their arguments to form MWEs (Cowie et al, 1983;
Fellbaum, 2002). Here, we focus on a broadly doc-
umented class of idiomatic MWEs that are formed
from the combination of a verb with a noun in its di-
rect object position, as in make a face. In the rest
of the paper, we refer to these verb+noun combi-
nations, which are potentially idiomatic, as VNCs.
In Section 2, we propose unsupervised methods that
classify a VNC token as an idiomatic or literal usage.
Section 3 describes our experimental setup, includ-
ing experimental expressions and their annotation.
In Section 4, we present a detailed discussion of our
results. Section 5 compares our work with similar
previous studies, and Section 6 concludes the paper.
2 Unsupervised Idiom Identification
We first explain an important linguistic property at-
tributed to idioms?that is, their syntactic fixedness
(Section 2.1). We then propose unsupervised meth-
ods that draw on this property to automatically dis-
tinguish between idiomatic and literal usages of an
expression (Section 2.2).
2.1 Syntactic Fixedness and Canonical Forms
Idioms tend to be somewhat fixed with respect to
the syntactic configurations in which they occur
(Nunberg et al, 1994). For example, pull one?s
weight tends to mainly appear in this form when
used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used
with a literal meaning. In their work on automati-
cally identifying idiom types, Fazly and Stevenson
(2006)?henceforth FS06?show that an idiomatic
VNC tends to have one (or at most a small number
of) canonical form(s), which are its most preferred
syntactic patterns. The preferred patterns can vary
across different idiom types, and can involve a num-
ber of syntactic properties: the voice of the verb (ac-
tive or passive), the determiner introducing the noun
(the, one?s, etc.), and the number of the noun (singu-
lar or plural). For example, while pull one?s weight
has only one canonical form, hold fire and hold one?s
fire are two canonical forms of the same idiom, as
listed in an idiom dictionary (Seaton and Macaulay,
2002).
In our work, we assume that in most cases, id-
iomatic usages of an expression tend to occur in a
small number of canonical form(s) for that idiom.
We also assume that, in contrast, the literal usages
of an expression are less syntactically restricted, and
are expressed in a greater variety of patterns. Be-
cause of their relative unrestrictiveness, literal us-
ages may occur in a canonical idiomatic form for
that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative
syntactic patterns for the expression, which we refer
to as the non-canonical forms of the idiom, are more
likely to be literal. Drawing on these assumptions,
we develop three unsupervised methods that deter-
mine, for each VNC token in context, whether it has
an idiomatic or a literal interpretation.
2.2 Statistical Methods
The following paragraphs elaborate on our proposed
methods for identifying the idiomatic and literal us-
ages of a VNC: the CForm method that uses knowl-
edge of canonical forms only, and two Diff methods
that draw on further contextual evidence as well. All
three methods draw on our assumptions described
above, that usages in the canonical form for an id-
iom are more likely to be idiomatic, and those in
other forms are more likely to be literal. Thus, for
all three methods, we need access to the canonical
form of the idiom. Since we want our token iden-
tification methods to be unsupervised, we adopt the
42
unsupervised statistical method of FS06 for finding
canonical forms for an idiomatic VNC. This method
determines the canonical forms of an expression to
be those forms whose frequency is much higher than
the average frequency of all its forms.
CForm: The underlying assumption of this
method is that information about the canonical
form(s) of an idiom type is extremely informative
in classifying the meaning of its individual instances
(tokens) as literal or idiomatic. Our CForm classi-
fies a token as idiomatic if it occurs in the automat-
ically determined canonical form(s) for that expres-
sion, and as literal otherwise.
Di : Our two Di methods combine local con-
text information with knowledge about the canon-
ical forms of an idiom type to determine if its to-
ken usages are literal or idiomatic. In developing
these methods, we adopt a distributional approach
to meaning, where the meaning of an expression is
approximated by the words with which it co-occurs
(Firth, 1957). Although there may be fine-grained
differences in meaning across the idiomatic usages
of an expression, as well as across its literal usages,
we assume that the idiomatic and literal usages cor-
respond to two coarse-grained senses of the expres-
sion. Since we further assume these two groups
of usages will have more in common semantically
within each group than between the two groups, we
expect that literal and idiomatic usages of an ex-
pression will typically occur with different sets of
words. We will refer then to each of the literal and
idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that
each may have multiple fine-grained senses. Clearly,
the success of our method depends on the extent to
which these assumptions hold.
We estimate the meaning of a set of usages of an
expression e as a word frequency vector ~v
e
where
each dimension i of ~v
e
is the frequency with which
e co-occurs with word i across the usages of e. We
similarly estimate the meaning of a single token of
an expression t as a vector ~v
t
capturing that usage.
To determine if an instance of an expression is literal
or idiomatic, we compare its co-occurrence vector to
the co-occurrence vectors representing each of the
literal and idiomatic meanings of the expression. We
use a standard measure of distributional similarity,
cosine, to compare co-occurrence vectors.
In supervised approaches, such as that of Katz and
Giesbrecht (2006), co-occurrence vectors for literal
and idiomatic meanings are formed from manually-
annotated training data. Here, we propose unsuper-
vised methods for estimating these vectors. We use
one way of estimating the idiomatic meaning of an
expression, and two ways for estimating its literal
meaning, yielding two methods for token classifica-
tion.
Our first Diff method draws further on our expec-
tation that canonical forms are more likely idiomatic
usages, and non-canonical forms are more likely lit-
eral usages. We estimate the idiomatic meaning of
an expression by building a co-occurrence vector,
~v
I -CF
, for all uses of the expression in its auto-
matically determined canonical form(s). Since we
hypothesize that idiomatic usages of an expression
tend to occur in its canonical form, we expect these
co-occurrence vectors to be largely representative of
the idiomatic usage of the expression. We similarly
estimate the literal meaning by constructing a co-
occurrence vector, ~v
L-NCF
, of all uses of the expres-
sion in its non-canonical forms. We use the term
Di
I-CF;L-NCF
to refer to this method.
Our second Diff method also uses the vector
~v
I -CF
to estimate the idiomatic meaning of an ex-
pression. However, this approach follows that of
Katz and Giesbrecht (2006) in assuming that literal
meanings are compositional. The literal meaning of
an expression is thus estimated by composing (sum-
ming and then normalizing) the co-occurrence vec-
tors for its component words. The resulting vec-
tor is referred to as ~v
L-Comp
, and this method as
Di
I-CF;L-Comp
.
For both Diff methods, if the meaning of
an instance of an expression is determined to
be more similar to its idiomatic meaning (e.g.,
cosine (~v
t
; ~v
I-CF
) > cosine (~v
t
; ~v
L-NCF
)), then
we label it as an idiomatic usage. Otherwise, it is
labeled as literal.1
1We also performed experiments using a KNN classifier
in which the co-occurrence vector for a token was compared
against the co-occurrence vectors for the canonical and non-
canonical forms of that expression, which were assumed to
be idiomatic and literal usages respectively. However, perfor-
mance was generally worse using this method.
43
Note that all three of our proposed techniques for
token identification depend on how accurately the
canonical forms of an expression can be acquired.
FS06?s canonical form acquisition technique, which
we use here, works well if the idiomatic usage of
a VNC is sufficiently frequent compared to its lit-
eral usage. In our experiments, we examine the
performance of our proposed classification methods
for VNCs with different proportions of idiomatic-to-
literal usages.
3 Experimental Setup
3.1 Experimental Expressions and Annotation
We use data provided by FS06, which consists of a
list of VNCs and their canonical forms. From this
data, we discarded expressions whose frequency in
the British National Corpus2 (BNC) is lower than
20, in an effort to make sure that there would be lit-
eral and idiomatic usages of each expression. The
frequency cut-off further ensures an accurate esti-
mate of the vectors representing each of the lit-
eral and idiomatic meanings of the expression. We
also discarded expressions that were not found in at
least one of two dictionaries of idioms (Seaton and
Macaulay, 2002; Cowie et al, 1983). This process
resulted in the selection of 60 candidate expressions.
For each of these 60 expressions, 100 sentences
containing its usage were randomly selected from
the automatically parsed BNC (Collins, 1999), using
the automatic VNC identification method described
by FS06. For an expression which occurs less than
100 times in the BNC, all of its usages were ex-
tracted. Our primary judge, a native English speaker
and an author of this paper, then annotated each use
of each candidate expression as one of literal, id-
iomatic, or unknown. When annotating a token, the
judge had access to only the sentence in which it oc-
curred, and not the surrounding sentences. If this
context was insufficient to determine the class of the
expression, the judge assigned the unknown label.
Idiomaticity is not a binary property, rather it is
known to fall on a continuum from completely se-
mantically transparent, or literal, to entirely opaque,
or idiomatic. The human annotators were required
to pick the label, literal or idiomatic, that best fit the
2http://www.natcorp.ox.ac.uk
usage in their judgment; they were not to use the un-
known label for intermediate cases. Figurative ex-
tensions of literal meanings were classified as literal
if their overall meaning was judged to be fairly trans-
parent, as in You turn right when we hit the road at
the end of this track (taken from the BNC). Some-
times an idiomatic usage, such as had words in I
was in a bad mood, and he kept pestering me, so
we had words, is somewhat directly related to its
literal meaning, which is not the case for more se-
mantically opaque idioms such as hit the roof. The
above sentence was classified as idiomatic since the
idiomatic meaning is much more salient than the lit-
eral meaning.
Based on the primary judge?s annotations, we re-
moved expressions with fewer than 5 instances of
either of their literal or idiomatic meanings, leav-
ing 28 expressions. The remaining expressions were
then split into development (DEV) and test (TEST)
sets of 14 expressions each. The data was divided
such that DEV and TEST would be approximately
equal with respect to the frequency, and proportion
of idiomatic-to-literal usages, of their expressions.
Before consensus annotation, DEV and TEST con-
tained a total of 813 and 743 tokens, respectively.
A second human judge, also a native English-
speaking author of this paper, then annotated DEV
and TEST. The observed agreement and unweighted
kappa score on TEST were 76% and 0:62 respec-
tively. The judges discussed tokens on which they
disagreed to achieve a consensus annotation. Final
annotations were generated by removing tokens that
received the unknown label as the consensus anno-
tation, leaving DEV and TEST with a total of 573 and
607 tokens, and an average of 41 and 43 tokens per
expression, respectively.
3.2 Creation of Co-occurrence Vectors
We create co-occurrence vectors for each expression
in our study from counts in the BNC. We form co-
occurrence vectors for the following items.
 Each token instance of the target expression
 The target expression in its automatically deter-
mined canonical form(s)
 The target expression in its non-canonical
form(s)
44
 The verb in the target expression
 The noun in the target expression
The co-occurrence vectors measure the frequency
with which the above items co-occur with each of
1000 content bearing words in the same sentence.3
The content bearing words were chosen to be the
most frequent words in the BNC which are used as
a noun, verb, adjective, adverb, or determiner. Al-
though determiners are often in a typical stoplist, we
felt it would be beneficial to use them here. Deter-
miners have been shown to be very informative in
recognizing the idiomaticity of MWE types, as they
are incorporated in the patterns used to automati-
cally determine canonical forms (Fazly and Steven-
son, 2006).4
3.3 Evaluation and Baseline
Our baseline for comparison is that of always pre-
dicting an idiomatic label, the most frequent class
in our development data. We also compare our un-
supervised methods against the supervised method
proposed by Katz and Giesbrecht (2006). In this
study, co-occurrence vectors for the tokens were
formed from uses of a German idiom manually an-
notated as literal or idiomatic. Tokens were classi-
fied in a leave-one-out methodology using k-nearest
neighbours, with k = 1. We report results using this
method (1NN) as well as one which considers a to-
ken?s 5 nearest neighbours (5NN). In all cases, we
report the accuracy macro-averaged across the ex-
perimental expressions.
4 Experimental Results and Analysis
In Section 4.1, we discuss the overall performance
of our proposed unsupervised methods. Section 4.2
explores possible causes of the differences observed
in the performance of the methods. We examine
our estimated idiomatic and literal vectors, and com-
pare them with the actual vectors calculated from
3We also considered 10 and 20 word windows on either side
of the target expression, but experiments on development data
indicated that using the sentence as a window performed better.
4We employed singular value decomposition (Deerwester et
al., 1990) to reduce the dimensionality of the co-occurrence
vectors. This had a negative effect on the results, likely be-
cause information about determiners, which occur frequently
with many expressions, is lost in the dimensionality reduction.
Method %Acc (%RER)
Baseline 61.9 -
Unsupervised Di
I -CF ; L-Comp
67.8 (15.5)
Di
I -CF ; L-NCF
70.1 (21.5)
CForm 72.4 (27.6)
Supervised 1NN 72.4 (27.6)
5NN 76.2 (37.5)
Table 1: Macro-averaged accuracy (%Acc) and relative error
reduction (%RER) over TEST.
manually-annotated data. Results reported in Sec-
tions 4.1 and 4.2 are on TEST (results on DEV have
very similar trends). Section 4.3 then examines the
performance of the unsupervised methods on ex-
pressions with different proportions of idiomatic-to-
literal usages. This section presents results on TEST
and DEV combined, as explained below.
4.1 Overall Performance
Table 4.1 shows the macro-averaged accuracy on
TEST of our three unsupervised methods, as well as
that of the baseline and the two supervised methods
for comparison (see Section 3.3). The best super-
vised performance and the best unsupervised perfor-
mance are indicated in boldface. As the table shows,
all three unsupervised methods outperform the base-
line, confirming that the canonical forms of an ex-
pression, and local context, are both informative in
distinguishing literal and idiomatic instances of the
expression.
The table also shows that Di
I -CF ;L-NCF
per-
forms better than Di
I -CF ;L-Comp
. This suggests
that estimating the literal meaning of an expression
using the non-canonical forms is more accurate than
using the composed vector, ~v
L-Comp
. In Section 4.2
we find more evidence for this. Another interesting
observation is that CForm has the highest perfor-
mance (among unsupervised methods), very closely
followed by Di
I -CF ;L-NCF
. These results confirm
our hypothesis that canonical forms?which reflect
the overall behaviour of a VNC type?are strongly
informative about the class of a token, perhaps even
more so than the local context of the token. Im-
portantly, this is the case even though the canonical
forms that we use are imperfect knowledge obtained
automatically through an unsupervised method.
Our results using 1NN, 72:4%, are comparable
45
Vectors cosine Vectors cosine
~a
idm
and ~a
lit
.55
~v
I -CF
and ~a
lit
.70 ~v
I -CF
and ~a
idm
.90
~v
L-NCF
and ~a
lit
.80 ~v
L-NCF
and ~a
idm
.60
~v
L-Comp
and ~a
lit
.72 ~v
L-Comp
and ~a
idm
.76
Table 2: Average similarity between the actual vectors (~a) and
the estimated vectors (~v), for the idiomatic and literal meanings.
to those of Katz and Giesbrecht (2006) using this
method on their German data (72%). However, their
baseline is slightly lower than ours at 58%, and
they only report results for 1 expression with 67 in-
stances. Interestingly, our best unsupervised results
are in line with the results using 1NN and not sub-
stantially lower than the results using 5NN.
4.2 A Closer Look into the Estimated Vectors
In this section, we compare our estimated idiomatic
and literal vectors with the actual vectors for these
usages calculated from manually-annotated data.
Such a comparison helps explain some of the differ-
ences we observed in the performance of the meth-
ods. Table 4.2 shows the similarity between the esti-
mated and actual vectors representing the idiomatic
and literal meanings, averaged over the 14 TEST ex-
pressions. Actual vectors, referred to as ~a
idm
and
~a
lit
, are calculated over idiomatic and literal usages
of the expressions as determined by the human an-
notations. Estimated vectors, ~v
I -CF
, ~v
L-CF
, and
~v
L-Comp
, are calculated using our methods described
in Section 2.2.
For comparison purposes, the first row of Ta-
ble 4.2 shows the average similarity between the
actual idiomatic and literal vectors, ~a
idm
and ~a
lit
.
These vectors are expected to be very dissimilar,
hence the low average cosine between them serves
as a baseline for comparison. We now look into the
relative similarity of each estimated vector, ~v
I -CF
,
~v
L-CF
, ~v
L-Comp
, with these two vectors.
The second row of the table shows that, as de-
sired, our estimated idiomatic vector, ~v
I -CF
, is no-
tably more similar to the actual idiomatic vector than
to the actual literal vector. Also, ~v
L-NCF
is more
similar to the actual literal vector than to the actual
idiomatic vector (third row). Surprisingly, however,
~v
L-Comp
is somewhat similar to both actual literal
and idiomatic vectors (in fact it is slightly more simi-
lar to the latter). These results suggest that the vector
composed of the context vectors for the constituents
of an expression may not always be the best estimate
of the literal meaning of the expression.5 Given this
observation, the overall better-than-baseline perfor-
mance of Di
I-CF;L-Comp
might seem unjustified at
a first glance. However, we believe this performance
is mainly due to an accurate estimate of ~v
I -CF
.
4.3 Performance Based on Class Distribution
We further divide our 28 DEV and TEST expres-
sions according to their proportion of idiomatic-to-
literal usages, as determined by the human annota-
tors. In order to have a sufficient number of expres-
sions in each group, here we merge DEV and TEST
(we refer to the new set as DT). DT
I
high
contains
17 expressions with 65%?90% of their usages be-
ing idiomatic?i.e., their idiomatic usage is domi-
nant. DT
I
low
contains 11 expressions with 8%?58%
of their occurrences being idiomatic?i.e., their id-
iomatic usage is not dominant.
Table 4.3 shows the average accuracy of all the
methods on these two groups of expressions, with
the best performance on each group shown in bold-
face. On DT
I
high
, both Di
I -CF ;L-NCF
and CForm
outperform the baseline, with CForm having the
highest reduction in error rate. The two methods per-
form similarly to each other on DT
I
low
, though note
that the error reduction of CForm is more in line
with its performance on DT
I
high
. These results show
that even for VNCs whose idiomatic meaning is
not dominant?i.e., those in DT
I
low
?automatically-
acquired canonical forms can help with their token
classification.
An interesting observation in Table 4.3 is the
inconsistent performance of Di
I -CF ;L-Comp
: the
method has a very poor performance on DT
I
high
, but
outperforms the other two unsupervised methods on
DT
I
low
. As we noted earlier in Section 2.2, the more
frequent the idiomatic meaning of an expression,
the more reliable the acquired canonical forms for
that expression. Since the performance of CForm
and Di
I -CF ;L-NCF
depends highly on the accu-
racy of the automatically acquired canonical forms,
it is not surprising that these two methods perform
5This was also noted by Katz and Giesbrecht (2006) in their
second experiment.
46
Method DT
I
high
DT
I
low
Baseline 81.4 (-) 35.0 (-)
Unsuper- Di
I -CF ; L-Comp
73.1 (-44.6) 58.6 (36.3)
vised Di
I -CF ; L-NCF
82.3 (4.8) 52.7 (27.2)
CForm 84.7 (17.7) 53.4 (28.3)
Super- 1NN 78.3 (-16.7) 65.8 (47.4)
vised 5NN 82.3 (4.8) 72.4 (57.5)
Table 3: Macro-averaged accuracy over DEV and TEST, di-
vided according to the proportion of idiomatic-to-literal usages.
worse than Di
I -CF ;L-Comp
on VNCs whose id-
iomatic usage is not dominant.
The high performance of the supervised meth-
ods on DT
I
low
also confirms that the poorer perfor-
mance of the unsupervised methods on these VNCs
is likely due to the inaccuracy of the canonical forms
extracted for them. Interestingly, when canonical
forms can be extracted with a high accuracy (i.e.,
for VNCs in DT
I
high
) the performance of the unsu-
pervised methods is comparable to (or even slightly
better than) that of the best supervised method. One
possible way of improving the performance of unsu-
pervised methods is thus to develop more accurate
techniques for the automatic acquisition of canoni-
cal forms.
5 Related Work
Various properties of MWEs have been exploited
in developing automatic identification methods for
MWE types (Lin, 1999; Krenn and Evert, 2001; Fa-
zly and Stevenson, 2006). Much research has ad-
dressed the non-compositionality of MWEs as an
important property related to their idiomaticity, and
has used it in the classification of both MWE types
and tokens (Baldwin et al, 2003; McCarthy et al,
2003; Katz and Giesbrecht, 2006). We also make
use of this property in an MWE token classification
task, but in addition, we draw on other salient char-
acteristics of MWEs which have been previously
shown to be useful for their type classification (Evert
et al, 2004; Fazly and Stevenson, 2006).
The idiomatic/literal token classification methods
of Birke and Sarkar (2006) and Katz and Giesbrecht
(2006) rely primarily on the local context of a to-
ken, and fail to exploit specific linguistic properties
of non-literal language. Our results suggest that such
properties are often more informative than the local
context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher
(2005) distinguishes between compositional and
non-compositional English verb-particle con-
struction tokens. Their classifier incorporates
linguistically-motivated features, such as the degree
of separation between the verb and particle. Here,
we focus on a different class of English MWEs,
verb+noun combinations. Moreover, by making
a more direct use of their syntactic behaviour, we
develop unsupervised token classification methods
that perform well. The unsupervised token classifier
of Hashimoto et al (2006) uses manually-encoded
information about allowable and non-allowable
syntactic transformations of Japanese idioms?that
are roughly equivalent to our notions of canonical
and non-canonical forms. The rule-based classifier
of Uchiyama et al (2005) incorporates syntac-
tic information about Japanese compound verbs
(JCVs), a type of MWE composed of two verbs.
In both cases, although the classifiers incorporate
syntactic information about MWEs, their manual
development limits the scalability of the approaches.
Uchiyama et al (2005) also propose a statistical
token classification method for JCVs. This method
is similar to ours, in that it also uses type-based
knowledge to determine the class of each token
in context. However, their method is supervised,
whereas our methods are unsupervised. Moreover,
Uchiyama et al (2005) evaluate their methods on a
set of JCVs that are mostly monosemous. Here, we
intentionally exclude such cases from consideration,
and focus on those MWEs that have two clear id-
iomatic and literal meanings, and that are frequently
used with either meaning.
6 Conclusions
While a great deal of research has focused on prop-
erties of MWE types, such as their compositional-
ity, less attention has been paid to issues surround-
ing MWE tokens. In this study, we have developed
techniques for a semantic classification of tokens of
a potential MWE in context. We focus on a broadly
documented class of English MWEs that are formed
from the combination of a verb and a noun in its
direct object position, referred to as VNCs. We an-
notated a total of 1180 tokens for 28 VNCs accord-
47
ing to whether they are a literal or idiomatic usage,
and we found that approximately 40% of the to-
kens were literal usages. These figures indicate that
automatically determining whether a VNC token is
used idiomatically or literally is of great importance
for NLP applications. In this work, we have pro-
posed three unsupervised methods that perform such
a task. Our proposed methods incorporate automati-
cally acquired knowledge about the overall syntactic
behaviour of a VNC type, in order to do token classi-
fication. More specifically, our methods draw on the
syntactic fixedness of VNCs?a property which has
been largely ignored in previous studies of MWE
tokens. Our results confirm the usefulness of this
property as incorporated into our methods. All our
methods outperform the baseline of always predict-
ing the most frequent class. Moreover, considering
our approach is unsupervised, our best accuracy of
72:4% is not substantially lower than the accuracy
of a standard supervised approach at 76:2%.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL-06, 329?336.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Anthony P. Cowie, Ronald Mackin, and Isabel R. Mc-
Caig. 1983. Oxford Dictionary of Current Idiomatic
English, volume 2. Oxford University Press.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert, Ulrich Heid, and Kristina Spranger. 2004.
Identifying morphosyntactic preferences in colloca-
tions. In Proceedings LREC-04.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of EACL-06, 337?344.
Christiane Fellbaum. 2002. VP idioms in the lexicon:
Topics for research using a very large corpus. In
S. Busemann, editor, Proceedings of the KONVENS-
02 Conference.
John R. Firth. 1957. A synopsis of linguistic theory
1930?1955. In Studies in Linguistic Analysis (special
volume of the Philological Society), 1?32. The Philo-
logical Society, Oxford.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line be-
tween literal and idiomatic meanings. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, 353?360.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, 12?19.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL-01 Workshop
on Collocations.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491?538.
Jon Patrick and Jeremy Fletcher. 2005. Classifying verb-
particle constructions by verb arguments. In Proceed-
ings of the Second ACL-SIGSEM Workshop on the Lin-
guistic Dimensions of Prepositions and their use in
Computational Linguistics Formalisms and Applica-
tions, 200?209.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of CICLing-02, 1?15.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. HarperCollins
Publishers, second edition.
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, Special Issue on
Multiword Expressions, 19(4):497?512.
Sriram Venkatapathy and Aravid Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N)
collocations by integrating features. In Proceedings of
HLT/EMNLP-05, 899?906.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL-06
Workshop on Multiword Expressions in a Multilingual
Context, 33?40.
48
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Unsupervised Model for Text Message Normalization
Paul Cook
Department of Computer Science
University of Toronto
Toronto, Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
Cell phone text messaging users express them-
selves briefly and colloquially using a variety
of creative forms. We analyze a sample of cre-
ative, non-standard text message word forms
to determine frequent word formation pro-
cesses in texting language. Drawing on these
observations, we construct an unsupervised
noisy-channel model for text message normal-
ization. On a test set of 303 text message
forms that differ from their standard form, our
model achieves 59% accuracy, which is on par
with the best supervised results reported on
this dataset.
1 Text Messaging
Cell phone text messages?or SMS?contain many
shortened and non-standard forms due to a variety
of factors, particularly the desire for rapid text entry
(Grinter and Eldridge, 2001; Thurlow, 2003).1 Fur-
thermore, text messages are written in an informal
register; non-standard forms are used to reflect this,
and even for personal style (Thurlow, 2003). These
factors result in tremendous linguistic creativity, and
hence many novel lexical items, in the language of
text messaging, or texting language.
Normalization of non-standard forms?
converting non-standard forms to their standard
forms?is a challenge that must be tackled before
other types of natural language processing can
take place (Sproat et al, 2001). In the case of
text messages, text-to-speech synthesis may be
1The number of characters in a text message may also be
limited to 160 characters, although this is not always the case.
particularly useful for the visually impaired; au-
tomatic translation has also been considered (e.g.,
Aw et al, 2006). For texting language, given the
abundance of creative forms, and the wide-ranging
possibilities for creating new forms, normalization
is a particularly important problem, and has indeed
received some attention in computational linguistics
(e.g., Aw et al, 2006; Choudhury et al, 2007;
Kobus et al, 2008).
In this paper we propose an unsupervised noisy
channel method for texting language normalization,
that gives performance on par with that of a super-
vised system. We pursue unsupervised approaches
to this problem, as large collections of text mes-
sages, and their corresponding standard forms, are
not readily available.2 Furthermore, other forms of
computer-mediated communication, such as Inter-
net messaging, exhibit creative phenomena similar
to text messaging, although at a lower frequency
(Ling and Baron, 2007). Moreover, technological
changes, such as new input devices, are likely to
have an impact on the language of such media (Thur-
low, 2003).3 An unsupervised approach, drawing
on linguistic properties of creative word formations,
has the potential to be adapted for normalization of
text in other similar genres?such as Internet dis-
cussion forums?without the cost of developing a
large training corpus. Moreover, normalization may
be particularly important for such genres, given the
2One notable exception is Fairon and Paumier (2006), al-
though this resource is in French. The resource used in our
study, Choudhury et al (2007), is quite small in comparison.
3The rise of other technology, such as word prediction, could
reduce the use of abbreviations, although it?s not clear such
technology is widely used (Grinter and Eldridge, 2001).
71
Formation type Freq. Example
Stylistic variation 152 betta (better)
Subseq. abbrev. 111 dng (doing)
Prefix clipping 24 hol (holiday)
Syll. letter/digit 19 neway (anyway)
G-clipping 14 talkin (talking)
Phonetic abbrev. 12 cuz (because)
H-clipping 10 ello (hello)
Spelling error 5 darliog (darling)
Suffix clipping 4 morrow (tomorrow)
Punctuation 3 b/day (birthday)
Unclear 34 mobs (mobile)
Error 12 gal (*girl)
Total 400
Table 1: Frequency of texting forms in the development
set by formation type.
need for applications such as translation and ques-
tion answering.
We observe that many creative texting forms are
the result of a small number of specific word for-
mation processes. Rather than using a generic er-
ror model to capture all of them, we propose a mix-
ture model in which each word formation process is
modeled explicitly according to linguistic observa-
tions specific to that formation.
2 Analysis of Texting Forms
To better understand the creative processes present
in texting language, we categorize the word forma-
tion process of each texting form in our development
data, which consists of 400 texting forms paired with
their standard forms.4 Several iterations of catego-
rization were done in order to determine sensible
categories, and ensure categories were used consis-
tently. Since this data is only to be used to guide
the construction of our system, and not for formal
evaluation, only one judge (a native English speak-
ing author of this paper) categorized the expressions.
The findings are presented in Table 1.
Stylistic variations, by far the most frequent cat-
egory, exhibit non-standard spelling, such as repre-
4Most texting forms have a unique standard form; however,
some have multiple standard forms, e.g., will and well can both
be shortened to wl. In such cases we choose the category of the
most frequent standard form; in the case of frequency ties we
choose arbitrarily among the categories of the standard forms.
senting sounds phonetically. Subsequence abbrevi-
ations, also very frequent, are composed of a sub-
sequence of the graphemes in a standard form, of-
ten omitting vowels. These two formation types ac-
count for approximately 66% of our development
data; the remaining formation types are much less
frequent. Prefix clippings and suffix clippings con-
sist of a prefix or suffix, respectively, of a standard
form, and in some cases a diminutive ending; we
also consider clippings which omit just a g or h from
a standard form as they are rather frequent.5 A sin-
gle letter or digit can be used to represent a syllable;
we refer to these as syllabic (syll.) letter/digit. Pho-
netic abbreviations are variants of clippings and sub-
sequence abbreviations where some sounds in the
standard form are represented phonetically. Several
texting forms appear to be spelling errors; we took
the layout of letters on cell phone keypads into ac-
count when making this judgement. The items that
did not fit within the above texting form categories
were marked as unclear. Finally, for some expres-
sions the given standard form did not appear to be
appropriate. For example, girl is not the standard
form for the texting form gal; rather, gal is an En-
glish word that is a colloquial form of girl. Such
cases were marked as errors.
No texting forms in our development data corre-
spond to multiple standard form words, e.g., wanna
for want to.6 Since such forms are not present in our
development data, we assume that a texting form al-
ways corresponds to a single standard form word.
It is important to note that some text forms have
properties of multiple categories, e.g., bak (back)
could be considered a stylistic variation or a subse-
quence abbreviation. In such cases, we simply at-
tempt to assign the most appropriate category.
The design of our model for text message normal-
ization, presented below, uses properties of the ob-
served formation processes.
3 An Unsupervised Noisy Channel Model
for Text Message Normalization
Let S be a sentence consisting of standard forms
s1s2...sn; in this study the standard forms are reg-
5Thurlow (2003) also observes an abundance of g-clippings.
6A small number of similar forms, however, appear with a
single standard form word, and are therefore marked as errors.
72
ular English words. Let T be a sequence of texting
forms t1t2...tn, which are the texting language real-
ization of the standard forms, and may differ from
the standard forms. Given a sequence of texting
forms T , the challenge is then to determine the cor-
responding standard forms S.
Following Choudhury et al (2007)?and vari-
ous approaches to spelling error correction, such
as, e.g., Mays et al (1991)?we model text mes-
sage normalization using a noisy channel. We
want to find argmaxSP (S|T ). We apply Bayes
rule and ignore the constant term P (T ), giving
argmaxSP (T |S)P (S). Making the independence
assumption that each ti depends only on si, and not
on the context in which it occurs, as in Choudhury
et al, we express P (T |S) as a product of probabili-
ties: argmaxS (
?
i P (ti|si))P (S).
We note in Section 2 that many texting forms are
created through a small number of specific word for-
mation processes. Rather than model each of these
processes at once using a generic model for P (ti|si),
as in Choudhury et al, we instead create several such
models, each corresponding to one of the observed
common word formation processes. We therefore
rewrite P (ti|si) as ?wf P (ti|si,wf )P (wf) wherewf is a word formation process, e.g., subsequence
abbreviation. Since, like Choudhury et al, we focus
on the word model, we simplify our model as below.
argmaxsi
?
wf
P (ti|si,wf )P (wf )P (si)
We next explain the components of the model,
P (ti|si,wf ), P (wf ), and P (si), referred to as the
word model, word formation prior, and language
model, respectively.
3.1 Word Models
We now consider which of the word formation pro-
cesses discussed in Section 2 should be captured
with a word model P (ti|si,wf ). We model stylis-
tic variations and subsequence abbreviations simply
due to their frequency. We also choose to model
prefix clippings since this word formation process is
common outside of text messaging (Kreidler, 1979;
Algeo, 1991) and fairly frequent in our data. Al-
though g-clippings and h-clippings are moderately
frequent, we do not model them, as these very spe-
cific word formations are also (non-prototypical)
graphemes w i th ou t
phonemes w I T au t
Table 2: Grapheme?phoneme alignment for without.
subsequence abbreviations. We do not model syl-
labic letters and digits, or punctuation, explicitly; in-
stead, we simply substitute digits with a graphemic
representation (e.g., 4 is replaced by for), and re-
move punctuation, before applying the model. The
other less frequent formations?phonetic abbrevia-
tions, spelling errors, and suffix clippings?are not
modeled; we hypothesize that the similarity of these
formation processes to those we do model will allow
the system to perform reasonably well on them.
3.1.1 Stylistic Variations
We propose a probabilistic version of edit-
distance?referred to here as edit-probability?
inspired by Brill and Moore (2000) to model
P (ti|si, stylistic variation). To compute edit-
probability, we consider the probability of each edit
operation?substitution, insertion, and deletion?
instead of its cost, as in edit-distance. We then sim-
ply multiply the probabilities of edits as opposed to
summing their costs.
In this version of edit-probability, we allow two-
character edits. Ideally, we would compute the edit-
probability of two strings as the sum of the edit-
probability of each partitioning of those strings into
one or two character segments. However, following
Brill and Moore, we approximate this by the prob-
ability of the partition with maximum probability.
This allows us to compute edit-probability using a
simple adaptation of edit-distance, in which we con-
sider edit operations spanning two characters at each
cell in the chart maintained by the algorithm.
We then estimate two probabilities: P (gt|gs, pos)
is the probability of texting form grapheme gt given
standard form grapheme gs at position pos , where
pos is the beginning, middle, or end of the word;
P (ht|ps, hs, pos) is the probability of texting form
graphemes ht given the standard form phonemes ps
and graphemes hs at position pos . ht, ps, and hs can
be a single grapheme or phoneme, or a bigram.
We compute edit-probability between the
graphemes of si and ti. When filling each cell
in the chart, we consider edit operations between
73
segments of si and ti of length 0?2, referred to as a
and b, respectively. If a aligns with phonemes in si,
we also consider those phonemes, p. In our lexicon,
the graphemes and phonemes of each word are
aligned according to the method of Jiampojamarn
et al (2007). For example, the alignment for
without is given in Table 2. The probability of
each edit operation is then determined by three
properties?the length of a, whether a aligns with
any phonemes in si, and if so, p?as shown below:
|a|= 0 or 1, not aligned w/ si phonemes: P (b|a, pos)
|a|= 2, not aligned w/ si phonemes: 0
|a|= 1 or 2, aligned w/ si phonemes: P (b|p, a, pos)
3.1.2 Subsequence Abbreviations
We model subsequence abbreviations according
to the equation below:
P (ti|si, subseq abrv) =
{
c if ti is a subseq of si
0 otherwise
where c is a constant.
Note that this is similar to the error model for
spelling correction presented by Mays et al (1991),
in which all words (in our terms, all si) within
a specified edit-distance of the out-of-vocabulary
word (ti in our model) are given equal probability.
The key difference is that in our formulation, we
only consider standard forms for which the texting
form is potentially a subsequence abbreviation.
In combination with the language model,
P (ti|si, subseq abbrev) assigns a non-zero prob-
ability to each standard form si for which ti is
a subsequence, according to the likelihood of si
(under the language model). The models interact
in this way since we expect a standard form to be
recognizable relative to the other words for which ti
could be a subsequence abbreviation
3.1.3 Prefix Clippings
We model prefix clippings similarly to subse-
quence abbreviations.
P (ti|si, prefix clipping) =
?
??
??
c if ti is possible
pre. clip. of si
0 otherwise
Kreidler (1979) observes that clippings tend to be
mono-syllabic and end in a consonant. Further-
more, when they do end in a vowel, it is often
of a regular form, such as telly for television and
breaky for breakfast. We therefore only consider
P (ti|si, prefix clipping) if ti is a prefix clipping ac-
cording to the following heuristics: ti is mono-
syllabic after stripping any word-final vowels, and
subsequently removing duplicated word-final con-
sonants (e.g, telly becomes tel, which is a candidate
prefix clipping). If ti is not a prefix clipping accord-
ing to these criteria, P (ti|si) simply sums over all
models except prefix clipping.
3.2 Word Formation Prior
Keeping with our goal of an unsupervised method,
we estimate P (wf ) with a uniform distribution. We
also consider estimating P (wf ) using maximum
likelihood estimates (MLEs) from our observations
in Section 2. This gives a model that is not fully
unsupervised, since it relies on labelled training
data. However, we consider this a lightly-supervised
method, since it only requires an estimate of the fre-
quency of the relevant word formation types, and not
labelled texting form?standard form pairs.
3.3 Language Model
Choudhury et al (2007) find that using a bigram lan-
guage model estimated over a balanced corpus of
English had a negative effect on their results com-
pared with a unigram language model, which they
attribute to the unique characteristics of text messag-
ing that were not reflected in the corpus. We there-
fore use a unigram language model for P (si), which
also enables comparison with their results. Never-
theless, alternative language models, such as higher
order ngram models, could easily be used in place of
our unigram language model.
4 Materials and Methods
4.1 Datasets
We use the data provided by Choudhury et al (2007)
which consists of texting forms?extracted from a
collection of 900 text messages?and their manu-
ally determined standard forms. Our development
data?used for model development and discussed in
Section 2?consists of the 400 texting form types
that are not in Choudhury et al?s held-out test set,
and that are not the same as one of their standard
74
forms. The test data consists of 1213 texting forms
and their corresponding standard forms. A subset of
303 of these texting forms differ from their standard
form.7 This subset is the focus of this study, but we
also report results on the full dataset.
4.2 Lexicon
We construct a lexicon of potential standard forms
such that it contains most words that we expect to
encounter in text messages, yet is not so large as
to make it difficult to identify the correct standard
form. Our subjective analysis of the standard forms
in the development data is that they are frequent,
non-specialized, words. To reflect this observation,
we create a lexicon consisting of all single-word en-
tries containing only alphabetic characters found in
both the CELEX Lexical Database (Baayen et al,
1995) and the CMU Pronouncing Dictionary.8 We
remove all words of length one (except a and I) to
avoid choosing, e.g., the letter r as the standard form
for the texting form r. We further limit the lexicon
to words in the 20K most frequent alphabetic uni-
grams, ignoring case, in the Web 1T 5-gram Corpus
(Brants and Franz, 2006). The resulting lexicon con-
tains approximately 14K words, and excludes only
three of the standard forms?cannot, email, and on-
line?for the 400 development texting forms.
4.3 Model Parameter Estimation
MLEs for P (gt|gs, pos)?needed to estimate
P (ti|si, stylistic variation)?could be estimated
from texting form?standard form pairs. However,
since our system is unsupervised, no such data is
available. We therefore assume that many texting
forms, and other similar creative shortenings, occur
on the web. We develop a number of character
substitution rules, e.g., s? z, and use them to create
hypothetical texting forms from standard words.
We then compute MLEs for P (gt|gs, pos) using the
frequencies of these derived forms on the web.
7Choudhury et al report that this dataset contains 1228 tex-
ting forms. We found it to contain 1213 texting forms cor-
responding to 1228 standard forms (recall that a texting form
may have multiple standard forms). There were similar incon-
sistencies with the subset of texting forms that differ from their
standard forms. Nevertheless, we do not expect these small dif-
ferences to have an appreciable effect on the results.
8http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
We create the substitution rules by examining ex-
amples in the development data, considering fast
speech variants and dialectal differences (e.g., voic-
ing), and drawing on our intuition. The derived
forms are produced by applying the substitution
rules to the words in our lexicon. To avoid con-
sidering forms that are themselves words, we elimi-
nate any form found in a list of approximately 480K
words taken from SOWPODS9 and the Moby Word
Lists.10 Finally, we obtain the frequency of the de-
rived forms from the Web 1T 5-gram Corpus.
To estimate P (ht|ps, hs, pos), we first esti-
mate two simpler distributions: P (ht|hs, pos) and
P (ht|ps, pos). P (ht|hs, pos) is estimated in the
same manner as P (gt|gs, pos), except that two char-
acter substitutions are allowed. P (ht|ps, pos) is es-
timated from the frequency of ps, and its align-
ment with ht, in a version of CELEX in which
the graphemic and phonemic representation of each
word is many?many aligned using the method of
Jiampojamarn et al (2007).11 P (ht|ps, hs, pos)
is then an evenly-weighted linear combination of
P (ht|hs, pos) and P (ht|ps, pos). Finally, we
smooth each of P (gt|gs, pos) and P (ht|ps, hs, pos)
using add-alpha smoothing.
We set the constant c in our word models for
subsequence abbreviations and prefix clippings such
that
?
si P (ti|si,wf )P (si) = 1. We similarly nor-
malize P (ti|si, stylistic variation)P (si).
We use the frequency of unigrams (ignoring case)
in the Web 1T 5-gram Corpus to estimate our lan-
guage model. We expect the language of text mes-
saging to be more similar to that found on the web
than that in a balanced corpus of English.
4.4 Evaluation Metrics
To evaluate our system, we consider three accuracy
metrics: in-top-1, in-top-10, and in-top-20.12 In-
top-n considers the system correct if a correct stan-
dard form is in the n most probable standard forms.
The in-top-1 accuracy shows how well the system
determines the correct standard form; the in-top-10
9http://en.wikipedia.org/wiki/SOWPODS
10http://icon.shef.ac.uk/Moby/
11We are very grateful to Sittichai Jiampojamarn for provid-
ing this alignment.
12These are the same metrics used by Choudhury et al
(2007), although we refer to them by different names.
75
Model % accuracy
Top-1 Top-10 Top-20
Uniform 59.4 83.8 87.8
MLE 55.4 84.2 86.5
Choudhury et al 59.9 84.3 88.7
Table 3: % in-top-1, in-top-10, and in-top-20 accuracy
on test data using both estimates for P (wf ). The results
reported by Choudhury et al (2007) are also shown.
and in-top-20 accuracies may be indicative of the
usefulness of the output of our system in other tasks
which could exploit a ranked list of standard forms,
such as machine translation.
5 Results and Discussion
In Table 3 we report the results of our system using
both the uniform estimate and the MLE of P (wf ).
Note that there is no meaningful random baseline
to compare against here; randomly ordering the
14K words in our lexicon gives very low accuracy.
The results using the uniform estimate of P (wf )?
a fully unsupervised system?are very similar to
the supervised results of Choudhury et al (2007).
Surprisingly, when we estimate P (wf ) using MLEs
from the development data?resulting in a lightly-
supervised system?the results are slightly worse
than when using the uniform estimate of this proba-
bility. Moreover, we observe the same trend on de-
velopment data where we expect to have an accurate
estimate of P (wf ) (results not shown). We hypothe-
size that the ambiguity of the categories of text forms
(see Section 2) results in poor MLEs for P (wf ),
thus making a uniform distribution, and hence fully-
unsupervised approach, more appropriate.
Results by Formation Type We now consider in-
top-1 accuracy for each word formation type, in Ta-
ble 4. We show results for the same word forma-
tion processes as in Table 1, except for h-clippings
and punctuation, as no words of these categories are
present in the test data. We present results using the
same experimental setup as before with a uniform
estimate of P (wf ) (All), and using just the model
corresponding to the word formation process (Spe-
cific), where applicable.13
13In this case our model then becomes, for each word forma-
tion process wf , argmaxsiP (ti|si,wf )P (si).
Formation type Freq. % in-top-1 acc.
n = 303 Specific All
Stylistic variation 121 62.8 67.8
Subseq. abbrev. 65 56.9 46.2
Prefix clipping 25 44.0 20.0
G-clipping 56 - 91.1
Syll. letter/digit 16 - 50.0
Unclear 12 - 0.0
Spelling error 5 - 80.0
Suffix clipping 1 - 0.0
Phonetic abbrev. 1 - 0.0
Error 1 - 0.0
Table 4: Frequency (Freq.), and % in-top-1 accuracy us-
ing the formation-specific model where applicable (Spe-
cific) and all models (All) with a uniform estimate for
P (wf ), presented by formation type.
We first examine the top panel of Table 3 where
we compare the performance on each word forma-
tion type for both experimental conditions (Specific
and All). We first note that the performance using
the formation-specific model on subsequence abbre-
viations and prefix clippings is better than that of
the overall model. This is unsurprising since we ex-
pect that when we know a texting form?s formation
process, and invoke a corresponding specific model,
our system should outperform a model designed to
handle a range of formation types. However, this is
not the case for stylistic variations; here the over-
all model performs better than the specific model.
We observed in Section 2 that some texting forms
do not fit neatly into our categorization scheme; in-
deed, many stylistic variations are also analyzable
as subsequence abbreviations. Therefore, the subse-
quence abbreviation model may benefit normaliza-
tion of stylistic variations. This model, used in iso-
lation on stylistic variations, gives an in-top-1 accu-
racy of 33.1%, indicating that this may be the case.
Comparing the performance of the individual
word models on only word types that they were de-
signed for (column Specific in Table 4), we see that
the prefix clipping model is by far the lowest, in-
dicating that in the future we should consider ways
of improving this word model. One possibility is
to incorporate phonemic knowledge. For example,
both friday and friend have the same probability un-
76
der P (ti|si, prefix clipping) for the texting form fri,
which has the standard form friday in our data. (The
language model, however, does distinguish between
these forms.) However, if we consider the phonemic
representations of these words, friday might emerge
as more likely. Syllable structure information may
also be useful, as we hypothesize that clippings will
tend to be formed by truncating a word at a syllable
boundary. We may similarly be able to improve our
estimate of P (ti|si, subseq. abrrev.). For example,
both text and taxation have the same probability un-
der this distribution, but intuitively text, the correct
standard form in our data, seems more likely. We
could incorporate knowledge about the likelihood of
omitting specific characters, as in Choudhury et al
(2007), to improve this estimate.
We now examine the lower panel of Table 4, in
which we consider the performance of the overall
model on the word formation types that are not ex-
plicitly modeled. The very high accuracy on g-
clippings indicates that since these forms are also a
type of subsequence abbreviation, we do not need to
construct a separate model for them. We in fact also
conducted experiments in which g-clippings and h-
clippings were modeled explicitly, but found these
extra models to have little effect on the results.
Recall from Section 3.1 our hypothesis that suf-
fix clippings, spelling errors, and phonetic abbrevia-
tions have common properties with formation types
that we do model, and therefore the system will per-
form reasonably well on them. Here we find pre-
liminary evidence to support this hypothesis as the
accuracy on these three word formation types (com-
bined) is 57.1%. However, we must interpret this
result cautiously as it only considers seven expres-
sions. On the syllabic letter and digit texting forms
the accuracy is 50.0%, indicating that our heuris-
tic to replace digits in texting forms with an ortho-
graphic representation is reasonable.
The performance on types of expressions that
we did not consider when designing the system?
unclear and error?is very poor. However, this has
little impact on the overall performance as these ex-
pressions are rather infrequent.
Results by Model We now consider in-top-1 ac-
curacy using each model on the 303 test expres-
sions; results are shown in Table 5. No model on its
Model % in-top-1 accuracy
Stylistic variation 51.8
Subseq. Abbrev. 44.2
Prefix clipping 10.6
Table 5: % in-top-1 accuracy on the 303 test expressions
using each model individually.
own gives results comparable to those of the over-
all model (59.4%, see Table 3). This indicates that
the overall model successfully combines informa-
tion from the specific word formation models.
Each model used on its own gives an accuracy
greater than the proportion of expressions of the
word formation type for which the model was de-
signed (compare accuracies in Table 5 to the num-
ber of expressions of the corresponding word forma-
tion type in the test data in Table 4). As we note in
Section 2, the distinctions between the word forma-
tion types are not sharp; these results show that the
shared properties of word formation types enable a
model for a specific formation type to infer the stan-
dard form of texting forms of other formation types.
All Unseen Data Until now we have discussed re-
sults on our test data of 303 texting forms which dif-
fer from their standard forms. We now consider the
performance of our system on all 1213 unseen tex-
ting forms, 910 of which are identical to their stan-
dard form. Since our model was not designed with
such expressions in mind, we slightly adapt it for
this new task; if ti is in our lexicon, we return that
form as si, otherwise we apply our model as usual,
using the uniform estimate of P (wf ). This gives
an in-top-1 accuracy of 88.2%, which is very sim-
ilar to the results of Choudhury et al (2007) on this
data of 89.1%. Note, however, that Choudhury et al
only report results on this dataset using a uniform
language model;14 since we use a unigram language
model, it is difficult to draw firm conclusions about
the performance of our system relative to theirs.
6 Related Work
Aw et al (2006) model text message normaliza-
tion as translation from the texting language into the
14Choudhury et al do use a unigram language model for their
experiments on the 303 texting forms which differ from their
standard forms (see Section 3.3).
77
standard language. Kobus et al (2008) incorporate
ideas from both machine translation and automatic
speech recognition for text message normalization.
However, both of these approaches are supervised,
and have only limited means for normalizing texting
forms that do not occur in the training data.
Our work, like that of Choudhury et al (2007),
can be viewed as a noisy-channel model for spelling
error correction (e.g., Mays et al, 1991; Brill and
Moore, 2000), in which texting forms are seen as
a kind of spelling error. Furthermore, like our ap-
proach to text message normalization, approaches to
spelling correction have incorporated phonemic in-
formation (Toutanova and Moore, 2002).
The word model of the supervised approach of
Choudhury et al consists of hidden Markov models,
which capture properties of texting language similar
to those of our stylistic variation model. We pro-
pose multiple word models?corresponding to fre-
quent texting language formation processes?and an
unsupervised method for parameter estimation.
7 Conclusions
We analyze a sample of texting forms to determine
frequent word formation processes in creative tex-
ting language. Drawing on these observations, we
construct an unsupervised noisy-channel model for
text message normalization. On an unseen test set
of 303 texting forms that differ from their standard
form, our model achieves 59% accuracy, which is on
par with that obtained by the supervised approach of
Choudhury et al (2007) on the same data.
More research is required to determine the impact
of our normalization method on the performance of
a system that further processes the resulting text. In
the future, we intend to improve our word models by
incorporating additional linguistic knowledge, such
as information about syllable structure. Since con-
text likely plays a role in human interpretation of
texting forms, we also intend to examine the perfor-
mance of higher order ngram language models.
Acknowledgements
This work is financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada, the University of Toronto, and the Dictio-
nary Society of North America.
References
John Algeo, editor. 1991. Fifty Years Among the New
Words. Cambridge University Press, Cambridge.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proc. of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 33?40. Sydney.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX Lexical Database (release 2). Linguistic Data
Consortium, University of Pennsylvania.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus version 1.1.
Eric Brill and Robert C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of ACL 2000, pages 286?293. Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition, 10(3/4):157?174.
Ce?drick Fairon and Se?bastien Paumier. 2006. A trans-
lated corpus of 30,000 French SMS. In Proceedings of
LREC 2006. Genoa, Italy.
Rebecca E. Grinter and Margery A. Eldridge. 2001. y do
tngrs luv 2 txt msg. In Proceedings of the 7th Euro-
pean Conf. on Computer-Supported Cooperative Work
(ECSCW ?01), pages 219?238. Bonn, Germany.
Sittichai Jiampojamarn, Gregorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments and
hidden markov models to letter-to-phoneme conver-
sion. In Proc. of NAACL-HLT 2007, pages 372?379.
Rochester, NY.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one? In Proc. of the 22nd Int. Conf. on
Computational Linguistics, pp. 441?448. Manchester.
Charles W. Kreidler. 1979. Creating new words by short-
ening. English Linguistics, 13:24?36.
Rich Ling and Naomi S. Baron. 2007. Text messaging
and IM: Linguistic comparison of American college
data. Journal of Language and Social Psychology,
26:291?98.
Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991.
Context based spelling correction. Information Pro-
cessing and Management, 27(5):517?522.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15:287?333.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proc. of ACL 2002, pages 144?151. Philadelphia.
78
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1624?1635, Dublin, Ireland, August 23-29 2014.
Novel Word-sense Identification
Paul Cook
?
, Jey Han Lau
?
, Diana McCarthy
?
and Timothy Baldwin
?
? Department of Computing and Information Systems, The University of Melbourne
? Department of Philosophy, King?s College London
? University of Cambridge
paulcook@unimelb.edu.au, jeyhan.lau@gmail.com,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
Automatic lexical acquisition has been an active area of research in computational linguistics
for over two decades, but the automatic identification of new word-senses has received attention
only very recently. Previous work on this topic has been limited by the availability of appropriate
evaluation resources. In this paper we present the largest corpus-based dataset of diachronic
sense differences to date, which we believe will encourage further work in this area. We then
describe several extensions to a state-of-the-art topic modelling approach for identifying new
word-senses. This adapted method shows superior performance on our dataset of two different
corpus pairs to that of the original method for both: (a) types having taken on a novel sense over
time; and (b) the token instances of such novel senses.
1 Novel word-senses
The meanings of words change over time with, in particular, established words taking on new senses. For
example, the usages of drop, wall, and blow up in the following sentences correspond to relatively-recent
senses of these words that appear to be quite common in text related to popular culture, but are not listed
in many dictionaries; for example, they are all missing from WordNet 3.0 (Fellbaum, 1998).
1. The reissue album drops March 27 and is an extension of Perry?s huge 2010 Teenage Dream. [drops
= ?comes out?, ?is released? ]
2. On Facebook, you can plainly see much of the data the site has on you, because it?s posted to your
wall. [wall = ?Facebook wall?, ?personal electronic noticeboard? ]
3. Why would I give him my number so he can blow up my phone the way he does my inbox. [blow up
= ?overwhelm with messages? ]
Computational lexicons are an essential component of systems for a variety of natural language process-
ing (NLP) tasks. The success of such systems, therefore, depends on the quality of the lexicons they use,
and (semi-)automatic techniques for identifying new word-senses could benefit applied NLP by helping
to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in
addition to new words themselves; methods which identify new word-senses could therefore also help to
keep dictionaries current.
In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses.
Specifically, we consider the identification of word-senses that are not attested in a reference corpus,
taken to represent standard usage, but that are attested in a focus corpus of newer texts.
Lau et al. (2012) introduced the task of novel sense identification. They presented a method for
identifying novel word-senses ? described here in Section 4 ? and evaluated this method on a very
small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al.
(2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new word-
senses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1624
as follows. After discussing related work in Section 2, we present a substantially-expanded evaluation
dataset in Section 3, that is based on a second corpus pair and consists of many more lemmas with a
novel sense. We describe the models used by Lau et al. and Cook et al., and our new extensions to
them, in Section 4. In Section 5 we analyse the results of novel sense identification, and consider a new
baseline for this task. We demonstrate that the extended methods give an improvement over the original
method of Lau et al. We conclude by discussing some previously-unexplored variations on novel sense
identification, and limitations of the approaches considered.
The primary contributions of this paper are: (1) development of a novel sense detection dataset much
larger than has been used in research to date; (2) development and evaluation of a new baseline for
novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only
the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4)
extension of the novel sense detection method of Cook et al. to automatically acquire information about
the expected domain(s) of novel senses.
2 Related work
Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently
in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to
identify specific types of semantic change ? widening and narrowing, and amelioration and pejoration,
respectively ? based on specific properties of these phenomena. Gulordava and Baroni (2011) identify
diachronic sense change in an n-gram database, but using a model that is not restricted to any particular
type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for
identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able
to identify words that have undergone a change in meaning, but not the token instances which give rise
to these sense differences.
Bamman and Crane (2011) use a parallel Latin?English corpus to induce word senses and build a
WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al.
(2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based
approaches there is a clear connection between (induced) word-senses and tokens, making it possible to
identify usages of a specific (new) sense.
Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010)
consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either
marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant word-
senses in corpora, including differences between domains. However, this approach does not identify new
senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domain-
specific parallel corpus with novel translations.
The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel word-
senses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a
natural account of polysemy and not only identifies word types that have a novel sense, but identifies
the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing
sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further
extensions to this method.
3 Datasets
Evaluating approaches to identifying semantic change is a challenge due to the lack of appropriate evalu-
ation resources (i.e., corpora for the appropriate time periods, known to exhibit particular sense changes);
indeed, most previous approaches have used very small datasets (e.g., Sagi et al., 2009; Cook and Steven-
son, 2010; Bamman and Crane, 2011). In this study we consider two datasets of relatively newly-coined
word-senses: (1) an extended version of the dataset based on the BNC (Burnard, 2000) and ukWaC (Fer-
raresi et al., 2008) used by Lau et al. (2012); and (2) a new dataset based on the SiBol/Port Corpus.
1
This
1
http://www3.lingue.unibo.it/blog/clb/?page_id=8
1625
is the largest dataset for evaluating approaches to identifying diachronic semantic change constructed
from corpus evidence to be presented to date.
3.1 BNC?ukWaC
Lau et al. (2012) take the written portion of the BNC (approximately 87 million words of British English
from the late 20th century) as the reference corpus, and a similarly-sized random sample of documents
from the ukWaC (a Web corpus built from the .uk domain in 2007) as the focus corpus. They used
TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora.
A set of words that has acquired a new sense between the late 20th and early 21st centuries ? the time
periods of the reference and focus corpora ? is required. The Concise Oxford English Dictionary aims
to document contemporary usage, and has been published in numerous editions including Thompson
(1995, COD95) and Soanes and Stevenson (2005, COD08), enabling the identification of new senses
amongst the entries in COD08 relative to COD95. Manually searching these dictionaries for new senses
would be time intensive, but new words often correspond to concepts that are culturally salient (Ayto,
2006), and one can leverage this observation to speed up the process of finding some candidate words
with novel senses.
2
Between the time periods of the reference and focus corpora, computers and the Internet have become
much more mainstream in society. Lau et al. therefore extracted all headwords in COD08 whose entries
contain the word computing. They then carefully annotated these lemmas to identify those that indeed
exhibit the novel sense indicated in the dictionary in the corpora. Here, we expand Lau et al.?s dataset by
extracting all headwords including any of the following words code, computer, internet, network, online,
program, web, and website. We then follow a similar annotation process to Lau et al.
An annotator read the entries for the selected lexical items in COD95 and COD08, and identified those
which have a clear sense related to computers or the Internet in COD08 that is not present in COD95;
such senses are referred to as novel senses. This process, along with all the annotation in this section
(including Section 3.2), is carried out by native English-speaking authors of this paper and graduate
students in computational linguistics.
To ensure that the words identified from the dictionaries do in fact have a new sense in the ukWaC
sample compared to the BNC, we examine word sketches (Kilgarriff et al., 2004)
3
for each of these
lemmas in the BNC and ukWaC for collocates that likely correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel sense in the BNC, or fail to find evidence of the novel
sense in the ukWaC sample.
4
We further examine the usage of these words in the corpora. We extract a random sample of 100
usages of each lemma from the BNC and ukWaC sample, and annotate these usages as to whether they
correspond to the novel sense or not. This binary distinction is easier than fine-grained sense annotation,
and since we do not use these annotations for formal evaluation ? only for selecting items for our dataset
? we do not carry out an inter-annotator agreement study here. We eliminate any lemma for which we
find evidence of the novel sense in the usages from the BNC, or for which we do not find evidence of the
novel sense in the ukWaC sample usages.
5
This process resulted in the identification of two lemmas not in the dataset of Lau et al., with frequency
greater than 1000 in the ukWaC sample, and having a novel sense in the ukWaC compared to the BNC
(feed (n) and visit (v)). Combining these new lemmas with the dataset of Lau et al. gives an expanded
dataset consisting of seven lemmas. For both of the two new lemmas, a second annotator annotated
the sample of 100 usages from the ukWaC. The observed agreement and unweighted Kappa for this
annotation task for all seven lemmas is 97.4% and 0.93, respectively, indicating that this is indeed a
relatively easy annotation task. The annotators discussed the small number of disagreements to reach
2
We access the dictionaries in the same way as Lau et al., namely we use COD08 online via http://oxfordreference.
com, and the paper version of COD95.
3
http://www.sketchengine.co.uk/
4
We examine word sketches for the full ukWaC because this version of the corpus is available through the Sketch Engine.
5
We use the IMS Open Corpus Workbench (http://cwb.sourceforge.net/) to extract usages of our target lemmas
from the corpora. This extraction process fails in a number of cases, and so we also eliminate such items from our dataset.
1626
BNC?ukWaC
Lemma Frequency Novel sense definition
domain (n) 41 Internet domain
export (v) 28 export data
feed (n) 23 data feed
mirror (n) 10 mirror website
poster (n) 4 one who posts online
visit (v) 28 access a website
worm (n) 30 malicious program
SiBol/Port
Lemma Frequency Novel sense definition
cloud (n) 9 Internet-based computational resources
drag (v) 1 move on a computer screen using a mouse
follower (n) 34 Twitter follower
help (n) 1 displayed instructions, e.g., help menu
hit (n) 2 search hit
platform (n) 22 computing platform
poster (n) 5 one who posts online
reader (n) 3 e-reader
rip (v) 1 copy music
site (n) 39 website
text (n) 39 text message
visit (v) 7 access a website
wall (n) 2 Facebook wall
Table 1: Lemmas in the BNC?ukWaC and SiBol/Port datasets. For each lemma, the frequency of its
novel sense in the annotated sample of usages from the focus corpus, and a definition of its novel sense,
are shown.
consensus. The seven lemmas in this dataset are shown in Table 1, along with definitions of their novel
senses, and the frequencies of their novel senses in the focus corpus.
Lau et al. compared the novelty of the lemmas with a novel sense to that of a same-size set of distractor
lemmas not having a novel sense. Here we consider a much larger set of 50 distractors ? 25 nouns and
25 verbs ? randomly sampled from a similar frequency range as the items with a novel sense.
One shortcoming of this dataset (and indeed the subset of it used by Lau et al.) is that text types are
represented to different extents in the BNC and ukWaC, with, for example, texts related to the Internet
being much more common in the ukWaC. Such differences in corpus composition are a noted challenge
for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010). In the
following subsection we therefore consider the creation of a new dataset from more-comparable corpora.
3.2 SiBol/Port
The SiBol/Port Corpus consists of texts from several British newspapers for the years 1993, 2005, and
2010; we use the 1993 and 2010 portions of this corpus ? referred to as SP1993 and SP2010 ? as
our reference and focus corpora, respectively. SP1993 and SP2010 contain approximately 93M and 99M
words, respectively. In contrast to BNC?ukWaC, our reference and focus corpora are now comparable,
in that they both consist of texts from British newspapers but they differ with respect to the specific year.
The novel word-senses in the BNC?ukWaC dataset are all related to computers and the Internet, but
there has been recent lexical semantic change unrelated to technology as well (e.g., sick can be used to
mean ?excellent?). In an effort to include such non-technical novel senses in this new dataset, we obtain
a list of headwords for which a sense was added to the Macmillan English Dictionary for Advanced
1627
Learners (MEDAL)
6
since its first edition (Rundell and Fox, 2002), courtesy of Macmillan Dictionaries.
Beginning with these candidates from MEDAL, and the items extracted from COD from Section 3.1, we
discard any lemma whose frequency is less than 1000 in SP1993 or SP2010.
As for the BNC?ukWaC dataset, an annotator examined word sketches for these lemmas. However, it
is possible that the novel sense for a lemma is present in a corpus, but that we fail to find evidence for it in
that lemma?s word sketch. We therefore also obtain judgements from two annotators as to whether each
novel sense is expected to be very infrequent (or unattested) in SP2010. To reduce subsequent annotation
effort, we discard any lemma for which its novel sense is believed to be infrequent in SP2010 by both
judges, and is not found in the word sketch from SP2010.
Annotators then annotate a random sample of 100 usages of each lemma in the reference and focus
corpora as before, and again eliminate any lemma for which we find evidence of its novel sense in the
reference corpus, or fail to find evidence of that sense in the focus corpus. We identify thirteen lemmas
having a novel sense in SP2010 relative to SP1993. These lemmas are also shown in Table 1.
We obtain a second set of annotations for the usages of these lemmas in the sample from SP2010,
with each lemma being annotated by a different annotator than before. The observed agreement and
unweighted Kappa between the two sets of annotations is 96.2% and 0.81, respectively. In cases of
disagreement, a final annotation is again reached through discussion.
We randomly select 164 lemmas (116 nouns and 48 verbs) from a similar frequency range as the
lemmas having a novel sense, to serve as distractors.
Both the BNC?ukWaC and SiBol/Port datasets have been made available.
7
4 The WSI-based approach to novel word-sense detection
In this section we describe the WSI-based method of Lau et al. (2012) for detecting novel senses, and an
extension of this method from Cook et al. (2013). We then present new extensions of this method.
The Lau et al. (2012) WSI model is based on a Hierarchical Dirichlet Process (HDP, Teh et al., 2006),
which is a non-parametric variant of a topic model that, like the commonly-used Latent Dirichlet Allo-
cation (LDA, Blei et al., 2003), learns topics (in the form of multinomial probability distributions over
words) and per-document topic assignments (in the form of multinomial probability distributions over
topics) for a collection of documents; unlike LDA, however, it also optimises the number of topics in an
unsupervised data-driven manner. In the context of WSI, by creating ?documents? that consist of sen-
tences containing a target word, we can view the topics learnt by topic models as the sense representation
of the target word. Indeed, topic models have been previously applied to WSI (e.g., Brody and Lapata,
2009; Yao and Van Durme, 2011).
To generate the input for the topic model, the documents are tokenised (in this case, a ?document? is
a short context, typically 1?3 sentences, containing a target word) into a bag of words. All words are
lemmatised, and stopwords and low frequency terms are removed. Positional word features ? commonly
used in WSI ? for each of the three words to the left and right of the target word are also included.
To induce the senses of a target word w from a given set of usages of w, HDP is run on those usages
(represented according to the features described above) to induce topics; these topics are then interpreted
as representing the senses of w (one topic per sense). To determine the sense assigned to each instance,
the system aggregates over the topic assignments for each word in the context of w, and selects the topic
with the highest aggregated probability, i.e., argmax
z
P (t = z|d), where d is a document and t is a topic.
Recently, Lau et al. (2013a,b) found this method to give the overall best performance on two WSI
shared tasks (Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013), demonstrating that the method
is competitive with the state-of-the-art in WSI, and appropriate as the basis for a method for identifying
novel word-senses.
6
http://www.macmillandictionary.com/
7
http://www.csse.unimelb.edu.au/
~
tim/etc/novel-sense-dataset.tgz
1628
4.1 Novel Sense Detection
Following Lau et al. (2012), to detect novel senses of a target word using this WSI method, we jointly
topic model two corpora: a reference corpus ? taken to represent standard usage ? and a focus corpus
of newer texts potentially containing novel senses. In other words, we extract usages of a target word w
from both corpora, and then topic model the pooled instances of w. Under this approach, the discovered
topics are applicable to both corpora, so there is no need to reconcile two different sets of topics. For the
experiments in this paper, we extract three sentences of context for each usage, one sentence to either
side of the usage of the target word.
As each usage is given a sense assignment, we can identify novel senses ? senses present in the focus
corpus, but unattested in the reference corpus ? based on differences in the sense distribution for a given
word between the two corpora. Lau et al. present a Novelty score which is proportional to the following:
Novelty
Ratio
(s) =
p
f
(s)
p
r
(s)
(1)
where p
f
(s) and p
r
(s) are the proportion of usages of a given word corresponding to sense s in the focus
corpus and reference corpus, respectively, calculated using smoothed maximum likelihood estimates.
The score for a given lemma is the maximum score for any of its induced senses. We refer to the novel
sense for a lemma as the induced sense corresponding to this maximum.
4.2 Alternative Formulations of Novelty
The WSI system underlying the approach of Lau et al. labels each usage of a target lemma with an
induced sense. Therefore, any approach to identifying keywords ? words that are substantially more
frequent in one corpus than another ? can potentially be applied to identify novel senses, by viewing
?words? as (word,sense) tuples. We consider a version of Novelty based on the difference in relative
frequency of an induced sense in the focus and reference corpora, as below:
Novelty
Diff
(s) = p
f
(s)? p
r
(s) (2)
We consider a further new variant of Novelty based on the log-likelihood ratio of an induced sense in the
two corpora, referred to as Novelty
LLR
.
4.3 Incorporating knowledge of expected topics of novel senses
Cook et al. (2013) extended Lau et al.?s method by incorporating the observation that many neologisms
are related to topics that are culturally salient (e.g., Ayto, 2006); nowadays we see many neologisms
related to computing and the Internet. Indeed this observation was used to construct the gold-standard
dataset for this study. Cook et al. identified a set of words, W , related to computing and the Inter-
net, based on manual analysis of keywords for the corpora they considered. They then formulated the
Relevance of an induced sense s for a given word as follows:
Relevance
Manual
(s) =
?
w?W
p(w|s) (3)
For a given lemma, Relevance
Manual
is the maximum of this score for any of its induced senses, similar
to Novelty.
Following Cook et al., we calculate Relevance and Novelty for each induced sense of each lemma,
and then rank all the induced senses by these measures independently. We then compute the rank sum
of each induced sense of each lemma under these two rankings. The final score for a given lemma is
then the rank sum of its highest-ranked sense, and this sense is taken as that lemma?s novel sense. We
refer to this new method as ?Rank Sum?. Cook et al. only considered Novelty and Rank Sum; here we
additionally consider Relevance on its own.
For the keywords, we manually construct a set of words related to computing and the Internet, the
topics for which we expect to observe many novel senses in both of our datasets, in a similar way to
Cook et al. In order to minimize annotation effort, we concentrate on words that are more-frequent in the
1629
focus corpus than the reference corpus. For a given corpus pair, we begin by computing the keywords
for those corpora using Kilgarriff?s (2009) method.
8
Two annotators ? both computational linguists
and not authors of this paper ? independently scanned the top-1000 keywords for the focus corpus, and
selected those that were, based on their intuition, related to computing and the Internet. We then took
the topically-relevant words for a given corpus pair to be those in the intersection of the sets of words
selected by the two annotators. For BNC?ukWaC and SiBol/Port this gives 102 and 30 topically-relevant
words, respectively. This annotation required, on average, 23 minutes per annotator per corpus pair to
complete. Examples of the keywords selected for SiBol/Port include broadband, click, device, online,
and tweet.
4.4 Automatically-extracting keywords
We propose a new fully-automated method for identifying a set of topically-relevant keywords. Because
of the differences in corpus composition, the BNC?ukWaC keywords are often related to computing and
the Internet. To automatically obtain topically-relevant words, we take the top-1000 keywords for the
ukWaC relative to the BNC (i.e., the same keywords annotated for the BNC?ukWaC in Section 4.3).
The keywords for SiBol/Port are less-clearly related to the topics of interest, so we therefore use the
topically-relevant keywords from BNC?ukWaC for both datasets.
5 Results
In the following subsections we consider results at the type and then token level.
5.1 Type-level results
In these experiments we rank all items ? lemmas with a novel sense, and distractors ? by the various
Novelty, Relevance and Rank Sum methods for the BNC?ukWaC and SiBol/Port datasets. When a
lemma takes on a new sense, it might also increase in frequency. We therefore also consider a baseline in
which we rank the lemmas by the ratio of their frequency in the focus corpus and the reference corpus.
This baseline has not been previously considered by Lau et al. (2012) or Cook et al. (2013).
To compare approaches, we examine precision?recall curves in Figures 1 and 2. In an applied setting,
we envision these ranked lists being manually examined; we are therefore primarily interested in the
highly-ranked items, i.e., the left portion of the precision?recall curves.
For BNC?ukWaC (Figure 1), Novelty
Diff
and Novelty
Ratio
perform much better than Novelty
LLR
, but
not better than the frequency ratio baseline, at least for the left-most portion of the precision?recall
curve. Surprisingly, for Relevance, Relevance
Auto
outperforms Relevance
Manual
. This could be because
the focus corpus exhibits a clear topical bias towards computing and the Internet (the expected domain
of many neologisms in the focus corpus), and therefore a larger set of potentially noisy keywords is
more informative than a smaller, hand-selected set. All of the measures including the baseline, except
for Novelty
LLR
, assign higher scores to lemmas with a gold-standard novel sense than the distractors,
according to a one-sided Wilcoxon rank sum test (p < 0.05 in each case).
Turning to SiBol/Port in Figure 2, the frequency ratio baseline is much less effective here; the fre-
quency of the gold-standard novel senses is much lower overall than for BNC?ukWaC. All of the Novelty
and Relevance methods outperform the baseline, and ? with the exception of Novelty
Ratio
? rank the
lemmas with a gold-standard novel sense higher than the distractors (again using a one-sided Wilcoxon
rank sum test and p < 0.05). Furthermore, in this case, Relevance
Manual
outperforms Relevance
Auto
, as
expected.
In terms of the three Novelty measures, only Novelty
Diff
ranked items with a novel sense higher than
the distractors for both datasets. We therefore also show results for the Rank Sum approach combin-
ing Novelty
Diff
and each of Relevance
Manual
and Relevance
Auto
, denoted Rank Sum
Diff,manual
and Rank
Sum
Diff,auto
, respectively, in Figures 1 and 2. For both BNC?ukWaC and SiBol/Port, Rank Sum
Diff,manual
8
Using this method, the keywordness score for a given word is simply the ratio of its frequency per million words, plus a
constant, in two corpora; we set the constant to 100, the value recommended by Kilgarriff.
1630
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 1: Precision?recall curve for the BNC?ukWaC dataset.
gives the best performance, and is a clear improvement over either of the individual methods. As ex-
pected, the performance of Rank Sum
Diff,auto
is not as good, but is nevertheless an improvement over the
frequency ratio baseline for both datasets and provides an alternative to manual scrutiny of the keywords.
To further examine the potential of incorporating knowledge of the expected domains of novel senses
to improve novel sense identification, we consider the case of cloud (n) from the SiBol/Port dataset. The
highest-probability words for the topic with highest Novelty
Diff
are the following: ash, volcanic, flight,
@card@,
9
travel, airline, volcano, airport, air, cloud. This sense appears to be related to the eruption
of the Eyjafjallajo?kull volcano, a major event in 2010 (the year from which the SiBol/Port focus corpus
is taken). Such topical differences, which do not correspond to a novel sense, are a problem for any
approach to identifying lexical semantic differences between two corpora based on differences in the
lexical context of a target word, and indeed observations such as this motivated our use of the methods
incorporating Relevance. The highest probability words for the topic with highest Relevance
Auto
are
the following: cloud, @card@, company, service, business, computing, market, security, datum, need.
This topic appears to correspond to the expected novel sense of Internet-based computational resources,
demonstrating the potential to improve a system for identifying novel word-senses by incorporating
knowledge of the expected domains of neologisms. Moreover, incorporating Relevance is particularly
powerful for avoiding false positives. For example, the distractor clause (n) is the lemma with the
sixth-highest Novelty
Diff
for SiBol/Port. The highest probability words for the corresponding topic are
the following: contract, @card@, club, player, million, england, capello, manager, sign, deal. This
induced sense appears to be related to clauses in Fabio Capello?s contract as manager of the England
national football team, and is not a novel sense of clause. However, none of the induced senses of clause
have high Relevance
Auto
or Relevance
Manual
, and so incorporating information from Relevance can avoid
incorrectly identifying this lemma as having a novel sense.
9
A generic token signifying a cardinal number.
1631
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 2: Precision?recall curve for the SiBol/Port dataset.
5.2 Token-level results
In this section, we consider the token-level identification of instances of the gold-standard novel senses.
We compare Novelty, Relevance, and Rank Sum to a baseline that assigns all usages of a lemma to a
single topic which is selected as the novel sense; in this case recall is 1, and precision is proportional to
the frequency of the novel sense. We further consider the theoretical upper-bound of a method which
selects a single topic as the novel sense, based on the output of the HDP-based WSI method; this oracle
selects the best topic in terms of F-score as the novel sense. Results are presented in Table 2.
Each variant of Novelty and Relevance is an improvement over the baseline, although the Relevance
measures don?t perform as well as the Novelty ones, despite this dataset only containing novel senses
related to computing (despite our efforts to include non-technical novel senses). For consistency with
the presentation of the type-level results, we again consider Rank Sum using Novelty
Diff
, even though it
doesn?t perform as well as Novelty
LLR
or Novelty
Ratio
on BNC?ukWaC. Using either automatically- or
manually-obtained keywords, the performance of Rank Sum on BNC?ukWaC is remarkably on par with
the upper-bound, although for SiBol/Port there is little or no improvement over Novelty
Diff
. Neverthe-
less, these findings are further indication that novel sense identification can be improved by incorporating
information about the topics for which we expect to see novel senses. However, this approach is par-
ticularly helpful at the type-level, where information about the expected topics of novel senses prevents
lemmas not having a novel sense (i.e., the distractors) from being assigned high novelty.
6 Discussion and conclusion
The methods considered in this paper could be applied to any corpus pair, and potentially to identify
lexical semantic differences between, for example, domains or language varieties. The focus of this
study is English; sufficiently-large comparable corpora of national varieties of English (e.g., British and
American English), are not readily-available, but could potentially be inexpensively constructed in the
future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports
1632
Method
F-score
BNC?ukWaC SiBol/Port
Novelty
Diff
0.57 0.29
Novelty
LLR
0.67 0.28
Novelty
Ratio
0.66 0.28
Relevance
Auto
0.48 0.24
Relevance
Manual
0.45 0.27
Rank Sum
Diff,auto
0.72 0.30
Rank Sum
Diff,manual
0.72 0.29
Upper-bound 0.72 0.42
Baseline 0.36 0.20
Table 2: Token-level F-score for the BNC?ukWaC and SiBol/Port datasets using variants of Novelty,
Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown.
and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed
very high Novelty
Ratio
for many distractors (selected in a similar way to our other experiments). Unlike
the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to
cooccur with very different words in the corpora, and Novelty
Ratio
will consequently be high. To address
vocabulary differences between corpora, in their experiments on identifying lexical semantic differences
between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word
to those with moderate frequency in each of the two corpora used. We considered a similar restriction in
experiments on SiBol/Port, but did not see an overall improvement in performance.
We demonstrated that the performance of a method for identifying novel word-senses can be improved
by incorporating information ? acquired manually or automatically ? about the expected topics of
novel senses, which tend to be related to culturally-salient concepts. In future work, we intend to consider
improved approaches for automatically identifying topically-relevant words by incorporating information
about the top keywords of a corpus harvested from the Web for the domain of interest (e.g., PVS et al.,
2012). We also believe that topic models could be useful for identifying emerging or changing domains
themselves given the reference and focus corpus, and related work in this area (e.g., Wang andMcCallum,
2006; Blei and Lafferty, 2007).
To conclude, we have presented the largest type- and token-level dataset of diachronic sense differ-
ences to date, drawing on two pairs of corpora, and have made this dataset available. We applied a
recently-proposed WSI-based method to the task of finding sense differences in this data. We demon-
strated that, while the method shows promise, on a type-based task it is comparable to a a simple fre-
quency baseline, which had not been previously considered for this task. We carried out the first empirical
evaluation of a recently-proposed extension of this method that incorporates manually-acquired knowl-
edge of the expected domains of new senses, and found it to have superior performance at both the type
and token level. We further proposed and evaluated an approach that only uses this domain knowledge,
and a method for automating its acquisition.
Acknowledgments
We thank Michael Rundell and Macmillan Dictionaries for providing the list of headwords added to
MEDAL since its first edition, and Charlotte Taylor for providing us with early access to SiBol/Port. We
also thank Richard Fothergill, Karl Grieser, and Andrew Mackinlay for their help in annotation. This
research was supported in part by funding from the Australian Research Council.
References
John Ayto. 2006. Movers and Shakers: A Chronology of Words that Shaped our Age. Oxford University
Press, Oxford.
1633
David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1?10. Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research,
3:993?1022.
David M. Blei and John D. Lafferty. 2007. Latent dirichlet allocation. The Annals of Applied Statistics,
1(1):17?35.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?111. Athens, Greece.
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford
University Computing Services.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages
1435?1445. Sofia, Bulgaria.
Paul Cook and Graeme Hirst. 2011. Automatic identification of words with novel but infrequent
senses. In Proceedings of the 25th Pacific Asia Conference on Language Information and Compu-
tation (PACLIC 25), pages 265?274. Singapore.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties
of English? In Actes des 11es Journ
?
ees Internationales d?Analyse Statistique des Donn
?
ees Textuelles /
Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages 281?293.
Lie`ge, Belgium.
Paul Cook, Jey Han Lau, Michael Rundell, Diana McCarthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for detecting new word-senses. In Electronic lexicography
in the 21st century: thinking outside the paper. Proceedings of the eLex 2013 conference, pages 49?65.
Tallinn, Estonia.
Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh International Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34. Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evalu-
ating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54. Marrakech, Morocco.
Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of
semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics, pages 67?71. Edinburgh, Scotland.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-2013 task 13: Word sense induction for graded
and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation
(SemEval 2013), pages 290?299. Atlanta, USA.
Adam Kilgarriff. 2009. Simple maths for keywords. In Proceedings of the Corpus Linguistics Confer-
ence. Liverpool, UK.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceed-
ings of the Eleventh EURALEX International Congress (EURALEX 2004), pages 105?116. Lorient,
France.
Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predom-
inant sense acquisition. In Proceedings of Human Language Technology Conference and Conference
1634
on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 419?426. Van-
couver, Canada.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a. unimelb: Topic modelling-based word sense
induction. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),
pages 307?311. Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b. unimelb: Topic modelling-based word sense
induction for web snippet clustering. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221. Atlanta, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense
induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter
of the Association for Computational Linguistics (EACL 2012), pages 591?601. Avignon, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics, 33(4):553?590.
Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 task 11: Word sense induction and dis-
ambiguation within an end-user application. In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?201. Atlanta, USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language Engineering, 16(4):469?491.
Avinesh PVS, Diana McCarthy, Dominic Glennon, and Jan Pomika?lek. 2012. Domain specific corpora
from the Web. In Proceedings of the 15th Euralex International Congress, pages 336?342. Oslo,
Norway.
Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies (ACL 2011),
pages 305?310. Portland, USA.
Michael Rundell and Gwyneth Fox, editors. 2002. Macmillan English Dictionary for Advanced Learn-
ers. Macmillan Education, Oxford, UK.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word
meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?111. Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing, pages 44?49. Manchester, UK.
Catherine Soanes and Angus Stevenson, editors. 2008. The Concise Oxford English Dictionary. Oxford
University Press, Oxford, UK, eleventh (revised) edition. Oxford Reference Online.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Association, 101:1566?1581.
Della Thompson, editor. 1995. The Concise Oxford Dictionary of Current English. Oxford University
Press, Oxford, UK, ninth edition.
Xuerei Wang and Andrew McCallum. 2006. Topics over time: A non-Markov continuous-time model of
topical trends. In Proceedings of the Eleventh International Conference on Knowledge Discovery and
Data Mining, pages 424?433. Philadelphia, USA.
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10?14.
Portland, USA.
1635
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 421?432, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Automatically Constructing a Normalisation Dictionary for Microblogs
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
Microblog normalisation methods often utilise
complex models and struggle to differenti-
ate between correctly-spelled unknown words
and lexical variants of known words. In this
paper, we propose a method for construct-
ing a dictionary of lexical variants of known
words that facilitates lexical normalisation via
simple string substitution (e.g. tomorrow for
tmrw). We use context information to generate
possible variant and normalisation pairs and
then rank these by string similarity. Highly-
ranked pairs are selected to populate the dic-
tionary. We show that a dictionary-based ap-
proach achieves state-of-the-art performance
for both F-score and word error rate on a stan-
dard dataset. Compared with other methods,
this approach offers a fast, lightweight and
easy-to-use solution, and is thus suitable for
high-volume microblog pre-processing.
1 Lexical Normalisation
A staggering number of short text ?microblog? mes-
sages are produced every day through social me-
dia such as Twitter (Twitter, 2011). The immense
volume of real-time, user-generated microblogs that
flows through sites has been shown to have utility
in applications such as disaster detection (Sakaki et
al., 2010), sentiment analysis (Jiang et al2011;
Gonza?lez-Iba?n?ez et al2011), and event discovery
(Weng and Lee, 2011; Benson et al2011). How-
ever, due to the spontaneous nature of the posts,
microblogs are notoriously noisy, containing many
non-standard forms ? e.g., tmrw ?tomorrow? and
2day ?today? ? which degrade the performance of
natural language processing (NLP) tools (Ritter et
al., 2010; Han and Baldwin, 2011). To reduce this
effect, attempts have been made to adapt NLP tools
to microblog data (Gimpel et al2011; Foster et al
2011; Liu et al2011b; Ritter et al2011). An al-
ternative approach is to pre-normalise non-standard
lexical variants to their standard orthography (Liu et
al., 2011a; Han and Baldwin, 2011; Xue et al2011;
Gouws et al2011). For example, se u 2morw!!!
would be normalised to see you tomorrow! The nor-
malisation approach is especially attractive as a pre-
processing step for applications which rely on key-
word match or word frequency statistics. For ex-
ample, earthqu, eathquake, and earthquakeee ? all
attested in a Twitter corpus ? have the standard
form earthquake; by normalising these types to their
standard form, better coverage can be achieved for
keyword-based methods, and better word frequency
estimates can be obtained.
In this paper, we focus on the task of lexical nor-
malisation of English Twitter messages, in which
out-of-vocabulary (OOV) tokens are normalised to
their in-vocabulary (IV) standard form, i.e., a stan-
dard form that is in a dictionary. Following other re-
cent work on lexical normalisation (Liu et al2011a;
Han and Baldwin, 2011; Gouws et al2011; Liu et
al., 2012), we specifically focus on one-to-one nor-
malisation in which one OOV token is normalised to
one IV word.
Naturally, not all OOV words in microblogs are
lexical variants of IV words: named entities, e.g.,
are prevalent in microblogs, but not all named en-
tities are included in our dictionary. One chal-
lenge for lexical normalisation is therefore to dis-
421
tinguish those OOV tokens that require normalisa-
tion from those that are well-formed. Recent un-
supervised approaches have not attempted to distin-
guish such tokens from other types of OOV tokens
(Cook and Stevenson, 2009; Liu et al2011a), lim-
iting their applicability to real-world normalisation
tasks. Other approaches (Han and Baldwin, 2011;
Gouws et al2011) have followed a cascaded ap-
proach in which lexical variants are first identified,
and then normalised. However, such two-step ap-
proaches suffer from poor lexical variant identifica-
tion performance, which is propagated to the nor-
malisation step. Motivated by the observation that
most lexical variants have an unambiguous standard
form (especially for longer tokens), and that a lexi-
cal variant and its standard form typically occur in
similar contexts, in this paper we propose methods
for automatically constructing a lexical normalisa-
tion dictionary ? a dictionary whose entries consist
of (lexical variant, standard form) pairs ? that en-
ables type-based normalisation.
Despite the simplicity of this dictionary-based
normalisation method, we show it to outperform
previously-proposed approaches. This very fast,
lightweight solution is suitable for real-time pro-
cessing of the large volume of streaming microblog
data available from Twitter, and offers a simple solu-
tion to the lexical variant detection problem that hin-
ders other normalisation methods. Furthermore, this
dictionary-based method can be easily integrated
with other more-complex normalisation approaches
(Liu et al2011a; Han and Baldwin, 2011; Gouws
et al2011) to produce hybrid systems.
After discussing related work in Section 2, we
present an overview of our dictionary-based ap-
proach to normalisation in Section 3. In Sections 4
and 5 we experimentally select the optimised con-
text similarity parameters and string similarity re-
ranking method. We present experimental results on
the unseen test data in Section 6, and offer some con-
cluding remarks in Section 7.
2 Related Work
Given a token t, lexical normalisation is the task
of finding argmaxP (s|t) ? argmaxP (t|s)P (s),
where s is the standard form, i.e., an IV word. Stan-
dardly in lexical normalisation, t is assumed to be an
OOV token, relative to a fixed dictionary. In prac-
tice, not all OOV tokens should be normalised; i.e.,
only lexical variants (e.g., tmrw ?tomorrow?) should
be normalised and tokens that are OOV but other-
wise not lexical variants (e.g., iPad ?iPad?) should
be unchanged. Most work in this area focuses only
on the normalisation task itself, oftentimes assuming
that the task of lexical variant detection has already
been completed.
Various approaches have been proposed to esti-
mate the error model, P (t|s). For example, in work
on spell-checking, Brill and Moore (2000) improve
on a standard edit-distance approach by consider-
ing multi-character edit operations; Toutanova and
Moore (2002) build on this by incorporating phono-
logical information. Li et al2006) utilise distri-
butional similarity (Lin, 1998) to correct misspelled
search queries.
In text message normalisation, Choudhury et al
(2007) model the letter transformations and emis-
sions using a hidden Markov model (Rabiner, 1989).
Cook and Stevenson (2009) and Xue et al2011)
propose multiple simple error models, each of which
captures a particular way in which lexical variants
are formed, such as phonetic spelling (e.g., epik
?epic?) or clipping (e.g., walkin ?walking?). Never-
theless, optimally weighting the various error mod-
els in these approaches is challenging.
Without pre-categorising lexical variants into dif-
ferent types, Liu et al2011a) collect Google
search snippets from carefully-designed queries
from which they then extract noisy lexical variant?
standard form pairs. These pairs are used to train
a conditional random field (Lafferty et al2001) to
estimate P (t|s) at the character level. One short-
coming of querying a search engine to obtain train-
ing pairs is it tends to be costly in terms of time and
bandwidth. Here we exploit microblog data directly
to derive (lexical variant, standard form) pairs, in-
stead of relying on external resources. In more-
recent work, Liu et al2012) endeavour to improve
the accuracy of top-n normalisation candidates by
integrating human cognitive inference, character-
level transformations and spell checking in their nor-
malisation model. The encouraging results shift the
focus to reranking and promoting the correct nor-
malisation to the top-1 position. However, like much
previous work on lexical normalisation, this work
422
assumes perfect lexical variant detection.
Aw et al2006) and Kaufmann and Kalita (2010)
consider normalisation as a machine translation task
from lexical variants to standard forms using off-the-
shelf tools. These methods do not assume that lexi-
cal variants have been pre-identified; however, these
methods do rely on large quantities of labelled train-
ing data, which is not available for microblogs.
Recently, Han and Baldwin (2011) and Gouws
et al2011) propose two-step unsupervised ap-
proaches to normalisation, in which lexical vari-
ants are first identified, and then normalised. They
approach lexical variant detection by using a con-
text fitness classifier (Han and Baldwin, 2011) or
through dictionary lookup (Gouws et al2011).
However, the lexical variant detection of both meth-
ods is rather unreliable, indicating the challenge
of this aspect of normalisation. Both of these
approaches incorporate a relatively small normal-
isation dictionary to capture frequent lexical vari-
ants with high precision. In particular, Gouws et
al. (2011) produce a small normalisation lexicon
based on distributional similarity and string simi-
larity (Lodhi et al2002). Our method adopts a
similar strategy using distributional/string similarity,
but instead of constructing a small lexicon for pre-
processing, we build a much wider-coverage nor-
malisation dictionary and opt for a fully lexicon-
based end-to-end normalisation approach. In con-
trast to the normalisation dictionaries of Han and
Baldwin (2011) and Gouws et al2011) which fo-
cus on very frequent lexical variants, we focus on
moderate frequency lexical variants of a minimum
character length, which tend to have unambiguous
standard forms; our intention is to produce normali-
sation lexicons that are complementary to those cur-
rently available. Furthermore, we investigate the im-
pact of a variety of contextual and string similarity
measures on the quality of the resulting lexicons.
In summary, our dictionary-based normalisation ap-
proach is a lightweight end-to-end method which
performs both lexical variant detection and normal-
isation, and thus is suitable for practical online pre-
processing, despite its simplicity.
3 A Lexical Normalisation Dictionary
Before discussing our method for creating a normal-
isation dictionary, we first discuss the feasibility of
such an approach.
3.1 Feasibility
Dictionary lookup approaches to normalisation have
been shown to have high precision but low recall
(Han and Baldwin, 2011; Gouws et al2011). Fre-
quent (lexical variant, standard form) pairs such as
(u, you) are typically included in the dictionaries
used by such methods, while less-frequent items
such as (g0tta, gotta) are generally omitted. Be-
cause of the degree of lexical creativity and large
number of non-standard forms observed on Twitter,
a wide-coverage normalisation dictionary would be
expensive to construct manually. Based on the as-
sumption that lexical variants occur in similar con-
texts to their standard forms, however, it should
be possible to automatically construct a normalisa-
tion dictionary with wider coverage than is currently
available.
Dictionary lookup is a type-based approach to
normalisation, i.e., every token instance of a given
type will always be normalised in the same way.
However, lexical variants can be ambiguous, e.g., y
corresponds to ?you? in yeah, y r right! LOL but
?why? in AM CONFUSED!!! y you did that? Nev-
ertheless, the relative occurrence of ambiguous lex-
ical variants is small (Liu et al2011a), and it has
been observed that while shorter variants such as y
are often ambiguous, longer variants tend to be un-
ambiguous. For example bthday and 4eva are un-
likely to have standard forms other than ?birthday?
and ?forever?, respectively. Therefore, the normali-
sation lexicons we produce will only contain entries
for OOVs with character length greater than a spec-
ified threshold, which are likely to have an unam-
biguous standard form.
3.2 Overview of approach
Our method for constructing a normalisation dictio-
nary is as follows:
Input: Tokenised English tweets
1. Extract (OOV, IV) pairs based on distributional
similarity.
423
2. Re-rank the extracted pairs by string similarity.
Output: A list of (OOV, IV) pairs ordered by string
similarity; select the top-n pairs for inclusion in
the normalisation lexicon.
In Step 1, we leverage large volumes of Twitter
data to identify the most distributionally-similar IV
type for each OOV type. The result of this pro-
cess is a set of (OOV, IV) pairs, ranked by dis-
tributional similarity. The extracted pairs will in-
clude (lexical variant, standard form) pairs, such as
(tmrw, tomorrow), but will also contain false posi-
tives such as (Tusday, Sunday) ? Tusday is a lexical
variant, but its standard form is not ?Sunday? ? and
(Youtube,web) ? Youtube is an OOV named en-
tity, not a lexical variant. Nevertheless, lexical vari-
ants are typically formed from their standard forms
through regular processes (Thurlow, 2003) ? e.g.,
the omission of characters ? and from this per-
spective Sunday and web are not plausible standard
forms for Tusday and Youtube, respectively. In Step
2, we therefore capture this intuition to re-rank the
extracted pairs by string similarity. The top-n items
in this re-ranked list then form the normalisation lex-
icon, which is based only on development data.
Although computationally-expensive to build,
this dictionary can be created offline. Once built,
it then offers a very fast approach to normalisation.
We can only reliably compute distributional simi-
larity for types that are moderately frequent in a cor-
pus. Nevertheless, many lexical variants are suffi-
ciently frequent to be able to compute distributional
similarity, and can potentially make their way into
our normalisation lexicon. This approach is not suit-
able for normalising low-frequency lexical variants,
nor is it suitable for shorter lexical variant types
which ? as discussed in Section 3.1 ? are more
likely to have an ambiguous standard form. Never-
theless, previously-proposed normalisation methods
that can handle such phenomena also rely in part on
a normalisation lexicon. The normalisation lexicons
we create can therefore be easily integrated with pre-
vious approaches to form hybrid normalisation sys-
tems.
4 Contextually-similar Pair Generation
Our objective is to extract contextually-similar
(OOV, IV) pairs from a large-scale collection of mi-
croblog data. Fundamentally, the surrounding words
define the primary context, but there are different
ways of representing context and different similar-
ity measures we can use, which may influence the
quality of generated normalisation pairs.
In representing the context, we experimentally ex-
plore the following factors: (1) context window size
(from 1 to 3 tokens on both sides); (2) n-gram or-
der of the context tokens (unigram, bigram, trigram);
(3) whether context words are indexed for relative
position or not; and (4) whether we use all context
tokens, or only IV words. Because high-accuracy
linguistic processing tools for Twitter are still under
exploration (Liu et al2011b; Gimpel et al2011;
Ritter et al2011; Foster et al2011), we do not
consider richer representations of context, for exam-
ple, incorporating information about part-of-speech
tags or syntax. We also experiment with a number
of simple but widely-used geometric and informa-
tion theoretic distance/similarity measures. In par-
ticular, we use Kullback?Leibler (KL) divergence
(Kullback and Leibler, 1951), Jensen?Shannon (JS)
divergence (Lin, 1991), Euclidean distance and Co-
sine distance.
We use a corpus of 10 million English tweets to do
parameter tuning over, and a larger corpus of tweets
in the final candidate ranking. All tweets were col-
lected from September 2010 to January 2011 via
the Twitter API.1 From the raw data we extract
English tweets using a language identification tool
(Lui and Baldwin, 2011), and then apply a simpli-
fied Twitter tokeniser (adapted from O?Connor et al
(2010)). We use the Aspell dictionary (v6.06)2 to
determine whether a word is IV, and only include
in our normalisation dictionary OOV tokens with
at least 64 occurrences in the corpus and character
length ? 4, both of which were determined through
empirical observation. For each OOV word type in
the corpus, we select the most similar IV type to
form (OOV, IV) pairs. To further narrow the search
space, we only consider IV words which are mor-
phophonemically similar to the OOV type, follow-
ing settings in Han and Baldwin (2011).3
1https://dev.twitter.com/docs/
streaming-api/methods
2http://aspell.net/
3We only consider IV words within an edit distance of 2 or a
phonemic edit distance of 1 from the OOV type, and we further
424
In order to evaluate the generated pairs, we ran-
domly selected 1000 OOV words from the 10 mil-
lion tweet corpus. We set up an annotation task
on Amazon Mechanical Turk,4 presenting five in-
dependent annotators with each word type (with no
context) and asking for corrections where appropri-
ate. For instance, given tmrw, the annotators would
likely identify it as a non-standard variant of ?to-
morrow?. For correct OOV words like iPad, on the
other hand, we would expect them to leave the word
unchanged. If 3 or more of the 5 annotators make
the same suggestion (in the form of either a canoni-
cal spelling or leaving the word unchanged), we in-
clude this in our gold standard for evaluation. In
total, this resulted in 351 lexical variants and 282
correct OOV words, accounting for 63.3% of the
1000 OOV words. These 633 OOV words were used
as (OOV, IV) pairs for parameter tuning. The re-
mainder of the 1000 OOV words were ignored on
the grounds that there was not sufficient consensus
amongst the annotators.5
Contextually-similar pair generation aims to in-
clude as many correct normalisation pairs as pos-
sible. We evaluate the quality of the normalisation
pairs using ?Cumulative Gain? (CG):
CG =
N ??
i=1
rel?i
Suppose there are N ? correct generated pairs
(oovi, ivi), each of which is weighted by rel?i, the
frequency of oovi to indicate its relative importance;
for example, (thinkin, thinking) has a higher weight
than (g0tta, gotta) because thinkin is more frequent
than g0tta in our corpus. In this evaluation we don?t
consider the position of normalisation pairs, and nor
do we penalise incorrect pairs. Instead, we push dis-
tinguishing between correct and incorrect pairs into
the downstream re-ranking step in which we incor-
porate string similarity information.
Given the development data and CG, we run an
exhaustive search of parameter combinations over
only consider the top 30% most-frequent of these IV words.
4https://www.mturk.com/mturk/welcome
5Note that the objective of this annotation task is to identify
lexical variants that have agreed-upon standard forms irrespec-
tive of context, as a special case of the more general task of
lexical normalisation (where context may or may not play a sig-
nificant role in the determination of the normalisation).
our development corpus. The five best parameter
combinations are shown in Table 1. We notice the
CG is almost identical for the top combinations. As
a context window size of 3 incurs a heavy process-
ing and memory overhead over a size of 2, we use
the 3rd-best parameter combination for subsequent
experiments, namely: context window of?2 tokens,
token bigrams, positional index, and KL divergence
as our distance measure.
To better understand the sensitivity of the method
to each parameter, we perform a post-hoc parame-
ter analysis relative to a default setting (as under-
lined in Table 2), altering one parameter at a time.
The results in Table 2 show that bigrams outper-
form other n-gram orders by a large margin (note
that the evaluation is based on a log scale), and
information-theoretic measures are superior to the
geometric measures. Furthermore, it also indicates
using the positional indexing better captures context.
However, there is little to distinguish context mod-
elling with just IV words or all tokens. Similarly,
the context window size has relatively little impact
on the overall performance, supporting our earlier
observation from Table 1.
5 Pair Re-ranking by String Similarity
Once the contextually-similar (OOV, IV) pairs are
generated using the selected parameters in Section
4, we further re-rank this set of pairs in an at-
tempt to boost morphophonemically-similar pairs
like (bananaz, bananas), and penalise noisy pairs
like (paninis, beans).
Instead of using the small 10 million tweet cor-
pus, from this step onwards, we use a larger cor-
pus of 80 million English tweets (collected over the
same period as the development corpus) to develop
a larger-scale normalisation dictionary. This is be-
cause once pairs are generated, re-ranking based on
string comparison is much faster. We only include
in the dictionary OOV words with a token frequency
> 15 to include more OOV types than in Section 4,
and again apply a minimum length cutoff of 4 char-
acters.
To measure how well our re-ranking method pro-
motes correct pairs and demotes incorrect pairs (in-
cluding both OOV words that should not be nor-
malised, e.g. (Youtube,web), and incorrect normal-
425
Rank Window size n-gram Positional index? Lex. choice Sim/distance measure log(CG)
1 ?3 2 Yes All KL divergence 19.571
2 ?3 2 No All KL divergence 19.562
3 ?2 2 Yes All KL divergence 19.562
4 ?3 2 Yes IVs KL divergence 19.561
5 ?2 2 Yes IVs JS divergence 19.554
Table 1: The five best parameter combinations in the exhaustive search of parameter combinations
Window size n-gram Positional index? Lexical choice Similarity/distance measure
?1 19.325 1 19.328 Yes 19.328 IVs 19.335 KL divergence 19.328
?2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227
?3 19.328 3 19.324 JS divergence 19.311
Cosine 19.170
Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs. We tune one parameter at
a time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in each
case is indicated in bold.
isations for lexical variants, e.g. (bcuz, cause)), we
modify our evaluation metric from Section 4 to
evaluate the ranking at different points, using Dis-
counted Cumulative Gain (DCG@N : Jarvelin and
Kekalainen (2002)):
DCG@N = rel1 +
N?
i=2
reli
log2 (i)
where reli again represents the frequency of the
OOV, but it can be gain (a positive number) or loss
(a negative number), depending on whether the ith
pair is correct or incorrect. Because we also expect
correct pairs to be ranked higher than incorrect pairs,
DCG@N takes both factors into account.
Given the generated pairs and the evaluation met-
ric, we first consider three baselines: no re-ranking
(i.e., the final ranking is that of the contextual simi-
larity scores), and re-rankings of the pairs based on
the frequencies of the OOVs in the Twitter corpus,
and the IV unigram frequencies in the Google Web
1T corpus (Brants and Franz, 2006) to get less-noisy
frequency estimates. We also compared a variety of
re-rankings based on a number of string similarity
measures that have been previously considered in
normalisation work (reviewed in Section 2). We ex-
periment with standard edit distance (Levenshtein,
1966), edit distance over double metaphone codes
(phonetic edit distance: (Philips, 2000)), longest
common subsequence ratio over the consonant edit
distance of the paired words (hereafter, denoted as
consonant edit distance: (Contractor et al2010)),
and a string subsequence kernel (Lodhi et al2002).
In Figure 1, we present the DCG@N results for
each of our ranking methods at different rank cut-
offs. Ranking by OOV frequency is motivated by
the assumption that lexical variants are frequently
used by social media users. This is confirmed
by our findings that lexical pairs like (goin, going)
and (nite, night) are at the top of the ranking.
However, many proper nouns and named entities
are also used frequently and ranked at the top,
mixed with lexical variants like (Facebook, speech)
and (Youtube,web). In ranking by IV word fre-
quency, we assume the lexical variants are usually
derived from frequently-used IV equivalents, e.g.
(abou, about). However, many less-frequent lexical
variant types have high-frequency (IV) normalisa-
tions. For instance, the highest-frequency IV word
the has more than 40 OOV lexical variants, such as
tthe and thhe. These less-frequent types occupy the
top positions, reducing the cumulative gain. Com-
pared with these two baselines, ranking by default
contextual similarity scores delivers promising re-
sults. It successfully ranks many more intuitive nor-
malisation pairs at the top, such as (2day, today)
and (wknd,weekend), but also ranks some incorrect
pairs highly, such as (needa, gotta).
The string similarity-based methods perform bet-
ter than our baselines in general. Through man-
ual analysis, we found that standard edit dis-
426
tance ranking is fairly accurate for lexical vari-
ants with low edit distance to their standard forms,
but fails to identify heavily-altered variants like
(tmrw, tomorrow). Consonant edit distance is simi-
lar to standard edit distance, but places many longer
words at the top of the ranking. Edit distance
over double metaphone codes (phonetic edit dis-
tance) performs particularly well for lexical vari-
ants that include character repetitions ? commonly
used for emphasis on Twitter ? because such rep-
etitions do not typically alter the phonetic codes.
Compared with the other methods, the string subse-
quence kernel delivers encouraging results. It mea-
sures common character subsequences of length n
between (OOV, IV) pairs. Because it is computa-
tionally expensive to calculate similarity for larger
n, we choose n=2, following Gouws et al2011).
As N (the lexicon size cut-off) increases, the per-
formance drops more slowly than the other meth-
ods. Although this method fails to rank heavily-
altered variants such as (4get, forget) highly, it typi-
cally works well for longer words. Given that we fo-
cus on longer OOVs (specifically those longer than
4 characters), this ultimately isn?t a great handicap.
6 Evaluation
Given the re-ranked pairs from Section 5, here we
apply them to a token-level normalisation task us-
ing the normalisation dataset of Han and Baldwin
(2011).
6.1 Metrics
We evaluate using the standard evaluation metrics of
precision (P), recall (R) and F-score (F) as detailed
below. We also consider the false alarm rate (FA)
and word error rate (WER), also as shown below.
FA measures the negative effects of applying nor-
malisation; a good approach to normalisation should
not (incorrectly) normalise tokens that are already
in their standard form and do not require normalisa-
tion.6 WER, like F-score, shows the overall benefits
of normalisation, but unlike F-score, measures how
many token-level edits are required for the output to
be the same as the ground truth data. In general, dic-
tionaries with a high F-score/low WER and low FA
6FA + P ? 1 because some lexical variants might be incor-
rectly normalised.
are preferable.
P =
# correctly normalised tokens
# normalised tokens
R =
# correctly normalised tokens
# tokens requiring normalisation
F =
2PR
P +R
FA =
# incorrectly normalised tokens
# normalised tokens
WER =
# token edits needed after normalisation
# all tokens
6.2 Results
We select the three best re-ranking methods, and
best cut-off N for each method, based on the
highest DCG@N value for a given method over
the development data, as presented in Figure 1.
Namely, they are string subsequence kernel (S-dict,
N=40,000), double metaphone edit distance (DM-
dict, N=10,000) and default contextual similarity
without re-ranking (C-dict, N=10,000).7
We evaluate each of the learned dictionaries in Ta-
ble 3. We also compare each dictionary with the
performance of the manually-constructed Internet
slang dictionary (HB-dict) used by Han and Bald-
win (2011), the small automatically-derived dictio-
nary of Gouws et al2011) (GHM-dict), and com-
binations of the different dictionaries. In addition,
the contribution of these dictionaries in hybrid nor-
malisation approaches is also presented, in which we
first normalise OOVs using a given dictionary (com-
bined or otherwise), and then apply the normalisa-
tion method of Gouws et al2011) based on con-
sonant edit distance (GHM-norm), or the approach
of Han and Baldwin (2011) based on the summation
of many unsupervised approaches (HB-norm), to the
remaining OOVs. Results are shown in Table 3, and
discussed below.
6.2.1 Individual Dictionaries
Overall, the individual dictionaries derived by the
re-ranking methods (DM-dict, S-dict) perform bet-
7We also experimented with combining ranks using Mean
Reciprocal Rank. However, the combined rank didn?t improve
performance on the development data. We plan to explore other
ranking aggregation methods in future work.
427
N
 c
ut
?o
ffs
Discounted Cumulative Gain
10K
30K
50K
70K
90K
110K
130K
150K
170K
190K
?
60
K
?
40
K
?
20
K0
20
K
40
K
W
ith
ou
t r
er
a
n
k
O
OV
 fr
eq
ue
nc
y
IV
 fr
eq
ue
nc
y
Ed
it 
di
st
an
ce
Co
ns
on
an
t e
di
t d
ist
.
Ph
on
et
ic
 e
di
t d
ist
.
St
rin
g 
su
bs
eq
. k
e
rn
e
l
Figure 1: Re-ranking based on different string similarity methods.
ter than that based on contextual similarity (C-dict)
in terms of precision and false alarm rate, indicating
the importance of re-ranking. Even though C-dict
delivers higher recall ? indicating that many lexi-
cal variants are correctly normalised ? this is offset
by its high false alarm rate, which is particularly un-
desirable in normalisation. Because S-dict has better
performance than DM-dict in terms of both F-score
and WER, and a much lower false alarm rate than
C-dict, subsequent results are presented using S-dict
only.
Both HB-dict and GHM-dict achieve better than
90% precision with moderate recall. Compared to
these methods, S-dict is not competitive in terms of
either precision or recall. This result seems rather
discouraging. However, considering that S-dict is an
automatically-constructed dictionary targeting lexi-
cal variants of varying frequency, it is not surprising
that the precision is worse than that of HB-dict ?
which is manually-constructed ? and GHM-dict ?
which includes entries only for more-frequent OOVs
for which distributional similarity is more accurate.
Additionally, the recall of S-dict is hampered by the
restriction on lexical variant token length of 4 char-
acters.
6.2.2 Combined Dictionaries
Next we look to combining HB-dict, GHM-dict
and S-dict. In combining the dictionaries, a given
OOV word can be listed with different standard
forms in different dictionaries. In such cases we use
the following preferences for dictionaries ? moti-
vated by our confidence in the normalisation pairs
of the dictionaries ? to resolve conflicts: HB-dict
> GHM-dict > S-dict.
When we combine dictionaries in the second sec-
tion of Table 3, we find that they contain com-
plementary information: in each case the recall
and F-score are higher for the combined dictio-
nary than any of the individual dictionaries. The
combination of HB-dict+GHM-dict produces only
a small improvement in terms of F-score over HB-
dict (the better-performing dictionary) suggesting
that, as claimed, HB-dict and GHM-dict share many
frequent normalisation pairs. HB-dict+S-dict and
GHM-dict+S-dict, on the other hand, improve sub-
428
Method Precision Recall F-Score False Alarm Word Error Rate
C-dict 0.474 0.218 0.299 0.298 0.103
DM-dict 0.727 0.106 0.185 0.145 0.102
S-dict 0.700 0.179 0.285 0.162 0.097
HB-dict 0.915 0.435 0.590 0.048 0.066
GHM-dict 0.982 0.319 0.482 0.000 0.076
HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052
GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061
HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063
HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049
GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135
HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124
HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081
HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079
Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone ren-
dering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al2011) (GHM-dict), the
Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition,
we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined
unsupervised approach of Han and Baldwin (2011) (HB-norm).
stantially over HB-dict and GHM-dict, respectively,
indicating that S-dict contains markedly different
entries to both HB-dict and GHM-dict. The best F-
score and WER are obtained using the combination
of all three dictionaries, HB-dict+GHM-dict+S-dict.
Furthermore, the difference between the results us-
ing HB-dict+GHM-dict+S-dict and HB-dict+GHM-
dict is statistically significant (p < 0.01), based on
the computationally-intensive Monte Carlo method
of Yeh (2000), demonstrating the contribution of S-
dict.
6.2.3 Hybrid Approaches
The methods of Gouws et al2011) (i.e.
GHM-dict+GHM-norm) and Han and Baldwin
(2011) (i.e. HB-dict+HB-norm) have lower preci-
sion and higher false alarm rates than the dictionary-
based approaches; this is largely caused by lex-
ical variant detection errors.8 Using all dic-
tionaries in combination with these methods ?
HB-dict+GHM-dict+S-dict+GHM-norm and HB-
dict+GHM-dict+S-dict+HB-norm ? gives some
improvements, but the false alarm rates remain high.
Despite the limitations of a pure dictionary-based
approach to normalisation ? discussed in Section
3.1 ? the current best practical approach to normal-
8Here we report results that do not assume perfect detection
of lexical variants, unlike the original published results in each
case.
Error type OOV
Standard form
Dict. Gold
(a) plurals playe players player
(b) negation unlike like dislike
(c) possessives anyones anyone anyone?s
(d) correct OOVs iphone phone iphone
(e) test data errors durin during durin
(f) ambiguity siging signing singing
Table 4: Error types in the combined dictionary (HB-
dict+GHM-dict+S-dict)
isation is to use a lexicon, combining hand-built and
automatically-learned normalisation dictionaries.
6.3 Discussion and Error Analysis
We first manually analyse the errors in the combined
dictionary (HB-dict+GHM-dict+S-dict) and give ex-
amples of each error type in Table 4. The most fre-
quent word errors are caused by slight morphologi-
cal variations, including plural forms (a), negations
(b), possessive cases (c), and OOVs that are correct
and do not require normalisation (d). In addition, we
also notice some missing annotations where lexical
variants are skipped by human annotations but cap-
tured by our method (e). Ambiguity (f) definitely
exists in longer OOVs, however, these cases do not
appear to have a strong negative impact on the nor-
malisation performance. An example of a remain-
429
Length cut-off (N ) #Variants Precision Recall (? N ) Recall (all) False Alarm
?4 556 0.700 0.381 0.179 0.162
?5 382 0.814 0.471 0.152 0.122
?6 254 0.804 0.484 0.104 0.131
?7 138 0.793 0.471 0.055 0.122
Table 5: S-dict normalisation results broken down according to OOV token length. Recall is presented both over the
subset of instances of length ? N in the data (?Recall (? N )?), and over the entirety of the dataset (?Recall (all)?);
?#Variants? is the number of token instances of the indicated length in the test dataset.
ing miscellaneous error is bday ?birthday?, which is
mis-normalised as day.
To further study the influence of OOV word
length relative to the normalisation performance, we
conduct a fine-grained analysis of the performance
of the derived dictionary (S-dict) in Table 5, bro-
ken down across different OOV word lengths. The
results generally support our hypothesis that our
method works better for longer OOV words. The
derived dictionary is much more reliable for longer
tokens (length 5, 6, and 7 characters) in terms of pre-
cision and false alarm. Although the recall is rela-
tively modest, in the future we intend to improve re-
call by mining more normalisation pairs from larger
collections of microblog data.
7 Conclusions and Future Work
In this paper, we describe a method for automat-
ically constructing a normalisation dictionary that
supports normalisation of microblog text through di-
rect substitution of lexical variants with their stan-
dard forms. After investigating the impact of dif-
ferent distributional and string similarity methods
on the quality of the dictionary, we present ex-
perimental results on a standard dataset showing
that our proposed methods acquire high quality
(lexical variant, standard form) pairs, with reason-
able coverage, and achieve state-of-the-art end-to-
end lexical normalisation performance on a real-
world token-level task. Furthermore, this dictionary-
lookup method combines the detection and normali-
sation of lexical variants into a simple, lightweight
solution which is suitable for processing of high-
volume microblog feeds.
In the future, we intend to improve our dictionary
by leveraging the constantly-growing volume of mi-
croblog data, and considering alternative ways to
combine distributional and string similarity. In addi-
tion to direct evaluation, we also want to explore the
benefits of applying normalisation for downstream
social media text processing applications, e.g. event
detection.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their insightful comments, and Stephan
Gouws for kindly sharing his data and discussing his
work.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT centre of Excel-
lence programme.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING/ACL 2006, pages
33?40, Sydney, Australia.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT 2011), pages 389?398, Port-
land, Oregon, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 286?293,
Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157?174.
430
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
pages 189?196, Beijing, China.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ?09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78, Boulder, USA.
Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,
Joseph L. Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS Tagging and Parsing the Twitterverse.
In Analyzing Microtext: Papers from the 2011 AAAI
Workshop, volume WS-11-05 of AAAI Workshops,
pages 20?25, San Francisco, CA, USA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 42?47,
Portland, Oregon, USA.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in Twitter:
a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-HLT
2011), pages 581?586, Portland, Oregon, USA.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82?90, Edinburgh,
Scotland, UK.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 368?378,
Portland, Oregon, USA.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
151?160, Portland, Oregon, USA.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22:49?
86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of COL-
ING/ACL 2006, pages 1025?1032, Sydney, Australia.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 36th An-
nual Meeting of the ACL and 17th International Con-
ference on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Quebec, Canada.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
71?76, Portland, Oregon, USA.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 359?367,
Portland, Oregon, USA.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012), Jeju, Republic of Korea.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. J. Mach. Learn. Res., 2:419?
444.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
431
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for Twitter. In Proceedings of the 4th In-
ternational Conference on Weblogs and Social Media
(ICWSM 2010), pages 384?385, Washington, USA.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38?43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2010), pages 172?180, Los Angeles,
USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 1524?1534, Edinburgh,
Scotland, UK.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on the World Wide
Web (WWW 2010), pages 851?860, Raleigh, North
Carolina, USA.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the
ACL and 3rd Annual Meeting of the NAACL (ACL-02),
pages 144?151, Philadelphia, USA.
Official Blog Twitter. 2011. 200 million tweets per day.
Retrived at August 17th, 2011.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the 5th International
Conference on Weblogs and Social Media (ICWSM
2011), Barcelona, Spain.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74?79,
San Francisco, USA.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), pages 947?953,
Saarbru?cken, Germany.
432
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
Long Duong,
12
Trevor Cohn,
1
Karin Verspoor,
1
Steven Bird,
1
and Paul Cook
1
1
Department of Computing and Information Systems,
The University of Melbourne
2
National ICT Australia, Victoria Research Laboratory
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
Abstract
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
?correct? errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
1 Introduction
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only ?good? training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T . Compared
with the state of the art (T?ackstr?om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T?ackstr?om et al. (2013)?s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
886
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P , based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
2 Background and Related Work
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
? or estimate part of the tagger ? on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,
1
a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T?ackstr?om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary?s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn?t use any par-
allel text but used Wiktionary instead. T?ackstr?om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn?t use parallel data but instead exploited four
hours of manual annotation to build?4,000 tokens
or ?3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
1
http://www.wiktionary.org/
887
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
? Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) ? evaluated on CoNLL data (Buchholz and Marsi, 2006).
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper ?guide?, we can take advantage
of very limited annotated data.
2.1 Annotated data
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
2.2 Universal tagset
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.
2
Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
Table 2: The size of annotated data from
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
2
Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
888
3 Directly Projected Model (DPM)
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
3.1 Parallel data
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T?ackstr?om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The ?No LP? model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T?ackstr?om et al. (2013)
scored 84.9% on the same test set.
3.2 Label projection
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T?ackstr?om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column ?Without Mapping? from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column ?Cov-
erage? shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column ?With Mapping?) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ? 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
3.3 The model
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k <= 3)
WC Word class
Table 4: Feature template for a maximum entropy
tagger
We ignore tokens that don?t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (?1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
889
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.
3
We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ? 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
P (t|w) =
1
Z(w)
exp
D
?
j=1
?
j
f
j
(w, t) ,
where Z(w) =
?
t
exp
?
D
j=1
?
j
f
j
(w, t) is the
normalization factor to ensure the probabilities
P (t|w) sum to one. Here f
j
is a feature function
and ?
j
is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {w
i
, t
i
}
N
i=1
, subject to a
zero-mean Gaussian regularisation term,
L = logP (?)
N
?
i=1
P (t
(i)
|w
(i)
)
= ?
D
?
j=1
?
2
j
2?
2
+
N
?
i=1
D
?
j=1
?
j
f
j
(w
i
, t
i
)? logZ(w
i
)
where the regularisation term limits over-fitting,
an important concern when using large feature
3
T?ackstr?om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ?gold standard?
lattice over label sequences.
sets. For our experiments we set ?
2
= 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (?All features model?). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T?ackstr?om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
890
4 Correction Model
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model s
o
=
P (t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P
?
(t|w) as the product between a base
model and a maximum entropy classifier,
P
?
(t|w) ? P (t|w) exp
D
?
j=1
?
j
f
j
(w, t)
where here we use the DPM model as base model
P (t|w). Under this setup, where P
?
uses the same
features as P , and both are log-linear models, this
simplifies to
P
?
(t|w) ? exp
?
?
D
?
j=1
?
j
f
j
(w, t) +
D
?
j=1
?
j
f
j
(w, t)
?
?
? exp
D
?
j=1
(?
j
+ ?
j
) f
j
(w, t) (1)
where the constant of proportionality is Z
?
(w) =
?
t
exp
?
D
j=1
(?
j
+ ?
j
) f
j
(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters ?
j
= ?
j
+ ?
j
, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over ? is equivalent to a Gaussian prior over the
aggregate weights, ?
j
? N (?
j
, ?
2
). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model ?. The resulting training
objective is
L
corr
= logP (t|w, ?)?
1
2?
2
D
?
j=1
(?
j
? ?
j
)
2
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2?
2
.
4.1 Regulariser sensitivity
Careful tuning of the regularisation term ?
2
is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of ?
2
lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of ?
2
forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for ?
2
. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of ?
2
. This is under-
standable because the small ?
2
makes the model
891
ll
l
l
l l l l l
l l
0.0
1 0.1 1 10 70 100 100
0
100
00
1e+
05
1e+
06
1e+
07
Variance
80
84
88
Acc
ura
cy (%
) 
l Average Acc
Figure 1: Sensitivity of regularisation parameter
?
2
against the average accuracy measured on 8
languages on the development set
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if ?
2
is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (? 88%
accuracy). We select the value of ?
2
= 70, which
maximizes the accuracy on the development set.
4.2 The model
Using the value of ?
2
= 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T?ackstr?om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
l
l l
l l l
l l
l l l
100 300 500 700 100
0
150
0
200
0
500
0
100
00
150
00
500
00
Data Size
65
75
85
95
Acc
urac
y (%
) 
l Correction ModelSupervised Model
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
5 Analysis
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don?t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don?t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
892
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T?ackstr?om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ?correction? model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
L
corr
crf
= logP (t|w, ?)
?
1
2?
2
1
?
j?F
1
(?
j
? ?
j
)
2
?
1
2?
2
2
?
j?F
2
?
2
j
(2)
where F
1
is the set of features referring to only
one label as in the DPM maxent model and F
2
is the set of features over label pairs. The union
of F = F
1
? F
2
is the set of all features for
the CRF. We perform grid search using held out
893
data as before for ?
2
1
and ?
2
2
. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T?ackstr?om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
6 Two-output model
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don?t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of ?100k sentences,
4
and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
6.1 The model
Traditionally, MaxEnt classifiers are trained us-
ing a single label.
5
The method we propose is
trained with pairs of output labels: one for the
4
http://www.ark.cs.cmu.edu/global-voices/
5
Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
Malagasy tag (t
M
) and one for the universal tag
(t
U
), which are both predicted conditioned on a
Malagasy word (w
M
) in context. Our two-output
model is defined as
P (t
M
, t
U
|w
M
) =
1
Z(w
M
)
exp
(
D
?
j=1
?
j
f
M
j
(w, t
M
)
+
E
?
j=1
?
j
f
U
j
(w, t
U
) +
F
?
j=1
?
j
f
B
j
(w, t
M
, t
U
)
)
(3)
where f
M
, f
U
, f
B
are the feature functions con-
sidering t
M
only, t
U
only, and over both outputs
t
M
and t
U
respectively, and Z(w
M
) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights ? are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels t
M
. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (t
M
, w
M
) to become
(t
M
, t
U
, w
M
). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the ? to bias to-
wards the DPM parameters, while ? and ? would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving ?5.3% (absolute) im-
894
Model Accuracy (%)
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
Table 7: The performance of different models for
Malagasy.
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
7 Conclusion
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ?22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn?t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
Acknowledgments
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
895
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582?590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ?00), pages 224?231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243?1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634?639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138?147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583?592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746?754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?04),
pages 222?229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154?161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79?86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 9?16.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
896
Computational Linguistics, pages 742?751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693?723, May.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ?03), pages 173?180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ?01, pages 1?8.
897
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792?1797,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Detecting Non-compositional MWE Components using Wiktionary
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We propose a simple unsupervised ap-
proach to detecting non-compositional
components in multiword expressions
based on Wiktionary. The approach makes
use of the definitions, synonyms and trans-
lations in Wiktionary, and is applicable to
any type of MWE in any language, assum-
ing the MWE is contained in Wiktionary.
Our experiments show that the proposed
approach achieves higher F-score than
state-of-the-art methods.
1 Introduction
A multiword expression (MWE) is a combina-
tion of words with lexical, syntactic or seman-
tic idiosyncrasy (Sag et al., 2002; Baldwin and
Kim, 2009). An MWE is considered (semanti-
cally) ?non-compositional? when its meaning is
not predictable from the meaning of its compo-
nents. Conversely, compositional MWEs are those
whose meaning is predictable from the meaning
of the components. Based on this definition, a
component is compositional within an MWE, if its
meaning is reflected in the meaning of the MWE,
and it is non-compositional otherwise.
Understanding which components are non-
compositional within an MWE is important in
NLP applications in which semantic information
is required. For example, when searching for
spelling bee, we may also be interested in docu-
ments about spelling, but not those which contain
only bee. For research project, on the other hand,
we are likely to be interested in documents which
contain either research or project in isolation, and
for swan song, we are only going to be interested
in documents which contain the phrase swan song,
and not just swan or song.
In this paper, we propose an unsupervised ap-
proach based on Wikitionary for predicting which
components of a given MWE have a composi-
tional usage. Experiments over two widely-used
datasets show that our approach outperforms state-
of-the-art methods.
2 Related Work
Previous studies which have considered MWE
compositionality have focused on either the iden-
tification of non-compositional MWE token in-
stances (Kim and Baldwin, 2007; Fazly et al.,
2009; Forthergill and Baldwin, 2011; Muzny and
Zettlemoyer, 2013), or the prediction of the com-
positionality of MWE types (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). The
identification of non-compositional MWE tokens
is an important task when a word combination
such as kick the bucket or saw logs is ambiguous
between a compositional (generally non-MWE)
and non-compositional MWE usage. Approaches
have ranged from the unsupervised learning of
type-level preferences (Fazly et al., 2009) to su-
pervised methods specific to particular MWE con-
structions (Kim and Baldwin, 2007) or applica-
ble across multiple constructions using features
similar to those used in all-words word sense
disambiguation (Forthergill and Baldwin, 2011;
Muzny and Zettlemoyer, 2013). The prediction
of the compositionality of MWE types has tradi-
tionally been couched as a binary classification
task (compositional or non-compositional: Bald-
win et al. (2003), Bannard (2006)), but more re-
cent work has moved towards a regression setup,
where the degree of the compositionality is pre-
dicted on a continuous scale (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). In ei-
ther case, the modelling has been done either over
the whole MWE (Reddy et al., 2011; Salehi and
Cook, 2013), or relative to each component within
the MWE (Baldwin et al., 2003; Bannard, 2006).
In this paper, we focus on the binary classification
of MWE types relative to each component of the
1792
MWE.
The work that is perhaps most closely related to
this paper is that of Salehi and Cook (2013) and
Salehi et al. (2014), who use translation data to
predict the compositionality of a given MWE rel-
ative to each of its components, and then combine
those scores to derive an overall compositionality
score. In both cases, translations of the MWE and
its components are sourced from PanLex (Bald-
win et al., 2010; Kamholz et al., 2014), and if
there is greater similarity between the translated
components and MWE in a range of languages,
the MWE is predicted to be more compositional.
The basis of the similarity calculation is unsuper-
vised, using either string similarity (Salehi and
Cook, 2013) or distributional similarity (Salehi et
al., 2014). However, the overall method is su-
pervised, as training data is used to select the
languages to aggregate scores across for a given
MWE construction. To benchmark our method,
we use two of the same datasets as these two pa-
pers, and repurpose the best-performing methods
of Salehi and Cook (2013) and Salehi et al. (2014)
for classification of the compositionality of each
MWE component.
3 Methodology
Our basic method relies on analysis of lexical
overlap between the component words and the def-
initions of the MWE in Wiktionary, in the man-
ner of Lesk (1986). That is, if a given component
can be found in the definition, then it is inferred
that the MWE carries the meaning of that compo-
nent. For example, the Wiktionary definition of
swimming pool is ?An artificially constructed pool
of water used for swimming?, suggesting that the
MWE is compositional relative to both swimming
and pool. If the MWE is not found in Wiktionary,
we use Wikipedia as a backoff, and use the first
paragraph of the (top-ranked) Wikipedia article as
a proxy for the definition.
As detailed below, we further extend the basic
method to incorporate three types of information
found in Wiktionary: (1) definitions of each word
in the definitions, (2) synonyms of the words in the
definitions, and (3) translations of the MWEs and
components.
3.1 Definition-based Similarity
The basic method uses Boolean lexical overlap be-
tween the target component of the MWE and a
definition. A given MWE will often have multiple
definitions, however, begging the question of how
to combine across them, for which we propose the
following three methods.
First Definition (FIRSTDEF): Use only the
first-listed Wiktionary definition for the MWE,
based on the assumption that this is the predom-
inant sense.
All Definitions (ALLDEFS): In the case that
there are multiple definitions for the MWE, cal-
culate the lexical overlap for each independently
and take a majority vote; in the case of a tie, label
the component as non-compositional.
Idiom Tag (ITAG): In Wiktionary, there is fa-
cility for users to tag definitions as idiomatic.
1
If,
for a given MWE, there are definitions tagged as
idiomatic, use only those definitions; if there are
no such definitions, use the full set of definitions.
3.2 Synonym-based Definition Expansion
In some cases, a component is not explicitly men-
tioned in a definition, but a synonym does occur,
indicating that the definition is compositional in
that component. In order to capture synonym-
based matches, we optionally look for synonyms
of the component word in the definition,
2
and ex-
pand our notion of lexical overlap to include these
synonyms.
For example, for the MWE china clay, the defi-
nition is kaolin, which includes neither of the com-
ponents. However, we find the component word
clay in the definition for kaolin, as shown below.
A fine clay, rich in kaolinite, used in ce-
ramics, paper-making, etc.
This method is compatible with the three
definition-based similarity methods described
above, and indicated by the +SYN suffix (e.g.
FIRSTDEF+SYN is FIRSTDEF with synonym-
based expansion).
3.3 Translations
A third information source in Wiktionary that can
be used to predict compositionality is sense-level
translation data. Due to the user-generated na-
ture of Wiktionary, the set of languages for which
1
Although the recall of these tags is low (Muzny and
Zettlemoyer, 2013).
2
After removing function words, based on a stopword list.
1793
ENC EVPC
WordNet 91.1% 87.5%
Wiktionary 96.7% 96.2%
Wiktionary+Wikipedia 100.0% 96.2%
Table 1: Lexical coverage of WordNet, Wik-
tionary and Wiktionary+Wikipedia over our two
datasets.
translations are provided varies greatly across lexi-
cal entries. Our approach is to take whatever trans-
lations happen to exist in Wiktionary for a given
MWE, and where there are translations in that lan-
guage for the component of interest, use the LCS-
based method of Salehi and Cook (2013) to mea-
sure the string similarity between the translation
of the MWE and the translation of the compo-
nents. Unlike Salehi and Cook (2013), however,
we do not use development data to select the opti-
mal set of languages in a supervised manner, and
instead simply take the average of the string simi-
larity scores across the available languages. In the
case of more than one translation in a given lan-
guage, we use the maximum string similarity for
each pairing of MWE and component translation.
Unlike the definition and synonym-based ap-
proach, the translation-based approach will pro-
duce real rather than binary values. To combine
the two approaches, we discretise the scores given
by the translation approach. In the case of dis-
agreement between the two approaches, we label
the given MWE as non-compositional. This re-
sults in higher recall and lower precision for the
task of detecting compositionality.
3.4 An Analysis of Wiktionary Coverage
A dictionary-based method is only as good as the
dictionary it is applied to. In the case of MWE
compositionality analysis, our primary concern is
lexical coverage in Wiktionary, i.e., what propor-
tion of a representative set of MWEs is contained
in Wiktionary. We measure lexical coverage rela-
tive to the two datasets used in this research (de-
scribed in detail in Section 4), namely 90 En-
glish noun compounds (ENCs) and 160 English
verb particle constructions (EVPCs). In each case,
we calculated the proportion of the dataset that
is found in Wiktionary, Wiktionary+Wikipedia
(where we back off to a Wikipedia document in the
case that a MWE is not found in Wiktionary) and
WordNet (Fellbaum, 1998). The results are found
in Table 1, and indicate perfect coverage in Wik-
tionary+Wikipedia for the ENCs, and very high
coverage for the EVPCs. In both cases, the cov-
erage of WordNet is substantially lower, although
still respectable, at around 90%.
4 Datasets
As mentioned above, we evaluate our method over
the same two datasets as Salehi and Cook (2013)
(which were later used, in addition to a third
dataset of German noun compounds, in Salehi
et al. (2014)): (1) 90 binary English noun com-
pounds (ENCs, e.g. spelling bee or swimming
pool); and (2) 160 English verb particle construc-
tions (EVPCs, e.g. stand up and give away). Our
results are not directly comparable with those of
Salehi and Cook (2013) and Salehi et al. (2014),
however, who evaluated in terms of a regression
task, modelling the overall compositionality of the
MWE. In our case, the task setup is a binary clas-
sification task relative to each of the two compo-
nents of the MWE.
The ENC dataset was originally constructed by
Reddy et al. (2011), and annotated on a contin-
uous [0, 5] scale for both overall compositional-
ity and the component-wise compositionality of
each of the modifier and head noun. The sampling
was random in an attempt to make the dataset bal-
anced, with 48% of compositional English noun
compounds, of which 51% are compositional in
the first component and 60% are compositional in
the second component. We generate discrete la-
bels by discretising the component-wise composi-
tionality scores based on the partitions [0, 2.5] and
(2.5, 5]. On average, each NC in this dataset has
1.4 senses (definitions) in Wiktionary.
The EVPC dataset was constructed by Ban-
nard (2006), and manually annotated for com-
positionality on a binary scale for each of the
head verb and particle. For the 160 EVPCs,
76% are verb-compositional and 48% are particle-
compositional. On average, each EVPC in this
dataset has 3.0 senses (definitions) in Wiktionary.
5 Experiments
The baseline for each dataset takes the form of
looking for a user-annotated idiom tag in the Wik-
tionary lexical entry for the MWE: if there is an id-
iomatic tag, both components are considered to be
non-compositional; otherwise, both components
are considered to be compositional. We expect
this method to suffer from low precision for two
1794
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 66.7 68.2 67.4 66.7 83.3 74.1
LCS 60.0 77.7 67.7 81.6 68.1 64.6
DS 62.1 88.6 73.0 80.5 86.4 71.2
DS+DSL2 62.5 92.3 74.5 78.4 89.4 70.6
LCS+DS+DSL2 66.3 87.5 75.4 82.1 80.6 70.1
FIRSTDEF 59.4 93.2 72.6 54.2 88.9 67.4
ALLDEFS 59.5 100.0 74.6 52.9 100.0 69.2
ITAG 60.3 100.0 75.2 54.5 100.0 70.6
FIRSTDEF+SYN 64.9 84.1 73.3 63.8 83.3 72.3
ALLDEFS+SYN 64.5 90.9 75.5 60.4 88.9 71.9
ITAG+SYN 64.5 90.9 75.5 61.8 94.4 74.7
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
82.9 85.3 84.1 81.9 80.0 69.8
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
81.2 88.1 84.5 87.3 80.6 73.3
ITAG+SYN
COMB(LCS+DS+DSL2)
81.0 88.1 84.1 88.0 81.1 73.9
Table 2: Compositionality prediction results over the ENC dataset, relative to the first component (the
modifier noun) and the second component (the head noun)
reasons: first, the guidelines given to the annota-
tors of our datasets might be different from what
Wiktionary contributors assume to be an idiom.
Second, the baseline method assumes that for any
non-compositional MWE, all components must be
equally non-compositional, despite the wealth of
MWEs where one or more components are com-
positional (e.g. from the Wiktionary guidelines
for idiom inclusion,
3
computer chess, basketball
player, telephone box).
We also compare our method with: (1) ?LCS?,
the string similarity-based method of Salehi and
Cook (2013), in which 54 languages are used;
(2) ?DS?, the monolingual distributional similarity
method of Salehi et al. (2014); (3) ?DS+DSL2?,
the multilingual distributional similarity method
of Salehi et al. (2014), including supervised lan-
guage selection for a given dataset, based on cross-
validation; and (4) ?LCS+DS+DSL2?, whereby
the first three methods are combined using a su-
pervised support vector regression model. In
each case, the continuous output of the model
is equal-width discretised to generate a binary
classification. We additionally present results for
the combination of each of the six methods pro-
posed in this paper with LCS, DS and DSL2, us-
ing a linear-kernel support vector machine (rep-
resented with the suffix ?
COMB(LCS+DS+DSL2)
? for
a given method). The results are based on cross-
3
http://en.wiktionary.org/wiki/
Wiktionary:Idioms_that_survived_RFD
validation, and for direct comparability, the parti-
tions are exactly the same as Salehi et al. (2014).
Tables 2 and 3 provide the results when our pro-
posed method for detecting non-compositionality
is applied to the ENC and EVPC datasets, respec-
tively. The inclusion of translation data was found
to improve all of precision, recall and F-score
across the board for all of the proposed methods.
For reasons of space, results without translation
data are therefore omitted from the paper.
Overall, the simple unsupervised methods pro-
posed in this paper are comparable with the unsu-
pervised and supervised state-of-the-art methods
of Salehi and Cook (2013) and Salehi et al. (2014),
with ITAG achieving the highest F-score for the
ENC dataset and for the verb components of the
EVPC dataset. The inclusion of synonyms boosts
results in most cases.
When we combine each of our proposed meth-
ods with the string and distributional similar-
ity methods of Salehi and Cook (2013) and
Salehi et al. (2014), we see substantial improve-
ments over the comparable combined method of
?LCS+DS+DSL2? in most cases, demonstrating
both the robustness of the proposed methods and
their complementarity with the earlier methods. It
is important to reinforce that the proposed meth-
ods make no language-specific assumptions and
are therefore applicable to any type of MWE and
any language, with the only requirement being that
the MWE of interest be listed in the Wiktionary for
1795
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 24.6 36.8 29.5 59.6 40.5 48.2
LCS 36.5 49.2 39.3 61.5 63.7 60.3
DS 32.8 34.1 33.5 80.9 19.6 29.7
DS+DSL2 31.8 72.4 44.2 74.8 27.5 36.6
LCS+DS+DSL2 36.1 62.6 45.8 77.9 42.8 49.2
FIRSTDEF 24.8 84.2 38.3 54.5 94.0 69.0
ALLDEFS 25.0 97.4 39.8 53.6 97.6 69.2
ITAG 26.2 89.5 40.5 54.6 91.7 68.4
FIRSTDEF+SYN 32.9 65.8 43.9 60.4 65.5 62.9
ALLDEFS+SYN 28.4 81.6 42.1 62.5 77.4 69.1
ITAG+SYN 30.5 65.8 41.7 57.8 61.9 59.8
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
34.0 65.3 44.7 83.6 67.3 65.4
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
37.4 70.9 48.9 80.4 65.9 63.0
ITAG+SYN
COMB(LCS+DS+DSL2)
35.6 70.9 47.4 83.5 64.9 64.2
Table 3: Compositionality prediction results over the EVPC dataset, relative to the first component (the
head verb) and the second component (the particle)
that language.
6 Error Analysis
We analysed all items in each dataset where the
system score differed from that of the human
annotators. For both datasets, the majority of
incorrectly-labelled items were compositional but
predicted to be non-compositional by our sys-
tem, as can be seen in the relatively low preci-
sion scores in Tables 2 and 3. In many of these
cases, the prediction based on definitions and syn-
onyms was compositional but the prediction based
on translations was non-compositional. In such
cases, we arbitrarily break the tie by labelling the
instance as non-compositional, and in doing so
favour recall over precision.
Some of the incorrectly-labelled ENCs have
a gold-standard annotation of around 2.5, or in
other words are semi-compositional. For exam-
ple, the compositionality score for game in game
plan is 2.82/5, but our system labels it as non-
compositional; a similar thing happens with figure
and the EVPC figure out. Such cases demonstrate
the limitation of approaches to MWE composi-
tionality that treat the problem as a binary clas-
sification task.
On average, the EVPCs have three senses,
which is roughly twice the number for ENCs. This
makes the prediction of compositionality harder,
as there is more information to combine across (an
effect that is compounded with the addition of syn-
onyms and translations). In future work, we hope
to address this problem by first finding the sense
which matches best with the sentences given to the
annotators.
7 Conclusion
We have proposed an unsupervised approach for
predicting the compositionality of an MWE rel-
ative to each of its components, based on lexi-
cal overlap using Wiktionary, optionally incorpo-
rating synonym and translation data. Our experi-
ments showed that the various instantiations of our
approach are superior to previous state-of-the-art
supervised methods. All code to replicate the re-
sults in this paper has been made publicly avail-
able at https://github.com/bsalehi/
wiktionary_MWE_compositionality.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
References
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
1796
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M.
Colowick. 2010. PanLex and LEXTRACT: Trans-
lating all words of all languages of the world. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
37?40, Beijing, China.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Richard Forthergill and Timothy Baldwin. 2011.
Fleshing it out: A supervised approach to MWE-
token and MWE-type classification. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 911?
919, Chiang Mai, Thailand.
David Kamholz, Jonathan Pool, and Susan Colowick.
2014. PanLex: Building a resource for panlingual
lexical translation. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), pages 3145?3150, Reyk-
javik, Iceland.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th Annual International Conference on Systems
Documentation, pages 24?26, Ontario, Canada.
Grace Muzny and Luke Zettlemoyer. 2013. Auto-
matic idiom identification in Wiktionary. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1417?
1421, Seattle, USA.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014.
Using distributional similarity of multi-way transla-
tions to predict multiword expression composition-
ality. In Proceedings of the 14th Conference of the
EACL (EACL 2014), pages 472?481, Gothenburg,
Sweden.
1797
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Word Sense Induction for Novel Sense Detection
Jey Han Lau,?? Paul Cook,? Diana McCarthy, ?
David Newman,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
? Lexical Computing
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
Abstract
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
1 Introduction
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling ? using La-
tent Dirichlet Allocation (LDA: Blei et al(2003))
and derivative approaches ? and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics ? in the form of
multinomial probability distributions over words
? and per-document topic assignments ? in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
2 Methodology
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
591
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document?s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models?s probabilistic assignment of
topics to words for the WSI task.
2.1 Data Representation and Pre-processing
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The ?document? in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pado? and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
2.2 Topic Modelling
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (?instance?
in WSI parlance) as our distribution over senses
for that word.
1Notwithstanding the one sense per discourse heuristic
(Gale et al 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al 2006).
In our initial experiments, we use LDA topic
modelling, which requires us to set T , the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P (t = z|d) then; (2) draw a word w from
the chosen topic P (w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
P (w|d) =
T
?
z=1
P (w|t = z)P (t = z|d).
In standard LDA, the user needs to specify the
number of topics T . In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al(2006)),3 where, for each
document, a distribution of mixture components
P (t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ?
DP (?,H); (2) for each document d, generate dis-
tribution P (t|d) ? DP (?0, G0); (3) draw a la-
tent topic z from the document?s mixture compo-
nent distribution P (t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P (w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, argmaxz P (t = z|d).
3 SemEval Experiments
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/?blei/
topicmodeling.html) in our experiments.
4The two HDP parameters ? and ?0 control the variabil-
ity of senses in the documents. In particular, ? controls the
degree of sharing of topics across documents ? a high ?
value leads to more topics, as topics for different documents
are more dissimilar. ?0, on the other hand, controls the de-
gree of mixing of topics within a document? a high ?0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
592
Soroa, 2007; Manandhar et al 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
3.1 SemEval-2010
3.1.1 Dataset and Methodology
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al 2010), and
are hence omitted from this paper.
In our original experiments with LDA, we set
the number of topics (T ) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
1. LDA+Variable T : LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T : LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T ), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom ? randomly assign each test instance to one
of four senses; (2) Baseline MFS ? most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
3.2 SemEval-2010 Results
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
593
System WSD (80%/20%)All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark
UoY 0.62 0.67 0.59
Table 1: WSD F-score over the SemEval-2010 dataset
Looking first at the results for LDA, we see
that the first LDA approach (variable T ) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn?t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn?t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are ?junk? senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the ?Vari-
able T ? version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the ?genuine? topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly ?split? a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
594
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
3.3 Comparison with other Topic Modelling
Approaches to WSI
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a ?lay-
ered? feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn?t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al(2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
595
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
3.4 Discussion
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD?s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD?s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
4 Identifying Novel Senses
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
1. We all know Frankie is adorable, but does he
have swag? [swag = ?style?]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ?send a mes-
sage on Twitter?]
These senses of swag and tweet are not included
in many dictionaries or computational lexicons ?
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) ? yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin?
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
596
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
4.1 Method
Given two corpora ? a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts ? we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (?Nov?) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
Nov(w) = max
({
ps(ti)? pr(ti)
pr(ti)
: ti ? T
})
(1)
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
4.2 Data
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC ? consisting primar-
ily of British English text from the late 20th cen-
tury ? as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al 2008) ? a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types ? as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
597
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation ? only for selecting items
for our dataset ? we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) ?In-
ternet domain?; export (v) ?export data?; mirror
(n) ?mirror website?; poster (n) ?one who posts
online?; and worm (n) ?malicious program?. For
each of the 5 lemmas with novel senses, a sec-
ond annotator ? also a native English-speaking
author of this paper ? annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
4.3 Results
We compute novelty (?Nov?, Equation 1) for all
10 items in our dataset, based on the output of the
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 ?
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 ?
cinema (n) 9.7 1.30 ?
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 ?
symptom (n) 2.1 1.16 ?
Table 4: Novelty score (?Nov?), ratio of frequency in
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
?Novelty? in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p < .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn ?Freq. ratio? in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p > .05).
Examining the frequency of the novel senses?
shown in column ?Novel sense freq.? in Table 4
? we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n?t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
598
Lemma
Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (?Internet domain?) being split across two
induced senses ? the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma?s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (?Nov?), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense ? in terms of F-score ?
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses ? in
terms of F-score ? as the novel sense could of-
fer further improvements. In this case ? results
shown in the final three columns of Table 5 ?
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
5 Conclusion
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
599
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1?10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103?111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233?237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67?71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-He?le`ne Corre?ard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125?
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355?358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116?126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177?182,
Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161?199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
600
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, Oregon.
601
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?72,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Support Platform for Event Detection using Social Intelligence
Timothy Baldwin, Paul Cook, Bo Han, Aaron Harwood,
Shanika Karunasekera and Masud Moshtaghi
Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
Abstract
This paper describes a system designed
to support event detection over Twitter.
The system operates by querying the data
stream with a user-specified set of key-
words, filtering out non-English messages,
and probabilistically geolocating each mes-
sage. The user can dynamically set a proba-
bility threshold over the geolocation predic-
tions, and also the time interval to present
data for.
1 Introduction
Social media and micro-blogs have entered the
mainstream of society as a means for individu-
als to stay in touch with friends, for companies
to market products and services, and for agen-
cies to make official announcements. The attrac-
tions of social media include their reach (either
targeted within a social network or broadly across
a large user base), ability to selectively pub-
lish/filter information (selecting to publish cer-
tain information publicly or privately to certain
groups, and selecting which users to follow),
and real-time nature (information ?push? happens
immediately at a scale unachievable with, e.g.,
email). The serendipitous takeoff in mobile de-
vices and widespread support for social media
across a range of devices, have been significant
contributors to the popularity and utility of social
media.
While much of the content on micro-blogs de-
scribes personal trivialities, there is also a vein of
high-value content ripe for mining. As such, or-
ganisations are increasingly targeting micro-blogs
for monitoring purposes, whether it is to gauge
product acceptance, detect events such as traffic
jams, or track complex unfolding events such as
natural disasters.
In this work, we present a system intended
to support real-time analysis and geolocation of
events based on Twitter. Our system consists of
the following steps: (1) user selection of key-
words for querying Twitter; (2) preprocessing of
the returned queries to rapidly filter out messages
not in a pre-selected set of languages, and option-
ally normalise language content; (3) probabilistic
geolocation of messages; and (4) rendering of the
data on a zoomable map via a purpose-built web
interface, with facility for rich user interaction.
Our starting in the development of this system
was the Ushahidi platform,1 which has high up-
take for social media surveillance and information
dissemination purposes across a range of organ-
isations. The reason for us choosing to imple-
ment our own platform was: (a) ease of integra-
tion of back-end processing modules; (b) extensi-
bility, e.g. to visualise probabilities of geolocation
predictions, and allow for dynamic thresholding;
(c) code maintainability; and (d) greater logging
facility, to better capture user interactions.
2 Example System Usage
A typical user session begins with the user spec-
ifying a disjunctive set of keywords, which are
used as the basis for a query to the Twitter
Streaming API.2 Messages which match the query
are dynamically rendered on an OpenStreetMap
mash-up, indexed based on (grid cell-based) loca-
tion. When the user clicks on a location marker,
they are presented with a pop-up list of messages
matching the location. The user can manipulate a
time slider to alter the time period over which to
present results (e.g. in the last 10 minutes, or over
1http://ushahidi.com/
2https://dev.twitter.com/docs/
streaming-api
69
Figure 1: A screenshot of the system, with a pop-up presentation of the messages at the indicated location.
the last hour), to gain a better sense of report re-
cency. The user can further adjust the threshold of
the prediction accuracy for the probabilistic mes-
sage locations to view a smaller number of mes-
sages with higher-confidence locations, or more
messages that have lower-confidence locations.
A screenshot of the system for the following
query is presented in Figure 1:
study studying exam ?end of semester?
examination test tests school exams uni-
versity pass fail ?end of term? snow
snowy snowdrift storm blizzard flurry
flurries ice icy cold chilly freeze freez-
ing frigid winter
3 System Details
The system is composed of a front-end, which
provides a GUI interface for query parameter in-
put, and a back-end, which computes a result for
each query. The front-end submits the query pa-
rameters to the back-end via a servlet. Since
the result for the query is time-dependent, the
back-end regularly re-evaluates the query, gener-
ating an up-to-date result at regular intervals. The
front-end regularly polls the back-end, via another
servlet, for the latest results that match its submit-
ted query. In this way, the front-end and back-end
are loosely coupled and asynchronous.
Below, we describe details of the various mod-
ules of the system.
3.1 Twitter Querying
When the user inputs a set of keywords, this is is-
sued as a disjunctive query to the Twitter Stream-
ing API, which returns a streamed set of results
in JSON format. The results are parsed, and
piped through to the language filtering, lexical
normalisation, and geolocation modules, and fi-
nally stored in a flat file, which the GUI interacts
with.
3.2 Language Filtering
For language identification, we use langid.py,
a language identification toolkit developed at
The University of Melbourne (Lui and Baldwin,
2011).3 langid.py combines a naive Bayes
classifier with cross-domain feature selection to
provide domain-independent language identifica-
tion. It is available under a FOSS license as
a stand-alone module pre-trained over 97 lan-
guages. langid.py has been developed specif-
ically to be able to keep pace with the speed
of messages through the Twitter ?garden hose?
feed on a single-CPU machine, making it par-
ticularly attractive for this project. Additionally,
in an in-house evaluation over three separate cor-
pora of Twitter data, we have found langid.py
to be overall more accurate than other state-of-
the-art language identification systems such as
3http://www.csse.unimelb.edu.au/
research/lt/resources/langid
70
lang-detect4 and the Compact Language De-
tector (CLD) from the Chrome browser.5
langid.py returns a monolingual prediction
of the language content of a given message, and is
used to filter out all non-English tweets.
3.3 Lexical Normalisation
The prevalence of noisy tokens in microblogs
(e.g. yr ?your? and soooo ?so?) potentially hin-
ders the readability of messages. Approaches
to lexical normalisation?i.e., replacing noisy to-
kens by their standard forms in messages (e.g.
replacing yr with your)?could potentially over-
come this problem. At present, lexical normali-
sation is an optional plug-in for post-processing
messages.
A further issue related to noisy tokens is that
it is possible that a relevant tweet might contain
a variant of a query term, but not that query term
itself. In future versions of the system we there-
fore aim to use query expansion to generate noisy
versions of query terms to retrieve additional rel-
evant tweets. We subsequently intend to perform
lexical normalisation to evaluate the precision of
the returned data.
The present lexical normalisation used by our
system is the dictionary lookup method of Han
and Baldwin (2011) which normalises noisy to-
kens only when the normalised form is known
with high confidence (e.g. you for u). Ultimately,
however, we are interested in performing context-
sensitive lexical normalisation, based on a reim-
plementation of the method of Han and Baldwin
(2011). This method will allow us to target a
wider variety of noisy tokens such as typos (e.g.
earthquak ?earthquake?), abbreviations (e.g. lv
?love?), phonetic substitutions (e.g. b4 ?before?)
and vowel lengthening (e.g. goooood ?good?).
3.4 Geolocation
A vital component of event detection is the de-
termination of where the event is happening, e.g.
to make sense of reports of traffic jams or floods.
While Twitter supports device-based geotagging
of messages, less than 1% of messages have geo-
tags (Cheng et al 2010). One alternative is to re-
turn the user-level registered location as the event
4http://code.google.com/p/
language-detection/
5http://code.google.com/p/
chromium-compact-language-detector/
location, based on the assumption that most users
report on events in their local domicile. However,
only about one quarter of users have registered lo-
cations (Cheng et al 2010), and even when there
is a registered location, there?s no guarantee of
its quality. A better solution would appear to be
the automatic prediction of the geolocation of the
message, along with a probabilistic indication of
the prediction quality.6
Geolocation prediction is based on the terms
used in a given message, based on the assumption
that it will contain explicit mentions of local place
names (e.g. London) or use locally-identifiable
language (e.g. jawn, which is characteristic of the
Philadelphia area). By including a probability
with the prediction, we can give the system user
control over what level of noise they are prepared
to see in the predictions, and hopefully filter out
messages where there is insufficient or conflicting
geolocating evidence.
We formulate the geolocation prediction prob-
lem as a multinomial naive Bayes classification
problem, based on its speed and accuracy over the
task. Given a message m, the task is to output the
most probable location locmax ? {loci}n1 for m.
User-level classification can be performed based
on a similar formulation, by combining the total
set of messages from a given user into a single
combined message.
Given a message m, the task is to find
argmaxi P (loci|m) where each loci is a grid cell
on the map. Based on Bayes? theorem and stan-
dard assumptions in the naive Bayes formulation,
this is transformed into:
argmax
i
P (loci)
v?
j
P (wj |loci)
To avoid zero probabilities, we only consider to-
kens that occur at least twice in the training data,
and ignore unseen words. A probability is calcu-
lated for the most-probable location by normalis-
ing over the scores for each loci.
We employ the method of Ritter et al(2011) to
tokenise messages, and use token unigrams as fea-
tures, including any hashtags, but ignoring twitter
mentions, URLs and purely numeric tokens. We
6Alternatively, we could consider a hybrid approach of
user- and message-level geolocation prediction, especially
for users where we have sufficient training data, which we
plan to incorporate into a future version of the system.
71
ll
l
l
l
l
l l l
l l l
10000 20000 30000 40000
0.15
0.20
0.25
0.30
0.35
0.40
Feature Number
Pre
dict
ion 
Acc
urac
y
Figure 2: Accuracy of geolocation prediction, for
varying numbers of features based on information gain
also experimented with included the named en-
tity predictions of the Ritter et al(2011) method
into our system, but found that it had no impact
on predictive accuracy. Finally, we apply feature
selection to the data, based on information gain
(Yang and Pedersen, 1997).
To evaluate the geolocation prediction mod-
ule, we use the user-level geolocation dataset
of Cheng et al(2010), based on the lower 48
states of the USA. The user-level accuracy of our
method over this dataset, for varying numbers of
features based on information gain, can be seen
in Figure 2. Based on these results, we select the
top 36,000 features in the deployed version of the
system.
In the deployed system, the geolocation pre-
diction model is trained over one million geo-
tagged messages collected over a 4 month pe-
riod from July 2011, resolved to 0.1-degree lat-
itude/longitude grid cells (covering the whole
globe, excepting grid locations where there were
less than 8 messages). For any geotagged mes-
sages in the test data, we preserve the geotag and
simply set the probability of the prediction to 1.0.
3.5 System Interface
The final output of the various pre-processing
modules is a list of tweets that match the query,
in the form of an 8-tuple as follows:
? the Twitter user ID
? the Twitter message ID
? the geo-coordinates of the message (either
provided with the message, or automatically
predicted)
? the probability of the predicated geolocation
? the text of the tweet
In addition to specifying a set of keywords for
a given session, system users can dynamically se-
lect regions on the map, either via the manual
specification of a bounding box, or zooming the
map in and out. They can additionally change
the time scale to display messages over, specify
the refresh interval and also adjust the threshold
on the geolocation predictions, to not render any
messages which have a predictive probability be-
low the threshold. The size of each place marker
on the map is rendered proportional to the num-
ber of messages at that location, and a square is
superimposed over the box to represent the max-
imum predictive probability for a single message
at that location (to provide user feedback on both
the volume of predictions and the relative confi-
dence of the system at a given location).
References
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based ap-
proach to geo-locating twitter users. In Proceedings
of the 19th ACM international conference on In-
formation and knowledge management, CIKM ?10,
pages 759?768, Toronto, ON, Canada. ACM.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011),
pages 368?378, Portland, USA.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 553?561, Chiang Mai, Thailand.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An
experimental study. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1524?1534, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
412?420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
72
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472?481,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Using Distributional Similarity of Multi-way Translations to Predict
Multiword Expression Compositionality
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We predict the compositionality of multi-
word expressions using distributional sim-
ilarity between each component word and
the overall expression, based on transla-
tions into multiple languages. We evaluate
the method over English noun compounds,
English verb particle constructions and
German noun compounds. We show that
the estimation of compositionality is im-
proved when using translations into multi-
ple languages, as compared to simply us-
ing distributional similarity in the source
language. We further find that string sim-
ilarity complements distributional similar-
ity.
1 Compositionality of MWEs
Multiword expressions (hereafter MWEs) are
combinations of words which are lexically, syntac-
tically, semantically or statistically idiosyncratic
(Sag et al., 2002; Baldwin and Kim, 2009). Much
research has been carried out on the extraction and
identification of MWEs
1
in English (Schone and
Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009)
and other languages (Dias, 2003; Evert and Krenn,
2005; Salehi et al., 2012). However, considerably
less work has addressed the task of predicting the
meaning of MWEs, especially in non-English lan-
guages. As a step in this direction, the focus of
this study is on predicting the compositionality of
MWEs.
An MWE is fully compositional if its meaning
is predictable from its component words, and it is
non-compositional (or idiomatic) if not. For ex-
ample, stand up ?rise to one?s feet? is composi-
1
In this paper, we follow Baldwin and Kim (2009) in
considering MWE ?identification? to be a token-level disam-
biguation task, and MWE ?extraction? to be a type-level lex-
icon induction task.
tional, because its meaning is clear from the mean-
ing of the components stand and up. However, the
meaning of strike up ?to start playing? is largely
unpredictable from the component words strike
and up.
In this study, following McCarthy et al. (2003)
and Reddy et al. (2011), we consider composition-
ality to be graded, and aim to predict the degree
of compositionality. For example, in the dataset
of Reddy et al. (2011), climate change is judged
to be 99% compositional, while silver screen is
48% compositional and ivory tower is 9% com-
positional. Formally, we model compositionality
prediction as a regression task.
An explicit handling of MWEs has been shown
to be useful in NLP applications (Ramisch, 2012).
As an example, Carpuat and Diab (2010) proposed
two strategies for integrating MWEs into statisti-
cal machine translation. They show that even a
large scale bilingual corpus cannot capture all the
necessary information to translate MWEs, and that
in adding the facility to model the compositional-
ity of MWEs into their system, they could improve
translation quality. Acosta et al. (2011) showed
that treating non-compositional MWEs as a sin-
gle unit in information retrieval improves retrieval
effectiveness. For example, while searching for
documents related to ivory tower, we are almost
certainly not interested in documents relating to
elephant tusks.
Our approach is to use a large-scale multi-way
translation lexicon to source translations of MWEs
and their component words, and then model the
relative similarity between each of the component
words and the MWE, using distributional similar-
ity based on monolingual corpora for the source
language and each of the target languages. Our
hypothesis is that using distributional similarity
in more than one language will improve the pre-
diction of compositionality. Importantly, in order
to make the method as language-independent and
472
broadly-applicable as possible, we make no use of
corpus preprocessing such as lemmatisation, and
rely only on the availability of a translation dictio-
nary and monolingual corpora.
Our results confirm our hypothesis that distri-
butional similarity over the source language in ad-
dition to multiple target languages improves the
quality of compositionality prediction. We also
show that our method can be complemented with
string similarity (Salehi and Cook, 2013) to further
improve compositionality prediction. We achieve
state-of-the-art results over two datasets.
2 Related Work
Most recent work on predicting the composi-
tionality of MWEs can be divided into two
categories: language/construction-specific and
general-purpose. This can be at either the token-
level (over token occurrences of an MWE in a cor-
pus) or type-level (over the MWE string, indepen-
dent of usage). The bulk of work on composition-
ality has been language/construction-specific and
operated at the token-level, using dedicated meth-
ods to identify instances of a given MWE, and
specific properties of the MWE in that language
to predict compositionality (Lin, 1999; Kim and
Baldwin, 2007; Fazly et al., 2009).
General-purpose token-level approaches such
as distributional similarity have been commonly
applied to infer the semantics of a word/MWE
(Schone and Jurafsky, 2001; Baldwin et al., 2003;
Reddy et al., 2011). These techniques are based
on the assumption that the meaning of a word is
predictable from its context of use, via the neigh-
bouring words of token-level occurrences of the
MWE. In order to predict the compositionality of
a given MWE using distributional similarity, the
different contexts of the MWE are compared with
the contexts of its components, and the MWE is
considered to be compositional if the MWE and
component words occur in similar contexts.
Identifying token instances of MWEs is not al-
ways easy, especially when the component words
do not occur sequentially. For example consider
put on in put your jacket on, and put your jacket
on the chair. In the first example put on is an
MWE while in the second example, put on is a
simple verb with prepositional phrase and not an
instance of an MWE. Moreover, if we adopt a con-
servative identification method, the number of to-
ken occurrences will be limited and the distribu-
tional scores may not be reliable. Additionally,
for morphologically-rich languages, it can be dif-
ficult to predict the different word forms a given
MWE type will occur across, posing a challenge
for our requirement of no language-specific pre-
processing.
Pichotta and DeNero (2013) proposed a token-
based method for identifying English phrasal
verbs based on parallel corpora for 50 languages.
They show that they can identify phrasal verbs bet-
ter when they combine information from multiple
languages, in addition to the information they get
from a monolingual corpus. This finding lends
weight to our hypothesis that using translation data
and distributional similarity from each of a range
of target languages, can improve compositionality
prediction. Having said that, the general applica-
bility of the method is questionable ? there are
many parallel corpora involving English, but for
other languages, this tends not to be the case.
Salehi and Cook (2013) proposed a general-
purpose type-based approach using translation
data from multiple languages, and string similar-
ity between the MWE and each of the compo-
nent words. They use training data to identify the
best-10 languages for a given family of MWEs, on
which to base the string similarity, and once again
find that translation data improves their results
substantially. Among the four string similarity
measures they experimented with, longest com-
mon substring was found to perform best. Their
proposed method is general and applicable to dif-
ferent families of MWEs in different languages. In
this paper, we reimplement the method of Salehi
and Cook (2013) using longest common substring
(LCS), and both benchmark against this method
and combine it with our distributional similarity-
based method.
3 Our Approach
To predict the compositionality of a given MWE,
we first measure the semantic similarity between
the MWE and each of its component words
2
using
distributional similarity based on a monolingual
corpus in the source language. We then repeat the
process for translations of the MWE and its com-
ponent words into each of a range of target lan-
guages, calculating distributional similarity using
2
Note that we will always assume that there are two
component words, but the method is easily generalisable to
MWEs with more than two components.
473
MWE component1 component2 
score1 score2 
Translations 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Translate 
(using Panlex) 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Figure 1: Outline of our approach to computing
the distributional similarity (DS) of translations
of an MWE with each of its component words,
for a given target language. score
1
and score
2
are the similarity for the first and second compo-
nents, respectively. We obtain translations from
Panlex, and use Wikipedia as our corpus for each
language.
a monolingual corpus in the target language (Fig-
ure 1). We additionally use supervised learning to
identify which target languages (or what weights
for each language) optimise the prediction of com-
positionality (Figure 2). We hypothesise that by
using multiple translations ? rather than only in-
formation from the source language ? we will be
able to better predict compositionality.
We optionally combine our proposed approach
with string similarity, calculated based on the
method of Salehi and Cook (2013), using LCS.
Below, we detail our method for calculating dis-
tributional similarity in a given language, the dif-
ferent methods for combining distributional simi-
larity scores into a single estimate of composition-
ality, and finally the method for selecting the target
languages to use in calculating compositionality.
3.1 Calculating Distributional Similarity
In order to be consistent across all languages and
be as language-independent as possible, we calcu-
CSmethod CSmethod 
Score1 for each language Score2 for each language 
21 )1( ss ?? ??
Compositionality  score 
s1 s2 
Figure 2: Outline of the method for combin-
ing distributional similarity scores from multiple
languages, across the components of the MWE.
CS
method
refers to one of the methods described
in Section 3.2 for calculating compositionality.
late distributional similarity in the following man-
ner for a given language.
Tokenisation is based on whitespace delimiters
and punctuation; no lemmatisation or case-folding
is carried out. Token instances of a given MWE
or component word are identified by full-token n-
gram matching over the token stream. We assume
that all full stops and equivalent characters for
other orthographies are sentence boundaries, and
chunk the corpora into (pseudo-)sentences on the
basis of them. For each language, we identify the
51st?1050th most frequent words, and consider
them to be content-bearing words, in the manner
of Sch?utze (1997). This is based on the assump-
tion that the top-50 most frequent words are stop
words, and not a good choice of word for calculat-
ing distributional similarity over. That is not to say
that we can?t calculate the distributional similarity
for stop words, however (as we will for the verb
particle construction dataset ? see Section 4.3.2)
they are simply not used as the dimensions in our
calculation of distributional similarity.
We form a vector of content-bearing words
across all token occurrences of the target word,
474
on the basis of these content-bearing words. Dis-
tributional similarity is calculated over these con-
text vectors using cosine similarity. Accord-
ing to Weeds (2003), using dependency rela-
tions with the neighbouring words of the target
word can better predict the meaning of the target
word. However, in line with our assumption of no
language-specific preprocessing, we just use word
co-occurrence.
3.2 Calculating Compositionality
First, we need to calculate a combined composi-
tionality score from the individual distributional
similarities between each component word and the
MWE. Following Reddy et al. (2011), we combine
the component scores using the weighted mean (as
shown in Figure 2):
comp = ?s
1
+ (1? ?)s
2
(1)
where s
1
and s
2
are the scores for the first and
the second component, respectively. We use dif-
ferent ? settings for each dataset, as detailed in
Section 4.3.
We experiment with a range of methods for cal-
culating compositionality, as follows:
CS
L1
: calculate distributional similarity using
only distributional similarity in the source
language corpus (This is the approach used
by Reddy et al. (2011), as discussed in Sec-
tion 2).
CS
L2N
: exclude the source language, and com-
pute the mean of the distributional similarity
scores for the best-N target languages. The
value of N is selected according to training
data, as detailed in Section 3.3.
CS
L1+L2N
: calculate distributional similarity
over both the source language (CS
L1
) and
the mean of the best-N languages (CS
L2N
),
and combine via the arithmetic mean.
3
This
is to examine the hypothesis that using
multiple target languages is better than just
using the source language.
CS
SVR(L1+L2 )
: train a support vector regressor
(SVR: Smola and Sch?olkopf (2004)) over the
distributional similarities for all 52 languages
(source and target languages).
3
We also experimented with taking the mean over all the
languages ? target and source ? but found it best to com-
bine the scores for the target languages first, to give more
weight to the source language.
CS
string
: calculate string similarity using the
LCS-based method of Salehi and Cook
(2013).
4
CS
string+L1
: calculate the mean of the string
similarity (CS
string
) and distributional sim-
ilarity in the source language (Salehi and
Cook, 2013).
CS
all
: calculate the mean of the string similarity
(CS
string
) and distributional similarity scores
(CS
L1
and CS
L2N
).
3.3 Selecting Target Languages
We experiment with two approaches for combin-
ing the compositionality scores from multiple tar-
get languages.
First, inCS
L2N
(andCS
L1+L2N
andCS
all
that
build off it), we use training data to rank the target
languages according to Pearson?s correlation be-
tween the predicted compositionality scores and
the gold-standard compositionality judgements.
Based on this ranking, we take the best-N lan-
guages, and combine the individual composition-
ality scores by taking the arithmetic mean. We se-
lect N by determining the value that optimises the
correlation over the training data. In other words,
the selection ofN and accordingly the best-N lan-
guages are based on nested cross-validation over
training data, independently of the test data for that
iteration of cross-validation.
Second in CS
SVR(L1+L2 )
, we combine the
compositionality scores from the source and all 51
target languages into a feature vector, and train an
SVR over the data using LIBSVM.
5
4 Resources
In this section, we describe the resources required
by our method, and also the datasets used to eval-
uate our method.
4.1 Monolingual Corpora for Different
Languages
We collected monolingual corpora for each of 52
languages (51 target languages + 1 source lan-
guage) from XML dumps of Wikipedia. These
languages are based on the 54 target languages
4
Due to differences in our random partitioning, our re-
ported results over the two English datasets differ slightly
over the results of Salehi and Cook (2013) using the same
method.
5
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm
475
used by Salehi and Cook (2013), excluding Span-
ish because we happened not to have a dump of
Spanish Wikipedia, and also Chinese and Japanese
because of the need for a language-specific word
tokeniser. The raw corpora were preprocessed us-
ing the WP2TXT toolbox
6
to eliminate XML tags,
HTML tags and hyperlinks, and then tokenisa-
tion based on whitespace and punctuation was per-
formed. The corpora vary in size from roughly
750M tokens for English, to roughly 640K tokens
for Marathi.
4.2 Multilingual Dictionary
To translate the MWEs and their components,
we follow Salehi and Cook (2013) in using Pan-
lex (Baldwin et al., 2010). This online dictio-
nary is massively multilingual, covering more than
1353 languages. For each MWE dataset (see Sec-
tion 4.3), we translate the MWE and component
words from the source language into each of the
51 languages.
In instances where there is no direct translation
in a given language for a term, we use a pivot lan-
guage to find translation(s) in the target language.
For example, the English noun compound silver
screen has direct translations in only 13 languages
in Panlex, including Vietnamese (ma`n bac) but
not French. There is, however, a translation of
ma`n bac into French (cine?ma), allowing us to
infer an indirect translation between silver screen
and cine?ma. In this way, if there are no direct
translations into a particular target language, we
search for a single-pivot translation via each of our
other target languages, and combine them all to-
gether as our set of translations for the target lan-
guage of interest.
In the case that no translation (direct or indirect)
can be found for a given source language term into
a particular target language, the compositionality
score for that target language is set to the average
across all target languages for which scores can be
calculated for the given term. If no translations are
available for any target language (e.g. the term is
not in Panlex) the compositionality score for each
target language is set to the average score for that
target language across all other source language
terms.
6
http://wp2txt.rubyforge.org/
4.3 Datasets
We evaluate our proposed method over three
datasets (two English, one German), as described
below.
4.3.1 English Noun Compounds (ENC)
Our first dataset is made up of 90 binary English
noun compounds, from the work of Reddy et al.
(2011). Each noun compound was annotated by
multiple annotators using the integer scale 0 (fully
non-compositional) to 5 (fully compositional). A
final compositionality score was then calculated
as the mean of the scores from the annotators.
If we simplistically consider 2.5 as the threshold
for compositionality, the dataset is relatively well
balanced, containing 48% compositional and 52%
non-compositional noun compounds. Following
Reddy et al. (2011), in combining the component-
wise distributional similarities for this dataset, we
weight the first component in Equation 1 higher
than the second (? = 0.7).
4.3.2 English Verb Particle Constructions
(EVPC)
The second dataset contains 160 English verb par-
ticle constructions (VPCs), from the work of Ban-
nard (2006). In this dataset, a verb particle con-
struction consists of a verb (the head) and a prepo-
sitional particle (e.g. hand in, look up or battle on).
For each component word (the verb and parti-
cle, respectively), multiple annotators were asked
whether the VPC entails the component word. In
order to translate the dataset into a regression task,
we calculate the overall compositionality as the
number of annotations of entailment for the verb,
divided by the total number of verb annotations for
that VPC. That is, following Bannard et al. (2003),
we only consider the compositionality of the verb
component in our experiments (and as such ? = 1
in Equation 1).
One area of particular interest with this dataset
will be the robustness of the method to function
words (the particles), both under translation and
in terms of calculating distributional similarity, al-
though the findings of Baldwin (2006) for English
prepositions are at least encouraging in this re-
spect. Additionally, English VPCs can occur in
?split? form (e.g. put your jacket on, from our
earlier example), which will complicate identifi-
cation, and the verb component will often be in-
flected and thus not match under our identification
strategy (for both VPCs and the component verbs).
476
Dataset Language Frequency Family
ENC
Italian 100 Romance
French 99 Romance
German 86 Germanic
Vietnamese 83 Viet-Muong
Portuguese 62 Romance
EVPC
Bulgarian 100 Slavic
Breton 100 Celtic
Occitan 100 Romance
Indonesian 100 Indonesian
Slovenian 100 Slavic
GNC
Polish 100 Slavic
Lithuanian 99 Baltic
Finnish 74 Uralic
Bulgarian 72 Slavic
Czech 40 Slavic
Table 1: The 5 best languages for the ENC, EVPC
and GNC datasets. The language family is based
on Voegelin and Voegelin (1977).
4.3.3 German Noun Compounds (GNC)
Our final dataset is made up of 246 German noun
compounds (von der Heide and Borgwaldt, 2009;
Schulte im Walde et al., 2013). Multiple anno-
tators were asked to rate the compositionality of
each German noun compound on an integer scale
of 1 (non-compositional) to 7 (compositional).
The overall compositionality score is then calcu-
lated as the mean across the annotators. Note that
the component words are provided as part of the
dataset, and that there is no need to perform de-
compounding. Following Schulte im Walde et al.
(2013), we weight the first component higher in
Equation 1 (? = 0.8) when calculating the overall
compositionality score.
This dataset is significant in being non-English,
and also in that German has relatively rich mor-
phology, which we expect to impact on the iden-
tification of both the MWE and the component
words.
5 Results
All experiments are carried out using 10 iterations
of 10-fold cross validation, randomly partitioning
the data independently on each of the 10 iterations,
and averaging across all 100 test partitions in our
presented results. In the case of CS
L2N
and other
methods that make use of it (i.e. CS
L1+L2N
and
CS
all
), the languages selected for a given training
fold are then used to compute the compositionality
scores for the instances in the test set. Figures 3a,
3b and 3c are histograms of the number of times
each N is selected over 100 folds on ENC, EVPC
and GNC datasets, respectively. From the his-
tograms, N = 6, N = 15 and N = 2 are the most
commonly selected settings for ENC, EVPC and
GNC, respectively. That is, multiple languages are
generally used, but more languages are used for
English VPCs than either of the compound noun
datasets. The 5 most-selected languages for ENC,
EVPC and GNC are shown in Table 1. As we
can see, there are some languages which are al-
ways selected for a given dataset, but equally the
commonly-selected languages vary considerably
between datasets.
Further analysis reveals that 32 (63%) target
languages for ENC, 25 (49%) target languages
for EVPC, and only 5 (10%) target languages for
GNC have a correlation of r ? 0.1 with gold-
standard compositionality judgements. On the
other hand, 8 (16%) target languages for ENC, 2
(4%) target languages for EVPC, and no target lan-
guages for GNC have a correlation of r ? ?0.1.
5.1 ENC Results
English noun compounds are relatively easy to
identify in a corpus,
7
because the components oc-
cur sequentially, and the only morphological vari-
ation is in noun number (singular vs. plural). In
other words, the precision for our token match-
ing method is very high, and the recall is also
acceptably high. Partly as a result of the ease
of identification, we get a high correlation of
r = 0.700 for CS
L1
(using only source language
data). Using only target languages (CS
L2N
), the
results drop to r = 0.434, but when we combine
the two (CS
L1+L2N
), the correlation is higher
than using only source or target language data, at
r = 0.725. When we combine all languages us-
ing SVR, the results rise slightly higher again to
r = 0.744, which is slightly above the correla-
tion of the state-of-the-art method of Salehi and
Cook (2013), which combines their method with
the method of Reddy et al. (2011) (CS
string+L1
).
These last two results support our hypothesis that
using translation data can improve the prediction
of compositionality. The results for string similar-
ity on its own (CS
string
, r = 0.644) are slightly
lower than those using only source language dis-
tributional similarity, but when combined with
7
Although see Lapata and Lascarides (2003) for discus-
sion of the difficulty of reliably identifying low-frequency
English noun compounds.
477
0 5 10 15 20 250
5
1015
2025
bestN
Frequency
(a) ENC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(b) EVPC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(c) GNC
Figure 3: Histograms displaying how many times a given N is selected as the best number of languages
over each dataset. For example, according to the GNC chart, there is a peak for N = 2, which shows
that over 100 folds, the best-2 languages achieved the highest correlation on 18 folds.
Method Summary of the Method ENC EVPC GNC
CS
L1
Source language 0.700 0.177 0.141
CS
L2N
Best-N target languages 0.434 0.398 0.113
CS
L1+L2N
Source + best-N target languages 0.725 0.312 0.178
CS
SVR(L1+L2 )
SVR (Source + all 51 target languages) 0.744 0.389 0.085
CS
string
String Similarity (Salehi and Cook, 2013) 0.644 0.385 0.372
CS
string+L1
CS
string
+CS
L1
(Salehi and Cook, 2013) 0.739 0.360 0.353
CS
all
CS
L1
+ CS
L2N
+ CS
string
0.732 0.417 0.364
Table 2: Pearson?s correlation on the ENC, EVPC and GNC datasets
CS
L1+L2N
(i.e. CS
all
) there is a slight rise in cor-
relation (from r = 0.725 to r = 0.732).
5.2 EVPC Results
English VPCs are hard to identify. As discussed
in Section 2, VPC components may not occur se-
quentially, and even when they do occur sequen-
tially, they may not be a VPC. As such, our sim-
plistic identification method has low precision and
recall (hand analysis of 927 identified VPC in-
stances would suggest a precision of around 74%).
There is no question that this is a contributor to
the low correlation for the source language method
(CS
L1
; r = 0.177). When we use target lan-
guages instead of the source language (CS
L2N
),
the correlation jumps substantially to r = 0.398.
When we combine English and the target lan-
guages (CS
L1+L2N
), the results are actually lower
than just using the target languages, because of
the high weight on the target language, which is
not desirable for VPCs, based on the source lan-
guage results. Even for CS
SVR(L1+L2 )
, the re-
sults (r = 0.389) are slightly below the target
language-only results. This suggests that when
predicting the compositionality of MWEs which
are hard to identify in the source language, it may
actually be better to use target languages only. The
results for string similarity (CS
string
: r = 0.385)
are similar to those for CS
L2N
. However, as with
the ENC dataset, when we combine string simi-
larity and distributional similarity (CS
all
), the re-
sults improve, and we achieve the state-of-the-art
for the dataset.
In Table 3, we present classification-based eval-
478
Method Precision Recall F-score (? = 1) Accuracy
Bannard et al. (2003) 60.8 66.6 63.6 60.0
Salehi and Cook (2013) 86.2 71.8 77.4 69.3
CS
all
79.5 89.3 82.0 74.5
Table 3: Results (%) for the binary compositionality prediction task on the EVPC dataset
uation over a subset of EVPC, binarising the com-
positionality judgements in the manner of Bannard
et al. (2003). Our method achieves state-of-the-art
results in terms of overall F-score and accuracy.
5.3 GNC Results
German is a morphologically-rich language, with
marking of number and case on nouns. Given
that we do not perform any lemmatization or other
language-specific preprocessing, we inevitably
achieve low recall for the identification of noun
compound tokens, although the precision should
be nearly 100%. Partly because of the resultant
sparseness in the distributional similarity method,
the results for CS
L1
are low (r = 0.141), al-
though they are lower again when using target lan-
guages (r = 0.113). However, when we combine
the source and target languages (CS
L1+L2N
) the
results improve to r = 0.178. The results for
CS
SVR(L1+L2 )
, on the other hand, are very low
(r = 0.085). Ultimately, simple string similar-
ity achieves the best results for the dataset (r =
0.372), and this result actually drops slightly when
combined with the distributional similarities.
To better understand the reason for the lacklus-
tre results using SVR, we carried out error analysis
and found that, unlike the other two datasets, about
half of the target languages return scores which
correlate negatively with the human judgements.
When we filter these languages from the data, the
score for SVR improves appreciably. For example,
over the best-3 languages overall, we get a corre-
lation score of r = 0.179, which is slightly higher
than CS
L1+L2N
.
We further investigated the reason for getting
very low and sometimes negative correlations with
many of our target languages. We noted that
about 24% of the German noun compounds in
the dataset do not have entries in Panlex. This
contrasts with ENC where only one instance does
not have an entry in Panlex, and EVPC where all
VPCs have translations in at least one language in
Panlex. We experimented with using string sim-
ilarity scores in the case of such missing transla-
tions, as opposed to the strategy described in Sec-
tion 4.2. The results for CS
SVR(L1+L2 )
rose to
r = 0.269, although this is still below the correla-
tion for just using string similarity.
Our results on the GNC dataset using string
similarity are competitive with the state-of-the-art
results (r = 0.45) using a window-based distribu-
tional similarity approach over monolingual Ger-
man data (Schulte im Walde et al., 2013). Note,
however, that their method used part-of-speech in-
formation and lemmatisation, where ours does not,
in keeping with the language-independent philos-
ophy of this research.
6 Conclusion and Future Work
In this study, we proposed a method to predict the
compositionality of MWEs based on monolingual
distributional similarity between the MWE and
each of its component words, under translation
into multiple target languages. We showed that
using translation and multiple target languages en-
hances compositionality modelling, and also that
there is strong complementarity between our ap-
proach and an approach based on string similarity.
In future work, we hope to address the ques-
tion of translation sparseness, as observed for the
GNC dataset. We also plan to experiment with un-
supervised morphological analysis methods to im-
prove identification recall, and explore the impact
of tokenization. Furthermore, we would like to in-
vestigate the optimal number of stop words and
content-bearing words for each language, and to
look into the development of general unsupervised
methods for compositionality prediction.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
479
References
Otavio Acosta, Aline Villavicencio, and Viviane Mor-
eira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Parsing and Generation to the Real
World, pages 101?109, Portland, USA.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37?
40, Beijing, China.
Timothy Baldwin. 2006. Distributional similarity and
preposition semantics. In Patrick Saint-Dizier, ed-
itor, Computational Linguistics Dimensions of Syn-
tax and Semantics of Prepositions, pages 197?210.
Springer, Dordrecht, Netherlands.
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proceedings of the ACL
2003 workshop on Multiword expressions: analysis,
acquisition and treatment-Volume 18, pages 65?72,
Sapporo, Japan.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 242?245, Los
Angeles, USA.
Ga?el Dias. 2003. Multiword unit hybrid extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 41?48, Sapporo, Japan.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):450?466.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of the 11th Conference of
the European Chapter for the Association of Compu-
tational Linguistics (EACL-2003), pages 235?242,
Budapest, Hungary.
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 317?324, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis, ac-
quisition and treatment-Volume 18, pages 73?80,
Sapporo, Japan.
Pavel Pecina. 2008. Lexical Association Measures:
Collocation Extraction. Ph.D. thesis, Faculty of
Mathematics and Physics, Charles University in
Prague, Prague, Czech Republic.
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2013), Seattle, USA.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66, Jeju Island, Ko-
rea.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
480
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly.
2012. Automatic identification of Persian light verb
constructions. In Proceedings of the 13th Inter-
national Conference on Intelligent Text Processing
Computational Linguistics (CICLing-2012), pages
201?210, New Delhi, India.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem. In Proceedings of the 6th
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 100?108,
Hong Kong, China.
Sabine Schulte im Walde, Stefan M?uller, and Stephen
Roller. 2013. Exploring vector space models to
predict the compositionality of German noun-noun
compounds. In Proceedings of the Second Joint
Conference on Lexical and Computational Seman-
tics, Atlanta, USA.
Hinrich Sch?utze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI Publications, Stanford, USA.
Alex J Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222.
Charles Frederick Voegelin and Florence Marie
Voegelin. 1977. Classification and index of the
world?s languages, volume 4. New York: Elsevier.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter, Basis und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages
51?74.
Julie Elizabeth Weeds. 2003. Measures and applica-
tions of lexical distributional similarity. Ph.D. the-
sis, University of Sussex.
481
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215?220,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Sense per Tweeter ... and Other Lexical Semantic Tales of Twitter
Spandana Gella, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In recent years, microblogs such as Twit-
ter have emerged as a new communication
channel. Twitter in particular has become
the target of a myriad of content-based
applications including trend analysis and
event detection, but there has been little
fundamental work on the analysis of word
usage patterns in this text type. In this
paper ? inspired by the one-sense-per-
discourse heuristic of Gale et al. (1992)
? we investigate user-level sense distri-
butions, and detect strong support for ?one
sense per tweeter?. As part of this, we con-
struct a novel sense-tagged lexical sample
dataset based on Twitter and a web corpus.
1 Introduction
Social media applications such as Twitter enable
users from all over the world to create and share
web content spontaneously. The resulting user-
generated content has been identified as having
potential in a myriad of applications including
real-time event detection (Petrovi?c et al., 2010),
trend analysis (Lau et al., 2012) and natural dis-
aster response co-ordination (Earle et al., 2010).
However, the dynamism and conversational na-
ture of the text contained in social media can
cause problems for traditional NLP approaches
such as parsing (Baldwin et al., 2013), mean-
ing that most content-based approaches use sim-
ple keyword search or a bag-of-words representa-
tion of the text. This paper is a first step towards
full lexical semantic analysis of social media text,
in investigating the sense distribution of a range
of polysemous words in Twitter and a general-
purpose web corpus.
The primary finding of this paper is that there
are strong user-level lexical semantic priors in
Twitter, equivalent in strength to document-level
lexical semantic priors, popularly termed the ?one
sense per discourse? heuristic (Gale et al., 1992).
This has potential implications for future applica-
tions over Twitter which attempt to move beyond a
simple string-based meaning representation to ex-
plicit lexical semantic analysis.
2 Related Work
The traditional approach to the analysis of word-
level lexical semantics is via word sense dis-
ambiguation (WSD), where usages of a given
word are mapped onto discrete ?senses? in a pre-
existing sense inventory (Navigli, 2009). The most
popular sense inventory used in WSD research has
been WordNet (Fellbaum, 1998), although its fine-
grained sense distinctions have proven to be diffi-
cult to make for human annotators and WSD sys-
tems alike. This has resulted in a move towards
more coarse-grained sense inventories (Palmer et
al., 2004; Hovy et al., 2006; Navigli et al., 2007),
or alternatively away from pre-existing sense in-
ventories altogether, towards joint word sense in-
duction (WSI) and disambiguation (Navigli and
Vannella, 2013; Jurgens and Klapaftis, 2013).
Two heuristics that have proven highly powerful
in WSD and WSI research are: (1) first sense tag-
ging, and (2) one sense per discourse. First sense
tagging is based on the observation that sense dis-
tributions tend to be Zipfian, such that if the pre-
dominant or ?first? sense can be identified, simply
tagging all occurrences of a given word with this
sense can achieve high WSD accuracy (McCarthy
et al., 2007). Unsurprisingly, there are significant
differences in sense distributions across domains
(cf. cloud in the COMPUTING and METEOROLOG-
ICAL domains), motivating the need for unsuper-
vised first sense learning over domain-specific cor-
pora (Koeling et al., 2005).
One sense per discourse is the observation that
a given word will often occur with a single sense
across multiple usages in a single document (Gale
215
et al., 1992). Gale et al. established the heuristic
on the basis of 9 ambiguous words using a coarse-
grained sense inventory, finding that the probabil-
ity of a given pair of usages of a word taken from a
given document having the same sense was 94%.
However, Krovetz (1998) found that for a fine-
grained sense inventory, only 67% of words exhib-
ited the single-sense-per-discourse property for all
documents in a corpus.
A radically different view on WSD is word us-
age similarity, whereby two usages of a given
word are rated on a continuous scale for similar-
ity, in isolation of any sense inventory (Erk et al.,
2009). Gella et al. (2013) constructed a word us-
age similarity dataset for Twitter messages, and
developed a topic modelling approach to the task,
building on the work of Lui et al. (2012). To the
best of our knowledge, this has been the only at-
tempt to carry out explicit word-level lexical se-
mantic analysis of Twitter text.
3 Dataset Construction
In order to study sense distributions of words in
Twitter, we need a sense inventory to annotate
against, and also a set of Twitter messages to an-
notate. Further, as a point of comparison for the
sense distributions in Twitter, we require a second
corpus; here we use the ukWaC (Ferraresi et al.,
2008), a corpus built from web documents.
For the sense inventory, we chose the Macmil-
lan English Dictionary Online
1
(MACMILLAN,
hereafter), on the basis of: (1) its coarse-grained
general-purpose sense distinctions, and (2) its reg-
ular update cycle (i.e. it contains many recently-
emerged senses). These criteria are important
in terms of inter-annotator agreement (especially
as we crowdsourced the sense annotation, as de-
scribed below) and also sense coverage. The
other obvious candidate sense inventory which po-
tentially satisfied these criteria was ONTONOTES
(Hovy et al., 2006), but a preliminary sense-
tagging exercise indicated that MACMILLAN bet-
ter captured Twitter-specific usages.
Rather than annotating all words, we opted for
a lexical sample of 20 polysemous nouns, as listed
in Table 1. Our target nouns were selected to span
the high- to mid-frequency range in both Twitter
and the web corpus, and have at least 3 MACMIL-
LAN senses. The average sense ambiguity is 5.5.
1
http://www.macmillandictionary.com
band bar case charge deal
degree field form function issue
job light match panel paper
position post rule sign track
Table 1: The 20 target nouns used in this research
3.1 Data Sampling
We sampled tweets from a crawl made using the
Twitter Streaming API from January 3, 2012 to
February 29, 2012. The web corpus was built from
ukWaC (Ferraresi et al., 2008), which was based
on a crawl of the .uk domain from 2007. In con-
trast to ukWaC, the tweets are not restricted to doc-
uments from any particular country.
For both corpora, we first selected only the
English documents using langid.py, an off-the-
shelf language identification tool (Lui and Bald-
win, 2012). We next identified documents which
contained nominal usages of the target words,
based on the POS tags supplied with the corpus
in the case of ukWaC, and the output of the CMU
ARK Twitter POS tagger v2.0 (Owoputi et al.,
2012) in the case of Twitter.
For Twitter, we are interested in not just the
overall lexical distribution of each target noun,
but also per-user lexical distributions. As such,
we construct two Twitter-based datasets: (1)
TWITTER
RAND
, a random sample of 100 usages of
each target noun; and (2) TWITTER
USER
, 5 usages
of each target noun from each member of a ran-
dom sample of 20 Twitter users. Naively select-
ing users for TWITTER
USER
without filtering re-
sulted in a preponderance of messages from ac-
counts that were clearly bots, e.g. from commer-
cial sites with a single post per item advertised for
sale, with artificially-skewed sense distributions.
In order to obtain a more natural set of messages
from ?real? people, we introduced a number of
user-level filters, including removing users who
posted the same message with different user men-
tions or hashtags, and users who used the target
nouns more than 50 times over a 2-week period.
From the remaining users, we randomly selected
20 users per target noun, resulting in 20 nouns ?
20 users ? 5 messages = 2000 messages.
For ukWaC, we similarly constructed two
datasets: (1) UKWAC
RAND
, a random sample
of 100 usages of each target noun; and (2)
UKWAC
DOC
, 5 usages of each target noun from 20
documents which contained that noun in at least
216
Figure 1: Screenshot of a sense annotation HIT for position
5 sentences. 5 such sentences were selected for
annotation, resulting in a total of 20 nouns ? 20
documents ? 5 sentences = 2000 sentences.
3.2 Annotation Settings
We sense-tagged each of the four datasets using
Amazon Mechanical Turk (AMT). Each Human
Intelligence Task (HIT) comprised 5 occurrences
of a given target noun, with the target noun high-
lighted in each. Sense definitions and an exam-
ple sentence (where available) were provided from
MACMILLAN. Turkers were free to select multi-
ple sense labels where applicable, in line with best
practice in sense labelling (Mihalcea et al., 2004).
We also provided an ?Other? sense option, in cases
where none of the MACMILLAN senses were ap-
plicable to the current usage of the target noun. A
screenshot of the annotation interface for a single
usage is provided in Figure 1.
Of the five sentences in each HIT, one was a
heldout example sentence for one of the senses of
the target noun, taken from MACMILLAN. This
gold-standard example was used exclusively for
quality assurance purposes, and used to filter the
annotations as follows:
1. Accept all HITs from Turkers whose gold-
standard tagging accuracy was ? 80%;
2. Reject all HITs from Turkers whose gold-
standard tagging accuracy was ? 20%;
3. Otherwise, accept single HITs with correct
gold-standard sense tags, or at least 2/4 (non-
gold-standard) annotations in common with
Turkers who correctly annotated the gold-
standard usage; reject any other HITs.
This style of quality assurance has been shown
to be successful for sense tagging tasks on AMT
(Bentivogli et al., 2011; Vuurens et al., 2011), and
resulted in us accepting around 95% of HITs.
In total, the annotation was made up of 500
HITs (= 2000/4 usages per HIT) for each of the
four datasets, each of which was annotated by
5 Turkers. Our analysis of sense distribution is
based on only those HITs which were accepted in
accordance with the above methodology, exclud-
ing the gold-standard items. We arrive at a single
sense label per usage by unweighted voting across
the annotations, allowing multiple votes from a
single Turker in the case of multiple sense annota-
tions. In this, the ?Other? sense label is considered
as a discrete sense label.
Relative to the majority sense, inter-annotator
agreement post-filtering was respectably high in
terms of Fleiss? kappa at ? = 0.64 for both
UKWAC
RAND
and UKWAC
DOC
. For TWITTER
USER
,
the agreement was actually higher at ? = 0.71, but
for TWITTER
RAND
it was much weaker, ? = 0.47.
All four datasets have been released for pub-
lic use: http://www.csse.unimelb.edu.au/
~
tim/etc/twitter_sense.tgz.
4 Analysis
In TWITTER
USER
, the proportion of users who used
a target noun with one sense across all 5 usages
ranged from 7/20 for form to 20/20 for degree, at
an average of 65%. That is, for 65% of users, a
given noun (with average polysemy = 5.5 senses)
is used with the same sense across 5 separate mes-
sages. For UKWAC
DOC
the proportion of docu-
ments with a single sense of a given target noun
217
Partition Agreement (%)
Gale et al. (1992) document 94.4
TWITTER
USER
user 95.4
TWITTER
USER
? 62.9
TWITTER
RAND
? 55.1
UKWAC
DOC
document 94.2
UKWAC
DOC
? 65.9
UKWAC
RAND
? 60.2
Table 2: Pairwise agreement for each dataset,
based on different partitions of the data (??? indi-
cates no partitioning, and exhaustive comparison)
across all usages ranged from 1/20 for case to
20/20 for band, at an average of 63%. As such,
the one sense per tweeter heuristic is at least as
strong as the one sense per discourse heuristic in
UKWAC
DOC
.
Looking back to the original work of Gale et
al. (1992), it is important to realise that their re-
ported agreement of 94% was calculated pairwise
between usages in a given document. When we
recalculate the agreement in TWITTER
USER
and
UKWAC
DOC
using this methodology, as detailed
in Table 2 (calculating pairwise agreement within
partitions of the data based on ?user? and ?docu-
ment?, respectively), we see that the numbers for
our datasets are very close to those of Gale et al.
on the basis of more than twice as many nouns,
and many more instances per noun. Moreover, the
one sense per tweeter trend again appears to be
slightly stronger than the one sense per discourse
heuristic in UKWAC
DOC
.
One possible interpretation of these results is
that they are due to a single predominant sense,
common to all users/documents rather than user-
specific predominant senses. To test this hy-
pothesis, we calculate the pairwise agreement for
TWITTER
USER
and UKWAC
DOC
across all anno-
tations (without partitioning on user/document),
and also for TWITTER
RAND
and UKWAC
RAND
.
The results are, once again, presented in Ta-
ble 2 (with partition indicated as ??? for the
respective datasets), and are substantially lower
in all cases (< 66%). This indicates that the
first sense preference varies considerably between
users/documents. Note that the agreement is
slightly lower for TWITTER
RAND
and UKWAC
RAND
simply because of the absence of the biasing effect
for users/documents.
Comparing TWITTER
RAND
and UKWAC
RAND
,
there were marked differences in first sense pref-
erences, with 8/20 of the target nouns having a
different first sense across the two corpora. One
surprising observation was that the sense distri-
butions in UKWAC
RAND
were in general more
skewed than in TWITTER
RAND
, with the entropy of
the sense distribution being lower (= more biased)
in UKWAC
RAND
for 15/20 of the target nouns.
All datasets included instances of ?Other?
senses (i.e. usages which didn?t conform to any
of the MACMILLAN senses), with the highest rel-
ative such occurrence being in TWITTER
RAND
at
12.3%, as compared to 6.6% for UKWAC
RAND
.
Interestingly, the number of such usages in
the user/document-biased datasets was around
half these numbers, at 7.4% and 3.6% for
TWITTER
USER
and UKWAC
DOC
, respectively.
5 Discussion
It is worthwhile speculating why Twitter users
would have such a strong tendency to use a given
word with only one sense. This could arise in
part due to patterns of user behaviour, in a given
Twitter account being used predominantly to com-
ment on a favourite sports team or political events,
and as such is domain-driven. Alternatively, it can
perhaps be explained by the ?reactive? nature of
Twitter, in that posts are often emotive responses
to happenings in a user?s life, and while different
things excite different individuals, a given individ-
ual will tend to be excited by events of similar
kinds. Clearly more research is required to test
these hypotheses.
One highly promising direction for this research
would be to overlay analysis of sense distributions
with analysis of user profiles (e.g. Bergsma et al.
(2013)), and test the impact of geospatial and soci-
olinguistic factors on sense preferences. We would
also like to consider the impact of time on the one
sense per tweeter heuristic, and consider whether
?one sense per Twitter conversation? also holds.
To summarise, we have investigated sense dis-
tributions in Twitter and a general web corpus,
over both a random sample of usages and a sample
of usages from a single user/document. We found
strong evidence for Twitter users to use a given
word with a single sense, and also that individual
first sense preferences differ between users, sug-
gesting that methods for determining first senses
on a per user basis could be valuable for lexical se-
mantic analysis of tweets. Furthermore, we found
that sense distributions in Twitter are overall less
skewed than in a web corpus.
218
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Luisa Bentivogli, Marcello Federico, Giovanni
Moretti, and Michael Paul. 2011. Getting expert
quality from the crowd for machine translation
evaluation. Proceedings of the MT Summmit,
13:521?528.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
1010?1019, Atlanta, USA.
Paul Earle, Michelle Guy, Richard Buckmaster, Chris
Ostrum, Scott Horvath, and Amy Vaughan. 2010.
OMG earthquake! can Twitter improve earth-
quake response? Seismological Research Letters,
81(2):246?251.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing
(ACL-IJCNLP 2009), pages 10?18, Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Mo-
rocco.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233?237.
Spandana Gella, Paul Cook, and Bo Han. 2013. Unsu-
pervised word usage similarity in social media texts.
In Proceedings of the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 248?253, Atlanta, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Robert Krovetz. 1998. More than one sense per dis-
course. NEC Princeton NJ Labs., Research Memo-
randum.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012. On-line trend analysis with topic models:
#twitter trends detection topic model online. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages
33?41, Dunedin, New Zealand.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25?28, Barcelona,
Spain.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
219
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Sasa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to twitter. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 181?189, Los Angeles, USA.
Jeroen Vuurens, Arjen P de Vries, and Carsten Eick-
hoff. 2011. How much spam can you take? an anal-
ysis of crowdsourcing results to increase accuracy.
In Proc. ACM SIGIR Workshop on Crowdsourcing
for Information Retrieval (CIR 2011), pages 21?26.
220
Automatically Identifying the Source Words
of Lexical Blends in English
Paul Cook?
University of Toronto
Suzanne Stevenson??
University of Toronto
Newly coined words pose problems for natural language processing systems because they are not
in a system?s lexicon, and therefore no lexical information is available for such words. A common
way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and
pharmaceutical. We propose a statistical model for inferring a blend?s source words drawing on
observed linguistic properties of blends; these properties are largely based on the recognizability
of the source words in a blend. We annotate a set of 1,186 recently coined expressions which
includes 515 blends, and evaluate our methods on a 324-item subset. In this first study of
novel blends we achieve an accuracy of 40% on the task of inferring a blend?s source words,
which corresponds to a reduction in error rate of 39% over an informed baseline. We also
give preliminary results showing that our features for source word identification can be used
to distinguish blends from other kinds of novel words.
1. Lexical Blends
Neologisms?newly coined words or new senses of an existing word?are constantly
being introduced into a language (Algeo 1980; Lehrer 2003), often for the purpose
of naming a new concept. Domains that are culturally prominent or that are rapidly
advancing, such as electronic communication and the Internet, often contain many ne-
ologisms, although novel words arise throughout a language (Ayto 1990, 2006; Knowles
and Elliott 1997). Consequently, any natural language processing (NLP) system operat-
ing on recently produced text will encounter new words. Because lexical resources are
often a key component of an NLP system, performance of the entire system will likely
suffer due to missing lexical information for neologisms. Ideally, an NLP system could
identify neologisms as such, and then infer various aspects of their syntactic or seman-
tic properties necessary for the computational task at hand. Recent approaches to this
kind of lexical acquisition task typically infer the target lexical information from sta-
tistical distributional properties of the terms. However, this technique is generally not
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada, E-mail: pcook@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada, E-mail: suzanne@cs.toronto.edu.
Submission received: 4 November 2008; revised submission received: 23 May 2009; accepted for publication:
24 June 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
applicable to neologisms, which are relatively infrequent due to their recent intro-
duction into the language. Fortunately, linguistic observations regarding neologisms?
namely, that they are formed through specific word formation processes?can give
insights for automatically learning their lexical properties.
New words come about through a variety of means, including derivational mor-
phology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983;
Plag 2003). Computational work on neologisms has largely focused on particular word
formation processes, and has exploited information about the formation process to
learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney
2005; Baker and Brew 2008, for example). Subtractive word formations?words formed
from partial orthographic or phonological content from existing words?have received
a fair amount of attention recently in computational linguistics, particularly under the
heading of inferring the long form of acronyms, especially in the bio-medical domain
(e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou
2006, for example).
Lexical blends?the focus of this study?also known as blends, are another common
type of subtractive word formation. Most blends are formed by combining a prefix
of one source word with a suffix of another source word, as in brunch (breakfast and
lunch). There may be overlap in the contribution of the source words, as in fantabulous
( fantastic and fabulous). It is also possible that one or both source words are entirely
present, for example, gaydar (gay radar) and jetiquette ( jet etiquette). We refer to blends
such as these as simple two-word sequential blends, and focus on this common type
of blend in this article. Blends in which (part of) a word is inserted within another
(e.g., entertoyment, a blend of entertainment and toy) and blends formed from more than
two source words (e.g., nofriendo from no, friend, and Nintendo) are rare. In Algeo?s
(1991) study of new words, approximately 5% were blends. However, in our analysis
of 1,186 words taken from a popular neologisms Web site, approximately 43% were
blends. Clearly, computational techniques are needed that can augment lexicons with
knowledge of novel blends.
The precise nature and intended use of a computational lexicon will determine
the degree of processing required of a novel blend. In some cases it may suffice for
the lexical entry for a blend to simply consist of its source words. For example, a
system that employs a measure of distributional similarity may benefit from replacing
occurrences of a blend?likely a recently coined and hence low frequency item?by its
source words, for which distributional information is likely available. In other cases,
further semantic reasoning about the blend and its source words may be required (e.g.,
determining the semantic relationship between the source words as an approximation
of the meaning of the blend). However, any approach to handling blends will need
to recognize that a novel word is a blend and identify its source words. These two
tasks are the focus of this article. Specifically, we draw on linguistic knowledge of how
blends are formed as the basis for automatically determining the source words of a
blend. Language users create blends that tend to be interpretable by others. Tapping into
properties of blends believed to contribute to the recognizability of their source words?
and hence the interpretability of the resulting blend?we develop statistical measures
that indicate whether a word pair is likely the source words for a given blend. Moreover,
the fact that a novel word is determined to have a ?good? source word pair may be
evidence that it is in fact a blend, because we are unlikely to find two words that are a
?good? source word pair for a non-blend. Thus, the statistical measures we develop
for source word identification may also be useful in recognizing a novel word as a
blend.
130
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
To our knowledge, the only computational treatment of blends is our earlier work
that presents preliminary statistical methods and results for the two tasks of recognizing
an unknown word as a blend and identifying its source words (Cook and Stevenson
2007). Here we extend that work in a number of important directions. We expand the
statistical features to better capture co-occurrence patterns of the source words that
can indicate the likelihood of their combination into a blend. We present experimental
results confirming that the extended features provide a substantial improvement over
the earlier work on source word identification. We further propose a means, based on
linguistic factors of the source words, for pruning the number of word pairs that are
considered for a blend. This filtering heuristic greatly reduces the number of candidate
source words processed, while giving modest gains in performance, even though this
method excludes the correct word pair from consideration for a number of blends. We
then consider the use of a much larger lexicon of candidate source words, which could
potentially improve performance greatly as it contains the correct source word pair for
many more blends than a smaller lexicon.
We also make improvements to the earlier experimental data set and methods.
In our earlier study, we use a data set consisting of a list of blends extracted from
a dictionary. In the current work, we annotate a set of 324 blends (with their source
words) from a recent database of neologisms, to enable a more legitimate testing of
our method, on truly novel blends. Experiments on this new data set show that the
recent blends differ from established blends in terms of their statistical properties,
and emphasize the need for further resources of neologisms. We also experiment
with a machine learning approach to combine the information from the statistical
features in a more sophisticated manner than in our previous work. Finally, we per-
form more extensive experiments on distinguishing blends from other kinds of novel
words.
2. A Statistical Model of Lexical Blends
We present statistical features that are used to automatically infer the source words of
a word known to be a lexical blend, and show that the same features can be used to
distinguish blends from other types of neologisms. First, given a blend, we generate
all word pairs that could have formed the blend. This set is termed the candidate set,
and the word pairs it contains are referred to as candidate pairs (Section 2.1). Next, we
extract a number of linguistically motivated statistical features for each candidate pair,
as well as filter from the candidate sets those pairs that are unlikely to be source words
due to their linguistic properties (Section 2.2). Later, we explain how we use the features
to rank the candidate pairs according to how likely they are the source words for that
blend. Interestingly, the ?goodness? of a candidate pair is also related to how likely the
word is actually a blend.
2.1 Candidate Sets
To create the candidate set for a blend, we first generate each prefix?suffix pair such that
the blend is composed of the prefix followed by the suffix. (In this work, prefix and suffix
refer to the beginning or ending of a string, regardless of whether those portions are
affixes.) We restrict the prefixes and suffixes to be of length two or more. This heuristic
reduces the size of the candidate sets, yet generally does not exclude a blend?s source
131
Computational Linguistics Volume 36, Number 1
words from its candidate set since it is uncommon for a source word to contribute less
than two letters. For example, for brunch (breakfast+lunch) we consider the following
prefix?suffix pairs: br, unch; bru, nch; brun, ch. For each prefix?suffix pair, we then find in
a lexicon all words beginning with the prefix and all words ending in the suffix, ignoring
hyphens and whitespace, and take the Cartesian product of the prefix words and suffix
words to form a list of candidate word pairs. The candidate set for the blend is the union
of the candidate word pairs for all its prefix?suffix pairs. Note that in this example, the
candidate pair brute crunch would be included twice: once for the prefix?suffix pair br,
unch; and once again for bru, nch. Unlike in our previous study, we remove all such
duplicate pairs from the final candidate set. A candidate set for architourist, a blend of
architecture and tourist, is given in Table 1.
2.2 Statistical Features
Our statistical features are motivated by properties of blends observed in corpus-based
studies, and by cognitive factors in human interpretation of blends, particularly relating
to how easily humans can recognize a blend?s source words. All the features are formu-
lated to give higher values for more likely candidate pairs. We organize the features into
four groups?frequency; length, contribution, and phonology; semantics; and syllable
structure?and describe each feature group in the following subsections.
2.2.1 Frequency. Various frequency properties of the source words influence how easily a
language user recognizes the words that form a blend. Because blends are most usefully
coined when the source words can be readily deduced, we hypothesize that frequency-
based features will be useful in identifying blends and their source words. We propose
ten features that draw on the frequency of candidate source words.
Lehrer (2003) presents a study in which humans are asked to give the source
words for blends. Among her findings are that frequent source words are more
easily recognizable. Our first two features?the frequency of each candidate word,
freq(w1) and freq(w2)?reflect this finding. Lehrer also finds that the recognizability of
a source word is further affected by both the number of words in its neighborhood?
the set of words which begin/end with the prefix/suffix which that source word
Table 1
A candidate set for architourist, a blend of architecture and tourist.
archimandrite tourist
archipelago tourist
architect behaviourist
architect tourist
architectural behaviourist
architectural tourist
architecturally behaviourist
architecturally tourist
architecture behaviourist
architecture tourist
archives tourist
archivist tourist
132
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
contributes?and the frequencies of those words. (Gries [2006] reports a similar finding.)
Our next two features capture this insight:
freq(w1)
freq(prefix)
freq(w2)
freq(suffix)
(1)
where freq(prefix) is the sum of the frequency of all words beginning with prefix, and
similarly for freq(suffix).
These four features were used in our previous study; the following six features are
new in this study.
Because we observe that blends are often formed from two words that co-occur
in language use, our previous study (Cook and Stevenson 2007) included a feature,
the pointwise mutual information of w1 and w2, to reflect this. However, this feature
provides only a weak indication that there is a semantic relation between two words
sufficient to lead to them being blended. Here we propose six new features that capture
various co-occurrence frequencies as follows.
A blend?s source words often correspond to a common sequence of words, for
example, camouflanguage is camouflaged language. We therefore include two features
based on Dice?s co-efficient to capture the frequency with which the source words occur
consecutively:
2 ? freq(w1 w2)
freq(w1)+ freq(w2)
2 ? freq(w2 w1)
freq(w1)+ freq(w2)
(2)
Because many blends can be paraphrased by a conjunctive phrase?for example, brocco-
flower is broccoli and cauliflower?we also use a feature that reflects how often the candi-
date words are used in this way:
2 ? ( freq(w1 and w2)+ freq(w2 and w1))
freq(w1 and)+ freq(and w1)+ freq(w2 and)+ freq(and w2)
(3)
Furthermore, some blends can be paraphrased by a noun modified by a prepositional
phrase, for example, a nicotini is a martini with nicotine. Lauer (1995) suggests eight
prepositional paraphrases for identifying the semantic relationship between the modi-
fier and head in a noun compound. Using the same paraphrases, the following feature
measures how often two candidate source words occur with any of the following
prepositions P between them: about, at, for, from, in, of, on, with:
2 ? ( freq(w1 P w2)+ freq(w2 P w1))
freq(w1 P)+ freq(P w1)+ freq(w2 P)+ freq(P w2)
(4)
where freq(w P v) is the sum of the frequency of w and v occurring with each of the eight
prepositions between w and v, and freq(w P) is the sum of the frequency of w occurring
with each of the eight prepositions immediately following w.
133
Computational Linguistics Volume 36, Number 1
Because the previous three features target the source words occurring in very spe-
cific patterns, we also count the candidate source words occurring in any of the patterns
in an effort to avoid data sparseness problems.
2 ? ( freq(w1 w2)+ freq(w2 w1)+ freq(w1 and w2)
+freq(w2 and w1)+ freq(w1 P w2)+ freq(w2 P w1))
freq(w1)+ freq(w2)
(5)
Finally, because the above patterns are very specific, and do not capture general co-
occurrence information which may also be useful in identifying a blend?s source words,
we include the following feature which counts the candidate source words co-occurring
within a five-word window.
2 ? freq(w1,w2 in a 5 word window)
freq(w1)+ freq(w2)
(6)
2.2.2 Length, Contribution, and Phonology. Ten features tap into properties of the ortho-
graphic or phonetic composition of the source words and blend. In our previous work
on blends, we found such features unhelpful in source word identification. Here, we
propose revised versions of our old features, and a few new ones. Note that although we
use information about the phonological and/or syllabic structure of the source words,
we do not assume such knowledge for the blend itself, since it is a neologism for which
such lexical information is typically unavailable.
The first word in a conjunct tends to be shorter than the second, and this also seems
to be the case for the source words in blends (Kelly 1998; Gries 2004). The first three
features therefore capture this tendency based on the graphemic, phonemic, and syllabic
length of w2 relative to w1, respectively:
lengraphemes(w2)
lengraphemes(w1)+ lengraphemes(w2)
(7)
lenphonemes(w2)
lenphonemes(w1)+ lenphonemes(w2)
(8)
lensyllables(w2)
lensyllables(w1)+ lensyllables(w2)
(9)
A blend and its second source word also tend to be similar in length, possibly because,
similar to compounds, the second source word of a blend is often the head; therefore
it is this word that determines the overall phonological structure of the resulting blend
(Kubozono 1990). The following feature captures this property using graphemic length
134
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
as an approximation to phonemic length, because as stated previously, we assume no
phonological information about the blend.
1 ?
|lengraphemes(blend) ? lengraphemes(w2)|
max(lengraphemes(blend), lengraphemes(w2))
(10)
We hypothesize that a candidate source word is more likely if it contributes more
graphemes to a blend. We use two ways to measure contribution in terms of graph-
emes: contseq(w, b) is the length of the longest prefix/suffix of word w which blend b
begins/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and
b. This yields four features:
contseq(w1, b)
lengraphemes(w1)
contseq(w2, b)
lengraphemes(w2)
(11)
contlcs(w1, b)
lengraphemes(w1)
contlcs(w2, b)
lengraphemes(w2)
(12)
Note that for some blends, such as spamdex (spam index), contseq and contlcs will be equal;
however, this is not the case in general, as in the blend tomacco (tomato and tobacco) in
which tomato overlaps with the blend not only in its prefix toma, but also in the final o.
In order to be recognizable in a blend, the shorter source word will tend to con-
tribute more material, relative to its length, than the longer source word (Gries 2004).
We formulate the following feature which is positive only when this is the case:
(
contseq(w1, b)
lengraphemes(w1)
?
contseq(w2, b)
lengraphemes(w2)
)
?
(
lengraphemes(w2) ? lengraphemes(w1)
lengraphemes(w1)+ lengraphemes(w2)
)
(13)
For this feature we don?t have strong motivation for choosing one measure of contribu-
tion over the other, and therefore use contseq, the simpler version of contribution.
Finally, the source words in a blend are often phonologically similar, as in sheeple
(sheep people); the following feature captures this (Gries 2006):
LCSphonemes(w1,w2) (14)
2.2.3 Semantics. We include two semantic features from our previous study that are
based on Lehrer?s (2003) observation that people can more easily identify the source
words of a blend when there is a semantic relation between them.
As noted, blends are often composed of two semantically similar words, reflecting a
conjunction of their concepts. For example, a pug and a beagle are both a kind of dog, and
can be combined to form the blend puggle. Similarly an exergame is a blend of exercise and
game, both of which are types of activity. Our first semantic feature captures similarity
using an ontological similarity measure, which is calculated over an ontology populated
with word frequencies from a corpus.
The source words of some blends are not semantically similar (in the sense of their
relative positions within an ontology), but are semantically related. For example, the
source words of slanguist?slang and linguist?are related in that slang is a type of lan-
guage and a linguist studies language. Our second semantic feature is a measure of se-
mantic relatedness using distributional similarity between word co-occurrence vectors.
135
Computational Linguistics Volume 36, Number 1
2.2.4 Syllable Structure. Kubozono (1990) notes that the split of a source word?into the
prefix/suffix it contributes to the blend and the remainder of the word?occurs at a
syllable boundary or immediately after the onset of the syllable. Because this syllable
structure property holds sufficiently often, we use it as a filter over candidate pairs
(rather than as an additional statistical feature) in an effort to reduce the size of the
candidate sets. Candidate sets can be very large, and we expect that our features will
be more successful at selecting the correct source word pair from a smaller candidate
set. In our subsequent results, we analyze the reduction in candidate set size using this
syllable structure heuristic, and its impact on performance.
3. Creating a Data Set of Recent Blends
The data set used in our previous work on blends contains dictionary words whose
etymological entry indicates they were formed from a blend of two words. Using a
dictionary in this way provides an objective method for selecting experimental expres-
sions and indicating their gold standard source words. However, it results in a data set
of blends that are sufficiently established in the language to appear in a dictionary. Truly
novel blends?neologisms which have been recently added to the language?may have
differing properties from fully established forms in a dictionary. In particular, many
of our features are based on properties of the source words, both individually and in
relation to each other, that may not hold for expressions that entered the language some
time ago. For example, although meld is a blend of melt and weld, the current frequency
of the phrase melt and weld may not be as common as the source word co-occurrences
for newly coined expressions. Thus, an important step to support further research
on blends is to develop a data set of recent neologisms that are judged to be lexical
blends.
To develop a data set of recently coined blends we drew on www.wordspy.com, a
popular Web site documenting English neologisms (and a small number of rare or
specialized terms) that have been recently used in a recordable medium such as a
newspaper or book, and that (typically) are not found in currently available dictionaries.
A (partial) sample entry from Wordspy is given in Table 2. The words on this Web
site satisfy our goal of being new; however, they include many kinds of neologisms,
not just blends. We thus annotated the data set to identify the blends and their source
Table 2
The Wordspy definition, and first citation given, for the blend staycation.
staycation n. A stay-at-home vacation. Also: stay-cation.
?staycationer n.
Example Citation:
Amy and Adam Geurden of Hollandtown, Wis., had planned a long summer of short,
fun getaways with their kids, Eric, 6, Holly, 3, and Jake, 2. In the works were water-park
visits, roller-coaster rides, hiking adventures and a whirlwind weekend in Chicago.
Then Amy did the math: their Chevy Suburban gets 17 miles to the gallon and, with gas
prices topping $4, the family would have spent about $320 on fill-ups alone. They?ve
since scrapped their plans in favor of a ?staycation? around the backyard swimming
pool.
?Linda Stern, ?Try Freeloading Off Friends!,? Newsweek, May 26, 2008
136
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
words. (In cases where multiple source words were found to be equally acceptable, all
source words judged to be valid were included in the annotation.) Most expressions
in Wordspy include both a definition and an example usage, making the task fairly
straightforward.
As of 17 July 2008 Wordspy contained 1,186 single word entries. One author anno-
tated each of these words as a blend or not a blend, and indicated the source words
for each blend. To ensure validity of the annotation task, the other author similarly
annotated 100 words randomly sampled from the 1,186. On this subset of 100 words, ob-
served agreement on both the blend/non-blend annotation and the component source
word identification was 92%, with an unweighted Kappa score of .84. On four blends,
the judges gave different variants of the same source word; for example, fuzzy buzzword
and fuzz buzzword for the blend fuzzword. These items were counted as agreements, and
all variants were considered correct source words.
Given the high level of agreement between the annotators, only one person anno-
tated all 1,186 items. A total of 515 words were judged to be blends, with 351 being
simple two-word sequential blends whose source words are not proper nouns (this
latter type of blend being the focus of this study). Table 3 shows the variety of blends
encountered in the Wordspy data, organized according to a categorization scheme we
devised. Of the simple two-word sequential blends, we restrict our experimental data
set to the 324 items whose entries included a citation of their usage, as we have evidence
that they have in fact been used; moreover, such blends may be less likely to be nonce
formations?expressions which are used once but do not become part of the language.
The usage data in the citations can also be used in the future for semantic features based
on contextual information. We refer to this new data set of 324 items as WORDSPLEND
(a blend of Wordspy and blend).
4. Materials and Methods
4.1 Experimental Expressions
The data set used in our previous study of blends consisted of expressions from the
Macquarie Dictionary (Delbridge 1981) with an etymology entry indicating that they
are blends. All of our statistical features were devised using the development portion of
this data set, enabling us to use the full WORDSPLEND data set for testing. To compare
our results to those in our earlier study, we also perform experiments on a subset of
the previous data set. We are uncertain as to whether a number of the blends from the
Macquarie Dictionary are in fact blends. For example, it does not match our intuition
that clash is a blend of clap and dash. We created a second data set of confirmed blends,
MAC-CONF, consisting of only those blends from Macquarie that are found in at least
one of two additional dictionaries with an etymology entry indicating that they are
blends. We report results on the 30 expressions in the unseen test portion of MAC-CONF.
4.2 Experimental Resources
We generate candidate sets using two different lexicons: the CELEX lexicon (Baayen,
Piepenbrock, and Gulikers 1995),1 and a wordlist created from the Web 1T 5-gram
1 From CELEX, we use lemmas as potential source words, as it is uncommon for a source word to be an
inflected form?there are no such examples in our development data.
137
Computational Linguistics Volume 36, Number 1
Table 3
Types of blends and their frequency in Wordspy data.
Blend type Freq. Example
Simple two-word sequential blends 351 digifeiter
(digital counterfeiter)
Proper nouns 50 Japanimation
( Japanese animation)
Affixes 61 prevenge
(pre-revenge)
Common one-letter prefix 10 e-business
(electronic business)
Non-source word material 7 aireoke
(air guitar karaoke)
w2 contributes a prefix 10 theocon
(theological conservative)
Foreign word 4 sousveillance
(French sous, meaning under, and English
surveillance)
Non-sequential blends 6 entertoyment
(entertainment blended with toy)
w1 contributes a suffix 5 caponomics
(salary cap economics)
Multiple source words 6 MoSoSo
(mobile social software)
Other 5 CUV
(car blended with initialism SUV)
Corpus (Brants and Franz 2006). These are discussed further herein. The frequency
information needed to calculate the frequency features is extracted from the Web 1T 5-
gram Corpus. The length, contribution, and phonology features, as well as the syllable
structure filter, are calculated on the basis of the source words themselves, or are derived
from information in CELEX (when CELEX is the lexicon in use).2 We compute semantic
similarity between the source words using Jiang and Conrath?s (1997) measure in the
WordNet::Similarity package (Pedersen, Patwardhan, and Michelizzi 2004), and we
compute semantic relatedness of the pair using the cosine between word co-occurrence
vectors using software provided by Mohammad and Hirst (2006).
We conduct separate experiments with the two different lexicons for candidate set
creation. We began by using CELEX, because it contains rich phonological information
that some of our features draw on. However, in our analysis of the results, we noted
that for many expressions the correct candidate pair is not in the candidate set. Many
of the blends in WORDSPLEND are formed from words which are themselves new
words, often coined for concepts related to the Internet, such as download, for example;
such words are not listed in CELEX. This motivated us to create a lexicon from a
2 Note that it would be possible to automatically infer the phonological and syllabic information required
for our features using automatic approaches for text-to-phoneme conversion and syllabification (Bartlett,
Kondrak, and Cherry 2008, for example). Although such techniques currently provide noisy information,
phonological and syllabic information for the blend itself could also be inferred, allowing the
development of features that exploit this information. We leave exploring such possibilities for future
work.
138
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
recent data set (the Web 1T 5-gram Corpus) that would be expected to contain many
of these new coinages. To form a lexicon from this corpus, we extract the 100K most
frequent words, restricted to lowercase and all-alphabetic forms. Using this lexicon we
expect the correct source word pair to be in the candidate set for more expressions.
However, this comes at the expense of potentially larger candidate sets, due to the
larger lexicon size. Furthermore, since this lexicon does not contain phonological or
syllabic representations of each word, we cannot extract three features: the feature for
the syllable heuristic, and the two features that capture the tendency for the second
source word to be longer than the first in terms of phonemes and syllables. (We do cal-
culate the phonological similarity between the two candidate source words, in terms of
graphemes.)
4.3 Experimental Methods
Because each of our features is designed to have a high value for a correct source
word pair and a low value otherwise, we can simply sum the features for each candi-
date pair to get a score for each pair indicating its degree of goodness as a source word
pair for the blend under consideration. However, because our various features have
values falling on differing ranges, we first normalize the feature values by subtracting
the mean of that feature within that candidate set and dividing by the corresponding
standard deviation. We also take the arctan of each resulting feature value to reduce the
influence of outliers. We then sum the feature values for each candidate pair, and order
the pairs within each candidate set according to this sum. This ranks the pairs in terms
of decreasing degree of goodness as a source word pair. We refer to this method as the
feature ranking approach.
We also use a machine learning approach applied to the features in a training
regimen. Our task can be viewed as a classification problem in which each candidate
pair is either a positive instance (the correct source word pair) or a negative instance
(an incorrect source word pair). However, a standard machine learning algorithm does
not directly apply because of the structure of the problem space. In classification,
we typically look for a hyperplane that separates the positive and negative training
examples. In the context of our problem, this corresponds to separating all the correct
candidate pairs (for all blends in our data set) from all the incorrect candidate pairs.
However, such an approach is undesirable as it ignores the structure of the candidate
sets; it is only necessary to separate the correct source word pair for a given blend from
the corresponding incorrect candidate pairs (i.e., for the same blend). This is also in
line with the formulation of our features, which are designed to give relatively higher
values to correct candidate pairs than incorrect candidate pairs within the candidate set
for a given blend; it is not necessarily the case that the feature values for the correct
candidate pair for a given blend will be higher than those for an incorrect candidate
pair for another blend. In other words, the features are designed to give values that are
relative to the candidates for a particular blend.
To address this issue, we use a version of the perceptron algorithm similar to
that proposed by Shen and Joshi (2005). In this approach, the classifier is trained
by only adjusting the perceptron weight vector when the correct candidate pair is
not scored higher than the incorrect pairs for the target blend (not across all the can-
didate pairs for all blends). Furthermore, to accommodate for the large variation in
candidate set size we use an uneven margin?in this case the distance between the
weighted sum of the feature vector for a correct and incorrect candidate pair?of
139
Computational Linguistics Volume 36, Number 1
1
#correct cand. pairs ? #incorrect cand. pairs . We therefore learn a single weight vector such that,
within each candidate set, the correct candidate pairs are scored higher than the in-
correct candidate pairs by a factor of this margin. When updating the weight vector,
we multiply the update that we add to the weight vector by a factor of this margin
to prevent the classifier from being overly influenced by large candidate sets. During
testing, each candidate pair is ranked according to the weighted sum of its feature
vector. To evaluate this approach, on each of WORDSPLEND and MAC-CONF we per-
form 10-fold cross-validation with 10 random restarts. In these experiments, we use our
syllable heuristic as a feature, rather than as a filter, to allow the learner to weight it
appropriately.
4.4 Evaluation Metrics
We evaluate our methods according to two measures: accuracy and mean reciprocal
rank (MRR). Under the accuracy measure, the system is scored as correct if it ranks one
of the correct source word pairs for a given blend first, and as incorrect otherwise. The
MRR gives the mean of the rank of the highest ranked correct source word pair for each
blend. Although accuracy is more stringent than MRR, we are interested in MRR to see
where the system ranks the correct source word pair in the case that it is not ranked
first. We compare the accuracy of our system against a chance (random) baseline, and
an informed baseline in which the feature ranking approach is applied using just two of
our features, the frequency of each candidate source word.
5. Experimental Results
5.1 Candidate Sets
We begin by examining some properties of the candidate sets created using CELEX as
the lexicon, also referred to as the CELEX candidate set, in rows 2?4 of Table 4. First,
in the second row of this table, we observe that only 78?83% of expressions have both
source words in CELEX. For the other 17?22% of expressions, our system is always
incorrect, because the CELEX candidate set cannot contain the correct source words.
Table 4
Percent of expressions (% exps) with their source words in each lexical resource and candidate
set (CS), and after applying the syllable heuristic filter on the CELEX CS, as well as median CS
size, for both the WORDSPLEND and MAC-CONF data sets.
Lexical resource or CS WORDSPLEND MAC-CONF
% exps Med. CS size % exps Med. CS size
CELEX 78 - 83 -
CELEX CS 76 117 83 121
CELEX CS after syllable filter 71 71 77 92
Web 1T lexicon 92 - - -
Web 1T CS 89 442 - -
140
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
The percentages reported in this row thus serve as an upper bound on the task for each
data set.
The third row of Table 4 shows the percentage of expressions for which the CELEX
candidate set contains the correct source words. Note that in most cases, if the source
words are in CELEX, they are also in the CELEX candidate set. The only expressions
in WORDSPLEND for which that is not the case are those in which a source word
contributes a single letter to the blend. We could remove our restriction that each source
word contribute at least two letters; however, this would cause the candidate sets to be
much larger and likely reduce accuracy.
We now look at the effect of filtering the CELEX candidate sets to include only those
candidate pairs that are valid according to our syllable heuristic. This process results in
a 24?39% reduction in median candidate set size, but only excludes the source words
from the candidate set for a relatively small number of expressions (5?6%), as shown in
the fourth row of Table 4. We will further examine the effectiveness of this heuristic in
the following subsection.
Now we examine the candidate sets created using the lexicon derived from the
Web 1T 5-gram Corpus.3 In the final two rows of Table 4 we see that, as expected, many
more expressions have their source words in the Web 1T lexicon than in CELEX, and
furthermore, more expressions have their source words in the candidate sets created
using the Web 1T lexicon than in the candidate sets formed from CELEX. This means
that the upper bound for our task is much higher when using the Web 1T lexicon than
when using CELEX. However, this comes at the cost of creating much larger candidate
sets; we examine this trade-off more thoroughly herein.
5.2 Source Word Identification
In the following subsections we present results using the feature ranking approach
(Section 5.2.1), and analyze some of the errors the system makes in these experiments
(Section 5.2.2). We then consider results using the modified perceptron algorithm (Sec-
tion 5.2.3), and finally we compare our results to our previous study and human
performance (Section 5.2.4).
5.2.1 Feature Ranking. Table 5 gives the accuracy using the feature ranking approach for
both the random and informed baselines, each feature group, and the combination of
all features, on each data set, using both the CELEX and Web 1T lexicons in the case of
WORDSPLEND. Feature groups and combinations marked with an asterisk are signifi-
cantly better than the informed baseline at the 0.05 confidence level using McNemar?s
Test.4
We first note that the informed baseline is an improvement over the random base-
line in all cases, which points to the importance of word frequency in blend formation.
We also see that the informed baseline is quite a bit higher on WORDSPLEND than MAC-
CONF. Inspection of candidate sets?created from the CELEX lexicon?that include the
correct source words reveals that the average source word frequency for WORDSPLEND
3 Syllable structure information is not available for all words in the Web 1T lexicon; therefore, we do not
apply the syllable heuristic filter to the pairs in these candidate sets (see Section 4.2). We do not create
candidate sets for MAC-CONF using the Web 1T lexicon since this lexicon was constructed specifically in
response to the kinds of new words found in WORDSPLEND.
4 McNemar?s Test is a non-parametric test that can be applied to correlated, nominal data.
141
Computational Linguistics Volume 36, Number 1
Table 5
Percent accuracy on blends in WORDSPLEND and MAC-CONF using the feature ranking
approach. The size of each data set is given in parentheses. The lexicon employed (CELEX or
WEB 1T) is indicated. The best accuracy obtained using this approach for each data set and
lexicon is shown in boldface. * = results that are significantly better than the informed baseline.
Features WORDSPLEND MAC-CONF
(324) (30)
CELEX WEB 1T CELEX
Random Baseline 6 3 1
Informed Baseline 27 27 7
Frequency 32* 32* 30*
Len./Cont./Phono. 20 20 7
Semantic 15 13 20
All 38* 42* 37*
All+Syllable 40* - 37*
is much higher than for MAC-CONF (118M vs. 34M). On the other hand, the average for
non-source words in the candidate sets is similar across these data sets (11M vs. 9M).
Thus, although source words are more frequent than non-source words for both data
sets, frequency is a much more reliable indicator of being a source word for truly novel
blends than for established blends. This finding emphasizes the need for a data set such
as WORDSPLEND to evaluate methods for processing neologisms.
All of the individual feature groups outperform the random baseline. We also see
that our frequency features are better than the informed baseline. Although source
word frequency (the informed baseline) clearly plays an important role in forming inter-
pretable blends, this finding confirms that additional aspects of source word frequency
beyond their unigram counts also play an important role in blend formation. Also note
that the semantic features are substantially better than the informed baseline?although
not significantly so?on MAC-CONF, but not on WORDSPLEND. This result demon-
strates the importance of testing on true neologisms to have an accurate assessment
of a method. It also supports our future plan to explore alternative semantic features,
such as those that draw on the context of usage of a blend (as provided in our new
data set).
We expect using all the features to provide an improvement in performance over
any individual feature group, because they tap into very different types of information
about blends. Indeed, the combination of all features (All) does perform better than
the frequency features, supporting our hypothesis that the information provided by the
different feature groups is complementary.5
Looking at the results on WORDSPLEND using the Web 1T lexicon, we see that as
expected, due to the larger candidate sets, the random baseline is lower than when
using the CELEX lexicon. However, the informed baseline, and each feature group
used on its own, give very similar results, with only a small difference observed for
the semantic features. The combination of all features gives slightly higher performance
5 This difference is significant (p < 0.01) according to McNemar?s test for the WORDSPLEND data set using
both the CELEX and Web 1T lexicons. The difference was not significant for MAC-CONF.
142
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
using the Web 1T lexicon than the CELEX lexicon, although again this difference is
rather small.
Recall that we wanted to see if the use of our syllable heuristic filter to reduce
candidate set size would have a negative impact on performance. Table 5 shows that
the accuracy on all features when we apply our syllable heuristic filter (All+Syllable) is
at least as good as when we do not apply the filter (All). This is the case even though
the syllable heuristic filter removes the correct source word pairs for 5?6% of the blends
(see Table 4). It seems that the words this heuristic excludes from consideration are
not those that the features rank highly, indicating that it is a reasonable method for
pruning candidate sets. Moreover, reducing candidate set size will enable future work
to explore features that are more expensive to extract than those currently used. Given
the promising results using the Web1T lexicon, we also intend to examine ways to
automatically estimate the syllable filtering heuristic for words for which we do not
have syllable structure information.
5.2.2 Error Analysis. We now examine some cases where the system ranks an incorrect
candidate pair first, to try to determine why the system makes the errors it does. We
focus on the expressions in WORDSPLEND using the CELEX lexicon, as we are able to
extract all of our features for this experimental setup. First, we observe that when con-
sidering feature groups individually, the frequency features perform best; however, in
many cases, they also contribute to errors. This seems to be primarily due to (incorrect)
candidate pairs that occur very frequently together. For example, in the case of mathlete
(math athlete), the candidate pair male and athlete co-occurs much more frequently than
the correct source word pair, causing the system to rank the incorrect source word
pair first. We observe a similar situation for cutensil (cute utensil), where the candidate
pair cup and utensil often co-occur. In both these cases, phonological information for
the blend itself could help as, for example, cute ([kjut]) contributes more phonemes to
cutensil ([kjutEnsl
"
]) than cup ([k2p]).
Turning to the length, contribution, and phonology features, we see that although
many blends exhibit the properties on which these features are based, there are also
many blends which do not. For example, our first feature in this group captures the
property that the second source word tends to be longer than the first; however, this
is not the case for some blends, such as testilie (testify and lie). Furthermore, even for
blends for which the second source word is longer than the first, there may exist a
candidate pair that has a higher value for this feature than the correct source word
pair. In the case of banalysis?banal analysis?banal electrolysis is a better source word
pair according to this feature. These observations, and similar issues with other length,
contribution, and phonology features, likely contribute to the poor performance of
this feature group. Moreover, such findings motivate approaches such as our modified
perceptron algorithm?discussed in the following subsection?that learn a weighting
for the features.
Finally, for the semantic features, we find cases where a blend?s source words are
similar and related, but there is another (incorrect) candidate pair which is more similar
and related according to these features. For example, puggle, a blend of pug and beagle,
has the candidate source words push and struggle which are more semantically similar
and related than the correct source word pair. In this case, the part-of-speech of the
candidate source words, along with contextual knowledge indicating the part-of-speech
of the blend, may be useful; blending pug and beagle would result in a noun, while a
blend of push and struggle would likely be a verb. Another example is camikini, a blend
143
Computational Linguistics Volume 36, Number 1
of camisole and bikini. Both of these source words are women?s garments, so we would
expect them to have a moderately high similarity. However, the semantic similarity
feature assigns this candidate pair the lowest possible score, since these words do not
occur in the corpus from which this feature is estimated.
5.2.3 Modified Perceptron. Table 6 shows the average accuracy of the modified perceptron
algorithm for the informed baseline and the combination of all features plus the feature
corresponding to the syllable heuristic, on each data set, using both the CELEX and Web
1T lexicons in the case of WORDSPLEND. We don?t compare this method directly against
the results using the feature ranking approach because our perceptron experiments are
conducted using cross-validation, rather than a held-out test set methodology. Examin-
ing the results using the combination of All+Syllable, we see that for each data set and
lexicon the mean accuracy over the 10-fold cross-validation is significantly higher than
that obtained using the informed baseline, according to an unpaired t-test (p < 0.0001
in each case).
Interestingly, on WORDSPLEND using the combination of all features, we see higher
performance using the CELEX lexicon than the Web 1T lexicon. We hypothesize that
this is due to the training data in the latter case containing many more negative ex-
amples (incorrect candidate pairs?due to the larger candidate sets). It is worth noting
that, despite the differing experimental methodologies, the results are in fact not very
different from those obtained in the feature ranking approach. One limitation of this
perceptron algorithm is that it assumes that the training data is linearly separable.
In future work, we will try other machine learning techniques that do not make this
assumption.
5.2.4 Discussion. We now compare the feature ranking results on MAC-CONF here of
37% accuracy, to our previous best results on this data set of 27% accuracy, also using
feature ranking (Cook and Stevenson 2007). To make this comparison, we should con-
sider the differing baselines and upper bounds across the experiments. The informed
baseline in our previous study on MAC-CONF is 13%, substantially higher than the
7% in the current study. Recall that the first row of Table 4 shows the upper bound
using the CELEX lexicon on this data set to be 83%. By contrast, in our previous
work we only use blends whose source words appear in the lexicon we used there
(Macquarie), so the upper bound for that study is 100%. Taking these factors into
account, the best results in our previous study correspond to a reduction in error rate
(RER) over the informed baseline of 16%, while the feature ranking method here using
Table 6
Percent accuracy on blends in WORDSPLEND and MAC-CONF using the modified perceptron
algorithm. The size of each data set is given in parentheses. The lexicon employed (CELEX or
WEB 1T) is indicated. * = results that are significantly better than the informed baseline.
Features WORDSPLEND MAC-CONF
(324) (30)
CELEX WEB 1T CELEX
Informed Baseline 23 24 7
All+Syllable 40* 37* 35*
144
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
the combination of all features and the syllable heuristic filter achieves a much higher
RER of 39%.6
Lehrer (2003) finds human performance for determining the source words of blends
to be 34% to 79%?depending on the blends considered?which indicates the difficulty
of this task.7 Our best accuracy on each data set of 37%?42% is quite respectable
in comparison. These accuracies correspond to mean reciprocal ranks of 0.47?0.51,
and the random baseline on WORDSPLEND and MAC-CONF in terms of this mea-
sure is 0.03?0.07. This indicates that even when our system is incorrect, the correct
source word pair is still ranked fairly high. Such information about the best inter-
pretations of a blend could be useful in semi-automated methods, such as computer-
aided translation, where a human may not be familiar with a novel blend in the source
text.
6. Blend Identification
The statistical features we have developed may also be informative about whether
or not a word is in fact a blend?that is, we expect that if a novel word has ?good?
candidate source words, then the word is more likely to be a blend than the result of
another word formation process. Because our features are designed to be high for a
blend?s source words and low for other word pairs, we hypothesize that the highest
scoring candidate pairs for blends will be higher than those of non-blends.
To test this hypothesis, we first create a data set of non-blends from our earlier
annotation, which found 671 non-blends out of the 1,186 Wordspy expressions (see
Section 3). From these words, we eliminate all those beginning with a capital letter
(to exclude words formed from proper nouns) or containing a non-letter character (to
exclude acronyms and initialisms). This results in 663 non-blends.
We create candidate sets for the non-blends using the CELEX lexicon. Using the
CELEX lexicon allows us to extract?and consider the contribution of?all of our length,
contribution, and phonology features, some of which are not available when using the
Web 1T lexicon. The candidate sets resulting from using the CELEX lexicon were also
much smaller than when using the Web 1T lexicon. We calculate the features for the non-
blends as we did for the blends, and then order all expressions (both blends and non-
blends) according to the sum of the features for their highest-scoring candidate source
word pair. We use the same feature groups and combinations presented in Table 5.
Rather than set an arbitrary cut-off to distinguish blends from non-blends, we instead
give receiver operating characteristic (ROC) curves for some of these experiments.
ROC curves plot true positive rate versus false positive rate as the cut-off is varied
(see Figure 1). The top-left corner represents perfect classification, with points further
towards the top-left from the diagonal (a random classifier) being ?better.? We see that
the informed baseline is a substantial improvement over a random classifier, and the
combination All+Syllable is a further improvement over the informed baseline. The in-
dividual feature groups (not shown in Figure 1) do not perform as well as All+Syllable.
6 Reduction in error rate =
accuracy?baseline
upper bound?baseline .
7 Note that the high level of interannotator agreement achieved in our annotation task (Section 3) may
seem surprising in the context of Lehrer?s results. However, our task is much easier, because our
annotators were given a definition of the blend, whereas Lehrer?s subjects were not.
145
Computational Linguistics Volume 36, Number 1
Figure 1
ROC curves for blend identification.
In future work, we plan to re-examine this task and develop methods specifically for
identifying blends and other types of neologism.
7. Related Work
As discussed in Section 1, techniques generally used in the automatic acquisition of
syntactic and semantic properties of words are not applicable here, because they use
corpus statistics that cannot be accurately estimated for low frequency items, such as
the novel lexical blends considered in this study (Hindle 1990; Lapata and Brew 2004;
Joanis, Stevenson, and James 2008, for example). Other work has used the context
in which an unknown word occurs, along with domain-specific knowledge, to infer
aspects of its meaning and syntax (Granger 1977; Cardie 1993; Hastings and Lytinen
1994, for example). These studies have been able to learn properties of an unknown
word from just one usage, or a small number of usages; however, the domain-specific
resources that these studies rely on limit their applicability to general text.
Techniques for inferring lexical properties of neologisms can make use of infor-
mation that is typically not available in other lexical acquisition tasks?specifically,
knowledge of the processes through which neologisms are formed. Computational
work on neologisms has tended to focus on tasks pertaining to a specific type of
neologism, such as identifying and inferring the long form of acronyms (Schwartz and
Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example),
recognizing loanwords (Baker and Brew 2008; Alex 2008, for example), and identifying
and expanding clippings (Means 1988, for example). This study focuses on the tasks
of identifying, and inferring the source words of, lexical blends, a common type of
neologism, which have been previously unaddressed except for our preliminary work
in Cook and Stevenson (2007).
In addition to knowledge about a word?s formation process, for many types of
neologism, information about its phonological and orthographic content can be used to
146
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
infer aspects of its syntactic and semantic properties. This is the case for neologisms that
are composed of existing words or affixes (e.g., compounds and derivations) or partial
orthographic or phonological material from existing words or affixes (e.g., acronyms,
clippings, and blends). For example, in the case of part-of-speech tagging, information
about the suffix of an unknown word can be used to determine its part-of-speech
(Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the
long form of an acronym, the letters which compose a given acronym can be used to
determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney
2005; Okazaki and Ananiadou 2006, for example).
The latter approach to acronyms is somewhat similar to the way in which we use
knowledge of the letters that make up a blend to form candidate sets and determine
the most likely source words. However, in the case of acronyms, each word in a long
form typically contributes only one letter to the acronym, while for blends, a source
word usually contributes more than one letter. At first glance, it may appear that this
makes the task of source word identification easier for blends, since there is more source
word material available to work with. However, acronyms have two properties that
help in their identification. First, there is less uncertainty in the ?split? of an acronym,
because each letter is usually contributed by a separate word. By contrast, due to the
large variation in the amount of material contributed by the source words in blends,
one of the challenges in blend identification is to determine which material in the blend
belongs to each source word. Second, and more importantly, acronyms are typically
introduced in regular patterns (e.g., the long form followed by the acronym capitalized
and in parentheses) which can be exploited in acronym identification and long form
inference; in the case of blends there is no counterpart for this information.
8. Conclusions
We propose a statistical model for inferring the source words of lexical blends?a very
frequent class of new words?based largely on properties related to the recognizability
of their source words. We also introduce a method based on syllable structure for re-
ducing the number of words that are considered as possible source words. We evaluate
our methods on two data sets, one consisting of novel blends, the other containing
established blends; in both cases our features significantly outperform an informed
baseline. Moreover, the results in this study are substantially better than those reported
previously (Cook and Stevenson 2007). We further show that our methods for source
word identification can also be used to distinguish blends from other word types. In
addition, we annotate a data set of newly coined expressions which will support future
research not only on lexical blends, but on neologisms in general.
Our future plans include expanding our techniques for identifying blends to ad-
dress the more general problem of determining the formation process of a novel word.
We further intend to apply our source word identification methods to other types of
neologisms formed from material from existing words, such as clippings (e.g., lab for
laboratory).
Acknowledgments
This article is an extended and updated
version of a paper that appeared in the
Proceedings of the Tenth Conference of the
Pacific Association for Computational
Linguistics (PACLING-2007). We thank the
anonymous reviewers of this article for their
comments, which have helped us to improve
the quality of this work. We also thank the
members of the computational linguistics
group at the University of Toronto for their
comments and feedback on our research.
This work was financially supported by the
National Sciences and Engineering Research
147
Computational Linguistics Volume 36, Number 1
Council of Canada, the Ontario Graduate
Scholarship program, and the University
of Toronto.
References
Alex, Beatrice. 2008. Comparing
corpus-based to Web-based lookup
techniques for automatic English
inclusion detection. In Proceedings of
the Sixth International Language
Resources and Evaluation Conference
(LREC?08), pages 2693?2697,
Marrakech.
Algeo, John. 1980. Where do all the new
words come from? American Speech,
55(4):264?277.
Algeo, John, editor. 1991. Fifty Years Among
the New Words. Cambridge University
Press, Cambridge.
Ayto, John, editor. 1990. The Longman Register
of New Words, volume 2. Longman,
London.
Ayto, John. 2006. Movers and Shakers: A
Chronology of Words that Shaped our Age.
Oxford University Press, Oxford.
Baayen, R. Harald, Richard Piepenbrock, and
Leon Gulikers. 1995. The CELEX Lexical
Database (release 2) [CD-ROM].
Philadelphia, PA: Linguistic Data
Consortium, University of
Pennsylvania [distributor].
Baker, Kirk and Chris Brew. 2008.
Statistical identification of English
loanwords in Korean using automatically
generated training data. In Proceedings
of the Sixth International Language
Resources and Evaluation Conference
(LREC?08), pages 1159?1163, Marrakech.
Bartlett, Susan, Grzegorz Kondrak, and
Colin Cherry. 2008. Automatic
syllabification with structured SVMs
for letter-to-phoneme conversion. In
Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL-08): Human Language
Technologies, pages 568?576,
Columbus, OH.
Bauer, Laurie. 1983. English Word-formation.
Cambridge University Press, Cambridge.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram Corpus version 1.1. Linguistic
Data Consortium, Philadelphia, PA.
Brill, Eric. 1994. Some advances in
transformation-based part of speech
tagging. In Proceedings of the Twelfth
National Conference on Artificial Intelligence,
pages 722?727, Seattle, WA.
Cardie, Claire. 1993. A case-based approach
to knowledge acquisition for
domain-specific sentence analysis. In
Proceedings of the Eleventh National
Conference on Artificial Intelligence,
pages 798?803, Washington, DC.
Cook, Paul and Suzanne Stevenson. 2007.
Automagically inferring the source words
of lexical blends. In Proceedings of the Tenth
Conference of the Pacific Association for
Computational Linguistics (PACLING-2007),
pages 289?297, Melbourne.
Delbridge, Arthur, editor. 1981. The Macquarie
Dictionary. Macquarie Library, Sydney.
Granger, Richard H. 1977. FOUL-UP: A
program that figures out the meanings of
words from context. In Proceedings of the
Fifth International Joint Conference on
Artificial Intelligence, pages 172?178,
Cambridge, MA.
Gries, Stefan Th. 2004. Shouldn?t it be
breakfunch? A quantitative analysis of the
structure of blends. Linguistics,
42(3):639?667.
Gries, Stefan Th. 2006. Cognitive
determinants of subtractive
word-formation processes: A corpus-based
perspective. Cognitive Linguistics,
17(4):535?558.
Hastings, Peter M. and Steven L. Lytinen.
1994. The ups and downs of lexical
acquisition. In Proceedings of the Twelfth
National Conference on Artificial
Intelligence, pages 754?759, Seattle, WA.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting
of the Association for Computational
Linguistics, pages 268?275, Pittsburgh, PA.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference
on Research in Computational Linguistics
(ROCLING X), pages 19?33, Taiwan.
Joanis, Eric, Suzanne Stevenson, and
David James. 2008. A general feature space
for automatic verb classification. Natural
Language Engineering, 14(3):337?367.
Kelly, Michael H. 1998. To ?brunch? or to
?brench?: Some aspects of blend
structure. Linguistics, 36(3):579?590.
Knowles, Elizabeth and Julia Elliott, editors.
1997. The Oxford Dictionary of New Words.
Oxford University Press, New York.
Kubozono, Haruo. 1990. Phonological
constraints on blending in English as a
case for phonology-morphology interface.
Yearbook of Morphology, 3:1?20.
148
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?73.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Noun
Compounds. Ph.D. thesis, Macquarie
University, Sydney.
Lehrer, Adrienne. 2003. Understanding
trendy neologisms. Italian Journal of
Linguistics, 15(2):369?382.
Means, Linda G. 1988. Cn yur cmputr raed
ths? In Proceedings of the Second Conference
on Applied Natural Language Processing,
pages 93?100, Austin, TX.
Mikheev, Andrei. 1997. Automatic rule
induction for unknown-word guessing.
Computational Linguistics, 23(3):405?423.
Mohammad, Saif and Graeme Hirst. 2006.
Distributional measures of
concept-distance: A task-oriented
evaluation. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2006),
pages 35?43, Sydney.
Nadeau, David and Peter D. Turney. 2005. A
supervised learning approach to acronym
identification. In Proceedings of the
Eighteenth Canadian Conference on Artificial
Intelligence (AI?2005), pages 319?329,
Victoria.
Okazaki, Naoaki and Sophia Ananiadou.
2006. A term recognition approach to
acronym recognition. In Proceedings of the
21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics (Coling-ACL
2006), pages 643?650, Sydney.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004.
Wordnet::Similarity?Measuring the
relatedness of concepts. In Demonstration
Papers at the Human Language Technology
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 38?41, Boston, MA.
Plag, Ingo. 2003. Word-formation in
English. Cambridge University Press,
Cambridge.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech
tagging. In Proceedings of the Conference
on Empirical Methods in Natural
Language Processing, pages 133?142,
Philadelphia, PA.
Schwartz, Ariel S. and Marti A. Hearst.
2003. A simple algorithm for identifying
abbreviation definitions in biomedical
texts. In Proceedings of the Pacific
Symposium on Biocomputing (PSB 2003),
pages 451?462, Lihue, HI.
Shen, Libin and Aravind K. Joshi. 2005.
Ranking and reranking with perceptron.
Machine Learning, 60(1):73?96.
149

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 634?639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simpler unsupervised POS tagging with bilingual projections
Long Duong, 12 Paul Cook, 1 Steven Bird, 1 and Pavel Pecina2
1 Department of Computing and Information Systems, The University of Melbourne
2 Charles University in Prague, Czech Republic
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au,
sbird@unimelb.edu.au, pecina@ufal.mff.cuni.cz
Abstract
We present an unsupervised approach to
part-of-speech tagging based on projec-
tions of tags in a word-aligned bilingual
parallel corpus. In contrast to the exist-
ing state-of-the-art approach of Das and
Petrov, we have developed a substantially
simpler method by automatically identi-
fying ?good? training sentences from the
parallel corpus and applying self-training.
In experimental results on eight languages,
our method achieves state-of-the-art re-
sults.
1 Unsupervised part-of-speech tagging
Currently, part-of-speech (POS) taggers are avail-
able for many highly spoken and well-resourced
languages such as English, French, German, Ital-
ian, and Arabic. For example, Petrov et al (2012)
build supervised POS taggers for 22 languages us-
ing the TNT tagger (Brants, 2000), with an aver-
age accuracy of 95.2%. However, many widely-
spoken languages ? including Bengali, Javanese,
and Lahnda ? have little data manually labelled
for POS, limiting supervised approaches to POS
tagging for these languages.
However, with the growing quantity of text
available online, and in particular, multilingual
parallel texts from sources such as multilin-
gual websites, government documents and large
archives of human translations of books, news, and
so forth, unannotated parallel data is becoming
more widely available. This parallel data can be
exploited to bridge languages, and in particular,
transfer information from a highly-resourced lan-
guage to a lesser-resourced language, to build un-
supervised POS taggers.
In this paper, we propose an unsupervised ap-
proach to POS tagging in a similar vein to the
work of Das and Petrov (2011). In this approach,
a parallel corpus for a more-resourced language
having a POS tagger, and a lesser-resourced lan-
guage, is word-aligned. These alignments are ex-
ploited to infer an unsupervised tagger for the tar-
get language (i.e., a tagger not requiring manually-
labelled data in the target language). Our ap-
proach is substantially simpler than that of Das
and Petrov, the current state-of-the art, yet per-
forms comparably well.
2 Related work
There is a wealth of prior research on building un-
supervised POS taggers. Some approaches have
exploited similarities between typologically simi-
lar languages (e.g., Czech and Russian, or Telugu
and Kannada) to estimate the transition probabil-
ities for an HMM tagger for one language based
on a corpus for another language (e.g., Hana et al,
2004; Feldman et al, 2006; Reddy and Sharoff,
2011). Other approaches have simultaneously
tagged two languages based on alignments in a
parallel corpus (e.g., Snyder et al, 2008).
A number of studies have used tag projection
to copy tag information from a resource-rich to
a resource-poor language, based on word align-
ments in a parallel corpus. After alignment, the
resource-rich language is tagged, and tags are pro-
jected from the source language to the target lan-
guage based on the alignment (e.g., Yarowsky and
Ngai, 2001; Das and Petrov, 2011). Das and
Petrov (2011) achieved the current state-of-the-art
for unsupervised tagging by exploiting high con-
fidence alignments to copy tags from the source
language to the target language. Graph-based la-
bel propagation was used to automatically produce
more labelled training data. First, a graph was
constructed in which each vertex corresponds to
a unique trigram, and edge weights represent the
syntactic similarity between vertices. Labels were
then propagated by optimizing a convex function
to favor the same tags for closely related nodes
634
Model Coverage Accuracy
Many-to-1 alignments 88% 68%
1-to-1 alignments 68% 78%
1-to-1 alignments: Top 60k sents 91% 80%
Table 1: Token coverage and accuracy of many-
to-one and 1-to-1 alignments, as well as the top
60k sentences based on alignment score for 1-to-1
alignments, using directly-projected labels only.
while keeping a uniform tag distribution for un-
related nodes. A tag dictionary was then extracted
from the automatically labelled data, and this was
used to constrain a feature-based HMM tagger.
The method we propose here is simpler to that
of Das and Petrov in that it does not require con-
vex optimization for label propagation or a feature
based HMM, yet it achieves comparable results.
3 Tagset
Our tagger exploits the idea of projecting tag infor-
mation from a resource-rich to resource-poor lan-
guage. To facilitate this mapping, we adopt Petrov
et al?s (2012) twelve universal tags: NOUN,
VERB, ADJ, ADV, PRON (pronouns), DET (de-
terminers and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation), and X
(all other categories, e.g., foreign words, abbrevia-
tions). These twelve basic tags are common across
taggers for most languages.
Adopting a universal tagset avoids the need
to map between a variety of different, language-
specific tagsets. Furthermore, it makes it possi-
ble to apply unsupervised tagging methods to lan-
guages for which no tagset is available, such as
Telugu and Vietnamese.
4 A Simpler Unsupervised POS Tagger
Here we describe our proposed tagger. The key
idea is to maximize the amount of information
gleaned from the source language, while limit-
ing the amount of noise. We describe the seed
model and then explain how it is successively re-
fined through self-training and revision.
4.1 Seed Model
The first step is to construct a seed tagger from
directly-projected labels. Given a parallel corpus
for a source and target language, Algorithm 1 pro-
vides a method for building an unsupervised tag-
ger for the target language. In typical applications,
the source language would be a better-resourced
language having a tagger, while the target lan-
guage would be lesser-resourced, lacking a tagger
and large amounts of manually POS-labelled data.
Algorithm 1 Build seed model
1: Tag source side.
2: Word align the corpus with Giza++ and re-
move the many-to-one mappings.
3: Project tags from source to target using the re-
maining 1-to-1 alignments.
4: Select the top n sentences based on sentence
alignment score.
5: Estimate emission and transition probabilities.
6: Build seed tagger T.
We eliminate many-to-one alignments (Step 2).
Keeping these would give more POS-tagged to-
kens for the target side, but also introduce noise.
For example, suppose English and French were
the source and target language, respectively. In
this case alignments such as English laws (NNS)
to French les (DT) lois (NNS) would be expected
(Yarowsky and Ngai, 2001). However, in Step 3,
where tags are projected from the source to target
language, this would incorrectly tag French les as
NN. We build a French tagger based on English?
French data from the Europarl Corpus (Koehn,
2005). We also compare the accuracy and cov-
erage of the tags obtained through direct projec-
tion using the French Melt POS tagger (Denis and
Sagot, 2009). Table 1 confirms that the one-to-one
alignments indeed give higher accuracy but lower
coverage than the many-to-one alignments. At
this stage of the model we hypothesize that high-
confidence tags are important, and hence eliminate
the many-to-one alignments.
In Step 4, in an effort to again obtain higher
quality target language tags from direct projection,
we eliminate all but the top n sentences based on
their alignment scores, as provided by the aligner
via IBM model 3. We heuristically set this cutoff
to 60k to balance the accuracy and size of the seed
model.1 Returning to our preliminary English?
French experiments in Table 1, this process gives
improvements in both accuracy and coverage.2
1We considered values in the range 60?90k, but this
choice had little impact on the accuracy of the model.
2We also considered using all projected labels for the top
60k sentences, not just 1-to-1 alignments, but in preliminary
experiments this did not perform as well, possibly due to the
previously-observed problems with many-to-one alignments.
635
The number of parameters for the emission prob-
ability is |V | ? |T | where V is the vocabulary and
T is the tag set. The transition probability, on the
other hand, has only |T |3 parameters for the tri-
gram model we use. Because of this difference
in number of parameters, in step 5, we use dif-
ferent strategies to estimate the emission and tran-
sition probabilities. The emission probability is
estimated from all 60k selected sentences. How-
ever, for the transition probability, which has less
parameters, we again focus on ?better? sentences,
by estimating this probability from only those sen-
tences that have (1) token coverage > 90% (based
on direct projection of tags from the source lan-
guage), and (2) length > 4 tokens. These cri-
teria aim to identify longer, mostly-tagged sen-
tences, which we hypothesize are particularly use-
ful as training data. In the case of our preliminary
English?French experiments, roughly 62% of the
60k selected sentences meet these criteria and are
used to estimate the transition probability. For un-
aligned words, we simply assign a random POS
and very low probability, which does not substan-
tially affect transition probability estimates.
In Step 6 we build a tagger by feeding the es-
timated emission and transition probabilities into
the TNT tagger (Brants, 2000), an implementation
of a trigram HMM tagger.
4.2 Self training and revision
For self training and revision, we use the seed
model, along with the large number of target lan-
guage sentences available that have been partially
tagged through direct projection, in order to build
a more accurate tagger. Algorithm 2 describes
this process of self training and revision, and as-
sumes that the parallel source?target corpus has
been word aligned, with many-to-one alignments
removed, and that the sentences are sorted by
alignment score. In contrast to Algorithm 1, all
sentences are used, not just the 60k sentences with
the highest alignment scores.
We believe that sentence alignment score might
correspond to difficulty to tag. By sorting the sen-
tences by alignment score, sentences which are
more difficult to tag are tagged using a more ma-
ture model. Following Algorithm 1, we divide
sentences into blocks of 60k.
In step 3 the tagged block is revised by com-
paring the tags from the tagger with those ob-
tained through direct projection. Suppose source
Algorithm 2 Self training and revision
1: Divide target language sentences into blocks
of n sentences.
2: Tag the first block with the seed tagger.
3: Revise the tagged block.
4: Train a new tagger on the tagged block.
5: Add the previous tagger?s lexicon to the new
tagger.
6: Use the new tagger to tag the next block.
7: Goto 3 and repeat until all blocks are tagged.
language word wsi is aligned with target language
word wtj with probability p(wtj |wsi ), T si is the tag
for wsi using the tagger available for the source
language, and T tj is the tag for wtj using the tagger
learned for the target language. If p(wtj |wsi ) > S,
where S is a threshold which we heuristically set
to 0.7, we replace T tj by T si .
Self-training can suffer from over-fitting, in
which errors in the original model are repeated
and amplified in the new model (McClosky et al,
2006). To avoid this, we remove the tag of
any token that the model is uncertain of, i.e., if
p(wtj |wsi ) < S and T tj ?= T si then T tj = Null. So,
on the target side, aligned words have a tag from
direct projection or no tag, and unaligned words
have a tag assigned by our model.
Step 4 estimates the emission and transition
probabilities as in Algorithm 1. In Step 5, emis-
sion probabilities for lexical items in the previous
model, but missing from the current model, are
added to the current model. Later models therefore
take advantage of information from earlier mod-
els, and have wider coverage.
5 Experimental Results
Using parallel data from Europarl (Koehn, 2005)
we apply our method to build taggers for the same
eight target languages as Das and Petrov (2011)
? Danish, Dutch, German, Greek, Italian, Por-
tuguese, Spanish and Swedish ? with English as
the source language. Our training data (Europarl)
is a subset of the training data of Das and Petrov
(who also used the ODS United Nations dataset
which we were unable to obtain). The evaluation
metric and test data are the same as that used by
Das and Petrov. Our results are comparable to
theirs, although our system is penalized by having
less training data. We tag the source language with
the Stanford POS tagger (Toutanova et al, 2003).
636
Danish Dutch German Greek Italian Portuguese Spanish Swedish Average
Seed model 83.7 81.1 83.6 77.8 78.6 84.9 81.4 78.9 81.3
Self training + revision 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the method
of Das and Petrov (2011). The best results on each language, and on average, are shown in bold.
0 5 10 15 20 25 30
Iteration
50
60
70
80
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
0 5 10 15 20 25 30
Iteration
70
75
80
85
90
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of
known tokens for Italian (left) and Dutch (right).
Table 2 shows results for our seed model, self
training and revision, and the results reported by
Das and Petrov. Self training and revision im-
prove the accuracy for every language over the
seed model, and gives an average improvement
of roughly two percentage points. The average
accuracy of self training and revision is on par
with that reported by Das and Petrov. On individ-
ual languages, self training and revision and the
method of Das and Petrov are split ? each per-
forms better on half of the cases. Interestingly, our
method achieves higher accuracies on Germanic
languages ? the family of our source language,
English?while Das and Petrov perform better on
Romance languages. This might be because our
model relies on alignments, which might be more
accurate for more-related languages, whereas Das
and Petrov additionally rely on label propagation.
Compared to Das and Petrov, our model per-
forms poorest on Italian, in terms of percentage
point difference in accuracy. Figure 1 (left panel)
shows accuracy, accuracy on known words, accu-
racy on unknown words, and proportion of known
tokens for each iteration of our model for Italian;
iteration 0 is the seed model, and iteration 31 is
the final model. Our model performs poorly on
unknown words as indicated by the low accuracy
on unknown words, and high accuracy on known
words compared to the overall accuracy. The poor
performance on unknown words is expected be-
cause we do not use any language-specific rules
to handle this case. Moreover, on average for the
final model, approximately 10% of the test data
tokens are unknown. One way to improve the per-
formance of our tagger might be to reduce the pro-
portion of unknown words by using a larger train-
ing corpus, as Das and Petrov did.
We examine the impact of self-training and re-
vision over training iterations. We find that for
all languages, accuracy rises quickly in the first
5?6 iterations, and then subsequently improves
only slightly. We exemplify this in Figure 1 (right
panel) for Dutch. (Findings are similar for other
languages.) Although accuracy does not increase
much in later iterations, they may still have some
benefit as the vocabulary size continues to grow.
6 Conclusion
We have proposed a method for unsupervised POS
tagging that performs on par with the current state-
of-the-art (Das and Petrov, 2011), but is substan-
tially less-sophisticated (specifically not requiring
convex optimization or a feature-based HMM).
The complexity of our algorithm is O(nlogn)
compared to O(n2) for that of Das and Petrov
637
(2011) where n is the size of training data.3 We
made our code are available for download.4
In future work we intend to consider using a
larger training corpus to reduce the proportion of
unknown tokens and improve accuracy. Given
the improvements of our model over that of Das
and Petrov on languages from the same family
as our source language, and the observation of
Snyder et al (2008) that a better tagger can be
learned from a more-closely related language, we
also plan to consider strategies for selecting an ap-
propriate source language for a given target lan-
guage. Using our final model with unsupervised
HMM methods might improve the final perfor-
mance too, i.e. use our final model as the ini-
tial state for HMM, then experiment with differ-
ent inference algorithms such as ExpectationMax-
imization (EM), Variational Bayers (VB) or Gibbs
sampling (GS).5 Gao and Johnson (2008) compare
EM, VB and GS for unsupervised English POS
tagging. In many cases, GS outperformed other
methods, thus we would like to try GS first for our
model.
7 Acknowledgements
This work is funded by Erasmus Mundus
European Masters Program in Language and
Communication Technologies (EM-LCT) and
by the Czech Science Foundation (grant no.
P103/12/G084). We would like to thank Prokopis
Prokopidis for providing us the Greek Treebank
and Antonia Marti for the Spanish CoNLL 06
dataset. Finally, we thank Siva Reddy and Span-
dana Gella for many discussions and suggestions.
References
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth con-
ference on Applied natural language processing
(ANLP ?00), pages 224?231. Seattle, Washing-
ton, USA.
Dipanjan Das and Slav Petrov. 2011. Unsu-
pervised part-of-speech tagging with bilingual
graph-based projections. In Proceedings of
3We re-implemented label propagation from Das and
Petrov (2011). It took over a day to complete this step on
an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but
only 15 minutes for our model.
4https://code.google.com/p/universal-tagger/
5We in fact have tried EM, but it did not help. The overall
performance dropped slightly. This might be because self-
training with revision already found the local maximal point.
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies - Volume 1 (ACL 2011), pages
600?609. Portland, Oregon, USA.
Pascal Denis and Beno??t Sagot. 2009. Coupling
an annotated corpus and a morphosyntactic lex-
icon for state-of-the-art POS tagging with less
human effort. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation, pages 721?736. Hong Kong,
China.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of
new morpho-syntactically annotated resources.
In Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?06), pages 549?554. Genoa, Italy.
Jianfeng Gao and Mark Johnson. 2008. A com-
parison of bayesian estimators for unsupervised
hidden markov model pos taggers. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08,
pages 344?352. Association for Computational
Linguistics, Stroudsburg, PA, USA.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP ?04), pages 222?229. Barcelona,
Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Proceed-
ings of the Tenth Machine Translation Summit
(MT Summit X), pages 79?86. AAMT, Phuket,
Thailand.
David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Effective self-training for pars-
ing. In Proceedings of the main conference on
Human Language Technology Conference of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06),
pages 152?159. New York, USA.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A universal part-of-speech tagset. In
Proceedings of the Eight International Confer-
ence on Language Resources and Evaluation
(LREC?12), pages 2089?2096. Istanbul, Turkey.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS Taggers (and other tools) for Indian
638
languages: An experiment with Kannada using
Telugu resources. In Proceedings of the IJC-
NLP 2011 workshop on Cross Lingual Infor-
mation Access: Computational Linguistics and
the Information Need of Multilingual Societies
(CLIA 2011). Chiang Mai, Thailand.
Benjamin Snyder, Tahira Naseem, Jacob Eisen-
stein, and Regina Barzilay. 2008. Unsupervised
multilingual learning for POS tagging. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP
?08), pages 1041?1050. Honolulu, Hawaii.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-
rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the
2003 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics on Human Language Technology - Vol-
ume 1 (NAACL ?03), pages 173?180. Edmon-
ton, Canada.
David Yarowsky and Grace Ngai. 2001. Induc-
ing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In
Proceedings of the Second Meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technolo-
gies (NAACL ?01), pages 1?8. Pittsburgh, Penn-
sylvania, USA.
639
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?12,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Stacking-based Approach to Twitter User Geolocation Prediction
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
We implement a city-level geolocation
prediction system for Twitter users. The
system infers a user?s location based on
both tweet text and user-declared metadata
using a stacking approach. We demon-
strate that the stacking method substan-
tially outperforms benchmark methods,
achieving 49% accuracy on a benchmark
dataset. We further evaluate our method
on a recent crawl of Twitter data to in-
vestigate the impact of temporal factors
on model generalisation. Our results sug-
gest that user-declared location metadata
is more sensitive to temporal change than
the text of Twitter messages. We also de-
scribe two ways of accessing/demoing our
system.
1 Introduction
In this paper, we present and evaluate a geoloca-
tion prediction method for Twitter users.1 Given
a user?s tweet data as input, the task of user level
geolocation prediction is to infer a primary loca-
tion (i.e., ?home location?: Mahmud et al (2012))
for the user from a discrete set of pre-defined loca-
tions (Cheng et al, 2010). For instance, President
Obama?s location might be predicted to be Wash-
ington D.C., USA, based on his public tweets and
profile metadata.
Geolocation information is essential to location-
based applications, like targeted advertising and
local event detection (Sakaki et al, 2010;
MacEachren et al, 2011). However, the means
to obtain such information are limited. Although
Twitter allows users to specify a plain text de-
scription of their location in their profile, these de-
scriptions tend to be ad hoc and unreliable (Cheng
1We only use public Twitter data for experiments and ex-
emplification in this study.
et al, 2010). Recently, user geolocation predic-
tion based on a user?s tweets has become popular
(Wing and Baldridge, 2011; Roller et al, 2012),
based on the assumption that tweets implicitly
contain locating information, and with appropri-
ate statistical modeling, the true location can be
inferred. For instance, if a user frequently men-
tions NYC, JFK and yankees, it is likely that they
are from New York City, USA.
In this paper, we discuss an implementation of
a global city-level geolocation prediction system
for English Twitter users. The system utilises both
tweet text and public profile metadata for model-
ing and inference. Specifically, we train multino-
mial Bayes classifiers based on location indica-
tive words (LIWs) in tweets (Han et al, 2012),
and user-declared location and time zone meta-
data. These base classifiers are further stacked
(Wolpert, 1992) using logistic regression as the
meta-classifier. The proposed stacking model is
compared with benchmarks on a public geolo-
cation dataset. Experimental results demonstrate
that our stacking model outperforms benchmark
methods by a large margin, achieving 49% accu-
racy on the test data. We further evaluate the stack-
ing model on a more recent crawl of public tweets.
This experiment tests the effectiveness of a geolo-
cation model trained on ?old? data when applied to
?new? data. The results reveal that user-declared
locations are more variable over time than tweet
text and time zone data.
2 Background and Related Work
Identifying the geolocation of objects has been
widely studied in the research literature over target
objects including webpages (Zong et al, 2005),
search queries (Backstrom et al, 2008), Flickr im-
ages (Crandall et al, 2009) and Wikipedia ed-
itors (Lieberman and Lin, 2009). Recently, a
considerable amount of work has been devoted
to extending geolocation prediction for Twitter
7
users (Cheng et al, 2010; Eisenstein et al, 2010).
The geolocations are usually represented by un-
ambiguous city names or a partitioning of the
earth?s surface (e.g., grid cells specified by lati-
tude/longitude). User geolocation is generally re-
lated to a ?home? location where a user regularly
resides, and user mobility is ignored. Twitter al-
lows users to declare their home locations in plain
text in their profile, however, this data has been
found to be unstructured and ad hoc in preliminary
research (Cheng et al, 2010; Hecht et al, 2011).
While popular for desktop machine geoloca-
tion, methods that map IP addresses to physical
locations (Buyukokkten et al, 1999) cannot be
applied to Twitter-based user geolocation, as IPs
are only known to the service provider and are
non-trivial to retrieve in a mobile Internet environ-
ment. Although social network information has
been proven effective in inferring user locations
(Backstrom et al, 2010; Sadilek et al, 2012; Rout
et al, 2013), we focus exclusively on message
and metadata information in this paper, as they are
more readily accessible.
Text data tends to contain salient geospatial ex-
pressions that are particular to specific regions.
Attempts to leverage this data directly have been
based on analysis of gazetted expressions (Leid-
ner and Lieberman, 2011) or the identification of
geographical entities (Quercini et al, 2010; Qin et
al., 2003). However these methods are limited in
their ability to capture informal geospatial expres-
sions (e.g. Brissie for Brisbane) and more non-
geospatial terms which are associated with partic-
ular locations (e.g. ferry for Seattle or Sydney).
Beyond identifying geographical references us-
ing off-the-shelf tools, more sophisticated meth-
ods have been introduced in the social media
realm. Cheng et al (2010) built a simple gen-
erative model based on tweet words, and fur-
ther added words which are local to particular re-
gions and applied smoothing to under-represented
locations. Kinsella et al (2011) applied differ-
ent similarity measures to the task, and investi-
gated the relative difficulty of geolocation predic-
tion at city, state, and country levels. Wing and
Baldridge (2011) introduced a grid-based repre-
sentation for geolocation modeling and inference
based on fixed latitude and longitude values, and
aggregated all tweets in a single cell. Their ap-
proach was then based on lexical similarity us-
ing KL-divergence. One drawback to the uniform-
sized cell representation is that it introduces class
imbalance: urban areas tend to contain far more
tweets than rural areas. Based on this observa-
tion, Roller et al (2012) introduced an adaptive
grid representation in which cells contain approx-
imately the same number of users, based on a KD-
tree partition. Given that most tweets are from
urban areas, Han et al (2012) consider a city-
based class division, and explore different feature
selection methods to extract ?location indicative
words?, which they show to improve prediction
accuracy. Additionally, time zone information has
been incorporated in a coarse-to-fine hierarchical
model by first determining the time zone, and then
disambiguating locations within it (Mahmud et al,
2012). Topic models have also been applied to the
task, in capturing regional linguistic differences
(Eisenstein et al, 2010; Yin et al, 2011; Hong et
al., 2012).
When designing a practical geolocation sys-
tem, simple models such as naive Bayes and near-
est prototype methods (e.g., based on KL diver-
gence) have clear advantages in terms of train-
ing and classification throughput, given the size of
the class set (often numbering in the thousands of
classes) and sheer volume of training data (poten-
tially in the terabytes of data). This is particularly
important for online systems and downstream ap-
plications that require timely predictions. As such,
we build off the text-based naive Bayes-based ge-
olocation system of Han et al (2012), which our
experiments have shown to have a good balance of
tractability and accuracy. By selecting a reduced
set of ?location indicative words?, prediction can
be further accelerated.
3 Methodology
In this study, we adopt the same city-based rep-
resentation and multinomial naive Bayes learner
as Han et al (2012). The city-based representa-
tion consists of 3,709 cities throughout the world,
and is obtained by aggregating smaller cities with
the largest nearby city. Han et al (2012) found
that using feature selection to identify ?location
indicative words? led to improvements in geoloca-
tion performance. We use the same feature selec-
tion technique that they did. Specifically, feature
selection is based on information gain ratio (IGR)
(Quinlan, 1993) over the city-based label set for
each word.
In the original research of Han et al (2012),
8
only the text of Twitter messages was used,
and training was based exclusively on geotagged
tweets, despite these accounting for only around
1% of the total public data on Twitter. In this
research, we include additional non-geotagged
tweets (e.g., posted from a non-GPS enabled de-
vice) for those users who have geotagged tweets
(allowing us to determine a home location for the
user).
In addition to including non-geotagged data in
modeling and inference, we further take advan-
tage of the text-based metadata embedded in a
user?s public profile (and included in the JSON ob-
ject for each tweet). This metadata is potentially
complementary to the tweet message and of bene-
fit for geolocation prediction, especially the user-
declared location and time zone, which we con-
sider here. Note that these are in free text rather
than a structured data format, and that while there
are certainly instances of formal place name de-
scriptions (e.g., Edinburgh, UK), they are often
informal (e.g., mel for Melbourne). As such, we
adopt a statistical approach to model each selected
metadata field, by capturing the text in the form
of character 4-grams, and training a multinomial
naive Bayes classifier for each field.
To combine together the tweet text and meta-
data fields, we use stacking (Wolpert, 1992). The
training of stacking consists of two steps. First,
a multinomial naive Bayes base classifier (L0) is
learned for each data type using 10-fold cross
validation. This is carried out for the tweet
text (TEXT), user-declared location (MB-LOC) and
user-declared time zone (MB-TZ). Next, a meta-
classifier (L1 classifier) is trained over the base
classifiers, using a logistic regression learner (Fan
et al, 2008).
4 Evaluation and Discussion
In this section, we compare our proposed stack-
ing approach with existing benchmarks on a public
dataset, and investigate the impact of time using a
recently collected dataset.
4.1 Evaluation Measures
In line with other work on user geolocation pre-
diction, we use three evaluation measures:
? Acc : The percentage of correct city-level
predictions.
? Acc@161 : The percentage of predicted lo-
cations which are within a 161km (100 mile)
Methods Acc Acc@161 Median
KL .117 .277 793
MB .126 .262 913
KL-NG .260 .487 181
MB-NG .280 .492 170
MB-LOC .405 .525 92
MB-TZ .064 .171 1330
STACKING .490 .665 9
Table 1: Results over WORLD
radius of the home location (Cheng et al,
2010), to capture near-misses (e.g., Edin-
burgh UK being predicted as Glasgow, UK).
? Median : The median distance from the pre-
dicted city to the home location (Eisenstein et
al., 2010).
4.2 Comparison with Benchmarks
We base our evaluation on the publicly-available
WORLD dataset of Han et al (2012). The dataset
contains 1.4M users whose tweets are primarily
identified as English based on the output of the
langid.py language identification tool (Lui and
Baldwin, 2012), and who have posted at least 10
geotagged tweets. The city-level home location
for a geotagged user is determined as follows.
First, each of a user?s geotagged tweets is mapped
to its nearest city (based on the same set of 3,709
cities used for the city-based location representa-
tion). Then, the most frequent city for a user is
selected as the home location.
To benchmark our method, we reimplement
two recently-published state-of-the-art methods:
(1) the KL-divergence nearest prototype method
of Roller et al (2012) based on KD-tree parti-
tioned grid cells, which we denote as KL; and
(2) the multinomial naive Bayes city-level geolo-
cation model of Han et al (2012), which we de-
note as MB. Because of the different class repre-
sentations, Acc numbers are not comparable be-
tween the benchmarks. To remedy this, we find
the closest city to the centroid of each grid cell in
the KD-tree representation, and map the classifi-
cation onto this city. We present results including
non-geotagged data for users with geotagged mes-
sages for the two methods, as KL-NG and MB-
NG, respectively. We also present results based
on the user-declared location (MB-LOC) and time
zone (MB-TZ), and finally the stacking method
(STACKING) which combines MB-NG, MB-LOC
and MB-TZ. The results are shown in Table 1.
9
The approximate doubling of Acc for KL-
NG and MB-NG over KL and MB, respectively,
demonstrates the high utility of non-geotagged
data in tweet text-based geolocation prediction.
Of the two original models, we can see that MB
is comparable to KL, in line with the findings of
Han et al (2012). The MB-LOC results are by far
the highest of all the base classifiers. Contrary to
the suggestion of Cheng et al (2010) that user-
declared locations are too unreliable to use for user
geolocation, we find evidence indicating that they
are indeed a valuable source of information for this
task. The best overall results are achieved for the
stacking approach (STACKING), assigning almost
half of the test users to the correct city-level lo-
cation, and improving more than four-fold on the
previous-best accuracy (i.e., MB). These results
also suggest that there is strong complementarity
between user metadata and tweet text.
4.3 Evaluation on Time-Heterogeneous Data
In addition to the original held-out test data
(WORLDtest) from WORLD, we also developed a
new geotagged evaluation dataset using the Twit-
ter Streaming API.2 This new LIVEtest dataset is
intended to evaluate the impact of time on predic-
tive accuracy. The training and test data in WORLD
are time-homogeneous as they are randomly sam-
pled from data collected in a relatively narrow time
window. In contrast, LIVEtest is much newer, col-
lected more than 1 year later than WORLD. Given
that Twitter users and topics change over time, an
essential question is whether the statistical model
learned from the ?old? training data is still effec-
tive over the ?new? test data?
The LIVEtest data was collected over 48 hours
from 2013/03/03 to 2013/03/05. By selecting
users with at least 10 geotagged tweets and a de-
clared language of English, 55k users were ob-
tained. For each user, their recent status updates
were aggregated, and non-English users were fil-
tered out based on the language predictions of
langid.py. For some users with geotagged
tweets from many cities, the most frequent city
might not be an appropriate representation of their
home location for evaluation. To improve the eval-
uation data quality, we therefore exclude users
who have less than 50% of their geotagged tweets
originating from a single city. After filtering, 32k
2https://dev.twitter.com/docs/api/1.1/
get/statuses/sample
LIVEtest Acc Acc@161 Median
MB-NG .268 (?.012) .510 (?.018) 151 ( ?19)
MB-LOC .326 (?.079) .465 (?.060) 306 (+214)
MB-TZ .065 (+.001) .160 (?.011) 1529 (+199)
STACKING .406 (?.084) .614 (?.051) 40 ( +31)
Table 2: Results over LIVEtest, and the absolute
fluctuation over the results for WORLDtest
users were obtained, forming the final LIVEtest
dataset. In the final LIVEtest, the smallest class
has only one test user, and the largest class has
569 users. The mean users per city is 27.76.
The results over LIVEtest, and the difference
in absolute score over WORLDtest, are shown in
Table 2. The stacked model accuracy numbers
drop 5?8% on LIVEtest, and the median error
distance increases moderately by 31km. Over-
all, the numbers suggest inference on WORLDtest,
which is time-homogenous with the training data
(taken from WORLD), is an easier classification
than LIVEtest, which is time-heterogeneous with
the training data. Training on ?old? data and test-
ing on ?new? data is certainly possible, however.
Looking over the results of the base classifiers, we
can see that the biggest hit is for MB-LOC clas-
sifier. In contrast, the accuracy for MB-NG and
MB-TZ is relatively stable (other than the sharp in-
crease in the median error distance for MB-TZ).
5 Architecture and Access
In this section, we describe the architecture of the
proposed geolocation system, as well as two ways
of accessing the live system.3 The core structure
of the system consists of two parts: (1) the inter-
face; (2) the back-end geolocation service.
We offer two interfaces to access the system: a
Twitter bot and a web interface. The Twitter bot
account is: @MELBLTFSD. A daemon process de-
tects any user mentions of the bot in tweets via
keyword matching through the Twitter search API.
The screen name of the tweet author is extracted
and sent to the back-end geolocation service, and
the predicted user geolocation is sent to the Twitter
user in a direct message, as shown in Figure 1.
Web access is via http://hum.csse.unimelb.
edu.au:9000/geo.html. Users can input a Twit-
ter user screen name through the web interface,
whereby a call is made to the back-end geoloca-
tion service to geolocate that user. The geoloca-
3The source code is available from https://github.
com/tq010or/acl2013
10
Figure 2: Web interface for user geolocation. The numbered green markers represent geotagged tweets.
These coordinates are utilised to validate our predictions, and are not used in the geolocation process.
The red marker is the predicted city-based user geolocation.
Figure 1: Twitter bot interface. When the Twit-
ter bot is mentioned in a tweet, that user is sent a
direct message with the predicted geolocation.
tion results are rendered on a map (along with any
geotagged tweets for the user) as in Figure 2.4
The back-end geolocation service crawls recent
tweets for a given user in real time,5 and word
and n-gram features are extracted from both the
text and the user metadata. These features are sent
to the L0 classifiers (TEXT, MB-LOC and MB-TZ),
and the L0 results are further fed into the L1 clas-
sifier for the final prediction.
6 Summary and Future Work
In this paper, we presented a city-level geoloca-
tion prediction system for Twitter users. Over a
public dataset, our stacking method ? exploiting
both tweet text and user metadata ? substantially
4Currently, only Google Chrome is supported. https:
//www.google.com/intl/en/chrome/
5Up to 200 tweets are crawled, the upper bound of mes-
sages returned per single request based on Twitter API v1.1.
outperformed benchmark methods. We further
evaluated model generalisation on a newer, time-
heterogeneous dataset. The overall results de-
creased by 5?8% in accuracy, compared with num-
bers on time-homogeneous data, primarily due to
the poor generalisation of the MB-LOC classifier.
In future work, we plan to further investigate
the cause of the MB-LOC classifier accuracy de-
crease on the new dataset. In addition, we?d like
to study differences in prediction accuracy across
cities. For cities with reliable predictions, the sys-
tem can be adapted as a preprocessing module for
downstream applications, e.g., local event detec-
tion based on users with reliable predictions.
Acknowledgements
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT centre
of Excellence programme.
References
Lars Backstrom, Jon Kleinberg, Ravi Kumar, and Jas-
mine Novak. 2008. Spatial variation in search en-
gine queries. In Proc. of WWW, pages 357?366,
Beijing, China.
Lars Backstrom, Eric Sun, and Cameron Marlow.
2010. Find me if you can: improving geographi-
cal prediction with social and spatial proximity. In
Proc. of WWW, pages 61?70, Raleigh, USA.
11
Orkut Buyukokkten, Junghoo Cho, Hector Garcia-
Molina, Luis Gravano, and Narayana Shivakumar.
1999. Exploiting geographical location informa-
tion of web pages. In ACM SIGMOD Workshop on
The Web and Databases, pages 91?96, Philadelphia,
USA.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based
approach to geo-locating twitter users. In Proc. of
CIKM, pages 759?768, Toronto, Canada.
David J. Crandall, Lars Backstrom, Daniel Hutten-
locher, and Jon Kleinberg. 2009. Mapping the
world?s photos. In Proc. of WWW, pages 761?770,
Madrid, Spain.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP,
pages 1277?1287, Cambridge, MA, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by find-
ing location indicative words. In Proc. of COLING,
pages 1045?1062, Mumbai, India.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from justin bieber?s heart:
the dynamics of the location field in user profiles.
In Proc. of SIGCHI, pages 237?246, Vancouver,
Canada.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the twit-
ter stream. In Proc. of WWW, pages 769?778, Lyon,
France.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in glasgow?: mod-
eling locations with tweets. In Proc. of the 3rd In-
ternational Workshop on Search and Mining User-
generated Contents, pages 61?68, Glasgow, UK.
Jochen L. Leidner and Michael D. Lieberman. 2011.
Detecting geographical references in the form of
place names and associated spatial natural language.
SIGSPATIAL Special, 3(2):5?11.
Michael D. Lieberman and Jimmy Lin. 2009. You
are where you edit: Locating wikipedia contributors
through edit histories. In ICWSM.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proc.
of the ACL, pages 25?30, Jeju Island, Korea.
Alan M. MacEachren, Anuj Jaiswal, Anthony C.
Robinson, Scott Pezanowski, Alexander Savelyev,
Prasenjit Mitra, Xiao Zhang, and Justine Blanford.
2011. Senseplace2: Geotwitter analytics support for
situational awareness. In IEEE Conference on Vi-
sual Analytics Science and Technology, pages 181?
190, Rhode Island, USA.
Jalal Mahmud, Jeffrey Nichols, and Clemens Drews.
2012. Where is this tweet from? inferring home lo-
cations of twitter users. In Proc. of ICWSM, Dublin,
Ireland.
Teng Qin, Rong Xiao, Lei Fang, Xing Xie, and Lei
Zhang. 2003. An efficient location extraction algo-
rithm by leveraging web contextual information. In
Proc. of SIGSPATIAL, pages 55?62, San Jose, USA.
Gianluca Quercini, Hanan Samet, Jagan Sankara-
narayanan, and Michael D. Lieberman. 2010. De-
termining the spatial reader scopes of news sources
using local lexicons. In Proc. of the 18th Interna-
tional Conference on Advances in Geographic In-
formation Systems, pages 43?52, San Jose, USA.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
USA.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language mod-
els on an adaptive grid. In Proc. of EMNLP, pages
1500?1510, Jeju Island, Korea.
Dominic Rout, Kalina Bontcheva, Daniel Preot?iuc-
Pietro, and Trevor Cohn. 2013. Where?s @wally?:
a classification approach to geolocating users based
on their social ties. In Proc. of the 24th ACM Con-
ference on Hypertext and Social Media, pages 11?
20, Paris, France.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. of WSDM, pages 723?732,
Seattle, USA.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proc. of WWW,
pages 851?860, Raleigh, USA.
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL, pages 955?964, Portland,
USA.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5(2):241?259.
Zhijun Yin, Liangliang Cao, Jiawei Han, Chengxiang
Zhai, and Thomas Huang. 2011. Geographical
topic discovery and comparison. In Proc. of WWW,
pages 247?256, Hyderabad, India.
Wenbo Zong, Dan Wu, Aixin Sun, Ee-Peng Lim,
and Dion Hoe-Lian Goh. 2005. On assigning
place names to geography related web pages. In
ACM/IEEE Joint Conference on Digital Libraries,
pages 354?362.
12
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
Jey Han Lau,
?
Paul Cook,
?
Diana McCarthy,
?
Spandana Gella,
?
and Timothy Baldwin
?
? Dept of Philosophy, King?s College London
? Dept of Computing and Information Systems, The University of Melbourne
? University of Cambridge
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
Abstract
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren?t attested
in the corpus, and identifying novel senses
in the corpus which aren?t captured in the
sense inventory.
1 Introduction
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al, 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions ? and predominant senses too ?
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al, 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al, 2007; Li et
al., 2010; Lau et al, 2012; Preiss and Stevenson,
2013; Cai et al, 2007; Knopp et al, 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses ? i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
259
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses ? i.e. senses that are
attested in the corpus but not in the sense inven-
tory ? would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al, 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
2 Background and Related Work
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al, 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al, 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al, 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al, 2007). Mc-
Carthy et al (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al, 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (?corroborators?), each of which, in turn,
depends on the document?s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al
260
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al, 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al, 2012). In a similar vein,
Peirsman et al (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
3 Methodology
Our methodology is based on the WSI system
described in Lau et al (2012),
1
which has been
shown (Lau et al, 2012; Lau et al, 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al, 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.
2
Note that in
their original work, Lau et al (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
1
Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2
This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
marginal, we make no use of parser-based features
in this paper.
3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.
4
To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.
5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic?
sense alignment methodology with portability in
mind ? it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.
6
We then calculate the Jensen?
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense s
i
and topic t
j
is:
sim(s
i
, t
j
) = 1? JS(S?T ) (1)
where S and T are the multinomial distributions
3
For hyper-parameters ? and ?, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al (2006).
4
To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5
The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6
Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al, 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
261
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
over words for sense s
i
and topic t
j
, respectively,
and JS(X?Y ) is the Jensen?Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(s
i
, t
j
)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense s
i
is given as follows:
prevalence(s
i
) =
T
?
j
(sim(s
i
, t
j
)? P (t
j
)) (2)
=
T
?
j
(
sim(s
i
, t
j
)?
f(t
j
)
?
T
k
f(t
k
)
)
where f(t
j
) is the frequency of topic t
j
(i.e. the
number of usages assigned to topic t
j
), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
4 WordNet Experiments
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) Acc
UB
, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);
7
and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (Acc
UB
), calculated as follows:
ERR = 1?
Acc
UB
? Acc
Acc
UB
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar?s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p < 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p > 0.1). Note
that there is still much room for improvement with
7
The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
262
Dataset
FS
CORPUS
MKWC HDP-WSI
Acc
UB
Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
Table 2: WSD accuracy for MKWC and HDP-WSI
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FS
CORPUS
upper bound] is indicated in boldface).
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
Table 3: Sense distribution evaluation of MKWC
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al, 2009). To this
end, we introduce a second evaluation metric:
the Jensen?Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p < 0.05) but the results
for the other two datasets are not (p > 0.1 in each
case).
Dataset
FS
CORPUS
FS
DICT
HDP-WSI
Acc
UB
Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FS
CORPUS
upper bound] is indicated in boldface).
Dataset FS
CORPUS
FS
DICT
HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
Table 5: Sense distribution evaluation of HDP-
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.
8
The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al, 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
?flat? sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al, 2007) com-
pared to a dependency one,
9
it is not stated how
8
McCarthy et al (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
9
The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/
?
lindek/downloads.htm.
263
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
5 Macmillan Experiments
In our second set of experiments, we move to a
new dataset (Gella et al, to appear) based on text
from ukWaC (Ferraresi et al, 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary
10
(henceforth ?Macmillan?). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.
11
In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al, 2004; Navigli et al, 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3?Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al, 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
?Other? in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
10
http://www.macmillandictionary.com/
11
Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly ?Other?), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
5.1 Learning Sense Distributions
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (?FS
CORPUS
?), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (?FS
DICT
?), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FS
DICT
) over both datasets (McNe-
mar?s test; p < 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.
12
This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12
The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al (2005) is comparable to UKWAC.
264
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),
13
making
the task slightly easier in the Macmillan case.
5.2 Identification of Unattested Senses
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren?t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren?t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
st-affinity(s
i
) =
?
T
j
sim(s
i
, t
j
)
?
S
k
?
T
l
sim(s
k
, t
l
)
(3)
where sim(s
i
, t
j
) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13
Note that the set of lemmas differs between the respec-
tive datasets, so this isn?t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,
14
evaluated using
sense-level precision (P ), recall (R) and F-score
(F ) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092? 0.044 over UKWAC
and 0.125?0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
5.3 Identification of Novel Senses
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn?t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these ?novel senses?, and define ?novel
sense identification? to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren?t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn?t ob-
served in the corpus. Also, while we have annota-
tions of ?Other? usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can?t use the
?Other? annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
14
We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
265
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
20 0.0?0.2 0.052?0.009 0.35 0.42 0.36
9 0.2?0.4 0.089?0.024 0.24 0.59 0.29
6 0.4?0.6 0.061?0.004 0.63 0.64 0.63
Table 7: Classification of usages with novel sense for all target lemmas.
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
9 0.2?0.4 0.093?0.023 0.50 0.66 0.52
6 0.4?0.6 0.099?0.018 0.73 0.90 0.80
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0?0.2; medium = 0.2?
0.4; and high = 0.4?0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if > 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al, 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
ts-affinity(t
j
) =
?
S
i
sim(s
i
, t
j
)
?
T
l
?
S
k
sim(s
k
, t
l
)
(4)
where, once again, sim(s
i
, t
j
) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances ? instances from target lem-
mas that have a sense artificially removed and
those that do not ? impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
266
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0?0.2 0.4543
9 11 0.2?0.4 0.0391
6 14 0.4?0.6 0.0247
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
novelty(w) = min
t
j
(
max
s
i
sim(s
i
, t
j
)
f(t
j
)
)
(5)
where f(t
j
) is the frequency of topic t
j
in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as a low-probability topic for
a high-frequency word, or a high-probability topic
for a low-frequency word) are indicative of a novel
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of a one-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
novel sense) are statistically different.
15
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p < 0.05), but the results are less promising for
the low-frequency novel senses.
6 Discussion
Our methodologies for the two proposed tasks of
identifying unused and novel senses are simple
15
Note that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas in the dataset.
extensions to demonstrate the flexibility and ro-
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of sim(s
i
, t
j
) for com-
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
A natural next step for this research would be to
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised WSD of individual token
occurrences of a given word.
In summary, we have proposed a topic
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian Re-
search Council.
267
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33?41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136?145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277?281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum?e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435?1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010?
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89?96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28?34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49?65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128?135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47?54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
268
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561?568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics ? Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233?
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V?olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97?103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591?601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307?311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217?221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24?26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220?1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280?287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303?308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
269
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121?128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680?684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga?etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257?282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566?1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala, Sweden.
270
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 248?253, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Unsupervised Word Usage Similarity in Social Media Texts
Spandana Gella,? Paul Cook,? and Bo Han??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,
hanb@student.unimelb.edu.au
Abstract
We propose an unsupervised method for au-
tomatically calculating word usage similar-
ity in social media data based on topic mod-
elling, which we contrast with a baseline dis-
tributional method and Weighted Textual Ma-
trix Factorization. We evaluate these meth-
ods against a novel dataset made up of human
ratings over 550 Twitter message pairs anno-
tated for usage similarity for a set of 10 nouns.
The results show that our topic modelling ap-
proach outperforms the other two methods.
1 Introduction
In recent years, with the growing popularity of so-
cial media applications, there has been a steep rise
in the amount of ?post?-based user-generated text
(including microblog posts, status updates and com-
ments) (Bennett, 2012). This data has been iden-
tified as having potential for applications ranging
from trend analysis (Lau et al, 2012a) and event de-
tection (Osborne et al, 2012) to election outcome
prediction (O?Connor et al, 2010). However, given
that posts are generally very short, noisy and lack-
ing in context, traditional NLP approaches tend to
perform poorly over social media data (Hong and
Davison, 2010; Ritter et al, 2011; Han et al, 2012).
This is the first paper to address the task of lexi-
cal semantic interpretation in microblog data based
on word usage similarity. Word usage similar-
ity (USIM: Erk et al (2009)) is a relatively new
paradigm for capturing similarity in the usages of
a given word independently of any lexicon or sense
inventory. The task is to rate on an ordinal scale the
similarity in usage between two different usages of
the same word. In doing so, it avoids common issues
in conventional word sense disambiguation, relating
to sense underspecification, the appropriateness of a
static sense inventory to a given domain, and the in-
ability to capture similarities/overlaps between word
senses. As an example of USIM, consider the fol-
lowing pairing of Twitter posts containing the target
word paper:
1. Deportation of Afghan Asylum Seekers from
Australia : This paper aims to critically evalu-
ate a newly signed agree.
2. @USER has his number on a piece of paper
and I walkd off!
The task is to predict a real-valued number in the
range [1, 5] for the similarity in the respective us-
ages of paper, where 1 indicates the usages are com-
pletely different and 5 indicates they are identical.
In this paper we develop a new USIM dataset
based on Twitter data. In experiments on this dataset
we demonstrate that an LDA-based topic modelling
approach outperforms a baseline distributional se-
mantic approach and Weighted Textual Matrix Fac-
torization (WTMF: Guo and Diab (2012a)). We
further show that context expansion using a novel
hashtag-based strategy improves both the LDA-
based method and WTMF.
2 Related Work
Word sense disambiguation (WSD) is the task of
determining the particular sense of a word from a
given set of pre-defined senses (Navigli, 2009). It
248
contrasts with word sense induction (WSI), where
the senses of a given target word are induced from
an unannotated corpus of usages, and the induced
senses are then used to disambiguate each token us-
age of the word (Manandhar et al, 2010; Lau et
al., 2012b). WSD and WSI have been the predomi-
nant paradigms for capturing and evaluating lexical
semantics, and both assume that each usage corre-
sponds to exactly one of a set of discrete senses of
the target word, and that any prediction other than
the ?correct? sense is equally wrong.
Erk et al (2009) showed that, given a sense in-
ventory, there is a high likelihood of multiple senses
being compatible with a given usage, and proposed
USIM as a means of capturing the similarity in us-
age between a pairing of usages of a given word.
As part of their work, they released a dataset, which
Lui et al (2012) recently developed a topic mod-
elling approach over. Based on extensive experi-
mentation, they demonstrated the best results with
a single topic model for all target words based on
full document context. Our topic modelling-based
approach to USIM builds off the approach of Lui
et al (2012). Guo and Diab (2012a) observed that,
when applied to short texts, the effectiveness of la-
tent semantic approaches can be boosted by expand-
ing the text to include ?missing? words. Based on
this, they proposed Weighted Textual Matrix Factor-
ization (WTMF), based on weighted matrix factor-
ization (Srebro and Jaakkola, 2003). Here we ex-
periment with both LDA based topic modeling and
WTMF to estimate word similarities in twitter data.
LDA based topic modeling has been earlier studied
on Twitter data for tweet classification (Ramage et
al., 2010) and tweet clustering (Jin et al, 2011).
3 Data Preparation
This section describes the construction of the USIM-
tweet dataset based on microblog posts (?tweets?)
from Twitter. We describe the pre-processing steps
taken to sample the tweets in our datasets, outline
the annotation process, and then describe the back-
ground corpora used in our experiments.
3.1 Data preprocessing
Around half of Twitter is non-English (Hong et al,
2011), so our first step was to automatically identify
English tweets using langid.py (Lui and Bald-
win, 2012). We next performed lexical normaliza-
tion using the dictionary of Han et al (2012) to con-
vert lexical variants (e.g., tmrw) to their standard
forms (e.g., tomorrow) and reduce data sparseness.
As our target words, we chose the 10 nouns from
the original USIM dataset of Erk et al (2009) (bar,
charge, execution, field, figure, function, investiga-
tor, match, paper, post), and identified tweets con-
taining the target words as nouns using the CMU
Twitter POS tagger (Owoputi et al, 2012).
3.2 Annotation Settings and Data
To collect word usage similarity scores for Twitter
message pairs, we used a setup similar to that of
Erk et al (2009) using Amazon Mechanical Turk:
we asked the annotators to rate each sentence pair
with an integer score in the range [1, 5] using sim-
ilar annotation guidelines to Erk et al We ran-
domly sampled twitter messages from the TREC
2011 microblog dataset,1 and for each of our 10
nouns, we collected 55 pairs of messages satisfying
the preprocessing described in Section 3.1. These
55 pairs are chosen such that each tweet has at least
4 content words (nouns, verbs, adjectives and ad-
verbs) and at least 70+% of its post-normalized to-
kens in the Aspell dictionary (v6.06)2; these restric-
tions were included in an effort to ensure the tweets
would contain sufficient linguistic content to be in-
terpretable.3 We created 110 Mechanical Turk jobs
(referred to as HITs), with each HIT containing 5
randomly-selected message pairs. For this annota-
tion the tweets were presented in their original form,
i.e., without lexical normalisation applied. Each HIT
was completed by 10 ?turkers?, resulting in a total
of 5500 annotations. The annotation was restricted
to turkers based in the United States having had at
least 95% of their previous HITs accepted. In total,
the annotation was carried out by 68 turkers, each
completing between 1 and 100 HITs.
To detect outlier annotators, we calculated the av-
erage Spearman correlation score (?) of every an-
notator by correlating their annotation values with
every other annotator and taking the average. We
1http://trec.nist.gov/data/tweets/
2http://aspell.net/
3In future analyses we intend to explore the potential impact
of these restrictions on the resulting dataset.
249
Word Orig Exp Word Orig Exp
bar 180k 186k function 26k 27k
charge 41k 43k investigator 17k 19k
execution 28k 30k field 72k 75k
figure 28k 29k match 126k 133k
paper 210k 218k post 299k 310k
Table 1: The number of tweets for each word in each
background corpus (?Orig? = ORIGINAL; ?Exp?
= EXPANDED; RANDEXPANDED, not shown, con-
tains the same number of tweets as EXPANDED).
accepted all the annotations of annotators whose av-
erage ? is greater than 0.6; this corresponded to 95%
of the annotators. Two annotators had a negative
average ? and their annotations (only 4 HITs to-
tal) were discarded. For the other annotators (i.e.,
0 ? ? ? 0.6), we accepted each of their HITs on
a case by case basis; a HIT was accepted only if
at least 2 out of 5 of the annotations for that HIT
were within ?2.0 of the mean for that annotation
based on the judgments of the other turkers. (21
HITS were discarded using this heuristic.) We fur-
ther eliminated 7 HITS which have incomplete judg-
ments. In total only 32 HITs (of the 1100 HITs com-
pleted) were discarded through these heuristics. The
weighted average Spearman correlation over all an-
notators after this filtering is 0.681, which is some-
what higher than the inter-annotator agreement of
0.548 reported by Erk et al (2009). This dataset is
available for download.
3.3 Background Corpus
We created three background corpora based on data
from the Twitter Streaming API in February 2012
(only tweets satisfying the preprocessing steps in
Section 3.1 were chosen).
ORIGINAL: 1 million tweets which contain at least
one of the 10 target nouns;
EXPANDED: ORIGINAL plus an additional 40k
tweets containing at least 1 hashtag attested in
ORIGINAL with an average frequency of use of
10?35 times/hour (medium frequency);
RANDEXPANDED: ORIGINAL plus 40k randomly
sampled tweets containing the same target
nouns.
We select medium-frequency hashtags because low-
frequency hashtags tend to be ad hoc and non-
thematic in nature, while high-frequency hash-
tags are potentially too general to capture us-
age similarity. Statistics for ORIGINAL and EX-
PANDED/RANDEXPANDED are shown in Table 1.
RANDEXPANDED is sampled such that it has the
same number of tweets as EXPANDED.
4 Methodology
We propose an LDA topic modelling-based ap-
proach to the USIM task, which we contrast with
a baseline distributional model and WTMF. In all
these methods, the similarity between two word us-
ages is measured using cosine similarity between the
vector representation of each word usage.
4.1 Baseline
We represent each target word usage in a tweet as a
second-order co-occurrence vector (Schu?tze, 1998).
A second-order co-occurrence vector is built from
the centroid (summation) of all the first-order co-
occurrence vectors of the context words in the same
tweet as the target word.
The first-order co-occurrence vector for a given
target word represents the frequency with which that
word co-occurs in a tweet with other context words.
Each first-order vector is built from all tweets which
contain a context word and the target word catego-
rized as noun in the background corpus, thus sensi-
tizing the first-order vector to the target word. We
use the most frequent 10000 words (excluding stop-
words) in the background corpus as our first-order
vector dimensions/context words. Context words
(dimensions) in the first-order vectors are weighted
by mutual information.
Second-order co-occurrence is used as the context
representation to reduce the effects of data sparse-
ness in the tweets (which cannot be more than 140
codepoints in length).
4.2 Weighted Textual Matrix Factorization
WTMF (Guo and Diab, 2012b) addresses the data
sparsity problem suffered by many latent variable
250
Model ORIGINAL EXPANDED RANDEXPANDED
Baseline 0.09 0.08 0.09
WTMF 0.02 0.09 0.06
LDA 0.20 0.29 0.18
Table 2: Spearman rank correlation (?) for each
method based on each background corpus. The best
result for each corpus is shown in bold.
models by predicting ?missing? words on the ba-
sis of the message content, and including them in
the vector representation. Guo and Diab showed
WTMF to outperform LDA on the SemEval-2012
semantic textual similarity task (STS) (Agirre et al,
2012). The semantic space required for this model
as applied here is built from the background tweets
corresponding to the target word. We experimented
with the missing weight parameter wm of WTMF
in the range [0.05, 0.01, 0.005, 0.0005] and with di-
mensions K=100 and report the best results (wm =
0.0005).
4.3 Topic Modelling
Latent Dirichlet Allocation (LDA) (Blei et al, 2003)
is a generative model in which a document is mod-
eled as a finite mixture of topics, where each topic is
represented as a multinomial distribution of words.
We treat each tweet as a document. Topics sensi-
tive to each target word are generated from its corre-
sponding background tweets. We topic model each
target word individually,4 and create a topic vector
for each word usage based on the topic allocations of
the context words in that usage. We use Gibbs sam-
pling in Mallet (McCallum, 2002) for training and
inference of the LDA model. We experimented with
the number of topics T for each target word ranging
from 2 to 500. We optimized the hyper parameters
by choosing those which best fit the data every 20 it-
erations over a total of 800 iterations, following 200
burn-in iterations.
4Unlike Lui et al (2012) we found a single topic model for
all target words to perform very poorly.
l
l l
l
l l l
l
l l
l l l
l
l l l
2 3 5 8 10 20 30 50 100 150 200 250 300 350 400 450 500
T
?
0.1
0.0
0.1
0.2
0.3
Sp
ea
rm
an
 
co
rre
lati
on
 
? l Original
Expanded
RandExpanded
Figure 1: Spearman rank correlation (?) for LDA for
varying numbers of topics (T ) using different back-
ground corpora.
5 Results
We evaluate the above methods for word usage sim-
ilarity on the dataset constructed in Section 3.2. We
evaluate our models against the mean human ratings
using Spearman?s rank correlation. Table 2 presents
results for each method using each background cor-
pus. The results for LDA are for the optimal set-
ting for T (8, 5, and 20 for ORIGINAL, EXPANDED,
and RANDEXPANDED, respectively). LDA is su-
perior to both the baseline and WTMF using each
background corpus. The performance of LDA im-
proves for EXPANDED but not RANDEXPANDED,
over ORIGINAL, demonstrating the effectiveness of
our hashtag based corpus expansion strategy.
In Figure 1 we plot the rank correlation of LDA
across all words against the number of topics (T ).
As the number of topics increases beyond a certain
number, the rank correlation decreases. LDA trained
on EXPANDED consistently outperforms ORIGINAL
and RANDEXPANDED for lower values of T (i.e.,
T <= 20).
In Table 3, we show results for LDA over each tar-
get word, for ORIGINAL and EXPANDED. (Results
for RANDEXPANDED are not shown but are similar
to ORIGINAL.) Results are shown for the optimal
T for each lemma, and the optimal T over all lem-
mas. Optimizing T for each lemma gives an indica-
tion of the upperbound of the performance of LDA,
and unsurprisingly gives better performance than us-
251
Lemma
ORIGINAL EXPANDED
Per lemma Global Per lemma Global
? (T ) ? (T=8) ? (T ) ? (T=5)
bar 0.39 (10) 0.28 0.35 (50) 0.1
charge 0.27 (30) 0.04 0.33 (20) ?0.08
execution 0.43 (8) 0.43 0.58 (5) 0.58
field 0.46 (5) 0.33 0.53 (10) 0.32
figure 0.24 (150) 0.06 0.24 (250) 0.14
function 0.44 (8) 0.44 0.40 (10) 0.27
investigator 0.3 (30) 0.05 0.50 (5) 0.50
match 0.28 (5) 0.26 0.45 (5) 0.45
paper 0.29 (30) 0.20 0.32 (30) 0.22
post 0.1 (3) ?0.13 0.2 (30) ?0.01
Table 3: Spearman?s ? using LDA for the optimal T
for each lemma (Per lemma) and the best T over all
lemmas (Global) using ORIGINAL and EXPANDED.
? values that are significant at the 0.05 level are
shown in bold.
ing a fixed T for all lemmas. This suggests that ap-
proaches that learn an appropriate number of topics
(e.g., HDP, (Teh et al, 2006)) could give further im-
provements; however, given the size of the dataset,
the computational cost of HDP could be a limitation.
Contrasting our results with a fixed number of
topics to those of Lui et al (2012), our highest rank
correlation of 0.29 (T = 5 using EXPANDED) is
higher than the 0.11 they achieved over the origi-
nal USIM dataset (where the documents offer an or-
der of magnitude more context). The higher inter-
annotator agreement for USIM-tweet compared to
the original USIM dataset (Section 3.2), combined
with this finding, demonstrates that USIM over mi-
croblog data is indeed a viable task.
Returning to the performance of LDA relative
to WTMF in Table 2, the poor performance of
WTMF is somewhat surprising here given WTMF?s
encouraging performance on the somewhat similar
SemEval-2012 STS task. This difference is possi-
bly due to the differences in the tasks: usage simi-
larity measures the similarity of the usage of a tar-
get word while STS measures the similarity of two
texts. Differences in domain ? i.e., Twitter here
and more standard text for STS ? could also be a
factor. WTMF attempts to alleviate the data spar-
sity problem by adding information from ?missing?
words in a text by assigning a small weight to these
missing words. Because of the prevalence of lexical
variation on Twitter, some missing words might be
counted multiple times (e.g., coool, kool, and kewl
all meaning roughly cool) thus indirectly assigning
higher weights to the missing words leading to the
lower performance of WTMF compared to LDA.
6 Summary
We have analysed word usage similarity in mi-
croblog data. We developed a new dataset (USIM-
tweet) for usage similarity of nouns over Twitter.
We applied a topic modelling approach to this task,
and contrasted it with baseline and benchmark meth-
ods. Our results show that the LDA-based approach
outperforms the other methods over microblog data.
Moreover, our novel hashtag-based corpus expan-
sion strategy substantially improves the results.
In future work, we plan to expand our annotated
dataset, experiment with larger background corpora,
and explore alternative corpus expansion strategies.
We also intend to further analyse the difference in
performance LDA and WTMF on similar data.
Acknowledgements
We are very grateful to Timothy Baldwin for his
tremendous help with this work. We additionally
thank Diana McCarthy for her insightful comments
on this paper. We also acknowledge the European
Erasmus Mundus Masters Program in Language and
Communication Technologies from the European
Commission.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence programme.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montreal, Canada.
Shea Bennett. 2012. Twitter on track for
500 million total users by March, 250 mil-
lion active users by end of 2012. http:
//www.mediabistro.com/alltwitter/
twitter-active-total-users_b17655.
252
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of the Joint conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing (ACL-IJCNLP
2009), pages 10?18, Singapore.
Weiwei Guo and Mona Diab. 2012a. Modeling sen-
tences in the latent space. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864?872, Jeju, Republic of Korea.
Weiwei Guo and Mona Diab. 2012b. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 586?590, Montreal, Canada.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary for
microblogs. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning 2012,
pages 421?432, Jeju, Republic of Korea.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proc. of the First
Workshop on Social Media Analytics, pages 80?88.
Lichan Hong, Gregoria Convertino, and Ed H. Chi. 2011.
Language matters in Twitter: A large scale study. In
Proceedings of the 5th International Conference on
Weblogs and Social Media (ICWSM 2011), pages 518?
521, Barcelona, Spain.
Ou Jin, Nathan N Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Proc.
of the 20th ACM International Conference on Informa-
tion and Knowledge Management, pages 775?784.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012a. On-line trend analysis with topic models:
#twitter trends detection topic model online. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012b. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics (EACL
2012), pages 591?601, Avignon, France.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012) Demo Session,
pages 25?30, Jeju, Republic of Korea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages 33?
41, Dunedin, New Zealand.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
4th International Conference on Weblogs and Social
Media, pages 122?129, Washington, USA.
Miles Osborne, Sasa Petrovic?, Richard McCreadie, Craig
Macdonald, and Iadh Ounis. 2012. Bieber no more:
First story detection using Twitter and Wikipedia. In
Proceedings of the SIGIR 2012 Workshop on Time-
aware Information Access, Portland, USA.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, and Nathan Schneider. 2012. Part-of-speech
tagging for Twitter: Word clusters and other advances.
Technical Report CMU-ML-12-107, Carnegie Mellon
University.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, volume 5, pages 130?137.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1524?1534, Edinburgh, UK.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the
20th International Conference on Machine Learning,
Washington, USA.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
253
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 266?275, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Predicting the Compositionality of Multiword Expressions
Using Translations in Multiple Languages
Bahar Salehi?? and Paul Cook?
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au
Abstract
In this paper, we propose a simple, language-
independent and highly effective method for
predicting the degree of compositionality of
multiword expressions (MWEs). We compare
the translations of an MWE with the trans-
lations of its components, using a range of
different languages and string similarity mea-
sures. We demonstrate the effectiveness of
the method on two types of English MWEs:
noun compounds and verb particle construc-
tions. The results show that our approach is
competitive with or superior to state-of-the-art
methods over standard datasets.
1 Compositionality of MWEs
A multiword expression (MWE) is any combina-
tion of words with lexical, syntactic or semantic
idiosyncrasy (Sag et al, 2002; Baldwin and Kim,
2009), in that the properties of the MWE are not
predictable from the component words. For exam-
ple, with ad hoc, the fact that neither ad nor hoc are
standalone English words, makes ad hoc a lexically-
idiosyncratic MWE; with shoot the breeze, on the
other hand, we have semantic idiosyncrasy, as the
meaning of ?to chat? in usages such as It was good
to shoot the breeze with you1 cannot be predicted
from the meanings of the component words shoot
and breeze.
Semantic idiosyncrasy has been of particular in-
terest to NLP researchers, with research on bi-
nary compositional/non-compositional MWE clas-
1The example is taken from http://www.
thefreedictionary.com
sification (Lin, 1999; Baldwin et al, 2003), or
a three-way compositional/semi-compositional/non-
compositional distinction (Fazly and Stevenson,
2007). There has also been research to suggest that
MWEs span the entire continuum from full compo-
sitionality to full non-compositionality (McCarthy et
al., 2003; Reddy et al, 2011).
Investigating the degree of MWE compositional-
ity has been shown to have applications in informa-
tion retrieval and machine translation (Acosta et al,
2011; Venkatapathy and Joshi, 2006). As an exam-
ple of an information retrieval system, if we were
looking for documents relating to rat race (mean-
ing ?an exhausting routine that leaves no time for
relaxation?2), we would not be interested in docu-
ments on rodents. These results underline the need
for methods for broad-coverage MWE composition-
ality prediction.
In this research, we investigate the possibility of
using an MWE?s translations in multiple languages
to measure the degree of the MWE?s compositional-
ity, and investigate how literal the semantics of each
component is within the MWE. We use Panlex to
translate the MWE and its components, and compare
the translations of the MWE with the translations
of its components using string similarity measures.
The greater the string similarity, the more composi-
tional the MWE is.
Whereas past research on MWE compositionality
has tended to be tailored to a specific MWE type
(McCarthy et al, 2007; Kim and Baldwin, 2007;
Fazly et al, 2009), our method is applicable to
any MWE type in any language. Our experiments
2This definition is from WordNet 3.1.
266
over two English MWE types demonstrate that our
method is competitive with state-of-the-art methods
over standard datasets.
2 Related Work
Most previous work on measuring MWE composi-
tionality makes use of lexical, syntactic or semantic
properties of the MWE. One early study on MWE
compositionality was Lin (1999), who claimed that
the distribution of non-compositional MWEs (e.g.
shoot the breeze) differs significantly from the dis-
tribution of expressions formed by substituting one
of the components with a semantically similar word
(e.g. shoot the wind). Unfortunately, the method
tends to fall down in cases of high statistical id-
iosyncrasy (or ?institutionalization?): consider fry-
ing pan which is compositional but distributionally
very different to phrases produced through word-
substitution such as sauteing pan or frying plate.
Some research has investigated the syntactic
properties of MWEs, to detect their composition-
ality (Fazly et al, 2009; McCarthy et al, 2007).
The assumption behind these methods is that non-
compositional MWEs are more syntactically fixed
than compositional MWEs. For example, make a de-
cision can be passivised, but shoot the breeze cannot.
One serious problem with syntax-based methods is
their lack of generalization: each type of MWE has
its own characteristics, and these characteristics dif-
fer from one language to another. Moreover, some
MWEs (such as noun compounds) are not flexible
syntactically, no matter whether they are composi-
tional or non-compositional (Reddy et al, 2011).
Much of the recent work on MWEs focuses on
their semantic properties, measuring the semantic
similarity between the MWE and its components us-
ing different resources, such as WordNet (Kim and
Baldwin, 2007) or distributional similarity relative
to a corpus (e.g. based on Latent Semantic Analysis:
Schone and Jurafsky (2001), Bannard et al (2003),
Reddy et al (2011)). The size of the corpus is im-
portant in methods based on distributional similarity.
Unfortunately, however, large corpora are not avail-
able for all languages.
Reddy et al (2011) hypothesize that the num-
ber of common co-occurrences between a given
MWE and its component words indicates the de-
gree of compositionality of that MWE. First, the co-
occurrences of a given MWE/word are considered
as the values of a vector. They then measure the
Cosine similarity between the vectors of the MWE
and its components. Bannard et al (2003) presented
four methods to measure the compositionality of En-
glish verb particle constructions. Their best result
is based on the previously-discussed method of Lin
(1999) for measuring compositionality, but uses a
more-general distributional similarity model to iden-
tify synonyms.
Recently, a few studies have investigated using
parallel corpora to detect the degree of composi-
tionality (Melamed, 1997; Moiro?n and Tiedemann,
2006; de Caseli et al, 2010; Salehi et al, 2012).
The general approach is to word-align the source
and target language sentences and analyse align-
ment patterns for MWEs (e.g. if the MWE is al-
ways aligned as a single ?phrase?, then it is a strong
indicator of non-compositionality). de Caseli et
al. (2010) consider non-compositional MWEs to be
those candidates that align to the same target lan-
guage unit, without decomposition into word align-
ments. Melamed (1997) suggests using mutual in-
formation to investigate how well the translation
model predicts the distribution of words in the tar-
get text given the distribution of words in the source
text. Moiro?n and Tiedemann (2006) show that en-
tropy is a good indicator of compositionality, be-
cause word alignment models are often confused by
non-compositional MWEs. However, this assump-
tion does not always hold, especially when deal-
ing with high-frequency non-compositional MWEs.
Salehi et al (2012) tried to solve this problem with
high frequency MWEs by using word alignment in
both directions.3 They computed backward and for-
ward entropy to try to remedy the problem with es-
pecially high-frequency phrases. However, their as-
sumptions were not easily generalisable across lan-
guages, e.g., they assume that the relative frequency
of a specific type of MWE (light verb constructions)
in Persian is much greater than in English.
Although methods using bilingual corpora are in-
tuitively appealing, they have a number of draw-
backs. The first and the most important problem
3The IBM models (Brown et al, 1993), e.g., are not bidi-
rectional, which means that the alignments are affected by the
alignment direction.
267
is data: they need large-scale parallel bilingual cor-
pora, which are available for relatively few language
pairs. Second, since they use statistical measures,
they are not suitable for measuring the composition-
ality of MWEs with low frequency. And finally,
most experiments have been carried out on English
paired with other European languages, and it is not
clear whether the results translate across to other
language pairs.
3 Resources
In this research, we use the translations of MWEs
and their components to estimate the relative de-
gree of compositionality of a MWE. There are
several resources available to translate words into
various languages such as Babelnet (Navigli and
Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et
al., 2010) and Google Translate.6 As we are ide-
ally after broad coverage over multiple languages
and MWEs/component words in a given language,
we exclude Babelnet and Wiktionary from our cur-
rent research. Babelnet covers only six languages
at the time of writing this paper, and in Wiktionary,
because it is constantly being updated, words and
MWEs do not have translations into the same lan-
guages. This leaves translation resources such as
Panlex and Google Translate. However, after man-
ually analysing the two resources for a range of
MWEs, we decided not to use Google Translate for
two reasons: (1) we consider the MWE out of con-
text (i.e., we are working at the type level and do not
consider the usage of the MWE in a particular sen-
tence), and Google Translate tends to generate com-
positional translations of MWEs out of context; and
(2) Google Translate provides only one translation
for each component word/MWE. This left Panlex.
Panlex is an online translation database that is
freely available. It contains lemmatized words and
MWEs in a large variety of languages, with lemma-
based (and less frequently sense-based) links be-
tween them. The database covers more than 1353
languages, and is made up of 12M lemmas and ex-
pressions. The translations are sourced from hand-
made electronic dictionaries, making it more accu-
4http://lcl.uniroma1.it/babelnet/
5http://www.wiktionary.org/
6http://translate.google.com/
rate than translation dictionaries generated automat-
ically, e.g. through word alignment. Usually there
are several direct translations for a word/MWE
from one language to another, as in translations
which were extracted from electronic dictionaries. If
there is no direct translation for a word/MWE in the
database, we can translate indirectly via one or more
pivot languages (indirect translation: Soderland et
al. (2010)). For example, English ivory tower has
direct translations in only 13 languages in Panlex,
including French (tour d?ivoire) but not Esperanto.
There is, however, a translation of tour d?ivoire into
Esperanto (ebura turo), allowing us to infer an indi-
rect translation between ivory tower and ebura turo.
4 Dataset
We evaluate our method over two datasets, as de-
scribed below.
REDDY (Reddy et al, 2011): 90 English (binary)
noun compounds (NCs), where the overall NC and
each component word has been annotated for com-
positionality on a scale from 0 (non-compositional)
to 5 (compositional). In order to avoid issues
with polysemy, the annotators were presented with
each NC in a sentential context. The authors tried
to achieve a balance of compositional and non-
compositional NCs: based on a threshold of 2.5, the
dataset consists of 43 (48%) compositional NCs, 46
(51%) NCs with a compositional usage of the first
component, and 54 (60%) NCs with a compositional
usage of the second component.
BANNARD (Bannard, 2006): 160 English verb
particle constructions (VPCs) were annotated for
compositionality relative to each of the two compo-
nent words (the verb and the particle). Each annota-
tor was asked to annotate each of the verb and parti-
cle as yes, no or don?t know. Based on the ma-
jority annotation, among the 160 VPCs, 122 (76%)
are verb-compositional and 76 (48%) are particle-
compositional.
We compute the proportion of yes tags to get the
compositionality score. This dataset, unlike REDDY,
does not include annotations for the compositional-
ity of the whole VPC, and is also less balanced, con-
taining more VPCs which are verb-compositional
than verb-non-compositional.
268
Score
Panlex
ComponentsMWE
Translate
Compare
...
...
Translations
Figure 1: Schematic of our proposed method
5 Method
To predict the degree of compositionality of an
MWE, we require a way to measure the semantic
similarity of the MWE with its components. Our
hypothesis is that compositional MWEs are more
likely to be word-for-word translations in a given
language than non-compositional MWEs. Hence, if
we can locate the translations of the components in
the translation of the MWE, we can deduce that it
is compositional. Our second hypothesis is that the
more languages we use as the basis for determin-
ing translation similarity between the MWE and its
component words, the more accurately we will be
able to estimate compositionality. Thus, rather than
using just one translation language, we experiment
with as many languages as possible.
Figure 1 provides a schematic outline of our
method. The MWE and its components are trans-
lated using Panlex. Then, we compare the transla-
tion of the MWE with the translations of its compo-
nents. In order to locate the translation of each com-
ponent in the MWE translation, we use string simi-
English Persian Translation
kick the bucket mord
kick zad
the ?
bucket satl
make a decision tasmim gereft
make sakht
a yek
decision tasmim
public service khadamaat omumi
public omumi
service khedmat
Table 1: English MWEs and their components with their
translation in Persian. Direct matches between the trans-
lation of a MWE and its components are shown in bold;
partial matches are underlined.
larity measures. The score shown in Figure 1 is de-
rived from a given language. In Section 6, we show
how to combine scores across multiple languages.
As an example of our method, consider the
English-to-Persian translation of kick the bucket as
a non-compositional MWE and make a decision as
a semi-compositional MWE (Table 1).7 By locating
the translation of decision (tasmim) in the translation
ofmake a decision (tasmim gereftan), we can deduce
that it is semi-compositional. However, we cannot
locate any of the component translations in the trans-
lation of kick the bucket. Therefore, we conclude
that it is non-compositional. Note that in this simple
example, the match is word-level, but that due to the
effects of morphophonology, the more likely situa-
tion is that the components don?t match exactly (as
we observe in the case of khadamaat and khedmat
for the public service example), which motivates our
use of string similarity measures which can capture
partial matches.
We consider the following string similarity mea-
sures to compare the translations. In each case,
we normalize the output value to the range [0, 1],
where 1 indicates identical strings and 0 indicates
completely different strings. We will indicate the
translation of the MWE in a particular language t as
MWE t, and the translation of a given component in
7Note that the Persian words are transliterated into English
for ease of understanding.
269
language t as component t.
Longest common substring (LCS): The LCS
measure finds the longest common substring be-
tween two strings. For example, the LCS between
ABABC and BABCAB is BABC. We calculate a nor-
malized similarity value based on the length of the
LCS as follows:
LongestCommonString (MWE t, component t)
min(len(MWE t), len(component t))
Levenshtein (LEV1): The Levenshtein distance
calculates for the number of basic edit operations re-
quired to transpose one word into the other. Edits
consist of single-letter insertions, deletions or sub-
stitutions. We normalize LEV1 as follows:
1? LEV1 (MWE
t, component t)
max(len(MWE t), len(component t))
Levenshtein with substitution penalty (LEV2):
One well-documented feature of Levenshtein dis-
tance (Baldwin, 2009) is that substitutions are in fact
the combination of an addition and a deletion, and as
such can be considered to be two edits. Based on this
observation, we experiment with a variant of LEV1
with this penalty applied for substitutions. Similarly
to LEV1, we normalize as follows:
1? LEV2 (MWE
t, component t)
len(MWE t) + len(component t)
Smith Waterman (SW) This method is based on
the Needleman-Wunsch algorithm,8 and was devel-
oped to locally-align two protein sequences (Smith
and Waterman, 1981). It finds the optimal simi-
lar regions by maximizing the number of matches
and minimizing the number of gaps necessary to
align the two sequences. For example, the opti-
mal local sequence for the two sequences below is
AT??ATCC, in which ?-? indicates a gap:
8The Needleman-Wunsch (NW) algorithm, was designed to
align two sequences of amino-acids (Needleman and Wunsch,
1970). The algorithm looks for the sequence alignment which
maximizes the similarity. As with the LEV score, NW min-
imizes edit distance, but also takes into account character-to-
character similarity based on the relative distance between char-
acters on the keyboard. We exclude this score, because it is
highly similar to the LEV scores, and we did not obtain encour-
aging results using NW in our preliminary experiments.
Seq1: ATGCATCCCATGAC
Seq2: TCTATATCCGT
As the example shows, it looks for the longest com-
mon string but has an in-built mechanism for includ-
ing gaps in the alignment (with penalty). This char-
acteristic of SW might be helpful in our task, be-
cause there may be morphophonological variations
between the MWE and component translations (as
seen above in the public service example). We nor-
malize SW similarly to LCS:
len(alignedSequence)
min(len(MWE t), len(component t))
6 Computational Model
Given the scores calculated by the aforementioned
string similarity measures between the translations
for a given component word and the MWE, we need
some way of combining scores across component
words.9 First, we measure the compositionality of
each component within the MWE (s1 and s2):
s1 = f1(sim1(w1,MWE), ..., simi(w1,MWE ))
s2 = f1(sim1(w2,MWE), ..., simi(w2,MWE ))
where sim is a string similarity measure, simi indi-
cates that the calculation is based on translations in
language i, and f1 is a score combination function.
Then, we compute the overall compositionality of
the MWE (s3) from s1 and s2 using f2:
s3 = f2(s1, s2)
Since we often have multiple translations for a given
component word/MWE in Panlex, we exhaustively
compute the similarity between each MWE transla-
tion and component translation, and use the highest
similarity as the result of simi. If an instance does
not have a direct/indirect translation in Panlex, we
assign a default value, which is the mean of the high-
est and lowest annotation score (2.5 for REDDY and
0.5 for BANNARD). Note that word order is not an
issue in our method, as we calculate the similarity
independently for each MWE component.
In this research, we consider simple functions for
f1 such as mean, median, product, min and max. f2
9Note that in all experiments we only combine scores given
by the same string similarity measure.
270
NC
Language Frequency Family
Czech 100 Slavic
Norwegian 100 Germanic
Portuguese 100 Romance
Thai 99 Kam-thai
French 95 Romance
Chinese 94 Chinese
Dutch 93 Germanic
Romanian 91 Romance
Hindi 67 Indic
Russian 43 Slavic
Table 2: The 10 best languages for REDDY using LCS.
was selected to be the same as f1 in all situations,
except when we use mean for f1. Here, following
Reddy et al (2011), we experimented with weighted
mean:
f2(s1, s2) = ?s1 + (1? ?)s2
Based on 3-fold cross validation, we chose ? = 0.7
for REDDY.10
Since we do not have judgements for the com-
positionality of the full VPC in BANNARD (we in-
stead have separate judgements for the verb and
particle), we cannot use f2 for this dataset. Ban-
nard et al (2003) observed that nearly all of the
verb-compositional instances were also annotated as
particle-compositional by the annotators. In line
with this observation, we use s1 (based on the verb)
as the compositionality score for the full VPC.
7 Language Selection
Our method is based on the translation of an MWE
into many languages. In the first stage, we chose 54
languages for which relatively large corpora were
available.11 The coverage, or the number of in-
stances which have direct/indirect translations in
Panlex, varies from one language to another. In
preliminary experiments, we noticed that there is
a high correlation (about 0.50 for BANNARD and
10We considered values of ? from 0 to 1, incremented by 0.1.
11In future work, we intend to look at the distribution of trans-
lations of the given MWE and its components in corpora for
many languages. The present method does not rely on the avail-
ability of large corpora.
VPC:verb
Language Frequency Family
Basque 100 Basque
Lithuanian 100 Baltic
Slovenian 100 Slavic
Hebrew 99 Semitic
Arabic 98 Semitic
Czech 95 Slavic
Slovak 92 Slavic
Latin 79 Italic
Tagalog 74 Austronesian
Polish 44 Slavic
Table 3: The 10 best languages for the verb component
of BANNARD using LCS.
VPC:particle
Language Frequency Family
French 100 Romance
Icelandic 100 Germanic
Thai 100 Kam-thai
Indonesian 92 Indonesian
Spanish 90 Romance
Tamil 87 Dravidian
Turkish 83 Turkic
Catalan 79 Romance
Occitan 76 Romance
Romanian 69 Romance
Table 4: The 10 best languages for the particle compo-
nent of BANNARD using LCS.
about 0.80 for REDDY) between the usefulness of
a language and its translation coverage on MWEs.
Therefore, we excluded languages with MWE trans-
lation coverage of less than 50%. Based on nested
10-fold cross validation in our experiments, we se-
lect the 10 most useful languages for each cross-
validation training partition, based on the Pearson
correlation between the given scores in that language
and human judgements.12 The 10 best languages
are selected based only on the training set for each
fold. (The languages selected for each fold will later
be used to predict the compositionality of the items
in the testing portion for that fold.) In Tables 2, 3
12Note that for VPCs, we calculate the compositionality of
only the verb part, because we don?t have the human judge-
ments for the whole VPC.
271
f1 sim() N1 N2 NC
Mean
SW 0.541 0.396 0.637
LCS 0.525 0.431 0.649
LEV1 0.405 0.200 0.523
LEV2 0.481 0.263 0.577
Prod
SW 0.451 0.287 0.410
LCS 0.430 0.233 0.434
LEV1 0.299 0.128 0.311
LEV2 0.294 0.188 0.364
Median
SW 0.443 0.334 0.544
LCS 0.408 0.365 0.553
LEV1 0.315 0.054 0.376
LEV2 0.404 0.134 0.523
Min
SW 0.420 0.176 0.312
LCS 0.347 0.225 0.307
LEV1 0.362 0.310 0.248
LEV2 0.386 0.345 0.338
Max
SW 0.371 0.408 0.345
LCS 0.406 0.430 0.335
LEV1 0.279 0.362 0.403
LEV2 0.380 0.349 0.406
Table 5: Correlation on REDDY (NCs). N1, N2 and NC,
are the first component of the noun compound, its second
component, and the noun compound itself, respectively.
and 4, we show how often each language was se-
lected in the top-10 languages over the combined
100 (10?10) folds of nested 10-fold cross valida-
tion, based on LCS.13 The tables show that the se-
lected languages were mostly consistent over the
folds. The languages are a mixture of Romance,
Germanic and languages from other families (based
on Voegelin and Voegelin (1977)), with no standout
language which performs well in all cases (indeed,
no language occurs in all three tables). Additionally,
there is nothing in common between the verb and the
particle top-10 languages.
8 Results
As mentioned before, we perform nested 10-fold
cross-validation to select the 10 best languages on
the training data for each fold. The selected lan-
guages for a given fold are then used to compute s1
13Since our later results show that LCS and SW have higher
results, we only show the best languages using LCS. These
largely coincide with those for SW.
f1 sim() Verb Particle
Mean
SW 0.369 0.510
LCS 0.406 0.509
LEV1 0.335 0.454
LEV2 0.340 0.460
Prod
SW 0.315 0.316
LCS 0.339 0.299
LEV1 0.322 0.280
LEV2 0.342 0.284
Median
SW 0.316 0.409
LCS 0.352 0.423
LEV1 0.295 0.387
LEV2 0.309 0.368
Min
SW 0.262 0.210
LCS 0.329 0.251
LEV1 0.307 0.278
LEV2 0.310 0.281
Max
SW 0.141 0.288
LCS 0.268 0.299
LEV1 0.145 0.450
LEV2 0.170 0.398
Table 6: Correlation on BANNARD (VPC), based on the
best-10 languages for the verb and particle individually
and s2 (and s3 for NCs) for each instance in the test
set for that fold. The scores are compared with hu-
man judgements using Pearson?s correlation. The
results are shown in Tables 5 and 6. Among the five
functions we experimented with for f1, Mean per-
forms much more consistently than the others. Me-
dian is less prone to noise, and therefore performs
better than Prod, Max and Min, but it is still worse
than Mean.
For the most part, LCS and SW perform better
than the other measures. There is little to separate
these two methods, partly because they both look for
a sequence of similar characters, unlike LEV1 and
LEV2 which do not consider contiguity of match.
The results support our hypothesis that using mul-
tiple target languages rather than one, results in a
more accurate prediction of MWE compositionality.
Our best result using the 10 selected languages on
REDDY is 0.649, as compared to the best single-
language correlation of 0.497 for Portuguese. On
BANNARD, the best LCS result for the verb com-
ponent is 0.406, as compared to the best single-
272
language correlation of 0.350 for Lithuanian.
Reddy et al (2011) reported a correlation of 0.714
on REDDY. Our best correlation is 0.649. Note that
Reddy et al (2011) base their method on identifi-
cation of MWEs in a corpus, thus requiring MWE-
specific identification. Given that this has been
shown to be difficult for MWE types including En-
glish VPCs (McCarthy et al, 2003; Baldwin, 2005),
the fact that our method is as competitive as this is
highly encouraging, especially when you consider
that it can equally be applied to different types of
MWEs in other languages. Moreover, the computa-
tional processing required by methods based on dis-
tributional similarity is greater than our method, as
it does not require processing a large corpus.
Finally, we experimented with combining our
method (STRINGSIMMEAN) with a reimplementation
of the method of Reddy et al (2011), based on sim-
ple averaging, as detailed in Table 7. The results are
higher than both component methods and the state-
of-the-art for REDDY, demonstrating the comple-
mentarity between our proposed method and meth-
ods based on distributional similarity.
In Table 8, we compare our results
(STRINGSIMMEAN) with those of Bannard et
al. (2003), who interpreted the dataset as a binary
classification task. The dataset used in their study
is a subset of BANNARD, containing 40 VPCs, of
which 29 (72%) were verb compositional and 23
(57%) were particle compositional. By applying a
threshold of 0.5 over the output of our regression
model, we binarize the VPCs into the compositional
and non-compositional classes. According to the
results shown in Table 6, LCS is a better similarity
measure for this task. Our proposed method has
higher results than the best results of Bannard et
al. (2003), in part due to their reliance on VPC
identification, and the low recall on the task, as
reported in the paper. Our proposed method does
not rely on a corpus or MWE identification.
9 Error Analysis
We analyse items in REDDY which have a high dif-
ference (more than 2.5) between the human anno-
tation and our scores (using LCS and Mean). The
words are cutting edge, melting pot, gold mine and
ivory tower, which are non-compositional accord-
ing to REDDY. After investigating their translations,
we came to the conclusion that the first three MWEs
have word-for-word translations in most languages.
Hence, they disagree with our hypothesis that word-
for-word translation is a strong indicator of compo-
sitionality. The word-for-word translations might be
because of the fact that they have both compositional
and non-compositional senses, or because they are
calques (loan translations). However, we have tried
to avoid such problems with calques by using trans-
lations into several languages.
For ivory tower (?a state of mind that is discussed
as if it were a place?)14 we noticed that we have a di-
rect translation into 13 languages. Other languages
have indirect translations. By checking the direct
translations, we noticed that, in French, the MWE is
translated to tour and tour d?ivoire. A noisy (wrong)
translation of tour ?tower? resulted in wrong indirect
translations for ivory tower and an inflated estimate
of compositionality.
10 Conclusion and Future Work
In this study, we proposed a method to predict MWE
compositionality based on the translation of the
MWE and its component words into multiple lan-
guages. We used string similarity measures between
the translations of the MWE and each of its compo-
nents to predict the relative degree of composition-
ality. Among the four similarity measures that we
experimented with, LCS and SW were found to be
superior to edit distance-based methods. Our best re-
sults were found to be competitive with state-of-the-
art results using vector-based approaches, and were
also shown to complement state-of-the-art methods.
In future work, we are interested in investigating
whether alternative ways of combining our proposed
method with vector-based models can lead to fur-
ther enhancements in results. These models could
be especially effective when comparing translations
which are roughly synonymous but not string-wise
similar.
Acknowledgments
We would like to thank Timothy Baldwin, Su Nam
Kim, and the anonymous reviewers for their valu-
able comments and suggestions.
14This definition is from Wordnet 3.1.
273
sim() STRINGSIMMEAN STRINGSIMMEAN + Reddy et al
SW 0.637 0.735
LCS 0.649 0.742
LEV1 0.523 0.724
LEV2 0.577 0.726
Table 7: Correlation after combining Reddy et al?s method and our method with Mean for f1 (STRINGSIMMEAN ). The
correlation using Reddy et al?s method is 0.714.
Method Precision Recall F-score (? = 1) Accuracy
Bannard et al (2003) 0.608 0.666 0.636 0.600
STRINGSIMMEAN 0.862 0.718 0.774 0.693
Table 8: Results for the classification task. STRINGSIMMEAN is our method using Mean for f1
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
References
Otavio Costa Acosta, Aline Villavicencio, and Viviane P
Moreira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval. In
Proceedings of the ALC Workshop on MWEs: from
Parsing and Generation to the Real World (MWE
2011), pages 101?109.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-2003 Workshop on Multiword Expres-
sions: Analysis, Acquisition and Treatment, pages 89?
96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all words
of all languages of the world. In Proceedings of the
23rd International Conference on Computational Lin-
guistics: Demonstrations, pages 37?40.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 65?72.
Colin James Bannard. 2006. Acquiring Phrasal Lexicons
from Corpora. Ph.D. thesis, University of Edinburgh.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Helena Medeiros de Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1):59?77.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the ACL 2007Workshop on A Broader Per-
spective on Multiword Expressions, pages 9?16.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of english verb-particle constructions
using semantic similarity. In Proceedings of the 7th
Meeting of the Pacific Association for Computational
Linguistics (PACLING 2007), pages 40?48.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages 317?
324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
274
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 73?80.
Diana McCarthy, Sriram Venkatapathy, and Aravind K
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Fifth Workshop on Very Large Cor-
pora. EMNLP.
Begona Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multi-wordexpressions in a multilingual
context, pages 33?40.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 216?225, Uppsala, Sweden.
Saul B Needleman and Christian D Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of IJCNLP, pages 210?
218.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceed-
ings of the 3rd International Conference on Intelligent
Text Processing Computational Linguistics (CICLing-
2002), pages 189?206. Springer.
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly. 2012.
Automatic identification of Persian light verb con-
structions. In Proceedings of the 13th International
Conference on Intelligent Text Processing Computa-
tional Linguistics (CICLing-2012), pages 201?210.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary headwords
a solved problem. In Proceedings of the 6th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2001), pages 100?108.
TF Smith and MS Waterman. 1981. Identification of
commonmolecular subsequences. Molecular Biology,
147:195?197.
Stephen Soderland, Oren Etzioni, Daniel S Weld, Kobi
Reiter, Michael Skinner, Marcus Sammer, Jeff Bilmes,
et al 2010. Panlingual lexical translation via proba-
bilistic inference. Artificial Intelligence, 174(9):619?
637.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 20?27.
Charles Frederick Voegelin and FlorenceMarie Voegelin.
1977. Classification and index of the world?s lan-
guages, volume 4. Elsevier Science Ltd.
275
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction for Web Snippet
Clustering
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for Task 11
of SemEval-2013. In the task, participants
are provided with a set of ambiguous search
queries and the snippets returned by a search
engine, and are asked to associate senses with
the snippets. The snippets are then clus-
tered using the sense assignments and sys-
tems are evaluated based on the quality of the
snippet clusters. Our system adopts a pre-
existing Word Sense Induction (WSI) method-
ology based on Hierarchical Dirichlet Process
(HDP), a non-parametric topic model. Our
system is trained over extracts from the full
text of English Wikipedia, and is shown to per-
form well in the shared task.
1 Introduction
The basic premise behind research on word sense
disambiguation (WSD) is that there exists a static,
discrete set of word senses that can be used to la-
bel distinct usages of a given word (Agirre and Ed-
monds, 2006; Navigli, 2009). There are various pit-
falls underlying this premise, including: (1) what
sense inventory is appropriate for a particular task
(given that sense inventories can vary considerably
in their granularity and partitioning of word usages)?
(2) given that word senses tend to take the form of
prototypes, is discrete labelling a felicitous represen-
tation of word usages, especially for non-standard
word usages? (3) how should novel word usages be
captured under this model? and (4) given the rapid
pace of language evolution on real-time social me-
dia such as Twitter and Facebook, is it reasonable
to assume a static sense inventory? Given this back-
drop, there has been a recent growth of interest in the
task of word sense induction (WSI), where the word
sense representation for a given word is automati-
cally inferred from a given data source, and word
usages are labelled (often probabilistically) accord-
ing to that data source. While WSI has considerable
appeal as a task, intrinsic cross-comparison of WSI
systems is fraught with many of the same issues as
WSD (Agirre and Soroa, 2007; Manandhar et al,
2010), leading to a move towards task-based WSI
evaluation, such as in Task 11 of SemEval-2013, ti-
tled ?Evaluating Word Sense Induction & Disam-
biguation within an End-User Application?.
This paper presents the UNIMELB system entry to
SemEval-2013 Task 11. Our method is based heav-
ily on the WSI methodology proposed by Lau et
al. (2012) for novel word sense detection. Largely
the same methodology was also applied to SemEval-
2013 Task 13 on WSI (Lau et al, to appear).
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012) for the task of novel word
sense detection. The core machinery of our sys-
tem is driven by a Latent Dirichlet Allocation (LDA)
topic model (Blei et al, 2003). In LDA, the model
learns latent topics for a collection of documents,
and associates these latent topics with every docu-
ment in the collection. A topic is represented by
a multinomial distribution of words, and the asso-
ciation of topics with documents is represented by a
multinomial distribution of topics, with one distribu-
tion per document. The generative process of LDA
217
for drawing word w in document d is as follows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
The number of topics, T , is a parameter in LDA,
and the model tends to be highly sensitive to this set-
ting. To remove the need for parameter tuning over
development data, we make use of a non-parametric
variant of LDA, in the form of a Hierarchical Dirich-
let Process (HDP: Teh et al (2006)). HDP learns the
number of topics based on data, and the concentra-
tion parameters ? and ?0 control the variability of
topics in the documents (for details of HDP please
refer to the original paper, Teh et al (2006)).
To apply HDP in the context of WSI, the latent
topics are interpreted as the word senses, and the
documents are usages that contain the target word of
interest (or search query in the case of Task 11). That
is, given a search query (e.g. Prince of Persia), a
?document? in our application is a sentence/snippet
containing the target word. In addition to the bag of
words surrounding the target word, we also include
positional context word information, as used in the
original methodology of Lau et al (2012). That is,
we introduce an additional word feature for each of
the three words to the left and right of the target
word. An example of the topic model features for
a context sentence is given in Table 1.
2.1 Background Corpus and Preprocessing
As part of the task setup, we were provided with
snippets for each search query, constituting the doc-
uments for the topic model for that query (each
search query is topic-modelled separately). Our sys-
tem uses only the text of the snippets as features, and
ignores the URL information. The text of the snip-
pets is tokenised and lemmatised using OpenNLP
and Morpha (Minnen et al, 2001).
As there are only 64 snippets for each query in
the test dataset, which is very small by topic mod-
elling standards, we turn to English Wikipedia to
expand the data, by extracting all context sentences
that contain the search query in the full collection
of Wikipedia articles.1 Each extracted usage is a
three-sentence context containing the search query:
the original sentence that contains the actual usage
and its preceding and succeeding sentences. The
extraction of usages from Wikipedia significantly
increases the amount of information for the topic
model to learn the senses for the search queries. To
give an estimate: for very ambiguous queries such
as queen we extracted almost 150,000 usages from
Wikipedia; for most queries, however, this number
tends to be a few thousand usages.
To summarise, for each search query we apply the
HDP model to the combined collection of the 64
snippets and the extracted usages from Wikipedia.
The topic model learns the senses/topics for all
documents in the collection, but we only use the
sense/topic distribution for the 64 snippets as they
are the documents that are evaluated in the shared
task.
Our English Wikipedia collection is tokenised and
lemmatised using OpenNLP and Morpha (Minnen et
al., 2001). The search queries provided in the task,
however, are not lemmatised. Two approaches are
used to extract the usages of search queries from
Wikipedia:
HDP-CLUSTERS-LEMMA Search queries are lem-
matised using Morpha (Minnen et al, 2001),
and both the original and lemmatised forms are
used for extraction;2
HDP-CLUSTERS-NOLEMMA Search queries are
not lemmatised and only their original forms
are used for extraction.
1The Wikipedia dump was retrieved on November 28th
2009.
2Morpha requires the part-of-speech (POS) of a given word,
which is determined by the majority POS aggregated over all of
that word?s occurrences in Wikipedia.
218
Search query dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of topic model features.
System F1 ARI RI JI
Avg. No. of Avg. Cluster
Clusters Size
HDP-CLUSTERS-LEMMA 0.6830 0.2131 0.6522 0.3302 6.6300 11.0756
HDP-CLUSTERS-NOLEMMA 0.6803 0.2149 0.6486 0.3375 6.5400 11.6803
TASK11.DULUTH.SYS1.PK2 0.5683 0.0574 0.5218 0.3179 2.5300 26.4533
TASK11.DULUTH.SYS7.PK2 0.5878 0.0678 0.5204 0.3103 3.0100 25.1596
TASK11.DULUTH.SYS9.PK2 0.5702 0.0259 0.5463 0.2224 3.3200 19.8400
TASK11-SATTY-APPROACH1 0.6709 0.0719 0.5955 0.1505 9.9000 6.4631
TASK11-UKP-WSI-WACKY-LLR 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434
TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702
TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098
RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441
SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000
ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000
GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630
Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON
and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.3 For each
search query, we apply HDP to induce the senses,
and a distribution of senses is produced for each
?document? in the model. As the snippets in the test
dataset correspond to the documents in the model
and evaluation is based on ?hard? clusters of snip-
pets, we assign a sense to each snippet based on the
sense (= topic) which has the highest probability for
that snippet.
The task requires participants to produce a ranked
list of snippets for each induced sense, based on the
relative fit between the snippet and the sense. We in-
duce the ranking based on the sense probabilities as-
signed to the senses, such that snippets that have the
highest probability of the induced sense are ranked
highest, and snippets with lower sense probabilities
3Our implementation can be accessed via https://
github.com/jhlau/hdp-wsi.
are ranked lower.
Two classes of evaluation are used in the shared
task:
1. cluster quality measures: Jaccard Index (JI),
RandIndex (RI), Adjusted RandIndex (ARI)
and F1;
2. diversification of search results: Subtopic Re-
call@K and Subtopic Precision@r.
Details of the evaluation measures are described in
Navigli and Vannella (2013).
The idea behind the second form of evaluation
(i.e. diversification of search results) is that search
engine results should cluster the results based on
senses (of the query term in the documents) given an
ambiguous query. For example, if a user searches for
apple, the search engine may return results related to
both the computer brand sense and the fruit sense of
apple. Given this assumption, the best WSI/WSD
system is the one that can correctly identify the di-
versity of senses in the snippets.
219
Figure 1: Subtopic Recall@K for all participating systems.
Cluster quality, subtopic recall@K and subtopic
precision@r results for all systems entered in the
task are presented in Table 2, Figure 1 and Figure 2,
respectively.
In terms of cluster quality, our systems
(HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA) consistently outperform the other teams
for all measures except for the Jaccard Index (where
we rank second and third, by a narrow margin). The
average number of induced clusters and the average
cluster size of our systems are similar to those
of the gold standard system (GOLD), indicating
that our systems are learning an appropriate sense
granularity.
In terms of diversification of search results, our
systems perform markedly better than most teams,
other than RAKESH which trails closely behind our
systems (despite a relatively low ranking in terms of
the cluster quality evaluation). Overall, the results
are encouraging and our system performs very well
over the task.
4 Discussion and Conclusion
Our system adopts the WSI system proposed in Lau
et al (2012) with no parameters tuned for this task,
and performs very well over it. Parameter tuning and
exploiting URL information in the snippets could
potentially boost the system performance further.
Other background corpora (such as news articles)
could also be used to increase the size of the training
data. We leave these ideas for future work.
Inspecting the difference between the HDP-
CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA approaches, only 6 out of the 100
lemmas have a lemmatised form which differs from
the original query composition: pods (pod), ten
commandments (ten commandment), guild wars
(guild war), stand by me (stand by i), sisters of
mercy (sister of mercy) and lord of the flies (lord of
the fly). In most cases, including the lemmatised
query results in the extraction of additional useful
usages, e.g. using only the original form lord of
the flies would extract no usages from Wikipedia
(because this corpus has itself been lemmatised).
In other cases, however, including the lemmatised
forms results in many common noun usages, e.g.
the number of usages of the lemmatised pod is
significantly greater than that of the original form
pods (which corresponds to proper noun usages in
the lemmatised corpus), resulting in senses being
induced only for common noun usages of pods. The
220
Figure 2: Subtopic Precision@r for all participating systems.
advantages and disadvantages of both approaches
are reflected in the results: performance is mixed
and no one method clearly outperforms the other.
To conclude, we apply a topic model-based WSI
methodology to the task of web result clustering, us-
ing English Wikipedia as an external resource for ex-
tracting additional usages. Our system is completely
unsupervised and requires no annotated resources,
and appears to perform very well on the task.
References
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer, Dordrecht, Netherlands.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proc. of the 4th International Work-
shop on Semantic Evaluations, pages 7?12, Prague,
Czech Republic.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction. In Proc. of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 task 11: Evaluating word sense induction & dis-
ambiguation within an end-user application. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
221
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 307?311, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for shared
task 13 ?Word Sense Induction for Graded and
Non-Graded Senses? of SemEval-2013. The
task is on word sense induction (WSI), and
builds on earlier SemEval WSI tasks in ex-
ploring the possibility of multiple senses be-
ing compatible to varying degrees with a sin-
gle contextual instance: participants are asked
to grade senses rather than selecting a sin-
gle sense like most word sense disambigua-
tion (WSD) settings. The evaluation measures
are designed to assess how well a system per-
ceives the different senses in a contextual in-
stance. We adopt a previously-proposed WSI
methodology for the task, which is based on a
Hierarchical Dirichlet Process (HDP), a non-
parametric topic model. Our system requires
no parameter tuning, uses the English ukWaC
as an external resource, and achieves encour-
aging results over the shared task.
1 Introduction
In our previous work (Lau et al, 2012) we devel-
oped a word-sense induction (WSI) system based on
topic modelling, specifically a Hierarchical Dirich-
let Process (Teh et al, 2006). In evaluations over
the SemEval-2007 and SemEval-2010 WSI tasks we
achieved performance on par with the current state-
of-the art. The SemEval-2007 and SemEval-2010
WSI tasks assumed that each usage of a word has
a single gold-standard sense. In this paper we apply
this WSI method ?off-the-shelf?, with no adaptation,
to the novel SemEval-2013 task of ?Word Sense In-
duction for Graded and Non-Graded Senses?. Given
that the topic model allocates a multinomial distri-
bution over topics to each word usage (?document?,
in topic modelling terms), the SemEval-2013 WSI
task is an ideal means for evaluating this aspect of
the topic model.
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012), and also applied to
SemEval-2013 Task 11 on WSI for web snippet
clustering (Lau et al, to appear). The core machin-
ery of our system is driven by a Latent Dirichlet Al-
location (LDA) topic model (Blei et al, 2003). In
LDA, the model learns latent topics for a collection
of documents, and associates these latent topics with
every document in the collection. A topic is repre-
sented by a multinomial distribution of words, and
the association of topics with documents is repre-
sented by a multinomial distribution of topics, a dis-
tribution for each document. The generative process
of LDA for drawing word w in document d is as fol-
lows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
307
The number of topics, T , is a parameter in LDA.
We relax this assumption by extending the model
to be non-parametric, using a Hierarchical Dirichlet
Process (HDP: (Teh et al, 2006)). HDP learns the
number of topics based on data, with the concentra-
tion parameters ? and ?0 controlling the variability
of topics in the documents (for details of HDP please
refer to the original paper).
To apply HDP to WSI, the latent topics are in-
terpreted as the word senses, and the documents are
usages that contain the target word of interest. That
is, given a target word (e.g. paper), a ?document?
in our application is a sentence context surround-
ing the target word. In addition to the bag of words
surrounding the target word, we also include posi-
tional context word information, which was used in
our earlier work (Lau et al, 2012). That is, we in-
troduce an additional word feature for each of the
three words to the left and right of the target word.
An example of the topic model features is given in
Table 1.
2.1 Background Corpus and Preprocessing
The test dataset provides us with contextual in-
stances for each target word, and these instances
constitute the documents for the topic model. The
text of the test data is tokenised and lemmatised us-
ing OpenNLP and Morpha (Minnen et al, 2001).
Note, however, that there are only 100 instances
for most target words in the test dataset, and as such
the dataset may be too small for the topic model
to induce meaningful senses. To this end, we turn
to the English ukWaC ? a web corpus of approxi-
mately 1.9 billion tokens ? to expand the data, by
extracting context sentences that contain the target
word. Each extracted usage is a three-sentence con-
text containing the target word: the original sentence
that contains the actual usage and its preceding and
succeeding sentences. The extraction of usages from
the ukWaC significantly increases the amount of in-
formation for the topic model to learn the senses for
the target words from. However, HDP is compu-
tationally intensive, so we limit the number of ex-
tracted usages from the ukWaC using two sampling
approaches:
UNIMELB (5P) Take a 5% random sample of us-
ages;
UNIMELB (50K) Limit the maximum number of
randomly-sampled usages to 50,000 instances.
The usages from the ukWaC are tokenised and
lemmatised using TreeTagger (Schmid, 1994), as
provided by the corpus.
To summarise, for each target word we apply
the HDP model to the combined collection of the
test instances (provided by the shared task) and
the extracted usages from the English ukWaC (not-
ing that each instance/usage corresponds to a topic
model ?document?). The topic model learns the
senses/topics for all documents in the collection, but
we only use the sense/topic distribution for the test
instances as they are the ones evaluated in the shared
task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.1 For each
target word, we apply HDP to induce the senses, and
a distribution of senses is produced for each ?docu-
ment? in the model. To grade the senses for the in-
stances in the test dataset, we apply the sense proba-
bilities learnt by the topic model as the sense weights
without any modification.
To illustrate the senses induced by our model, we
present the top-10 words of the induced senses for
the verb strike in Table 2. Although 13 senses in
total are induced and some of them do not seem very
coherent, only the first 8 senses ? the more coherent
ones ? are observed (i.e., have non-zero probability
for any usage) in the test dataset.
Two forms of evaluation are used in the task:
WSD evaluation and clustering comparison. For
WSD evaluation, three measures are used: (1)
Jaccard Index (JI), which measures the degree of
overlap between the induced senses and the gold
senses; (2) positionally-weighted Kendall?s tau (KT:
(Kumar and Vassilvitskii, 2010)), which measures
the correlation between the ranking of the induced
senses and that of the gold senses; and (3) nor-
malised discounted cumulative gain (NDCG), which
1These settings were considered ?vague? priors in Teh et
al. (2006). They were tested in Lau et al (2012) and the
model was shown to be robust under different parameter set-
tings. As such we decided to keep the settings. The imple-
mentation of our WSI system can be accessed via GitHub:
https://github.com/jhlau/hdp-wsi.
308
Target word dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of the topic model features.
Sense Num Top-10 Terms
1 strike @card@ worker union war iraq week pay government action
2 strike hand god head n?t look face fall leave blow
3 strike @card@ balance court company case need balance #1 order claim
4 strike ball @card@ minute game goal play player shot half
5 strike @card@ people fire disaster area road car ship lightning
6 @card@ strike new news post deal april home business week
7 strike n?t people thing think way life book find new
8 @card@ strike coin die john church police age house william
9 div ukl syn color hunter text-decoration australian verb condom font-size
10 invent rocamadour cost mp3 terminal total wav honor omen node
11 training run rush kata performance marathon exercise technique workout interval
12 wrong qha september/2000 sayd ? hawksmoor thyna pan salt common
13 zidane offering stone blow zidane #-1 type type #2 zidane #1 blow #3 materials
Table 2: The top-10 terms for each of the senses induced for the verb strike by the HDP model.
measures the correlation between the weights of
the induced senses and that of the gold senses.
For clustering comparison, fuzzy normalised mu-
tual information (FNMI) and fuzzy b-cubed (FBC)
are used. Note that the WSD systems participat-
ing in this shared task are not evaluated with clus-
tering comparison metrics, as they do not induce
senses/clusters in the same manner as WSI systems.
WSI systems produce senses that are different to
the gold standard sense inventory (WordNet 3.1),
and the induced senses are mapped to the gold stan-
dard senses using the 80/20 validation setting. De-
tails of this mapping procedure are described in Jur-
gens (2012).
Results for all test instances are presented in Ta-
ble 3. Note that many baselines are used, only some
of which we present in this paper, namely: (1) RAN-
DOM ? label instances with one of three random in-
duced senses; (2) SEMCOR MFS ? label instances
with the most frequently occurring sense in Semcor;
(3) TEST MFS ? label instances with the most fre-
quently occurring sense in the test dataset. To bench-
mark our method, we present one or two of the best
systems from each team.
Looking at Table 3, our system performs encour-
agingly well. Although not the best system, we
achieve results close to the best system for each eval-
uation measure.
Most of the instances in the data were annotated
with only one sense; only 11% were annotated with
two senses, and 0.5% with three. As a result, the
task organisers categorised the instances into single-
sense instances and multi-sense instances to bet-
ter analyse the performance of participating sys-
tems. Results for single-sense and multi-sense in-
stances are presented in Table 4 and Table 5, re-
spectively. Note that for single-sense instances, only
precision is used for WSD evaluation as the Jaccard
Index, positionally-weighted Kendall?s tau and nor-
malised discounted cumulative gain are not applica-
ble. Our system performs relatively well, and trails
marginally behind the best system in most cases.
4 Conclusion
We adopt a WSI methodology from Lau et al (2012)
for the task of grading senses in a WSD setting.
309
System JI KT NDCG FNMI FBC
RANDOM 0.244 0.633 0.287 0.018 0.382
SEMCOR MFS 0.455 0.465 0.339 ? ?
TEST MFS 0.552 0.560 0.412 ? ?
AI-KU 0.197 0.620 0.387 0.065 0.390
AI-KU (REMOVE5-AD1000) 0.244 0.642 0.332 0.039 0.451
LA SAPIENZA (2) 0.149 0.510 0.383 ? ?
UOS (TOP-3) 0.232 0.625 0.374 0.045 0.448
UNIMELB (5P) 0.218 0.614 0.365 0.056 0.459
UNIMELB (50K) 0.213 0.620 0.371 0.060 0.483
Table 3: Results for all instances. The best-performing system is indicated in boldface.
System Precision FNMI FBC
RANDOM 0.555 0.010 0.359
SEMCOR MFS 0.477 ? ?
TEST MFS 0.578 ? ?
AI-KU 0.641 0.045 0.351
AI-KU (REMOVE5-AD1000) 0.628 0.026 0.421
UOS (TOP-3) 0.600 0.028 0.414
UNIMELB (5P) 0.596 0.035 0.421
UNIMELB (50K) 0.605 0.039 0.441
Table 4: Results for single-sense instances. The best-performing system is indicated in boldface.
System JI KT NDCG FNMI FBC
RANDOM 0.429 0.548 0.236 0.006 0.113
SEMCOR MFS 0.283 0.373 0.197 ? ?
TEST MFS 0.354 0.426 0.248 ? ?
AI-KU 0.394 0.617 0.317 0.029 0.078
AI-KU (REMOVE5-AD1000) 0.434 0.586 0.291 0.004 0.116
LA SAPIENZA (2) 0.263 0.531 0.365 ? ?
UOS (#WN SENSES) 0.387 0.628 0.314 0.036 0.037
UNIMELB (5P) 0.426 0.586 0.287 0.019 0.130
UNIMELB (50K) 0.414 0.602 0.299 0.021 0.134
Table 5: Results for multi-sense instances. The best-performing system is indicated in boldface.
310
With no parameter tuning and using only the English
ukWaC as an external resource, our system performs
relatively well at the task.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
David Jurgens. 2012. An evaluation of graded sense
disambiguation using word sense induction. In Proc.
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012), pages 189?198,
Montre?al, Canada.
Ravi Kumar and Sergei Vassilvitskii. 2010. Generalized
distances between rankings. In Proc. of the 19th Inter-
national Conference on the World Wide Web (WWW
2010), pages 571?580, Raleigh, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction for web snippet clustering. In Proc. of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of the Confer-
ence on New Methods in Natural Language Process-
ing, Manchester, 1994.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
311
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 61?69,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
No sentence is too confusing to ignore
Paul Cook
Department of Computer Science
University of Toronto
Toronto, Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
We consider sentences of the form No
X is too Y to Z, in which X is a noun
phrase, Y is an adjective phrase, and Z
is a verb phrase. Such constructions are
ambiguous, with two possible (and oppo-
site!) interpretations, roughly meaning ei-
ther that ?Every X Zs?, or that ?No X Zs?.
The interpretations have been noted to de-
pend on semantic and pragmatic factors.
We show here that automatic disambigua-
tion of this pragmatically complex con-
struction can be largely achieved by us-
ing features of the lexical semantic prop-
erties of the verb (i.e., Z) participating in
the construction. We discuss our experi-
mental findings in the context of construc-
tion grammar, which suggests a possible
account of this phenomenon.
1 No noun is too adjective to verb
Consider the following two sentences:
(1) No interest is too narrow to deserve its own
newsletter.
(2) No item is too minor to escape his attention.
Each of these sentences has the form of No X is too
Y to Z, where X, Y, and Z are a noun phrase, ad-
jective phrase, and verb phrase, respectively. Sen-
tence (1) is generally taken to mean that every in-
terest deserves its own newsletter, regardless of
how narrow it is. On the other hand, (2) is typi-
cally interpreted as meaning that no item escapes
his attention, regardless of how minor it is. That
is, sentences with the identical form of No X is too
Y to Z either can mean that ?every X Zs?, or can
mean the opposite?that ?no X Zs?!1
1Note that in examples (1) and (2), the nouns interest and
item are the subjects of the verbs deserve and escape, respec-
This ?verbal illusion? (Wason and Reich, 1979),
so-called because there are two opposite inter-
pretations for the very same structure, is of in-
terest to us for two reasons. First, the con-
tradictory nature of the possible meanings has
been explained in terms of pragmatic factors con-
cerning the relevant presuppositions of the sen-
tences. According to Wason and Reich (1979)
(as explained in more detail below), sentences
such as (2) are actually nonsensical, but people
coerce them into a sensible reading by revers-
ing the interpretation. One of our goals in this
work is to explore whether computational lin-
guistic techniques?specifically automatic corpus
analysis drawing on lexical resources?can help
to elucidate the factors influencing interpretation
of such sentences across a collection of actual us-
ages.
The second reason for our interest in this con-
struction is that it illustrates a complex ambigu-
ity that can cause difficulty for natural language
processing applications that seek to semantically
interpret text. Faced with the above two sen-
tences, a parsing system (in the absence of spe-
cific knowledge of this construction) will presum-
ably find the exact same structure for each, giv-
ing no basis on which to determine the correct
meaning from the parse. (Unsurprisingly, when
we run the C&C Parser (Curran et al, 2007) on (1)
and (2) it assigns the same structure to each sen-
tence.) Our second goal in this work is thus to ex-
plore whether increased linguistic understanding
of this phenomenon could be used to disambiguate
such examples automatically. Specifically, we use
this construction as an example of the kind of
difficulties faced in semantic interpretation when
meaning may be determined by pragmatic or other
extra-syntactic factors, in order to explore whether
tively. In this construction the noun can also be the object of
the verb, as in the title of this paper which claims no sentence
can/should be ignored.
61
lexical semantic features can be used as cues to
resolving pragmatic ambiguity when a complex
semantico-pragmatic model is not feasible.
In the remainder of this paper, we present the
first computational study of the No X is too Y to
Z phenomenon, which attempts to automatically
determine the meaning of instances of this seman-
tically and pragmatically complex construction. In
Section 2 we present previous analyses of this
construction, and our hypothesis. In Section 3,
we describe the creation of a dataset of instances
that verifies that both interpretations (?every? and
?no?) indeed occur in corpora. We then analyze
the human annotations in this dataset in more de-
tail in Section 4. In Section 5, we present the fea-
ture model we use to describe the instances, which
taps into the lexical semantics and polarity of the
constituents. In Section 6, we describe machine
learning experiments and classification results that
support our hypothesis that the interpretation of
this construction largely depends on the semantics
of its component verb. In Section 7 we suggest that
our results support an analysis of this phenomenon
within construction grammar, and point to some
future directions in our research in Section 8.
2 Background and our proposal
The No X is too Y to Z construction was investi-
gated by Wason and Reich (1979), and discussed
more recently by Pullum (2004) and Liberman
(2009a,b). Here we highlight some of the most
important properties of this complex phenomenon.
Our presentation owes much to the lucid discus-
sion and clarification of this topic, and of the work
of Wason and Reich specifically, by Liberman.
Wason and Reich argue that the compositional
interpretation of sentences of the form of (1) and
(2) is ?every X Zs?. Intuitively, this can be under-
stood by considering a sentence identical to sen-
tence (1), but without a negative subject: This in-
terest is too narrow to deserve its own newslet-
ter, which means that ?this interest is so narrow
that it does not deserve a newsletter?. This ex-
ample indicates that the meaning of too narrow
to deserve its own newsletter is ?so narrow that
it does not deserve a newsletter?. When this neg-
ative ?too? assertion is compositionally combined
with the No interest subject of sentence (1), it re-
sults in a meaning with two negatives: ?No inter-
est is so narrow that it does not deserve a newslet-
ter?, or simply, ?Every interest deserves a newslet-
ter?. Wason and Reich note that in sentences such
as (1), the compositional ?every? interpretation is
consistent with common beliefs about the world,
and thus refer to such sentences as ?pragmatic?.
By contrast, the compositional interpretation of
sentences such as (2) does not correspond to our
common sense beliefs. Consider an analogous
(non-negative subject) sentence to sentence (2)?
i.e., This item is too minor to escape his attention.
It is nonsensical that ?This item is so minor that
it does not escape his attention?, since being more
?minor? entails more likelihood of escaping atten-
tion, not less. The compositional interpretation of
(2) is similarly nonsensical?i.e., that ?No item
is so minor that it does not escape his attention?;
Such sentences are thus termed ?non-pragmatic?
by Wason and Reich, who argue that the com-
plexity of the non-pragmatic sentences?arising in
part due to the number of negations they contain?
causes the listener or reader to misconstrue them.
According to their reasoning, listeners choose an
interpretation that is consistent with their beliefs
about the world?namely that ?no X Zs?, in this
case that ?No item escapes his attention??instead
of the compositional interpretation (?Every item
escapes his attention?).
While Wason and Reich focus on the compo-
sitional semantics and pragmatics of these sen-
tences, they also note that the non-pragmatic ex-
amples typically use a verb that itself has some
aspect of negation, such as ignore, miss, and over-
look. This property is also pointed out by Pullum
(2004), who notes that avoid in his example of
the construction means ?manage to not do? some-
thing. Building on this observation, we hypothe-
size that lexical properties of the component con-
stituents of this construction, particularly the verb,
can be important cues to its semantico-pragmatic
interpretation. Specifically, we hypothesize that
the pragmatic (?every? interpretation) and non-
pragmatic (?no? interpretation) sentences will tend
to involve verbs with different semantics. Given
that verbs of different semantic classes have differ-
ent selectional preferences, we also expect to see
the ?every? and ?no? sentences associated with se-
mantically different nouns and adjectives.
3 Dataset
3.1 Extraction
To create a dataset of usages of the construction
no NP is too AP to VP?referred to as the tar-
62
get construction?we use two corpora: the British
National Corpus (Burnard, 2000), an approxi-
mately one hundred million word corpus of late-
twentieth century British English, and The New
York Times Annotated Corpus (Sandhaus, 2008),
approximately one billion words of non-newswire
text from the New York Times from the years
1987?2006. We extract all sentences in these cor-
pora containing the sequence of strings no, is too,
and to separated by one or more words. We then
manually filter all sentences that do not have no
NP as the subject of is too, or that do not have to
VP as an argument of is too. After removing dupli-
cates, this results in 170 sentences. We randomly
select 20 of these sentences for development data,
leaving 150 sentences for testing.
Although we find only 170 examples of the
target construction in 1.1 billion words of text,
note that our extraction process is quite strict and
misses some relevant usages. For example, we do
not extract sentences of the form Nothing is too Y
to Z in which the subject NP does not contain the
word no. Nor do we extract usages of the related
construction No X is too Y for Z, where Z is an NP
related to a verb, as in No interest is too narrow
for attention. (We would only extract the latter if
there were an infinitive verb embedded in or fol-
lowing the NP.) In the present study we limit our
consideration to sentences of the form discussed
by Wason and Reich (1979), but intend to con-
sider related constructions such as these?which
appear to exhibit the same ambiguity as the target
construction?in the future.
We next manually identify the noun, adjective,
and verb that participate in the target construction
in each sentence. Although this could be done au-
tomatically using a parser (e.g., Collins, 2003) or
chunker (e.g., Abney, 1991), here we want to en-
sure error-free identification. We also note a num-
ber of sentences containing co-ordination, such as
in the following example.
(3) These days, no topic is too recent or
specialized to disqualify it from museum
apotheosis.
This sentence contains two instances of the tar-
get construction: one corresponding to the noun-
adjective-verb triple topic, recent, disqualify, and
the other to the triple topic, specialized, disqual-
ify. In general, we consider each unique noun-
adjective-verb triple participating in the target con-
struction as a separate instance.
3.2 Annotation
We used Amazon Mechanical Turk (AMT,
https://www.mturk.com/) to obtain judge-
ments as to the correct interpretation of each in-
stance of the target construction in both the devel-
opment and testing datasets. For each instance, we
generated two paraphrases, one corresponding to
each of the interpretations discussed in Section 1.
We then presented the given instance of the target
construction along with its two paraphrases to an-
notators through AMT, as shown in Table 1. In
generating the paraphrases, one of the authors se-
lected the most appropriate paraphrase, in their
judgement, where can in the paraphrases in Ta-
ble 1 was selected from can, should, will, and ?.
Note that the paraphrases do not contain the ad-
jective from the target construction. In the case of
multiple instances of the target construction with
differing adjectives but the same noun and verb,
we only solicited judgements for one instance, and
used these judgements for the other instances. In
our dataset we observe that all instances obtained
from the same sentence which differ only with re-
spect to their noun or verb have the same inter-
pretation. We therefore believe that instances with
the same noun and verb but a different adjective
are unlikely to differ in their interpretation.
Instructions:
? Read the sentence below.
? Based on your interpretation of that sen-
tence, select the answer that most closely
matches your interpretation.
? Select ?I don?t know? if neither answer is
close to your interpretation, or if you are
really unsure.
That success was accomplished in large part to
tight control on costs , and no cost is too small
to be scrutinized .
? Every cost can be scrutinized.
? No cost can be scrutinized.
? I don?t know.
Enter any feedback you have about this HIT. We
greatly appreciate you taking the time to do so.
Table 1: A sample of the Amazon Mechanical
Turk annotation task.
63
We also allowed the judges to optionally enter
any feedback about the annotation task which in
some cases?discussed in the following section?
was useful in determining whether the judges
found a particular instance difficult to annotate.2
For each instance of the target construction we
obtained three judgements from unique workers
on AMT. For approximately 80% of the items,
the judgements were unanimous. In the remaining
cases we solicited four additional judgements, and
used the majority judgement. We paid $0.05 per
judgement; the average time spent on each annota-
tion was approximately twenty seconds, resulting
in an average hourly wage of about $10.
The development data was also annotated by
three native English speaking experts (compu-
tational linguists with extensive linguistic back-
ground, two of whom are also authors of this pa-
per). The inter-annotator agreement among these
judges is very high, with pairwise observed agree-
ments of 1.00, 0.90, and 0.90, and corresponding
unweighted Kappa scores of 1.00, 0.79, and 0.79.
The majority judgements of these annotators are
the same as those obtained from AMT on the de-
velopment data, giving us confidence in the reli-
ability of the AMT judgements. These findings
are consistent with those of Snow et al (2008) in
showing that AMT judgements can be as reliable
as those of expert judges.
Finally, we remove a small number of items
from the testing dataset which were difficult to
paraphrase due to ellipsis of the verb participating
in the target construction, or an extra negation in
the verb phrase. We further remove one sentence
because we believe the paraphrases we provided
are in fact misleading. The number of sentences
and of instances (i.e., noun-verb-adjective triples)
of the target construction in the development and
testing datasets is given in Table 2. 160 of the 199
testing instances (80%) have the ?every? interpre-
tation, with the remainder having the ?no? inter-
pretation.
4 Analysis of annotation
We now more closely examine the annotations ob-
tained from AMT to better determine the extent to
2In other cases the comments were more humourous. In
response to the following sentence If you?ve ever yearned
to live on Sesame Street, where no problem is too big to be
solved by a not-too-big slice of strawberry-rhubarb pie, this
is the spot for you, one judge told us her preferred types of
pie.
Dataset # sentences # instances
Development 20 33
Test 140 199
Table 2: The number of sentences containing the
target construction, and the number of resulting in-
stances.
which they are reliable. We also consider specific
instances of the target construction that are judged
inconsistently to establish some of the causes of
disagreement.
One of the three experts who annotated the de-
velopment items (discussed in Section 3.2) also
annotated twenty items selected at random from
the testing data. In this case two instances are
judged differently than the majority judgement ob-
tained from AMT. These instances are given below
with the noun, adjective and verb in the target con-
struction underlined.
(4) When it comes to the clash of candidates on
national television, no detail, it seems, is too
minor for negotiation, no risk too small to
eliminate.
(5) Lectures by big-name Wall Street felons will
show why no swindler is too big to beat the
rap by peaching on small-timers.
For sentence (4), the AMT judgements were unan-
imously for the ?no? interpretation whereas the
expert annotator chose the ?every? interpretation.
We are uncertain as to the reason for this disagree-
ment, but are convinced that the ?every? interpre-
tation is the intended one.
In the case of sentence (5), the AMT judge-
ments were split four?three for the ?every? and
?no? interpretations, respectively, while the ex-
pert annotator chose the ?no? interpretation. For
this sentence the provided paraphrases were Ev-
ery swindler can beat the rap and No swindler
can beat the rap. If attention in the sentence
is restricted to the target construction?i.e., no
swindler is too big to beat the rap by peaching
on small-timers?either of the ?no? and ?every?
interpretations is possible. That is, this clause
alone can mean that ?no swindler is ?big? enough
to be able to beat the rap? (the ?no? interpreta-
tion), or that ?no swindler is ?big? enough that they
64
are above peaching on small-timers? (or in other
words, ?every swindler is able to beat the rap by
peaching on small-timers?, the ?every? interpreta-
tion). However, the intention of the sentence as the
?no? interpretation is clear from the referral in the
main clause to big-name Wall Street felons, which
implies that ?big? swindlers have not beaten the
rap. Since the AMT annotators may not be devot-
ing a large amount of attention to the task, they
may focus only on the target construction and not
the preliminary disambiguating material. In this
event, they may be choosing between the ?every?
and ?no? interpretations based on how cynical they
are of the ability (or lack thereof) of the American
legal system to punish Wall Street criminals.
We also examine a small number of examples
in the testing set which do not receive a clear
majority judgement from AMT. For this analysis
we consider items for which the difference in the
number of judgements for each of the ?every? and
the ?no? interpretations is one or less This gives
four instances of the target construction, one of
which we have already discussed above, example
(5); the others are presented below, again with the
noun, adjective, and verb participating in the target
construction underlined:
(6) Where are our priorities when we so
carefully weigh costs and medical efficacy in
deciding to offer a medical lifeline to the
elderly, yet no amount of money is too great
to spend on the debatable paths we?ve taken
in our war against terror?
(7) No neighborhood is too remote to diminish
Mr. Levine?s determination to discover and
announce some previously unheralded treat.
(8) No one is too remote anymore to be
concerned about style, Ms. Hansen
suggested.
In example (6) the author is using the target con-
struction to express somebody else?s viewpoint
that ?any amount should be spent on the war
against terror?. Therefore the literal reading of
the target construction appears to be the ?every?
interpretation. However, this construction is be-
ing used rhetorically (as part of the overall sen-
tence) to express the author?s belief that ?too much
money is being spent on the war against terror?,
which is close in meaning to the ?no? interpreta-
tion. It appears that the annotators are split be-
tween these two readings. For sentence (7) the
atypicality of neighbourhood as the subject of di-
minish may make this instance particularly diffi-
cult for the judges. Sentence (8) appears to us to be
a clear example of the ?every? interpretation. The
paraphrases for this usage are ?Everyone should
be concerned about style? and ?No one should be
concerned about style?. In this case it is possible
that the judges are biased by their beliefs about
whether one should be concerned about style, and
that this is giving rise to the lack of agreement.
These examples illustrate that some of these us-
ages are clearly complex for people to annotate.
Such complex examples may require more context
to be annotated with confidence.
5 Model
To test our hypothesis that the interaction of the se-
mantics of the noun, adjective, and verb in the tar-
get construction contributes to its pragmatic inter-
pretation, we represent each instance in our dataset
as a vector of features that capture aspects of the
semantics of its component words.
WordNet To tap into general lexical semantic
properties of the words in the construction, we
use features that draw on the semantic classes of
words in WordNet (Fellbaum, 1998). These bi-
nary features each represent a synset in WordNet,
and are turned on or off for the component words
(the noun, adjective, and verb) in each instance
of the target construction. A synset feature is on
for a word if the synset occurs on the path from
all senses of the word to the root, and off other-
wise. We use WordNet version 3.0 accessed using
NLTK version 2.0 (Bird et al, 2009).
Polarity Because of the observation that the
verb in the target construction, in particular, has
some property of negativity in the ?no? interpre-
tation, we also use features representing the se-
mantic polarity of the noun, adjective, and verb
in each instance. The features are tertiary, repre-
senting positive, neutral, or negative polarity. We
obtain polarity information from the subjectivity
lexicon provided by Wilson et al (2005), and con-
sider words to be neutral if they have both positive
and negative polarity, or are not in the lexicon.
6 Experimental results
6.1 Experimental setup
To evaluate our model we conduct a 5-fold cross-
validation experiment using the items in the test-
65
ing dataset. When partitioning the items in the
testing dataset into the five parts necessary for the
cross-validation experiment, we ensure that all the
instances of the target construction from a single
sentence are in the same part. This ensures that
no instance used for training is from the same sen-
tence as an instance used for testing. We further
ensure that the proportion of items in each class is
roughly the same in each split.
For each of the five runs, we linearly scale the
training data to be in the range [?1, 1], and ap-
ply the same transformation to the testing data.
We train a support vector machine (LIBSVM ver-
sion 2.9, Chang and Lin, 2001) with a radial ba-
sis function kernel on the training portion in each
run, setting the cost and gamma parameters using
cross-validation on just the training portion, and
then test the classifier on the testing portion for
that run using the same parameter settings. We
micro-average the accuracy obtained on each of
the five runs. Finally, we repeat each 5-fold cross-
validation experiment five times, with five random
splits, and report the average accuracy over these
trials.
6.2 Results
Results for experiments using various subsets of
the features are presented in Table 3. We re-
strict the component word?the noun, adjective, or
verb?for which we extract features to those listed
in column ?Word?, and extract only the features
given in column ?Features? (WordNet, polarity, or
all). The majority baseline is 80%, corresponding
to always selecting the ?every? interpretation. Ac-
curacies shown in boldface are significantly better
than the majority class baseline using a paired t-
test. (In all cases where the difference is signifi-
cant, we obtain p ? 0.01.)
We first consider the results using features ex-
tracted only for the noun, adjective, or verb indi-
vidually, using all features. The best accuracy in
this group of experiments, 87%, is achieved using
the verb features, and is significantly higher than
the majority baseline. On the other hand, the clas-
sifiers trained on the noun and adjective features
individually perform no better than the baseline.
These results support our hypothesis that lexical
semantic properties of the component verb in the
No X is too Y to Z construction do indeed play
an important role in determining its interpretation.
Although we proposed that selectional constraints
from the verb would also lead to differing seman-
tics of the nouns and adjectives in the two interpre-
tations, our WordNet features are likely too sim-
plistic to capture this effect, if it does hold. Before
ruling out the semantic contribution of these words
to the interpretation, we need to explore whether
a more sophisticated model of selectional prefer-
ences, as in Ciaramita and Johnson (2000) or Clark
and Weir (2002), yields more informative features
for the noun and adjective.
Experimental setup % accuracy
Word Features
Noun All 80
Adjective All 80
Verb All 87
All WordNet 88
All Polarity 80
All All 88
Majority baseline 80
Table 3: % accuracy on testing data for each exper-
imental condition and the majority baseline. Ac-
curacies in boldface are statistically significantly
different from the baseline.
We now consider the results using the WordNet
and polarity features individually, but extracted for
all three component words. The WordNet features
perform as well as the best results using all fea-
tures for all three words, which gives further sup-
port to our hypothesis that the semantics of the
components of the target construction are related
to its interpretation. The polarity features perform
poorly. This is perhaps unsurprising as polarity is
a poor approximation to the property of ?negativ-
ity? that we are attempting to capture. Moreover,
many of the nouns, adjectives, and verbs in our
dataset either have neutral polarity or are not in
the polarity lexicon, and therefore the polarity fea-
tures are not very discriminative. In future work,
we plan to examine the WordNet classes of the
verbs that occur in the ?no? interpretation to try to
more precisely characterize the property of nega-
tivity that these verbs tend to have.
6.3 Error analysis
To better understand the errors our classifier is
making, we examine the specific instances which
are classified incorrectly. Here we focus on the
experiment using all features for all three com-
ponent words. There are 23 instances which are
66
consistently mis-classified in all runs of the exper-
iment. According to the AMT judgements, each of
these instances corresponds to the ?no? interpreta-
tion. These errors reflect the bias of the classifier
towards the more frequent class, the ?every? inter-
pretation.
We further note that two of the instances dis-
cussed in Section 4?examples (4) and (6)?are
among those instances consistently classified in-
correctly. The majority judgement from AMT for
both of these instances is the ?no? interpretation,
while in our assessment they are in fact the ?ev-
ery? interpretation. We are therefore not surprised
to see these items ?mis-classified? as ?every?.
Example (8) was incorrectly classified in one
trial. In this case we agree with the gold-standard
label obtained from AMT in judging this instance
as the ?every? interpretation; nevertheless, this
does appear to be a difficult instance given the low
agreement observed for the AMT judgements.
It is interesting that no items with an ?every? in-
terpretation are consistently misclassified. In the
context of our overall results showing the impact
of the verb features on performance, we conclude
that the ?no? interpretation arises due to particular
lexical semantic properties of certain verbs. We
suspect then that the consistent errors on the 21
truly misclassified expressions (23 minus the 2 in-
stances discussed above that we believe to be an-
notated incorrectly) are due to sparse data. That
is, if it is indeed the verb that plays a major role in
leading to a ?no? interpretation, there may simply
be insufficient numbers of such verbs for training
a supervised model in a dataset with only 39 ex-
amples of those usages.
7 Discussion
We have presented the first computational study of
the semantically and pragmatically complex con-
struction No X is too Y to Z. We have developed
a computational model that automatically disam-
biguates the construction with an accuracy of 88%,
reducing the error-rate over the majority-baseline
by 40%. The model uses features that tap into the
lexical semantics of the component words partic-
ipating in the construction, particularly the verb.
These results demonstrate that lexical properties
can be successful in resolving an ambiguity pre-
viously thought to depend on complex pragmatic
inference over presuppositions (as in Wason and
Reich (1979)).
These results can be usefully situated within
the context of linguistic and psycholinguistic work
on semantic interpretation processing. Beginning
around 20 years ago, work in modeling of human
semantic preferences has focused on the extent to
which properties of lexical items influence the in-
terpretation of various linguistic ambiguities (e.g.,
Trueswell and Tanenhaus, 1994). While semantic
context and plausibility are also proposed to play
a role in human interpretation of ambiguous sen-
tences (e.g., Crain and Steedman, 1985; Altmann
and Steedman, 1988), it has been pointed out that
it would be difficult to ?operationalize? the com-
plex interactions of presuppositional factors with
real-world knowledge in a precise algorithm for
disambiguation (Jurafsky, 1996). Although not in-
tended as proposing a cognitive model, the work
here can be seen as connected to these lines of re-
search, in investigating the extent to which lexical
factors can be used as proxies to more ?hidden?
features that underlie the appropriate interpreta-
tion of a pragmatically complex construction.
Moreover, as in the approach of Jurafsky
(1996), the phenomenon we investigate here may
be best considered within a constructional analy-
sis (e.g., Langacker, 1987), in which both the syn-
tactic construction and the particular lexical items
contribute to the determination of the meaning of a
usage. We suggest that a clause of the form No X is
too Y to Z might be the (identical) surface expres-
sion of two underlying constructions?one with
the ?every? interpretation and one with the ?no?
interpretation?which place differing constraints
on the semantics of the verb. (E.g., in the ?no?
interpretation, the verb typically has some ?neg-
ative? semantic property, as noted in Section 2.)
Looked at from the other perspective, the lexical
semantic properties of the verb might determine
which No X is too Y to Z construction (and associ-
ated interpretation) it is compatible with. Our re-
sults support this view, by showing that semantic
classes of verbs have predictive value in selecting
the correct interpretation.
Note that such a constructional analysis of
this phenomenon assumes that both interpretations
of these sentences are linguistically valid, given
the appropriate lexical instantiation. This stands
in contrast to the analysis of Wason and Reich
(1979), which presumes that people are apply-
ing some higher-level reasoning to ?correct? an
ill-formed statement in the case of the ?no? in-
67
terpretation. While such extra-grammatical infer-
ence may play a role in support of language under-
standing when people are faced with noisy data, it
seems unlikely to us that a construction that is used
quite readily and with a predictable interpretation
is nonsensical according to rules of grammar. Our
results point to an alternative linguistic analysis,
one whose further development may also help to
improve automatic disambiguation of instances of
No X is too Y to Z. In the next section, we discuss
directions for future work that could elaborate on
these preliminary findings.
8 Future Work
One limitation of this study is that the dataset used
is rather small, consisting of just 199 instances
of the target construction. As discussed in Sec-
tion 3.1, the extraction process we use to obtain
our experimental items has low recall; in particular
it misses variants of the target construction such as
Nothing is too Y to Z and No X is too Y for Z. In
the future we intend to expand our dataset by ex-
tracting such usages. Furthermore, the data used
in the present study is primarily taken from news
text. While we do not adopt the view of some that
usages of the target construction having the ?no?
interpretation are errors, it could be the case that
such usages are more frequent in less formal text.
In the future we also intend to extract usages of
the target construction from datasets of less formal
text, such as blogs (e.g., Burton et al, 2009).
Constructions other than No X is too Y to Z ex-
hibit a similar ambiguity. For example, the con-
struction X didn?t wait to Y is ambiguous between
?X did Y right away? and ?X didn?t do Y at all?
(Karttunen, 2007). In the future we would like to
extend our study to consider more such construc-
tions which are ambiguous due to the interpreta-
tion of negation.
In Section 4 we note that for some instances the
complexity of the sentences containing the target
construction may make it difficult for the anno-
tators to judge the meaning of the target. In the
future we intend to present simplified versions of
these sentences?which retain the noun, adjective,
and verb from the target construction in the orig-
inal sentence?to the judges to avoid this issue.
Such an approach will also help us to focus more
clearly on observable lexical semantic effects.
We are particularly interested in further explor-
ing the hypothesis that it is the semantics of the
component verb that gives rise to the meaning of
the target construction. Recall Pullum?s (2004)
observation that the verb in the ?no? interpretation
involves explicitly not acting. Using this intuition,
we have informally observed that it is largely pos-
sible to (manually) predict the interpretation of the
target construction knowing only the component
verb. We are interested in establishing the extent to
which this observation holds, and precisely which
aspects of a verb?s meaning give rise to the inter-
pretation of the target construction.
Our current model of the semantics of the target
construction does not capture Wason and Reich?s
(1979) observation that the compositional mean-
ing of instances having the ?no? interpretation is
non-pragmatic. While we do not adopt their view
that these usages are somehow ?errors?, we do
think that their observation can indicate other pos-
sible lexical semantic properties that may help to
identify the correct interpretation. Taking the clas-
sic example from Wason and Reich, no head in-
jury is too trivial to ignore, one clue to the ?no?
interpretation is that generally a head injury is not
something that is ignored. On the other hand, con-
sidering Wason and Reich?s example no missile is
too small to ban, it is widely believed that missiles
should be banned. We would like to add features
that capture this knowledge to our model.
In preliminary experiments we have used co-
occurrence information as an approximation to
this knowledge. (For example, we would expect
that head injury would tend to co-occur less with
ignore than with antonymous verbs such as treat
or address.) Although our early results using
co-occurrence features do not indicate that they
are an improvement over the other features con-
sidered (WordNet and polarity), it may also be
the case that our present formulation of these co-
occurrence features does not effectively capture
the intended knowledge. In the future we plan
to further consider such features, especially those
that model the selectional preferences of the verb
participating in the target construction.
These several strands of future work?
increasing the size of the dataset, improving the
quality of annotation, and exploring additional
features in our computational model?will en-
able us to extend our linguistic analysis of this
interesting phenomenon, as well as to improve
performance on automatic disambiguation of this
complex construction.
68
Acknowledgments
We thank Magali Boizot-Roche and Timothy
Fowler for their help in preparing the data for this
study. This research was financially supported by
the Natural Sciences and Engineering Research
Council of Canada and the University of Toronto.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, ed-
itors, Principle-Based Parsing: Computation
and Psycholinguistics, pages 257?278. Kluwer
Academic Publishers.
Gerry T. M. Altmann and Mark Steedman. 1988.
Interaction with context during human sentence
processing. Cognition, 30(3):191?238.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with
Python. O?Reilly Media Inc.
Lou Burnard. 2000. The British National Cor-
pus Users Reference Guide. Oxford University
Computing Services.
Kevin Burton, Akshay Java, and Ian Soboroff.
2009. The ICWSM 2009 Spinn3r Dataset. In
Proc. of the Third International Conference on
Weblogs and Social Media. San Jose, CA.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.
ntu.edu.tw/
?
cjlin/libsvm.
Massimiliano Ciaramita and Mark Johnson. 2000.
Explaining away ambiguity: Learning verb se-
lectional preference with Bayesian networks. In
Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
2000), pages 187?193. Saarbru?cken, Germany.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hier-
archy. Computational Linguistics, 28(2):187?
206.
Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Compu-
tational Linguistics, 29(4):589?637.
Stephen Crain and Mark Steedman. 1985. On
not being led up the garden path: The use
of context by the psychological syntax pro-
cessor. In David R. Dowty, Lauri Karttunen,
and Arnold M. Zwicky, editors, Natural lan-
guage parsing: Psychological, computational,
and theoretical perspectives, pages 320?358.
Cambridge University Press, Cambridge.
James Curran, Stephen Clark, and Johan Bos.
2007. Linguistically motivated large-scale NLP
with C&C and Boxer. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 33?36. Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. Wordnet: An
Electronic Lexical Database. Bradford Books.
Daniel Jurafsky. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science, 20(2):137?194.
Lauri Karttunen. 2007. Word play. Computational
Linguistics, 33(4):443?467.
Ronald W. Langacker. 1987. Foundations of
Cognitive Grammar: Theoretical Prerequisites,
volume 1. Stanford University Press, Stanford.
Mark Liberman. 2009a. No detail too small.
Retrieved 9 February 2010 from http://
languagelog.ldc.upenn.edu/nll/.
Mark Liberman. 2009b. No wug is too
dax to be zonged. Retrieved 9 February
2010 from http://languagelog.ldc.
upenn.edu/nll/.
Geoffrey K. Pullum. 2004. Too complex to
avoid judgment? Retrieved 7 April 2010 from
http://itre.cis.upenn.edu/
?
myl/
languagelog/.
Evan Sandhaus. 2008. The New York Times An-
notated Corpus. Linguistic Data Consortium,
Philadelphia, PA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky,
and Andrew Y. Ng. 2008. Cheap and fast ? But
is it good? Evaluating non-expert annotations
for natural language tasks. In Proceedings of
EMNLP-2008, pages 254?263. Honolulu, HI.
John Trueswell and Michael J. Tanenhaus. 1994.
Toward a lexicalist framework for constraint-
based syntactic ambiguity resolution. In
Charles Clifton, Lyn Frazier, and Keith Rayner,
editors, Perspectives on Sentence Processing,
pages 155?179. Lawrence Erlbaum, Hillsdale,
NJ.
Peter Wason and Shuli Reich. 1979. A verbal il-
lusion. The Quarterly Journal of Experimental
Psychology, 31(4):591?597.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of HLT/EMNLP-2005, pages 347?354. Vancou-
ver, Canada.
69
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 52?57,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Automatically Assessing Whether a Text Is Cliche?d,
with Applications to Literary Analysis
Paul Cook
Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
paulcook@unimelb.edu.au
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, Canada M5S 3G4
gh@cs.toronto.edu
Abstract
Cliche?s, as trite expressions, are predom-
inantly multiword expressions, but not all
MWEs are cliche?s. We conduct a prelimi-
nary examination of the problem of determin-
ing how cliche?d a text is, taken as a whole, by
comparing it to a reference text with respect
to the proportion of more-frequent n-grams, as
measured in an external corpus. We find that
more-frequent n-grams are over-represented
in cliche?d text. We apply this finding to the
?Eumaeus? episode of James Joyce?s novel
Ulysses, which literary scholars believe to be
written in a deliberately cliche?d style.
1 Cliche?s
In the broadest sense a cliche? is a tired, overused,
unoriginal idea, whether it be in music, in the vi-
sual arts, in the plot of a novel or drama, or in the
language of literature, journalism, or rhetoric. Here,
we are interested only in cliche?s of linguistic form.
Cliche?s are overused, unoriginal expressions that ap-
pear in a context where something more novel might
have reasonably been expected, or which masquer-
ade as something more original, more novel, or more
creative than they actually are. A cliche? is a kind of
ersatz novelty or creativity that is, ipso facto, unwel-
come or deprecated by the reader. Cliche?s appear to
be intuitively recognized by readers, but are difficult
to define more formally.
Cliche?s are predominantly multiword expressions
(MWEs) and are closely related to the idea of formu-
laic language, which for Wray (2002, 2008, summa-
rized in 2009) is a psycholinguistic phenomenon: a
formula is stored and retrieved as a single prefabri-
cated unit, without deeper semantic analysis, even if
it is made up of meaningful smaller units and regard-
less of whether it is or isn?t semantically transparent.
She demonstrates that formulaic language is a het-
erogeneous phenomenon, encompassing many types
of MWEs including fixed expressions (Sag et al,
2002, e.g., whys and wherefores), semi-fixed expres-
sions (e.g., hoist with/by his own petard ?injured
by that with which he would injure others?), and
syntactically-flexible expressions (e.g., sb1 haul sb2
over the coals ?reprimand severely?, allowing also
the passive sb2 was hauled over the coals (by sb1)).
Formulaic language can exhibit any of the types of
idiomaticity required by Baldwin and Kim (2010)
for an expression to be considered an MWE, i.e.,
lexical (de rigueur), syntactic (time and again), se-
mantic (fly off the handle ?lose one?s temper?), prag-
matic (nice to see you), and statistical idiomaticity
(which many of the previous examples also exhibit).
Another theme relating formulaic language to
MWEs is that of a common or preferred (though
not necessarily invariable) way for native speakers to
express an idea, i.e., institutionalization; for exam-
ple, felicitations to someone having a birthday are
usually expressed as happy birthday or (largely in
British English) many happy returns rather than any
of the many other semantically similar possibilities
(#merry birthday; cf. merry Christmas).
However, formulaic language, including cliche?s,
goes beyond the typical view of MWEs in that it
has a cultural aspect as well as a purely linguis-
tic aspect, as it includes catchphrases and allusions
to language in popular culture, such as well-known
52
lines from songs, jokes, advertisements, books, and
movies (curiouser and curiouser from Lewis Car-
roll?s Alice?s Adventures in Wonderland; go ahead,
make my day ?I dare you to attack me or do some-
thing bad, for if you do I will take great pleasure in
defeating and punishing you? from the 1983 Clint
Eastwood movie Sudden Impact).
Furthermore, not all formulaic language is
cliche?d; a weather forecast, for example, has no pre-
tensions of being linguistically creative or original,
but it would be a mistake to think of it as cliche?d,
no matter how formulaic it might be. Conversely,
a cliche? might not be formulaic from Wray?s psy-
cholinguistic perspective ? stored and recognized
as a single unit ? even if its occurrence is at least
frequent enough in relevant contexts for it to be rec-
ognized as familiar, trite, and unoriginal.
Finally, not all MWEs are cliche?s. Verb?particle
constructions such as look up (?seek information in
a resource?) and clear out are common expressions,
but aren?t unoriginal in the sense of being tired and
over-used. Moreover, they are not attempts at cre-
ativity. On the other hand, cliche?s are typically
MWEs. Some particularly long cliche?s, however,
are more prototypical of proverbs than MWEs (e.g.,
the grass is always greener on the other side). Sin-
gle words can also be trite and over-used, although
this tends to be strongly context dependent.
This paper identifies cliche?s as an under-studied
problem closely related to many issues of interest
to the MWE community. We propose a preliminary
method for assessing the degree to which a text is
cliche?d, and then show how such a method can con-
tribute to literary analysis. Specifically, we apply
this approach to James Joyce?s novel Ulysses to of-
fer insight into the ongoing literary debate about the
use of cliche?s in this work.
2 Related work
Little research in computational linguistics has
specifically addressed cliche?s. The most relevant
work is that of Smith et al (2012) who propose a
method for identifying cliche?s in song lyrics, and
determining the extent to which a song is cliche?d.
Their method combines information about rhymes
and the df-idf of trigrams (tf-idf, but using docu-
ment frequency instead of term frequency) in song
lyrics. However, this method isn?t applicable for our
goal of determining how cliche?d an arbitrary text is
with a focus on literary analysis, because in this case
rhyming is not a typical feature of the texts. More-
over, repetition in song lyrics motivated their df-idf
score, but this is not a salient feature of the texts we
consider.
In his studies of cliche?s in Ulysses, Byrnes (2012)
has drawn attention to the concept of the cliche? den-
sity of a text, i.e., the number of cliche?s per unit
of text (e.g., 1000 words). Byrnes manually iden-
tified cliche?s in Ulysses, but given a comprehensive
cliche? lexicon, automatically measuring cliche? den-
sity appears to be a straightforward application of
MWE identification ? i.e., determining which to-
kens in a text are part of an MWE. Although much
research on identification has focused on specific
kinds of MWEs (Baldwin and Kim, 2010), whereas
cliche?s are a mix of types, simple regular expres-
sions could be used to identify many fixed and semi-
fixed cliche?s. Nevertheless, an appropriate cliche?
lexicon would be required for this approach. More-
over, because of the relationship between cliche?s
and culture, to be applicable to historical texts, such
as for the literary analysis of interest to us, a lexicon
for the appropriate time period would be required.
Techniques for MWE extraction could potentially
be used to (semi-) automatically build a cliche? lex-
icon. Much work in this area has again focused
on specific types of MWEs ? e.g., verb?particle
constructions (Baldwin, 2005) or verb?noun com-
binations (Fazly et al, 2009) ? but once more the
heterogeneity of cliche?s limits the applicability of
such approaches for extracting them. Methods based
on strength of association ? applied to n-grams
or words co-occurring through some other relation
such as syntactic dependency (see Evert, 2008, for
an overview) ? could be applied to extract a wider
range of MWEs, although here most research has
focused on two-word co-occurrences, with consid-
erably less attention paid to longer MWEs. Even
if general-purpose MWE extraction were a solved
problem, methods would still be required to distin-
guish between MWEs that are and aren?t cliche?s.
53
3 Cliche?-density of known-cliche?d text
Frequency per se is not a necessary or defining crite-
rion of formulaic language. Wray (2002) points out
that even in quite large corpora, many undoubted in-
stances of formulaic language occur infrequently or
not at all; for example, Moon (1998) found that for-
mulae such as kick the bucket and speak for your-
self! occurred zero times in her 18 million?word
representative corpus of English. Nevertheless in
a very large corpus we?d expect a formulaic ex-
pression to be more frequent than a more-creative
expression suitable in the same context. Viewing
cliche?s as a type of formulaic language, we hypoth-
esized that a highly-cliche?d text will tend to contain
more n-grams whose frequency in an external cor-
pus is medium or high than a less-cliche?d text of the
same size.
We compared a text known to contain many
cliche?s to more-standard text. As a highly-
cliche?d text we created a document consisting
solely of a sample of 1,988 cliche?s from a web-
site (clichesite.com) that collects them.1 For a
reference ?standard? text we used the written por-
tion of the British National Corpus (BNC, Burnard,
2000). But because a longer text will tend to contain
a greater proportion of low-frequency n-gram types
(as measured in an external corpus) than a shorter
text, it is therefore crucial to our analysis that we
compare equal-size texts. We down-sampled our
reference text to the same size as our highly-cliche?d
text, by randomly sampling sentences.
For each 1?5-gram type in each document (i.e.,
in the sample of cliche?s and in the sample of sen-
tences from the BNC), we counted its frequency in
an external corpus, the Web 1T 5-gram Corpus (Web
1T, Brants and Franz, 2006). Histograms for the fre-
quencies are shown in Figure 1. The x-axis is the
log of the frequency of the n-gram in the corpus,
and the y-axis is the proportion of n-grams that had
that frequency. The dark histogram is for the sam-
ple from the BNC, and the light histogram is for the
cliche?s; the area where the two histograms overlap is
medium grey. For 1-grams, the two histograms are
quite similar; hence the following observations are
1Because we don?t know the coverage of this resource, it
would not be appropriate to use it for an MWE-identification
approach to measuring cliche?-density.
not merely due to simple differences in word fre-
quency. For the 3?5-grams, the light areas show that
the cliche?s contain many more n-gram types with
medium or high frequency in Web 1T than the sam-
ple of sentences from the BNC. For each of the 3?5-
grams, the types in the sample of cliche?s are signif-
icantly more frequent than those in the BNC using
a Wilcoxon rank sum test (p  0.001). The his-
togram for the 2-grams, included for completeness,
is beginning to show the trend observed for the 3?5-
grams, but there is no significant difference in mean
frequency in this case.
This finding supports our hypothesis that cliche?d
text contains more higher-frequency n-grams than
standard text. In light of this finding, in the follow-
ing section we apply this n-gram?based analysis to
the study of cliche?s in Ulysses.
4 Assessing cliche?-density for literary
analysis
Ulysses, by James Joyce, first published in 1922, is
generally regarded as one of the greatest English-
language novels of the twentieth century. It
is divided into 18 episodes written in widely
varying styles and genres. For example, some
episodes are, or contain, long passages of stream-
of-consciousness thought of one of the characters;
another is written in catechism-style question-and-
answer form; some parts are relatively conventional.
Byrnes (2010, 2012) points out that it has long
been recognized that, intuitively, some parts of the
novel are written in deliberately formulaic, cliche?d
language, whereas some other parts use novel, cre-
ative language. However, this intuitive impression
had not previously been empirically substantiated.
Byrnes took the simple step of actually counting the
cliche?s in four episodes of the book and confirmed
the intuition. In particular, he found that the ?Eu-
maeus? episode contained many more cliche?s than
the other episodes considered. However, these re-
sults are based on a single annotator identifying the
cliche?s ? Byrnes himself ? working with an infor-
mal definition of the concept, and possibly biased
by expected outcomes. By automatically and objec-
tively measuring the extent to which ?Eumaeus? is
cliche?d, we can offer further evidence ? of a very
different type ? to this debate.
54
Figure 1: Histograms for the log frequency of n-grams in a sample of sentences from the BNC and a collection of
known cliche?s. 1?5-grams are shown from left to right, top to bottom.
We compared ?Eumaeus? to a background text
consisting of episodes 1?2 and 4?10 of Ulysses,
which are not thought to be written in a marked
style. Because formulaic language could vary over
time, we selected an external corpus from the time
period leading up to the publication of Ulysses ?
the Google Books NGram Corpus (Michel et al,
2011) for the years 1850?1910 (specifically, the
?English 2012? version of this corpus). We down-
sampled each episode, by randomly sampling sen-
tences, to the size of the smallest, to ensure that we
compared equal-size texts.
Figures 2 and 3 show histograms for the fre-
quencies in the external corpus of the 1?5-grams
in ?Eumaeus? and in the background episodes. If
?Eumaeus? is more-cliche?d than the background
episodes, then, given our results in Section 3 above,
we would expect it to contain more high-frequency
higher-order n-grams. We indeed observe this in the
histograms for the 3- and 4-grams. The differences
for each of the 3?5-grams are again significant us-
ing Wilcoxon rank sum tests (p 0.001 for 3- and
4-grams, p < 0.005 for 5-grams), although the ef-
fect is less visually striking than in the analysis in
Section 3, particularly for the 5-grams. One possi-
ble reason for this difference is that in the analysis
in Section 3 the known-cliche?d text was artificial in
the sense that it was a list of expressions, as opposed
to natural text.
We further compared the mean frequency of the
3-, 4-, and 5-grams in ?Eumaeus? to that of each in-
dividual background episode, again down-sampling
by randomly sampling sentences, to ensure that
equal-size texts are compared. In each case we find
that the mean n-gram frequency is highest in ?Eu-
maeus?. These results are consistent with Byrnes?s
finding that ?Eumaeus? is written in a cliche?d style.
5 Conclusions
Cliche?s are an under-studied problem in computa-
tional linguistics that is closely related to issues of
interest to the MWE community. In our prelimi-
nary analysis, we showed that a highly-cliche?d text
contains more higher-frequency n-gram types than a
more-standard text. We then applied this approach
to literary analysis, confirming beliefs about the use
of cliche?s in the ?Eumaeus? episode of Ulysses.
55
Figure 2: Histograms for the log frequency of n-grams in
the ?Eumaeus? episode of Ulysses and episodes known
to be non-cliche?d. 1-, and 2-grams are shown on the top
and bottom, respectively.
Acknowledgments
We thank Timothy Baldwin and Bahar Salehi for
their insightful comments on this work. This work
was supported financially by the Natural Sciences
and Engineering Research Council of Canada.
References
Timothy Baldwin. 2005. The deep lexical acquisi-
tion of English verb-particle constructions. Com-
puter Speech and Language, Special Issue on
Multiword Expressions, 19(4):398?414.
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Figure 3: Histograms for the log frequency of n-grams in
the ?Eumaeus? episode of Ulysses and episodes known
to be non-cliche?d. 3-, 4-, and 5-grams are shown on the
top, middle, and bottom, respectively.
56
Damerau, editors, Handbook of Natural Lan-
guage Processing, Second Edition, pages 267?
292. CRC Press, Boca Raton, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Robert Byrnes. 2010. A statistical analysis of
the ?Eumaeus? phrasemes in James Joyce?s
Ulysses. In Actes des 10es Journe?es inter-
nationales d?Analyse statistique des Donne?es
Textuelles / Proceedings of the 10th International
Conference on Textual Data Statistical Analysis,
pages 289?295. Rome, Italy.
Robert Byrnes. 2012. The stylometry of cliche?
density and character in James Joyce?s Ulysses.
In Actes des 11es Journe?es internationales
d?Analyse statistique des Donne?es Textuelles /
Proceedings of the 11th International Conference
on Textual Data Statistical Analysis, pages 239?
246. Lie`ge, Belgium.
Stefan Evert. 2008. Corpora and collocations. In
Corpus Linguistics. An International Handbook.
Article 58. Mouton de Gruyter, Berlin.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguis-
tics, 35(1):61?103.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, William
Brockman, The Google Books Team, Joseph P.
Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,
Jon Orwant, Steven Pinker, Martin A. Nowak, and
Erez Lieberman Aiden. 2011. Quantitative anal-
ysis of culture using millions of digitized books.
Science, 331(6014):176?182.
Rosamund Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-Based Approach.
Clarendon Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLING 2002), pages 1?15.
Alex G. Smith, Christopher X. S. Zee, and Alexan-
dra L. Uitdenbogerd. 2012. In your eyes: Iden-
tifying cliche?s in song lyrics. In Proceedings of
the Australasian Language Technology Associa-
tion Workshop 2012, pages 88?96. Dunedin, New
Zealand.
Alison Wray. 2002. Formulaic Language and the
Lexicon. Cambridge University Press.
Alison Wray. 2008. Formulaic Language: Pushing
the Boundaries. Oxford University Press.
57
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129?138,
Dublin, Ireland, August 23 2014.
Exploring Methods and Resources for Discriminating Similar Languages
Marco Lui
??
, Ned Letcher
?
, Oliver Adams
?
,
Long Duong
??
, Paul Cook
?
and Timothy Baldwin
??
?
Department of Computing and Information Systems
The University of Melbourne
?
NICTA Victoria
mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au,
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici-
pants to build an automatic language identification system to discriminate between 13 languages
in 6 groups of highly-similar languages (or national varieties of the same language). In this
paper, we describe the submissions made by team UniMelb-NLP, which took part in both the
closed and open categories. We present the text representations and modeling techniques used,
including cross-lingual POS tagging as well as fine-grained tags extracted from a deep grammar
of English, and discuss additional data we collected for the open submissions, utilizing custom-
built web corpora based on top-level domains as well as existing corpora.
1 Introduction
Language identification (LangID) is the problem of determining what natural language a document is
written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994;
Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accu-
racy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further
work is accurate discrimination between closely-related languages (Ljube?si?c et al., 2007; Tiedemann
and Ljube?si?c, 2012). The problem has been explored for specific groups of confusable languages, such
as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and
Ljube?si?c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre,
2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task
(Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the
work on these various language groups by proposing a task on a single dataset containing text from 13
languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014).
In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took
part in both the closed and the open categories, submitting to the main component (Groups A-E) as well
as the separate English component (Group F). For our closed submissions, we focused on comparing a
conventional LangID methodology based on individual words and language-indicative letter sequences
(Section 2.1) to a methodology that uses a de-lexicalized representation of language (Section 2.3). For
Groups A-E we use cross-lingual POS-tagger adaptation (Section 2.3.1) to convert the raw text to a
POS stream using a per-group tagger, and use n-grams of POS tags as our de-lexicalized representation.
For English, we also use a de-lexicalized representation based on lexical types extracted from a deep
grammar (Section 2.3.2), which can be thought of as a very fine-grained tagset. For the open submissions,
we constructed new web-based corpora using a standard methodology, targeting per-language top-level
domains (Section 2.4.2). We also compiled additional training data from existing corpora (Section 2.4.1).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
129
2 Overview
Our main focus was to explore novel methods and sources of training data for discriminating similar
languages. In this section, we describe techniques and text representations that we tested, as well as the
external data sources that we used to build language identifiers for this task.
2.1 Language-Indicative Byte Sequences
Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is
robust to variation in languages across different sources of text. The LD feature set can be thought of as
language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly
characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin
(2012) present langid.py,
1
an off-the-shelf LangID system that utilizes the LD feature set. In this
work, we re-train langid.py using the training data provided by the shared task organizers, and use
this as a baseline result representative of the state-of-the-art in LangID.
2.2 Hierarchical LangID
In LangID research to date, systems generally do not take into account any form of structure in the
class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use
of this structure by introducing a two-level LangID model. The first level implements a single group-
level classifier, which takes an input sentence and identifies the language group (A?F) that the sentence
is from. The output of this group-level classifier is used to select a corresponding per-group classifier,
that is trained only on data for languages in the group. This per-group classifier is applied to the input
sentence and the output thereof is the final label for the sentence.
2.3 De-Lexicalized Text Representation for DSL
One of the challenges in a machine learning approach to discriminating similar languages is to learn
differences between languages that are truly representative of the distinction between varieties, rather
than differences that are merely representative of peculiarities of the training data (Kilgarriff, 2001). One
possible confounding factor is the topicality of the training data ? if the data for each variety is drawn
from different datasets, it is possible that a classifier will simply learn the topical differences between
datasets. Diwersy et al. (2014) carried out a study of colligations in French varieties, where the variation
in the grammatical function of noun lemmas was studied across French-language newspapers from six
countries. In their initial analysis the found that the characteristic features of each country included the
name of the country and other country-specific proper nouns, which resulted in near 100% classification
accuracy but do not provide any insight into national varieties from a linguistic perspective.
One strategy that has been proposed to mitigate the effect of such topical differences is the use of
a de-lexicalized text representation (Lui and Cook, 2013). The de-lexicalization is achieved through
the use of a Part-Of-Speech tagger, which labels each word in a sentence according to its word class
(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first
considered for native language identification (NLI), where they were used as a proxy for syntax in order
to capture certain types of grammatical errors (Wong and Dras, 2009). Syntactic structure is known to
vary across national dialects (Trudgill and Hannah, 2008), so Lui and Cook (2013) investigated POS plus
function word n-grams as a proxy for syntactic structure, and used this representation to build classifiers
to discriminate between Canadian, British and American English. They found that classifiers using such a
representation achieved above-baseline results, indicating some systematic differences between varieties
could be captured through the use of such a de-lexicalized representation. In this work, we explore this
idea further ? in particular, we examine (1) the applicability of de-lexicalized text representations to
other languages using automatically-induced crosslingual POS taggers, and (2) the difference in accuracy
for discriminating English varieties between representations based on a coarse-grained universal tagset
(Section 2.3.1) as compared to a very fine-grained tagset used in deep parsing (Section 2.3.2).
1
http://github.com/saffsd/langid.py
130
Sandy quit on Tuesday Sandy quit Tuesday
UT NOUN VERB ADP NOUN NOUN VERB NOUN
LTT n - pn v np
*
p np i-tmp n - c-dow n - pn v np
*
n - c-dow
British English American English
Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical
type tagset (LTT).
2.3.1 Crosslingual POS Tagging
A key issue in generating de-lexicalized text representations based on POS tags is the lack of availability
of POS taggers for many languages. While some languages have some tools available for POS tagging
(e.g. Treaties (Schmid, 1994) has parameter files for Spanish and Portuguese), the availability of POS
taggers is far from universal. To address this problem for the purposes of discriminating similar lan-
guages, we draw on previous work in unsupervised cross-lingual POS tagging (Duong et al., 2013) to
build a POS tagger for each group of languages, a method which we will refer to hereafter as ?UMPOS?.
UMPOS employs a 12-tag Universal Tagset introduced by Petrov et al. (2012), which consists of the
tags NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner or article), ADP
(preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNCT (punctua-
tion), and X (all other categories, e.g., foreign words or abbreviations). These twelve basic tags constitute
a ?universal? tagset in that they can be used to describe the morphosyntax of any language at a coarse
level.
UMPOS generates POS taggers for new languages in an unsupervised fashion, by making use of
parallel data and an existing POS tagger. The input for UMPOS is: (1) parallel data between the source
and target languages; and (2) a supervised POS tagger for the source language. The output will be the
tagger for the target language. The parallel data acts as a bridge to transfer POS annotation information
from the source language to the target language.
The steps used in UMPOS are as follow. First, we collect parallel data which has English as the source
language, drawing from Europarl (Koehn, 2005) and EUbookshop (Skadin??s et al., 2014). UMPOS word-
aligns the parallel data using the Giza++ alignment tool (Och and Ney, 2003). The English side is POS-
tagged using the Stanford POS tagger (Toutanova et al., 2003), and the POS tags are then projected from
English to the target language based solely on one-to-one mappings. Using the sentence alignment score,
UMPOS ranks the ?goodness? of projected sentences and builds a seed model for the target language on
a subset of the parallel data. To further improve accuracy, UMPOS builds the final model by applying
self-training with revision to the rest of the data as follows: (1) the parallel corpus data is divided into
different blocks; (2) the first block is tagged using the seed model; (3) the block is revised based on
alignment confidence; (4) a new tagger is trained on the first block and then used to tag the second block.
This process continues until all blocks are tagged. In experiments on a set of 8 languages, Duong et al.
(2013) report accuracy of 83.4%, which is state-of-the-art for unsupervised POS tagging.
2.3.2 English Tagging Using ERG Lexical Types
Focusing specifically on language Group F ? British English and American English ? we leveraged
linguistic information from the analyses produced by the English Resource Grammar (ERG: Flickinger
(2002)), a broad-coverage, handcrafted grammar of English in the HPSG framework (Pollard and Sag,
1994) and developed within the DELPH-IN
2
research initiative. In particular, we extracted the lexical
types assigned to tokens by the parser for the best analysis of each input string. In accordance with
the heavily lexicalized nature of HPSG, lexical types are the primary means of distinguishing between
different morphosyntactic contexts in which a given lexical entry can occur. They can be thought of as
fine-grained POS tags, containing subcategorisation information in addition to part of speech informa-
tion, and semantic information in cases that it directly impacts on morphosyntax. The version of the
ERG we used (the ?1212? release) has almost 1000 lexical types.
Table 1 illustrates an example of the type of syntactic variation that can be captured with the finer-
2
http://www.delph-in.net
131
Group Language Code
Web Corpora Existing Corpora
TLD # words # datasets # words
A Bosnian bs .ba 817383 4 715602
A Croatian hr .hr 43307311 5 1536623
A Serbian sr .rs 1374787 4 1204684
B Indonesian id .id 23812382 3 564824
B Malaysian my .my 2596378 3 535221
C Czech cz .cz 17103140 8 2181486
C Slovakian sk .sk 17253001 8 2308083
D Brazilian Portuguese pt-BR .br 27369673 4 860065
D European Portuguese pt-PT .pt 22620401 8 2860321
E Argentine Spanish es-AR .ar 45913651 2 619500
E Peninsular Spanish es-ES .es 30965338 9 3458462
F British English en-GB .uk 20375047 1 523653
F American English en-US .us 21298230 1 527915
Table 2: Word count of training data used for open submissions.
grained lexical types, that would be missed with the coarse-grained universal tagset. In American En-
glish, both Sandy resigned on Tuesday and Sandy resigned on Tuesday are acceptable whereas British
English does not permit the omission of the preposition before dates. In the coarse-grained tagset, the
American English form results in a sequence VERB : NOUN, which is not particularly interesting as we
expect this to occur in both English varieties, whereas the fine-grained lexical types allow us to capture
the sequence v np
*
ntr : n - c-dow (verb followed by count noun [day of week]), which we expect
to see in American English but not in British English.
Since the ERG models a sharp notion of grammaticality, not all inputs receive an analysis ? whether
due to gaps in the coverage of the grammar or genuinely ungrammatical input. The ERG achieved
a coverage of 86% over the training data across both British English and American English. Sentences
which failed to parse were excluded from use as input into the classifier. However the inability to classify
any sentence which we cannot parse is unsatisfactory. We solved this problem by generating lexical type
features for sentences which failed to parse using the ERG-trained ?ubertagger of Dridan (2013), which
performs both tokenisation and supertagging of lexical types and improves parser efficiency by reducing
ambiguity in the input lattice to the parser.
2.4 External Corpora
The DSL shared task invited two categories of participation: (1) Closed, using only training data provided
by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To
participate in the latter category, we sourced additional training data through: (1) collection of data
relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The
information about the additional training data is shown in Table 2.
2.4.1 Existing Corpora
We collected training data from a number of existing corpora, as shown in Table 3. Many of the cor-
pora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text
corpora commonly used for research in machine translation. The exceptions are: (1) debian, which
was constructed using translations of message strings from the Debian operating system,
3
; (2) BNC ?
the British National Corpus (Burnard, 2000); (3) OANC ? the open component of the Second Release
of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);
4
a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences
from each of the BNC and OANC, which we used as training data to generate ERG lextype features (Sec-
tion 2.3.2) for British English (en-GB) and American English (en-US), respectively. From RCV2 we
3
http://www.debian.org
4
http://trec.nist.gov/data/reuters/reuters.html
132
bs hr sr pt-PT pt-BR id my cz sk es-ES es-AR en-US en-GB
BNC X
debian X X X X X X X X X X X
ECB X X X X
EMEA X X X X
EUconst X X X X
Europarl X X X X
hrenWaC X
KDE4 X X X X X X X X X
KDEdoc X X X X
OANC X
OpenSubtitles X X X X X X X X X X
RCV2 X X
SETIMES2 X X X
Tatoeba X X
Table 3: Training data compiled from existing corpora.
used the Latin American Spanish news stories as a proxy for Argentine Spanish (es-AR). Note that, for
a given text source, we didn?t necessarily use data for all available languages. For example, debian
contains British English and American English translations, which we did not use.
2.4.2 Web Corpus Construction
Each existing corpus we describe in Section 2.4.1 provides incomplete coverage over the set of languages
in the shared task dataset. In order to have a resource that covers all the languages in the shared task
drawn from a single source, we constructed web corpora for each language. Our approach was strongly
inspired by the approach used to create ukWaC (Ferraresi et al., 2008), and the creation of each sub-
language?s corpus involved crawling the top level domains of the primary countries associated with
those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this
approach is that text found in the top-level domains (TLDs) of those countries will primarily be of
the sub-language dominant in that country. For instance, we assume that Portuguese text found when
crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will
be primarily Brazilian Portuguese.
The process of creating a corpus for each sub-language involved translating a sample of 200 of the
original ukWaC queries into each language using Panlex (Baldwin et al., 2010).
5
These queries were then
submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining
results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended
them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a
Heritrix 3.1.1
6
instance with default settings other than constraining the crawled content to the relevant
TLD.
Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach,
only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Jus-
text (Pomik?alek, 2011) was used to extract text from the selected documents. langid.py (Lui and
Baldwin, 2012) was then used to discard documents whose text was not in the relevant language or lan-
guage group. The corpus was then refined through deduplication. First, near-deduplication was done at
the paragraph level using Onion (Pomik?alek, 2011) with its default settings. Then, exact-match sentence-
level deduplication, ignoring whitespace and case, was applied.
3 Results and Discussion
Table 4 summarizes the runs submitted by team UniMelb NLP to the VarDial DSL shared task. We
submitted the maximum number of runs allowed, i.e. 3 closed runs and 3 open runs, to both the ?general?
Groups A?E subtask as well as the English-specific Group F subtask. We applied different methods to
Group F, as some of the tools (the ERG) and resources (BNC/OANC) were specific to English. For clarity
in discussion, we have labeled each of our runs according to a 3-letter code: the first letter indicates the
5
A sample of the queries was used because of time and resource limitations.
6
https://webarchive.jira.com/wiki/display/Heritrix
133
Run Description
Macro-avg F-Score
dev tst
Grp A-E closed
AC1 langid.py 13-way 0.822 0.817
AC2 langid.py per-group 0.923 0.918
AC3 POS features 0.683 0.671
Grp F closed
FC1 Lextype features 0.559 0.415
FC2 langid.py per-group 0.548 0.403
FC3 POS features 0.545 0.435
Grp A-E open
AO1 Ext Corpora (word-level model) 0.705 0.703
AO2 Web Corpora (word-level model) 0.771 0.767
AO3 5-way voting 0.881 0.878
Grp F open
FO1 Lextype features using BNC/OANC training data 0.491 0.572
FO2 Web Corpora (word-level model) 0.490 0.581
FO3 5-way voting 0.574 0.442
Table 4: Summary of the official runs submitted by UniMelbNLP. ?dev? indicates scores from our
internal testing on the development partition of the dataset.
subtask (A for Groups A?E, F for Group F), the second indicates Closed (?C?) or Open (?O?), and the
final digit indicates the run number.
AC1 represents a benchmark result based on the LangID system (Lui and Baldwin, 2012). We used
the training tools provided with langid.py to generate a new model using the training data provided
by the shared task organizers, noting that as only data from a single source is used, we are not able to
fully exploit the cross-domain feature selection (Lui and Baldwin, 2011) implemented by langid.py.
The macro-averaged F-score across groups is substantially lower than that on standard LangID datasets
(Lui and Baldwin, 2012).
AC2 and FC2 are a straightforward implementation of hierarchical LangID (Section 2.2), using
mostly-default settings of langid.py. A 6-way group-level classifier is trained, and well as 6 different
per-group classifiers. We increase the number of features selected per class (i.e. group or language) to
500 from the default of 300, to compensate for the smaller number of classes (langid.py off-the-
shelf supports 97 languages). In our internal testing on the provided development data, the group-level
classifier achieved 100% accuracy in classifying sentences at the group level, essentially reducing the
problem to within-group disambiguation. Despite being one of the simplest approaches, overall this
was our best-performing submission for Groups A?E. It also represents a substantial improvement on
AC1, further emphasizing the need to implement hierarchical LangID in order to attain high accuracy in
discriminating similar languages.
AC3 and FC3 are based solely on POS-tag sequences generated by UMPOS, and implement a hierar-
chical LangID approach similar to AC2/FC2. Each sentence in the training data is mapped to a POS-tag
sequence in the 12-tag universal tagset, using the per-group POS tagger for the language group. Each tag
was represented using a single character, allowing us to make use of langid.py to train 6 per-group
classifiers based on n-grams of POS-tags. We used n-grams of order 1?6, and selected 5000 top-ranked
sequences per-language. To classify test data, the same group-level classifier used in AC2 was used to
map sentences to language groups, and then the per-group POS tagger was applied to derive the corre-
sponding stream of POS tags for each sentence. The corresponding per-group classifier trained on POS
tag sequences was then applied to produce the final label for the sentence. For Groups A?E, we find that
134
bs hr sr id my cz sk
T 53.0 23.2 60.0 VHN 0.9 1.3 .1.1 1.0 1.2
TV 32.4 13.2 43.3 DHN 0.1 0.1 1.1. 2.0 2.2
NT 31.5 13.9 43.2 N.1. 12.1 3.1 1.1 4.0 4.4
TVN 24.8 9.8 34.3 N.N 63.3 48.0 .N.1 0.5 0.7
VT 19.4 6.1 27.1 .DNV 1.8 1.1 .C 39.0 33.5
TN 29.1 10.9 29.6 DH 1.7 2.1 .1.. 0.7 1.0
NTV 18.6 8.4 29.4 N.DN 3.2 2.0 .P 51.2 41.8
TVNN 16.8 6.9 23.7 VH 11.3 14.9 1. 14.0 13.9
NVT 11.2 2.9 15.5 PNV1 0.5 0.4 1.. 1.2 1.6
VTV 11.0 3.2 17.0 .1. 13.2 3.8 .R 44.0 30.0
pt-BR pt-PT es-AR es-ES en-GB en-US
X 3.4 2.8 .. 22.6 43.3 NNN 48.2 43.2
N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4
.NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0
XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9
NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5
D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4
NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5
.NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8
N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3
NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6
Table 5: Top 10 POS features per-group by Information Gain, along with percentage of sentences in each
language in which the feature appears. The notation used is as follows: . = punctuation, J = adjective,
P = pronoun, R = adverb, C = conjunction, D = determiner/article, N = noun, 1 = numeral, H = pronoun,
T = particle, V = verb, and X = others
the POS-tag sequence features are not as effective as the character n-grams used in AC2. Nonetheless,
the results attained are above baseline, indicating that there are systematic differences between languages
in each group that can be captured by an unsupervised approach to POS-tagging using a coarse-grained
tagset. This extends the similar observation made by Lui and Cook (2013) on varieties of English, show-
ing that the same is true for the other language groups in this shared task. Also of interest is the higher
accuracy attained by the POS-tag features on Groups A?E (i.e. AC3) than on English (Group F, FC3).
The top-10 sequences per-group are presented in Table 5, where it can be seen that the sequences are
often slightly more common in one language in the group than the other language(s). One limitation of
the Information Gain based feature selection used in langid.py is that each feature is scored inde-
pendently, and each language receives a binarized score. This can be seen in the features selected for
Group A, where all the top-10 features selected involve particles (labelled T). Overall, this indicates that
Croatian (hr) appears to use particles much less frequently than Serbian (sr) or Bosnian (bs), which is
an intriguing finding. However, most of the top-10 features are redundant in that they all convey very
similar information.
Similar to FC3, a hierarchical LangID approach is used in FC1, in conjunction with per-group classi-
fiers based on a sequence of tags derived from the original sentence. The difference between the taggers
used for FC3 and FC1 is that the FC3 tagger utilizes the 12-tag universal tagset, whereas the FC1 tagger
uses the English-specific lexical types from the ERG (Section 2.3.2), a set of approximately 1000 tags.
There is hence a trade-off to be made between the degree of distinction between tags, and the relative
sparsity of the data ? having a larger tagset means that any given sequence of tags is proportionally less
likely to occur. On the basis of the results of FC1 and FC3 on the dev data, the lexical type features
marginally outperform the coarse-grained universal tagset. However, this result is made harder to inter-
pret by the mismatch between the dev and tst partitions of the shared task dataset. We will discuss
this issue in more detail below, in the context of examining the results on Group F for the open category.
In the open category, we focused primarily on the effect of using different sources of training data.
AO1 and AO2 both implement a hierarchical LangID approach, again using the group-level classifier
from AC2. For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level
representation, with feature selection by Information Gain. The difference between the two is that A01
uses samples from existing text corpora (Section 2.4.1), whereas A02 uses web corpora that we prepared
specifically for this shared task (Section 2.4.2). In terms of accuracy, both types of corpora perform
135
substantially better than baseline, indicating that at the word level, there are differences between the
language varieties that are consistent across the different corpus types. This result is complementary to
Cook and Hirst (2012), who found that web corpora from specific top-level domains were representative
of national varieties of English. AO2 (web corpora) outperforms AO1 (existing corpora), further high-
lighting the relevance of web corpora as a source of training data for discriminating similar languages.
However, our models trained on external data were not able to outperform the models trained on the
official training data for Groups A?E. A03 consists of a 5-way majority vote between results AC1, AC2,
AC3, AO1 and AO2. Including the predictions from the closed submissions substantially improves the
result with respect to AO1/AO2, but overall our best result for Groups A?E was obtained by run AC2.
For Group F, FO1 utilizes ERG lexical type features in the same manner as FC1, the difference being
that FC1 uses the shared task trn partition, whereas FO1 uses sentences sampled from existing corpora,
specifically BNC for en-GB and OANC for en-US. FO2 implements the same concept as AO2, namely a
word-level naive Bayes model trained using web corpora. For the Group F (i.e. English) subtask, this was
our best-performing submission overall. FO3 is a 5-way vote between FC1, FC2, FC3, FO1 and FO2,
similar to AO3. Notably, our Group F submissions based on the supplied training data all performed
substantially better on the dev partition of the shared task dataset than on the tst partition. The inverse
is true for our submissions based on external corpora, where all our entries performed substantially better
on the tst partition than on the dev partition. Furthermore, the differences are fairly large, particularly
since Group F is a binary classification task with a 50% baseline. This implies that, at least under our
models, the en-GB portion of the trn partition is a better model of the en-US portion of the tst partition
than the en-GB portion thereof. This is likely due to the manual intervention that was only carried out
on the test portion of the dataset (Zampieri et al., 2014).
Our Group F results appear to be inferior to previous work on discriminating English varieties (Lui
and Cook, 2013). However, there are a number of differences that make it difficult to compare the
results: Lui and Cook (2013) studied differences between Australian, British and Canadian English,
whereas the shared task focused on differences between British and American English. Lui and Cook
(2013) also draw on training data from a variety of domains (national corpora, web corpora and Twitter
messages), whereas the shared task used a dataset collected from newspaper texts (Tan et al., 2014).
Consistent with Cook and Hirst (2012) and Lui and Cook (2013), we found that web corpora appear to be
representative of national varieties, and consistent with Lui and Cook (2013) we found that de-lexicalized
representations of text are able to provide better than baseline discrimination between national varieties.
Overall, these results highlight the need for further research into discriminating between varieties of
English.
4 Conclusion
Discriminating between similar languages is an interesting sub-problem in language identification, and
the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater
detail. Our most successful methods implement straightforward hierarchical LangID, firstly identifying
the language group that a sentence belongs to, before identifying the specific language. We examined a
number of text representations for the per-group language identifiers, including a standard representation
for language identification based on language-indicative byte sequences, as well as with de-lexicalized
text representations. We found that the performance of de-lexicalized representations was above baseline,
however we were not able to fully investigate approaches to integrating predictions from lexicalized
and de-lexicalized text representations due to time constraints. We also found that when using external
corpora, web corpora constructed by scraping per-country top-level domains performed as well as (if
not better than) data collected from existing text corpora, supporting the hypothesis that web corpora
are representative of national varieties of respective languages. Overall, our best result was obtained by
applying two-level hierarchical LangID, firstly identifying the language group that a sentence belongs
to, and then disambiguating within each group. Our best result was achieved by applying an existing
LangID method (Lui and Baldwin, 2012) to both the group-level and the per-group classification tasks.
136
Acknowledgments
The authors wish to thank Li Wang, Rebecca Dridan and Bahar Salehi for their kind assistance with
this research. NICTA is funded by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian Research Council through the
ICT Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL HLT 2010), pages 229?237, Los Angeles, USA.
Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of
all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics:
Demonstrations, pages 37?40, Beijing, China.
Marco Baroni and Silvia Bernardini. 2004. BootCaT: Bootstrapping corpora and terms from the Web. In Pro-
ceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004).
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University
Computing Services.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas, USA.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties of
English? In Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages
281?293, Li`ege, Belgium.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A weakly supervised multivariate approach to the study
of language variation. In Benedikt Szmrecsanyi and Bernhard W?alchli, editors, Aggregating Dialectology,
Typology, and Register Analysis. Linguistic Variation in Text and Speech. De Gruyter, Berlin.
Rebecca Dridan. 2013. Ubertagging. Joint segmentation and supertagging for English. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1201?1212, Seattle, USA.
Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013. Simpler unsupervised POS tagging with bilin-
gual projections. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 634?639.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Morocco.
Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering. CSLI Publi-
cations, Stanford, USA.
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica
dei Dati Testuali (JADT), pages 263?268, Rome, Italy.
Nancy Ide and Catherine Macleod. 2001. The American National Corpus: A standardized resource of American
English. In Proceedings of Corpus Linguistics 2001, pages 274?280, Lancaster, UK.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):97?133.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth
Machine Translation Summit (MT Summit X), pages 79?86, Phuket, Thailand.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identification : how to distinguish similar
languages ? In 29th International Conference on Information Technology Interfaces, pages 541?546.
Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceed-
ings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553?561,
Chiang Mai, Thailand.
137
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages
25?30, Jeju, Republic of Korea.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013, pages 5?15, Brisbane, Australia.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.
European Language Resources Association (ELRA).
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Jan Pomik?alek. 2011. Removing Boilerplate and Duplicate Content from Web Corpora. Ph.D. thesis, Masaryk
University.
John M. Prager. 1999. Linguini: language identification for multilingual documents. In Proceedings of the 32nd
Annual Hawaii International Conference on Systems Sciences (HICSS-32), Maui, Hawaii.
Bali Ranaivo-Malancon. 2006. Automatic Identification of Close Languages - Case study : Malay and Indonesian.
ECTI Transaction on Computer and Information Technology, 2(2):126?134.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Conference
on New Methods in Natural Language Processing, Manchester, 1994.
Raivis Skadin??s, J?org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free:
Building and using the EU Bookshop corpus. In Proceedings of the 9th International Conference on Language
Resources and Evaluation (LREC-2014), Reykjavik, Iceland.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
W. J. Teahan. 2000. Text Classification and Segmentation Using Minimum Cross-Entropy. In Proceedings the 6th
International Conference ?Recherche dInformation Assistee par Ordinateur? (RIAO00), pages 943?961, Paris,
France.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2619?
2634, Mumbai, India.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International
Conference on Language Resources and Evaluation (LREC 2012), pages 2214?2218, Istanbul, Turkey.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL
?03), pages 173?180, Edmonton, Canada.
Peter Trudgill and Jean Hannah. 2008. International English: A guide to varieties of Standard English. Hodder
Education, London, UK, 5th edition.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceed-
ings of the Australasian Language Technology Workshop 2009 (ALTW 2009), pages 53?61, Sydney, Australia.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS 2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN 2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann. 2014. A report on the DSL shared task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial), Dublin, Ireland.
138
