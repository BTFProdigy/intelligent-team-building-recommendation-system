Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 688?697, Prague, June 2007. c?2007 Association for Computational Linguistics
The Infinite PCFG using Hierarchical Dirichlet Processes
Percy Liang Slav Petrov Michael I. Jordan Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, petrov, jordan, klein}@cs.berkeley.edu
Abstract
We present a nonparametric Bayesian model
of tree structures based on the hierarchical
Dirichlet process (HDP). Our HDP-PCFG
model allows the complexity of the grammar
to grow as more training data is available.
In addition to presenting a fully Bayesian
model for the PCFG, we also develop an ef-
ficient variational inference procedure. On
synthetic data, we recover the correct gram-
mar without having to specify its complex-
ity in advance. We also show that our tech-
niques can be applied to full-scale parsing
applications by demonstrating its effective-
ness in learning state-split grammars.
1 Introduction
Probabilistic context-free grammars (PCFGs) have
been a core modeling technique for many as-
pects of linguistic structure, particularly syntac-
tic phrase structure in treebank parsing (Charniak,
1996; Collins, 1999). An important question when
learning PCFGs is how many grammar symbols
to allocate to the learning algorithm based on the
amount of available data.
The question of ?how many clusters (symbols)??
has been tackled in the Bayesian nonparametrics
literature via Dirichlet process (DP) mixture mod-
els (Antoniak, 1974). DP mixture models have since
been extended to hierarchical Dirichlet processes
(HDPs) and HDP-HMMs (Teh et al, 2006; Beal et
al., 2002) and applied to many different types of
clustering/induction problems in NLP (Johnson et
al., 2006; Goldwater et al, 2006).
In this paper, we present the hierarchical Dirich-
let process PCFG (HDP-PCFG). a nonparametric
Bayesian model of syntactic tree structures based
on Dirichlet processes. Specifically, an HDP-PCFG
is defined to have an infinite number of symbols;
the Dirichlet process (DP) prior penalizes the use
of more symbols than are supported by the training
data. Note that ?nonparametric? does not mean ?no
parameters?; rather, it means that the effective num-
ber of parameters can grow adaptively as the amount
of data increases, which is a desirable property of a
learning algorithm.
As models increase in complexity, so does the un-
certainty over parameter estimates. In this regime,
point estimates are unreliable since they do not take
into account the fact that there are different amounts
of uncertainty in the various components of the pa-
rameters. The HDP-PCFG is a Bayesian model
which naturally handles this uncertainty. We present
an efficient variational inference algorithm for the
HDP-PCFG based on a structured mean-field ap-
proximation of the true posterior over parameters.
The algorithm is similar in form to EM and thus in-
herits its simplicity, modularity, and efficiency. Un-
like EM, however, the algorithm is able to take the
uncertainty of parameters into account and thus in-
corporate the DP prior.
Finally, we develop an extension of the HDP-
PCFG for grammar refinement (HDP-PCFG-GR).
Since treebanks generally consist of coarsely-
labeled context-free tree structures, the maximum-
likelihood treebank grammar is typically a poor
model as it makes overly strong independence as-
sumptions. As a result, many generative approaches
to parsing construct refinements of the treebank
grammar which are more suitable for the model-
ing task. Lexical methods split each pre-terminal
symbol into many subsymbols, one for each word,
and then focus on smoothing sparse lexical statis-
688
tics (Collins, 1999; Charniak, 2000). Unlexicalized
methods refine the grammar in a more conservative
fashion, splitting each non-terminal or pre-terminal
symbol into a much smaller number of subsymbols
(Klein and Manning, 2003; Matsuzaki et al, 2005;
Petrov et al, 2006). We apply our HDP-PCFG-GR
model to automatically learn the number of subsym-
bols for each symbol.
2 Models based on Dirichlet processes
At the heart of the HDP-PCFG is the Dirichlet pro-
cess (DP) mixture model (Antoniak, 1974), which is
the nonparametric Bayesian counterpart to the clas-
sical finite mixture model. In order to build up an
understanding of the HDP-PCFG, we first review
the Bayesian treatment of the finite mixture model
(Section 2.1). We then consider the DP mixture
model (Section 2.2) and use it as a building block
for developing nonparametric structured versions of
the HMM (Section 2.3) and PCFG (Section 2.4).
Our presentation highlights the similarities between
these models so that each step along this progression
reflects only the key differences.
2.1 Bayesian finite mixture model
We begin by describing the Bayesian finite mixture
model to establish basic notation that will carry over
the more complex models we consider later.
Bayesian finite mixture model
? ? Dirichlet(?, . . . , ?) [draw component probabilities]
For each component z ? {1, . . . ,K}:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point]
The model has K components whose prior dis-
tribution is specified by ? = (?1, . . . , ?K). The
Dirichlet hyperparameter ? controls how uniform
this distribution is: as ? increases, it becomes in-
creasingly likely that the components have equal
probability. For each mixture component z ?
{1, . . . ,K}, the parameters of the component ?z are
drawn from some prior G0. Given the model param-
eters (?,?), the data points are generated i.i.d. by
first choosing a component and then generating from
a data model F parameterized by that component.
In document clustering, for example, each data
point xi is a document represented by its term-
frequency vector. Each component (cluster) z
has multinomial parameters ?z which specifies a
distribution F (?;?z) over words. It is custom-
ary to use a conjugate Dirichlet prior G0 =
Dirichlet(??, . . . , ??) over the multinomial parame-
ters, which can be interpreted as adding ???1 pseu-
docounts for each word.
2.2 DP mixture model
We now consider the extension of the Bayesian finite
mixture model to a nonparametric Bayesian mixture
model based on the Dirichlet process. We focus
on the stick-breaking representation (Sethuraman,
1994) of the Dirichlet process instead of the stochas-
tic process definition (Ferguson, 1973) or the Chi-
nese restaurant process (Pitman, 2002). The stick-
breaking representation captures the DP prior most
explicitly and allows us to extend the finite mixture
model with minimal changes. Later, it will enable us
to readily define structured models in a form similar
to their classical versions. Furthermore, an efficient
variational inference algorithm can be developed in
this representation (Section 2.6).
The key difference between the Bayesian finite
mixture model and the DP mixture model is that
the latter has a countably infinite number of mixture
components while the former has a predefined K.
Note that if we have an infinite number of mixture
components, it no longer makes sense to consider
a symmetric prior over the component probabilities;
the prior over component probabilities must decay in
some way. The stick-breaking distribution achieves
this as follows. We write ? ? GEM(?) to mean
that ? = (?1, ?2, . . . ) is distributed according to the
stick-breaking distribution. Here, the concentration
parameter ? controls the number of effective com-
ponents. To draw ? ? GEM(?), we first generate
a countably infinite collection of stick-breaking pro-
portions u1, u2, . . . , where each uz ? Beta(1, ?).
The stick-breaking weights ? are then defined in
terms of the stick proportions:
?z = uz
?
z?<z
(1 ? uz?). (1)
The procedure for generating ? can be viewed as
iteratively breaking off remaining portions of a unit-
689
0 1?1 ?2 ?3 ...
Figure 1: A sample ? ? GEM(1).
length stick (Figure 1). The component probabilities
{?z} will decay exponentially in expectation, but
there is always some probability of getting a smaller
component before a larger one. The parameter ? de-
termines the decay of these probabilities: a larger ?
implies a slower decay and thus more components.
Given the component probabilities, the rest of the
DP mixture model is identical to the finite mixture
model:
DP mixture model
? ? GEM(?) [draw component probabilities]
For each component z ? {1, 2, . . . }:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point xn]
2.3 HDP-HMM
The next stop on the way to the HDP-PCFG is the
HDP hidden Markov model (HDP-HMM) (Beal et
al., 2002; Teh et al, 2006). An HMM consists of a
set of hidden states, where each state can be thought
of as a mixture component. The parameters of the
mixture component are the emission and transition
parameters. The main aspect that distinguishes it
from a flat finite mixture model is that the transi-
tion parameters themselves must specify a distribu-
tion over next states. Hence, we have not just one
top-level mixture model over states, but also a col-
lection of mixture models, one for each state.
In developing a nonparametric version of the
HMM in which the number of states is infinite, we
need to ensure that the transition mixture models
of each state share a common inventory of possible
next states. We can achieve this by tying these mix-
ture models together using the hierarchical Dirichlet
process (HDP) (Teh et al, 2006). The stick-breaking
representation of an HDP is defined as follows: first,
the top-level stick-breaking weights ? are drawn ac-
cording to the stick-breaking prior as before. Then,
a new set of stick-breaking weights ?? are generated
according based on ?:
?? ? DP(??,?), (2)
where the distribution of DP can be characterized
in terms of the following finite partition property:
for all partitions of the positive integers into sets
A1, . . . , Am,
(??(A1), . . . ,??(Am)) (3)
? Dirichlet
(
???(A1), . . . , ???(Am)
)
,
where ?(A) =
?
k?A ?k.
1 The resulting ?? is an-
other distribution over the positive integers whose
similarity to ? is controlled by a concentration pa-
rameter ??.
HDP-HMM
? ? GEM(?) [draw top-level state weights]
For each state z ? {1, 2, . . . }:
??Ez ? Dirichlet(?) [draw emission parameters]
??Tz ? DP(?
?, ?) [draw transition parameters]
For each time step i ? {1, . . . , n}:
?xi ? F (?;?Ezi) [emit current observation]
?zi+1 ? Multinomial(?Tzi) [choose next state]
Each state z is associated with emission param-
eters ?Ez . In addition, each z is also associated
with transition parameters ?Tz , which specify a dis-
tribution over next states. These transition parame-
ters are drawn from a DP centered on the top-level
stick-breaking weights ? according to Equations (2)
and (3). Assume that z1 is always fixed to a special
START state, so we do not need to generate it.
2.4 HDP-PCFG
We now present the HDP-PCFG, which is the focus
of this paper. For simplicity, we consider Chomsky
normal form (CNF) grammars, which has two types
of rules: emissions and binary productions. We con-
sider each grammar symbol as a mixture component
whose parameters are the rule probabilities for that
symbol. In general, we do not know the appropriate
number of grammar symbols, so our strategy is to
let the number of grammar symbols be infinite and
place a DP prior over grammar symbols.
1Note that this property is a specific instance of the general
stochastic process definition of Dirichlet processes.
690
HDP-PCFG
? ? GEM(?) [draw top-level symbol weights]
For each grammar symbol z ? {1, 2, . . . }:
??Tz ? Dirichlet(?
T ) [draw rule type parameters]
??Ez ? Dirichlet(?
E) [draw emission parameters]
??Bz ? DP(?
B ,??T ) [draw binary production parameters]
For each node i in the parse tree:
?ti ? Multinomial(?Tzi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Ezi) [emit terminal symbol]
?If ti = BINARY-PRODUCTION:
??(zL(i), zR(i)) ? Multinomial(?
B
zi) [generate children symbols]
?
?Bz
?Tz
?Ez
z ?
z1
z2
x2
z3
x3
T
Parameters Trees
Figure 2: The definition and graphical model of the HDP-PCFG. Since parse trees have unknown structure,
there is no convenient way of representing them in the visual language of traditional graphical models.
Instead, we show a simple fixed example tree. Node 1 has two children, 2 and 3, each of which has one
observed terminal child. We use L(i) and R(i) to denote the left and right children of node i.
In the HMM, the transition parameters of a state
specify a distribution over single next states; simi-
larly, the binary production parameters of a gram-
mar symbol must specify a distribution over pairs
of grammar symbols for its children. We adapt the
HDP machinery to tie these binary production distri-
butions together. The key difference is that now we
must tie distributions over pairs of grammar sym-
bols together via distributions over single grammar
symbols.
Another difference is that in the HMM, at each
time step, both a transition and a emission are made,
whereas in the PCFG either a binary production or
an emission is chosen. Therefore, each grammar
symbol must also have a distribution over the type
of rule to apply. In a CNF PCFG, there are only
two types of rules, but this can be easily generalized
to include unary productions, which we use for our
parsing experiments.
To summarize, the parameters of each grammar
symbol z consists of (1) a distribution over a finite
number of rule types ?Tz , (2) an emission distribu-
tion ?Ez over terminal symbols, and (3) a binary pro-
duction distribution ?Bz over pairs of children gram-
mar symbols. Figure 2 describes the model in detail.
Figure 3 shows the generation of the binary pro-
duction distributions ?Bz . We draw ?
B
z from a DP
centered on ??T , which is the product distribution
over pairs of symbols. The result is a doubly-infinite
matrix where most of the probability mass is con-
state
right child state
left child state
right child state
left child state
? ? GEM(?)
??T
?Bz ? DP(??
T )
Figure 3: The generation of binary production prob-
abilities given the top-level symbol probabilities ?.
First, ? is drawn from the stick-breaking prior, as
in any DP-based model (a). Next, the outer-product
??T is formed, resulting in a doubly-infinite matrix
matrix (b). We use this as the base distribution for
generating the binary production distribution from a
DP centered on ??T (c).
centrated in the upper left, just like the top-level dis-
tribution ??T .
Note that we have replaced the general
691
G0 and F (?Ezi) pair with Dirichlet(?
E) and
Multinomial(?Ezi) to specialize to natural language,
but there is no difficulty in working with parse
trees with arbitrary non-multinomial observations
or more sophisticated word models.
In many natural language applications, there is
a hard distinction between pre-terminal symbols
(those that only emit a word) and non-terminal sym-
bols (those that only rewrite as two non-terminal or
pre-terminal symbols). This can be accomplished
by letting ?T = (0, 0), which forces a draw ?Tz to
assign probability 1 to one rule type.
An alternative definition of an HDP-PCFG would
be as follows: for each symbol z, draw a distribution
over left child symbols lz ? DP(?) and an inde-
pendent distribution over right child symbols rz ?
DP(?). Then define the binary production distribu-
tion as their cross-product ?Bz = lzr
T
z . This also
yields a distribution over symbol pairs and hence de-
fines a different type of nonparametric PCFG. This
model is simpler and does not require any additional
machinery beyond the HDP-HMM. However, the
modeling assumptions imposed by this alternative
are unappealing as they assume the left child and
right child are independent given the parent, which
is certainly not the case in natural language.
2.5 HDP-PCFG for grammar refinement
An important motivation for the HDP-PCFG is that
of refining an existing treebank grammar to alle-
viate unrealistic independence assumptions and to
improve parsing accuracy. In this scenario, the set
of symbols is known, but we do not know how
many subsymbols to allocate per symbol. We in-
troduce the HDP-PCFG for grammar refinement
(HDP-PCFG-GR), an extension of the HDP-PCFG,
for this task.
The essential difference is that now we have a
collection of HDP-PCFG models for each symbol
s ? S, each one operating at the subsymbol level.
While these HDP-PCFGs are independent in the
prior, they are coupled through their interactions in
the parse trees. For completeness, we have also in-
cluded unary productions, which are essentially the
PCFG counterpart of transitions in HMMs. Finally,
since each node i in the parse tree involves a symbol-
subsymbol pair (si, zi), each subsymbol needs to
specify a distribution over both child symbols and
subsymbols. The former can be handled through
a finite Dirichlet distribution since all symbols are
known and observed, but the latter must be handled
with the Dirichlet process machinery, since the num-
ber of subsymbols is unknown.
HDP-PCFG for grammar refinement (HDP-PCFG-GR)
For each symbol s ? S:
??s ? GEM(?) [draw subsymbol weights]
?For each subsymbol z ? {1, 2, . . . }:
???Tsz ? Dirichlet(?
T ) [draw rule type parameters]
???Esz ? Dirichlet(?
E(s)) [draw emission parameters]
???usz ? Dirichlet(?
u) [unary symbol productions]
???bsz ? Dirichlet(?
b) [binary symbol productions]
??For each child symbol s? ? S:
????Uszs? ? DP(?
U ,?s?) [unary subsymbol prod.]
??For each pair of children symbols (s?, s??) ? S ? S:
????Bszs?s?? ? DP(?
B ,?s??
T
s??) [binary subsymbol]
For each node i in the parse tree:
?ti ? Multinomial(?Tsizi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Esizi) [emit terminal symbol]
?If ti = UNARY-PRODUCTION:
??sL(i) ? Multinomial(?
u
sizi) [generate child symbol]
??zL(i) ? Multinomial(?
U
sizisL(i)) [child subsymbol]
?If ti = BINARY-PRODUCTION:
??(sL(i), sR(i)) ? Mult(?sizi) [children symbols]
??(zL(i), zR(i)) ? Mult(?
B
sizisL(i)sR(i)) [subsymbols]
2.6 Variational inference
We present an inference algorithm for the HDP-
PCFG model described in Section 2.4, which can
also be adapted to the HDP-PCFG-GR model with
a bit more bookkeeping. Most previous inference
algorithms for DP-based models involve sampling
(Escobar and West, 1995; Teh et al, 2006). How-
ever, we chose to use variational inference (Blei
and Jordan, 2005), which provides a fast determin-
istic alternative to sampling, hence avoiding issues
of diagnosing convergence and aggregating samples.
Furthermore, our variational inference algorithm es-
tablishes a strong link with past work on PCFG re-
finement and induction, which has traditionally em-
ployed the EM algorithm.
In EM, the E-step involves a dynamic program
that exploits the Markov structure of the parse tree,
and the M-step involves computing ratios based on
expected counts extracted from the E-step. Our vari-
ational algorithm resembles the EM algorithm in
form, but the ratios in the M-step are replaced with
weights that reflect the uncertainty in parameter es-
692
??Bz
?Tz
?Ez
z ?
z1
z2 z3
T
Parameters Trees
Figure 4: We approximate the true posterior p over
parameters ? and latent parse trees z using a struc-
tured mean-field distribution q, in which the distri-
bution over parameters are completely factorized but
the distribution over parse trees is unconstrained.
timates. Because of this procedural similarity, our
method is able to exploit the desirable properties of
EM such as simplicity, modularity, and efficiency.
2.7 Structured mean-field approximation
We denote parameters of the HDP-PCFG as ? =
(?,?), where ? denotes the top-level symbol prob-
abilities and ? denotes the rule probabilities. The
hidden variables of the model are the training parse
trees z. We denote the observed sentences as x.
The goal of Bayesian inference is to compute the
posterior distribution p(?, z | x). The central idea
behind variational inference is to approximate this
intractable posterior with a tractable approximation.
In particular, we want to find the best distribution q?
as defined by
q?
def
= argmin
q?Q
KL(q(?, z)||p(?, z | x)), (4)
where Q is a tractable subset of distributions. We
use a structured mean-field approximation, meaning
that we only consider distributions that factorize as
follows (Figure 4):
Q
def
=
{
q(z)q(?)
K?
z=1
q(?Tz )q(?
E
z )q(?
B
z )
}
. (5)
We further restrict q(?Tz ), q(?
E
z ), q(?
B
z ) to be
Dirichlet distributions, but allow q(z) to be any
multinomial distribution. We constrain q(?) to be a
degenerate distribution truncated at K; i.e., ?z = 0
for z > K. While the posterior grammar does have
an infinite number of symbols, the exponential de-
cay of the DP prior ensures that most of the proba-
bility mass is contained in the first few symbols (Ish-
waran and James, 2001).2 While our variational ap-
proximation q is truncated, the actual PCFG model
is not. AsK increases, our approximation improves.
2.8 Coordinate-wise ascent
The optimization problem defined by Equation (4)
is intractable and nonconvex, but we can use a sim-
ple coordinate-ascent algorithm that iteratively op-
timizes each factor of q in turn while holding the
others fixed. The algorithm turns out to be similar in
form to EM for an ordinary PCFG: optimizing q(z)
is the analogue of the E-step, and optimizing q(?)
is the analogue of the M-step; however, optimizing
q(?) has no analogue in EM. We summarize each
of these updates below (see (Liang et al, 2007) for
complete derivations).
Parse trees q(z): The distribution over parse trees
q(z) can be summarized by the expected suffi-
cient statistics (rule counts), which we denote as
C(z ? zl zr) for binary productions and C(z ?
x) for emissions. We can compute these expected
counts using dynamic programming as in the E-step
of EM.
While the classical E-step uses the current rule
probabilities ?, our mean-field approximation in-
volves an entire distribution q(?). Fortunately, we
can still handle this case by replacing each rule prob-
ability with a weight that summarizes the uncer-
tainty over the rule probability as represented by q.
We define this weight in the sequel.
It is a common perception that Bayesian inference
is slow because one needs to compute integrals. Our
mean-field inference algorithm is a counterexample:
because we can represent uncertainty over rule prob-
abilities with single numbers, much of the existing
PCFG machinery based on EM can be modularly
imported into the Bayesian framework.
Rule probabilities q(?): For an ordinary PCFG,
the M-step simply involves taking ratios of expected
2In particular, the variational distance between the stick-
breaking distribution and the truncated version decreases expo-
nentially as the truncation level K increases.
693
counts:
?Bz (zl, zr) =
C(z ? zl zr)
C(z ? ??)
. (6)
For the variational HDP-PCFG, the optimal q(?) is
given by the standard posterior update for Dirichlet
distributions:3
q(?Bz ) = Dirichlet(?
B
z ;?
B??T + ~C(z)), (7)
where ~C(z) is the matrix of counts of rules with left-
hand side z. These distributions can then be summa-
rized with multinomial weights which are the only
necessary quantities for updating q(z) in the next it-
eration:
WBz (zl, zr)
def
= expEq[log?Bz (zl, zr)] (8)
=
e?(C(z?zl zr)+?
B?zl?zr )
e?(C(z???)+?B)
, (9)
where ?(?) is the digamma function. The emission
parameters can be defined similarly. Inspection of
Equations (6) and (9) reveals that the only difference
between the maximum likelihood and the mean-field
update is that the latter applies the exp(?(?)) func-
tion to the counts (Figure 5).
When the truncation K is large, ?B?zl?zr is near
0 for most right-hand sides (zl, zr), so exp(?(?)) has
the effect of downweighting counts. Since this sub-
traction affects large counts more than small counts,
there is a rich-get-richer effect: rules that have al-
ready have large counts will be preferred.
Specifically, consider a set of rules with the same
left-hand side. The weights for all these rules only
differ in the numerator (Equation (9)), so applying
exp(?(?)) creates a local preference for right-hand
sides with larger counts. Also note that the rule
weights are not normalized; they always sum to at
most one and are equal to one exactly when q(?) is
degenerate. This lack of normalization gives an ex-
tra degree of freedom not present in maximum like-
lihood estimation: it creates a global preference for
left-hand sides that have larger total counts.
Top-level symbol probabilities q(?): Recall that
we restrict q(?) = ???(?), so optimizing ? is
equivalent to finding a single best ??. Unlike q(?)
3Because we have truncated the top-level symbol weights,
the DP prior on ?Bz reduces to a finite Dirichlet distribution.
 
0
 
0.5 1
 
1.5 2  0
 
0.5
 
1
 
1.5
 
2
x
exp(?(x
)) x
Figure 5: The exp(?(?)) function, which is used in
computing the multinomial weights for mean-field
inference. It has the effect of reducing a larger frac-
tion of small counts than large counts.
and q(z), there is no closed form expression for
the optimal ??, and the objective function (Equa-
tion (4)) is not convex in ??. Nonetheless, we can
apply a standard gradient projection method (Bert-
sekas, 1999) to improve ?? to a local maxima.
The part of the objective function in Equation (4)
that depends on ?? is as follows:
L(??) = logGEM(??;?)+ (10)
K?
z=1
Eq[logDirichlet(?Bz ;?
B????T )]
See Liang et al (2007) for the derivation of the gra-
dient. In practice, this optimization has very little ef-
fect on performance. We suspect that this is because
the objective function is dominated by p(x | z) and
p(z | ?), while the contribution of p(? | ?) is mi-
nor.
3 Experiments
We now present an empirical evaluation of the HDP-
PCFG(-GR) model and variational inference tech-
niques. We first give an illustrative example of the
ability of the HDP-PCFG to recover a known gram-
mar and then present the results of experiments on
large-scale treebank parsing.
3.1 Recovering a synthetic grammar
In this section, we show that the HDP-PCFG-GR
can recover a simple grammar while a standard
694
S ? X1X1 | X2X2 | X3X3 | X4X4
X1 ? a1 | b1 | c1 | d1
X2 ? a2 | b2 | c2 | d2
X3 ? a3 | b3 | c3 | d3
X4 ? a4 | b4 | c4 | d4
S
Xi Xi
{ai, bi, ci, di} {ai, bi, ci, di}
(a) (b)
Figure 6: (a) A synthetic grammar with a uniform
distribution over rules. (b) The grammar generates
trees of the form shown on the right.
PCFG fails to do so because it has no built-in con-
trol over grammar complexity. From the grammar in
Figure 6, we generated 2000 trees. The two terminal
symbols always have the same subscript, but we col-
lapsed Xi to X in the training data. We trained the
HDP-PCFG-GR, with truncation K = 20, for both
S and X for 100 iterations. We set al hyperparame-
ters to 1.
Figure 7 shows that the HDP-PCFG-GR recovers
the original grammar, which contains only 4 sub-
symbols, leaving the other 16 subsymbols unused.
The standard PCFG allocates all the subsymbols to
fit the exact co-occurrence statistics of left and right
terminals.
Recall that a rule weight, as defined in Equa-
tion (9), is analogous to a rule probability for stan-
dard PCFGs. We say a rule is effective if its weight
is at least 10?6 and its left hand-side has posterior
is also at least 10?6. In general, rules with weight
smaller than 10?6 can be safely pruned without af-
fect parsing accuracy. The standard PCFG uses all
20 subsymbols of both S and X to explain the data,
resulting in 8320 effective rules; in contrast, the
HDP-PCFG uses only 4 subsymbols for X and 1 for
S, resulting in only 68 effective rules. If the thresh-
old is relaxed from 10?6 to 10?3, then only 20 rules
are effective, which corresponds exactly to the true
grammar.
3.2 Parsing the Penn Treebank
In this section, we show that our variational HDP-
PCFG can scale up to real-world data sets. We ran
experiments on the Wall Street Journal (WSJ) por-
tion of the Penn Treebank. We trained on sections
2?21, used section 24 for tuning hyperparameters,
and tested on section 22.
We binarize the trees in the treebank as follows:
for each non-terminal node with symbol X , we in-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
standard PCFG HDP-PCFG
Figure 7: The posteriors over the subsymbols of the
standard PCFG is roughly uniform, whereas the pos-
teriors of the HDP-PCFG is concentrated on four
subsymbols, which is the true number of symbols
in the grammar.
troduce a right-branching cascade of new nodes with
symbol X . The end result is that each node has at
most two children. To cope with unknown words,
we replace any word appearing fewer than 5 times
in the training set with one of 50 unknown word to-
kens derived from 10 word-form features.
Our goal is to learn a refined grammar, where each
symbol in the training set is split into K subsym-
bols. We compare an ordinary PCFG estimated with
maximum likelihood (Matsuzaki et al, 2005) and
the HDP-PCFG estimated using the variational in-
ference algorithm described in Section 2.6.
To parse new sentences with a grammar, we com-
pute the posterior distribution over rules at each span
and extract the tree with the maximum expected cor-
rect number of rules (Petrov and Klein, 2007).
3.2.1 Hyperparameters
There are six hyperparameters in the HDP-PCFG-
GR model, which we set in the following manner:
? = 1, ?T = 1 (uniform distribution over unar-
ies versus binaries), ?E = 1 (uniform distribution
over terminal words), ?u(s) = ?b(s) = 1N(s) , where
N(s) is the number of different unary (binary) right-
hand sides of rules with left-hand side s in the tree-
bank grammar. The two most important hyperpa-
rameters are ?U and ?B , which govern the sparsity
of the right-hand side for unary and binary rules.
We set ?U = ?B although more performance could
probably be gained by tuning these individually. It
turns out that there is not a single ?B that works for
all truncation levels, as shown in Table 1.
If the top-level distribution ? is uniform, the value
of ?B corresponding to a uniform prior over pairs of
children subsymbols is K2. Interestingly, the opti-
mal ?B appears to be superlinear but subquadratic
695
truncation K 2 4 8 12 16 20
best ?B 16 12 20 28 48 80
uniform ?B 4 16 64 144 256 400
Table 1: For each truncation level, we report the ?B
that yielded the highest F1 score on the development
set.
K PCFG PCFG (smoothed) HDP-PCFG
F1 Size F1 Size F1 Size
1 60.47 2558 60.36 2597 60.5 2557
2 69.53 3788 69.38 4614 71.08 4264
4 75.98 3141 77.11 12436 77.17 9710
8 74.32 4262 79.26 120598 79.15 50629
12 70.99 7297 78.8 160403 78.94 86386
16 66.99 19616 79.2 261444 78.24 131377
20 64.44 27593 79.27 369699 77.81 202767
Table 2: Shows development F1 and grammar sizes
(the number of effective rules) as we increase the
truncation K.
in K. We used these values of ?B in the following
experiments.
3.2.2 Results
The regime in which Bayesian inference is most
important is when training data is scarce relative to
the complexity of the model. We train on just sec-
tion 2 of the Penn Treebank. Table 2 shows how
the HDP-PCFG-GR can produce compact grammars
that guard against overfitting. Without smoothing,
ordinary PCFGs trained using EM improve as K in-
creases but start to overfit around K = 4. Simple
add-1.01 smoothing prevents overfitting but at the
cost of a sharp increase in grammar sizes. The HDP-
PCFG obtains comparable performance with a much
smaller number of rules.
We also trained on sections 2?21 to demon-
strate that our methods can scale up and achieve
broadly comparable results to existing state-of-the-
art parsers. When using a truncation level of K =
16, the standard PCFG with smoothing obtains an
F1 score of 88.36 using 706157 effective rules while
the HDP-PCFG-GR obtains an F1 score of 87.08 us-
ing 428375 effective rules. We expect to see greater
benefits from the HDP-PCFG with a larger trunca-
tion level.
4 Related work
The question of how to select the appropriate gram-
mar complexity has been studied in earlier work.
It is well known that more complex models nec-
essarily have higher likelihood and thus a penalty
must be imposed for more complex grammars. Ex-
amples of such penalized likelihood procedures in-
clude Stolcke and Omohundro (1994), which used
an asymptotic Bayesian model selection criterion
and Petrov et al (2006), which used a split-merge
algorithm which procedurally determines when to
switch between grammars of various complexities.
These techniques are model selection techniques
that use heuristics to choose among competing sta-
tistical models; in contrast, the HDP-PCFG relies on
the Bayesian formalism to provide implicit control
over model complexity within the framework of a
single probabilistic model.
Johnson et al (2006) also explored nonparamet-
ric grammars, but they do not give an inference al-
gorithm for recursive grammars, e.g., grammars in-
cluding rules of the form A ? BC and B ? DA.
Recursion is a crucial aspect of PCFGs and our
inference algorithm does handle it. Finkel et al
(2007) independently developed another nonpara-
metric model of grammars. Though their model is
also based on hierarchical Dirichlet processes and is
similar to ours, they present a different inference al-
gorithm which is based on sampling. Kurihara and
Sato (2004) and Kurihara and Sato (2006) applied
variational inference to PCFGs. Their algorithm is
similar to ours, but they did not consider nonpara-
metric models.
5 Conclusion
We have presented the HDP-PCFG, a nonparametric
Bayesian model for PCFGs, along with an efficient
variational inference algorithm. While our primary
contribution is the elucidation of the model and algo-
rithm, we have also explored some important empir-
ical properties of the HDP-PCFG and also demon-
strated the potential of variational HDP-PCFGs on a
full-scale parsing task.
696
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics, 2:1152?1174.
M. Beal, Z. Ghahramani, and C. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural
Information Processing Systems (NIPS), pages 577?
584.
D. Bertsekas. 1999. Nonlinear programming.
D. Blei and M. I. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Bayesian Analysis, 1:121?
144.
E. Charniak. 1996. Tree-bank grammars. In Association
for the Advancement of Artificial Intelligence (AAAI).
E. Charniak. 2000. A maximum-entropy-inspired parser.
In North American Association for Computational
Linguistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. D. Escobar and M. West. 1995. Bayesian density
estimation and inference using mixtures. Journal of
the American Statistical Association, 90:577?588.
T. S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Association for Computational Lin-
guistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
H. Ishwaran and L. F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96:161?173.
M. Johnson, T. Griffiths, and S. Goldwater. 2006. Adap-
tor grammars: A framework for specifying composi-
tional nonparametric Bayesian models. In Advances
in Neural Information Processing Systems (NIPS).
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Association for Computational Linguistics
(ACL), pages 423?430.
K. Kurihara and T. Sato. 2004. An application of the
variational Bayesian approach to probabilistic context-
free grammars. In International Joint Conference on
Natural Language Processing Workshop Beyond Shal-
low Analyses.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007. Nonparametric PCFGs using Dirichlet pro-
cesses. Technical report, Department of Statistics,
University of California at Berkeley.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Association for
Computational Linguistics (ACL).
S. Petrov and D. Klein. 2007. Learning and inference
for hierarchically split PCFGs. In Human Language
Technology and North American Association for Com-
putational Linguistics (HLT/NAACL).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL).
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications.
Y. W. Teh, M. I. Jordan, M. Beal, and D. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
697
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 112?119,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Alignment via Quadratic Assignment
Simon Lacoste-Julien
UC Berkeley, Berkeley, CA 94720
slacoste@cs.berkeley.edu
Ben Taskar
UC Berkeley, Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley, Berkeley, CA 94720
klein@cs.berkeley.edu
Michael I. Jordan
UC Berkeley, Berkeley, CA 94720
jordan@cs.berkeley.edu
Abstract
Recently, discriminative word alignment methods
have achieved state-of-the-art accuracies by extend-
ing the range of information sources that can be
easily incorporated into aligners. The chief advan-
tage of a discriminative framework is the ability
to score alignments based on arbitrary features of
the matching word tokens, including orthographic
form, predictions of other models, lexical context
and so on. However, the proposed bipartite match-
ing model of Taskar et al (2005), despite being
tractable and effective, has two important limita-
tions. First, it is limited by the restriction that
words have fertility of at most one. More impor-
tantly, first order correlations between consecutive
words cannot be directly captured by the model. In
this work, we address these limitations by enrich-
ing the model form. We give estimation and infer-
ence algorithms for these enhancements. Our best
model achieves a relative AER reduction of 25%
over the basic matching formulation, outperform-
ing intersected IBM Model 4 without using any
overly compute-intensive features. By including
predictions of other models as features, we achieve
AER of 3.8 on the standard Hansards dataset.
1 Introduction
Word alignment is a key component of most end-
to-end statistical machine translation systems. The
standard approach to word alignment is to construct
directional generative models (Brown et al, 1990;
Och and Ney, 2003), which produce a sentence in
one language given the sentence in another lan-
guage. While these models require sentence-aligned
bitexts, they can be trained with no further super-
vision, using EM. Generative alignment models do,
however, have serious drawbacks. First, they require
extensive tuning and processing of large amounts
of data which, for the better-performing models, is
a non-trivial resource requirement. Second, condi-
tioning on arbitrary features of the input is difficult;
for example, we would like to condition on the or-
thographic similarity of a word pair (for detecting
cognates), the presence of that pair in various dic-
tionaries, the similarity of the frequency of its two
words, choices made by other alignment systems,
and so on.
Recently, Moore (2005) proposed a discrimina-
tive model in which pairs of sentences (e, f) and
proposed alignments a are scored using a linear
combination of arbitrary features computed from the
tuples (a, e, f). While there are no restrictions on
the form of the model features, the problem of find-
ing the highest scoring alignment is very difficult
and involves heuristic search. Moreover, the param-
eters of the model must be estimated using averaged
perceptron training (Collins, 2002), which can be
unstable. In contrast, Taskar et al (2005) cast word
alignment as a maximum weighted matching prob-
lem, in which each pair of words (ej , fk) in a sen-
tence pair (e, f) is associated with a score sjk(e, f)
reflecting the desirability of the alignment of that
pair. Importantly, this problem is computationally
tractable. The alignment for the sentence pair is the
highest scoring matching under constraints (such as
the constraint that matchings be one-to-one). The
scoring model sjk(e, f) can be based on a rich fea-
ture set defined on word pairs (ej , fk) and their con-
text, including measures of association, orthogra-
phy, relative position, predictions of generative mod-
els, etc. The parameters of the model are estimated
within the framework of large-margin estimation; in
particular, the problem turns out to reduce to the
112
solution of a (relatively) small quadratic program
(QP). The authors show that large-margin estimation
is both more stable and more accurate than percep-
tron training.
While the bipartite matching approach is a use-
ful first step in the direction of discriminative word
alignment, for discriminative approaches to com-
pete with and eventually surpass the most sophisti-
cated generative models, it is necessary to consider
more realistic underlying statistical models. Note in
particular two substantial limitations of the bipartite
matching model of Taskar et al (2005): words have
fertility of at most one, and there is no way to incor-
porate pairwise interactions among alignment deci-
sions. Moving beyond these limitations?while re-
taining computational tractability?is the next major
challenge for discriminative word alignment.
In this paper, we show how to overcome both lim-
itations. First, we introduce a parameterized model
that penalizes different levels of fertility. While this
extension adds very useful expressive power to the
model, it turns out not to increase the computa-
tional complexity of the aligner, for either the pre-
diction or the parameter estimation problem. Sec-
ond, we introduce a more thoroughgoing extension
which incorporates first-order interactions between
alignments of consecutive words into the model. We
do this by formulating the alignment problem as a
quadratic assignment problem (QAP), where in ad-
dition to scoring individual edges, we also define
scores of pairs of edges that connect consecutive
words in an alignment. The predicted alignment is
the highest scoring quadratic assignment.
QAP is an NP-hard problem, but in the range of
problem sizes that we need to tackle the problem can
be solved efficiently. In particular, using standard
off-the-shelf integer program solvers, we are able to
solve the QAP problems in our experiments in under
a second. Moreover, the parameter estimation prob-
lem can also be solved efficiently by making use of
a linear relaxation of QAP for the min-max formu-
lation of large-margin estimation (Taskar, 2004).
We show that these two extensions yield signif-
icant improvements in error rates when compared
to the bipartite matching model. The addition of a
fertility model improves the AER by 0.4. Model-
ing first-order interactions improves the AER by 1.8.
Combining the two extensions results in an improve-
ment in AER of 2.3, yielding alignments of better
quality than intersected IBM Model 4. Moreover,
including predictions of bi-directional IBM Model
4 and model of Liang et al (2006) as features, we
achieve an absolute AER of 3.8 on the English-
French Hansards alignment task?the best AER re-
sult published on this task to date.
2 Models
We begin with a quick summary of the maximum
weight bipartite matching model in (Taskar et al,
2005). More precisely, nodes V = Vs ? V t cor-
respond to words in the ?source? (Vs) and ?tar-
get? (V t) sentences, and edges E = {jk : j ?
Vs, k ? V t} correspond to alignments between word
pairs.1 The edge weights sjk represent the degree
to which word j in one sentence can be translated
using the word k in the other sentence. The pre-
dicted alignment is chosen by maximizing the sum
of edge scores. A matching is represented using a
set of binary variables yjk that are set to 1 if word
j is assigned to word k in the other sentence, and 0
otherwise. The score of an assignment is the sum of
edge scores: s(y) = ?jk sjkyjk. For simplicity, let
us begin by assuming that each word aligns to one or
zero words in the other sentence; we revisit the issue
of fertility in the next section. The maximum weight
bipartite matching problem, arg maxy?Y s(y), can
be solved using combinatorial algorithms for min-
cost max-flow, expressed in a linear programming
(LP) formulation as follows:
max
0?z?1
?
jk?E
sjkzjk (1)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs,
where the continuous variables zjk are a relax-
ation of the corresponding binary-valued variables
yjk. This LP is guaranteed to have integral (and
hence optimal) solutions for any scoring function
s(y) (Schrijver, 2003). Note that although the above
LP can be used to compute alignments, combina-
torial algorithms are generally more efficient. For
1The source/target designation is arbitrary, as the models
considered below are all symmetric.
113
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
(a) (b)
Figure 2: An example fragment that requires fertility
greater than one to correctly label. (a) The guess of
the baseline M model. (b) The guess of the M+F
fertility-augmented model.
example, in Figure 1(a), we show a standard con-
struction for an equivalent min-cost flow problem.
However, we build on this LP to develop our exten-
sions to this model below. Representing the predic-
tion problem as an LP or an integer LP provides a
precise (and concise) way of specifying the model
and allows us to use the large-margin framework
of Taskar (2004) for parameter estimation described
in Section 3.
For a sentence pair x, we denote position pairs by
xjk and their scores as sjk. We let sjk = w>f(xjk)
for some user provided feature mapping f and ab-
breviate w>f(x,y) = ?jk yjkw>f(xjk). We can
include in the feature vector the identity of the two
words, their relative positions in their respective sen-
tences, their part-of-speech tags, their string similar-
ity (for detecting cognates), and so on.
2.1 Fertility
An important limitation of the model in Eq. (1) is
that in each sentence, a word can align to at most
one word in the translation. Although it is common
that words have gold fertility zero or one, it is cer-
tainly not always true. Consider, for example, the
bitext fragment shown in Figure 2(a), where back-
bone is aligned to the phrase e?pine dorsal. In this
figure, outlines are gold alignments, square for sure
alignments, round for possibles, and filled squares
are target algnments (for details on gold alignments,
see Section 4). When considering only the sure
alignments on the standard Hansards dataset, 7 per-
cent of the word occurrences have fertility 2, and 1
percent have fertility 3 and above; when considering
the possible alignments high fertility is much more
common?31 percent of the words have fertility 3
and above.
One simple fix to the original matching model is
to increase the right hand sides for the constraints
in Eq. (1) from 1 to D, where D is the maximum
allowed fertility. However, this change results in
an undesirable bimodal behavior, where maximum
weight solutions either have all words with fertil-
ity 0 or D, depending on whether most scores sjk
are positive or negative. For example, if scores tend
to be positive, most words will want to collect as
many alignments as they are permitted. What the
model is missing is a means for encouraging the
common case of low fertility (0 or 1), while allowing
higher fertility when it is licensed. This end can be
achieved by introducing a penalty for having higher
fertility, with the goal of allowing that penalty to
vary based on features of the word in question (such
as its frequency or identity).
In order to model such a penalty, we introduce
indicator variables zdj? (and zd?k) with the intended
meaning: node j has fertility of at least d (and node
k has fertility of at least d). In the following LP, we
introduce a penalty of
?
2?d?D sdj?zdj? for fertility
of node j, where each term sdj? ? 0 is the penalty
increment for increasing the fertility from d ? 1 to
d:
max
0?z?1
?
jk?E
sjkzjk (2)
?
?
j?Vs,2?d?D
sdj?zdj? ?
?
k?Vt,2?d?D
sd?kzd?k
s.t.
?
j?Vs
zjk ? 1 +
?
2?d?D
zd?k, ?k ? V t;
?
k?Vt
zjk ? 1 +
?
2?d?D
zdj?, ?j ? Vs.
We can show that this LP always has integral so-
lutions by a reduction to a min-cost flow problem.
The construction is shown in Figure 1(b). To ensure
that the new variables have the intended semantics,
we need to make sure that sdj? ? sd?j? if d ? d?,
so that the lower cost zdj? is used before the higher
cost zd?j? to increase fertility. This restriction im-
114
(a) (b) (c)
Figure 1: (a) Maximum weight bipartite matching as min-cost flow. Diamond-shaped nodes represent flow
source and sink. All edge capacities are 1, with edges between round nodes (j, k) have cost ?sjk, edges
from source and to sink have cost 0. (b) Expanded min-cost flow graph with new edges from source and to
sink that allow fertility of up to 3. The capacities of the new edges are 1 and the costs are 0 for solid edges
from source and to sink, s2j?, s2?k for dashed edges, and s3j?, s3?k for dotted edges. (c) Three types of pairs
of edges included in the QAP model, where the nodes on both sides correspond to consecutive words.
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
(a) (b)
Figure 3: An example fragment with a monotonic
gold alignment. (a) The guess of the baseline M
model. (b) The guess of the M+Q quadratic model.
plies that the penalty must be monotonic and convex
as a function of the fertility.
To anticipate the results that we report in Sec-
tion 4, adding fertility to the basic matching model
makes the target algnment of the backbone example
feasible and, in this case, the model correctly labels
this fragment as shown in Figure 2(b).
2.2 First-order interactions
An even more significant limitation of the model
in Eq. (1) is that the edges interact only indi-
rectly through the competition induced by the con-
straints. Generative alignment models like the
HMM model (Vogel et al, 1996) and IBM models 4
and above (Brown et al, 1990; Och and Ney, 2003)
directly model correlations between alignments of
consecutive words (at least on one side). For exam-
ple, Figure 3 shows a bitext fragment whose gold
alignment is strictly monotonic. This monotonicity
is quite common ? 46% of the words in the hand-
aligned data diagonally follow a previous alignment
in this way. We can model the common local align-
ment configurations by adding bonuses for pairs of
edges. For example, strictly monotonic alignments
can be encouraged by boosting the scores of edges
of the form ?(j, k), (j + 1, k + 1)?. Another trend,
common in English-French translation (7% on the
hand-aligned data), is the local inversion of nouns
and adjectives, which typically involves a pair of
edges ?(j, k + 1), (j + 1, k)?. Finally, a word in one
language is often translated as a phrase (consecutive
sequence of words) in the other language. This pat-
tern involves pairs of edges with the same origin on
one side: ?(j, k), (j, k+1)? or ?(j, k), (j+1, k)?. All
three of these edge pair patterns are shown in Fig-
ure 1(c). Note that the set of such edge pairs Q =
{jklm : |j ? l| ? 1, |k ? m| ? 1} is of linear size
in the number of edges.
Formally, we add to the model variables zjklm
which indicate whether both edge jk and lm are in
the alignment. We also add a corresponding score
sjklm, which we assume to be non-negative, since
the correlations we described are positive. (Nega-
tive scores can also be used, but the resulting for-
mulation we present below would be slightly differ-
ent.) To enforce the semantics zjklm = zjkzlm, we
use a pair of constraints zjklm ? zjk; zjklm ? zlm.
Since sjklm is positive, at the optimum, zjklm =
115
min(zjk, zlm). If in addition zjk, zlm are integral (0
or 1), then zjklm = zjkzlm. Hence, solving the fol-
lowing LP as an integer linear program will find the
optimal quadratic assignment for our model:
max
0?z?1
?
jk?E
sjkzjk +
?
jklm?Q
sjklmzjklm (3)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs;
zjklm ? zjk, zjklm ? zlm, ?jklm ? Q.
Note that we can also combine this extension with
the fertility extension described above.
To once again anticipate the results presented in
Section 4, the baseline model of Taskar et al (2005)
makes the prediction given in Figure 3(a) because
the two missing alignments are atypical translations
of common words. With the addition of edge pair
features, the overall monotonicity pushes the align-
ment to that of Figure 3(b).
3 Parameter estimation
To estimate the parameters of our model, we fol-
low the large-margin formulation of Taskar (2004).
Our input is a set of training instances {(xi,yi)}mi=1,
where each instance consists of a sentence pair xi
and a target algnment yi. We would like to find
parameters w that predict correct alignments on the
training data: yi = arg max
y?i?Yi
w>f(xi, y?i) for each i,
where Yi is the space of matchings for the sentence
pair xi.
In standard classification problems, we typically
measure the error of prediction, `(yi, y?i), using the
simple 0-1 loss. In structured problems, where we
are jointly predicting multiple variables, the loss is
often more complex. While the F-measure is a nat-
ural loss function for this task, we instead chose a
sensible surrogate that fits better in our framework:
weighted Hamming distance, which counts the num-
ber of variables in which a candidate solution y? dif-
fers from the target output y, with different penalty
for false positives (c+) and false negatives (c?):
`(y, y?) =
?
jk
[
c+(1 ? yjk)y?jk + c?(1 ? y?jk)yjk
]
.
We use an SVM-like hinge upper bound on
the loss `(yi, y?i), given by maxy?i?Yi [w>fi(y?i) +
`i(y?i) ? w>fi(yi)], where `i(y?i) = `(yi, y?i), and
fi(y?i) = f(xi, y?i). Minimizing this upper bound
encourages the true alignment yi to be optimal with
respect to w for each instance i:
min
||w||??
?
i
max
y?i?Yi
[w>fi(y?i) + `i(y?i)] ? w>fi(yi),
where ? is a regularization parameter.
In this form, the estimation problem is a mixture
of continuous optimization over w and combinato-
rial optimization over yi. In order to transform it
into a more standard optimization problem, we need
a way to efficiently handle the loss-augmented in-
ference, maxy?i?Yi [w>fi(y?i) + `i(y?i)]. This opti-
mization problem has precisely the same form as the
prediction problem whose parameters we are trying
to learn ? maxy?i?Yi w>fi(y?i) ? but with an addi-
tional term corresponding to the loss function. Our
assumption that the loss function decomposes over
the edges is crucial to solving this problem. We omit
the details here, but note that we can incorporate the
loss function into the LPs for various models we de-
scribed above and ?plug? them into the large-margin
formulation by converting the estimation problem
into a quadratic problem (QP) (Taskar, 2004). This
QP can be solved using any off-the-shelf solvers,
such as MOSEK or CPLEX.2 An important differ-
ence that comes into play for the estimation of the
quadratic assignment models in Equation (3) is that
inference involves solving an integer linear program,
not just an LP. In fact the LP is a relaxation of the in-
teger LP and provides an upper bound on the value
of the highest scoring assignment. Using the LP re-
laxation for the large-margin QP formulation is an
approximation, but as our experiments indicate, this
approximation is very effective. At testing time, we
use the integer LP to predict alignments. We have
also experimented with using just the LP relaxation
at testing time and then independently rounding each
fractional edge value, which actually incurs no loss
in alignment accuracy, as we discuss below.
2When training on 200 sentences, the QP we obtain contains
roughly 700K variables and 300K constraints and is solved in
roughly 10 minutes on a 2.8 GHz Pentium 4 machine. Aligning
the whole training set with the flow formulation takes a few
seconds, whereas using the integer programming (for the QAP
formulation) takes 1-2 minutes.
116
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
(a) (b) (c)
Figure 4: An example fragment with several multiple fertility sure alignments. (a) The guess of the M+Q
model with maximum fertility of one. (b) The guess of the M+Q+F quadratic model with fertility two
permitted. (c) The guess of the M+Q+F model with lexical fertility features.
4 Experiments
We applied our algorithms to word-level alignment
using the English-French Hansards data from the
2003 NAACL shared task (Mihalcea and Pedersen,
2003). This corpus consists of 1.1M automatically
aligned sentences, and comes with a validation set of
37 sentence pairs and a test set of 447 sentences. The
validation and test sentences have been hand-aligned
(see Och and Ney (2003)) and are marked with both
sure and possible alignments. Using these align-
ments, alignment error rate (AER) is calculated as:
(
1 ? |A ? S| + |A ? P ||A| + |S|
)
? 100%.
Here, A is a set of proposed index pairs, S is the
sure gold pairs, and P is the possible gold pairs.
For example, in Figure 4, proposed alignments are
shown against gold alignments, with open squares
for sure alignments, rounded open squares for possi-
ble alignments, and filled black squares for proposed
alignments.
The input to our algorithm is a small number of
labeled examples. In order to make our results more
comparable with Moore (2005), we split the origi-
nal set into 200 training examples and 247 test ex-
amples. We also trained on only the first 100 to
make our results more comparable with the exper-
iments of Och and Ney (2003), in which IBM model
4 was tuned using 100 sentences. In all our experi-
ments, we used a structured loss function that penal-
ized false negatives 10 times more than false posi-
tives, where the value of 10 was picked by using a
validation set. The regularization parameter ? was
also chosen using the validation set.
4.1 Features and results
We parameterized all scoring functions sjk, sdj?,
sd?k and sjklm as weighted linear combinations of
feature sets. The features were computed from
the large unlabeled corpus of 1.1M automatically
aligned sentences.
In the remainder of this section we describe the
improvements to the model performance as various
features are added. One of the most useful features
for the basic matching model is, of course, the set of
predictions of IBM model 4. However, computing
these features is very expensive and we would like to
build a competitive model that doesn?t require them.
Instead, we made significant use of IBM model 2 as
a source of features. This model, although not very
accurate as a predictive model, is simple and cheap
to construct and it is a useful source of features.
The Basic Matching Model: Edge Features In
the basic matching model of Taskar et al (2005),
called M here, one can only specify features on pairs
of word tokens, i.e. alignment edges. These features
117
include word association, orthography, proximity,
etc., and are documented in Taskar et al (2005). We
also augment those features with the predictions of
IBM Model 2 run on the training and test sentences.
We provided features for model 2 trained in each
direction, as well as the intersected predictions, on
each edge. By including the IBM Model 2 features,
the performance of the model described in Taskar et
al. (2005) on our test set (trained on 200 sentences)
improves from 10.0 AER to 8.2 AER, outperforming
unsymmetrized IBM Model 4 (but not intersected
model 4).
As an example of the kinds of errors the baseline
M system makes, see Figure 2 (where multiple fer-
tility cannot be predicted), Figure 3 (where a prefer-
ence for monotonicity cannot be modeled), and Fig-
ure 4 (which shows several multi-fertile cases).
The Fertility Model: Node Features To address
errors like those shown in Figure 2, we increased
the maximum fertility to two using the parameter-
ized fertility model of Section 2.1. The model learns
costs on the second flow arc for each word via fea-
tures not of edges but of single words. The score of
taking a second match for a word w was based on
the following features: a bias feature, the proportion
of times w?s type was aligned to two or more words
by IBM model 2, and the bucketed frequency of the
word type. This model was called M+F. We also in-
cluded a lexicalized feature for words which were
common in our training set: whether w was ever
seen in a multiple fertility alignment (more on this
feature later). This enabled the system to learn that
certain words, such as the English not and French
verbs like aurait commonly participate in multiple
fertility configurations.
Figure 5 show the results using the fertility exten-
sion. Adding fertility lowered AER from 8.5 to 8.1,
though fertility was even more effective in conjunc-
tion with the quadratic features below. The M+F set-
ting was even able to correctly learn some multiple
fertility instances which were not seen in the training
data, such as those shown in Figure 2.
The First-Order Model: Quadratic Features
With or without the fertility model, the model makes
mistakes such as those shown in Figure 3, where
atypical translations of common words are not cho-
sen despite their local support from adjacent edges.
In the quadratic model, we can associate features
with pairs of edges. We began with features which
identify each specific pattern, enabling trends of
monotonicity (or inversion) to be captured. We also
added to each edge pair the fraction of times that
pair?s pattern (monotonic, inverted, one to two) oc-
curred according each version of IBM model 2 (for-
ward, backward, intersected).
Figure 5 shows the results of adding the quadratic
model. M+Q reduces error over M from 8.5 to 6.7
(and fixes the errors shown in Figure 3). When both
the fertility and quadratic extensions were added,
AER dropped further, to 6.2. This final model is
even able to capture the diamond pattern in Figure 4;
the adjacent cycle of alignments is reinforced by the
quadratic features which boost adjacency. The ex-
ample in Figure 4 shows another interesting phe-
nomenon: the multi-fertile alignments for not and
de?pute? are learned even without lexical fertility fea-
tures (Figure 4b), because the Dice coefficients of
those words with their two alignees are both high.
However the surface association of aurait with have
is much higher than with would. If, however, lexi-
cal features are added, would is correctly aligned as
well (Figure 4c), since it is observed in similar pe-
riphrastic constructions in the training set.
We have avoided using expensive-to-compute fea-
tures like IBM model 4 predictions up to this point.
However, if these are available, our model can im-
prove further. By adding model 4 predictions to the
edge features, we get a relative AER reduction of
27%, from 6.5 to 4.5. By also including as features
the posteriors of the model of Liang et al (2006), we
achieve AER of 3.8, and 96.7/95.5 precision/recall.
It is comforting to note that in practice, the burden
of running an integer linear program at test time can
be avoided. We experimented with using just the LP
relaxation and found that on the test set, only about
20% of sentences have fractional solutions and only
0.2% of all edges are fractional. Simple rounding3
of each edge value in the LP solution achieves the
same AER as the integer LP solution, while using
about a third of the computation time on average.
3We slightly bias the system on the recall side by rounding
0.5 up, but this doesn?t yield a noticeable difference in the re-
sults.
118
Model Prec Rec AER
Generative
IBM 2 (E?F) 73.6 87.7 21.7
IBM 2 (F?E) 75.4 87.0 20.6
IBM 2 (intersected) 90.1 80.4 14.3
IBM 4 (E?F) 90.3 92.1 9.0
IBM 4 (F?E) 90.8 91.3 9.0
IBM 4 (intersected) 98.0 88.1 6.5
Discriminative (100 sentences)
Matching (M) 94.1 88.5 8.5
M + Fertility (F) 93.9 89.4 8.1
M + Quadratic (Q) 94.4 91.9 6.7
M + F + Q 94.8 92.5 6.2
M + F + Q + IBM4 96.4 94.4 4.5
Discriminative (200 sentences)
Matching (M) 93.4 89.7 8.2
M + Fertility (F) 93.6 90.1 8.0
M + Quadratic (Q) 95.0 91.1 6.8
M + F + Q 95.2 92.4 6.1
M + F + Q + IBM4 96.0 95.0 4.4
Figure 5: AER on the Hansards task.
5 Conclusion
We have shown that the discriminative approach to
word alignment can be extended to allow flexible
fertility modeling and to capture first-order inter-
actions between alignments of consecutive words.
These extensions significantly enhance the expres-
sive power of the discriminative approach; in partic-
ular, they make it possible to capture phenomena of
monotonicity, local inversion and contiguous fertil-
ity trends?phenomena that are highly informative
for alignment. They do so while remaining compu-
tationally efficient in practice both for prediction and
for parameter estimation.
Our best model achieves a relative AER reduc-
tion of 25% over the basic matching formulation,
beating intersected IBM Model 4 without the use
of any compute-intensive features. Including Model
4 predictions as features, we achieve a further rela-
tive AER reduction of 32% over intersected Model
4 alignments. By also including predictions of an-
other model, we drive AER down to 3.8. We are
currently investigating whether the improvement in
AER results in better translation BLEU score. Al-
lowing higher fertility and optimizing a recall bi-
ased cost function provide a significant increase in
recall relative to the intersected IBM model 4 (from
88.1% to 94.4%), with only a small degradation in
precision. We view this as a particularly promising
aspect of our work, given that phrase-based systems
such as Pharaoh (Koehn et al, 2003) perform better
with higher recall alignments.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In HLT-NAACL.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop, Building and Using parallel
Texts: Data Driven Machine Translation and Beyond,
pages 1?6, Edmonton, Alberta, Canada.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. HLT/EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
A. Schrijver. 2003. Combinatorial Optimization: Poly-
hedra and Efficiency. Springer.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
EMNLP.
B. Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In COLING
16, pages 836?841.
119
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 91?99,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning Semantic Correspondences with Less Supervision
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
A central problem in grounded language acqui-
sition is learning the correspondences between a
rich world state and a stream of text which refer-
ences that world state. To deal with the high de-
gree of ambiguity present in this setting, we present
a generative model that simultaneously segments
the text into utterances and maps each utterance
to a meaning representation grounded in the world
state. We show that our model generalizes across
three domains of increasing difficulty?Robocup
sportscasting, weather forecasts (a new domain),
and NFL recaps.
1 Introduction
Recent work in learning semantics has focused
on mapping sentences to meaning representa-
tions (e.g., some logical form) given aligned sen-
tence/meaning pairs as training data (Ge and
Mooney, 2005; Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007; Lu et al, 2008).
However, this degree of supervision is unrealistic
for modeling human language acquisition and can
be costly to obtain for building large-scale, broad-
coverage language understanding systems.
A more flexible direction is grounded language
acquisition: learning the meaning of sentences
in the context of an observed world state. The
grounded approach has gained interest in various
disciplines (Siskind, 1996; Yu and Ballard, 2004;
Feldman and Narayanan, 2004; Gorniak and Roy,
2007). Some recent work in the NLP commu-
nity has also moved in this direction by relaxing
the amount of supervision to the setting where
each sentence is paired with a small set of can-
didate meanings (Kate and Mooney, 2007; Chen
and Mooney, 2008).
The goal of this paper is to reduce the amount
of supervision even further. We assume that we are
given a world state represented by a set of records
along with a text, an unsegmented sequence of
words. For example, in the weather forecast do-
main (Section 2.2), the text is the weather report,
and the records provide a structured representation
of the temperature, sky conditions, etc.
In this less restricted data setting, we must re-
solve multiple ambiguities: (1) the segmentation
of the text into utterances; (2) the identification of
relevant facts, i.e., the choice of records and as-
pects of those records; and (3) the alignment of ut-
terances to facts (facts are the meaning represen-
tations of the utterances). Furthermore, in some
of our examples, much of the world state is not
referenced at all in the text, and, conversely, the
text references things which are not represented in
our world state. This increased amount of ambigu-
ity and noise presents serious challenges for learn-
ing. To cope with these challenges, we propose a
probabilistic generative model that treats text seg-
mentation, fact identification, and alignment in a
single unified framework. The parameters of this
hierarchical hidden semi-Markov model can be es-
timated efficiently using EM.
We tested our model on the task of aligning
text to records in three different domains. The
first domain is Robocup sportscasting (Chen and
Mooney, 2008). Their best approach (KRISPER)
obtains 67% F1; our method achieves 76.5%. This
domain is simplified in that the segmentation is
known. The second domain is weather forecasts,
for which we created a new dataset. Here, the
full complexity of joint segmentation and align-
ment arises. Nonetheless, we were able to obtain
reasonable results on this task. The third domain
we considered is NFL recaps (Barzilay and Lap-
ata, 2005; Snyder and Barzilay, 2007). The lan-
guage used in this domain is richer by orders of
magnitude, and much of it does not reference the
world state. Nonetheless, taking the first unsuper-
vised approach to this problem, we were able to
make substantial progress: We achieve an F1 of
53.2%, which closes over half of the gap between
a heuristic baseline (26%) and supervised systems
(68%?80%).
91
Dataset # scenarios |w| |T | |s| |A|
Robocup 1919 5.7 9 2.4 0.8
Weather 22146 28.7 12 36.0 5.8
NFL 78 969.0 44 329.0 24.3
Table 1: Statistics for the three datasets. We report average
values across all scenarios in the dataset: |w| is the number of
words in the text, |T | is the number of record types, |s| is the
number of records, and |A| is the number of gold alignments.
2 Domains and Datasets
Our goal is to learn the correspondence between a
text w and the world state s it describes. We use
the term scenario to refer to such a (w, s) pair.
The text is simply a sequence of words w =
(w1, . . . , w|w|). We represent the world state s as
a set of records, where each record r ? s is de-
scribed by a record type r.t ? T and a tuple of
field values r.v = (r.v1, . . . , r.vm).1 For exam-
ple, temperature is a record type in the weather
domain, and it has four fields: time, min, mean,
and max.
The record type r.t ? T specifies the field type
r.tf ? {INT, STR, CAT} of each field value r.vf ,
f = 1, . . . ,m. There are three possible field
types?integer (INT), string (STR), and categori-
cal (CAT)?which are assumed to be known and
fixed. Integer fields represent numeric properties
of the world such as temperature, string fields rep-
resent surface-level identifiers such as names of
people, and categorical fields represent discrete
concepts such as score types in football (touch-
down, field goal, and safety). The field type de-
termines the way we expect the field value to be
rendered in words: integer fields can be numeri-
cally perturbed, string fields can be spliced, and
categorical fields are represented by open-ended
word distributions, which are to be learned. See
Section 3.3 for details.
2.1 Robocup Sportscasting
In this domain, a Robocup simulator generates the
state of a soccer game, which is represented by
a set of event records. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a pass-
ing event; this type of record has two fields: arg1
(the actor) and arg2 (the recipient). As the game is
progressing, humans interject commentaries about
notable events in the game, e.g., pink1 passes back
to pink5 near the middle of the field. All of the
1To simplify notation, we assume that each record has m
fields, though in practice, m depends on the record type r.t.
fields in this domain are categorical, which means
there is no a priori association between the field
value pink1 and the word pink1. This degree of
flexibility is desirable because pink1 is sometimes
referred to as pink goalie, a mapping which does
not arise from string operations but must instead
be learned.
We used the dataset created by Chen and
Mooney (2008), which contains 1919 scenarios
from the 2001?2004 Robocup finals. Each sce-
nario consists of a single sentence representing a
fragment of a commentary on the game, paired
with a set of candidate records. In the annotation,
each sentence corresponds to at most one record
(possibly one not in the candidate set, in which
case we automatically get that sentence wrong).
See Figure 1(a) for an example and Table 1 for
summary statistics on the dataset.
2.2 Weather Forecasts
In this domain, the world state contains de-
tailed information about a local weather forecast
and the text is a short forecast report (see Fig-
ure 1(b) for an example). To create the dataset,
we collected local weather forecasts for 3,753
cities in the US (those with population at least
10,000) over three days (February 7?9, 2009) from
www.weather.gov. For each city and date, we
created two scenarios, one for the day forecast and
one for the night forecast. The forecasts consist of
hour-by-hour measurements of temperature, wind
speed, sky cover, chance of rain, etc., which rep-
resent the underlying world state.
This world state is summarized by records
which aggregate measurements over selected time
intervals. For example, one of the records states
the minimum, average, and maximum tempera-
ture from 5pm to 6am. This aggregation pro-
cess produced 22,146 scenarios, each containing
|s| = 36 multi-field records. There are 12 record
types, each consisting of only integer and categor-
ical fields.
To annotate the data, we split the text by punc-
tuation into lines and labeled each line with the
records to which the line refers. These lines are
used only for evaluation and are not part of the
model (see Section 5.1 for further discussion).
The weather domain is more complex than the
Robocup domain in several ways: The text w is
longer, there are more candidate records, and most
notably, w references multiple records (5.8 on av-
92
xbadPass(arg1=pink11,arg2=purple3)ballstopped()ballstopped()kick(arg1=pink11)turnover(arg1=pink11,arg2=purple3)
s
w:pink11 makes a bad pass and was picked off by purple3
(a) Robocup sportscasting
. . .rainChance(time=26-30,mode=Def)temperature(time=17-30,min=43,mean=44,max=47)windDir(time=17-30,mode=SE)windSpeed(time=17-30,min=11,mean=12,max=14,mode=10-20)precipPotential(time=17-30,min=5,mean=26,max=75)rainChance(time=17-30,mode=--)windChill(time=17-30,min=37,mean=38,max=42)skyCover(time=17-30,mode=50-75)rainChance(time=21-30,mode=--). . .
s
w:Occasional rain after 3am .Low around 43 .South wind between 11 and 14 mph .Chance of precipitation is 80 % .New rainfall amounts between aquarter and half of an inch possible .
(b) Weather forecasts
. . .rushing(entity=richie anderson,att=5,yds=37,avg=7.4,lg=16,td=0)receiving(entity=richie anderson,rec=4,yds=46,avg=11.5,lg=20,td=0)play(quarter=1,description=richie anderson ( dal ) rushed left side for 13 yards .)defense(entity=eric ogbogu,tot=4,solo=3,ast=1,sck=0,yds=0). . .
s w:. . .Former Jets player Richie Andersonfinished with 37 yards on 5 carriesplus 4 receptions for 46 yards .. . .
(c) NFL recaps
Figure 1: An example of a scenario for each of the three domains. Each scenario consists of a candidate set of records s and a
text w. Each record is specified by a record type (e.g., badPass) and a set of field values. Integer values are in Roman, string
values are in italics, and categorical values are in typewriter. The gold alignments are shown.
erage), so the segmentation of w is unknown. See
Table 1 for a comparison of the two datasets.
2.3 NFL Recaps
In this domain, each scenario represents a single
NFL football game (see Figure 1(c) for an exam-
ple). The world state (the things that happened
during the game) is represented by database tables,
e.g., scoring summary, team comparison, drive
chart, play-by-play, etc. Each record is a database
entry, for instance, the receiving statistics for a cer-
tain player. The text is the recap of the game?
an article summarizing the game highlights. The
dataset we used was collected by Barzilay and La-
pata (2005). The data includes 466 games during
the 2003?2004 NFL season. 78 of these games
were annotated by Snyder and Barzilay (2007),
who aligned each sentence to a set of records.
This domain is by far the most complicated of
the three. Many records corresponding to inconse-
quential game statistics are not mentioned. Con-
versely, the text contains many general remarks
(e.g., it was just that type of game) which are
not present in any of the records. Furthermore,
the complexity of the language used in the re-
cap is far greater than what we can represent us-
ing our simple model. Fortunately, most of the
fields are integer fields or string fields (generally
names or brief descriptions), which provide im-
portant anchor points for learning the correspon-
dences. Nonetheless, the same names and num-
bers occur in multiple records, so there is still un-
certainty about which record is referenced by a
given sentence.
3 Generative Model
To learn the correspondence between a text w and
a world state s, we propose a generative model
p(w | s) with latent variables specifying this cor-
respondence.
Our model combines segmentation with align-
ment. The segmentation aspect of our model is
similar to that of Grenager et al (2005) and Eisen-
stein and Barzilay (2008), but in those two models,
the segments are clustered into topics rather than
grounded to a world state. The alignment aspect
of our model is similar to the HMM model for
word alignment (Ney and Vogel, 1996). DeNero
et al (2008) perform joint segmentation and word
alignment for machine translation, but the nature
of that task is different from ours.
The model is defined by a generative process,
93
which proceeds in three stages (Figure 2 shows the
corresponding graphical model):
1. Record choice: choose a sequence of records
r = (r1, . . . , r|r|) to describe, where each
ri ? s.
2. Field choice: for each chosen record ri, se-
lect a sequence of fields fi = (fi1, . . . , fi|fi|),
where each fij ? {1, . . . ,m}.
3. Word choice: for each chosen field fij ,
choose a number cij > 0 and generate a se-
quence of cij words.
The observed text w is the terminal yield formed
by concatenating the sequences of words of all
fields generated; note that the segmentation of w
provided by c = {cij} is latent. Think of the
words spanned by a record as constituting an ut-
terance with a meaning representation given by the
record and subset of fields chosen.
Formally, our probabilistic model places a dis-
tribution over (r, f , c,w) and factorizes according
to the three stages as follows:
p(r, f , c,w | s) = p(r | s)p(f | r)p(c,w | r, f , s)
The following three sections describe each of
these stages in more detail.
3.1 Record Choice Model
The record choice model specifies a distribu-
tion over an ordered sequence of records r =
(r1, . . . , r|r|), where each record ri ? s. This
model is intended to capture two types of regu-
larities in the discourse structure of language. The
first is salience, that is, some record types are sim-
ply more prominent than others. For example, in
the NFL domain, 70% of scoring records are men-
tioned whereas only 1% of punting records are
mentioned. The second is the idea of local co-
herence, that is, the order in which one mentions
records tend to follow certain patterns. For ex-
ample, in the weather domain, the sky conditions
are generally mentioned first, followed by temper-
ature, and then wind speed.
To capture these two phenomena, we define a
Markov model on the record types (and given the
record type, a record is chosen uniformly from the
set of records with that type):
p(r | s) =
|r|?
i=1
p(ri.t | ri?1.t)
1
|s(ri.t)|
, (1)
where s(t)
def
= {r ? s : r.t = t} and r0.t is
a dedicated START record type.2 We also model
the transition of the final record type to a desig-
nated STOP record type in order to capture regu-
larities about the types of records which are de-
scribed last. More sophisticated models of coher-
ence could also be employed here (Barzilay and
Lapata, 2008).
We assume that s includes a special null record
whose type is NULL, responsible for generating
parts of our text which do not refer to any real
records.
3.2 Field Choice Model
Each record type t ? T has a separate field choice
model, which specifies a distribution over a se-
quence of fields. We want to capture salience
and coherence at the field level like we did at the
record level. For instance, in the weather domain,
the minimum and maximum fields of a tempera-
ture record are mentioned whereas the average is
not. In the Robocup domain, the actor typically
precedes the recipient in passing event records.
Formally, we have a Markov model over the
fields:3
p(f | r) =
|r|?
i=1
|fj |?
j=1
p(fij | fi(j?1)). (2)
Each record type has a dedicated null field with
its own multinomial distribution over words, in-
tended to model words which refer to that record
type in general (e.g., the word passes for passing
records). We also model transitions into the first
field and transitions out of the final field with spe-
cial START and STOP fields. This Markov structure
allows us to capture a few elements of rudimentary
syntax.
3.3 Word Choice Model
We arrive at the final component of our model,
which governs how the information about a par-
ticular field of a record is rendered into words. For
each field fij , we generate the number of words cij
from a uniform distribution over {1, 2, . . . , Cmax},
where Cmax is set larger than the length of the
longest text we expect to see. Conditioned on
2We constrain our inference to only consider record types
t that occur in s, i.e., s(t) 6= ?.
3During inference, we prohibit consecutive fields from re-
peating.
94
s
r
f
c,w
s
r1
f11
w1 ? ? ? w
c11
? ? ?
? ? ? ri
fi1
w ? ? ? w
ci1
? ? ? fi|fi|
w ? ? ? w
ci|fi|
? ? ? rn
? ? ? fn|fn|
w ? ? ? w|w|
cn|fn|
Record choice
Field choice
Word choice
Figure 2: Graphical model representing the generative model. First, records are chosen and ordered from the set s. Then fields
are chosen for each record. Finally, words are chosen for each field. The world state s and the words w are observed, while
(r, f , c) are latent variables to be inferred (note that the number of latent variables itself is unknown).
the fields f , the words w are generated indepen-
dently:4
p(w | r, f , c, s) =
|w|?
k=1
pw(wk | r(k).tf(k), r(k).vf(k)),
where r(k) and f(k) are the record and field re-
sponsible for generating word wk, as determined
by the segmentation c. The word choice model
pw(w | t, v) specifies a distribution over words
given the field type t and field value v. This distri-
bution is a mixture of a global backoff distribution
over words and a field-specific distribution which
depends on the field type t.
Although we designed our word choice model
to be relatively general, it is undoubtedly influ-
enced by the three domains. However, we can
readily extend or replace it with an alternative if
desired; this modularity is one principal benefit of
probabilistic modeling.
Integer Fields (t = INT) For integer fields, we
want to capture the intuition that a numeric quan-
tity v is rendered in the text as a word which
is possibly some other numerical value w due to
stylistic factors. Sometimes the exact value v is
used (e.g., in reporting football statistics). Other
times, it might be customary to round v (e.g., wind
speeds are typically rounded to a multiple of 5).
In other cases, there might just be some unex-
plained error, where w deviates from v by some
noise + = w ? v > 0 or ? = v ? w > 0. We
model + and ? as geometric distributions.5 In
4While a more sophisticated model of words would be
useful if we intended to use this model for natural language
generation, the false independence assumptions present here
matter less for the task of learning the semantic correspon-
dences because we always condition on w.
5Specifically, p(+;?+) = (1 ? ?+)+?1?+, where
?+ is a field-specific parameter; p(?;??) is defined analo-
gously.
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.5
p w(
w|
v=
13)
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.6
p w(
w|
v=
13)
(a) temperature.min (b) windSpeed.min
Figure 3: Two integer field types in the weather domain for
which we learn different distributions over the ways in which
a value v might appear in the text as a word w. Suppose the
record field value is v = 13. Both distributions are centered
around v, as is to be expected, but the two distributions have
different shapes: For temperature.min, almost all the mass
is to the left, suggesting that forecasters tend to report con-
servative lower bounds. For the wind speed, the mass is con-
centrated on 13 and 15, suggesting that forecasters frequently
round wind speeds to multiples of 5.
summary, we allow six possible ways of generat-
ing the word w given v:
v dve5 bvc5 round5(v) v ? ? v + +
Separate probabilities for choosing among these
possibilities are learned for each field type (see
Figure 3 for an example).
String Fields (t = STR) Strings fields are in-
tended to represent values which we expect to be
realized in the text via a simple surface-level trans-
formation. For example, a name field with value
v = Moe Williams is sometimes referenced in the
text by just Williams. We used a simple generic
model of rendering string fields: Let w be a word
chosen uniformly from those in v.
Categorical Fields (t = CAT) Unlike string
fields, categorical fields are not tied down to any
lexical representation; in fact, the identities of the
categorical field values are irrelevant. For each
categorical field f and possible value v, we have a
95
v pw(w | t, v)
0-25 , clear mostly sunny
25-50 partly , cloudy increasing
50-75 mostly cloudy , partly
75-100 of inch an possible new a rainfall
Table 2: Highest probability words for the categorical field
skyCover.mode in the weather domain. It is interesting to
note that skyCover=75-100 is so highly correlated with rain
that the model learns to connect an overcast sky in the world
to the indication of rain in the text.
separate multinomial distribution over words from
which w is drawn. An example of a categori-
cal field is skyCover.mode in the weather domain,
which has four values: 0-25, 25-50, 50-75,
and 75-100. Table 2 shows the top words for
each of these field values learned by our model.
4 Learning and Inference
Our learning and inference methodology is a fairly
conventional application of Expectation Maxi-
mization (EM) and dynamic programming. The
input is a set of scenarios D, each of which is a
text w paired with a world state s. We maximize
the marginal likelihood of our data, summing out
the latent variables (r, f , c):
max
?
?
(w,s)?D
?
r,f ,c
p(r, f , c,w | s; ?), (3)
where ? are the parameters of the model (all the
multinomial probabilities). We use the EM algo-
rithm to maximize (3), which alternates between
the E-step and the M-step. In the E-step, we
compute expected counts according to the poste-
rior p(r, f , c | w, s; ?). In the M-step, we op-
timize the parameters ? by normalizing the ex-
pected counts computed in the E-step. In our ex-
periments, we initialized EM with a uniform dis-
tribution for each multinomial and applied add-0.1
smoothing to each multinomial in the M-step.
As with most complex discrete models, the bulk
of the work is in computing expected counts under
p(r, f , c | w, s; ?). Formally, our model is a hier-
archical hidden semi-Markov model conditioned
on s. Inference in the E-step can be done using a
dynamic program similar to the inside-outside al-
gorithm.
5 Experiments
Two important aspects of our model are the seg-
mentation of the text and the modeling of the co-
herence structure at both the record and field lev-
els. To quantify the benefits of incorporating these
two aspects, we compare our full model with two
simpler variants.
? Model 1 (no model of segmentation or co-
herence): Each record is chosen indepen-
dently; each record generates one field, and
each field generates one word. This model is
similar in spirit to IBM model 1 (Brown et
al., 1993).
? Model 2 (models segmentation but not coher-
ence): Records and fields are still generated
independently, but each field can now gener-
ate multiple words.
? Model 3 (our full model of segmentation and
coherence): Records and fields are generated
according to the Markov chains described in
Section 3.
5.1 Evaluation
In the annotated data, each text w has been di-
vided into a set of lines. These lines correspond
to clauses in the weather domain and sentences in
the Robocup and NFL domains. Each line is an-
notated with a (possibly empty) set of records. Let
A be the gold set of these line-record alignment
pairs.
To evaluate a learned model, we com-
pute the Viterbi segmentation and alignment
(argmaxr,f ,c p(r, f , c | w, s)). We produce a pre-
dicted set of line-record pairsA? by aligning a line
to a record ri if the span of (the utterance corre-
sponding to) ri overlaps the line. The reason we
evaluate indirectly using lines rather than using ut-
terances is that it is difficult to annotate the seg-
mentation of text into utterances in a simple and
consistent manner.
We compute standard precision, recall, and F1
of A? with respect to A. Unless otherwise spec-
ified, performance is reported on all scenarios,
which were also used for training. However, we
did not tune any hyperparameters, but rather used
generic values which worked well enough across
all three domains.
5.2 Robocup Sportscasting
We ran 10 iterations of EM on Models 1?3. Ta-
ble 3 shows that performance improves with in-
creased model sophistication. We also compare
96
Method Precision Recall F1
Model 1 78.6 61.9 69.3
Model 2 74.1 84.1 78.8
Model 3 77.3 84.0 80.5
Table 3: Alignment results on the Robocup sportscasting
dataset.
Method F1
Random baseline 48.0
Chen and Mooney (2008) 67.0
Model 3 75.7
Table 4: F1 scores based on the 4-fold cross-validation
scheme in Chen and Mooney (2008).
our model to the results of Chen and Mooney
(2008) in Table 4.
Figure 4 provides a closer look at the predic-
tions made by each of our three models for a par-
ticular example. Model 1 easily mistakes pink10
for the recipient of a pass record because decisions
are made independently for each word. Model 2
chooses the correct record, but having no model
of the field structure inside a record, it proposes
an incorrect field segmentation (although our eval-
uation is insensitive to this). Equipped with the
ability to prefer a coherent field sequence, Model
3 fixes these errors.
Many of the remaining errors are due to the
garbage collection phenomenon familiar from
word alignment models (Moore, 2004; Liang et
al., 2006). For example, the ballstopped record
occurs frequently but is never mentioned in the
text. At the same time, there is a correlation be-
tween ballstopped and utterances such as pink2
holds onto the ball, which are not aligned to any
record in the annotation. As a result, our model
incorrectly chooses to align the two.
5.3 Weather Forecasts
For the weather domain, staged training was nec-
essary to get good results. For Model 1, we ran
15 iterations of EM. For Model 2, we ran 5 it-
erations of EM on Model 1, followed by 10 it-
erations on Model 2. For Model 3, we ran 5 it-
erations of Model 1, 5 iterations of a simplified
variant of Model 3 where records were chosen in-
dependently, and finally, 5 iterations of Model 3.
When going from one model to another, we used
the final posterior distributions of the former to ini-
Method Precision Recall F1
Model 1 49.9 75.1 60.0
Model 2 67.3 70.4 68.8
Model 3 76.3 73.8 75.0
Table 5: Alignment results on the weather forecast dataset.
[Model 1] r:f :w:
passarg2=pink10pink10 turns the ball over to purple5
[Model 2] r:f :w:
turnoverxpink10 turns the ball over arg2=purple5to purple5
[Model 3] r:f :w:
turnoverarg1=pink10pink10 xturns the ball over to arg2=purple5purple5
Figure 4: An example of predictions made by each of the
three models on the Robocup dataset.
tialize the parameters of the latter.6 We also pro-
hibited utterances in Models 2 and 3 from crossing
punctuation during inference.
Table 5 shows that performance improves sub-
stantially in the more sophisticated models, the
gains being greater than in the Robocup domain.
Figure 5 shows the predictions of the three models
on an example. Model 1 is only able to form iso-
lated (but not completely inaccurate) associations.
By modeling segmentation, Model 2 accounts for
the intermediate words, but errors are still made
due to the lack of Markov structure. Model 3
remedies this. However, unexpected structures
are sometimes learned. For example, the temper-
ature.time=6-21 field indicates daytime, which
happens to be perfectly correlated with the word
high, although high intuitively should be associ-
ated with the temperature.max field. In these cases
of high correlation (Table 2 provides another ex-
ample), it is very difficult to recover the proper
alignment without additional supervision.
5.4 NFL Recaps
In order to scale up our models to the NFL do-
main, we first pruned for each sentence the records
which have either no numerical values (e.g., 23,
23-10, 2/4) nor name-like words (e.g., those that
appear only capitalized in the text) in common.
This eliminated all but 1.5% of the record can-
didates per sentence, while maintaining an ora-
6It is interesting to note that this type of staged training
is evocative of language acquisition in children: lexical asso-
ciations are formed (Model 1) before higher-level discourse
structure is learned (Model 3).
97
[Model 1] r:f :w: cloudy , with a
windDirtime=6-21high near
temperaturemax=6363 .
windDirmode=SEeast southeast wind between
windSpeedmin=55 and
windSpeedmean=911 mph .
[Model 2] r:f :w:
rainChancemode=?cloudy ,
temperaturexwith a time=6-21high near max=6363 .
windDirmode=SEeast southeast wind xbetween 5 and
windSpeedmean=911 mph .
[Model 3] r:f :w:
skyCoverxcloudy ,
temperaturexwith a time=6-21high near max=6363 mean=56.
windDirmode=SEeast southeast xwind between
windSpeedmin=55 max=13and 11 xmph .
Figure 5: An example of predictions made by each of the three models on the weather dataset.
cle alignment F1 score of 88.7. Guessing a single
random record for each sentence yields an F1 of
12.0. A reasonable heuristic which uses weighted
number- and string-matching achieves 26.7.
Due to the much greater complexity of this do-
main, Model 2 was easily misled as it tried with-
out success to find a coherent segmentation of the
fields. We therefore created a variant, Model 2?,
where we constrained each field to generate ex-
actly one word. To train Model 2?, we ran 5 it-
erations of EM where each sentence is assumed
to have exactly one record, followed by 5 itera-
tions where the constraint was relaxed to also al-
low record boundaries at punctuation and the word
and. We did not experiment with Model 3 since
the discourse structure on records in this domain is
not at all governed by a simple Markov model on
record types?indeed, most regions do not refer to
any records at all. We also fixed the backoff prob-
ability to 0.1 instead of learning it and enforced
zero numerical deviation on integer field values.
Model 2? achieved an F1 of 39.9, an improve-
ment over Model 1, which attained 32.8. Inspec-
tion of the errors revealed the following problem:
The alignment task requires us to sometimes align
a sentence to multiple redundant records (e.g.,
play and score) referenced by the same part of the
text. However, our model generates each part of
text from only one record, and thus it can only al-
low an alignment to one record.7 To cope with this
incompatibility between the data and our notion of
semantics, we used the following solution: We di-
vided the records into three groups by type: play,
score, and other. Each group has a copy of the
model, but we enforce that they share the same
segmentation. We also introduce a potential that
couples the presence or absence of records across
7The model can align a sentence to multiple records pro-
vided that the records are referenced by non-overlapping
parts of the text.
Method Precision Recall F1
Random (with pruning) 13.1 11.0 12.0
Baseline 29.2 24.6 26.7
Model 1 25.2 46.9 32.8
Model 2? 43.4 37.0 39.9
Model 2? (with groups) 46.5 62.1 53.2
Graph matching (sup.) 73.4 64.5 68.6
Multilabel global (sup.) 87.3 74.5 80.3
Table 6: Alignment results on the NFL dataset. Graph match-
ing and multilabel are supervised results reported in Snyder
and Barzilay (2007).9
groups on the same segment to capture regular co-
occurrences between redundant records.
Table 6 shows our results. With groups, we
achieve an F1 of 53.2. Though we still trail su-
pervised techniques, which attain numbers in the
68?80 range, we have made substantial progress
over our baseline using an unsupervised method.
Furthermore, our model provides a more detailed
analysis of the correspondence between the world
state and text, rather than just producing a single
alignment decision. Most of the remaining errors
made by our model are due to a lack of calibra-
tion. Sometimes, our false positives are close calls
where a sentence indirectly references a record,
and our model predicts the alignment whereas the
annotation standard does not. We believe that fur-
ther progress is possible with a richer model.
6 Conclusion
We have presented a generative model of corre-
spondences between a world state and an unseg-
mented stream of text. By having a joint model
of salience, coherence, and segmentation, as well
as a detailed rendering of the values in the world
state into words in the text, we are able to cope
with the increased ambiguity that arises in this new
data setting, successfully pushing the limits of un-
supervision.
98
References
R. Barzilay and M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Human Language
Technology and Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 331?338, Vancouver,
B.C.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Linguis-
tics, 34:1?34.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19:263?311.
D. L. Chen and R. J. Mooney. 2008. Learning to sportscast:
A test of grounded language acquisition. In International
Conference on Machine Learning (ICML), pages 128?
135. Omnipress.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Empirical Methods in Natural Language Processing
(EMNLP), pages 314?323, Honolulu, HI.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised
topic segmentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 334?343.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a
neural theory of language. Brain and Language, 89:385?
392.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Computational
Natural Language Learning (CoNLL), pages 9?16, Ann
Arbor, Michigan.
P. Gorniak and D. Roy. 2007. Situated language understand-
ing as filtering perceived affordances. Cognitive Science,
31:197?231.
T. Grenager, D. Klein, and C. D. Manning. 2005. Unsu-
pervised learning of field segmentation models for infor-
mation extraction. In Association for Computational Lin-
guistics (ACL), pages 371?378, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.
R. J. Kate and R. J. Mooney. 2007. Learning language se-
mantics from ambiguous supervision. In Association for
the Advancement of Artificial Intelligence (AAAI), pages
895?900, Cambridge, MA. MIT Press.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-
ment. In North American Association for Computational
Linguistics (NAACL), pages 104?111, New York City. As-
sociation for Computational Linguistics.
W. Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer. 2008. A
generative model for parsing natural language to meaning
representations. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 783?792.
R. C. Moore. 2004. Improving IBM word alignment model
1. In Association for Computational Linguistics (ACL),
pages 518?525, Barcelona, Spain. Association for Com-
putational Linguistics.
H. Ney and S. Vogel. 1996. HMM-based word align-
ment in statistical translation. In International Conference
on Computational Linguistics (COLING), pages 836?841.
Association for Computational Linguistics.
J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning map-
pings. Cognition, 61:1?38.
B. Snyder and R. Barzilay. 2007. Database-text alignment
via structured multilabel classification. In International
Joint Conference on Artificial Intelligence (IJCAI), pages
1713?1718, Hyderabad, India.
C. Yu and D. H. Ballard. 2004. On the integration of ground-
ing language and learning objects. In Association for the
Advancement of Artificial Intelligence (AAAI), pages 488?
493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification with
probabilistic categorial grammars. In Uncertainty in Arti-
ficial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP/CoNLL), pages 678?687.
99
Learning Dependency-Based
Compositional Semantics
Percy Liang?
University of California, Berkeley
Michael I. Jordan??
University of California, Berkeley
Dan Klein?
University of California, Berkeley
Suppose we want to build a system that answers a natural language question by representing its
semantics as a logical form and computing the answer given a structured database of facts. The
core part of such a system is the semantic parser that maps questions to logical forms. Semantic
parsers are typically trained from examples of questions annotated with their target logical forms,
but this type of annotation is expensive.
Our goal is to instead learn a semantic parser from question?answer pairs, where the logical
form is modeled as a latent variable. We develop a new semantic formalism, dependency-based
compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The
model parameters are estimated using a simple procedure that alternates between beam search
and numerical optimization. On two standard semantic parsing benchmarks, we show that our
system obtains comparable accuracies to even state-of-the-art systems that do require annotated
logical forms.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
1. Introduction
One of the major challenges in natural language processing (NLP) is building systems
that both handle complex linguistic phenomena and require minimal human effort. The
difficulty of achieving both criteria is particularly evident in training semantic parsers,
where annotating linguistic expressions with their associated logical forms is expensive
but until recently, seemingly unavoidable. Advances in learning latent-variable models,
however, have made it possible to progressively reduce the amount of supervision
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: pliang@cs.stanford.edu.
?? Computer Science Division and Department of Statistics, University of California, Berkeley, CA 94720,
USA. E-mail: jordan@cs.berkeley.edu.
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: klein@cs.berkeley.edu.
Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for
publication: 18 April 2012.
doi:10.1162/COLI a 00127
Computational Linguistics Volume 39, Number 2
required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan
et al 2009; Liang, Jordan, and Klein 2009; Clarke et al 2010; Artzi and Zettlemoyer 2011;
Goldwasser et al 2011). In this article, we develop new techniques to learn accurate
semantic parsers from even weaker supervision.
We demonstrate our techniques on the concrete task of building a system to answer
questions given a structured database of facts; see Figure 1 for an example in the domain
of U.S. geography. This problem of building natural language interfaces to databases
(NLIDBs) has a long history in NLP, starting from the early days of artificial intelligence
with systems such as LUNAR (Woods, Kaplan, and Webber 1972), CHAT-80 (Warren
and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995]
for an overview). We believe NLIDBs provide an appropriate starting point for semantic
parsing because they lead directly to practical systems, and they allow us to temporarily
sidestep intractable philosophical questions on how to represent meaning in general.
Early NLIDBs were quite successful in their respective limited domains, but because
these systems were constructed frommanually built rules, they became difficult to scale
up, both to other domains and to more complex utterances. In response, against the
backdrop of a statistical revolution in NLP during the 1990s, researchers began to build
systems that could learn from examples, with the hope of overcoming the limitations of
rule-based methods. One of the earliest statistical efforts was the CHILL system (Zelle
and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has
been a healthy line of work yielding increasingly more accurate semantic parsers by
using new semantic representations andmachine learning techniques (Miller et al 1996;
Zelle and Mooney 1996; Tang andMooney 2001; Ge andMooney 2005; Kate, Wong, and
Mooney 2005; Zettlemoyer and Collins 2005; Kate andMooney 2006;Wong andMooney
2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007;
Kwiatkowski et al 2010, 2011).
Although statistical methods provided advantages such as robustness and portabil-
ity, however, their application in semantic parsing achieved only limited success. One
of the main obstacles was that these methods depended crucially on having examples
of utterances paired with logical forms, and this requires substantial human effort to
obtain. Furthermore, the annotators must be proficient in some formal language, which
drastically reduces the size of the annotator pool, dampening any hope of acquiring
enough data to fulfill the vision of learning highly accurate systems.
In response to these concerns, researchers have recently begun to explore the pos-
sibility of learning a semantic parser without any annotated logical forms (Clarke et al
Figure 1
The concrete objective: A system that answers natural language questions given a structured
database of facts. An example is shown in the domain of U.S. geography.
390
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 2
Our statistical methodology consists of two steps: (i) semantic parsing (p(z | x;?)): an utterance x
is mapped to a logical form z by drawing from a log-linear distribution parametrized by a
vector ?; and (ii) evaluation ([[z]]w): the logical form z is evaluated with respect to the world w
(database of facts) to deterministically produce an answer y. The figure also shows an example
configuration of the variables around the graphical model. Logical forms z are represented as
labeled trees. During learning, we are given w and (x, y) pairs (shaded nodes) and try to infer
the latent logical forms z and parameters ?.
2010; Artzi and Zettlemoyer 2011; Goldwasser et al 2011; Liang, Jordan, andKlein 2011).
It is in this vein that we develop our present work. Specifically, given a set of (x, y)
example pairs, where x is an utterance (e.g., a question) and y is the corresponding
answer, we wish to learn a mapping from x to y. What makes this mapping particularly
interesting is that it passes through a latent logical form z, which is necessary to capture
the semantic complexities of natural language. Also note that whereas the logical form
z was the end goal in much of earlier work on semantic parsing, for us it is just an
intermediate variable?a means towards an end. Figure 2 shows the graphical model
which captures the learning setting we just described: The question x, answer y, and
world/database w are all observed. We want to infer the logical forms z and the
parameters ? of the semantic parser, which are unknown quantities.
Although liberating ourselves from annotated logical forms reduces cost, it does
increase the difficulty of the learning problem. The core challenge here is program
induction: On each example (x, y), we need to efficiently search over the exponential
space of possible logical forms (programs) z and find ones that produce the target
answer y, a computationally daunting task. There is also a statistical challenge: How
do we parametrize the mapping from utterance x to logical form z so that it can be
learned from only the indirect signal y? To address these two challenges, we must first
discuss the issue of semantic representation. There are two basic questions here: (i) what
391
Computational Linguistics Volume 39, Number 2
should the formal language for the logical forms z be, and (ii) what are the compositional
mechanisms for constructing those logical forms?
The semantic parsing literature has considered many different formal languages
for representing logical forms, including SQL (Giordani and Moschitti 2009), Prolog
(Zelle and Mooney 1996; Tang and Mooney 2001), a simple functional query language
called FunQL (Kate, Wong, and Mooney 2005), and lambda calculus (Zettlemoyer and
Collins 2005), just to name a few. The construction mechanisms are equally diverse, in-
cluding synchronous grammars (Wong and Mooney 2007), hybrid trees (Lu et al 2008),
Combinatory Categorial Grammars (CCG) (Zettlemoyer and Collins 2005), and shift-
reduce derivations (Zelle and Mooney 1996). It is worth pointing out that the choice of
formal language and the construction mechanism are decisions which are really more
orthogonal than is often assumed?the former is concerned with what the logical forms
look like; the latter, with how to generate a set of possible logical forms compositionally
given an utterance. (How to score these logical forms is yet another dimension.)
Existing systems are rarely based on the joint design of the formal language and
the construction mechanism; one or the other is often chosen for convenience from
existing implementations. For example, Prolog and SQL have often been chosen as
formal languages for convenience in end applications, but they were not designed
for representing the semantics of natural language, and, as a result, the construction
mechanism that bridges the gap between natural language and formal language is
generally complex and difficult to learn. CCG (Steedman 2000) is quite popular in
computational linguistics (for example, see Bos et al [2004] and Zettlemoyer and Collins
[2005]). In CCG, logical forms are constructed compositionally using a small handful
of combinators (function application, function composition, and type raising). For a
wide range of canonical examples, CCG produces elegant, streamlined analyses, but
its success really depends on having a good, clean lexicon. During learning, there is
often a great amount of uncertainty over the lexical entries, which makes CCG more
cumbersome. Furthermore, in real-world applications, we would like to handle disflu-
ent utterances, and this further strains CCG by demanding either extra type-raising
rules and disharmonic combinators (Zettlemoyer and Collins 2007) or a proliferation of
redundant lexical entries for each word (Kwiatkowski et al 2010).
To cope with the challenging demands of program induction, we break away from
tradition in favor of a new formal language and construction mechanism, which we call
dependency-based compositional semantics (DCS). The guiding principle behind DCS
is to provide a simple and intuitive framework for constructing and representing logical
forms. Logical forms in DCS are tree structures calledDCS trees. The motivation is two-
fold: (i) DCS trees are meant to parallel syntactic dependency trees, which facilitates
parsing; and (ii) a DCS tree essentially encodes a constraint satisfaction problem, which
can be solved efficiently using dynamic programming to obtain the denotation of a DCS
tree. In addition, DCS provides a mark?execute construct, which provides a uniform
way of dealing with scope variation, a major source of trouble in any semantic for-
malism. The construction mechanism in DCS is a generalization of labeled dependency
parsing, which leads to simple and natural algorithms. To a linguist, DCS might appear
unorthodox, but it is important to keep in mind that our primary goal is effective
program induction, not necessarily to model new linguistic phenomena in the tradition
of formal semantics.
Armed with our new semantic formalism, DCS, we then define a discriminative
probabilistic model, which is depicted in Figure 2. The semantic parser is a log-linear
distribution over DCS trees z given an utterance x. Notably, z is unobserved, and we in-
stead observe only the answer y, which is obtained by evaluating z on a world/database
392
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
w. There are an exponential number of possible trees z, and usually dynamic program-
ming can be used to efficiently search over trees. However, in our learning setting
(independent of the semantic formalism), we must enforce the global constraint that
z produces y. This makes dynamic programming infeasible, so we use beam search
(though dynamic programming is still used to compute the denotation of a fixed DCS
tree). We estimate the model parameters with a simple procedure that alternates be-
tween beam search and optimizing a likelihood objective restricted to those beams. This
yields a natural bootstrapping procedure in which learning and search are integrated.
We evaluated our DCS-based approach on two standard benchmarks, GEO, a U.S.
geography domain (Zelle and Mooney 1996), and JOBS, a job queries domain (Tang and
Mooney 2001). On GEO, we found that our system significantly outperforms previous
work that also learns from answers instead of logical forms (Clarke et al 2010). What
is perhaps a more significant result is that our system obtains comparable accuracies to
state-of-the-art systems that do rely on annotated logical forms. This demonstrates the
viability of training accurate systems with much less supervision than before.
The rest of this article is organized as follows: Section 2 introduces DCS, our new
semantic formalism. Section 3 presents our probabilistic model and learning algorithm.
Section 4 provides an empirical evaluation of our methods. Section 5 situates this work
in a broader context, and Section 6 concludes.
2. Representation
In this section, we present the main conceptual contribution of this work, dependency-
based compositional semantics (DCS), using the U.S. geography domain (Zelle and
Mooney 1996) as a running example. To do this, we need to define the syntax and
semantics of the formal language. The syntax is defined in Section 2.2 and is quite
straightforward: The logical forms in the formal language are simply trees, which we
callDCS trees. In Section 2.3, we give a type-theoretic definition ofworlds (also known
as databases or models) with respect to which we can define the semantics of DCS trees.
The semantics, which is the heart of this article, contains two main ideas: (i) using
trees to represent logical forms as constraint satisfaction problems or extensions thereof,
and (ii) dealing with cases when syntactic and semantic scope diverge (e.g., for general-
ized quantification and superlative constructions) using a new construct which we call
mark?execute. We start in Section 2.4 by introducing the semantics of a basic version
of DCS which focuses only on (i) and then extend it to the full version (Section 2.5) to
account for (ii).
Finally, having fully specified the formal language, we describe a construction
mechanism for mapping a natural language utterance to a set of candidate DCS trees
(Section 2.6).
2.1 Notation
Operations on tuples will play a prominent role in this article. For a sequence1 v =
(v1, . . . , vk), we use |v| = k to denote the length of the sequence. For two sequences u
and v, we use u+ v = (u1, . . . ,u|u|, v1, . . . , v|v|) to denote their concatenation.
1 We use the term sequence to refer to both tuples (v1, . . . , vk ) and arrays [v1, . . . , vk]. For our purposes, there
is no functional difference between tuples and arrays; the distinction is convenient when we start to talk
about arrays of tuples.
393
Computational Linguistics Volume 39, Number 2
For a sequence of positive indices i = (i1, . . . , im), let vi = (vi1 , . . . , vim ) consist of the
components of v specified by i; we call vi the projection of v onto i. We use negative
indices to exclude components: v?i = (v(1,...,|v|)\i). We can also combine sequences of
indices by concatenation: vi,j = vi + vj. Some examples: if v = (a, b, c, d), then v2 = b,
v3,1 = (c, a), v?3 = (a, b, d), v3,?3 = (c, a, b, d).
2.2 Syntax of DCS Trees
The syntax of the DCS formal language is built from two ingredients, predicates and
relations:
 Let P be a set of predicates. We assume that P contains a special null
predicate ?, domain-independent predicates (e.g., count, <, >, and =), and
domain-specific predicates (for the U.S. geography domain, state, river,
border, etc.). Right now, think of predicates as just labels, which have yet
to receive formal semantics.
 LetR be the set of relations. Note that unlike the predicates P , which can
vary across domains, the relationsR are fixed. The full set of relations are
shown in Table 1. For now, just think of relations as labels?their semantics
will be defined in Section 2.4.
The logical forms in DCS are called DCS trees. A DCS tree is a directed rooted tree
in which nodes are labeled with predicates and edges are labeled with relations; each
node also maintains an ordering over its children. Formally:
Definition 1 (DCS trees)
Let Z be the set of DCS trees, where each z ? Z consists of (i) a predicate z.p ? P and (ii)
a sequence of edges z.e = (z.e1, . . . , z.em). Each edge e consists of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We will either draw a DCS tree graphically or write it compactly as ?p; r1 :c1; . . . ; rm :cm?
where p is the predicate at the root node and c1, . . . , cm are its m children connected via
edges labeled with relations r1, . . . , rm, respectively. Figure 3(a) shows an example of a
DCS tree expressed using both graphical and compact formats.
Table 1
Possible relations that appear on edges of DCS trees. Basic DCS uses only the join and aggregate
relations; the full version of DCS uses all of them.
RelationsR
Name Relation Description of semantic function
join
j
j? for j, j
? ? {1, 2, . . . } j-th component of parent = j?-th component of child
aggregate ? parent = set of feasible values of child
extract E mark node for extraction
quantify Q mark node for quantification, negation
compare C mark node for superlatives, comparatives
execute Xi for i ? {1, 2 . . . }? process marked nodes specified by i
394
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 3
(a) An example of a DCS tree (written in both the mathematical and graphical notations). Each
node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree zwith
only join relations encodes a constraint satisfaction problem, represented here as a lambda
calculus formula. For example, the root node label city corresponds to a unary predicate
city(c), the right child node label loc corresponds to a binary predicate loc() (where  is a
pair), and the edge between them denotes the constraint c1 = 1 (where the indices correspond to
the two labels on the edge). (c) The denotation of z is the set of feasible values for the root node.
A DCS tree is a logical form, but it is designed to look like a syntactic dependency
tree, only with predicates in place of words. As we?ll see over the course of this section,
it is this transparency between syntax and semantics provided by DCS which leads to a
simple and streamlined compositional semantics suitable for program induction.
2.3 Worlds
In the context of question answering, the DCS tree is a formal specification of the
question. To obtain an answer, we still need to evaluate the DCS tree with respect to
a database of facts (see Figure 4 for an example). We will use the term world to refer
Figure 4
We use the domain of U.S. geography as a running example. The figure presents an example of a
world w (database) in this domain. A world maps each predicate to a set of tuples. For example,
the depicted world wmaps the predicate loc to the set of pairs of places and their containers.
Note that functions (e.g., population) are also represented as predicates for uniformity. Some
predicates (e.g., count) map to an infinite number of tuples and would be represented implicitly.
395
Computational Linguistics Volume 39, Number 2
to this database (it is sometimes also called a model, but we avoid this term to avoid
confusion with the probabilistic model for learning that we will present in Section 3.1).
Throughout this work, we assume the world is fully observed and fixed, which is a
realistic assumption for building natural language interfaces to existing databases, but
questionable for modeling the semantics of language in general.
2.3.1 Types and Values. To define a world, we start by constructing a set of values V .
The exact set of values depends on the domain (we will continue to use U.S. geog-
raphy as a running example). Briefly, V contains numbers (e.g., 3 ? V), strings (e.g.,
Washington ? V), tuples (e.g., (3,Washington) ? V), sets (e.g., {3,Washington} ? V), and
other higher-order entities.
To be more precise, we construct V recursively. First, define a set of primitive values
V, which includes the following:
 Numeric values. Each value has the form x : t ? V, where x ? R is a real
number and t ? {number, ordinal, percent, length, . . . } is a tag. The tag
allows us to differentiate 3, 3rd, 3%, and 3 miles?this will be important in
Section 2.6.3. We simply write x for the value x :number.
 Symbolic values. Each value has the form x : t ? V, where x is a string (e.g.,
Washington) and t ? {string, city, state, river, . . . } is a tag. Again, the
tag allows us to differentiate, for example, the entitiesWashington :city
andWashington :state.
Now we build the full set of values V from the primitive values V. To define V , we
need a bit more machinery: To avoid logical paradoxes, we construct V in increasing
order of complexity using types (see Carpenter [1998] for a similar construction). The
casual reader can skip this construction without losing any intuition.
Define the set of types T to be the smallest set that satisfies the following properties:
1. The primitive type  ? T ;
2. The tuple type (t1, . . . , tk) ? T for each k ? 0 and each non-tuple type
ti ? T for i = 1, . . . , k; and
3. The set type {t} ? T for each tuple type t ? T .
Note that {}, {{}}, and (()) are not valid types.
For each type t ? T , we construct a corresponding set of values Vt:
1. For the primitive type t = , the primitive values V have already been
specified. Note that these types are rather coarse: Primitive values with
different tags are considered to have the same type .
2. For a tuple type t = (t1, . . . , tk), Vt is the cross product of the values of its
component types:
Vt = {(v1, . . . , vk) : ?i, vi ? Vti} (1)
396
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
3. For a set type t = {t?}, Vt contains all subsets of its element type t?:
Vt = {s : s ? Vt?} (2)
With this last condition, we ensure that all elements of a set must have the
same type. Note that a set is still allowed to have values with different tags
(e.g., {(Washington :city), (Washington :state)} is a valid set, which might
denote the semantics of the utterance things named Washington). Another
distinction is that types are domain-independent whereas tags tend to be
more domain-specific.
Let V = ?t?T Vt be the set of all possible values.
A world maps each predicate to its semantics, which is a set of tuples (see Figure 4
for an example). First, let TTUPLE ? T be the tuple types, which are the ones of the form
(t1, . . . , tk) for some k. Let V{TUPLE} denote all the sets of tuples (with the same type):
V{TUPLE}
def
=
?
t?TTUPLE
V{t} (3)
Now we define a world formally.
Definition 2 (World)
A world w : P ? V{TUPLE} ? {V} is a function that maps each non-null predicate p ?
P\{?} to a set of tuples w(p) ? V{TUPLE} and maps the null predicate ? to the set of all
values (w(?) = V).
For a set of tuplesAwith the same arity, let ARITY(A) = |x|, where x ? A is arbitrary;
if A is empty, then ARITY(A) is undefined. Now for a predicate p ? P and world w,
define ARITYw(p), the arity of predicate pwith respect to w, as follows:
ARITYw(p) =
{
1 if p = ?
ARITY(w(p)) if p = ?
(4)
The null predicate has arity 1 by fiat; the arity of a non-null predicate p is inherited from
the tuples in w(p).
Remarks. In higher-order logic and lambda calculus, we construct function types and
values, whereas in DCS, we construct tuple types and values. The two are equivalent
in representational power, but this discrepancy does point out the fact that lambda
calculus is based on function application, whereas DCS, as we will see, is based on
declarative constraints. The set type {(, )} in DCS corresponds to the function type
? (? bool). In DCS, there is no explicit bool type?it is implicitly represented by
using sets.
2.3.2 Examples. The world w maps each domain-specific predicate to a set of tuples
(usually a finite set backed by a database). For the U.S. geography domain, w has a
397
Computational Linguistics Volume 39, Number 2
predicate that maps to the set of U.S. states (state), another predicate that maps to the
set of pairs of entities and where they are located (loc), and so on:
w(state) = {(California :state), (Oregon :state), . . . } (5)
w(loc) = {(San Francisco :city,California :state), . . . } (6)
. . . (7)
To shorten notation, we use state abbreviations (e.g., CA = California :state).
The world w also specifies the semantics of several domain-independent predicates
(think of these as helper functions), which usually correspond to an infinite set of tuples.
Functions are represented in DCS by a set of input?output pairs. For example, the
semantics of the countt predicate (for each type t ? T ) contains pairs of sets S and their
cardinalities |S|:
w(countt) = {(S, |S|) : S ? V{(t)}} ? V{({(t)},)} (8)
As another example, consider the predicate averaget (for each t ? T ), which takes a
set of key?value pairs (with keys of type t) and returns the average value. For notational
convenience, we treat an arbitrary set of pairs S as a set-valued function: We let S1 = {x :
(x, y) ? S} denote the domain of the function, and abusing notation slightly, we define
the function S(x) = {y : (x, y) ? S} to be the set of values y that co-occur with the given
x. The semantics of averaget contains pairs of sets and their averages:
w(averaget) =
?
?
?
(S, z) : S ? V{(t,)}, z = |S1|
?1
?
x?S1
?
?|S(x)|?1
?
y?S(x)
y
?
?
?
?
?
? V{({(t,)},)}
(9)
Similarly, we can define the semantics of argmint and argmaxt, which each takes a set of
key?value pairs and returns the keys that attain the smallest (largest) value:
w(argmint) =
{
(S, z) : S ? V{(t,)}, z ? argmin
x?S1
min S(x)
}
? V{({(t,)},t)} (10)
w(argmaxt) =
{
(S, z) : S ? V{(t,)}, z ? argmax
x?S1
max S(x)
}
? V{({(t,)},t)} (11)
The extra min and max is needed because S(x) could contain more than one value. We
also impose that w(argmint) contains only (S, z) such that y is numeric for all (x, y) ? S;
thus argmint denotes a partial function (same for argmaxt).
These helper functions are monomorphic: For example, countt only computes
cardinalities of sets of type {(t)}. In practice, we mostly operate on sets of primitives
(t = ). To reduce notation, we omit t to refer to this version: count = count, average =
average, and so forth.
398
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
2.4 Semantics of DCS Trees without Mark?Execute (Basic Version)
The semantics or denotation of a DCS tree z with respect to a world w is denoted zw.
First, we define the semantics of DCS trees with only join relations (Section 2.4.1). In
this case, a DCS tree encodes a constraint satisfaction problem (CSP); this is important
because it highlights the constraint-based nature of DCS and also naturally leads to a
computationally efficient way of computing denotations (Section 2.4.2). We then allow
DCS trees to have aggregate relations (Section 2.4.3). The fragment of DCS which has
only join and aggregate relations is called basic DCS.
2.4.1 Basic DCS Trees as Constraint Satisfaction Problems. Let z be a DCS tree with only
join relations on its edges. In this case, z encodes a CSP as follows: For each node x in z,
the CSP has a variable with value a(x); the collection of these values is referred to as an
assignment a. The predicates and relations of z introduce constraints:
1. a(x) ? w(p) for each node x labeled with predicate p ? P ; and
2. a(x)j = a(y)j? for each edge (x, y) labeled with
j
j? ? R, which says that the
j-th component of a(x) must equal the j?-th component of a(y).
We say that an assignment a is feasible if it satisfies these two constraints. Next, for a node
x, defineV(x) = {a(x) : assignment a is feasible} as the set of feasible values for x?these
are the ones that are consistent with at least one feasible assignment. Finally, we define
the denotation of the DCS tree z with respect to the world w to be zw = V(x0), where
x0 is the root node of z.
Figure 3(a) shows an example of a DCS tree. The corresponding CSP has four vari-
ables c,m, , s.2 In Figure 3(b), we have written the equivalent lambda calculus formula.
The non-root nodes are existentially quantified, the root node c is ?-abstracted, and
all constraints introduced by predicates and relations are conjoined. The ?-abstraction
of c represents the fact that the denotation is the set of feasible values for c (note the
equivalence between the Boolean function ?c.p(c) and the set {c : p(c)}).
Remarks. Note that CSPs only allow existential quantification and conjunction. Why
did we choose this particular logical subset as a starting point, rather than allowing
universal quantification, negation, or disjunction? There seems to be something fun-
damental about this subset, which also appears in Discourse Representation Theory
(DRT) (Kamp and Reyle 1993; Kamp, van Genabith, and Reyle 2005). Briefly, logical
forms in DRT are called Discourse Representation Structures (DRSs), each of which
contains (i) a set of existentially quantified discourse referents (variables), (ii) a set of
conjoined discourse conditions (constraints), and (iii) nested DRSs. If we exclude nested
DRSs, a DRS is exactly a CSP.3 The default existential quantification and conjunction are
quite natural for modeling cross-sentential anaphora: New variables can be added to
2 Technically, the node is c and the variable is a(c), but we use c to denote the variable to simplify notation.
3 Unlike the CSPs corresponding to DCS trees, the CSPs corresponding to DRSs need not be
tree-structured, though economical DRT (Bos 2009) imposes a tree-like restriction on DRSs for
computational reasons.
399
Computational Linguistics Volume 39, Number 2
a DRS and connected to other variables. Indeed, DRT was originally motivated by these
phenomena (see Kamp and Reyle [1993] for more details).4
Tree-structured CSPs can capture unboundedly complex recursive structures?such
as cities in states that border states that have rivers that. . . . Trees are limited, however, in
that they are unable to capture long-distance dependencies such as those arising from
anaphora. For example, in the phrase a state with a river that traverses its capital, its binds
to state, but this dependence cannot be captured in a tree structure. A solution is
to simply add an edge between the its node and the state node that forces the two
nodes to have the same value. The result is still a well-defined CSP, though not a tree-
structured one. The situation would become trickier if we were to integrate the other
relations (aggregate, mark, and execute). We might be able to incorporate some ideas
from Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; White 2006),
given that hybrid logic extends the tree structures of modal logic with nominals, thereby
allowing a node to freely reference other nodes. In this article, however, we will stick to
trees and leave the full exploration of non-trees for future work.
2.4.2 Computation of Join Relations. So far, we have given a declarative definition of the
denotation zw of a DCS tree z with only join relations. Now we will show how to
compute zw efficiently. Recall that the denotation is the set of feasible values for the
root node. In general, finding the solution to a CSP is NP-hard, but for trees, we can
exploit dynamic programming (Dechter 2003). The key is that the denotation of a tree
depends on its subtrees only through their denotations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= w(p) ?
m
?
i=1
{v : vji = tj?i , t ? ciw} (12)
On the right-hand side of Equation (12), the first termw(p) is the set of values that satisfy
the node constraint, and the second term consists of an intersection across all m edges
of {v : vji = tj?i , t ? ciw}, which is the set of values v which satisfy the edge constraint
with respect to some value t for the child ci.
To further flesh out this computation, we express Equation (12) in terms of two
operations: join and project. Join takes a cross product of two sets of tuples and retains
the resulting tuples that match the join constraint:
A j,j? B = {u+ v : u ? A, v ? B,uj = vj?} (13)
Project takes a set of tuples and retains a fixed subset of the components:
A[i] = {vi : v ? A} (14)
The denotation in Equation (12) can now be expressed in terms of these join and project
operations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= ((w(p) j1,j?1 c1w)[i] ? ? ? jm,j?m cmw)[i] (15)
4 DRT started the dynamic semantics tradition where meanings are context-change potentials, a natural
way to capture anaphora. The DCS formalism presented here does not deal with anaphora, so we give it
a purely static semantics.
400
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
where i = (1, . . . , ARITYw(p)). Projecting onto i retains only components corresponding
to p.
The time complexity for computing the denotation of a DCS tree zw scales linearly
with the number of nodes, but there is also a dependence on the cost of performing the
join and project operations. For details on howwe optimize these operations and handle
infinite sets of tuples (for predicates such as count), see Liang (2011).
The denotation of DCS trees is defined in terms of the feasible values of a CSP, and
the recurrence in Equation (15) is only one way of computing this denotation. In light of
the extensions to come, however, we now consider Equation (15) as the actual definition
rather than just a computational mechanism. It will still be useful to refer to the CSP in
order to access the intuition of using declarative constraints.
2.4.3 Aggregate Relation. Thus far, we have focused on DCS trees that only use join
relations, which are insufficient for capturing higher-order phenomena in language. For
example, consider the phrase number of major cities. Suppose that number corresponds
to the count predicate, and that major cities maps to the DCS tree ?city; 11 :?major??. We
cannot simply join countwith the root of this DCS tree because count needs to be joined
with the set of major cities (the denotation of ?city; 11 :?major??), not just a single city.
We therefore introduce the aggregate relation (?) that takes a DCS subtree and
reifies its denotation so that it can be accessed by other nodes in its entirety. Consider a
tree ??;? :c?, where the root is connected to a child c via ?. The denotation of the root is
simply the singleton set containing the denotation of c:
??;? :c?w = {(cw)} (16)
Figure 5(a) shows the DCS tree for our running example. The denotation of the
middle node is {(s)}, where s is all major cities. Everything above this node is an
ordinary CSP: s constrains the count node, which in turns constrains the root node to
|s|. Figure 5(b) shows another example of using the aggregate relation ?. Here, the node
right above ? is constrained to be a set of pairs of major cities and their populations.
The average predicate then computes the desired answer.
To represent logical disjunction in natural language, we use the aggregate relation
and two predicates, union and contains, which are defined in the expected way:
w(union) = {(A,B,C) : C = A ? B,A ? V{},B ? V{}} (17)
w(contains) = {(A, x) : x ? A,A ? V{}} (18)
where A,B,C ? V{} are sets of primitive values (see Section 2.3.1). Figure 5(c) shows
an example of a disjunctive construction: We use the aggregate relations to construct
two sets, one containing Oregon, and the other containing states bordering Oregon. We
take the union of these two sets; contains takes the set and reads out an element, which
then constrains the city node.
Remarks. A DCS tree that contains only join and aggregate relations can be viewed as
a collection of tree-structured CSPs connected via aggregate relations. The tree struc-
ture still enables us to compute denotations efficiently based on the recurrences in
Equations (15) and (16).
Recall that a DCS tree with only join relations is a DRS without nested DRSs. The
aggregate relation corresponds to the abstraction operator in DRT and is one way of
401
Computational Linguistics Volume 39, Number 2
Figure 5
Examples of DCS trees that use the aggregate relation (?) to (a) compute the cardinality of a set,
(b) take the average over a set, (c) represent a disjunction over two conditions. The aggregate
relation sets the parent node deterministically to the denotation of the child node. Nodes with
the special null predicate ? are represented as empty nodes.
making nested DRSs. It turns out that the abstraction operator is sufficient to obtain
the full representational power of DRT, and subsumes generalized quantification and
disjunction constructs in DRT. By analogy, we use the aggregate relation to handle
disjunction (Figure 5(c)) and generalized quantification (Section 2.5.6).
DCS restricted to join relations is less expressive than first-order logic because it
does not have universal quantification, negation, and disjunction. The aggregate rela-
tion is analogous to lambda abstraction, and in basic DCS we use the aggregate relation
to implement those basic constructs using higher-order predicates such as not, every,
and union. We can also express logical statements such as generalized quantification,
which go beyond first-order logic.
2.5 Semantics of DCS Trees with Mark?Execute (Full Version)
Basic DCS includes two types of relations, join and aggregate, but it is already quite
expressive. In general, however, it is not enough just to be able to express the meaning
of a sentence using some logical form; we must be able to derive the logical form
compositionally and simply from the sentence.
Consider the superlative constructionmost populous city, which has a basic syntactic
dependency structure shown in Figure 6(a). Figure 6(b) shows that we can in principle
402
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 6
Two semantically equivalent DCS trees are shown in (b) and (c). The DCS tree in (b), which uses
the join and aggregate relations in the basic DCS, does not align well with the syntactic structure
of most populous city (a), and thus is undesirable. The DCS tree in (c), by using the mark?execute
construct, aligns much better, with city rightfully dominating its modifiers. The full version of
DCS allows us to construct (c), which is preferable to (b).
already use a DCS tree with only join and aggregate relations to express the correct
semantics of the superlative construction. Note, however, that the two structures are
quite divergent?the syntactic head is city and the semantic head is argmax. This diver-
gence runs counter to a principal desideratum of DCS, which is to create a transparent
interface between coarse syntax and semantics.
In this section, we introduce mark and execute relations, which will allow us to
use the DCS tree in Figure 6(c) to represent the semantics associated with Figure 6(a);
these two are more similar than (a) and (b). The focus of this section is on this mark?
execute construct?usingmark and execute relations to give proper semantically scoped
denotations to syntactically scoped tree structures.
The basic intuition of the mark?execute construct is as follows: We mark a node
low in the tree with a mark relation; then, higher up in the tree, we invoke it with a
corresponding execute relation (Figure 7). For our example in Figure 6(c), we mark the
population node, which puts the child argmax in a temporary store; when we execute
the city node, we fetch the superlative predicate argmax from the store and invoke it.
This divergence between syntactic and semantic scope arises in other linguistic
contexts besides superlatives, such as quantification and negation. In each of these
cases, the general template is the same: A syntactic modifier low in the tree needs to
have semantic force higher in the tree. A particularly compelling case of this divergence
happenswith quantifier scope ambiguity (e.g., Some river traverses every city5), where the
5 The two meanings are: (i) there is a river x such that x traverses every city; and (ii) for every city x, some
river traverses x.
403
Computational Linguistics Volume 39, Number 2
Figure 7
The template for the mark?execute construct. A mark relation (one of E, Q, C) ?stores? the
modifier. Then an execute relation (of the form Xi for indices i) higher up ?recalls? the
modifier and applies it at the desired semantic point.
quantifiers appear in fixed syntactic positions, but the surface and inverse scope read-
ings correspond to different semantically scoped denotations. Analogously, a single syn-
tactic structure involving superlatives can also yield two different semantically scoped
denotations?the absolute and relative readings (e.g., state bordering the largest state6).
The mark?execute construct provides a unified framework for dealing all these forms
of divergence between syntactic and semantic scope. See Figures 8 and 9 for concrete
examples of this construct.
2.5.1 Denotations.We now formalize the mark?execute construct. We saw that the mark?
execute construct appears to act non-locally, putting things in a store and retrieving
them later. This means that if we want the denotation of a DCS tree to only depend
on the denotations of its subtrees, the denotations need to contain more than the set of
feasible values for the root node, as was the case for basic DCS. We need to augment de-
notations to include information about all marked nodes, because these can be accessed
by an execute relation higher up in the tree.
More specifically, let z be a DCS tree and d = zw be its denotation. The denotation
d consists of n columns. The first column always corresponds to the root node of z,
and the rest of the columns correspond to non-root marked nodes in z. In the example
in Figure 10, there are two columns, one for the root state node and the other for size
node, which is marked by C. The columns are ordered according to a pre-order traversal
of z, so column 1 always corresponds to the root node. The denotation d contains a set of
arrays d.A, where each array represents a feasible assignment of values to the columns
of d; note that we quantify over non-marked nodes, so they do not correspond to any
column in the denotation. For example, in Figure 10, the first array in d.A corresponds to
assigning (OK) to the state node (column 1) and (TX, 2.7e5) to the size node (column 2).
If there are no marked nodes, d.A is basically a set of tuples, which corresponds to a
denotation in basic DCS. For each marked node, the denotation d also maintains a store
6 The two meanings are: (i) a state that borders Alaska (which is the largest state); and (ii) a state with the
highest score, where the score of a state x is the maximum size of any state that x borders (Alaska is
irrelevant here because no states border it).
404
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 8
Examples of DCS trees that use the mark?execute construct with the E and Q mark relations.
(a) The head verb borders, which needs to be returned, has a direct object statesmodified by
which. (b) The quantifier no is syntactically dominated by state but needs to take wider scope.
(c) Two quantifiers yield two possible readings; we build the same basic structure, marking
both quantifiers; the choice of execute relation (X12 versus X21) determines the reading. (d) We
use two mark relations, Q on river for the negation, and E on city to force the quantifier to be
computed for each value of city.
with information to be retrieved when that marked node is executed. A store ? for a
marked node contains the following: (i) the mark relation ?.r (C in the example), (ii) the
base denotation ?.b, which essentially corresponds to denotation of the subtree rooted at
the marked node excluding the mark relation and its subtree (?size?w in the example),
and (iii) the denotation of the child of the mark relation (?argmax?w in the example).
The store of any unmarked nodes is always empty (? = ?).
Definition 3 (Denotations)
Let D be the set of denotations, where each denotation d ? D consists of
 a set of arrays d.A, where each array a = [a1, . . . , an] ? d.A is a sequence of
n tuples for some n ? 0; and
405
Computational Linguistics Volume 39, Number 2
Figure 9
Examples of DCS trees that use the mark?execute construct with the E and C relation. (a,b,c)
Comparatives and superlatives are handled as follows: For each value of the node marked
by E, we compute a number based on the node marked by C; based on this information,
a subset of the values is selected as the possible values of the root node. (d) Analog of quantifier
scope ambiguity for superlatives: The placement of the execute relation determines an absolute
versus relative reading. (e) Interaction between a quantifier and a superlative: The lower execute
relation computes the largest city for each state; the second execute relation invokes most and
enforces that the major constraint holds for the majority of states.
406
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 10
Example of the denotation for a DCS tree (with the compare relation C). This denotation has two
columns, one for each active node?the root node state and the marked node size.
 a sequence of n stores d.? = (d.?1, . . . , d.?n), where each store ? contains a
mark relation ?.r ? {E, Q, C, ?}, a base denotation ?.b ? D ? {?}, and a
child denotation ?.c ? D ? {?}.
Note that denotations are formally defined without reference to DCS trees (just as sets
of tuples were in basic DCS), but it is sometimes useful to refer to the DCS tree that
generates that denotation.
For notational convenience, we write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. Also let
d.ri = d.?i.r, d.bi = d.?i.b, and d.ci = d.?i.c. Let d{?i = x} be the denotation which is
identical to d, except with d.?i = x; d{ri = x}, d{bi = x}, and d{ci = x} are defined
analogously. We also define a project operation for denotations: ??A;???[i] def= ??{ai : a ?
A};?i??. Extending this notation further, we use ? to denote the indices of the non-initial
columns with empty stores (i > 1 such that d.?i = ?). We can then use d[??] to represent
projecting away the non-initial columns with empty stores. For the denotation d in
Figure 10, d[1] keeps column 1, d[??] keeps both columns, and d[2,?2] swaps the two
columns.
In basic DCS, denotations are sets of tuples, which works quite well for repre-
senting the semantics of wh-questions such as What states border Texas? But what about
polar questions such as Does Louisiana border Texas? The denotation should be a simple
Boolean value, which basic DCS does not represent explicitly. Using our new deno-
tations, we can represent Boolean values explicitly using zero-column structures: true
corresponds to a singleton set containing just the empty array (dT = ??{[ ]}??) and false
is the empty set (dF = ?????).
Having described denotations as n-column structures, we now give the formal
mapping from DCS trees to these structures. As in basic DCS, this mapping is defined
recursively over the structure of the tree. We have a recurrence for each case (the first
line is the base case, and each of the others handles a different edge relation):
?p?w = ??{[v] : v ? w(p)}; ??? [base case] (19)

?
p; e;
j
j? :c
?

w
= ?p; e?w 
??
j,j?
cw [join] (20)
?p; e;? :c?w = ?p; e?w 
??
?,? ?
(
cw
)
[aggregate] (21)
407
Computational Linguistics Volume 39, Number 2
?p; e; Xi :c?w = ?p; e?w 
??
?,? xi(cw) [execute] (22)
?p; e; E :c?w =M(?p; e?w, E, cw) [extract] (23)
?p; e; C :c?w =M(?p; e?w, C, cw) [compare] (24)
?p; Q :c; e?w =M(?p; e?w, Q, cw) [quantify] (25)
We define the operations ??
j,j?
,?,Xi, andM in the remainder of this section.
2.5.2 Base Case. Equation (19) defines the denotation for a DCS tree z with a single node
with predicate p. The denotation of z has one column whose arrays correspond to the
tuples w(p); the store for that column is empty.
2.5.3 Join Relations. Equation (20) defines the recurrence for join relations. On the left-
hand side,
?
p; e;
j
j? :c
?
is a DCS tree with p at the root, a sequence of edges e followed by
a final edge with relation
j
j? connected to a child DCS tree c. On the right-hand side, we
take the recursively computed denotation of ?p; e?, the DCS tree without the final edge,
and perform a join-project-inactive operation (notated ??
j,j?
) with the denotation of the
child DCS tree c.
The join-project-inactive operation joins the arrays of the two denotations (this is
the core of the join operation in basic DCS?see Equation (13)), and then projects away
the non-initial empty columns:7
??A;??? ??
j,j?
??A?;???? = ??A??;?+ ????[??],where (26)
A?? = {a+ a? : a ? A, a? ? A?, a1j = a?1j?}
We concatenate all arrays a ? A with all arrays a? ? A? that satisfy the join condition
a1j = a
?
1j? . The sequences of stores are simply concatenated: (?+ ?
?). Finally, any non-
initial columns with empty stores are projected away by applying ?[??].
Note that the join works on column 1; the other columns are carried along for the
ride. As another piece of convenient notation, we use ? to represent all components, so
???,? imposes the join condition that the entire tuple has to agree (a1 = a
?
1).
2.5.4 Aggregate Relations. Equation (21) defines the recurrence for aggregate relations.
Recall that in basic DCS, aggregate (16) simply takes the denotation (a set of tuples) and
puts it into a set. Now, the denotation is not just a set, so we need to generalize this
operation. Specifically, the aggregate operation applied to a denotation forms a set out
of the tuples in the first column for each setting of the rest of the columns:
? (??A;???) = ??A? ? A??;??? (27)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ?i ? {2, . . . ,n}, [ai] ? ?i.b.A[1],??a1, a ? A}
7 The join and project operations are taken from relational algebra.
408
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
The aggregate operation takes the set of arrays A and produces two sets of arrays, A?
andA??, which are unioned (note that the stores do not change). The setA? is the one that
first comes to mind: For every setting of a2, . . . , an, we construct S(a), the set of tuples a
?
1
in the first column which co-occur with a2, . . . , an in A.
There is another case, however: what happens to settings of a2, . . . , an that do not
co-occur with any value of a?1 in A? Then, S(a) = ?, but note that A
? by construction will
not have the desired array [?, a2, . . . , an]. As a concrete example, suppose A = ? and we
have one column (n = 1). Then A? = ?, rather than the desired {[?]}.
Fixing this problem is slightly tricky. There are an infinite number of a2, . . . , anwhich
do not co-occur with any a?1 inA, so for which ones do we actually include [?, a2, . . . , an]?
Certainly, the answer to this question cannot come from A, so it must come from the
stores. In particular, for each column i ? {2, . . . ,n}, we have conveniently stored a base
denotation ?i.b. We consider any ai that occurs in column 1 of the arrays of this base
denotation ([ai] ? ?i.b.A[1]). For this a2, . . . , an, we include [?, a2, . . . , an] in A?? as long as
a2, . . . , an does not co-occur with any a1. An example is given in Figure 11.
The reason for storing base denotations is thus partially revealed: The arrays rep-
resent feasible values of a CSP and can only contain positive information. When we
aggregate, we need to access possibly empty sets of feasible values?a kind of negative
information, which can only be recovered from the base denotations.
Figure 11
An example of applying the aggregate operation, which takes a denotation and aggregates the
values in column 1 for every setting of the other columns. The base denotations (b) are used to
put in {} for values that do not appear in A (in this example, AK, corresponding to the fact that
Alaska does not border any states).
409
Computational Linguistics Volume 39, Number 2
2.5.5 Mark Relations. Equations (23), (24), and (25) each processes a different mark
relation. We define a general mark operation, M(d, r, c) which takes a denotation d, a
mark relation r ? {E, Q, C} and a child denotation c, and sets the store of d in column 1
to be (r, d, c):
M(d, r, c) = d{r1 = r, b1 = d, c1 = c} (28)
The base denotation of the first column b1 is set to the current denotation d. This, in
some sense, creates a snapshot of the current denotation. Figure 12 shows an example
of the mark operation.
2.5.6 Execute Relations. Equation (22) defines the denotation of a DCS tree where the last
edge of the root is an execute relation. Similar to the aggregate case (21), we recurse on
the DCS tree without the last edge (?p; e?) and then join it to the result of applying the
execute operation Xi to the denotation of the child (cw).
The execute operation Xi is the most intricate part of DCS and is what does the
heavy lifting. The operation is parametrized by a sequence of distinct indices i that
specifies the order in which the columns should be processed. Specifically, i indexes into
the subsequence of columns with non-empty stores. We then process this subsequence
of columns in reverse order, where processing a column means performing some op-
erations depending on the stored relation in that column. For example, suppose that
columns 2 and 3 are the only non-empty columns. Then X12 processes column 3 before
column 2. On the other hand, X21 processes column 2 before column 3. We first define
Figure 12
An example of applying the mark operation, which takes a denotation and modifies the store of
the column 1. This information is used by other operations such as aggregate and execute.
410
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 13
An example of applying the execute operation on column 1 with the extract relation E. The
denotation prior to execution consists of two columns: column 1 corresponds to the border
node; column 2 to the state node. The join relations and predicates CA and state constrain the
arrays A in the denotation to include only the states that border California. After execution, the
non-marked column 1 is projected away, leaving only the state column with its store emptied.
the execute operation Xi for a single column i. There are three distinct cases, depending
on the relation stored in column i:
Extraction. For a denotation d with the extract relation E in column i, executing Xi(d)
involves three steps: (i) moving column i to before column 1 (?[i,?i]), (ii) projecting
away non-initial empty columns (?[??]), and (iii) removing the store (?{?1 = ?}):
Xi(d) = d[i,?i][??]{?1 = ?} if d.ri = E (29)
An example is given in Figure 13. There are two main uses of extraction.
1. By default, the denotation of a DCS tree is the set of feasible values of the
root node (which occupies column 1). To return the set of feasible values
of another node, we mark that node with E. Upon execution, the feasible
values of that node move into column 1. Extraction can be used to handle
in situ questions (see Figure 8(a)).
2. Unmarked nodes (those that do not have an edge with a mark relation) are
existentially quantified and have narrower scope than all marked nodes.
Therefore, we can make a node x have wider scope than another node y by
411
Computational Linguistics Volume 39, Number 2
marking x (with E) and executing y before x (see Figure 8(d,e) for
examples). The extract relation E (in fact, any mark relation) signifies
that we want to control the scope of a node, and the execute relation
allows us to set that scope.
Generalized Quantification.Generalized quantifiers are predicates on two sets, a restrictor
A and a nuclear scope B. For example,
w(some) = {(A,B) : A ? B > 0} (30)
w(every) = {(A,B) : A ? B} (31)
w(no) = {(A,B) : A ? B = ?} (32)
w(most) = {(A,B) : |A ? B| > 1
2
|A|} (33)
We think of the quantifier as amodifier which always appears as the child of a Q relation;
the restrictor is the parent. For example, in Figure 8(b), no corresponds to the quantifier
and state corresponds to the restrictor. The nuclear scope should be the set of all states
that Alaska borders. More generally, the nuclear scope is the set of feasible values of the
restrictor node with respect to the CSP that includes all nodes between the mark and
execute relations. The restrictor is also the set of feasible values of the restrictor node,
but with respect to the CSP corresponding to the subtree rooted at that node.8
We implement generalized quantifiers as follows: Let d be a denotation and suppose
we are executing column i. We first construct a denotation for the restrictor dA and a
denotation for the nuclear scope dB. For the restrictor, we take the base denotation in
column i (d.bi)?remember that the base denotation represents a snapshot of the restric-
tor node before the nuclear scope constraints are added. For the nuclear scope, we take
the complete denotation d (which includes the nuclear scope constraints) and extract
column i (d[i,?i][??]{?1 = ?}?see (29)). We then construct dA and dB by applying the
aggregate operation to each. Finally, we join these sets with the quantifier denotation,
stored in d.ci:
xi(d) =
((
d.ci 
??
1,1 dA
)
??2,1 dB
)
[?1] if d.ri = Q,where (34)
dA = ? (d.bi) (35)
dB = ? (d[i,?i][??]{?1 = ?}) (36)
When there is one quantifier, think of the execute relation as performing a syntactic
rewriting operation, as shown in Figure 14(b). For more complex cases, we must defer
to (34).
Figure 8(c) shows an example with two interacting quantifiers. The denotation of
the DCS tree before execution is the same in both readings, as shown in Figure 15. The
8 Defined this way, we can only handle conservative quantifiers, because the nuclear scope will always be
a subset of the restrictor. This design decision is inspired by DRT, where it provides a way of modeling
donkey anaphora. We are not treating anaphora in this work, but we can handle it by allowing pronouns
in the nuclear scope to create anaphoric edges into nodes in the restrictor. These constraints naturally
propagate through the nuclear scope?s CSP without affecting the restrictor.
412
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 14
(a) An example of applying the execute operation on column iwith the quantify relation Q.
Before executing, note that A = {} (because Alaska does not border any states). The restrictor (A)
is the set of all states, and the nuclear scope (B) is empty. Because the pair (A,B) does exist in
w(no), the final denotation is ??{[ ]}?? (which represents true). (b) Although the execute operation
actually works on the denotation, think of it in terms of expanding the DCS tree. We introduce
an extra projection relation [?1], which projects away the first column of the child subtree?s
denotation.
quantifier scope ambiguity is resolved by the choice of execute relation: X12 gives the
surface scope reading, X21 gives the inverse scope reading.
Figure 8(d) shows how extraction and quantification work together. First, the no
quantifier is processed for each city, which is an unprocessed marked node. Here, the
extract relation is a technical trick to give city wider scope.
Comparatives and Superlatives. Comparative and superlative constructions involve com-
paring entities, and for this we rely on a set S of entity?degree pairs (x, y), where x is an
Figure 15
Denotation of Figure 8(c) before the execute relation is applied.
413
Computational Linguistics Volume 39, Number 2
entity and y is a numeric degree. Recall that we can treat S as a function, which maps
an entity x to the set of degrees S(x) associated with x. Note that this set can contain
multiple degrees. For example, in the relative reading of state bordering the largest state,
we would have a degree for the size of each neighboring state.
Superlatives use the argmax and argmin predicates, which are defined in Section 2.3.
Comparatives use the more and less predicates: w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; w(less) is defined analogously:
w(more) = {(S, x, y) : max S(x) > max S(y)} (37)
w(less) = {(S, x, y) : min S(x) < min S(y)} (38)
We use the same mark relation C for both comparative and superlative construc-
tions. In terms of the DCS tree, there are three key parts: (i) the root x, which corresponds
to the entity to be compared, (ii) the child c of a C relation, which corresponds to the
comparative or superlative predicate, and (iii) c?s parent p, which contains the ?degree
information? (which will be described later) used for comparison. We assume that the
root is marked (usually with a relation E). This forces us to compute a comparison
degree for each value of the root node. In terms of the denotation d corresponding to the
DCS tree prior to execution, the entity to be compared occurs in column 1 of the arrays
d.A, the degree information occurs in column i of the arrays d.A, and the denotation of
the comparative or superlative predicate itself is the child denotation at column i (d.ci).
First, we define a concatenating function +i (d), which combines the columns i of d
by concatenating the corresponding tuples of each array in d.A:
+i (??A;???) = ??A?;????, where (39)
A? = {a(1...i1 )\i+ [ai1 + ? ? ?+ ai|i| ]+ a(i1...n)\i : a ? A}
?? = ?(1...i1 )\i+ [?i1 ]+ ?(i1...n)\i
Note that the store of column i1 is kept and the others are discarded. As an example:
+2,1 (??{[(1), (2), (3)], [(4), (5), (6)]};?1,?2,?3??) = ??{[(2, 1), (3)], [(5, 4), (6)]};?2,?3??
(40)
We first create a denotation d? where column i, which contains the degree infor-
mation, is extracted to column 1 (and thus column 2 corresponds to the entity to be
compared). Next, we create a denotation dS whose column 1 contains a set of entity-
degree pairs. There are two types of degree information:
1. Suppose the degree information has arity 2 (ARITY(d.A[i]) = 2). This
occurs, for example, in most populous city (see Figure 9(b)), where column i
is the population node. In this case, we simply set the degree to the
second component of population by projection (???w 
??
1,2 d
?). Now
columns 1 and 2 contain the degrees and entities, respectively. We
concatenate columns 2 and 1 (+2,1 (?)) and aggregate to produce a
denotation dS which contains the set of entity?degree pairs in column 1.
2. Suppose the degree information has arity 1 (ARITY(d.A[i]) = 1). This
occurs, for example, in state bordering the most states (see Figure 9(a)), where
414
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
column i is the lower marked state node. In this case, the degree of an
entity from column 2 is the number of different values that column 1 can
take. To compute this, aggregate the set of values (?
(
d?
)
) and apply the
count predicate. Now with the degrees and entities in columns 1 and 2,
respectively, we concatenate the columns and aggregate again to obtain dS.
Having constructed dS, we simply apply the comparative/superlative predicate which
has been patiently waiting in d.ci. Finally, the store of d?s column 1 was destroyed by the
concatenation operation+2,1 (() ?), so wemust restore it with ?{?1 = d.?1}. The complete
operation is as follows:
xi(d) =
(
???w 
??
1,2
(
d.ci 
??
1,1 dS
))
{?1 = d.?1} if d.?i = C, d.?1 = ?, where (41)
dS =
?
?
?
?
(
+2,1
(
???w 
??
1,2 d
?
))
if ARITY(d.A[i]) = 2
?
(
+2,1
(
???w 
??
1,2
(
?count?w 
??
1,1 ?
(
d?
)
)))
if ARITY(d.A[i]) = 1
(42)
d? = d[i,?i][??]{?1 = ?} (43)
An example of executing the C relation is shown in Figure 16(a). As with executing a
Q relation, for simple cases we can think of executing a C relation as expanding a DCS
tree, as shown in Figure 16(b).
Figure 9(a) and Figure 9(b) show examples of superlative constructions with the ar-
ity 1 and arity 2 types of degree information, respectively. Figure 9(c) shows an example
of a comparative construction. Comparatives and superlatives use the same machinery,
differing only in the predicate: argmax versus ?more; 31 :TX? (more than Texas). But both
predicates have the same template behavior: Each takes a set of entity?degree pairs and
returns any entity satisfying some property. For argmax, the property is obtaining the
highest degree; for more, it is having a degree higher than a threshold. We can handle
generalized superlatives (the five largest or the fifth largest or the 5% largest) as well by
swapping in a different predicate; the execution mechanisms defined in Equation (41)
remain the same.
We saw that the mark?execute machinery allows decisions regarding quantifier
scope to be made in a clean and modular fashion. Superlatives also have scope am-
biguities in the form of absolute versus relative readings. Consider the example in
Figure 9(d). In the absolute reading, we first compute the superlative in a narrow scope
(the largest state is Alaska), and then connect it with the rest of the phrase, resulting in
the empty set (because no states border Alaska). In the relative reading, we consider the
first state as the entity we want to compare, and its degree is the size of a neighboring
state. In this case, the lower state node cannot be set to Alaska because there are no
states bordering it. The result is therefore any state that borders Texas (the largest state
that does have neighbors). The two DCS trees in Figure 9(d) show that we can naturally
account for this form of superlative ambiguity based on where the scope-determining
execute relation is placed without drastically changing the underlying tree structure.
Remarks. These scope divergence issues are not specific to DCS?every serious semantic
formalism must address them. Generative grammar uses quantifier raising to move the
quantifier from its original syntactic position up to the desired semantic position before
semantic interpretation even occurs (Heim and Kratzer 1998). Other mechanisms such
415
Computational Linguistics Volume 39, Number 2
Figure 16
(a) Executing the compare relation C for an example superlative construction (relative reading
of state bordering the largest state from Figure 9(d)). Before executing, column 1 contains the
entity to compare, and column 2 contains the degree information, of which only the second
component is relevant. After executing, the resulting denotation contains a single column with
only the entities that obtain the highest degree (in this case, the states that border Texas). (b) For
this example, think of the execute operation as expanding the original DCS tree, although the
execute operation actually works on the denotation, not the DCS tree. The expanded DCS tree
has the same denotation as the original DCS tree, and syntactically captures the essence of the
execute?compare operation. Going through the relations of the expanded DCS tree from
bottom to top: The X2 relation swaps columns 1 and 2; the join relation keeps only the second
component ((TX, 267K) becomes (267K)); +2,1 concatenates columns 2 and 1 ([(267K), (AR)]
becomes [(AR, 267K)]); ? aggregates these tuples into a set; argmax operates on this set and
returns the elements.
as Montague?s (1973) quantifying in, Cooper storage (Cooper 1975), and Carpenter?s
(1998) scoping constructor handle scope divergence during semantic interpretation.
Roughly speaking, these mechanisms delay application of a quantifier, ?marking? its
spot with a dummy pronoun (as inMontague?s quantifying in) or putting it in a store (as
in Cooper storage), and then ?executing? the quantifier at a later point in the derivation
either by performing a variable substitution or retrieving it from the store. Continuation,
from programming languages, is another solution (Barker 2002; Shan 2004); this sets the
semantics of a quantifier to be a function from its continuation (which captures all the
semantic content of the clause minus the quantifier) to the final denotation of the clause.
416
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Intuitively, continuations reverse the normal evaluation order, allowing a quantifier to
remain in situ but still outscope the rest of the clause. In fact, the mark and execute
relations of DCS are analogous to the shift and reset operators used in continuations.
One of the challenges with allowing flexible scope is that free variables can yield invalid
scopings, a well-known issuewith Cooper storage that the continuation-based approach
solves. Invalid scopings are filtered out by the construction mechanism (Section 2.6).
One difference between mark?execute in DCS and many other mechanisms is that
DCS trees (which contain mark and execute relations) are the final logical forms?the
handling of scope divergence occurs in the computing their denotations. The analog
in the other mechanisms resides in the construction mechanism?the actually final
logical form is quite simple.9 Therefore, we have essentially pushed the inevitable
complexity from the construction mechanism into the semantics of the logical form.
This is a conscious design decision: We want our construction mechanism, which maps
natural language to logical form, to be simple and not burdened with complex linguistic
issues, for our focus is on learning this mapping. Unfortunately, the denotation of our
logical forms (Section 2.5.1) do become more complex than those of lambda calculus
expressions, but we believe this is a reasonable tradeoff to make for our particular
application.
2.6 Construction Mechanism
We have thus far defined the syntax (Section 2.2) and semantics (Section 2.5) of DCS
trees, but we have only vaguely hinted at how these DCS trees might be connected
to natural language utterances by appealing to idealized examples. In this section, we
formally define the construction mechanism for DCS, which takes an utterance x and
produces a set of DCS trees ZL(x).
Because wemotivated DCS trees based on dependency syntax, it might be tempting
to take a dependency parse tree of the utterance, replace the words with predicates, and
attach some relations on the edges to produce a DCS tree. To a first approximation, this
is what we will do, but we need to be a bit more flexible for several reasons: (i) some
nodes in the DCS tree do not have predicates (e.g., children of an E relation or parent
of an Xi relation); (ii) nodes have predicates that do not correspond to words (e.g., in
California cities, there is a implicit loc predicate that bridges CA and city); (iii) some
words might not correspond to any predicates in our world (e.g., please); and (iv) the
DCS tree might not always be aligned with the syntactic structure depending on which
syntactic formalism one ascribes to. Although syntax was the inspiration for the DCS
formalism, we will not actually use it in construction.
It is also worth stressing the purpose of the construction mechanism. In linguistics,
the purpose of the construction mechanism is to try to generate the exact set of valid
logical forms for a sentence. We view the construction mechanism instead as simply a
way of creating a set of candidate logical forms. A separate step defines a distribution
over this set to favor certain logical forms over others. The construction mechanism
should therefore simply overapproximate the set of logical forms. Linguistic constraints
that are normally encoded in the construction mechanism (for example, in CCG, that
the disharmonic pair S/NP and S\NP cannot be coordinated, or that non-indefinite
quantifiers cannot extend their scope beyond clause boundaries) would be instead
9 In the continuation-based approach, this difference corresponds to the difference between assigning a
denotational versus an operational semantics.
417
Computational Linguistics Volume 39, Number 2
encoded as features (Section 3.1.1). Because feature weights are estimated from data,
one can view our approach as automatically learning the linguistic constraints relevant
to our end task.
2.6.1 Lexical Triggers. The construction mechanism assumes a fixed set of lexical triggers
L. Each trigger is a pair (s, p), where s is a sequence of words (usually one) and p is a
predicate (e.g., s = California and p = CA). We use L(s) to denote the set of predicates p
triggered by s ((s, p) ? L). We should think of the lexical triggers L not as pinning down
the precise predicate for each word, but rather as producing an overapproximation.
For example, Lmight contain {(city, city), (city, state), (city, river), . . . }, reflecting our
initial ignorance prior to learning.
We also define a set of trace predicates L(	), which can be introduced without an
overt lexical element. Their name is inspired by trace/null elements in syntax, but they
serve a more practical rather than a theoretical role here. As we shall see in Section 2.6.2,
trace predicates provide more flexibility in the construction of logical forms, allowing
us to insert a predicate based on the partial logical form constructed thus far and assess
its compatibility with the words afterwards (based on features), rather than insisting on
a purely lexically driven formalism. Section 4.1.3 describes the lexical triggers and trace
predicates that we use in our experiments.
2.6.2 Recursive Construction of DCS Trees. Given a set of lexical triggers L, we will now
describe a recursive mechanism for mapping an utterance x = (x1, . . . , xn) to ZL(x), a
set of candidate DCS trees for x. The basic approach is reminiscent of projective labeled
dependency parsing: For each span i..j of the utterance, we build a set of trees Ci,j(x).
The set of trees for the span 0..n is the final result:
ZL(x) = C0,n(x) (44)
Each set of DCS trees Ci,j(x) is constructed recursively by combining the trees of its
subspans Ci,k(x) and Ck?,j(x) for each pair of split points k, k
? (words between k and k?
are ignored). These combinations are then augmented via a function A and filtered via a
function F; these functions will be specified later. Formally, Ci,j(x) is defined recursively
as follows:
Ci,j(x) = F
(
A
(
{?p?i..j : p ? L(xi+1..j)} ?
?
i?k?k?<j
a?Ci,k(x)
b?Ck? ,j(x)
T1(a, b))
))
(45)
This recurrence has two parts:
 The base case: we take the phrase (sequence of words) over span i..j
and look up the set of predicates p in the set of lexical triggers. For each
predicate, we construct a one-node DCS tree. We also extend the definition
of DCS trees in Section 2.2 to allow each node to store the indices of the
span i..j that triggered the predicate at that node; this is denoted by ?p?i..j.
This span information will be useful in Section 3.1.1, where we will need
to talk about how an utterance x is aligned with a DCS tree z.
418
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
 The recursive case: T1(a, b), which we will define shortly, that takes two
DCS trees, a and b, and returns a set of new DCS trees formed by
combining a and b. Figure 17 shows this recurrence graphically.
We now focus on how to combine two DCS trees. Define Td(a, b) as the set of DCS
trees that result by making either a or b the root and connecting the other via a chain of
relations and at most d trace predicates (d is a small integer that keeps the set of DCS
trees manageable):
Td(a, b) = T
?
d
(a, b) ? T?
d
(b, a) (46)
Here, T
?
d
(a, b) is the set of DCS trees where a is the root; for T
?
d
(a, b), b is the root. The
former is defined recursively as follows:
T
?
0 (a, b) = ?, (47)
T
?
d
(a, b) =
?
r?R
p?L()
{?a.p; a.e; r :b? , ?a.p; a.e; r :?? :b??} ? T?
d?1(a, ?p; r :b?)
First, we consider all possible relations r ? R and try appending an edge to a with
relation r and child b (?a.p; a.e; r :b?); an aggregate relation ? can be inserted in addition
(?a.p; a.e; r :?? :b??). Of course, R contains an infinite number of join and execute rela-
tions, but only a small finite number of them make sense: We consider join relations
j
j? only for j ? {1, . . . , ARITY(a.p)} and j
? ? {1, . . . , ARITY(b.p)}, and execute relations Xi
for which i does not contain indices larger than the number of columns of bw. Next,
we further consider all possible trace predicates p ? L(	), and recursively try to connect
Figure 17
An example of the recursive construction of Ci,j(x), a set of DCS trees for span i..j.
419
Computational Linguistics Volume 39, Number 2
Figure 18
Given two DCS trees, a and b, T
?
1 (a, b) and T
?
1 (a, b) are the two sets of DCS trees formed by
combining a and bwith a at the root and b at the root, respectively; one trace predicate can be
inserted in between. In this example, the DCS trees which survive filtering (Section 2.6.3)
are shown.
awith the intermediate ?p; r :b?, now allowing d? 1 additional predicates. See Figure 18
for an example. In the other direction, T
?
d
is defined similarly:
T
?
0 (a, b) = ? (48)
T
?
d
(a, b) =
?
r?R
p?L()
{?b.p; r :a; b.e? , ?b.p; r :?? :a? ; b.e?} ? T?
d?1(a, ?p; r :b?)
Inserting trace predicates allows us to build logical forms with more predicates
than are explicitly triggered by the words. This ability is useful for several reasons.
Sometimes, there is a predicate not overtly expressed, especially in noun compounds
(e.g., California cities). For semantically light words such as prepositions (e.g., for) it is
difficult to enumerate all the possible predicates that they might trigger; it is simpler
computationally to try to insert trace predicates. We can even omit lexical triggers
for transitive verbs such as border because the corresponding predicate border can be
inserted as a trace predicate.
The function T1(a, b) connects two DCS trees via a path of relations and trace predi-
cates. The augmentation function A adds additional relations (specifically, E and/or Xi)
on a single DCS tree:
A(Z) =
?
z?Z
Xi?R
{z, ?z; E :???? , ?Xi :z? , ?Xi :?z; E :?????} (49)
2.6.3 Filtering using Abstract Interpretation. The construction procedure as described thus
far is extremely permissive, generating many DCS trees which are obviously wrong?
for example, ?state; 11 :?>;
2
1 ?3???, which tries to compare a state with the number 3. There
is nothing wrong with this expression syntactically: Its denotation will simply be empty
(with respect to the world). But semantically, this DCS tree is anomalous.
We cannot simply just discard DCS trees with empty denotations, because we
would incorrectly rule out ?state; 11 :?border;
2
1 ?AK???. The difference here is that even
though the denotation is empty in this world, it is possible that it might not be empty
420
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
in a different world where history and geology took another turn, whereas it is simply
impossible to compare cities and numbers.
Now let us quickly flesh out this intuition before falling into a philosophical dis-
cussion about possible worlds. Given a world w, we define an abstract world ?(w),
to be described shortly. We compute the denotation of a DCS tree z with respect to
this abstract world. If at any point in the computation we create an empty denotation,
we judge z to be impossible and throw it away. The filtering function F is defined as
follows:10
F(Z) = {z ? Z : ?z? subtree of z , z??(w).A = ?} (50)
Now we need to define the abstract world ?(w). The intuition is to map concrete
values to abstract values: 3 :length becomes ? :length,Oregon :state becomes ? :state,
and in general, primitive value x : t becomes ? : t. We perform abstraction on tuples
componentwise, so that (Oregon :state, 3 :length) becomes (? :state, ? :length). Our
abstraction of sets is slightly more complex: The empty set maps to the empty set, a set
containing values all with the same abstract value a maps to {a}, and a set containing
values with more than one abstract value maps to {MIXED}. Finally, a world maps each
predicate onto a set of (concrete) tuples; the corresponding abstract world maps each
predicate onto the set of abstract tuples. Formally, the abstraction function is defined as
follows:
?(x : t) = ? : t [primitive value] (51)
?((v1, . . . , vn)) = (?(v1), . . . ,?(vn)) [tuple] (52)
?(A) =
?
?
?
?
?
? if A = ?
{?(x) : x ? A} if |{?(x) : x ? A}| = 1
{MIXED} otherwise
[set] (53)
?(w) = ?p.{?(x) : x ? w(p)} [world] (54)
As an example, the abstract world might look like this:
?(w)(>) = {(? :number, ? :number, ? :number) (55)
(? :length, ? :length, ? :length), . . . }
?(w)(state) = {(? :state)} (56)
?(w)(AK) = {(? :state)} (57)
?(w)(border) = {(? :state, ? :state)} (58)
Now returning to our motivating example at the beginning of this section, we see
that the bad DCS tree has an empty abstract denotation ?state; 11 :?>;
2
1 ?3????(w) =
???; ???. The good DCS tree has a non-empty abstract denotation: ?state; 11 :?border;
2
1 ?AK????(w) = ??{(? :state)}; ???, as desired.
10 To further reduce the search space, F imposes a few additional constraints: for example, limiting the
number of columns to 2, and only allowing trace predicates between arity 1 predicates.
421
Computational Linguistics Volume 39, Number 2
Remarks. Computing denotations on an abstract world is called abstract interpretation
(Cousot and Cousot 1977) and is a very powerful framework commonly used in the
programming languages community. The idea is to obtain information about a program
(in our case, a DCS tree) without running it concretely, but rather just by running it
abstractly. It is closely related to type systems, but the type of abstractions one uses is
often much richer than standard type systems.
2.6.4 Comparison with CCG. We now compare our construction mechanism with CCG
(see Figure 19 for an example). The main difference is that our lexical triggers contain
less information than a lexical entry in a CCG. In CCG, the lexicon would have an entry
such as
major  N/N : ?f.?x.major(x) ? f (x) (59)
which gives detailed information about how this word should interact with its context.
In DCS construction, however, each lexical trigger only has the minimal amount of
information:
major  major (60)
A lexical trigger specifies a pre-theoretic ?meaning? of a word which does not commit
to any formalisms. One advantage of this minimality is that lexical triggers could be
easily obtained from non-expert supervision: One would only have to associate words
with database table names (predicates).
In some sense, the DCS construction mechanism pushes the complexity out of the
lexicon. In linguistics, this complexity usually would end up in the grammar, which
would be undesirable. We do not have to respect this tradeoff, however, because the
Figure 19
Comparison between the construction mechanisms of CCG and DCS. There are three principal
differences: First, in CCG, words are mapped onto lambda calculus expressions; in DCS, words
are just mapped onto predicates. Second, in CCG, lambda calculus expressions are built by
combining (e.g., via function application) two smaller expressions; in DCS, trees are combined
by inserting relations (and possibly other predicates between them). Third, in CCG, all words
map to logical expressions; in DCS, only a small subset of words (e.g., state and Texas) map to
predicates; the rest participate in features for scoring DCS trees.
422
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
construction mechanism only produces an overapproximation, which means it is possi-
ble to have both a simple ?lexicon? and a simple ?grammar.?
There is an important practical rationale for this design decision. During learning,
we never just have one clean lexical entry per word. Rather, there are often many
possible lexical entries (and to handle disfluent utterances or utterances in free word-
order languages, we might actually need many of them [Kwiatkowski et al 2010]):
major  N : ?x.major(x) (61)
major  N/N : ?f.?x.major(x) ? f (x) (62)
major  N\N : ?f.?x.major(x) ? f (x) (63)
. . . (64)
Now think of a DCS lexical trigger major  major as simply a compact representation for
a set of CCG lexical entries. Furthermore, the choice of the lexical entry is made not
at the initial lexical base case, but rather during the recursive construction by inserting
relations between DCS subtrees. It is exactly at this point that the choice can be made,
because after all, the choice is one that depends on context. The general principle is to
compactly represent the indeterminacy until one can resolve it. Compactly representing
a set of CCG lexical entries can also be done within the CCG framework by factoring
lexical entries into a lexeme and a lexical template (Kwiatkowski et al 2011).
Type raising is a combinator in CCG that traditionally converts x to ?f.f (x). In
recent work, Zettlemoyer and Collins (2007) introduced more general type-changing
combinators to allow conversion from one entity into a related entity in general (a
kind of generalized metonymy). For example, in order to parse Boston flights, Boston
is transformed to ?x.to(x, Boston). This type changing is analogous to inserting trace
predicates in DCS, but there is an important distinction: Type changing is a unary
operation and is unconstrained in that it changes logical forms into new ones without
regard for how they will be used downstream. Inserting trace predicates is a binary
operation that is constrained by the two predicates that it is mediating. In the example,
to would only be inserted to combine Boston with flight. This is another instance of
the general principle of delaying uncertain decisions until there is more information.
3. Learning
In Section 2, we defined DCS trees and a construction mechanism for producing a set
of candidate DCS trees given an utterance. We now define a probability distribution
over that set (Section 3.1) and an algorithm for estimating the parameters (Section 3.2).
The number of candidate DCS trees grows exponentially, so we use beam search to
control this growth. The final learning algorithm alternates between beam search and
optimization of the parameters, leading to a natural bootstrapping procedure which
integrates learning and search.
3.1 Semantic Parsing Model
The semantic parsing model specifies a conditional distribution over a set of candi-
date DCS trees C(x) given an utterance x. This distribution depends on a function
?(x, z) ? Rd, which takes a (x, z) pair and extracts a set of local features (see Section 3.1.1
423
Computational Linguistics Volume 39, Number 2
for a full specification). Associated with this feature vector is a parameter vector ? ? Rd.
The inner product between the two vectors, ?(x, z)?, yields a numerical score, which
intuitively measures the compatibility of the utterance x with the DCS tree z. We expo-
nentiate the score and normalize over C(x) to obtain a proper probability distribution:
p(z | x;C,?) = exp{?(x, z)??A(?; x,C)} (65)
A(?; x,C) = log
?
z?C(x)
exp{?(x, z)?} (66)
where A(?; x,C) is the log-partition function with respect to the candidate set function
C(x).
3.1.1 Features.We now define the feature vector ?(x, z) ? Rd, the core part of the seman-
tic parsing model. Each component j = 1, . . . , d of this vector is a feature, and ?(x, z)j
is the number of times that feature occurs in (x, z). Rather than working with indices,
we treat features as symbols (e.g., TRIGGERPRED[states, state]). Each feature captures
some property about (x, z) that abstracts away from the details of the specific instance
and allows us to generalize to new instances that share common features.
The features are organized into feature templates, where each feature template
instantiates a set of features. Figure 20 shows all the feature templates for a concrete
example. The feature templates are as follows:
 PREDHIT contains the single feature PREDHIT, which fires for each
predicate in z.
 PRED contains features {PRED[?(p)] : p ? P}, each of which fires on
?(p), the abstraction of predicate p, where
?(p) =
{
? : t if p = x : t
p otherwise
(67)
The purpose of the abstraction is to abstract away the details of concrete
values such as TX = Texas :state.
 PREDREL contains features {PREDREL[?(p),q] : p ? P ,q ? ({?,?}?
R)?}. A feature fires when a node x has predicate p and is connected via
some path q = (d1, r1), . . . , (dm, rm) to the lowest descendant node ywith
the property that each node between x and y has a null predicate. Each
(d, r) on the path represents an edge labeled with relation r connecting
to a left (d =?) or right (d =?) child. If x has no children, then m = 0.
The most common case is when m = 1, but m = 2 also occurs with the
aggregate and execute relations (e.g., PREDREL[count,? 11? ?] fires
for Figure 5(a)).
 PREDRELPRED contains features {PREDRELPRED[?(p),q,?(p?)] : p, p? ?
P ,q ? ({?,?}?R)?}, which are the same as PREDREL, except that we
include both the predicate p of x and the predicate p? of the descendant
node y. These features do not fire if m = 0.
424
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 20
For each utterance?DCS tree pair (x, z), we define a feature vector ?(x, z), whose j-th component
is the number of times a feature j occurs in (x, z). Each feature has an associated parameter ?j,
which is estimated from data in Section 3.2. The inner product of the feature vector and
parameter vector yields a compatibility score.
 TRIGGERPRED contains features {TRIGGERPRED[s, p] : s ?W?, p ? P},
whereW = {it,Texas, . . . } is the set of words. Each of these features fires
when a span of the utterance with words s triggers the predicate p?more
precisely, when a subtree ?p; e?i..j exists with s = xi+1..j. Note that these
lexicalized features use the predicate p rather than the abstracted
version ?(p).
 TRACEPRED contains features {TRACEPRED[s, p, d] : s ?W?, p ? P , d ?
{?,?}}, each of which fires when a trace predicate p has been inserted
425
Computational Linguistics Volume 39, Number 2
over a word s. The situation is the following: Suppose we have a subtree
a that ends at position k (there is a predicate in a that is triggered by a
phrase with right endpoint k) and another subtree b that begins at k?.
Recall that in the construction mechanism (46), we can insert a trace
predicate p ? L(	) between the roots of a and b. Then, for every word
xj between the spans of the two subtrees ( j = {k+ 1, . . . , k?}), the
feature TRACEPRED[xj, p, d] fires (d =? if b dominates a and d =?
if a dominates b).
 TRACEREL contains features {TRACEREL[s, d, r] : s ?W?, d ? {?,?}, r ?
R}, each of which fires when some trace predicate with parent relation r
has been inserted over a word s.
 TRACEPREDREL contains features {TRACEPREDREL[s, p, d, r] : s ?W?,
p ? P , d ? {?,?}, r ? R}, each of which fires when a predicate p is
connected via child relation r to some trace predicate over a word s.
These features are simple generic patterns which can be applied for modeling
essentially any distribution over sequences and labeled trees?there is nothing spe-
cific to DCS at all. The first half of the feature templates (PREDHIT, PRED, PREDREL,
PREDRELPRED) capture properties of the tree independent of the utterance, and
are similar to those used for syntactic dependency parsing. The other feature tem-
plates (TRIGGERPRED, TRACEPRED, TRACEREL, TRACEPREDREL) connect predicates
in the DCS tree with words in the utterance, similar to those in a model of machine
translation.
3.2 Parameter Estimation
We have now fully specified the details of the graphical model in Figure 2: Section 3.1
described semantic parsing and Section 2 described semantic evaluation. Next, we focus
on the inferential problem of estimating the parameters ? of the model from data.
3.2.1 Objective Function.We assume that our learning algorithm is given a training data
setD containing question?answer pairs (x, y). Because the logical forms are unobserved,
we work with log p(y | x;C,?), the marginal log-likelihood of obtaining the correct
answer y given an utterance x. This marginal log-likelihood sums over all z ? C(x) that
evaluate to y:
log p(y | x;C,?) = log p(z ? Cy(x) | x;C,?) (68)
= A(?; x,Cy)?A(?, x,C), where (69)
Cy(x)
def
= {z ? C(x) : zw = y} (70)
Here, Cy(x) is the set of DCS trees z with denotation y.
We call an example (x, y) ? D feasible if the candidate set of x contains a DCS
tree that evaluates to y (Cy(x) = ?). Define an objective function O(?,C) containing
two terms. The first term is the sum of the marginal log-likelihood over all feasible
426
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
training examples. The second term is a quadratic penalty on the parameters ? with
regularization parameter ?. Formally:
O(?,C) def=
?
(x,y)?D
Cy(x)=?
log p(y | x;C,?)? ?
2
???22 (71)
=
?
(x,y)?D
Cy(x)=?
(A(?; x,Cy)?A(?; x,C))? ?
2
???22
We would like to maximize O(?,C). The log-partition function A(?; ?, ?) is convex,
but O(?,C) is the difference of two log-partition functions and hence is not concave
(nor convex). Thus we resort to gradient-based optimization. A standard result is that
the derivative of the log-partition function is the expected feature vector (Wainwright
and Jordan 2008). Using this, we obtain the gradient of our objective function:11
?O(?,C)
??
=
?
(x,y)?D
Cy(x)=?
(
Ep(z|x;Cy,?)[?(x, z)]? Ep(z|x;C,?)[?(x, z)]
)
? ?? (72)
Updating the parameters in the direction of the gradient would move the parameters
towards the DCS trees that yield the correct answer (Cy) and away from overall can-
didate DCS trees (C). We can use any standard numerical optimization algorithm that
requires only black-box access to a gradient. Section 4.3.4 will discuss the empirical
ramifications of the choice of optimization algorithm.
3.2.2 Algorithm. Given a candidate set function C(x), we can optimize Equation (71) to
obtain estimates of the parameters ?. Ideally, we would use C(x) = ZL(x), the candidate
sets from our construction mechanism in Section 2.6, but we quickly run into the prob-
lem of computing Equation (72) efficiently. Note that ZL(x) (defined in Equation (44))
grows exponentially with the length of x. This by itself is not a show-stopper. Our
features (Section 3.1.1) decompose along the edges of the DCS tree, so it is possible
to use dynamic programming12 to compute the second expectation Ep(z|x;ZL,?)[?(x, z)]
of Equation (72). The problem is computing the first expectation Ep(z|x;ZyL ,?)
[?(x, z)],
which sums over the subset of candidate DCS trees z satisfying the constraint zw = y.
Though this is a smaller set, there is no efficient dynamic program for this set because
the constraint does not decompose along the structure of the DCS tree. Therefore, we
need to approximate ZyL , and, in fact, we will approximate ZL as well so that the two
expectations in Equation (72) are coherent.
Recall that ZL(x) was built by recursively constructing a set of DCS trees Ci,j(x)
for each span i..j. In our approximation, we simply use beam search, which truncates
each Ci,j(x) to include the (at most) K DCS trees with the highest score ?(x, z)
?. We
11 Notation: Ep(x)[f (x)] =
?
x p(x)f (x).
12 The state of the dynamic program would be the span i..j and the head predicate over that span.
427
Computational Linguistics Volume 39, Number 2
let C?i,j,?(x) denote this approximation and define the set of candidate DCS trees with
respect to the beam search:
Z?L,?(x) = C?0,n,?(x) (73)
We now have a chicken-and-egg problem: If we had good parameters ?, we
could generate good candidate sets C(x) using beam search Z?L,?(x). If we had good
candidate sets C(x), we could generate good parameters by optimizing our objective
O(?,C) in Equation (71). This problem leads to a natural solution: simply alternate
between the two steps (Figure 21). This procedure is not guaranteed to converge, due
to the heuristic nature of the beam search, but we have found it to be convergent in
practice.
Finally, we use the trained model with parameters ? to answer new questions x by
choosing the most likely answer y, summing out the latent logical form z:
F?(x)
def
= argmax
y
p(y | x;?, Z?L,?) (74)
= argmax
y
?
z?Z?L,?(x)
zw=y
p(z | x;?, Z?L,?) (75)
4. Experiments
We have now completed the conceptual part of this article?using DCS trees to rep-
resent logical forms (Section 2), and learning a probabilistic model over these trees
(Section 3). In this section, we evaluate and study our approach empirically. Our
main result is that our system can obtain comparable accuracies to state-of-the-art
systems that require annotated logical forms. All the code and data are available at
cs.stanford.edu/~pliang/software/.
4.1 Experimental Set-up
We first describe the data sets (Section 4.1.1) that we use to train and evaluate our
system. We then mention various choices in the model and learning algorithm (Sec-
tion 4.1.2). One of these choices is the lexical triggers, which are further discussed in
Section 4.1.3.
Figure 21
The learning algorithm alternates between updating the candidate sets based on beam search
and updating the parameters using standard numerical optimization.
428
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
4.1.1 Data sets. We tested our methods on two standard data sets, referred to in this
article as GEO and JOBS. These data sets were created by Ray Mooney?s group during
the 1990s and have been used to evaluate semantic parsers for over a decade.
U.S. Geography. The GEO data set, originally created by Zelle and Mooney (1996), con-
tains 880 questions about U.S. geography and a database of facts encoded in Prolog. The
questions in GEO ask about general properties (e.g., area, elevation, and population) of
geographical entities (e.g., cities, states, rivers, andmountains). Across all the questions,
there are 280 word types, and the length of an utterance ranges from 4 to 19 words,
with an average of 8.5 words. The questions involve conjunctions, superlatives, and
negation, but no generalized quantification. Each question is annotated with a logical
form in Prolog, for example:
Utterance: What is the highest point in Florida?
Logical form: answer(A,highest(A,(place(A),loc(A,B),const(B,stateid(florida)))))
Because our approach learns from answers, not logical forms, we evaluated the
annotated logical forms on the provided database to obtain the correct answers.
Recall that a world/database w maps each predicate p ? P to a set of tuples w(p).
Some predicates contain the set of tuples explicitly (e.g., mountain); others can be
derived (e.g., higher takes two entities x and y and returns true if elevation(x) >
elevation(y)). Other predicates are higher-order (e.g., sum, highest) in that they take
other predicates as arguments. We do not use the provided domain-specific higher-
order predicates (e.g., highest), but rather provide domain-independent higher-order
predicates (e.g., argmax) and the ordinary domain-specific predicates (e.g., elevation).
This provides more compositionality and therefore better generalization. Similarly, we
use more and elevation instead of higher. Altogether, P contains 43 predicates plus
one predicate for each value (e.g., CA).
Job Queries. The JOBS data set (Tang and Mooney 2001) contains 640 natural language
queries about job postings. Most of the questions ask for jobs matching various criteria:
job title, company, recruiter, location, salary, languages and platforms used, areas of
expertise, required/desired degrees, and required/desired years of experience. Across
all utterances, there are 388 word types, and the length of an utterance ranges from 2 to
23 words, with an average of 9.8 words.
The utterances are mostly based on conjunctions of criteria, with a sprinkling of
negation and disjunction. Here is an example:
Utterance: Are there any jobs using Java that are not with IBM?
Logical form: answer(A,(job(A),language(A,?java?),?company(A,?IBM?)))
The JOBS data set comes with a database, which we can use as the world w. When
the logical forms are evaluated on this database, however, close to half of the answers
are empty (no jobs match the requested criteria). Therefore, there is a large discrepancy
between obtaining the correct logical form (which has been the focus of most work on
semantic parsing) and obtaining the correct answer (our focus).
To bring these two into better alignment, we generated a random database as
follows: We created m = 100 jobs. For each job j, we go through each predicate p (e.g.,
company) that takes two arguments, a job, and a target value. For each of the possible
target values v, we add (j, v) to w(p) independently with probability ? = 0.8. For exam-
ple, for p = company, j = job37, we might add (job37, IBM) to w(company). The result is
429
Computational Linguistics Volume 39, Number 2
a database with a total of 23 predicates (which includes the domain-independent ones)
in addition to the value predicates (e.g., IBM).
The goal of using randomness is to ensure that two different logical forms will most
likely yield different answers. For example, consider two logical forms:
z1 = ?j.job( j) ? company( j, IBM), (76)
z2 = ?j.job( j) ? language( j, Java). (77)
Under the random construction, the denotation of z1 is S1, a random subset of the jobs,
where each job is included in S1 independently with probability ?, and the denotation
of z2 is S2, which has the same distribution as S1 but importantly is independent of S1.
Therefore, the probability that S1 = S2 is [?
2 + (1? ?)2]m, which is exponentially small
in m. This construction yields a world that is not entirely ?realistic? (a job might have
multiple employers), but it ensures that if we get the correct answer, we probably also
obtain the correct logical form.
4.1.2 Settings. There are a number of settings that control the tradeoffs between compu-
tation, expressiveness, and generalization power of our model, shown here. For now,
we will use generic settings chosen rather crudely; Section 4.3.4 will explore the effect
of changing these settings.
Lexical Triggers The lexical triggers L (Section 2.6.1) define the set of candidate DCS
trees for each utterance. There is a tradeoff between expressiveness and computa-
tional complexity: The more triggers we have, the more DCS trees we can consider
for a given utterance, but then either the candidate sets become too large or beam
search starts dropping the good DCS trees. Choosing lexical triggers is important
and requires additional supervision (Section 4.1.3).
Features Our probabilistic semantic parsing model is defined in terms of feature tem-
plates (Section 3.1.1). Richer features increase expressiveness but also might lead
to overfitting. By default, we include all the feature templates.
Number of training examples (n) An important property of any learning algorithm is
its sample complexity?how many training examples are required to obtain a
certain level of accuracy? By default, all training examples are used.
Number of training iterations (T) Our learning algorithm (Figure 21) alternates be-
tween updating candidate sets and updating parameters for T iterations. We use
T = 5 as the default value.
Beam size (K) The computation of the candidate sets in Figure 21 is based on beam
search where each intermediate state keeps at most K DCS trees. The default value
is K = 100.
Optimization algorithm To optimize the objective functionO(?,C) our default is to use
the standard L-BFGS algorithm (Nocedal 1980) with a backtracking line search for
choosing the step size.
Regularization (?) The regularization parameter ? > 0 in the objective functionO(?,C)
is another knob for controlling the tradeoff between fitting and overfitting. The
default is ? = 0.01.
4.1.3 Lexical Triggers. The lexical trigger set L (Section 2.6.1) is a set of entries (s, p), where
s is a sequence of words and p is a predicate. We run experiments on two sets of lexical
triggers: base triggers LB and augmented triggers LB+P.
430
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Base Triggers. The base trigger set LB includes three types of entries:
 Domain-independent triggers: For each domain-independent predicate
(e.g., argmax), we manually specify a few words associated with that
predicate (e.g., most). The full list is shown at the top of Figure 22.
 Values: For each value x that appears in the world (specifically,
x ? vj ? w(p) for some tuple v, index j, and predicate p), LB contains an
entry (x, x) (e.g., (Boston,Boston :city)). Note that this rule implicitly
specifies an infinite number of triggers.
Regarding predicate names, we do not add entries such as (city, city),
because we want our system to be language-independent. In Turkish,
for instance, we would not have the luxury of lexicographical cues that
associate citywith s?ehir. So we should think of the predicates as just
symbols predicate1, predicate2, and so on. On the other hand, values
in the database are generally proper nouns (e.g., city names) for which
there are generally strong cross-linguistic lexicographic similarities.
 Part-of-speech (POS) triggers:13 For each domain-specific predicate p,
we specify a set of POS tags T. Implicitly, LB contains all pairs (x, p) where
the word x has a POS tag t ? T. For example, for city, we would specify
NN and NNS, which means that any word which is a singular or plural
common noun triggers the predicate city. Note that city triggers city as
desired, but state also triggers city.
The POS triggers for GEO and JOBS domains are shown in the left side of
Figure 22. Note that some predicates such as traverse and loc are
not associated with any POS tags. Predicates corresponding to verbs and
prepositions are not included as overt lexical triggers, but rather included
as trace predicates L(	). In constructing the logical forms, nouns and
adjectives serve as anchor points. Trace predicates can be inserted between
these anchors. This strategy is more flexible than requiring each predicate
to spring from some word.
Augmented Triggers.We nowdefine the augmented trigger set LB+P, which containsmore
domain-specific information than LB. Specifically, for each domain-specific predicate
(e.g., city), we manually specify a single prototype word (e.g., city) associated with
that predicate. Under LB+P, city would trigger only city because city is a prototype
word, but townwould trigger all the NN predicates (city, state, country, etc.) because
it is not a prototype word.
Prototype triggers require only a modest amount of domain-specific supervision
(see the right side of Figure 22 for the entire list for GEO and JOBS). In fact, as we?ll see
in Section 4.2, prototype triggers are not absolutely required to obtain good accuracies,
but they give an extra boost and also improve computational efficiency by reducing the
set of candidate DCS trees.
13 To perform POS tagging, we used the Berkeley Parser (Petrov et al 2006), trained on the WSJ Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) and the Question Treebank (Judge, Cahill, and v. Genabith
2006)?thanks to Slav Petrov for providing the trained parser.
431
Computational Linguistics Volume 39, Number 2
Figure 22
Lexical triggers used in our experiments.
Finally, to determine triggering, we stem all words using the Porter stemmer (Porter
1980), so that mountains triggers the same predicates as mountain. We also decompose
superlatives into two words (e.g., largest is mapped to most large), allowing us to con-
struct the logical form more compositionally.
4.2 Comparison with Other Systems
We now compare our approach with existing methods. We used the same training-test
splits as Zettlemoyer and Collins (2005) (600 training and 280 test examples for GEO,
500 training and 140 test examples for JOBS). For development, we created five random
splits of the training data. For each split, we put 70% of the examples into a development
training set and the remaining 30% into a development test set. The actual test set was
only used for obtaining final numbers.
4.2.1 Systems that Learn from Question?Answer Pairs.We first compare our system (hence-
forth, LJK11) with Clarke et al (2010) (henceforth, CGCR10), which is most similar to
our work in that it also learns from question?answer pairs without using annotated
logical forms. CGCR10 works with the FunQL language and casts semantic parsing as
integer linear programming (ILP). In each iteration, the learning algorithm solves the
432
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 2
Results on GEO with 250 training and 250 test examples. Our system (LJK11 with base triggers
and no logical forms) obtains higher test accuracy than CGCR10, even when CGCR10 is trained
using logical forms.
System Accuracy (%)
CGCR10 w/answers (Clarke et al 2010) 73.2
CGCR10 w/logical forms (Clarke et al 2010) 80.4
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) 84.0
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) 87.6
ILP to predict the logical form for each training example. The examples with correct
predictions are fed to a structural support vector machine (SVM) and the model param-
eters are updated.
Though similar in spirit, there are some important differences between CGCR10
and our approach. They use ILP instead of beam search and structural SVM instead of
log-linear models, but the main difference is which examples are used for learning. Our
approach learns on any feasible example (Section 3.2.1), one where the candidate set
contains a logical form that evaluates to the correct answer. CGCR10 uses a much more
stringent criterion: The highest scoring logical formmust evaluate to the correct answer.
Therefore, for their algorithm to progress, the model already must be non-trivially good
before learning even starts. This is reflected in the amount of prior knowledge and
initialization that CGCR10 uses before learning starts: WordNet features, syntactic parse
trees, and a set of lexical triggers with 1.42 words per non-value predicate. Our system
with base triggers requires only simple indicator features, POS tags, and 0.5 words per
non-value predicate.
CGCR10 created a version of GEO which contains 250 training and 250 test exam-
ples. Table 2 compares the empirical results of this split. We see that our system (LJK11)
with base triggers significantly outperforms CGCR10 (84% vs. 73.2%), and it even
outperforms the version of CGCR10 that is trained using logical forms (84.0% vs. 80.4%).
If we use augmented triggers, we widen the gap by another 3.6 percentage points.14
4.2.2 State-of-the-Art Systems. We now compare our system (LJK11) with state-of-the-
art systems, which all require annotated logical forms (except PRECISE). Here is a brief
overview of the systems:
 COCKTAIL (Tang and Mooney 2001) uses inductive logic programming to
learn rules for driving the decisions of a shift-reduce semantic parser. It
assumes that a lexicon (mapping from words to predicates) is provided.
 PRECISE (Popescu, Etzioni, and Kautz 2003) does not use learning, but
instead relies on matching words to strings in the database using various
heuristics based on WordNet and the Charniak parser. Like our work, it
also uses database type constraints to rule out spurious logical forms. One
of the unique features of PRECISE is that it has 100% precision?it refuses
to parse an utterance which it deems semantically intractable.
14 Note that the numbers for LJK11 differ from those presented in Liang, Jordan, and Klein (2011), which
reports results based on 10 different splits rather than the set-up used by CGCR10.
433
Computational Linguistics Volume 39, Number 2
 SCISSOR (Ge and Mooney 2005) learns a generative probabilistic model
that extends the Collins (1999) models with semantic labels, so
that syntactic and semantic parsing can be done jointly.
 SILT (Kate, Wong, and Mooney 2005) learns a set of transformation rules
for mapping utterances to logical forms.
 KRISP (Kate and Mooney 2006) uses SVMs with string kernels to drive the
local decisions of a chart-based semantic parser.
 WASP (Wong and Mooney 2006) uses log-linear synchronous grammars to
transform utterances into logical forms, starting with word alignments
obtained from the IBM models.
 ?-WASP (Wong and Mooney 2007) extends WASP to work with logical
forms that contain bound variables (lambda abstraction).
 LNLZ08 (Lu et al 2008) learns a generative model over hybrid trees,
which are logical forms augmented with natural language words.
IBM model 1 is used to initialize the parameters, and a discriminative
reranking step works on top of the generative model.
 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear
model over CCG derivations. Starting with a manually constructed
domain-independent lexicon, the training procedure grows the lexicon
by adding lexical entries derived from associating parts of an utterance
with parts of the annotated logical form.
 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra
(disharmonic) combinators to increase the expressive power of the model.
 KZGS10 (Kwiatkowski et al 2010) uses a restricted higher-order
unification procedure, which iteratively breaks up a logical form into
smaller pieces. This approach gradually adds lexical entries of increasing
generality, thus obviating the need for the manually specified templates
used by ZC05 and ZC07 for growing the lexicon. IBM model 1 is used to
initialize the parameters.
 KZGS11 (Kwiatkowski et al 2011) extends KZGS10 by factoring lexical
entries into a template plus a sequence of predicates that fill the slots of
the template. This factorization improves generalization.
With the exception of PRECISE, all other systems require annotated logical forms,
whereas our system learns only from annotated answers. On the other hand, our system
does rely on a fewmanually specified lexical triggers, whereasmany of the later systems
essentially require no manually crafted lexica. For us, the lexical triggers play a crucial
role in the initial stages of learning because they constrain the set of candidate DCS
trees; otherwise we would face a hopelessly intractable search problem. The other
systems induce lexica using unsupervised word alignment (Wong and Mooney 2006,
2007; Kwiatkowski et al 2010, 2011) and/or on-line lexicon learning (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010, 2011). Unfortunately, we cannot use these
automatic techniques because they rely on having annotated logical forms.
Table 3 shows the results for GEO. Semantic parsers are typically evaluated on
the accuracy of the logical forms: precision (the accuracy on utterances which are
434
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 3
Results on GEO: Logical form accuracy (LF) and answer accuracy (Answer) of the various
systems. The first group of systems are evaluated using 10-fold cross-validation on all 880
examples; the second are evaluated on the 680+ 200 split of Zettlemoyer and Collins (2005).
Our system (LJK11) with base triggers obtains comparable accuracy to past work, whereas
with augmented triggers, our system obtains the highest overall accuracy.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 77.5 77.5
SCISSOR (Ge and Mooney 2005) 72.3 ?
SILT (Kate, Wong, and Mooney 2005) 54.1 ?
KRISP (Kate and Mooney 2006) 71.7 ?
WASP (Wong and Mooney 2006) 74.8 ?
?-WASP (Wong and Mooney 2007) 86.6 ?
LNLZ08 (Lu et al 2008) 81.8 ?
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
ZC07 (Zettlemoyer and Collins 2007) 86.1 ?
KZGS10 (Kwiatkowski et al 2010) 88.2 88.9
KZGS11 (Kwiatkowski et al 2010) 88.6 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 87.9
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 91.4
successfully parsed) and recall (the accuracy on all utterances). We only focus on recall
(a lower bound on precision) and simply use the word accuracy to refer to recall.15 Our
system is evaluated only on answer accuracy because our model marginalizes out the
latent logical form. All other systems are evaluated on the accuracy of logical forms. To
calibrate, we also evaluated KZGS10 on answer accuracy and found that it was quite
similar to its logical form accuracy (88.9% vs. 88.2%).16 This does not imply that our
system would necessarily have a high logical form accuracy because multiple logical
forms can produce the same answer, and our system does not receive a training signal
to tease them apart. Even with only base triggers, our system (LJK11) outperforms all
but two of the systems, falling short of KZGS10 by only one percentage point (87.9% vs.
88.9%).17 With augmented triggers, our system takes the lead (91.4% vs. 88.9%).
Table 4 shows the results for JOBS. The two learning-based systems (COCKTAIL
and ZC05) are actually outperformed by PRECISE, which is able to use strong database
type constraints. By exploiting this information and doing learning, we obtain the best
results.
4.3 Empirical Properties
In this section, we try to gain intuition into properties of our approach. All experiments
in this section were performed on random development splits. Throughout this section,
?accuracy? means development test accuracy.
15 Our system produces a logical form for every utterance, and thus our precision is the same as our recall.
16 The 88.2% corresponds to 87.9% in Kwiatkowski et al (2010). The difference is due to using a slightly
newer version of the code.
17 The 87.9% and 91.4% correspond to 88.6% and 91.1% in Liang, Jordan, and Klein (2011). These differences
are due to minor differences in the code.
435
Computational Linguistics Volume 39, Number 2
Table 4
Results on JOBS: Both PRECISE and our system use database type constraints, which results in a
decisive advantage over the other systems. In addition, LJK11 incorporates learning and
therefore obtains the highest accuracies.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 90.7
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 95.0
4.3.1 Error Analysis. To understand the type of errors our system makes, we examined
one of the development runs, which had 34 errors on the test set. We classified these
errors into the following categories (the number of errors in each category is shown in
parentheses):
 Incorrect POS tags (8): GEO is out-of-domain for our POS tagger, so the
tagger makes some basic errors that adversely affect the predicates that
can be lexically triggered. For example, the questionWhat states border
states . . . is tagged as WP VBZ NN NNS . . . , which means that the first states
cannot trigger state. In another example, major river is tagged as NNP
NNP, so these cannot trigger the appropriate predicates either, and thus
the desired DCS tree cannot even be constructed.
 Non-projectivity (3): The candidate DCS trees are defined by a projective
construction mechanism (Section 2.6) that prohibits edges in the DCS
tree from crossing. This means we cannot handle utterances such as
largest city by area, because the desired DCS tree would have city
dominating area dominating argmax. To construct this DCS tree,
we could allow local reordering of the words.
 Unseen words (2): We never saw at least or sea level at training time.
The former has the correct lexical trigger, but not a sufficiently large
feature weight (0) to encourage its use. For the latter, the problem is
more structural: We have no lexical triggers for 0 :length, and only
adding more lexical triggers can solve this problem.
 Wrong lexical triggers (7): Sometimes the error is localized to a single
lexical trigger. For example, the model incorrectly thinksMississippi
is the state rather than the river, and that Rochester is the city in
New York rather than the name, even though there are contextual
cues to disambiguate in these cases.
 Extra words (5): Sometimes, words trigger predicates that should be
ignored. For example, for population density, the first word triggers
population, which is used rather than density.
 Over-smoothing of DCS tree (9): The first half of our features (Figure 20)
are defined on the DCS tree alone; these produce a form of smoothing
that encourages DCS trees to look alike regardless of the words. We found
436
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
several instances where this essential tool for generalization went too
far. For example, in state of Nevada, the trace predicate border is inserted
between the two nouns, because it creates a structure more similar to
that of the common question what states border Nevada?
4.3.2 Visualization of Features.Having analyzed the behavior of our system for individual
utterances, let us move from the token level to the type level and analyze the learned
parameters of our model. We do not look at raw feature weights, because there are
complex interactions between them not represented by examining individual weights.
Instead, we look at expected feature counts, which we think are more interpretable.
Consider a group of ?competing? features J, for example J = {TRIGGERPRED[city,
p] : p ? P}. We define a distribution q(?) over J as follows:
q( j) =
Nj
?
j??J Nj?
, where (78)
Nj =
?
(x,y)?D
Ep(z|x,Z?L,?,?)[?(x, z)]
Think of q( j) as a marginal distribution (because all our features are positive) that
represents the relative frequencies with which the features j ? J fire with respect to
our training data set D and trained model p(z | x, Z?L,?,?). To appreciate the difference
between what this distribution and raw feature weights capture, suppose we had two
features, j1 and j2, which are identical (?(x, z)j1 ? ?(x, z)j2 ). The weights would be split
across the two features, but the features would have the same marginal distribution
(q(j1) = q(j2)). Figure 23 shows some of the feature distributions learned.
4.3.3 Learning, Search, Bootstrapping. Recall from Section 3.2.1 that a training example
is feasible (with respect to our beam search) if the resulting candidate set contains a
DCS tree with the correct answer. Infeasible examples are skipped, but an example may
become feasible in a later iteration. A natural question is how many training examples
are feasible in each iteration. Figure 24 shows the answer: Initially, only around 30% of
the training examples are feasible; this is not surprising given that all the parameters
are zero, so our beam search is essentially unguided. Training on just these examples
improves the parameters, however, and over the next few iterations, the number of
feasible examples steadily increases to around 97%.
In our algorithm, learning and search are deeply intertwined. Search is of course
needed to learn, but learning also improves search. The general approach is similar in
spirit to Searn (Daume, Langford, andMarcu 2009), althoughwe do not have any formal
guarantees at this point.
Our algorithm also has a bootstrapping flavor. The ?easy? examples are processed
first, where easy is defined by the ability of beam search to generate the correct answer.
This bootstrapping occurs quite naturally: Unlikemost bootstrapping algorithms, we do
not have to set a confidence threshold for accepting new training examples, something
that can be quite tricky to do. Instead, our threshold falls out of the discrete nature of
the beam search.
4.3.4 Effect of Various Settings. So far, we have used our approach with default settings
(Section 4.1.2). How sensitive is the approach to these choices? Table 5 shows the impact
of the feature templates. Figure 25 shows the effect of the number of training examples,
437
Computational Linguistics Volume 39, Number 2
Figure 23
Learned feature distributions. In a feature group (e.g., TRIGGERPRED[city, ?]), each feature is
associated with the marginal probability that the feature fires according to Equation (78). Note
that we have successfully learned that citymeans city, but incorrectly learned that sparsemeans
elevation (due to the confounding fact that Alaska is the most sparse state and has the highest
elevation).
number of training iterations, beam size, and regularization parameter. The overall
conclusion is that there are no big surprises: Our default settings could be improved
on slightly, but these differences are often smaller than the variation across different
development splits.
We now consider the choice of optimization algorithm to update the parameters
given candidate sets (see Figure 21). Thus far, we have been using L-BFGS (Nocedal
1980), which is a batch algorithm. Each iteration, we construct the candidate
Figure 24
The fraction of feasible training examples increases steadily as the parameters, and thus the
beam search improves. Each curve corresponds to a run on a different development split.
438
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 5
There are two classes of feature templates: lexical features (TRIGGERPRED,TRACE*) and
non-lexical features (PREDREL,PREDRELPRED). The lexical features are relatively much more
important for obtaining good accuracy (76.4% vs. 23.1%), but adding the non-lexical features
makes a significant contribution as well (84.7% vs. 76.4%).
Features Accuracy (%)
PRED 13.4? 1.6
PRED + PREDREL 18.4? 3.5
PRED + PREDREL + PREDRELPRED 23.1? 5.0
PRED + TRIGGERPRED 61.3? 1.1
PRED + TRIGGERPRED + TRACE* 76.4? 2.3
PRED + PREDREL + PREDRELPRED + TRIGGERPRED + TRACE* 84.7? 3.5
sets C(t)(x) for all the training examples before solving the optimization problem
argmax?O(?,C
(t) ). We now consider an on-line algorithm, stochastic gradient descent
(SGD) (Robbins and Monro 1951), which updates the parameters after computing
the candidate set for each example. In particular, we iteratively scan through the
training examples in a random order. For each example (x, y), we compute the
candidate set using beam search. We then update the parameters in the direction of
the gradient of the marginal log-likelihood for that example (see Equation (72)) with
step size t??:
?(t+1) ? ?(t) + t??
(
? log p(y | x; Z?L,?(t) ,?)
??
?
?
?
?=?(t)
)
(79)
The trickiest aspect of using SGD is selecting the correct step size: A small ? leads to
quick progress but also instability; a large ? leads to the opposite. We let L-BFGS and
SGD both take the same number of iterations (passes over the training set). Figure 26
shows that a very small value of ? (less than 0.2) is best for our task, even though
only values between 0.5 and 1 guarantee convergence. Our setting is slightly different
because we are interleaving the SGD updates with beam search, which might also
lead to unpredictable consequences. Furthermore, the non-convexity of the objective
function exacerbates the unpredictability (Liang and Klein 2009). Nonetheless, with
a proper ?, SGD converges much faster than L-BFGS and even to a slightly better
solution.
5. Discussion
The work we have presented in this article addresses three important themes. The
first theme is semantic representation (Section 5.1): How do we parametrize the mapping
from utterances to their meanings? The second theme is program induction (Section 5.2):
How do we efficiently search through the space of logical structures given a weak
feedback signal? Finally, the last theme is grounded language (Section 5.3): Howdowe use
constraints from the world to guide learning of language and conversely use language
to interact with the world?
439
Computational Linguistics Volume 39, Number 2
Figure 25
(a) The learning curve shows test accuracy as the number of training examples increases; about
300 examples suffices to get around 80% accuracy. (b) Although our algorithm is not guaranteed
to converge, the test accuracy is fairly stable (with one exception) with more training
iterations?hardly any overfitting occurs. (c) As the beam size increases, the accuracy increases
monotonically, although the computational burden also increases. There is a small gain from our
default setting of K = 100 to the more expensive K = 300. (d) The accuracy is relatively
insensitive to the choice of the regularization parameter for a wide range of values. In fact, no
regularization is also acceptable. This is probably because the features are simple, and the lexical
triggers and beam search already provide some helpful biases.
5.1 Semantic Representation
Since the late nineteenth century, philosophers and linguists have worked on elucidat-
ing the relationship between an utterance and its meaning. One of the pillars of formal
semantics is Frege?s principle of compositionality, that the meaning of an utterance
is built by composing the meaning of its parts. What these parts are and how they
are composed is the main question. The dominant paradigm, which stems from the
seminal work of Richard Montague (1973) in the early 1970s, states that parts are
lambda calculus expressions that correspond to syntactic constituents, and composition
is function application.
440
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 26
(a) Given the same number of iterations, compared to default batch algorithm (L-BFGS),
the on-line algorithm (stochastic gradient descent) is slightly better for aggressive step
sizes (small ?) and worse for conservative step sizes (large ?). (b) The on-line algorithm
(with an appropriate choice of ?) obtains a reasonable accuracy much faster than L-BFGS.
Consider the compositionality principle from a statistical point of view, where we
construe compositionality as factorization. Factorization, the way a statistical model
breaks into features, is necessary for generalization: It enables us to learn from pre-
viously seen examples and interpret new utterances. Projecting back to Frege?s orig-
inal principle, the parts are the features (Section 3.1.1), and composition is the DCS
construction mechanism (Section 2.6) driven by parameters learned from training
examples.
Taking the statistical view of compositionality, finding a good semantic represen-
tation becomes designing a good statistical model. But statistical modeling must also
deal with the additional issue of language acquisition or learning, which presents
complications: In absorbing training examples, our learning algorithm must inevitably
traverse through intermediate models that are wrong or incomplete. The algorithms
must therefore tolerate this degradation, and do so in a computationally efficient way.
For example, in the line of work on learning probabilistic CCGs (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010), many candidate lexical entries must be
entertained for each word even when polysemy does not actually exist (Section 2.6.4).
To improve generalization, the lexicon can be further factorized (Kwiatkowski et al
2011), but this is all done within the constraints of CCG. DCS represents a departure
from this tradition, which replaces a heavily lexicalized constituency-based formalism
with a lightly-lexicalized dependency-based formalism. We can think of DCS as a shift
in linguistic coordinate systems, which makes certain factorizations or features more
accessible. For example, we can define features on paths between predicates in a DCS
tree which capture certain lexical patterns much more easily than in a lambda calculus
expression or a CCG derivation.
DCS has a family resemblance to a semantic representation called natural logic form
(Alshawi, Chang, and Ringgaard 2011), which is also motivated by the benefits of work-
ing with dependency-based logical forms. The goals and the detailed structure of the
two semantic formalisms are different, however. Alshawi, Chang, and Ringgaard (2011)
focus on parsing complex sentences in an open domain where a structured database
or world does not exist. Whereas they do equip their logical forms with a full model-
theoretic semantics, the logical forms are actually closer to dependency trees: Quantifier
scope is left unspecified, and the predicates are simply the words.
441
Computational Linguistics Volume 39, Number 2
Perhaps not immediately apparent is the fact that DCS draws an important idea
from Discourse Representation Theory (DRT) (Kamp and Reyle 1993)?not from the
treatment of anaphora and presupposition which it is known for, but something closer
to its core. This is the idea of having a logical form where all variables are existentially
quantified and constraints are combined via conjunction?a Discourse Representation
Structure (DRS) in DRT, or a basic DCS tree with only join relations. Computationally,
these logical structures conveniently encode CSPs. Linguistically, it appears that existen-
tial quantifiers play an important role and should be treated specially (Kamp and Reyle
1993). DCS takes this core and focuses on semantic compositionality and computation,
whereas DRT focuses more on discourse and pragmatics.
In addition to the statistical view of DCS as a semantic representation, it is use-
ful to think about DCS from the perspective of programming language design. Two
programming languages can be equally expressive, but what matters is how simple it
is to express a desired type of computation in a given language. In some sense, we
designed the DCS formal language to make it easy to represent computations expressed
by natural language. An important part of DCS is themark?execute construct, a uniform
framework for dealing with the divergence between syntactic and semantic scope. This
construct allows us to build simple DCS tree structures and still handle the complexities
of phenomena such as quantifier scope variation. Compared to lambda calculus, think
of DCS as a higher-level programming language tailored to natural language, which
results in simpler programs (DCS trees). Simpler programs are easier for us to work
with and easier for an algorithm to learn.
5.2 Program Induction
Searching over the space of programs is challenging. This is the central computational
challenge of program induction, that of inferring programs (logical forms) from their
behavior (denotations). This problem has been tackled by different communities in
various forms: program induction in AI, programming by demonstration in Human?
Computer Interaction, and program synthesis in programming languages. The core
computational difficulty is that the supervision signal?the behavior?is a complex
function of the program that cannot be easily inverted. What program generated the
output Arizona, Nevada, and Oregon?
Perhaps somewhat counterintuitively, program induction is easier if we infer pro-
grams for not a single task but for multiple tasks. The intuition is that when the tasks
are related, the solution to one task can help another task, both computationally in
navigating the program space and statistically in choosing the appropriate program if
there are multiple feasible possibilities (Liang, Jordan, and Klein 2010). In our semantic
parsing work, we want to infer a logical form for each utterance (task). Clearly the tasks
are related because they use the same vocabulary to talk about the same domain.
Natural language also makes program induction easier by providing side informa-
tion (words) which can be used to guide the search. There have been several papers
that induce programs in this setting: Eisenstein et al (2009) induce conjunctive for-
mulae from natural language instructions, Piantadosi et al (2008) induce first-order
logic formulae using CCG in a small domain assuming observed lexical semantics,
and Clarke et al (2010) induce logical forms in semantic parsing. In the ideal case, the
words would determine the program predicates, and the utterance would determine
the entire program compositionally. But of course, this mapping is not given and must
be learned.
442
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
5.3 Grounded Language
In recent years, there has been an increased interest in connecting language with the
world.18 One of the primary issues in grounded language is alignment?figuring out
what fragments of utterances refer to what aspects of the world. In fact, semantic
parsers trained on examples of utterances and annotated logical form (those discussed
in Section 4.2.2) need to solve the task of aligning words to predicates. Some can learn
from utterances paired with a set of logical forms, one of which is correct (Kate and
Mooney 2007; Chen and Mooney 2008). Liang, Jordan, and Klein (2009) tackle the even
more difficult alignment problem of segmenting and aligning a discourse to a database
of facts, where many parts on either side are irrelevant.
If we know how the world relates to language, we can leverage structure in the
world to guide the learning and interpretation of language.We saw that type constraints
from the database/world reduce the set of candidate logical forms and lead to more
accurate systems (Popescu, Etzioni, and Kautz 2003; Liang, Jordan, and Klein 2011).
Even for syntactic parsing, information from the denotation of an utterance can be
helpful (Schuler 2003).
One of the exciting aspects about using the world for learning language is that
it opens the door to many new types of supervision. We can obtain answers given a
world, which are cheaper to obtain than logical forms (Clarke et al 2010; Liang, Jordan,
and Klein 2011). Other researchers have also pushed in this direction in various ways:
learning a semantic parser based on bootstrapping and estimating the confidence of its
own predictions (Goldwasser et al 2011), learning a semantic parser from user interac-
tions with a dialog system (Artzi and Zettlemoyer 2011), and learning to execute natural
language instructions from just a reward signal using reinforcement learning (Branavan
et al 2009; Branavan, Zettlemoyer, and Barzilay 2010; Branavan, Silver, and Barzilay
2011). In general, supervision from the world is indirectly related to the learning task,
but it is often much more plentiful and natural to obtain.
The benefits can also flow from language to the world. For example, previous work
learned to interpret language to troubleshoot aWindows machine (Branavan et al 2009;
Branavan, Zettlemoyer, and Barzilay 2010), win a game of Civilization (Branavan, Silver,
and Barzilay 2011), play a legal game of solitaire (Eisenstein et al 2009; Goldwasser and
Roth 2011), and navigate a map by following directions (Vogel and Jurafsky 2010; Chen
and Mooney 2011). Even when the objective in the world is defined independently of
language (e.g., in Civilization), language can provide a useful bias towards the non-
linguistic end goal.
6. Conclusions
The main conceptual contribution of this article is a new semantic formalism,
dependency-based compositional semantics (DCS), and techniques to learn a semantic
parser from question?answer pairs where the intermediate logical form (a DCS tree) is
induced in an unsupervised manner. Our final question?answering system was able to
match the accuracies of state-of-the-art systems that learn from annotated logical forms.
There is currently a significant conceptual gap between our question?answering
system (which can be construed as a natural language interface to a database) and
18 Here, world need not refer to the physical world, but could be any virtual world. The point is that the
world has non-trivial structure and exists extra-linguistically.
443
Computational Linguistics Volume 39, Number 2
open-domain question?answering systems. The former focuses on understanding a
question compositionally and computing the answer compositionally, whereas the lat-
ter focuses on retrieving and ranking answers from a large unstructured textual corpus.
The former has depth; the latter has breadth. Developing methods that can both model
the semantic richness of language and scale up to an open-domain setting remains an
open challenge.
We believe that it is possible to push our approach in the open-domain direction.
Neither DCS nor the learning algorithm is tied to having a clean rigid database, which
could instead be a database generated from a noisy information extraction process. The
key is to drive the learning with the desired behavior, the question?answer pairs. The
latent variable is the logical form or program, which just tries to compute the desired
answer by piecing together whatever information is available. Of course, there aremany
open challenges ahead, but with the proper combination of linguistic, statistical, and
computational insight, we hope to eventually build systems with both breadth and
depth.
Acknowledgments
We thank Luke Zettlemoyer and Tom
Kwiatkowski for providing us with data
and answering questions, as well as the
anonymous reviewers for their detailed
feedback. P. L. was supported by an NSF
Graduate Research Fellowship.
References
Alshawi, H., P. Chang, and M. Ringgaard.
2011. Deterministic statistical mapping
of sentences to underspecified
semantics. In International Conference
on Compositional Semantics (IWCS),
pages 15?24, Oxford.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Journal of Natural Language Engineering,
1:29?81.
Artzi, Y. and L. Zettlemoyer. 2011.
Bootstrapping semantic parsers from
conversations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 421?432, Edinburgh.
Baldridge, J. and G. M. Kruijff. 2002.
Coupling CCG with hybrid logic
dependency semantics. In Association
for Computational Linguistics (ACL),
pages 319?326, Philadelphia, PA.
Barker, C. 2002. Continuations and the
nature of quantification. Natural
Language Semantics, 10:211?242.
Bos, J. 2009. A controlled fragment of
DRT. InWorkshop on Controlled Natural
Language, pages 1?5.
Bos, J., S. Clark, M. Steedman, J. R. Curran,
and J. Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG
parser. In International Conference on
Computational Linguistics (COLING),
pages 1240?1246, Geneva.
Branavan, S., H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In
Association for Computational Linguistics and
International Joint Conference on Natural
Language Processing (ACL-IJCNLP),
pages 82?90, Singapore.
Branavan, S., D. Silver, and R. Barzilay. 2011.
Learning to win by reading manuals in a
Monte-Carlo framework. In Association
for Computational Linguistics (ACL),
pages 268?277.
Branavan, S., L. Zettlemoyer, and R. Barzilay.
2010. Reading between the lines: Learning
to map high-level instructions to
commands. In Association for Computational
Linguistics (ACL), pages 1268?1277,
Portland, OR.
Carpenter, B. 1998. Type-Logical Semantics.
MIT Press, Cambridge, MA.
Chen, D. L. and R. J. Mooney. 2008. Learning
to sportscast: A test of grounded language
acquisition. In International Conference on
Machine Learning (ICML), pages 128?135,
Helsinki.
Chen, D. L. and R. J. Mooney. 2011.
Learning to interpret natural language
navigation instructions from observations.
In Association for the Advancement
of Artificial Intelligence (AAAI),
pages 128?135, Cambridge, MA.
Clarke, J., D. Goldwasser, M. Chang,
and D. Roth. 2010. Driving semantic
parsing from the world?s response.
In Computational Natural Language
Learning (CoNLL), pages 18?27,
Uppsala.
444
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Collins, M. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Cooper, R. 1975.Montague?s semantic theory
and transformational syntax. Ph.D. thesis,
University of Massachusetts at Amherst.
Cousot, P. and R. Cousot. 1977. Abstract
interpretation: A unified lattice model for
static analysis of programs by construction
or approximation of fixpoints. In Principles
of Programming Languages (POPL),
pages 238?252, Los Angeles, CA.
Daume, H., J. Langford, and D. Marcu.
2009. Search-based structured prediction.
Machine Learning Journal (MLJ), 75:297?325.
Dechter, R. 2003. Constraint Processing.
Morgan Kaufmann.
Eisenstein, J., J. Clarke, D. Goldwasser,
and D. Roth. 2009. Reading to learn:
Constructing features from semantic
abstracts. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 958?967, Singapore.
Ge, R. and R. J. Mooney. 2005. A statistical
semantic parser that integrates syntax
and semantics. In Computational Natural
Language Learning (CoNLL), pages 9?16,
Ann Arbor, MI.
Giordani, A. and A. Moschitti. 2009.
Semantic mapping between natural
language questions and SQL queries
via syntactic pairing. In International
Conference on Applications of Natural
Language to Information Systems,
pages 207?221, Saarbru?cken.
Goldwasser, D., R. Reichart, J. Clarke,
and D. Roth. 2011. Confidence driven
unsupervised semantic parsing. In
Association for Computational Linguistics
(ACL), pages 1486?1495, Barcelona.
Goldwasser, D. and D. Roth. 2011. Learning
from natural instructions. In International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1794?1800, Portland, OR.
Heim, I. and A. Kratzer. 1998. Semantics in
Generative Grammar. Wiley-Blackwell,
Oxford.
Judge, J., A. Cahill, and J. v. Genabith.
2006. Question-bank: Creating a
corpus of parse-annotated questions.
In International Conference on Computational
Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 497?504,
Sydney.
Kamp, H. and U. Reyle. 1993. From Discourse
to Logic: An Introduction to the
Model-theoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
Kamp, H., J. van Genabith, and U. Reyle.
2005. Discourse representation theory.
In Handbook of Philosophical Logic,
Kluwer, Dordrecht.
Kate, R. J. and R. J. Mooney. 2006. Using
string-kernels for learning semantic
parsers. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 913?920, Sydney.
Kate, R. J. and R. J. Mooney. 2007.
Learning language semantics from
ambiguous supervision. In Association
for the Advancement of Artificial
Intelligence (AAAI), pages 895?900,
Cambridge, MA.
Kate, R. J., Y. W. Wong, and R. J. Mooney.
2005. Learning to transform natural to
formal languages. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1062?1068.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2010.
Inducing probabilistic CCG grammars
from logical form with higher-order
unification. In Empirical Methods in
Natural Language Processing (EMNLP),
pages1223?1233, Cambridge, MA.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2011.
Lexical generalization in CCG grammar
induction for semantic parsing. In
Empirical Methods in Natural Language
Processing (EMNLP), pages 1512?1523,
Cambridge, MA.
Liang, P. 2011. Learning Dependency-Based
Compositional Semantics. Ph.D. thesis,
University of California at Berkeley.
Liang, P., M. I. Jordan, and D. Klein. 2009.
Learning semantic correspondences
with less supervision. In Association for
Computational Linguistics and International
Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 91?99,
Singapore.
Liang, P., M. I. Jordan, and D. Klein. 2010.
Learning programs: A hierarchical
Bayesian approach. In International
Conference on Machine Learning (ICML),
pages 639?646, Haifa.
Liang, P., M. I. Jordan, and D. Klein.
2011. Learning dependency-based
compositional semantics. In Association
for Computational Linguistics (ACL),
pages 590?599, Portland, OR.
Liang, P. and D. Klein. 2009. Online EM for
unsupervised models. In North American
Association for Computational Linguistics
(NAACL), pages 611?619, Boulder, CO.
445
Computational Linguistics Volume 39, Number 2
Lu, W., H. T. Ng, W. S. Lee, and L. S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning
representations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 783?792, Honolulu, HI.
Marcus, M. P., M. A. Marcinkiewicz, and
B. Santorini. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
Miller, S., D. Stallard, R. Bobrow, and
R. Schwartz. 1996. A fully statistical
approach to natural language interfaces.
In Association for Computational Linguistics
(ACL), pages 55?61, Santa Cruz, CA.
Montague, R. 1973. The proper treatment
of quantification in ordinary English.
In J. Hiutikka, J. Moravcsik, and
P. Suppes, editors, Approaches to Natural
Language, pages 221?242, Dordrecht,
The Netherlands.
Nocedal, J. 1980. Updating quasi-Newton
matrices with limited storage.Mathematics
of Computation, 35:773?782.
Petrov, S., L. Barrett, R. Thibaux, and
D. Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 433?440, Sydney.
Piantadosi, S. T., N. D. Goodman, B. A. Ellis,
and J. B. Tenenbaum. 2008. A Bayesian
model of the acquisition of compositional
semantics. In Proceedings of the Thirtieth
Annual Conference of the Cognitive Science
Society, pages 1620?1625, Washington, DC.
Popescu, A., O. Etzioni, and H. Kautz. 2003.
Towards a theory of natural language
interfaces to databases. In International
Conference on Intelligent User Interfaces
(IUI), pages 149?157, Miami, FL.
Porter, M. F. 1980. An algorithm for suffix
stripping. Program, 14:130?137.
Robbins, H. and S. Monro. 1951. A stochastic
approximation method. Annals of
Mathematical Statistics, 22(3):400?407.
Schuler, W. 2003. Using model-theoretic
semantic interpretation to guide statistical
parsing and word recognition in a spoken
language interface. In Association for
Computational Linguistics (ACL),
pages 529?536, Sapporo.
Shan, C. 2004. Delimited continuations in
natural language. Technical report, ArXiv.
Available at http://arvix.org/abs/
cs.CL/0404006.
Steedman, M. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Tang, L. R. and R. J. Mooney. 2001. Using
multiple clause constructors in inductive
logic programming for semantic parsing.
In European Conference on Machine Learning,
pages 466?477, Freiburg.
Vogel, A. and D. Jurafsky. 2010. Learning
to follow navigational directions.
In Association for Computational Linguistics
(ACL), pages 806?814, Uppsala.
Wainwright, M. and M. I. Jordan. 2008.
Graphical models, exponential families,
and variational inference. Foundations and
Trends in Machine Learning, 1:1?307.
Warren, D. and F. Pereira. 1982. An efficient
easily adaptable system for interpreting
natural language queries. Computational
Linguistics, 8:110?122.
White, M. 2006. Efficient realization of
coordinate structures in combinatory
categorial grammar. Research on Language
and Computation, 4:39?75.
Wong, Y. W. and R. J. Mooney. 2006.
Learning for semantic parsing with
statistical machine translation. In North
American Association for Computational
Linguistics (NAACL), pages 439?446,
New York, NY.
Wong, Y. W. and R. J. Mooney. 2007.
Learning synchronous grammars for
semantic parsing with lambda calculus.
In Association for Computational Linguistics
(ACL), pages 960?967, Prague.
Woods, W. A., R. M. Kaplan, and
B. N. Webber. 1972. The lunar sciences
natural language information system:
Final report. Technical Report 2378,
Bolt Beranek and Newman Inc.,
Cambridge, MA.
Zelle, M. and R. J. Mooney. 1996. Learning to
parse database queries using inductive
logic programming. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1050?1055, Cambridge, MA.
Zettlemoyer, L. S. and M. Collins. 2005.
Learning to map sentences to logical
form: Structured classification with
probabilistic categorial grammars.
In Uncertainty in Artificial Intelligence
(UAI), pages 658?666.
Zettlemoyer, L. S. and M. Collins. 2007.
Online learning of relaxed CCG grammars
for parsing to logical form. In Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP/CoNLL), pages 678?687,
Prague.
446

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573?581,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Type-Based MCMC
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Most existing algorithms for learning latent-
variable models?such as EM and existing
Gibbs samplers?are token-based, meaning
that they update the variables associated with
one sentence at a time. The incremental na-
ture of these methods makes them suscepti-
ble to local optima/slow mixing. In this paper,
we introduce a type-based sampler, which up-
dates a block of variables, identified by a type,
which spans multiple sentences. We show im-
provements on part-of-speech induction, word
segmentation, and learning tree-substitution
grammars.
1 Introduction
A long-standing challenge in NLP is the unsu-
pervised induction of linguistic structures, for ex-
ample, grammars from raw sentences or lexicons
from phoneme sequences. A fundamental property
of these unsupervised learning problems is multi-
modality. In grammar induction, for example, we
could analyze subject-verb-object sequences as ei-
ther ((subject verb) object) (mode 1) or (subject
(verb object)) (mode 2).
Multimodality causes problems for token-based
procedures that update variables for one example at
a time. In EM, for example, if the parameters al-
ready assign high probability to the ((subject verb)
object) analysis, re-analyzing the sentences in E-step
only reinforces the analysis, resulting in EM getting
stuck in a local optimum. In (collapsed) Gibbs sam-
pling, if all sentences are already analyzed as ((sub-
ject verb) object), sampling a sentence conditioned
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
(a) token-based (b) sentence-based (c) type-based
Figure 1: Consider a dataset of 3 sentences, each of
length 5. Each variable is labeled with a type (1 or 2). The
unshaded variables are the ones that are updated jointly
by a sampler. The token-based sampler updates the vari-
able for one token at a time (a). The sentence-based sam-
pler updates all variables in a sentence, thus dealing with
intra-sentential dependencies (b). The type-based sam-
pler updates all variables of a particular type (1 in this ex-
ample), thus dealing with dependencies due to common
parameters (c).
on all others will most likely not change its analysis,
resulting in slow mixing.
To combat the problems associated with token-
based algorithms, we propose a new sampling algo-
rithm that operates on types. Our sampler would, for
example, be able to change all occurrences of ((sub-
ject verb) object) to (subject (verb object)) in one
step. These type-based operations are reminiscent of
the type-based grammar operations of early chunk-
merge systems (Wolff, 1988; Stolcke and Omohun-
dro, 1994), but we work within a sampling frame-
work for increased robustness.
In NLP, perhaps the the most simple and popu-
lar sampler is the token-based Gibbs sampler,1 used
in Goldwater et al (2006), Goldwater and Griffiths
(2007), and many others. By sampling only one
1In NLP, this is sometimes referred to as simply the col-
lapsed Gibbs sampler.
573
variable at a time, this sampler is prone to slow mix-
ing due to the strong coupling between variables.
A general remedy is to sample blocks of coupled
variables. For example, the sentence-based sampler
samples all the variables associated with a sentence
at once (e.g., the entire tag sequence). However, this
blocking does not deal with the strong type-based
coupling (e.g., all instances of a word should be
tagged similarly). The type-based sampler we will
present is designed exactly to tackle this coupling,
which we argue is stronger and more important to
deal with in unsupervised learning. Figure 1 depicts
the updates made by each of the three samplers.
We tested our sampler on three models: a
Bayesian HMM for part-of-speech induction (Gold-
water and Griffiths, 2007), a nonparametric
Bayesian model for word segmentation (Goldwater
et al, 2006), and a nonparametric Bayesian model of
tree substitution grammars (Cohn et al, 2009; Post
and Gildea, 2009). Empirically, we find that type-
based sampling improves performance and is less
sensitive to initialization (Section 5).
2 Basic Idea via a Motivating Example
The key technical problem we solve in this paper is
finding a block of variables which are both highly
coupled and yet tractable to sample jointly. This
section illustrates the main idea behind type-based
sampling on a small word segmentation example.
Suppose our dataset x consists of n occurrences
of the sequence a b. Our goal is infer z =
(z1, . . . , zn), where zi = 0 if the sequence is one
word ab, and zi = 1 if the sequence is two, a
and b. We can model this situation with a simple
generative model: for each i = 1, . . . , n, gener-
ate one or two words with equal probability. Each
word is drawn independently based on probabilities
? = (?a, ?b, ?ab) which we endow with a uniform
prior ? ? Dirichlet(1, 1, 1).
We marginalize out ? to get the following standard
expression (Goldwater et al, 2009):
p(z | x) ?
1(m)1(m)1(n?m)
3(n+m)
def
= g(m), (1)
where m =
?n
i=1 zi is the number of two-word se-
quences and a(k) = a(a + 1) ? ? ? (a + k ? 1) is the
200 400 600 8001000
m
-1411.4
-1060.3
-709.1
-358.0
-6.8
log
g(m
)
2 4 6 8 10
iteration
200
400
600
800
1000
m
Token
Type
(a) bimodal posterior (b) sampling run
Figure 2: (a) The posterior (1) is sharply bimodal (note
the log-scale). (b) A run of the token-based and type-
based samplers. We initialize both samplers with m = n
(n = 1000). The type-based sampler mixes instantly
(in fact, it makes independent draws from the posterior)
whereas the token-based sampler requires five passes
through the data before finding the high probability re-
gion m u 0.
ascending factorial.2 Figure 2(a) depicts the result-
ing bimodal posterior.
A token-based sampler chooses one zi to update
according to the posterior p(zi | z?i,x). To illus-
trate the mixing problem, consider the case where
m = n, i.e., all sequences are analyzed as two
words. From (1), we can verify that p(zi = 0 |
z?i,x) = O( 1n). When n = 1000, this means that
there is only a 0.002 probability of setting zi = 0,
a very unlikely but necessary first step to take to es-
cape this local optimum. Indeed, Figure 2(b) shows
how the token-based sampler requires five passes
over the data to finally escape.
Type-based sampling completely eradicates the
local optimum problem in this example. Let us take
a closer look at (1). Note that p(z | x) only depends
on a single integer m, which only takes one of n+ 1
values, not on the particular z. This shows that the
zis are exchangeable. There are
(n
m
)
possible val-
ues of z satisfying m =
?
i zi, each with the same
probability g(m). Summing, we get:
p(m | x) ?
?
z:m=
P
i zi
p(x, z) =
(
n
m
)
g(m). (2)
A sampling strategy falls out naturally: First, sample
the number m via (2). Conditioned on m, choose
2The ascending factorial function arises from marginaliz-
ing Dirichlet distributions and is responsible the rich-gets-richer
phenomenon: the larger n is, more we gain by increasing it.
574
the particular z uniformly out of the
(n
m
)
possibili-
ties. Figure 2(b) shows the effectiveness of this type-
based sampler.
This simple example exposes the fundamental
challenge of multimodality in unsupervised learn-
ing. Both m = 0 and m = n are modes due to the
rich-gets-richer property which arises by virtue of
all n examples sharing the same parameters ?. This
sharing is a double-edged sword: It provides us with
clustering structure but also makes inference hard.
Even though m = n is much worse (by a factor ex-
ponential in n) than m = 0, a na??ve algorithm can
easily have trouble escaping m = n.
3 Setup
We will now present the type-based sampler in full
generality. Our sampler is applicable to any model
which is built out of local multinomial choices,
where each multinomial has a Dirichlet process prior
(a Dirichlet prior if the number of choices is finite).
This includes most probabilistic models in NLP (ex-
cluding ones built from log-linear features).
As we develop the sampler, we will pro-
vide concrete examples for the Bayesian hidden
Markov model (HMM), the Dirichlet process uni-
gram segmentation model (USM) (Goldwater et al,
2006), and the probabilistic tree-substitution gram-
mar (PTSG) (Cohn et al, 2009; Post and Gildea,
2009).
3.1 Model parameters
A model is specified by a collection of multino-
mial parameters ? = {?r}r?R, where R is an in-
dex set. Each vector ?r specifies a distribution over
outcomes: outcome o has probability ?ro.
? HMM: Let K is the number of states. The set
R = {(q, k) : q ? {T,E}, k = 1, . . . ,K}
indexes the K transition distributions {?(T,k)}
(each over outcomes {1, . . . ,K}) and K emis-
sion distributions {?(E,k)} (each over the set of
words).
? USM: R = {0}, and ?0 is a distribution over (an
infinite number of) words.
? PTSG: R is the set of grammar symbols, and
each ?r is a distribution over labeled tree frag-
ments with root label r.
R index set for parameters
? = {?r}r?R multinomial parameters
? = {?r}r?R base distributions (fixed)
S set of sites
b = {bs}s?S binary variables (to be sampled)
z latent structure (set of choices)
z?s choices not depending on site s
zs:b choices after setting bs = b
?zs:b zs:b\z?s: new choices from bs = b
S ? S sites selected for sampling
m # sites in S assigned bs = 1
n = {nro} counts (sufficient statistics of z)
Table 1: Notation used in this paper. Note that there is a
one-to-one mapping between z and (b,x). The informa-
tion relevant for evaluating the likelihood is n. We use
the following parallel notation: n?s = n(z?s),ns:b =
n(zs:b),?ns = n(?zs).
3.2 Choice representation of latent structure z
We represent the latent structure z as a set of local
choices:3
? HMM: z contains elements of the form
(T, i, a, b), denoting a transition from state
a at position i to state b at position i + 1; and
(E, i, a, w), denoting an emission of word w
from state a at position i.
? USM: z contains elements of the form (i, w), de-
noting the generation of word w at character po-
sition i extending to position i+ |w| ? 1.
? PTSG: z contains elements of the form (x, t), de-
noting the generation of tree fragment t rooted at
node x.
The choices z are connected to the parameters ?
as follows: p(z | ?) =
?
z?z ?z.r,z.o. Each choice
z ? z is identified with some z.r ? R and out-
come z.o. Intuitively, choice z was made by drawing
drawing z.o from the multinomial distribution ?z.r.
3.3 Prior
We place a Dirichlet process prior on ?r (Dirichlet
prior for finite outcome spaces): ?r ? DP(?r, ?r),
where ?r is a concentration parameter and ?r is a
fixed base distribution.
3We assume that z contains both a latent part and the ob-
served input x, i.e., x is a deterministic function of z.
575
Let nro(z) = |{z ? z : z.r = r, z.o = o}| be the
number of draws from ?r resulting in outcome o, and
nr? =
?
o nro be the number of times ?r was drawn
from. Let n(z) = {nro(z)} denote the vector of
sufficient statistics associated with choices z. When
it is clear from context, we simply write n for n(z).
Using these sufficient statistics, we can write p(z |
?) =
?
r,o ?
nro(z)
ro .
We now marginalize out ? using Dirichlet-
multinomial conjugacy, producing the following ex-
pression for the likelihood:
p(z) =
?
r?R
?
o (?ro?ro)
(nro(z))
?r(nr?(z))
, (3)
where a(k) = a(a+1) ? ? ? (a+k?1) is the ascending
factorial. (3) is the distribution that we will use for
sampling.
4 Type-Based Sampling
Having described the setup of the model, we now
turn to posterior inference of p(z | x).
4.1 Binary Representation
We first define a new representation of the latent
structure based on binary variables b so that there is
a bijection between z and (b,x); z was used to de-
fine the model, b will be used for inference. We will
use b to exploit the ideas from Section 2. Specifi-
cally, let b = {bs}s?S be a collection of binary vari-
ables indexed by a set of sites S.
? HMM: If the HMM hasK = 2 states, S is the set
of positions in the sequence. For each s ? S , bs
is the hidden state at s. The extension to general
K is considered at the end of Section 4.4.
? USM: S is the set of non-final positions in the
sequence. For each s ? S , bs denotes whether
a word boundary exists between positions s and
s+ 1.
? PTSG: S is the set of internal nodes in the parse
tree. For s ? S, bs denotes whether a tree frag-
ment is rooted at node s.
For each site s ? S, let zs:0 and zs:1 denote the
choices associated with the structures obtained by
setting the binary variable bs = 0 and bs = 1, re-
spectively. Define z?s
def
= zs:0 ? zs:1 to be the set
of choices that do not depend on the value of bs, and
n?s
def
= n(z?s) be the corresponding counts.
? HMM: z?s includes all but the transitions into
and out of the state at s plus the emission at s.
? USM: z?s includes all except the word ending at
s and the one starting at s+ 1 if there is a bound-
ary (bs = 1); except the word covering s if no
boundary exists (bs = 0).
? PTSG: z?s includes all except the tree fragment
rooted at node s and the one with leaf s if bs = 1;
except the single fragment containing s if bs = 0.
4.2 Sampling One Site
A token-based sampler considers one site s at a time.
Specifically, we evaluate the likelihoods of zs:0 and
zs:1 according to (3) and sample bs with probability
proportional to the likelihoods. Intuitively, this can
be accomplished by removing choices that depend
on bs (resulting in z?s), evaluating the likelihood re-
sulting from setting bs to 0 or 1, and then adding the
appropriate choices back in.
More formally, let ?zs:b
def
= zs:b\z?s be the new
choices that would be added if we set bs = b ?
{0, 1}, and let ?ns:b
def
= n(?zs:b) be the corre-
sponding counts. With this notation, we can write
the posterior as follows:
p(bs = b | b\bs) ? (4)
?
r?R
?
o (?ro?ro + n
?s
ro )
(?ns:bro )
(?r + n
?s
r? )
(?ns:br? )
.
The form of the conditional (4) follows from the
joint (3) via two properties: additivity of counts
(ns:b = n?s + ?ns:b) and a simple property of as-
cending factorials (a(k+?) = a(k)(a+ k)(?)).
In practice, most of the entries of ?ns:b are zero.
For the HMM, ns:bro would be nonzero only for
the transitions into the new state (b) at position s
(zs?1 ? b), transitions out of that state (b? zs+1),
and emissions from that state (b? xs).
4.3 Sampling Multiple Sites
We would like to sample multiple sites jointly as in
Section 2, but we cannot choose any arbitrary subset
S ? S, as the likelihood will in general depend on
the exact assignment of bS
def
= {bs}s?S , of which
576
a b c a a b c a b c b
(a) USM
1 1 2 2 1 1 2 2
a b a b c b b e
(b) HMM
a
b
a a
b c
d e
c
d
b c
e
a b
(c) PTSG
Figure 3: The type-based sampler jointly samples all vari-
ables at a set of sites S (in green boxes). Sites in S are
chosen based on types (denoted in red). (a) HMM: two
sites have the same type if they have the same previous
and next states and emit the same word; they conflict un-
less separated by at least one position. (b) USM: two sites
have the same type if they are both of the form ab|c or
abc; note that occurrences of the same letters with other
segmentations do not match the type. (c) PTSG: analo-
gous to the USM, only for tree rather than sequences.
there are an exponential number. To exploit the ex-
changeability property in Section 2, we need to find
sites which look ?the same? from the model?s point
of view, that is, the likelihood only depends on bS
via m
def
=
?
s?S bs.
To do this, we need to define two notions, type and
conflict. We say sites s and s? have the same type if
the counts added by setting either bs or bs? are the
same, that is, ?ns:b = ?ns
?:b for b ? {0, 1}. This
motivates the following definition of the type of site
s with respect to z:
t(z, s)
def
= (?ns:0,?ns:1), (5)
We say that s and s? have the same type if t(z, s) =
t(z, s?). Note that the actual choices added (?zs:b
and ?zs
?:b) are in general different as s and s? cor-
respond to different parts of the latent structure, but
the model only depends on counts and is indifferent
to this. Figure 3 shows examples of same-type sites
for our three models.
However, even if all sites in S have the same
type, we still cannot sample bS jointly, since chang-
ing one bs might change the type of another site s?;
indeed, this dependence is reflected in (5), which
shows that types depend on z. For example, s, s? ?
S conflict when s? = s + 1 in the HMM or when
s and s? are boundaries of one segment (USM) or
one tree fragment (PTSG). Therefore, one additional
concept is necessary: We say two sites s and s? con-
flict if there is some choice that depends on both bs
and bs? ; formally, (z\z?s) ? (z\z?s
?
) 6= ?.
Our key mathematical result is as follows:
Proposition 1 For any set S ? S of non-conflicting
sites with the same type,
p(bS | b\bS) ? g(m) (6)
p(m | b\bS) ?
(
|S|
m
)
g(m), (7)
for some easily computable g(m), where m =
?
s?S bs.
We will derive g(m) shortly, but first note from
(6) that the likelihood for a particular setting of bS
depends on bS only via m as desired. (7) sums
over all
(|S|
m
)
settings of bS with m =
?
s?S bs.
The algorithmic consequences of this result is that
to sample bS , we can first compute (7) for each
m ? {0, . . . , |S|}, sample m according to the nor-
malized distribution, and then choose the actual bS
uniformly subject to m.
Let us now derive g(m) by generalizing (4).
Imagine removing all sites S and their dependent
choices and adding in choices corresponding to
some assignment bS . Since all sites in S are non-
conflicting and of the same type, the count contribu-
tion ?ns:b is the same for every s ? S (i.e., sites
in S are exchangeable). Therefore, the likelihood
of the new assignment bS depends only on the new
counts:
?nS:m
def
= m?ns:1 + (|S| ?m)?ns:0. (8)
Using these new counts in place of the ones in (4),
we get the following expression:
g(m) =
?
r?R
?
o (?ro?ro + nro(z
?S))
(?nS:mro )
?r + nr?(z?S)
(?nS:mr? )
. (9)
4.4 Full Algorithm
Thus far, we have shown how to sample bS given
a set S ? S of non-conflicting sites with the same
type. To complete the description of the type-based
577
Type-Based Sampler
for each iteration t = 1, . . . , T :
?for each pivot site s0 ? S:
??S ? TB(z, s0) (S is the type block centered at s0)
??decrement n and remove from z based on bS
??sample m according to (7)
??sample M ? S with |M | = m uniformly at random
??set bs = I[s ?M ] for each s ? S
??increment n and add to z accordingly
Figure 4: Pseudocode for the general type-based sampler.
We operate in the binary variable representation b of z.
Each step, we jointly sample |S| variables (of the same
type).
sampler, we need to specify how to choose S. Our
general strategy is to first choose a pivot site s0 ? S
uniformly at random and then set S = TB(z, s0) for
some function TB. Call S the type block centered at
s0. The following two criteria on TB are sufficient
for a valid sampler: (A) s0 ? S, and (B) the type
blocks are stable, which means that if we change bS
to any b?S (resulting in a new z
?), the type block cen-
tered at s0 with respect to z? does not change (that
is, TB(z?, s0) = S). (A) ensures ergodicity; (B),
reversibility.
Now we define TB as follows: First set S = {s0}.
Next, loop through all sites s ? S with the same type
as s0 in some fixed order, adding s to S if it does
not conflict with any sites already in S. Figure 4
provides the pseudocode for the full algorithm.
Formally, this sampler cycles over |S| transition
kernels, one for each pivot site. Each kernel (in-
dexed by s0 ? S) defines a blocked Gibbs move,
i.e. sampling from p(bTB(z,s0) | ? ? ? ).
Efficient Implementation There are two oper-
ations we must perform efficiently: (A) looping
through sites with the same type as the pivot site s0,
and (B) checking whether such a site s conflicts with
any site in S. We can perform (B) in O(1) time by
checking if any element of ?zs:bs has already been
removed; if so, there is a conflict and we skip s. To
do (A) efficiently, we maintain a hash table mapping
type t to a doubly-linked list of sites with type t.
There is anO(1) cost for maintaining this data struc-
ture: When we add or remove a site s, we just need
to add or remove neighboring sites s? from their re-
spective linked lists, since their types depend on bs.
For example, in the HMM, when we remove site s,
we also remove sites s?1 and s+1.
For the USM, we use a simpler solution: main-
tain a hash table mapping each word w to a list of
positions where w occurs. Suppose site (position) s
straddles words a and b. Then, to perform (A), we
retrieve the list of positions where a, b, and ab occur,
intersecting the a and b lists to obtain a list of posi-
tions where a b occurs. While this intersection is
often much smaller than the pre-intersected lists, we
found in practice that the smaller amount of book-
keeping balanced out the extra time spent intersect-
ing. We used a similar strategy for the PTSG, which
significantly reduces the amount of bookkeeping.
Skip Approximation Large type blocks mean
larger moves. However, such a block S is also sam-
pled more frequently?once for every choice of a
pivot site s0 ? S. However, we found that empir-
ically, bS changes very infrequently. To eliminate
this apparent waste, we use the following approxi-
mation of our sampler: do not consider s0 ? S as
a pivot site if s0 belongs to some block which was
already sampled in the current iteration. This way,
each site is considered roughly once per iteration.4
Sampling Non-Binary Representations We can
sample in models without a natural binary represen-
tation (e.g., HMMs with with more than two states)
by considering random binary slices. Specifically,
suppose bs ? {1, . . . ,K} for each site s ? S .
We modify Figure 4 as follows: After choosing a
pivot site s0 ? S , let k = bs0 and choose k
? uni-
formly from {1, . . . ,K}. Only include sites in one
of these two states by re-defining the type block to
be S = {s ? TB(z, s0) : bs ? {k, k?}}, and sam-
ple bS restricted to these two states by drawing from
p(bS | bS ? {k, k?}|S|, ? ? ? ). By choosing a random
k? each time, we allow b to reach any point in the
space, thus achieving ergodicity just by using these
binary restrictions.
5 Experiments
We now compare our proposed type-based sampler
to various alternatives, evaluating on marginal like-
4A site could be sampled more than once if it belonged to
more than one type block during the iteration (recall that types
depend on z and thus could change during sampling).
578
lihood (3) and accuracy for our three models:
? HMM: We learned a K = 45 state HMM on
the Wall Street Journal (WSJ) portion of the Penn
Treebank (49208 sentences, 45 tags) for part-of-
speech induction. We fixed ?r to 0.1 and ?r to
uniform for all r.
For accuracy, we used the standard metric based
on greedy mapping, where each state is mapped
to the POS tag that maximizes the number of cor-
rect matches (Haghighi and Klein, 2006). We did
not use a tagging dictionary.
? USM: We learned a USM model on the
Bernstein-Ratner corpus from the CHILDES
database used in Goldwater et al (2006) (9790
sentences) for word segmentation. We fixed ?0 to
0.1. The base distribution ?0 penalizes the length
of words (see Goldwater et al (2009) for details).
For accuracy, we used word token F1.
? PTSG: We learned a PTSG model on sections 2?
21 of the WSJ treebank.5 For accuracy, we used
EVALB parsing F1 on section 22.6 Note this is a
supervised task with latent-variables, whereas the
other two are purely unsupervised.
5.1 Basic Comparison
Figure 5(a)?(c) compares the likelihood and accu-
racy (we use the term accuracy loosely to also in-
clude F1). The initial observation is that the type-
based sampler (TYPE) outperforms the token-based
sampler (TOKEN) across all three models on both
metrics.
We further evaluated the PTSG on parsing. Our
standard treebank PCFG estimated using maximum
likelihood obtained 79% F1. TOKEN obtained an F1
of 82.2%, and TYPE obtained a comparable F1 of
83.2%. Running the PTSG for longer continued to
5Following Petrov et al (2006), we performed an initial pre-
processing step on the trees involving Markovization, binariza-
tion, and collapsing of unary chains; words occurring once are
replaced with one of 50 ?unknown word? tokens, using base
distributions {?r} that penalize the size of trees, and sampling
the hyperparameters (see Cohn et al (2009) for details).
6To evaluate, we created a grammar where the rule proba-
bilities are the mean values under the PTSG distribution: this
involves taking a weighted combination (based on the concen-
tration parameters) of the rule counts from the PTSG samples
and the PCFG-derived base distribution. We used the decoder
of DeNero et al (2009) to parse.
improve the likelihood but actually hurt parsing ac-
curacy, suggesting that the PTSG model is overfit-
ting.
To better understand the gains from TYPE
over TOKEN, we consider three other alterna-
tive samplers. First, annealing (TOKENanneal) is
a commonly-used technique to improve mixing,
where (3) is raised to some inverse temperature.7
In Figure 5(a)?(c), we see that unlike TYPE,
TOKENanneal does not improve over TOKEN uni-
formly: it hurts for the HMM, improves slightly for
the USM, and makes no difference for the PTSG. Al-
though annealing does increase mobility of the sam-
pler, this mobility is undirected, whereas type-based
sampling increases mobility in purely model-driven
directions.
Unlike past work that operated on types (Wolff,
1988; Brown et al, 1992; Stolcke and Omohun-
dro, 1994), type-based sampling makes stochastic
choices, and moreover, these choices are reversible.
Is this stochasticity important? To answer this, we
consider a variant of TYPE, TYPEgreedy: instead
of sampling from (7), TYPEgreedy considers a type
block S and sets bs to 0 for all s ? S if p(bS =
(0, . . . , 0) | ? ? ? ) > p(bS = (1, . . . , 1) | ? ? ? ); else
it sets bs to 1 for all s ? S. From Figure 5(a)?(c),
we see that greediness is disastrous for the HMM,
hurts a little for USM, and makes no difference on
the PTSG. These results show that stochasticity can
indeed be important.
We consider another block sampler, SENTENCE,
which uses dynamic programming to sample all
variables in a sentence (using Metropolis-Hastings
to correct for intra-sentential type-level coupling).
For USM, we see that SENTENCE performs worse
than TYPE and is comparable to TOKEN, suggesting
that type-based dependencies are stronger and more
important to deal with than intra-sentential depen-
dencies.
5.2 Initialization
We initialized all samplers as follows: For the USM
and PTSG, for each site s, we place a boundary (set
bs = 1) with probability ?. For the HMM, we set bs
to state 1 with probability ? and a random state with
7We started with a temperature of 10 and gradually de-
creased it to 1 during the first half of the run, and kept it at 1
thereafter.
579
3 6 9 12
time (hr.)
-1.1e7
-0.9e7
-9.1e6
-7.9e6
-6.7e6
log
-lik
elih
ood
3 6 9 12
time (hr.)
0.1
0.2
0.4
0.5
0.6
acc
ura
cy
2 4 6 8
time (min.)
-3.7e5
-3.2e5
-2.8e5
-2.4e5
-1.9e5
log
-lik
elih
ood Token
Tokenanneal
Typegreedy
Type
Sentence
2 4 6 8
time (min.)
0.1
0.2
0.4
0.5
0.6
F 1
3 6 9 12
time (hr.)
-6.2e6
-6.0e6
-5.8e6
-5.7e6
-5.5e6
log
-lik
elih
ood
(a) HMM (b) USM (c) PTSG
0.2 0.4 0.6 0.8 1.0
?
-7.1e6
-7.0e6
-6.9e6
-6.8e6
-6.7e6
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
acc
ura
cy
0.2 0.4 0.6 0.8 1.0
?
-3.5e5
-3.1e5
-2.7e5
-2.3e5
-1.9e5
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
F 1
0.2 0.4 0.6 0.8 1.0
?
-5.7e6
-5.6e6
-5.6e6
-5.5e6
-5.5e6
log
-lik
elih
ood
(d) HMM (e) USM (f) PTSG
Figure 5: (a)?(c): Log-likelihood and accuracy over time. TYPE performs the best. Relative to TYPE, TYPEgreedy
tends to hurt performance. TOKEN generally works worse. Relative to TOKEN, TOKENanneal produces mixed results.
SENTENCE behaves like TOKEN. (d)?(f): Effect of initialization. The metrics were applied to the current sample after
15 hours for the HMM and PTSG and 10 minutes for the USM. TYPE generally prefers larger ? and outperform the
other samplers.
probability 1 ? ?. Results in Figure 5(a)?(c) were
obtained by setting ? to maximize likelihood.
Since samplers tend to be sensitive to initializa-
tion, it is important to explore the effect of initial-
ization (parametrized by ? ? [0, 1]). Figure 5(d)?(f)
shows that TYPE is consistently the best, whereas
other samplers can underperform TYPE by a large
margin. Note that TYPE favors ? = 1 in general.
This setting maximizes the number of initial types,
and thus creates larger type blocks and thus enables
larger moves. Larger type blocks also mean more
dependencies that TOKEN is unable to deal with.
6 Related Work and Discussion
Block sampling, on which our work is built, is a clas-
sical idea, but is used restrictively since sampling
large blocks is computationally expensive. Past
work for clustering models maintained tractabil-
ity by using Metropolis-Hastings proposals (Dahl,
2003) or introducing auxiliary variables (Swendsen
and Wang, 1987; Liang et al, 2007). In contrast,
our type-based sampler simply identifies tractable
blocks based on exchangeability.
Other methods for learning latent-variable models
include EM, variational approximations, and uncol-
lapsed samplers. All of these methods maintain dis-
tributions over (or settings of) the latent variables of
the model and update the representation iteratively
(see Gao and Johnson (2008) for an overview in the
context of POS induction). However, these methods
are at the core all token-based, since they only up-
date variables in a single example at a time.8
Blocking variables by type?the key idea of
this paper?is a fundamental departure from token-
based methods. Though type-based changes have
also been proposed (Brown et al, 1992; Stolcke and
Omohundro, 1994), these methods operated greed-
ily, and in Section 5.1, we saw that being greedy led
to more brittle results. By working in a sampling
framework, we were able bring type-based changes
to fruition.
8While EM technically updates all distributions over latent
variables in the E-step, this update is performed conditioned on
model parameters; it is this coupling (made more explicit in
collapsed samplers) that makes EM susceptible to local optima.
580
References
P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and
R. L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467?
479.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars. In
North American Association for Computational Lin-
guistics (NAACL), pages 548?556.
D. B. Dahl. 2003. An improved merge-split sampler for
conjugate Dirichlet process mixture models. Techni-
cal report, Department of Statistics, University of Wis-
consin.
J. DeNero, M. Bansal, A. Pauls, and D. Klein. 2009.
Efficient parsing for transducer grammars. In North
American Association for Computational Linguistics
(NAACL), pages 227?235.
J. Gao and M. Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Empirical Methods in Natural
Language Processing (EMNLP), pages 344?352.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Explor-
ing the effects of context. Cognition, 112:21?54.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In North American Associ-
ation for Computational Linguistics (NAACL), pages
320?327.
P. Liang, M. I. Jordan, and B. Taskar. 2007. A
permutation-augmented sampler for Dirichlet process
mixture models. In International Conference on Ma-
chine Learning (ICML).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP).
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
International Colloquium on Grammatical Inference
and Applications, pages 106?118.
R. H. Swendsen and J. S. Wang. 1987. Nonuniversal
critical dynamics in MC simulations. Physics Review
Letters, 58:86?88.
J. G. Wolff. 1988. Learning syntax and meanings
through optimization and distributional analysis. In
Categories and processes in language acquisition,
pages 179?215.
581
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590?599,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Dependency-Based Compositional Semantics
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Compositional question answering begins by
mapping questions to logical forms, but train-
ing a semantic parser to perform this mapping
typically requires the costly annotation of the
target logical forms. In this paper, we learn
to map questions to answers via latent log-
ical forms, which are induced automatically
from question-answer pairs. In tackling this
challenging learning problem, we introduce a
new semantic representation which highlights
a parallel between dependency syntax and effi-
cient evaluation of logical forms. On two stan-
dard semantic parsing benchmarks (GEO and
JOBS), our system obtains the highest pub-
lished accuracies, despite requiring no anno-
tated logical forms.
1 Introduction
What is the total population of the ten largest cap-
itals in the US? Answering these types of complex
questions compositionally involves first mapping the
questions into logical forms (semantic parsing). Su-
pervised semantic parsers (Zelle and Mooney, 1996;
Tang and Mooney, 2001; Ge and Mooney, 2005;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007; Kwiatkowski et al, 2010) rely on
manual annotation of logical forms, which is expen-
sive. On the other hand, existing unsupervised se-
mantic parsers (Poon and Domingos, 2009) do not
handle deeper linguistic phenomena such as quan-
tification, negation, and superlatives.
As in Clarke et al (2010), we obviate the need
for annotated logical forms by considering the end-
to-end problem of mapping questions to answers.
However, we still model the logical form (now as a
latent variable) to capture the complexities of lan-
guage. Figure 1 shows our probabilistic model:
(parameters) (world)
? w
x z y
(question) (logical form) (answer)state with thelargest area x1
11
c
argmax
area
state
?? Alaska
z ? p?(z | x)
y = JzKw
Semantic Parsing Evaluation
Figure 1: Our probabilistic model: a question x is
mapped to a latent logical form z, which is then evaluated
with respect to a world w (database of facts), producing
an answer y. We represent logical forms z as labeled
trees, induced automatically from (x, y) pairs.
We want to induce latent logical forms z (and pa-
rameters ?) given only question-answer pairs (x, y),
which is much cheaper to obtain than (x, z) pairs.
The core problem that arises in this setting is pro-
gram induction: finding a logical form z (over an
exponentially large space of possibilities) that pro-
duces the target answer y. Unlike standard semantic
parsing, our end goal is only to generate the correct
y, so we are free to choose the representation for z.
Which one should we use?
The dominant paradigm in compositional se-
mantics is Montague semantics, which constructs
lambda calculus forms in a bottom-up manner. CCG
is one instantiation (Steedman, 2000), which is used
by many semantic parsers, e.g., Zettlemoyer and
Collins (2005). However, the logical forms there
can become quite complex, and in the context of
program induction, this would lead to an unwieldy
search space. At the same time, representations such
as FunQL (Kate et al, 2005), which was used in
590
Clarke et al (2010), are simpler but lack the full ex-
pressive power of lambda calculus.
The main technical contribution of this work is
a new semantic representation, dependency-based
compositional semantics (DCS), which is both sim-
ple and expressive (Section 2). The logical forms in
this framework are trees, which is desirable for two
reasons: (i) they parallel syntactic dependency trees,
which facilitates parsing and learning; and (ii) eval-
uating them to obtain the answer is computationally
efficient.
We trained our model using an EM-like algorithm
(Section 3) on two benchmarks, GEO and JOBS
(Section 4). Our system outperforms all existing
systems despite using no annotated logical forms.
2 Semantic Representation
We first present a basic version (Section 2.1) of
dependency-based compositional semantics (DCS),
which captures the core idea of using trees to rep-
resent formal semantics. We then introduce the full
version (Section 2.2), which handles linguistic phe-
nomena such as quantification, where syntactic and
semantic scope diverge.
We start with some definitions, using US geogra-
phy as an example domain. Let V be the set of all
values, which includes primitives (e.g., 3, CA ? V)
as well as sets and tuples formed from other values
(e.g., 3, {3, 4, 7}, (CA, {5}) ? V). Let P be a set
of predicates (e.g., state, count ? P), which are
just symbols.
A world w is mapping from each predicate p ?
P to a set of tuples; for example, w(state) =
{(CA), (OR), . . . }. Conceptually, a world is a rela-
tional database where each predicate is a relation
(possibly infinite). Define a special predicate ? with
w(?) = V . We represent functions by a set of input-
output pairs, e.g., w(count) = {(S, n) : n = |S|}.
As another example, w(average) = {(S, x?) :
x? = |S1|?1
?
x?S1 S(x)}, where a set of pairs S
is treated as a set-valued function S(x) = {y :
(x, y) ? S} with domain S1 = {x : (x, y) ? S}.
The logical forms in DCS are called DCS trees,
where nodes are labeled with predicates, and edges
are labeled with relations. Formally:
Definition 1 (DCS trees) Let Z be the set of DCS
trees, where each z ? Z consists of (i) a predicate
RelationsR
j
j? (join) E (extract)
? (aggregate) Q (quantify)
Xi (execute) C (compare)
Table 1: Possible relations appearing on the edges of a
DCS tree. Here, j, j? ? {1, 2, . . . } and i ? {1, 2, . . . }?.
z.p ? P and (ii) a sequence of edges z.e1, . . . , z.em,
each edge e consisting of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We write a DCS tree z as ?p; r1 : c1; . . . ; rm : cm?.
Figure 2(a) shows an example of a DCS tree. Al-
though a DCS tree is a logical form, note that it looks
like a syntactic dependency tree with predicates in
place of words. It is this transparency between syn-
tax and semantics provided by DCS which leads to
a simple and streamlined compositional semantics
suitable for program induction.
2.1 Basic Version
The basic version of DCS restrictsR to join and ag-
gregate relations (see Table 1). Let us start by con-
sidering a DCS tree z with only join relations. Such
a z defines a constraint satisfaction problem (CSP)
with nodes as variables. The CSP has two types of
constraints: (i) x ? w(p) for each node x labeled
with predicate p ? P; and (ii) xj = yj? (the j-th
component of x must equal the j?-th component of
y) for each edge (x, y) labeled with jj? ? R.
A solution to the CSP is an assignment of nodes
to values that satisfies all the constraints. We say a
value v is consistent for a node x if there exists a
solution that assigns v to x. The denotation JzKw (z
evaluated on w) is the set of consistent values of the
root node (see Figure 2 for an example).
Computation We can compute the denotation
JzKw of a DCS tree z by exploiting dynamic pro-
gramming on trees (Dechter, 2003). The recurrence
is as follows:
J
?
p; j1j?1 :c1; ? ? ? ;
jm
j?m
:cm
?
K
w
(1)
= w(p) ?
m?
i=1
{v : vji = tj?i , t ? JciKw}.
At each node, we compute the set of tuples v consis-
tent with the predicate at that node (v ? w(p)), and
591
Example: major city in California
z = ?city; 11 :?major? ; 11 :?loc; 21 :?CA???
1
1
1
1
major
2
1
CA
loc
city ?c?m?`?s .
city(c) ? major(m)?
loc(`) ? CA(s)?c1 = m1 ? c1 = `1 ? `2 = s1
(a) DCS tree (b) Lambda calculus formula
(c) Denotation: JzKw = {SF, LA, . . . }
Figure 2: (a) An example of a DCS tree (written in both
the mathematical and graphical notation). Each node is
labeled with a predicate, and each edge is labeled with a
relation. (b) A DCS tree z with only join relations en-
codes a constraint satisfaction problem. (c) The denota-
tion of z is the set of consistent values for the root node.
for each child i, the ji-th component of v must equal
the j?i-th component of some t in the child?s deno-
tation (t ? JciKw). This algorithm is linear in the
number of nodes times the size of the denotations.1
Now the dual importance of trees in DCS is clear:
We have seen that trees parallel syntactic depen-
dency structure, which will facilitate parsing. In
addition, trees enable efficient computation, thereby
establishing a new connection between dependency
syntax and efficient semantic evaluation.
Aggregate relation DCS trees that only use join
relations can represent arbitrarily complex compo-
sitional structures, but they cannot capture higher-
order phenomena in language. For example, con-
sider the phrase number of major cities, and suppose
that number corresponds to the count predicate.
It is impossible to represent the semantics of this
phrase with just a CSP, so we introduce a new ag-
gregate relation, notated ?. Consider a tree ??:c?,
whose root is connected to a child c via ?. If the de-
notation of c is a set of values s, the parent?s denota-
tion is then a singleton set containing s. Formally:
J??:c?Kw = {JcKw}. (2)
Figure 3(a) shows the DCS tree for our running
example. The denotation of the middle node is {s},
1Infinite denotations (such as J<Kw) are represented as im-
plicit sets on which we can perform membership queries. The
intersection of two sets can be performed as long as at least one
of the sets is finite.
number ofmajor cities
12
11
?
11major
city
??
count
??
average population ofmajor cities
12
11
?
11
11major
city
population
??
average
??
(a) Counting (b) Averaging
Figure 3: Examples of DCS trees that use the aggregate
relation (?) to (a) compute the cardinality of a set and (b)
take the average over a set.
where s is all major cities. Having instantiated s as
a value, everything above this node is an ordinary
CSP: s constrains the count node, which in turns
constrains the root node to |s|.
A DCS tree that contains only join and aggre-
gate relations can be viewed as a collection of tree-
structured CSPs connected via aggregate relations.
The tree structure still enables us to compute deno-
tations efficiently based on (1) and (2).
2.2 Full Version
The basic version of DCS described thus far han-
dles a core subset of language. But consider Fig-
ure 4: (a) is headed by borders, but states needs
to be extracted; in (b), the quantifier no is syntacti-
cally dominated by the head verb borders but needs
to take wider scope. We now present the full ver-
sion of DCS which handles this type of divergence
between syntactic and semantic scope.
The key idea that allows us to give semantically-
scoped denotations to syntactically-scoped trees is
as follows: We mark a node low in the tree with a
mark relation (one of E, Q, or C). Then higher up in
the tree, we invoke it with an execute relation Xi to
create the desired semantic scope.2
This mark-execute construct acts non-locally, so
to maintain compositionality, we must augment the
2Our mark-execute construct is analogous to Montague?s
quantifying in, Cooper storage, and Carpenter?s scoping con-
structor (Carpenter, 1998).
592
California borders which states?
x1
2 111CA
e
??
state
border
?? Alaska borders no states.
x1
2 111AK
q
no
state
border
?? Some river traverses every city.
x12
2 111
q
some
river
q
every
city
traverse
??
x21
2 111
q
some
river
q
every
city
traverse
??
(narrow) (wide)
city traversed by no rivers
x12
1 2e?? 11
q
no
river
traverse
city
??
(a) Extraction (e) (b) Quantification (q) (c) Quantifier ambiguity (q,q) (d) Quantification (q,e)
state borderingthe most states
x12
1 1e?? 21
c
argmax
state
border
state
??
state borderingmore states than Texas
x12
1 1e?? 21
c
31TX
more
state
border
state
??
state borderingthe largest state
11
21
x12
1 1e??
c
argmax
size
state
??
border
state
x12
1 1e?? 21
11
c
argmax
size
state
border
state
??
(absolute) (relative)
Every state?slargest city is major.
x1
x2
1 111
21
q
every
state
loc
c
argmax
size
city
major
??
(e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (q,c)
Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the
syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic
scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi)
higher up at the desired semantic point.
denotation d = JzKw to include any information
about the marked nodes in z that can be accessed
by an execute relation later on. In the basic ver-
sion, d was simply the consistent assignments to the
root. Now d contains the consistent joint assign-
ments to the active nodes (which include the root
and all marked nodes), as well as information stored
about each marked node. Think of d as consisting
of n columns, one for each active node according to
a pre-order traversal of z. Column 1 always corre-
sponds to the root node. Formally, a denotation is
defined as follows (see Figure 5 for an example):
Definition 2 (Denotations) Let D be the set of de-
notations, where each d ? D consists of
? a set of arrays d.A, where each array a =
[a1, . . . , an] ? d.A is a sequence of n tuples
(ai ? V?); and
? a list of n stores d.? = (d.?1, . . . , d.?n),
where each store ? contains a mark relation
?.r ? {E, Q, C, ?}, a base denotation ?.b ?
D?{?}, and a child denotation ?.c ? D?{?}.
We write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. We
use d{ri = x} to mean d with d.ri = d.?i.r = x
(similar definitions apply for d{?i = x}, d{bi = x},
and d{ci = x}).
The denotation of a DCS tree can now be defined
recursively:
J?p?Kw = ??{[v] : v ? w(p)}; ???, (3)
J
?
p; e; jj? :c
?
K
w
= Jp; eKw ./j,j? JcKw, (4)
J?p; e; ?:c?Kw = Jp; eKw ./?,? ? (JcKw) , (5)
J?p; e; Xi :c?Kw = Jp; eKw ./?,? Xi(JcKw), (6)
J?p; e; E :c?Kw = M(Jp; eKw, E, c), (7)
J?p; e; C :c?Kw = M(Jp; eKw, C, c), (8)
J?p; Q :c; e?Kw = M(Jp; eKw, Q, c). (9)
593
11
21
11
c
argmax
size
state
border
state
J?Kw
column 1 column 2
A:
(OK)(NM)(NV)? ? ?
(TX,2.7e5)(TX,2.7e5)(CA,1.6e5)? ? ?r: ? cb: ? J?size?Kwc: ? J?argmax?KwDCS tree Denotation
Figure 5: Example of the denotation for a DCS tree with
a compare relation C. This denotation has two columns,
one for each active node?the root node state and the
marked node size.
The base case is defined in (3): if z is a sin-
gle node with predicate p, then the denotation of z
has one column with the tuples w(p) and an empty
store. The other six cases handle different edge re-
lations. These definitions depend on several opera-
tions (./j,j? ,?,Xi,M) which we will define shortly,
but let us first get some intuition.
Let z be a DCS tree. If the last child c of z?s
root is a join ( jj?), aggregate (?), or execute (Xi) re-
lation ((4)?(6)), then we simply recurse on z with c
removed and join it with some transformation (iden-
tity, ?, or Xi) of c?s denotation. If the last (or first)
child is connected via a mark relation E, C (or Q),
then we strip off that child and put the appropriate
information in the store by invoking M.
We now define the operations ./j,j? ,?,Xi,M.
Some helpful notation: For a sequence v =
(v1, . . . , vn) and indices i = (i1, . . . , ik), let vi =
(vi1 , . . . , vik) be the projection of v onto i; we write
v?i to mean v[1,...,n]\i. Extending this notation to
denotations, let ??A;???[i] = ??{ai : a ? A};?i??.
Let d[??] = d[?i], where i are the columns with
empty stores. For example, for d in Figure 5, d[1]
keeps column 1, d[??] keeps column 2, and d[2,?2]
swaps the two columns.
Join The join of two denotations d and d? with re-
spect to components j and j? (? means all compo-
nents) is formed by concatenating all arrays a of d
with all compatible arrays a? of d?, where compat-
ibility means a1j = a?1j? . The stores are also con-
catenated (?+??). Non-initial columns with empty
stores are projected away by applying ?[1,??]. The
full definition of join is as follows:
??A;??? ./j,j? ??A
?;???? = ??A??;? + ????[1,??],
A?? = {a + a? : a ? A,a? ? A?, a1j = a
?
1j?}. (10)
Aggregate The aggregate operation takes a deno-
tation and forms a set out of the tuples in the first
column for each setting of the rest of the columns:
? (??A;???) = ??A? ?A??;??? (11)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ??a1,a ? A,
?2 ? i ? n, [ai] ? d.bi[1].A}.
2.2.1 Mark and Execute
Now we turn to the mark (M) and execute (Xi)
operations, which handles the divergence between
syntactic and semantic scope. In some sense, this is
the technical core of DCS. Marking is simple: When
a node (e.g., size in Figure 5) is marked (e.g., with
relation C), we simply put the relation r, current de-
notation d and child c?s denotation into the store of
column 1:
M(d, r, c) = d{r1 = r, b1 = d, c1 = JcKw}. (12)
The execute operation Xi(d) processes columns
i in reverse order. It suffices to define Xi(d) for a
single column i. There are three cases:
Extraction (d.ri = E) In the basic version, the
denotation of a tree was always the set of con-
sistent values of the root node. Extraction al-
lows us to return the set of consistent values of a
marked non-root node. Formally, extraction sim-
ply moves the i-th column to the front: Xi(d) =
d[i,?(i, ?)]{?1 = ?}. For example, in Figure 4(a),
before execution, the denotation of the DCS tree
is ??{[(CA, OR), (OR)], . . . }; ?; (E, J?state?Kw, ?)??;
after applying X1, we have ??{[(OR)], . . . }; ???.
Generalized Quantification (d.ri = Q) Gener-
alized quantifiers are predicates on two sets, a re-
strictor A and a nuclear scope B. For example,
w(no) = {(A,B) : A ? B = ?} and w(most) =
{(A,B) : |A ?B| > 12 |A|}.
In a DCS tree, the quantifier appears as the
child of a Q relation, and the restrictor is the par-
ent (see Figure 4(b) for an example). This in-
formation is retrieved from the store when the
594
quantifier in column i is executed. In particu-
lar, the restrictor is A = ? (d.bi) and the nu-
clear scope is B = ? (d[i,?(i, ?)]). We then
apply d.ci to these two sets (technically, denota-
tions) and project away the first column: Xi(d) =
((d.ci ./1,1 A) ./2,1 B) [?1].
For the example in Figure 4(b), the de-
notation of the DCS tree before execution is
???; ?; (Q, J?state?Kw, J?no?Kw)??. The restrictor
set (A) is the set of all states, and the nuclear scope
(B) is the empty set. Since (A,B) exists in no, the
final denotation, which projects away the actual pair,
is ??{[ ]}?? (our representation of true).
Figure 4(c) shows an example with two interact-
ing quantifiers. The quantifier scope ambiguity is
resolved by the choice of execute relation; X12 gives
the narrow reading and X21 gives the wide reading.
Figure 4(d) shows how extraction and quantification
work together.
Comparatives and Superlatives (d.ri = C) To
compare entities, we use a set S of (x, y) pairs,
where x is an entity and y is a number. For su-
perlatives, the argmax predicate denotes pairs of
sets and the set?s largest element(s): w(argmax) =
{(S, x?) : x? ? argmaxx?S1 maxS(x)}. For com-
paratives, w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; formally:
w(more) = {(S, x, y) : maxS(x) > maxS(y)}.
In a superlative/comparative construction, the
root x of the DCS tree is the entity to be compared,
the child c of a C relation is the comparative or su-
perlative, and its parent p contains the information
used for comparison (see Figure 4(e) for an exam-
ple). If d is the denotation of the root, its i-th column
contains this information. There are two cases: (i) if
the i-th column of d contains pairs (e.g., size in
Figure 5), then let d? = J???Kw ./1,2 d[i,?i], which
reads out the second components of these pairs; (ii)
otherwise (e.g., state in Figure 4(e)), let d? =
J???Kw ./1,2 J?count?Kw ./1,1 ? (d[i,?i]), which
counts the number of things (e.g., states) that occur
with each value of the root x. Given d?, we construct
a denotation S by concatenating (+i) the second and
first columns of d? (S = ? (+2,1 (d?{?2 = ?})))
and apply the superlative/comparative: Xi(d) =
(J???Kw ./1,2 (d.ci ./1,1 S)){?1 = d.?1}.
Figure 4(f) shows that comparatives are handled
using the exact same machinery as superlatives. Fig-
ure 4(g) shows that we can naturally account for
superlative ambiguity based on where the scope-
determining execute relation is placed.
3 Semantic Parsing
We now turn to the task of mapping natural language
utterances to DCS trees. Our first question is: given
an utterance x, what trees z ? Z are permissible? To
define the search space, we first assume a fixed set
of lexical triggers L. Each trigger is a pair (x, p),
where x is a sequence of words (usually one) and p
is a predicate (e.g., x = California and p = CA).
We use L(x) to denote the set of predicates p trig-
gered by x ((x, p) ? L). Let L() be the set of
trace predicates, which can be introduced without
an overt lexical trigger.
Given an utterance x = (x1, . . . , xn), we define
ZL(x) ? Z , the set of permissible DCS trees for
x. The basic approach is reminiscent of projective
labeled dependency parsing: For each span i..j, we
build a set of trees Ci,j and set ZL(x) = C0,n. Each
set Ci,j is constructed recursively by combining the
trees of its subspans Ci,k and Ck?,j for each pair of
split points k, k? (words between k and k? are ig-
nored). These combinations are then augmented via
a functionA and filtered via a functionF , to be spec-
ified later. Formally, Ci,j is defined recursively as
follows:
Ci,j = F
(
A
(
L(xi+1..j) ?
?
i?k?k?<j
a?Ci,k
b?Ck?,j
T1(a, b))
))
.
(13)
In (13), L(xi+1..j) is the set of predicates triggered
by the phrase under span i..j (the base case), and
Td(a, b) = ~Td(a, b) ? ~T d(b, a), which returns all
ways of combining trees a and b where b is a de-
scendant of a (~Td) or vice-versa ( ~T d). The former is
defined recursively as follows: ~T0(a, b) = ?, and
~Td(a, b) =
?
r?R
p?L()
{?a; r :b?} ? ~Td?1(a, ?p; r :b?).
The latter ( ~T k) is defined similarly. Essentially,
~Td(a, b) allows us to insert up to d trace predi-
cates between the roots of a and b. This is use-
ful for modeling relations in noun compounds (e.g.,
595
California cities), and it also allows us to underspec-
ify L. In particular, our L will not include verbs or
prepositions; rather, we rely on the predicates corre-
sponding to those words to be triggered by traces.
The augmentation function A takes a set of trees
and optionally attaches E and Xi relations to the
root (e.g., A(?city?) = {?city? , ?city; E :??}).
The filtering function F rules out improperly-typed
trees such as ?city; 00 :?state??. To further reduce
the search space, F imposes a few additional con-
straints, e.g., limiting the number of marked nodes
to 2 and only allowing trace predicates between ar-
ity 1 predicates.
Model We now present our discriminative se-
mantic parsing model, which places a log-linear
distribution over z ? ZL(x) given an utter-
ance x. Formally, p?(z | x) ? e?(x,z)
>?,
where ? and ?(x, z) are parameter and feature vec-
tors, respectively. As a running example, con-
sider x = city that is in California and z =
?city; 11 :?loc;
2
1 :?CA???, where city triggers city
and California triggers CA.
To define the features, we technically need to
augment each tree z ? ZL(x) with alignment
information?namely, for each predicate in z, the
span in x (if any) that triggered it. This extra infor-
mation is already generated from the recursive defi-
nition in (13).
The feature vector ?(x, z) is defined by sums of
five simple indicator feature templates: (F1) a word
triggers a predicate (e.g., [city, city]); (F2) a word
is under a relation (e.g., [that, 11]); (F3) a word is un-
der a trace predicate (e.g., [in, loc]); (F4) two pred-
icates are linked via a relation in the left or right
direction (e.g., [city, 11, loc, RIGHT]); and (F5) a
predicate has a child relation (e.g., [city, 11]).
Learning Given a training dataset D con-
taining (x, y) pairs, we define the regu-
larized marginal log-likelihood objective
O(?) =
?
(x,y)?D log p?(JzKw = y | x, z ?
ZL(x)) ? ????22, which sums over all DCS trees z
that evaluate to the target answer y.
Our model is arc-factored, so we can sum over all
DCS trees in ZL(x) using dynamic programming.
However, in order to learn, we need to sum over
{z ? ZL(x) : JzKw = y}, and unfortunately, the
additional constraint JzKw = y does not factorize.
We therefore resort to beam search. Specifically, we
truncate each Ci,j to a maximum of K candidates
sorted by decreasing score based on parameters ?.
Let Z?L,?(x) be this approximation of ZL(x).
Our learning algorithm alternates between (i) us-
ing the current parameters ? to generate the K-best
set Z?L,?(x) for each training example x, and (ii)
optimizing the parameters to put probability mass
on the correct trees in these sets; sets contain-
ing no correct answers are skipped. Formally, let
O?(?, ??) be the objective function O(?) with ZL(x)
replaced with Z?L,??(x). We optimize O?(?, ??) by
setting ?(0) = ~0 and iteratively solving ?(t+1) =
argmax? O?(?, ?
(t)) using L-BFGS until t = T . In all
experiments, we set ? = 0.01, T = 5, andK = 100.
After training, given a new utterance x, our system
outputs the most likely y, summing out the latent
logical form z: argmaxy p?(T )(y | x, z ? Z?L,?(T )).
4 Experiments
We tested our system on two standard datasets, GEO
and JOBS. In each dataset, each sentence x is an-
notated with a Prolog logical form, which we use
only to evaluate and get an answer y. This evalua-
tion is done with respect to a world w. Recall that
a world w maps each predicate p ? P to a set of
tuples w(p). There are three types of predicates in
P: generic (e.g., argmax), data (e.g., city), and
value (e.g., CA). GEO has 48 non-value predicates
and JOBS has 26. For GEO, w is the standard US
geography database that comes with the dataset. For
JOBS, if we use the standard Jobs database, close to
half the y?s are empty, which makes it uninteresting.
We therefore generated a random Jobs database in-
stead as follows: we created 100 job IDs. For each
data predicate p (e.g., language), we add each pos-
sible tuple (e.g., (job37, Java)) to w(p) indepen-
dently with probability 0.8.
We used the same training-test splits as Zettle-
moyer and Collins (2005) (600+280 for GEO and
500+140 for JOBS). During development, we fur-
ther held out a random 30% of the training sets for
validation.
Our lexical triggers L include the following: (i)
predicates for a small set of ? 20 function words
(e.g., (most, argmax)), (ii) (x, x) for each value
596
System Accuracy
Clarke et al (2010) w/answers 73.2
Clarke et al (2010) w/logical forms 80.4
Our system (DCS with L) 78.9
Our system (DCS with L+) 87.2
Table 2: Results on GEO with 250 training and 250
test examples. Our results are averaged over 10 random
250+250 splits taken from our 600 training examples. Of
the three systems that do not use logical forms, our two
systems yield significant improvements. Our better sys-
tem even outperforms the system that uses logical forms.
predicate x in w (e.g., (Boston, Boston)), and
(iii) predicates for each POS tag in {JJ, NN, NNS}
(e.g., (JJ, size), (JJ, area), etc.).3 Predicates
corresponding to verbs and prepositions (e.g.,
traverse) are not included as overt lexical trig-
gers, but rather in the trace predicates L().
We also define an augmented lexicon L+ which
includes a prototype word x for each predicate ap-
pearing in (iii) above (e.g., (large, size)), which
cancels the predicates triggered by x?s POS tag. For
GEO, there are 22 prototype words; for JOBS, there
are 5. Specifying these triggers requires minimal
domain-specific supervision.
Results We first compare our system with Clarke
et al (2010) (henceforth, SEMRESP), which also
learns a semantic parser from question-answer pairs.
Table 2 shows that our system using lexical triggers
L (henceforth, DCS) outperforms SEMRESP (78.9%
over 73.2%). In fact, although neither DCS nor
SEMRESP uses logical forms, DCS uses even less su-
pervision than SEMRESP. SEMRESP requires a lex-
icon of 1.42 words per non-value predicate, Word-
Net features, and syntactic parse trees; DCS requires
only words for the domain-independent predicates
(overall, around 0.5 words per non-value predicate),
POS tags, and very simple indicator features. In
fact, DCS performs comparably to even the version
of SEMRESP trained using logical forms. If we add
prototype triggers (use L+), the resulting system
(DCS+) outperforms both versions of SEMRESP by
a significant margin (87.2% over 73.2% and 80.4%).
3We used the Berkeley Parser (Petrov et al, 2006) to per-
form POS tagging. The triggers L(x) for a word x thus include
L(t) where t is the POS tag of x.
System GEO JOBS
Tang and Mooney (2001) 79.4 79.8
Wong and Mooney (2007) 86.6 ?
Zettlemoyer and Collins (2005) 79.3 79.3
Zettlemoyer and Collins (2007) 81.6 ?
Kwiatkowski et al (2010) 88.2 ?
Kwiatkowski et al (2010) 88.9 ?
Our system (DCS with L) 88.6 91.4
Our system (DCS with L+) 91.1 95.0
Table 3: Accuracy (recall) of systems on the two bench-
marks. The systems are divided into three groups. Group
1 uses 10-fold cross-validation; groups 2 and 3 use the in-
dependent test set. Groups 1 and 2 measure accuracy of
logical form; group 3 measures accuracy of the answer;
but there is very small difference between the two as seen
from the Kwiatkowski et al (2010) numbers. Our best
system improves substantially over past work, despite us-
ing no logical forms as training data.
Next, we compared our systems (DCS and DCS+)
with the state-of-the-art semantic parsers on the full
dataset for both GEO and JOBS (see Table 3). All
other systems require logical forms as training data,
whereas ours does not. Table 3 shows that even DCS,
which does not use prototypes, is comparable to the
best previous system (Kwiatkowski et al, 2010), and
by adding a few prototypes, DCS+ offers a decisive
edge (91.1% over 88.9% on GEO). Rather than us-
ing lexical triggers, several of the other systems use
IBM word alignment models to produce an initial
word-predicate mapping. This option is not avail-
able to us since we do not have annotated logical
forms, so we must instead rely on lexical triggers
to define the search space. Note that having lexical
triggers is a much weaker requirement than having
a CCG lexicon, and far easier to obtain than logical
forms.
Intuitions How is our system learning? Initially,
the weights are zero, so the beam search is essen-
tially unguided. We find that only for a small frac-
tion of training examples do the K-best sets contain
any trees yielding the correct answer (29% for DCS
on GEO). However, training on just these exam-
ples is enough to improve the parameters, and this
29% increases to 66% and then to 95% over the next
few iterations. This bootstrapping behavior occurs
naturally: The ?easy? examples are processed first,
where easy is defined by the ability of the current
597
model to generate the correct answer using any tree.
Our system learns lexical associations between
words and predicates. For example, area (by virtue
of being a noun) triggers many predicates: city,
state, area, etc. Inspecting the final parameters
(DCS on GEO), we find that the feature [area, area]
has a much higher weight than [area, city]. Trace
predicates can be inserted anywhere, but the fea-
tures favor some insertions depending on the words
present (for example, [in, loc] has high weight).
The errors that the system makes stem from mul-
tiple sources, including errors in the POS tags (e.g.,
states is sometimes tagged as a verb, which triggers
no predicates), confusion of Washington state with
Washington D.C., learning the wrong lexical asso-
ciations due to data sparsity, and having an insuffi-
ciently large K.
5 Discussion
A major focus of this work is on our semantic rep-
resentation, DCS, which offers a new perspective
on compositional semantics. To contrast, consider
CCG (Steedman, 2000), in which semantic pars-
ing is driven from the lexicon. The lexicon en-
codes information about how each word can used in
context; for example, the lexical entry for borders
is S\NP/NP : ?y.?x.border(x, y), which means
borders looks right for the first argument and left
for the second. These rules are often too stringent,
and for complex utterances, especially in free word-
order languages, either disharmonic combinators are
employed (Zettlemoyer and Collins, 2007) or words
are given multiple lexical entries (Kwiatkowski et
al., 2010).
In DCS, we start with lexical triggers, which are
more basic than CCG lexical entries. A trigger for
borders specifies only that border can be used, but
not how. The combination rules are encoded in the
features as soft preferences. This yields a more
factorized and flexible representation that is easier
to search through and parametrize using features.
It also allows us to easily add new lexical triggers
without becoming mired in the semantic formalism.
Quantifiers and superlatives significantly compli-
cate scoping in lambda calculus, and often type rais-
ing needs to be employed. In DCS, the mark-execute
construct provides a flexible framework for dealing
with scope variation. Think of DCS as a higher-level
programming language tailored to natural language,
which results in programs (DCS trees) which are
much simpler than the logically-equivalent lambda
calculus formulae.
The idea of using CSPs to represent semantics is
inspired by Discourse Representation Theory (DRT)
(Kamp and Reyle, 1993; Kamp et al, 2005), where
variables are discourse referents. The restriction to
trees is similar to economical DRT (Bos, 2009).
The other major focus of this work is program
induction?inferring logical forms from their deno-
tations. There has been a fair amount of past work on
this topic: Liang et al (2010) induces combinatory
logic programs in a non-linguistic setting. Eisen-
stein et al (2009) induces conjunctive formulae and
uses them as features in another learning problem.
Piantadosi et al (2008) induces first-order formu-
lae using CCG in a small domain assuming observed
lexical semantics. The closest work to ours is Clarke
et al (2010), which we discussed earlier.
The integration of natural language with denota-
tions computed against a world (grounding) is be-
coming increasingly popular. Feedback from the
world has been used to guide both syntactic parsing
(Schuler, 2003) and semantic parsing (Popescu et
al., 2003; Clarke et al, 2010). Past work has also fo-
cused on aligning text to a world (Liang et al, 2009),
using text in reinforcement learning (Branavan et al,
2009; Branavan et al, 2010), and many others. Our
work pushes the grounded language agenda towards
deeper representations of language?think grounded
compositional semantics.
6 Conclusion
We built a system that interprets natural language
utterances much more accurately than existing sys-
tems, despite using no annotated logical forms. Our
system is based on a new semantic representation,
DCS, which offers a simple and expressive alter-
native to lambda calculus. Free from the burden
of annotating logical forms, we hope to use our
techniques in developing even more accurate and
broader-coverage language understanding systems.
Acknowledgments We thank Luke Zettlemoyer
and Tom Kwiatkowski for providing us with data
and answering questions.
598
References
J. Bos. 2009. A controlled fragment of DRT. In Work-
shop on Controlled Natural Language, pages 1?5.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL). Association for Computa-
tional Linguistics.
B. Carpenter. 1998. Type-Logical Semantics. MIT Press.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Compu-
tational Natural Language Learning (CoNLL), pages
9?16, Ann Arbor, Michigan.
H. Kamp and U. Reyle. 1993. From Discourse to Logic:
An Introduction to the Model-theoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Kluwer, Dordrecht.
H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse
representation theory. In Handbook of Philosophical
Logic.
R. J. Kate and R. J. Mooney. 2007. Learning lan-
guage semantics from ambiguous supervision. In As-
sociation for the Advancement of Artificial Intelligence
(AAAI), pages 895?900, Cambridge, MA. MIT Press.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages. In
Association for the Advancement of Artificial Intel-
ligence (AAAI), pages 1062?1068, Cambridge, MA.
MIT Press.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Learning
programs: A hierarchical Bayesian approach. In In-
ternational Conference on Machine Learning (ICML).
Omnipress.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440. Associa-
tion for Computational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI).
W. Schuler. 2003. Using model-theoretic semantic inter-
pretation to guide statistical parsing and word recog-
nition in a spoken language interface. In Association
for Computational Linguistics (ACL). Association for
Computational Linguistics.
M. Steedman. 2000. The Syntactic Process. MIT Press.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In European Conference on Ma-
chine Learning, pages 466?477.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967, Prague, Czech Republic.
Association for Computational Linguistics.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP/CoNLL), pages 678?687.
599
