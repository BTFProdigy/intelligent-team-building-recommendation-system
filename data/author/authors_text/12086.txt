Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 24?31,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatically Extracting Word Relationships  as Templates for Pun Generation   Bryan Anthony Hong  and  Ethel Ong College of Computer Studies De La Salle University Manila, 1004 Philippines bashx5@yahoo.com, ethel.ong@delasalle.ph     Abstract 
Computational models can be built to capture the syntactic structures and semantic patterns of human punning riddles. This model is then used as rules by a computer to generate its own puns. This paper presents T-PEG, a sys-tem that utilizes phonetic and semantic lin-guistic resources to automatically extract word relationships in puns and store the knowledge in template form. Given a set of training ex-amples, it is able to extract 69.2% usable tem-plates, resulting in computer-generated puns that received an average score of 2.13 as com-pared to 2.70 for human-generated puns from user feedback. 
1 Introduction Previous works in computational humor have shown that by analyzing the syntax and semantics of how humans combine words to produce puns, computational models can be built to capture the linguistic aspects involved in this creative word-play. The model is then used in the design of com-puter systems that can generate puns which are almost at par with those of human-generated puns, as the case of the Joke Analysis and Production Engine or JAPE (Binsted et al 1997) system. The computational model used by the JAPE (Binsted, 1996) system is in the form of schemas and templates with rules describing the linguistic structures of human puns. The use of templates in NLP tasks is not new. Information extraction sys-tems (Muslea, 1999) have used templates as rules for extracting relevant information from large, un-structured text. Text generation systems use tem-
plates as linguistic patterns with variables (or slots) that can be filled in to generate syntactically cor-rect and coherent text for their human readers.  One common characteristic among these NLP systems was that the templates were constructed manually. This is a tedious and time-consuming task. Because of this, several researches in exam-ple-based machine translation systems, such as those in (Cicekli and G?venir, 2003) and in (Go et al 2007), have worked on automatically extracting templates from training examples. The learned templates are bilingual pairs of patterns with corre-sponding words and phrases replaced with vari-ables. Each template is a complete sentence to preserve the syntax and word order in the source text, regardless of the variance in the sentence structures of the source and target languages (Nunez et al 2008).  The motivation for T-PEG (Template-Based Pun Extractor and Generator) is to build a model of human-generated puns through the automatic iden-tification, extraction and representation of the word relationships in a template, and then using these templates as patterns for the computer to generate its own puns. T-PEG does not maintain its own lexical resources, but instead relies on publicly available lexicons, in order to perform these tasks. The linguistic aspects of puns and the resources utilized by T-PEG are presented in Section 2. Sections 3 and 4 discuss the algorithms for ex-tracting templates and generating puns, respec-tively. The tests conducted and the analysis of the results on the learned templates and generated puns follow in Section 5, to show the limitations of T-PEG?s approach and the level of humor in the gen-erated puns. The paper concludes with a summary of what T-PEG has been able to accomplish. 
24
2 Linguistic Resources  Ritchie (2005) defines a pun as ?a humorous writ-ten or spoken text which relies crucially on pho-netic similarity for its humorous effect?. Puns can be based on inexact matches between words (Bin-sted and Ritchie, 2001), where tactics include me-tathesis (e.g., throw stones and stow thrones) and substitution of a phonetically similar segment (e.g., glass and grass). In T-PEG, punning riddles are considered to be a class of jokes that use wordplay, specifically pronunciation, spelling, and possible semantic similarities and differences between words (Hong and Ong, 2008). Only puns using the question - answer format as shown in example (1) from (Bin-sted, 1996) are considered. Compound words are also included, underlined in example (2) from (Webb, 1978). (1) What do you call a beloved mammal? A dear deer. (2) What do barbers study? Short-cuts. The automatic tasks of analyzing human-generated puns in order to build a formal model of the word relationships present in the puns require the use of a number of linguistic resources. These same set of resources are used for later generation. STANDUP (Manurung et al 2008), for example, uses ?a database of word definitions, sounds and syntax to generate simple play-on-words jokes, or puns, on a chosen subject?. Aside from using WordNet (2006) as its lexical resource, STANDUP maintains its own lexical database of phonetic similarity ratings for pairs of words and phrases. Various works have already emphasized that puns can be generated by distorting a word in the source pun into a similar-sounding pun, e.g., (Ritchie, 2005 and Manurung et al 2008). This notion of phonetic similarity can be extended fur-ther by allowing puns containing words that sound similar to be generated, as shown in example (3), which was generated by T-PEG following the structure of (1). (3)  What do you call an overall absence? A whole hole. The Unisyn English Pronunciation lexicon (Fitt, 2002) was utilized for this purpose. The dictionary contains about 70,000 entries with phonetic tran-scriptions and is used by T-PEG to find the pro-nunciation of individual words and to locate 
similar sounding words for a given word. Because Unisyn also provides support in checking for spell-ing regularity, it is also used by T-PEG to check if a given word does exist, particularly when a com-pound word is split into its constituent syllables and determining if these individual syllables are valid words, such as the constituents ?short? and ?cuts? for the compound word ?shortcuts? in (2). The wordplay in punning riddles is not based on phonetic similarity alone, but may also involve the semantic links among words that make up the pun. These semantic relationships must also be identi-fied and captured in the template, such that the generated puns are not  only syntactically well-formed (due to the nature of templates) but also have consistent semantics with the source human pun, as shown in example (4) from (Binsted, 1996) and T-PEG?s counterpart in example (5). (4)   How is a car like an elephant? They both have trunks. (5)   How is a person like an elephant? They both have memory. Two resources are utilized for this purpose. WordNet (2006) is used to find the synonym of a given word, while ConceptNet (Liu and Singh, 2004) is used to determine the semantic relation-ships of words.  ConceptNet is a large-scale common sense knowledge base with about 1.6 million assertions. It focuses on contextual common sense reasoning, which can be used by a computer to understand concepts and situating these concepts on previous knowledge.   Relationship Types Examples IsA IsA headache pain IsA deer mammal PartOf PartOf window pane PartOf car trunk  PropertyOf PropertyOf pancake flat PropertyOf ghost dead MadeOf MadeOf snowman snow CapableOf CapableOf sun burn CapableOf animal eat LocationOf LocationOf money bank CanDo CanDo ball bounce ConceptuallyRelatedTo ConceptuallyRelatedTo wedding bride forest animal Table 1. Some Semantic Relationships of Concept-Net (Liu and Singh, 2004) 
25
The concepts can be classified into three general classes ? noun phrases, attributes, and activity phrases, and are connected by edges to form an ontology. Binary relationship types defined by the Open Mind Commonsense (OMCS) Project (Liu and Singh, 2004) are used to relate two concepts together, examples of which are shown in Table 1. 3 Extracting Punning Templates The structural regularities of puns are captured in T-PEG with the use of templates. A template is the combined notion of schemas and templates in (Binsted, 2006), and it contains the relationship between the words (lexemes) in a pun as well as its syntactical structure. The template constrains the set of words that can be used to fill-in the slots dur-ing the generation phase; it also preserves the syn-tactical structure of the source pun, to enable the generated puns to follow the same syntax. 3.1 Templates in T-Peg A template in T-PEG is composed of multiple parts. The first component is the source punning riddle, where variables replaced the keywords in the pun and also serve as slots that can be filled during the pun generation phase. Variables can be one of three types. A regular variable is a basic keyword in the source pun whose part-of-speech tag is a noun, a verb, or an adjective. Variables in the question-part of the pun are represented with Xn while Yn represent vari-ables in the answer-part (where n denotes the lexi-cal sequence of the word in the sentence starting at index 0). A similar-sound variable represents a word that has the same pronunciation as the regular variable, for example, deer and dear. A compound-word variable contains two regular or similar-sound variables that combine to form a word, for example sun and burn combine to form the word sunburn. A colon (:) is used to connect the variables com-prising a compound variable, for example, X1:X2. Word relationships may exist among the vari-ables in a pun. These word relationships comprise the second component of a template and are repre-sented <var1> <relationship type> <var2>. There are four types of binary word relation-ships captured by T-PEG. SynonymOf relation-ships specify that two variables are synonymous 
with each other, as derived from WordNet (2006). Compound-word (or IsAWord) relationships spec-ify that one variable combined with a second vari-able should form a word. Unisyn (Fiit, 2002) is used to check that the individual constituents as well as the combined word are valid. SoundsLike relationships specify that two variables have the same pronunciation as derived from Unisyn. Se-mantic relationships show the relationships of two variables derived from ConceptNet  (Liu and Singh, 2004), and can be any one of the relation-ship types presented in Table 1. 3.2 Learning Algorithm Template learning begins from a given corpus of training examples that is preprocessed by the tag-ger and the stemmer. The tagged puns undergo valid word selection to identify keywords (noun, verb, or adjective) as candidate variables. The can-didate variables are then paired with each other to identify any word relationships that may exist be-tween them. The word relationships are determined by the phonetic checker, the synonym checker, and the semantic analyzer. Only those candidate vari-ables with at least one word relationship with an-other candidate variable will be retained as final variables in the learned template. Table 2 presents the template for ?Which bird can lift the heaviest weights? The crane.? (Webb, 1978). Keywords are underlined. All of the ex-tracted word relationships in Table 2 were derived from ConceptNet. Notice that i) some word pairs may have one or more word relationships, for ex-ample, ?crane? and ?lift?; while ii) some candidate keywords may not have any relationships, i.e, the adjective ?heaviest?, thus it is not replaced with a variable in the resulting template. This second condition will be explored further in Section 5.  Source Pun Which bird can lift the heaviest weights? The crane. Template  Which <X1> can <X3> the heaviest <X6>? The <Y1>. Word Re-lationships X1 ConceptuallyRelatedTo X6 X6 ConceptuallyRelatedTo X1 Y1 IsA X1 X6 CapableOfReceivingAction X3 Y1 CapableOf X3 Y1 UsedFor X3 Table 2. Template with Semantic Relationships   identified through ConceptNet 
26
Table 3 presents another template from the pun ?What do you call a beloved mammal? A dear deer.? (Binsted, 1996), with the SynonymOf relationship derived from WordNet, the IsA relationship from ConceptNet, and the SoundsLike relationship from Unisyn. Notice the ?-0? suffix in variables Y1 and Y2. ?<var>-0? is used to repre-sent a word that is phonetically similar to <var>.  Source Pun What do you call a beloved mammal? A dear deer. Template  What do you call a <X5> <X6>? A <Y1> <Y2>. Word Re-lationships X5 SynonymOf Y1 X5 SynonymOf Y2-0 Y1-0 IsA X6 Y2 IsA X6 Y1 SoundsLike Y2 Y1-0 SoundsLike Y1 Y2-0 SoundsLike Y2 Table 3. Template with Synonym Relationships and Sounds-Like Relationships  A constituent word in a compound word (identi-fied through the presence of a dash ?-?) may also contain additional word relationships. Thus, in ?What  kind of fruit fixes taps? A plum-ber.? (Bin-sted, 1996), T-PEG learns the template shown in Table 4. The compound word relationship ex-tracted is Y1 IsAWord Y2 (plum IsAWord ber). Y1 (plum), which is a constituent of the compound word, has a relationship with another word in the pun, X3 (fruit).  Source Pun What kind of fruit fixes taps?  A plum-ber. Template  What kind of <X3> <X4> taps? A <Y1>:<Y2>. Word Re-lationships Y1 IsA X3 Y1 IsAWord Y2 Y1:Y2 CapableOf X4 Table 4. Template with Compound Word  The last phase of the learning algorithm in-volves template usability check to determine if the extracted template has any missing link. A tem-plate is usable if all of the word relationships form a connected graph. If the graph contains unreach-able node/s (that is, it has missing edges), the tem-plate cannot be used in the pun generation phase since not all of the variables will be filled with possible words. 
Consider a template with four variables named X3, X4, Y1 and Y2. The word relationships X3-X4, X4-Y1 and Y1-Y2 form a connected graph as shown in Figure 1(a). However, if only X3-X4 and Y1-Y2 relationships are available as shown in Figure 1(b), there is a missing edge such that if variable X3 has an initial possible word and is the starting point for generation, a corresponding word for variable X4 can be derived through the X3-X4 edge, but no words can be derived for variables Y1 and Y2.  
  (a) Connected Graph (b) Graph with Missing Edge Figure 1. Graphs for Word Relationships   This condition is exemplified in Table 5, where two disjoint subgraphs are created as a result of the missing ?house-wall? and ?wall-wal? relationships. Further discussion on this is found in Section 5.   Source Pun What nuts can you use to build a house? Wal-nuts. (Binsted, 1996) Template  What <X1> can you use to <X6> a <X8>? <Y0>-<Y1>. Word Re-lationships X8 CapableOfReceivingAction X6 X1 SoundsLike Y1 Y0 IsAWord Y1 Y0:Y1 IsA X1 Missing Relations Y0-0 PartOf X8 Y0-0 SoundsLike Y0 Table 5. Template with Missing Word Relationships where Y0-0 is the word ?wall? 4 Generating Puns from Templates The pun generation phase, having access to the library of learned templates and utilizing the same set of linguistic resources as the template learning algorithm, begins with a keyword input from the user. For each of the usable templates in the li-brary, the keyword is tested on each variable with the same POS tag, except for SoundsLike and IsA-Word relationships where tags are ignored. When a variable has a word, it is used to populate other variables with words that satisfy the word relation-ships in the template. 
27
T-PEG uses two approaches of populating the variables ? forward recursion and backward recur-sion. Forward recursion involves traversing the graph by moving from one node (variable in a template) to the next and following the edges of relationships. Consider the template in Table 6.  Human Joke How is a window like a headache? They are both panes. (Binsted, 1996) Template  How is a <X3> like a <X6>?  They are both <Y3>. Word Re-lationships Y3-0 SoundsLike Y3 X3 ConceptuallyRelatedTo Y3 Y3 ConceptuallyRelatedTo X3 Y3 PartOf X3 X6 ConceptallyRelatedTo Y3-0 X6 IsA Y3-0 Y3-0 ConceptuallyRelatedTo X6 Table 6. Sample Template for Pun Generation  Given the keyword ?garbage?, one possible se-quence of activities to locate words and populate the variables in this template is as follows: a. ?garbage? is tried on variable X6. b. X6 has three word relationships all of which are with Y3-0, so it is used to find possible words for Y3-0. ConceptNet returns an ?IsA? relationship with the word ?waste?. c. Y3-0 has only one word relationship and this is with Y3. Unisyn returns the phonetically similar word ?waist?. d. Y3 has two possible relationships with X3, and ConceptNet satisfies the ?PartOf? rela-tionship with the word ?trunk?. Since two variables may have more than one word relationships connecting them, relationship grouping is also performed. A word relationship group is said to be satisfied if at least one of the word relationships in the group is satisfied. Table 7 shows the relationship grouping and the word rela-tionship that was satisfied in each group for the template in Table 6.  Word Relationship Filled Template X6 ConceptallyRelatedTo Y3-0 X6 IsA Y3-0 Y3-0 ConceptuallyRelatedTo X6  garbage IsA waste Y3-0 SoundsLike Y3 waste SoundsLike waist X3 ConceptuallyRelatedTo Y3 Y3 ConceptuallyRelatedTo X3 Y3 PartOf X3    waist PartOf trunk  Table 7. Relationship Groups and Filled Template 
The filled template is passed to the surface real-izer, LanguageTool (Naber, 2007), to fix gram-matical errors, before displaying the resulting pun ?How is a trunk like a garbage? They are both waists.? to the user. The forward recursion approach may lead to a situation in which a variable has been filled with two different sets of words. This usually occurs when the graph contains a cycle, as shown in Fig-ure 2.  
 Figure 2. Graph with Cycle  Assume the process of populating the template begins at X0. The following edges and resulting set of possible words are retrieved in sequence: a. X0-X1 (Words retrieved for X1 ? A, B) b. X1-X2 (Words retrieved for X2 ? D, E, F) c. X2-X3 (Words retrieved for X3 ? G, H) d. X3-X1 (Words retrieved for X1 ? B, C) When the forward recursion algorithm reaches X3 in step (d), a second set of possible words for X1 is generated. Since the two sets of words for X1 do not match, the algorithm gets the intersection of (A, B) and (B, C) and assigns this to X1 (in this case, the word ?B? is assigned to X1). Backward recursion has to be performed starting from step (b) using the new set of words so that other vari-ables with relationships to X1 will also be checked for possible changes in their values. 5 Test Results Various tests were conducted to validate the com-pleteness of the word relationships in the learned template, the correctness of the generation algo-rithm, and the quality of the generated puns.  5.1 Evaluating the Learned Templates The corpus used in training T-PEG contained 39 punning riddles derived from JAPE (Binsted, 1996) and The Crack-a-Joke Book (Webb, 1978). Since one template is learned from each source 
28
pun, the size of the corpus is not a factor in deter-mining the quality of the generated jokes. Of the 39 resulting templates, only 27 (69.2%) are usable. The unusable templates contain missing word relationships that are caused by two factors. Unisyn contains entries only for valid words and not for syllables. Thus, in (6), the relationship be-tween ?house? and ?wall? is missing in the learned template shown in Table 5 because ?wal? is not found in Unisyn to produce ?wall?. In (7), Con-ceptNet is unable to determine the relationship be-tween ?infantry? and ?army?.  (6)  What nuts can you use to build a house? Wal-nuts. (Binsted, 1996) (7)  What part of the army could a baby join? The infant-ry. (Webb, 1978) The generation algorithm relies heavily on the presence of correct word relationships. 10 of the 27 usable templates were selected for manual evalua-tion by a linguist to determine the completeness of the extracted word relationships. A template is said to be complete if it is able to capture the essential word relationships in a pun. The evaluation criteria are based on the number of incorrect relationships as identified by the linguist, and includes missing relationship, extra relationship, or incorrect word pairing. A scoring system from 1 to 5 is used, where 5 means there are no incorrect relationship, 4 means there is one incorrect relationship, and so on. The learning algorithm received an average score of 4.0 out of 5, due to missing word relation-ships in some of the templates. Again, these were caused by limitations of the resources. For exam-ple, in (8), the linguist noted that no relationship between ?heaviest? and ?weight? (i.e., PropertyOf heavy weight) is included in the learned template presented in Table 2. (8)  What bird can lift the heaviest weights? The crane. (Webb, 1978) (9)  What kind of fruit fixes taps? The plum-ber. (Binsted, 1996) In (9), the linguist identified a missing relation-ship between ?tap? and ?plumber?, which is not extracted by the template shown in Table 4. The linguist also noted that the constituents of a compound word do not always form valid words, such as ?ber? in plum-ber of pun (9), and ?wal? in wal-nuts of pun (6). This type of templates were 
considered to contain incorrect relationships, and they may cause problems during generation be-cause similar sounding words could not be found for the constituent of the compound word that is not a valid word. 5.2 Evaluating the Generation Algorithm The generation algorithm was evaluated on two aspects. In the first test, a keyword from each of the source puns was used as input to T-PEG to de-termine if it can generate back the training corpus. From the 27 usable templates, 20 (74.07%) of the source puns were generated back. Regeneration failed in cases where a word in the source pun has multiple POS tags, as the case in (10), where ?cut? is tagged as a noun during learning, but verb dur-ing generation. In the learning phase, tagging is done at the sentence level, as opposed to a single-word tagging in the generation phase. (10)  What do barbers study? Short-cuts. (Webb, 1978) Since a keyword is tried on each variable with the same POS tag in the template, the linguistic resources provided the generation algorithm with a large set of possible words. Consider again the pun in (10), using its template and given the keyword ?farmer? as an example, the system generated 122 possible puns, some of which are listed in Table 8. Notice that only a couple of these seemed plausible puns, i.e., #3 and #7.  1. What do farmers study?  Egg - plant. 2. What do farmers study?  Power - plant. 3. What do farmers study?  Trans - plant. 4. What do farmers study?  Battle - ground. 5. What do farmers study?  Play - ground. 6. What do farmers study?  Battle - field. 7. What do farmers study?  Gar - field. Table 8. Excerpt of the Generated Puns Using ?farmer? as Keyword  In order to find out how this affects the overall performance of the system, the execution times in locating words for the different types of word rela-tionships were measured for the set of 20 regener-ated human puns. Table 9 shows the summary for the running time and the number of word relation-ships extracted for each relationship type. Another test was also conducted to validate the previous finding. A threshold for the maximum 
29
number of possible words to be generated was set to 50, resulting in a shorter running time as de-picted in Table 10. A negative outcome of using a threshold value is that only 16 (instead of 20) hu-man puns were regenerated. The other four cases failed because the threshold became restrictive and filtered out the words that should be generated.  Relationship Type Running Time # Relationships Synonym 2 seconds 2 IsAWord 875 seconds 5 Semantic 1,699 seconds 82 SoundsLike 979 seconds 8 Table 9. Running Time of the Generation Algorithm  Relationship Type Running Time # Relationships Synonym 2 seconds 2 IsAWord 321 seconds 4 Semantic 315 seconds 57 SoundsLike 273 seconds 8 Table 10. Running Time of the Generation Algorithm with Threshold = 50 Possible Words 5.3 Evaluating the Generated Puns Common words, such as man, farmer, cow, gar-bage, and computer, were fed to T-PEG so that the chances of these keywords being covered by the resources (specifically ConceptNet) are higher. An exception to this is the use of keywords with pos-sible homonyms (i.e., whole and hole) to increase the possibility of generating puns with SoundsLike relationships. As previously stated, the linguistic resources provided the generation algorithm with various words that generated a large set of puns. The pro-ponents manually went through this set, identifying which of the output seemed humorous, resulting in the subjective selection of eight puns that were then forwarded for user feedback. User feedback was gathered from 40 people to compare if the puns of T-PEG are as funny as their source human puns. 15 puns (7 pairs of human-T-PEG puns, with the last pair containing 1 human and 2 T-PEG puns) were rated from a scale of 0 to 5, with 5 being the funniest. This rating system was based on the joke judging process used in (Binsted, 1996), where 0 means it is not a joke, 1 is a pathetic joke, 2 is a ?not-so-bad? joke, 3 means average, 4 is quite funny, and 5 is really funny. T-PEG puns received an average score of 2.13 while the corresponding source puns received an 
average score of 2.70. Table 11 shows the scores of four pairs of punning riddles that were evalu-ated, with the input keyword used in generating the T-PEG puns enclosed in parentheses. Pun evalua-tion is very subjective and depends on the prior knowledge of the reader. Most of the users in-volved in the survey, for example, did not under-stand the relationship between elephant and memory1, accounting for its low feedback score.  Training Pun T-Peg Generated Pun What keys are furry? Mon-keys.  (Webb, 1978) (2.93) 
What verses are endless? Uni-verses. (Keyword: verses) (2.73) What part of a fish weighs the most? The scales. (Webb, 1978) (3.00) 
What part of a man lengthens the most? The shadow. (Keyword: man) (2.43) What do you call a lizard on the wall? A rep-tile. (Binsted, 1996) (2.33) 
What do you call a fire on the floor? A fire-wood. (Keyword: fire) (1.90) How is a car like an ele-phant? They both have trunks. (Binsted, 1996) (2.50) 
How is a person like an elephant? They both have memory. (Keyword: elephant) (1.50) Table 11. Sample Puns and User Feedback Scores  Although the generated puns of T-PEG did not receive scores that are as high as the puns in the training corpus, with an average difference rating of 0.57, this work is able to show that the available linguistic resources can be used to train computers to extract word relationships in human puns and to use these learned templates to automatically gener-ate their own puns. 6 Conclusions Puns have syntactic structures and semantic pat-terns that can be analyzed and represented in com-putational models. T-PEG has shown that these computational models or templates can be auto-matically extracted from training examples of hu-man puns with the use of available linguistic resources. The word relationships extracted are                                                            1 Elephant characters in children?s stories are usually por-trayed to have good memories, with the common phrase ?An elephant never forgets.? 
30
synonyms, is-a-word, sounds-like, and semantic relationships. User feedback further showed that the resulting puns are of a standard comparable to their source puns. A template is learned for each new joke fed to the T-PEG system. However, the quantity of the learned templates does not necessarily improve the quality of the generated puns. Future work for T-PEG involves exploring template refinement or merging, where a newly learned template may up-date previously learned templates to improve their quality.  T-PEG is also heavily reliant on the presence of word relationships from linguistic resources. This limitation can be addressed by adding some form of manual intervention to address the missing word relationships caused by limitations of the external resources, thereby increasing the number of usable templates. A different tagger that returns multiple tags may also be explored to consider all possible tags in both the learning and the generation phases. The manual process employed by the propo-nents in identifying which of the generated puns are indeed humorous is very time-consuming and subjective. Automatic humor recognition, similar to the works of Mihalcea and Pulman (2007), may be considered for future work. The template-learning algorithm of T-PEG can be applied in other NLP systems where the extrac-tion of word relationships can be explored further as a means of teaching vocabulary and related con-cepts to young readers. References Kim Binsted. 1996. Machine Humour: An Implemented Model of Puns. PhD Thesis, University of Edinburgh, Scotland. Kim Binsted, Anton Nijholt, Oliviero Stock, and Carlo Strapparava. 2006. Computational Humor. IEEE In-telligent Systems, 21(2):59-69. Kim Binsted and Graeme Ritchie. 1997. Computational Rules for Punning Riddles. HUMOR, the Interna-tional Journal of Humor Research, 10(1):25-76. Kim Binsted, Helen Pain, and Graeme Ritchie. 1997. Children's Evaluation of Computer-Generated Puns. Pragmatics and Cognition, 5(2):309-358.  Iyas Cicekli, and H. Atay G?venir. 2003. Learning Translation Templates from Bilingual Translation Examples. Recent Advances in Example-Based Ma-chine Translation, pp. 255-286, Kluwer Publishers. Susan Fitt 2002, Unisyn Lexicon Release. Available: http://www.cstr.ed.ac.uk/projects/unisyn/. 
Kathleen Go, Manimin Morga, Vince Andrew Nunez, Francis Veto, and Ethel Ong. 2007. Extracting and Using Translation Templates in an Example-Based Machine Translation System. Journal of Research in Science, Computing, and Engineering, 4(3):17-29.  Bryan Anthony Hong and Ethel Ong. 2008. Generating Punning Riddles from Examples. Proceedings of the Second International Symposium on Universal Com-munication, 347-352, Osaka, Japan. Hugo Liu, and Push Singh, 2004. ConceptNet ? A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal, 22(4):211-226, Springer Neth-erlands. Ruli Manurung, Graeme Ritchie, Helen Pain, and An-nalu Waller. 2008. Adding Phonetic Similarity Data to a Lexical Database. Applied Artificial Intelligence, Kluwer Academic Publishers, Netherlands. Rada Mihalcea and Stephen Pulman. 2007. Characteriz-ing Humour: An Exploration of Features in Humor-ous Texts. Computational Linguistics and Intelligent Text Processing, Lecture Notes in Computer Science, Vol. 4394, 337-347, Springer Berlin. Ion Muslea. 1999. Extraction Patterns for Information Extraction Tasks: A Survey. Proceedings AAAI-99 Workshop on Machine Learning for Information Ex-traction, American Association for Artificial In-telligence. Daniel Naber. 2003. A Rule-Based Style and Grammar Checker. Vince Andrew Nunez, Bryan Anthony Hong, and Ethel Ong. 2008. Automatically Extracting Templates from Examples for NLP Tasks. Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation, 452-459, Cebu, Philippines. Graeme Ritchie. 2005. Computational Mechanisms for Pun Generation. Proceedings of the 10th European Natural Language Generation Workshop, 125-132. Aberdeen. Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu Waller, and D. O?Mara. 2006. The STANDUP Inter-active Riddle Builder. IEEE Intelligent Systems 21(2):67-69. K. Webb, The Crack-a-Joke Book, Puffin Books, Lon-don, England, 1978. WordNet: A Lexical Database for the English Lan-guage. Princeton University, New Jersey, 2006.  
31
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 63?70,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Planning Author and Character Goals for Story Generation   Candice Jean Solis, Joan Tiffany Siy, Emerald Tabirao, and Ethel Ong College of Computer Studies  De La Salle University Manila, Philippines candice_solis13@yahoo.com, emeraldtabirao_dlsu@yahoo.com.ph, joan.tiffany.siy@gmail.com, ethel.ong@delasalle.ph 
 
  Abstract The design and content of the planning library of a story generation system dictates the con-tent quality of the story it produces. This paper presents the story planner component of Pic-ture Books, a system that generates stories for children aged 4 to 6 years based on a set of picture elements selected by the user. The planning library separates the design for the story patterns from the design of the semantic ontology that supplies the story?s domain knowledge. An evaluation of the system shows that the coherency and completeness of the plot is attributed to the story pattern design structure while the appropriateness of the con-tent is attributed to the semantic ontology. 1 Introduction Several researchers have developed story genera-tors capable of generating stories that closely resemble human-made stories. Some of these include TALE-SPIN (Meehan, 1977) that gener-ates stories through problem solving, MIN-STREL (Turner, 1992) that uses an episodic memory scheme for storing past problem-solving cases, and MAKEBELIEVE (Liu and Singh, 2002) that constructs stories with the use of logi-cal reasoning. Callaway and Lester (2002) observed that most story generators (SG), such as TALE-SPIN and MAKEBELIEVE, concentrated on the gen-eration of plots and the characters, with less em-phasis on linguistic phenomena, resulting in sto-ries that have good narrative quality but lacking in linguistic structures. Their STORYBOOK sys-tem addresses this by applying full-scale linguis-tic approaches to the narrative prose generation architecture of AUTHOR (Callaway, 2000). Loenneker (2005) made a similar observation regarding SG whose implicit goal is to generate a coherent narrative in a given genre with less em-phasis on discourse structure, and skipping 
document structuring and microplanning (Reiter and Dale, 2000) stages. In this paper, we present our story generator, Picture Books, which generates stories for chil-dren aged four to six. Picture Books derives the story elements from a given input set of picture elements (backgrounds, characters, and objects). The genre (fables) and story goal (moral lessons) are applied as separate parts of the system?s planning library, which contains story planning goals to convert abstract story specifications to coherent stories suitable for the target age group. The rest of this paper is subdivided as fol-lows. Section 2 presents some background in-formation on the specifications of a children?s story as well as the semantic ontology used by the planners of Picture Books. Section 3 dis-cusses the design of the plan library and the planning process involved in story generation, followed by the evaluation results on the content and grammar of the stories generated by the sys-tem in Section 4. The paper ends with a summary of future work that can be done to improve the generated stories.  2 Storytelling and Picture Books The motivation behind Picture Books is two-fold. First is the realization that stories are com-binations of various genres to produce a differing effect.  The genres can be considered as tem-plates that dictate the plot of the story.  It is therefore possible to create a story by indicating the story elements (i.e., characters, events, and settings), the genres, and the goal of the story to automatically produce a narrative text. The sec-ond motivation is that stories serve as rich sources of information that help develop a child?s knowledge.  However, young children recognize images of objects easily compared to words (Fields et al 2003), accounting for the popularity of picture-based story books that al-low readers to relate stories by using not only words, but pictures depicting the story. 
63
2.1 Story Specifications Interviews with child educators revealed that most of the published storybooks for children focus on themes to teach them lessons about proper behavior. These themes revolve around everyday activities like eating on time and brush-ing your teeth with lessons on being careful, be-ing honest, and the value of sharing. Themes also dictate the objects that can be used in the story, for example, in the eating healthy foods theme, possible positive objects supporting good behav-ior are apples and bananas, while cakes and can-dies are considered negative objects in cases of misbehavior (i.e., unhealthy foods). Another common characteristic of children's stories is the use of the fable form, wherein the story characters are portrayed by animals that can capture the imagination and attention of the target readers. The animals have simple traits that children can relate with, such as loyalty for dogs, playfulness for cats, kindness for rabbits, and bravery for tigers. They are also given names, such as Ellen the elephant, Rizzy the rab-bit, and Leo the lion, to give the impression that the characters are friends that the children are getting to know better through reading the story. In the linguistic aspect, stories for four year olds have simple sentence structures and contain line redundancy. This is lessened as the child grows older. The words used in the stories are not only simple and easy to understand, but also vary depending on the child's age in order to in-troduce new words to his vocabulary. Lessons and rules are emphasized by positive praises while improper behaviors are emphasized by re-vealing their consequences. Another aspect to consider in storytelling is the story?s title. Story titles are short and often 
contain the story?s theme as a hint to what the story is about. The main character?s name should also be included in the title, such as ?Leo the Lion Learns to Eat on Time?. In general, Machado (2003) showed that sto-ries follow a common and classic story pattern depicted in Figure 0, which flows from negative to positive and has the following outline: i. The main character wants something. ii. The main character is informed of the rules and/or restrictions. iii. The main character disobeys the rule. iv. The main character is either caught or experiences natural consequences of dis-obedience (e.g. a tooth ache from eating too much candy). v. The main character learns a lesson. 2.2 Semantic Ontology The knowledge resource dictates the amount of information that an SG system can output, thus highlighting its importance. Picture Books uses an ontology to have a flexible knowledge re-source that provides relevant concepts familiar to the target age group as well as applicable to the story being planned. Its design was adapted from ConceptNet (Liu and Singh, 2004a), a semantic resource with structure closely resembling that of WordNet (2006). The nodes used by ConceptNet are of three general classes representing noun phrases, attrib-utes, and activity phrases. A semantic relation-ship connects two concepts while a semantic category classifies them. The semantic relation-ships are binary relation types defined by Open Mind Commonsense project (Liu and Singh, 2004b). Table 1 lists some of these relationships defined in Picture Books following the form <re-lationship>(<concept1>, <concept2>). 
Quick, satisfying conclusion 
Introduction of setting and characters 
Introduction of problem or dilemma 
Rising action and plot development 
Insightful answer or solution 
Climactic scene 
Beginning 
Ending 
Figure 0. Common and Classic Story Pattern Form (Machado, 2003)  
64
Semantic Category Semantic Relationships Things IsA(headache, pain) - corresponds loosely to hypernym in WordNet  PropertyOf(apple, healthy) PartOf(window, pane) ? corresponds loosely to holonym in WordNet MadeOf(toy car, clay) Events FirstSubeventOf(tell bedtime story, sleep) EventForGoalEvent(go to grocery store, buy food) EventForGoalState(clean up, be neat)  EventRequiresObject(play, toy) Actions EffectOf(become dirty, itchy) EffectOfIsState(make friends, friendship) CapableOf(toy car, play) Table 1. ConceptNet semantic relationships (Liu and Singh, 2004b) with sample concepts of Picture Books 3 Planning the Story Picture Books has three major components ? a picture editor, a story planner, and a sentence planner. The Picture Editor is provided for users to specify the background or setting of the story (kitchen), and to select and ?stick? into the back-ground the set of characters (little sheep and mama sheep) and objects (cake, bread). An ex-ample picture is shown in Figure 2. The first child character placed in the picture will be the protagonist of the story, while the first adult sticker placed in the picture will be the par-ent of the protagonist. If there is no adult charac-ter, the protagonist's biological parent will be chosen as the adult character needed in the story. The first object sticker placed in the picture will assume the %object% variable used in planning the story - especially in ontology accesses. The rest are discarded. All elements in the picture, namely the back-ground, the characters and the objects (including sequence of placement), and the name and age of the user, are stored in an input content represen-tation (ICR) and passed on to the story planner. The Story Planner takes in the abstract ICR then performs three planning steps ? i) theme planning to select an appropriate theme for the story, ii) plot planning to instantiate the story plots depicting the theme, and iii) presentation planning to handle the planning of the story?s title, introduction and appropriate ending. A Story Organizer arranges these resulting story events as it is supposed to be presented to the user in an abstract story tree. 
The Sentence Planner then converts the ab-stract story tree representation to sentence speci-fications by performing referring expression generation, lexicalization, and phrase specifica-tion mapping. The Surface Realizer uses the sim-plenlg realiser (Venour and Reiter, 2008) to con-vert the sentence specifications to actual sen-tences that comprise the story.  
 Figure 2. Sample Picture with Stickers 3.1 Plan Library The plan library contains the set of instruc-tions and information on creating the plot of a story, and has three parts: (1) the story elements containing information regarding the characters, objects and settings of a story; (2) a set of story patterns to direct the story goal towards attain-ment of the moral lesson; and (3) a semantic on-tology containing concepts applicable to the tar-get domain, in this case, fables. Picture Books uses a set of predefined characters, objects and backgrounds, as inputs. These story elements are used to determine the actors, the setting and the theme of a story. Information on characters, such as the parents, is useful for identifying other characters in the story. The background serves as the main setting of the story, and combined with the selected objects, is used to determine the theme. Given a bedroom background, the set of available themes include being brave, being neat, being careful, being honest, and sleeping early. Objects that are available for this background include an alarm clock, a lamp, a pillow, and toys. The story pattern is used to direct the goal of the story and is composed of the theme, story plot, author goal and character goal. Each com-ponent is designed to subsume the next (i.e., theme subsumes story plot, and story plot sub-sumes author goal) to support different granular-
65
ity of story details and to lessen the redundancy as common story details are subsumed by a higher-level component. A theme dictates the plot of a story and is composed of four story plots (see Figure 3) namely, the problem, rising action, solution and climax which, according to Machado (2003), are the four fundamental stages of the main plot of any story. In the theme take_bath, these four plots would contain the following:  Problem:          Defy by not doing the rule  Rising Action:  Experience consequence Solution:          Do the lesson Climax:            Learn the benefit 
 Figure 3. Theme Structure  The theme is executed through the story plot. A story plot represents the major events in the theme and contains at least two author goals (shown in Figure  4) that represent the scenes comprising the event. 
 Figure 4. Story Plot Structure  
 Figure 5. Author Goal Structure  An author goal (Figure 5) is composed of a goal of the scene and the corresponding conse-quence of the goal to ensure consistency in the scene. Each goal and consequence component of 
an author goal is in turn filled up by at least one character goal.A character goal (Figure 6) rep-resents the unit of action a character or two char-acters do in order to depict the goal/consequence of the scene. This design of the character goal is based from the action operators of Uijlings (2006) and can be directly converted to simple declarative sentences. A character goal has five fields ? the action, the agens or doer of the action, the patiens or receiver of the action, the target, and the instru-ment. One character goal generates one sentence in the story with the agens as the subject, the ac-tion as the verb, the patiens as the character or object that undergoes the action verb, the target as the location or object of the action verb, and the instrument as the object used to perform the verb. This design allows all fields to be empty except for action and agens.  
 Figure 6. Character Goal Data Structure  The character goal is generic so that it only contains default values for action, agens and pa-tiens. During instantiation of a character goal for a particular scene, attribute values are passed to the character goal at the author goal level. For example, given the generic character goal adult tells main character (CGL01) has the following default attributes: action: tell agens: adult patiens: main character When a scene requires the adult to inform the main character of the lesson, the target attribute would then be assigned with the lesson value to denote that the adult is talking to the main char-acter about the lesson. The invocation of the character goal in the author goal level would be: CGL01(target:lesson) This customizes the character goal to ?adult tells the lesson to the main character? to fit the scene. Parameters for character goal attributes in-clude not only the story variables (i.e. object, lesson, background), but invocations to inner character goals and ontology accesses as well. An inner character goal is a character goal as-signed as an attribute of an outer character goal. 
66
It represents a clause in a sentence and is usually assigned as a value of the target attribute in the outer character goal, for example: CGL03(target:CGL05(target:lesson)) Picture Books would interpret the inner char-acter goal ?main character is not doing the les-son (CGL05)? first before appending it to the outer character goal ?secondary character told the adult character (CGL03)?, resulting in the following sentence specification: secondary character tells an adult that the main character is not doing the lesson The instrument attribute in a character goal may have the following value which would trigger an ontology search for a concept: onto<semantic_category>(%object%) The <semantic category> constrains the search coverage to concepts that are directly connected to the given input concept (%ob-ject%). For example, ontoSpatial(play) triggers a search for all concepts connected to play within the spatial semantic category (e.g., ?locationOf? and ?oftenNear? semantic relationships).  Character goals can also be created dynami-cally during runtime and are used to increase the length and the variation of the generated story. Dynamic character goals necessitate the search for relationship paths in the ontology. Two input concepts, the source and destination concepts, and the semantic category that constrains the search coverage, are needed when searching for relationship paths. Consider the following attribute value of a character goal:  ontoEvent(break object, punishment) This denotes that a search for a series of seman-tic relationships within the Event semantic cate-gory must be performed to relate break object to punishment. The resulting path is shown in Figure  7.  
 Figure 7. Sample Ontology Path Search Result  Each relationship in the resulting path, ex-cluding isA (hypernym) and conceptuallyRelat-edTo relationships, is mapped to a dynamic char-
acter goal. For example, in the first lastSubeven-tOf relationship in Figure 7, the second concept (?get punished?) is assigned to the action attrib-ute of the dynamic character goal, resulting in the abstract output sentence specifications ?Main character gets punished? and ?Main character is not allowed to play today?. 3.2 Planning Process The Theme Planner selects a theme by matching the input objects against the applicable objects of the candidate themes for a specified background. When the theme with the highest score has been determined, the Plot Planner instantiates the story plots of the chosen theme, beginning with the problem, followed by the rising action, then the solution and finally the climax. Given the theme eat_on_time with the following corre-sponding story plots: eat_on_time(defy_break_rule, experience_ consequence, inform_lesson, realize _benefit) 1 the first (problem) story plot, defy_break_rule, suggests that the main character will disobey an adult character?s rule/instruction. Executing this plot entails executing the author goals within it. defy_break_rule(inform_character_rule,         ignore _rule) Then the first author goal, inform_character _rule, suggesting that the adult character informs the main character of the rule first, is executed: inform_character_rule(adult_talk_character, character_not_want) Finally, the character goals in the author goal, adult_talk_character and character_not_want, are executed. Notice that it is only in the character goal level that instantiation of characters and objects are made. adult_talk_character(?tell?, ?Mommy Audrey?, ?Simon?, ?eat on time?, null) 2 character_not_want(?not want?, ?Simon?, null, ?eat on time?, null) These phrase specifications of character goals will be forwarded later to the surface realizer for translation to simple sentences to generate the text ?Mommy Audrey told Simon to eat on time.? and ?Simon did not want to eat on time.?, respectively.                                                  1 Theme format:  Theme(problem, rising action, solution, climax) 2 Character Goal format:  CG(action, agens, patiens, target, instrument) 
67
The planning process continues by backtracking to the first story plot and iterating until it exhausts all the theme?s story plots. A depth-first tree traversal algorithm is employed to recursively traverse the theme tree. An excerpt from the theme eat_on_time, starting from the story plot defy_break_rule, and its tree traversal, is shown in Figure  8. 
 Figure 8. Tree Traversal for the Problem ?Defy Break Rule? of the ?Eat_On_Time? Theme 3.3 Planning for Linguistic Variation Sentences generated by Picture Books vary in length and word complexity according to the user?s age. The variation in length is handled by specificity character goals (SCG) that are ap-pended as supporting details to their respective character goals. SCGs are designed so that their existence will give more detail to the preceding character goal while ensuring consistency, and that their non-existence will still make the story complete. SCG is not executed when the user is four years old, one SCG is executed when the user is five years old, and all of SCGs are exe-cuted when the user is six years old.  Age Story Excerpt Four Ellen was scared. She was sad. Teacher Sara saw that Ellen was crying. Teacher Sara told Ellen to be brave. Five Ellen was scared. She was lonely. Teacher Sara saw that Ellen was crying. Teacher Sara asked Ellen if everything was okay. She told her to be brave. Six Ellen was scared. She was upset.  Teacher Sara saw that she was crying. Teacher Sara asked Ellen if everything was okay. Ellen told teacher Sara that she was scared.  Teacher Sara told Ellen to be brave. Table 2. Excerpts of Stories with Linguistic Variation   Word complexity is addressed by storing dif-ferent synonymous lexical items in the lexicon. The appropriateness of the lexical items to the 
target age group was verified by child educators. Table 2 contains excerpts from Picture Books? stories for various ages, with boldfaced italics showing lexical variation, while underlined sen-tences are the executed SCGs. Additional sen-tences and lexical items can be generated by en-coding more SCGs into the plan library and the ontology, respectively. 4 Results and Analysis The knowledge base of Picture Books currently contains 9 backgrounds, 11 themes, 40 charac-ters, 37 objects, 77 author goals, and 61 character goals. The lexicon has been populated with 419 words appropriate for the target age group, while the ontology has 240 concepts and 369 semantic relationships. 15 exemplary stories generated by the system, consisting of five themes that vary per age (4, 5, and 6), were selected. Two child educators and a linguist manually validated the appropriateness of the content and the linguistic correctness of the 15 stories, respectively. Each story was rated per criterion from 1 to 4, with 4 being the highest. A rate of 4 means that the criterion is completely present in the story, 3 means that the criterion is present but incom-plete, 2 means the criterion is partially present, and 1 means the criterion is not present. 4.1 Evaluating the Story Content The evaluation on the story patterns? role in en-suring that the story goal is met is shown in Ta-ble 3, while the evaluation of the semantic ontol-ogy?s role in supplying domain knowledge to the story is shown in Table 4. The evaluators gave the stories high ratings in terms of plot completeness, validating that the stories have all the essential story elements (problem, rising action, the solution and the climax to the problem) as well as the introduc-tion of the time and setting of the story. This was attributed to the plan library structure consisting of themes, plots, author goals, and character goals. Because of this coherency in the plot, the understandability criterion received a high average score of 3.47. Criteria Average Story is understandable 3.47 The settings of the story were de-scribed 3.86 The characters in the story were described 3.67 Objects are present in the story 4.00 General Average 3.75 Table 3. Evaluation on the Story Patterns 
68
Criteria Average Sentences are coherent 2.67 The story has transition 3.47 The actions of the characters make sense 3.93 Story is appropriate to target age 3.80 Objects in the story were described 2.80 General Average 3.33 Table 4. Evaluation of the Semantic Ontology  Evaluations on the effectiveness of the semantic ontology, on the other hand, produced varying scores. The coherency of the sentences received a low average score of 2.67, because as seen in the excerpt below, a sentence (such as the underlined text) depicting that Ellen the elephant did something to show she is trying to be brave was missing in the generated story. She wanted to be brave. Ellen was brave. She wanted to play with others. She bravely intro-duced herself. Ellen made friends. Since the ontology did not contain any other information of what to do in order to be brave, the story content planner did not place any detail describing the action of the main character depicting her attempt to be brave. This can be remedied by adding more semantic relationships (that can be converted to sentences) between concepts. The excerpt from the generated story ?Rizzy the Rabbit Learns to be Honest? below also affected the coherency criterion when the second character, Denise the dog, suddenly appeared in the middle of the story when she could have been introduced with the first character, Rizzy the rabbit, at the start of the story. The evening was warm. Rizzy the rabbit was in the dining room. She played near a lamp. Rizzy broke the lamp. She was scared. Mommy Francine saw that the lamp was broken. Rizzy told Mommy Francine that Denise broke the lamp. The criterion regarding story transition pertains to the ease of transition of events that can be attributed to the path of relationships retrieved from the semantic ontology, which more often than not, introduce event transitions including character actions.  The criterion on appropriateness of the story content to the target age received a high average score of 3.80 mainly because the semantic ontology has been populated with concepts specifically for the target age group. 
While the presence of objects in the stories received a high score of 4.00 (see Table 3), the description of objects received a low score of 2.80 (see Table 4), because although objects are present in the stories, as in ?She played near a lamp.?, they were not described, i.e., ?breakable lamp?. This was remedied by adding object descriptions in the ontology. However, the selection of which description to be used was currently not dictated by the story theme, nor was the description subsequently used to direct the sequences of events in the rest of the story. 4.2 Evaluating the Linguistic Aspect Table 5 summarizes the evaluation on the gram-matical aspect of the generated stories. Since mostly simple sentences are generated, they are grammatically correct and coherent.   CRITERIA AVG Sentences are grammatically correct 3.20 Sentences are coherent 3.60 Pronouns are used correctly 3.45 Articles are used correctly 3.20 The story has transition 3.00 General Average 3.29 Table 5. Evaluation on Grammar Correctness  Most of the generated stories contain correct usage of pronouns, except for sentences such as ?Teacher Sara told Ellen that Ellen should be brave?, where the second reference to ?Ellen? should be replaced with the pronoun ?she?. The evaluators, however, noted the lack of possessive pronouns in the text below (the underlined words are the identified missing pronouns). Porky wanted to play. He played with his toys. His toys were scattered. There are also occurrences of missing articles, as shown in the excerpt below (the underlined words are the missing articles). He played near a glass of water.  Simon broke the glass of water. He was scared. Daddy Gary saw that the glass of water was broken. The generated stories received an average score of 3.0 for the transition criterion due to missing transitional devices, as shown in the ex-cerpt below (the underlined words are the miss-ing transitional devices). He apologized to her. Mommy Patricia then helped Porky to clean up. Soon he found the lost toys. 
69
5 Conclusion and Recommendations  The design of Picture Books? plan library, com-bined with planning operators in the form of story plots and character goals, demonstrated that grammatically correct and coherent stories for children aged 4 to 6 can be generated from a given set of predefined pictures, provided that the appropriate domain knowledge is present. The semantic ontology whose design was adapted from ConceptNet provided the domain knowledge that supplies factual information to the story.  The theme structure and its compo-nents model the way humans perform storytel-ling and also ensured that the generated stories will contain the four basic elements of problem, rising action, solution, and climax. The separa-tion of the plan library which dictates the story patterns from the semantic ontology gives flexi-bility to Picture Books such that it could easily be adapted to generate other story domain. Currently, Picture Books generates stories with simple declarative sentences. A future im-provement of the system is to extend the design of the character goal to generate stories with dia-logues. Although common animals that young children can relate with have been used as the main characters of the story, their traits, such as loyalty, bravery, and kindness, have not been considered in determining the flow of the story. A similar claim can be made for describing the objects used in the story, such that a story pro-moting the value of ?being thrifty? can use the ?expensive lamp? as an object, while the value of ?being careful? can use the ?breakable lamp? instead. These are aspects of storytelling that can be investigated in a future work. From a linguistic perspective, the current im-plementation of Picture Books can generate pro-nouns and articles, as well as a transitional de-vice at the last sentence in the story. However, possessive pronouns, the consistent use of arti-cles, and the generation of appropriate transi-tional devices in several parts of the story should be further explored. The design of the character goals can be extended by adding an attribute in-dicating a connection or the passage of time be-tween sentences. Rhetorical Structure Theory (Mann and Thompson, 1988) can also be applied to author goals and character goals for an effec-tive discourse structure that would provide a smoother story flow.   
References  Charles B. Callaway, and James C. Lester. 2002. Nar-rative Prose Generation. Artificial Intelligence, 139(2): 213-252. Charles B. Callaway. 2000. Narrative Prose Genera-tion. PhD thesis, North Carolina State University, Raleigh, NC. Robert Dale, and Ehud Reiter. 2000. Building Natural Language Generation Systems. Cambridge: Cam-bridge University Press. Marjorie Fields, Lois Groth, and Katherine Spangler. 2003. Let's Begin Reading Right: A Developmental Approach to Emergent Literacy, 5/E. Prentice Hall. Hugo Liu, and Push Singh. 2004a. Commonsense Reasoning in and over Natural Language. In Pro-ceedings of the 8th International Conference on Knowledge-Based Intelligent Information and En-gineering Systems, 293-306, Wellington, New Zea-land. Springer Berlin. Hugo Liu, and Push Singh, 2004b. ConceptNet ? A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal, 22(4): 211-226. Springer Netherlands. Hugo Liu, and Push Singh. 2002. Makebelieve: Using Commonsense Knowledge to Generate Stories. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 957-958, Edmonton, Al-berta, Canada. AAAI Press. Birte Loenneker. 2005. Narratological Knowledge for Natural Language Generation. In Proceedings of the Tenth European Workshop on Natural Lan-guage Generation, 91-100, Aberdeen, Scotland. Jeanne Machado. 2003. Storytelling. In Early Childhood Experiences in Language Arts: Emerging Literacy, 304-319. Clifton Park, N.Y. Thomson/Delmar Learning. William C. Mann, Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3), 243-281. James Meehan. 1977. TALE-SPIN, An Interactive Program that Writes Stories. Proceedings of the 5th International Joint Conference on Artificial Intelligence, 91-98. Cambridge, M.A. Scott R. Turner. 1992. Minstrel: A Computer Model of Creativity and Storytelling. Los Angeles, Cali-fornia: University of California. Jasper Uijlings. 2006. Designing a Virtual Environment for Story Generation. MSc Thesis, University of Amsterdam, Amsterdam. Chris Venour, Ehud Reiter. 2008. A Tutorial for Simplenlg (version 3.7) http://www.csd.abdn.ac.uk/~ereiter/simplenlg/ WordNet: A Lexical Database for the English Lan-guage. Princeton University, New Jersey, 2006. 
70
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 40?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Representing Story Plans in SUMO 
 
 
Jeffrey Cua   
Center for Human Language Technologies 
De La Salle University, Manila, Philippines 
  cuajeffreyleonardcompro1@yahoo.com 
Ethel Ong  
College of Computer Studies 
De La Salle University, Manila, Philippines 
ethel.ong@delasalle.ph 
 
Ruli Manurung 
Faculty of Computer Science 
University of Indonesia, Jakarta, Indonesia 
maruli@cs.ui.ac.id 
 
 
Adam Pease 
Articulate Software 
Angwin, California, USA 
apease@articulatesoftware.com 
 
 
Abstract 
Automatic story generation systems require a 
body of commonsense knowledge about the 
basic relationships between concepts we find 
everyday in our world in order to produce in-
teresting narratives that describe human ac-
tions and world events. This paper presents an 
ongoing work that investigates the use of 
Suggested Upper Merged Ontology (SUMO) 
to represent storytelling knowledge and its in-
ference engine Sigma to query actions and 
events that may take place in the story to be 
generated. The resulting story plan (fabula) is 
also represented in SUMO, allowing for a sin-
gle story representation to be realized in vari-
ous human languages. 
1 Introduction 
People combine words and events from their 
knowledge source of words, their meanings and 
their relationships in order to tell stories about their 
lives, their communities, and their daily expe-
riences. In order for computers to achieve the same 
level of expressiveness to provide a more fluent 
man-machine interaction, they must be provided 
with the same collection of knowledge about the 
basic relationships between things and events. 
Picture Books (Solis et al 2009), an automatic 
story generator that generates story text for child-
ren from a given input set of picture elements 
(backgrounds, characters and objects), utilized a 
semantic ontology whose design has been adapted 
from ConceptNet (Liu and Singh, 2004). The 
background serves as the setting of the story and is 
also used to determine the theme. Semantic con-
cepts needed by the story planner, specifically ob-
jects, story events, and character actions are 
classified according to the semantic categories of 
ConceptNet, namely things, spatial, events, ac-
tions, and functions. This mapping approach con-
strained the flexibility of the system, as new 
themes would entail repopulating the sequences of 
possible events manually into the knowledge base. 
Events and actions are selected according to their 
associated themes, and not marked with precondi-
tions that specify constraints under which certain 
actions can be performed and the corresponding 
consequential events that may arise. 
Swartjes (2006) developed a story world ontolo-
gy containing two layers, the upper story world 
ontology and the domain-specific world ontology. 
The upper story world ontology is independent of 
any story structures or story domains and models a 
vast amount of possible actions and events. It is 
also limited to high-level concepts that are meta, 
generic or abstract to address a broad range of do-
main areas. A domain-specific story world ontolo-
gy, on the other hand, applies the upper story 
world ontology to a certain story domain. 
Kooijman (2004) suggests the use of the Sug-
gested Upper Merged Ontology (SUMO) as an 
upper ontology to capture the semantics of world 
knowledge. SUMO (Niles and Pease, 2001) is an 
40
open source formal and public ontology. It is a col-
lection of well-defined and well-documented con-
cepts, interconnected into a logical theory. It 
numbers some 20,000 terms and 70,000 axioms. 
Axioms are in first-order logic form (with some 
higher order extensions) and reflect commonsense 
notions that are generally recognized among the 
concepts. They place a constraint on the interpreta-
tion of concepts and provide guidelines for auto-
mated reasoning systems such as Sigma (Pease, 
2003). Formal terms in SUMO are mapped to syn-
sets in WordNet (Pease, 2006). 
There are other noteworthy ontologies that can 
be considered. Like SUMO, Cyc (Lenat, 1995) is a 
large-scale, language-independent and extensible 
knowledge base and commonsense reasoning en-
gine, but it is proprietary and its open-source ver-
sion, OpenCyc1, has no inference rules. DOLCE 
(Gangemi, 2003) is a small-scale descriptive on-
tology with a cognitive orientation. BFO (Smith, 
1998) is another small-scale upper ontology sup-
porting domain ontologies developed for scientific 
research domain, such as biomedicine. Thus, no 
ontology other than SUMO had the characteristics 
of being comprehensive enough to include forma-
lizations that represent detailed elements of every-
day life (e.g., furniture, breaking an object, 
emotion), being open-source, having expressive-
ness of at least first order predicate calculus so that 
arbitrary rules about actions and consequences can 
be represented, having an associated open-source 
first-order inference engine, and a language gener-
ation capability so that stories can be automatically 
presented in multiple human languages 
This paper presents SUMOs (SUMO Stories), 
an automatic story generator that uses first-order 
logic to declaratively describe models of the world, 
specifically those aspects of the world that 
represent storytelling knowledge for children?s 
stories of the fable form. The story planner then 
utilizes an open source browsing and inference 
engine Sigma to infer this knowledge to generate a 
story plan (fabula) also in first-order logic form.  
Using first-order logic enables a less restricted 
semantics compared to description logic, which is 
commonly used for knowledge representation of 
large ontologies. Though having lesser constraints 
will have an impact on the speed of inference, it is 
overcome by the advantage of having greater re-
                                                          
1
 OpenCyc web site, http://www.opencyc.org/ 
presentational capability. In particular, the axi-
omatic nature of actions and their consequences, so 
essential for reasoning about narrative structures, is 
not supported by description logics, which focus 
on category and instance membership reasoning. 
Section 2 provides a background on the know-
ledge required by story generation and how these 
were represented in Picture Books, which is used 
as the basis for the storytelling knowledge. Section 
3 discusses the representation of the storytelling 
knowledge to SUMO. The SUMOs architecture 
depicting the interaction between the story planner 
and Sigma to derive the story plan is then pre-
sented in Section 4. The paper concludes with a 
summary of what we have accomplished so far, 
and presents further work that can be done. 
2 Storytelling Knowledge  
Theune and her colleagues (2006) presented five 
levels of the different aspects of a story that must 
be represented in the semantic network. These are 
the story world knowledge, character representa-
tions, a causal and temporal network to represent 
plot structures, representational model of narrato-
logical concepts, and the representation of the sto-
ry?s potential effects on the user. Only the first four 
levels are included in this study. 
According to Swartjes (2006), a story is com-
posed of a story world where the story takes place, 
the characters that interact in the story world, and 
the associated objects. Consider the story generat-
ed by Picture Books in Table 1 about Rizzy the 
rabbit who learns to be honest (Hong et al 2008). 
The afternoon was windy. Rizzy the rabbit was in the 
dining room. She played near a lamp. Rizzy broke the 
lamp. She was scared. Mommy Francine saw that the 
lamp was broken. Rizzy told Mommy Francine that Da-
niel broke the lamp. Daniel the dog told her that he did 
not break the lamp. Daniel was upset. He got punished. 
Mommy Francine told Daniel that he was grounded. He 
cried. Rizzy felt guilty. She told Mommy Francine that 
she broke the lamp. Mommy Francine told Rizzy that 
she should have been honest. Rizzy apologized to 
Mommy Francine. Mommy Francine forgave Rizzy. 
Rizzy apologized to Daniel. He forgave her. Mommy 
Francine told Rizzy to be honest. She told her that being 
honest is good. From that day onwards, Rizzy always 
was honest. 
Table 1. Sample story generated by Picture Books 
(Hong et al 2008) 
41
The story elements in Table 1 were determined 
from the background (i.e., dining room), the cha-
racters (i.e., Rizzy and her mommy Francine) and 
object (i.e., lamp) that the child user places into 
his/her picture using the Picture Editor of the sys-
tem in Figure 1. 
The background serves as the main setting of the 
story, and combined with the selected objects, is 
used to determine the theme. Consider the bed-
room setting. If the associated object is a lamp, 
then the theme is about bravery (i.e., do not be 
afraid of the dark). If the object is a set of toy 
blocks, the theme can be about being neat. In Pic-
ture Books, such associations are manually deter-
mined and entered into the database. In SUMOs, 
these associations should be inferred automatically 
through axioms that should be commonsense, and 
not be explicit encoding of narrative knowledge.  
 
Figure 2. Picture Editor (Hong et al 2008) 
 
Stories generated by Picture Books follow a ba-
sic plot dictated by Machado (2003) that flows 
from negative to positive and comprises four sub-
plots, namely the problem, rising action, solution 
and climax. The theme is subdivided into these 
four subplots, each representing a major event in 
the story. 
Each subplot contains at least two author goals 
representing the goal of the scene and the corres-
ponding consequence of the goal. An author goal is 
translated into one or more character goals, each 
representing an action performed by the character 
(main, secondary, or adult character) in order to 
achieve the author goal. A character goal translates 
directly to one declarative sentence in the generat-
ed story. Table 2 shows the author goals and the 
character goals for some of the sentences in the 
story in Table 1. 
The design of the character goal is based from 
the action operators of Uijlings (2006) which is 
easily transformed to a declarative sentence in ac-
tive voice using the surface realizer simpleNLG 
(Venour and Reiter, 2008). In the case of Picture 
Books, however, the approach resulted in a story 
where every sentence describes an action or a feel-
ing (i.e., scared, guilty, upset) that is performed by 
the character, as seen in Table 1. 
Subplot #1 
Author goal 1.1: 
Goal of the scene Child is doing an activity 
Character goal <character> plays <object> 
Resulting text Rizzy the rabbit played near a lamp. 
Author goal 1.2: 
Goal consequence Child caused a problem 
Character goal <character> destroys <object> 
Resulting text Rizzy broke the lamp. 
Subplot #2 
Author goal 2.1: 
Goal of the scene Child lied 
Character goal 
<main character> told <adult 
character> that <secondary 
character> <did the action> 
Resulting text Rizzy told Mommy Francine that Daniel the dog broke the lamp. 
Author goal 2.2: 
Goal consequence Another child gets punished 
Character goal #1 <secondary character> receives 
<punishment> 
Resulting text #1 Daniel the dog got punished. 
Character goal #2 
<adult character> issues <pu-
nishment> to <secondary cha-
racter> 
Resulting text #2 Mommy Francine told Daniel 
that he was grounded. 
Table 2. Sample author goals and character goals asso-
ciated with the theme Being Honest (Hong et al 2008) 
 
The story planner of Picture Books utilizes two 
types of knowledge, the operational knowledge 
and the domain knowledge. The operational know-
ledge contains a static description of the different 
backgrounds and their associated themes and ob-
jects, the child characters and their corresponding 
parent characters, as well as the occupation of the 
42
parents. For each theme, the set of character goals 
needed to instantiate the major events in the theme 
are also specified.  
The domain knowledge, on the other hand, con-
tains a semantic description of objects and events 
that can occur, as well as actions that can be per-
formed. For example, breaking an object results to 
getting punished, and grounded is a form of pu-
nishment. 
Character goals are instantiated by accessing the 
semantic ontology to search for concepts that are 
directly related to the input concept. There are two 
search methods. The first method searches for 
another concept that has a relationship with the 
given concept while satisfying the semantic cate-
gory. For example, ontoSpatial(?play?) triggers a 
search for all concepts connected to play within the 
spatial semantic category, such as the semantic 
relationship locationOf(?play?, ?park?). The second 
method searches for a path that semantically re-
lates the two given concepts. For example, ontoAc-
tion(?vase?, ?method of destruction?) triggers a 
search for a path to relate how a vase can be de-
stroyed, and yields the following relationships: 
CapableOf(?break?, ?vase?) 
Isa(?method of destruction?, ?break?)  
3 Representing Storytelling Knowledge in 
SUMO 
A crucial part of the work involved in the devel-
opment of SUMOs is the representation of the sto-
rytelling knowledge and the evolving story plan in 
SUMO and the use of the Sigma reasoning engine 
to infer story facts and events. 
The storytelling knowledge represented in 
SUMO includes the semantic description about 
concepts, objects and their relationships. From a 
given input set of story elements comprising the 
selected background, characters, and objects, a 
query is sent to Sigma to determine a possible 
starting action that can be performed by the main 
character in the story. The story then progresses 
based on the relationships of character actions and 
reactions, which are the stored facts in SUMO. 
Similar to Picture Books, the resulting story plan 
is created based on a pre-authored plot of problem, 
rising action, resolution and climax. But instead of 
attaching the next set of actions and emotions of 
characters to author goals, in SUMOs, the set of 
actions that a character can do ? reaction to events 
and objects, experience emotions such as joy and 
sadness, and subsequent actions based on their 
emotions ? are represented in SUMO logic. 
The storytelling knowledge was formulated us-
ing a set of predicates that can be classified into 
four main types. Factual predicates specify proper-
ties of characters, objects, and locations. Semantic 
predicates define the semantic relationships be-
tween concepts. Actions and events predicates de-
fine the causal relationships between actions and 
events. Thematic predicates represent a new set of 
predicates to relate story themes to actions. 
3.1 Conceptualizing Story Characters, Ob-
jects, and Backgrounds 
Factual predicates represent the characters, their 
roles, the locations, and the objects that may com-
prise a story. The class and subclass axioms of 
SUMO2 are used to define the set of characters, 
objects and locations.  
Children?s stories of the fable form are por-
trayed by animals that can capture the  imagina-
tion and attention of the readers. Animal characters 
are given names, such as Ellen the elephant, Rizzy 
the rabbit, and Leo the lion, to give the impression 
that the characters are friends that the children are 
getting to know better through reading the story 
(Solis et al 2009). Representing this in SUMO 
entails the use of the subclass axiom to represent 
class inheritance as shown below: 
(subclass RabbitCharacter StoryCharacter) 
Class definitions include slots that describe the 
attributes of instances of the class and their rela-
tions to other instances (Noy, 2001). A character in 
SUMOs has the attributes type (whether adult or 
child), gender, and name. An example axiom to 
represent a female child RabbitCharacter whose 
name will be ?Rizzy? is shown below. Similar 
axioms are defined for all the other characters. 
(=> 
  (and 
    (instance ?RABBIT RabbitCharacter) 
    (attribute ?RABBIT Female) 
    (attribute ?RABBIT Child)) 
  (name ?RABBIT "Rizzy")) 
Backgrounds and objects are also defined using 
the subclass axiom and inherit from existing 
classes in SUMO, for example, 
                                                          
2
 SUMO Ontology Portal, http://www.ontologyportal.org/ 
43
(subclass LivingRoom Room) 
(subclass Lamp LightFixture) 
(subclass Lamp ElectricDevice) 
(attribute Lamp Fragile) 
Further definitions can be provided for living 
room to differentiate it from other rooms, such as 
being disjoint from bathroom, and has a primary 
purpose of supporting social interaction, as shown 
below. Similarly, the definition for lamp can also 
be extended to distinguish it from other electric 
light fixtures, e.g., a lamp is moveable unlike a 
chandelier, but is plugged in when operating unlike 
a flashlight. 
(=> 
 (instance ?R LivingRoom) 
     (hasPurpose ?R 
          (exists (?S) 
              (and 
              (instance ?S SocialInteraction) 
              (located ?S ?R))))) 
(disjoint  LivingRoom Bathroom) 
3.2 Representing Semantic Concepts 
Aside from the properties of objects that are mod-
eled using the attribute axiom, semantic relation-
ships that may hold between two concepts 
involving types of activities or actions, character 
emotions, locations of objects, and abilities of cha-
racters or objects must also be modeled. Table 3 
shows sample semantic relationships for these con-
cepts as represented in Picture Books, following 
the semantic categories of ConceptNet (Liu and 
Singh, 2004). 
Objects IsA (doll, toys) 
Activities IsA (play games, activity) 
Concepts 
IsA (grounded, punishment) 
IsA (disorder, problem) 
IsA (no appetite, problem) 
IsA (dizzy, discomfort) 
IsA (itchy, discomfort) 
Emotions IsA (happy, emotion) IsA (scared, emotion) 
Reaction to 
Events 
EffectOf (break object, scared) 
EffectOf (meet new friends, smile) 
Location LocationOf (toys, toy store) 
Capability 
CapableOf (lamp, break) 
CapableOf (glass of water, break) 
CanBe (toys, scattered) 
Table 3. Semantic relationships in Picture Books based 
on ConceptNet (Hong et al 2008) 
In SUMOs, all isA(entity1, entity2) relations 
were replaced with the axiom (subclass entity1 
entity2). To specify that an entity is in a location, 
i.e., locationOf(toys, toy store), first, we create an 
instance of a toystore and then specify that a cer-
tain toy instance is in that toystore, as follows:  
(=> 
    (instance ?TOYSTORE ToyStore) 
    (exists (?TOY) 
        (and 
            (instance ?TOY Toy) 
            (located ?TOY ?TOYSTORE)))) 
The capability axiom is used to conceptualize 
the capability relation (capability ?process ?role 
?obj). It specifies that ?obj has the specified ?role 
in the ?process. For example, a lamp or a glass is 
the patient (receiver) of the process breaking, 
while a toy is the patient for the process scattering. 
(capability Breaking experiencer Lamp) 
(capability Breaking experiencer Glass) 
(capability Scattering experiencer Toy) 
Reaction to events is expressed using the if-else 
axiom of SUMO, for example, if a child character 
causes an accident (a damage), then he/she will 
feel anxiety. Emotions are represented using the 
attribute relation. 
 (=> 
    (and 
      (instance ?ACCIDENT Damaging) 
      (instance ?CHARACTER StoryCharacter) 
      (attribute ?CHARACTER Child) 
      (agent ?ACCIDENT ?CHARACTER)) 
    ((attribute ?CHARACTER Anxiety))) 
3.3 Conceptualizing Actions and Events 
Swartjes (2006) noted that organizing actions and 
events, and causally relating them, is an essential 
step in story generation. Independent of the story 
plot, the causes and effects of character actions can 
be used to describe the events that form the story.  
Actions define activities that can be performed 
by a character in the story, such as play, tell a lie, 
or cry. Events, on the other hand, occur in the story 
as a result of performing some actions, such as a 
lamp breaking as a result of a character or an ob-
ject hitting it. Swartjes (2006) further notes that 
events are not executed by a character. 
Action predicates are used to define the actions 
that may take place given a set of world state. Con-
sider the axiom below which provides a set of four 
44
possible actions ? RecreationOrExercise, Looking, 
Maintaining, and Poking ? that can be performed 
(as an agent) or experienced by a child character 
who is situated near a lamp object in the story 
world. These four actions are subclasses of the In-
tentionalProcess of SUMO. 
 (=> 
  (and 
    (orientation ?CHARACTER ?OBJECT Near) 
    (instance ?CHARACTER StoryCharacter) 
    (attribute ?CHARACTER Child) 
    (instance ?OBJECT Lamp)) 
  (and 
    (capability RecreationOrExercise  
experiencer ?CHARACTER) 
    (capability Looking experiencer ?CHARACTER) 
    (capability Maintaining experiencer ?CHARACTER) 
    (capability Poking experiencer ?CHARACTER))) 
Again, the capability relation is used but in this 
instance, to specify that the character has the role 
of experiencing the specified process. While both 
the agent and the experiencer roles represent the 
doer of a process, an experiencer does not entail a 
causal relation between its arguments. 
Event predicates are used to model explicit 
events that may take place as a result of some cha-
racter actions. Consider again the exists axiom be-
low which states that an instance of an event (in 
this case damaging) can occur when there is a 
child character (the agent) playing near a fragile 
object. The subprocess axiom is used to represent a 
temporally distinguished part of a process and also 
expresses a chain of cause and effect subprocesses 
for playing and damaging. The recipient (patient) 
of the event is the object. 
(=> 
 (and 
   (agent ?X ?CHARACTER) 
   (instance ?CHARACTER StoryCharacter) 
   (attribute ?CHARACTER Child) 
   (instance ?OBJECT Object) 
   (attribute ?OBJECT Fragile) 
   (instance ?X RecreationOrExercise) 
   (orientation ?CHARACTER ?OBJECT Near) 
 (exists (?DAMAGE) 
   (and 
     (instance ?DAMAGE Damaging) 
     (subProcess ?DAMAGE ?X) 
     (agent ?DAMAGE ?CHARACTER) 
     (patient ?DAMAGE ?OBJECT)))) 
Although suitable for inference, the given axiom 
does not fully capture the desired truth as the no-
tion of time is not represented. The axiom says ?if 
a child plays at any point in time, and is near an 
object at any point in time (not necessarily while 
playing), then the object gets damaged during 
playing?.  The more accurate axiom below uses 
holdsDuring to show that the time frames of the 
actual playing and being near the object are the 
same, thus increasing the likelihood of the charac-
ter who is playing to cause the damage. 
(=> 
 (and 
   (instance ?X RecreationOrExercise) 
   (agent ?X ?CHARACTER) 
   (instance ?CHARACTER StoryCharacter) 
   (attribute ?CHARACTER Child) 
   (instance ?OBJECT Object) 
   (attribute ?OBJECT Fragile) 
   (holdsDuring (WhenFn ?X) 
       (orientation ?CHARACTER ?OBJECT Near)) 
(exists (?DAMAGE) 
   (and 
     (instance ?DAMAGE Damaging) 
     (subProcess ?DAMAGE ?X) 
     (agent ?DAMAGE ?CHARACTER) 
     (patient ?DAMAGE ?OBJECT)))) 
As the representation shows, SUMO is quite ca-
pable of encoding temporal properties of events 
with its temporal qualification. However, inferenc-
ing with rules involving time relations between 
events is currently not supported by Sigma (Corda 
et al 2008). Nevertheless, efforts are underway to 
perform true higher-order logical inference (Sut-
cliffe et al 2009). 
The next step involves deriving axioms to 
represent the different ways in which an object can 
be damaged depending on its attribute, for exam-
ple, fragile objects can break while paper-based 
objects such as books and paintings can be torn. 
Consideration must also be made to determine if a 
damage is an accident or intentional. 
3.4 Conceptualizing Story Themes 
Themes can also be mapped to SUMO as thematic 
predicates, and the story planner can identify a 
theme either based on the first action that was per-
formed, or based on user selection. In the latter 
case, when Sigma returns all possible actions, the 
planner can choose one based on the theme. 
45
4 System Architecture 
The architecture of SUMOs, shown in Figure 2, 
has two main modules, the Story Editor and the 
Story Planner, both of which interact with Sigma3 
to retrieve story facts from the SUMO ontology as 
well as to assert new axioms representing the de-
veloping story plan back to SUMO. 
 
Figure 2. Architecture of SUMOs 
 
The Story Editor handles the generation of as-
sertions corresponding to the input picture ele-
ments specified by the user.  
The Story Planner is responsible for planning 
the flow of events in the story. It uses a meta-
knowledge about children?s story comprising of 
five phases ? introduction, problem, rising action, 
solution, and climax. The planner determines and 
phrases the queries that are sent to Sigma and ge-
nerates additional axioms based on the query re-
sults in order to expand the story plan. The 
generated axioms are asserted back to Sigma for 
inclusion in the SUMO ontology to be used again 
for further inferencing.  
Queries sent to Sigma can be classified into 
three categories. Concept-based queries concern 
classes and instances, and are used to determine 
direct and indirect subclass and class-instance rela-
tionships while relation-based queries infer know-
ledge by considering transitivity, symmetry and 
inversion of relations (Corda et al 2008). Action-
based queries identify a set of actions based on the 
                                                          
3
 Sigma Knowledge Engineering Environment,  
http://sigmakee.sourceforge.net  
current world state to drive the story. A fourth cat-
egory, time-event queries, currently not supported 
by Sigma, should reason about temporal and event-
based specifications. 
The interaction between the Story Planner and 
Sigma in Figure 2 raises an issue of search control. 
In Picture Books and SUMOs, information that 
guides the story planning can be bottom-up, i.e. the 
actions and events are determined based on what is 
possible within the story ontology, e.g. through the 
various capability axioms, or top-down, i.e. actions 
are selected based on Machado's narrative subplot 
knowledge. Currently, the Story Planner is respon-
sible for managing the process. However, if both 
these sources of knowledge and constraints can be 
represented in first-order logic, the search control 
of the story planning process can be recast as a 
theorem proving task, i.e. one that searches for a 
proof that satisfies all constraints. This is a future 
research direction. 
The following section presents a more detailed 
trace of system operation and the contents of a sto-
ry plan in first-order logic. 
4.1 Generating Story Plans 
The first part of the story plan contains assertions 
to represent the initial elements of the story. Using 
the story in Table 1 as an example, lines 1 to 6 be-
low assert the main child character and her parent, 
while lines 7 to 8 assert the background and the 
object, respectively.  
1>   (instance Rabbit1 RabbitCharacter) 
2>  (attribute Rabbit1 Child) 
3>  (attribute Rabbit1 Female) 
4>  (instance Rabbit2 RabbitCharacter) 
5>  (attribute Rabbit2 Adult) 
6>  (attribute Rabbit2 Female) 
7>  (instance LivingRoom1 LivingRoom) 
8>  (instance Lamp1 Lamp) 
The next step involves initializing the locations 
of these story elements. Currently, it is setup that 
all objects would be situated in the background and 
the first child character would always be near the 
first object, as shown in the assertions below.  
9>  (located Rabbit1 LivingRoom1) 
10>  (located Lamp1 LivingRoom1) 
11>  (orientation Rabbit1 Lamp1 Near) 
This, however, creates the assumption that the 
child character is already in the location near ob-
jects which he will interact with, which may not 
return 
results 
abstract 
story plan 
assertions 
assertions 
obtain 
results 
Story 
Editor SUMO 
Ontology 
(Story 
Ontology) 
SIGMA  
(Inference 
Engine) 
Story 
Planner 
Story plan  
(SUMO) 
query 
46
necessarily be true and reduces the flexibility of 
the system. In order to create more varied stories, 
the initial location can be identified based on the 
theme and the first event that the user would want 
to likely happen in the story. 
From the initial set of assertions, the story plan-
ner issues its first concept-based query to Sigma 
with ?(name Rabbit1 ?X)? to determine a name for 
the main character, Rabbit1, and receives ?Rizzy? 
as a result. This is asserted to the story plan as: 
12>  (name Rabbit1 ?Rizzy?) 
The next query is the first action-based query 
used to determine the first action to start the story 
flow. Given ?(capability ?X experiencer Rabbit1)?, 
which is intended for identifying the set of possible 
starting actions that the main character, Rabbit1, 
can perform with the object in the background, 
Sigma returns the following list (assuming the sto-
ry facts given in the previous section):  
X = [RecreationOrExercise, Looking, 
Maintaining, Poking]  
Assuming the planner selects RecreationOrEx-
ercise, the following assertions are then added to 
the story plan: 
13>  (instance RecOrEx1 RecreationOrExercise)  
14>  (agent RecOrEx1 Rabbit1) 
At this point, the introduction phase of the story 
plan has been completed. The problem phase be-
gins with a query to identify any instances of prob-
lems that can occur, i.e. ?(instance ?X Damaging)?. 
Damaging the object lamp causes its attribute to be 
changed, and again we query Sigma for this 
change of state with ?(attribute Lamp1 ?X)? yielding 
the result broken, and the corresponding emotional 
state of the character ?(attribute Rabbit1 ?X)?. The 
following assertions were added to the plan: 
15>  (instance (sk0 Rabbit1 Lamp1   
                   RecOrEx1) Damaging) 
16>  (attribute Lamp1 Broken) 
17>  (attribute Rabbit1 Anxiety) 
While a full explanation of skolemization is not 
possible here for space reasons, we note that the 
second argument of assertion #15 (derived from 
Sigma?s answer to the query) stands for the exis-
tence of an unnamed term, in this case, that there is 
an instance of a Damaging process. The agent 
(Rabbit1), patient (Lamp1), and the action (RecO-
rEx1) that caused the problem were all provided in 
the query result. 
4.2 Generating Surface Text 
SUMO-based story plans provide a form of inter-
lingua where story details are represented in logi-
cal form. The logical representation allows 
generation of the same story in different languages 
(that are connected to WordNet). Sigma already 
has a language generator, with templates for Eng-
lish, and an initial set for Tagalog (Borra et al 
2010).  Work is currently underway to enhance the 
existing language generator in Sigma and make the 
generated text more natural. Sigma can then be 
used to generate stories automatically from the 
knowledge asserted in the story generation process. 
5 Conclusions and Further Work 
The paper presented a preliminary work aimed at 
representing storytelling knowledge in SUMO and 
using Sigma as inference engine to assist the plan-
ner in generating story plans. Further work focuses 
on modeling the emotional state of the character as 
a result of some event (e.g., feeling worried, guilty 
or scared due to causing some problems in the 
world state), changes in character traits as the story 
progresses (e.g., from negative trait to positive trait 
as the story flows from rule violation to value ac-
quisition), and enhancing the representation for 
story themes. Once a set of knowledge has been 
developed, these should be evaluated systematical-
ly through validation of the rules for logical consis-
tency with the theorem prover. A future goal is to 
apply the metrics proposed by Callaway & Lester 
(2002) in StoryBook to evaluate with actual users 
if the generated stories are better and more varied 
as compared to that of Picture Books. 
Although SUMO is quite capable of 
representing time and sequences, reasoning with 
temporally qualified expression is challenging for 
any theorem prover. The works of (Sutcliffe et al 
2009) to extend the inference engine to handle rea-
soning over temporal relations should be explored 
further to allow SUMOs to generate story plans 
that consider temporal relations between actions 
and events. 
Finally, story generators will benefit its readers 
if the generated stories are narrated orally. SUMOs 
can be explored further to model various emotions 
to provide annotations in the surface story text 
which will then be fed to a text to speech tool for 
speech generation. 
47
References 
Borra, A., Pease, A., Roxas, R. and Dita, S. 2010. Intro-
ducing Filipino WordNet. In: Principles, Construc-
tion and Application of Multilingual Wordnets: 
Proceedings of the 5th Global WordNet Conference, 
Mumbai, India. 
Callaway, C. B., and Lester, J. C. 2002. Narrative Prose 
Generation. Artificial Intelligence, 139(2):213-252, 
Elsevier Science Publishers Ltd., Essex, UK.  
Corda, I., Bennett, B., and Dimitrova, V. 2008. Interact-
ing with an Ontology to Explore Historical Domains. 
Proceedings of the 2008 First International Work-
shop on Ontologies in Interactive Systems, 65-74, 
IEEE Computer Society. 
Gangemi, A., Guarino, N., Masolo, C., and Oltramari, 
A. 2003. AI Magazine, 24(3):13-24, Association for 
the Advancement of Artificial Intelligence. 
Kooijman, R. 2004. De virtuele verhalenverteller: 
voorstel voor het gebruik van een upper-ontology en 
een nieuwe architectuur. Technical  Report. Universi-
ty of Twente, Department of Electrical Engineering, 
Mathematics and Computer Science. 
Hong, A., Solis, C., Siy, J.T., and Tabirao, E. 2008. Pic-
ture Books: Automated Story Generator. Undergra-
duate Thesis, De La Salle University, Manila, 
Philippines. 
Lenat, D.B. 1995. Cyc: A Large-Scale Investment in 
Knowledge Infrastructure, Communications of the 
ACM, 38(11).  
Liu, H. and Singh, P. 2004. Commonsense Reasoning in 
and over Natural Language. Proceedings of the 8th 
International Conference on Knowledge-Based Intel-
ligent Information and Engineering Systems, 293-
306, Wellington, New Zealand, Springer Berlin. 
Machado, J. 2003. Storytelling. In Early Childhood 
Experiences in Language Arts: Emerging Literacy, 
304-319. Clifton Park, N.Y., Thomson/Delmar 
Learning. 
Niles, I. and Pease, A. 2001. Towards A Standard Upper 
Ontology.   Proceedings of Formal Ontology in 
Information Systems (FOIS 2001), 2-9, October 17-
19, Ogunquit, Maine, USA. 
Noy, N. and McGuinness, D. 2001. Ontology Develop-
ment 101: A Guide to Creating Your First Ontology. 
Stanford Knowledge Systems Laboratory Technical 
Report KSL-01-05 and Stanford Medical Informatics 
Technical Report SMI-2001-0880, March 2001. 
Ong, E. 2009. Prospects in Creative Natural Language 
Processing. Proceedings of the 6th National Natural 
Language Processing Research Symposium, De La 
Salle University, Manila, Philippines. 
 
 
 
 
 
Pease, A. 2006. Formal Representation of Concepts: 
The Suggested Upper Merged Ontology and Its Use 
in Linguistics. Ontolinguistics. How Ontological Sta-
tus Shapes the Linguistic Coding of Concepts. Schal-
ley, A.C. and Zaefferer, D. (ed.), Vorbereitung 
Berlin, New York. 
Pease, A. 2003. The Sigma Ontology Development En-
vironment. Working Notes of the IJCAI-2003 Work-
shop on Ontology and Distributed Systems, vol. 71 of 
CEUR Workshop Proceeding series. 
Riedl, M. and Young, R.M. 2004. An Intent-Driven 
Planner for Multi-Agent Story Generation. Proceed-
ings of the Third International Joint Conference on 
Autonomous Agents and Multi-Agent Systems, 186-
193, Washington DC, USA, IEEE Computer Society. 
Smith, B. 1998. The Basic Tools of Formal Ontology. 
Formal Ontology in Information Systems, Nicola Gu-
arino (ed),  IOS Press, Washington. Frontiers in Ar-
tificial Intelligence and Applications, 19-28. 
Solis, C., Siy, J.T., Tabirao, E., and Ong, E. 2009. Plan-
ning Author and Character Goals for Story Genera-
tion. Proceedings of the NAACL Human Language 
Technology 2009 Workshop on Computational Ap-
proaches to Linguistic Creativity, 63-70, Boulder, 
Colorado, USA. 
Sutcliffe, G., Benzm?ller, C., Brown, C.E., and Theiss, 
F. 2009. Progress in the Development of Automated 
Theorem Proving for Higher-order Logic. Automated 
Deduction, 22nd International Conference on Auto-
mated Deduction, Montreal, Canada, August 2-7, 
2009. Proceedings of the Lecture Notes in AI, vol. 
5663, 116-130, 2009, Springer. 
Swartjes, I. 2006. The Plot Thickens: Bringing Structure 
and Meaning into Automated Story Generation. Mas-
ter's Thesis, University of Twente, The Netherlands. 
Theune, M., Nijholt, A., Oinonen, K., and Uijlings J. 
2006. Designing a Story Database for Use in Auto-
matic Story Generation.  Proceedings 5th Interna-
tional Conference Entertainment Computing, 
Cambridge, UK. Lecturer Notes in Computer 
Science, 4161:298-301, Heidelberg, Springer Berlin. 
Uijlings, J.R.R. 2006. Designing a Virtual Environment 
for Story Generation. MS Thesis, University of Ams-
terdam, The Netherlands. 
Venour, C. and Reiter, E. 2008. A Tutorial for Sim-
plenlg. http://www.csd.abdn.ac.uk/~ereiter/simplenlg 
WordNet. 2006. WordNet: A Lexical Database for the 
English Language. Princeton University, New Jersey. 
48
