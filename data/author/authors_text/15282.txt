Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320?330,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 
 
Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora 
 
Bin Lu1,3*, Chenhao Tan2, Claire Cardie2 and Benjamin K. Tsou3,1 
1 Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong 
2 Department of Computer Science, Cornell University, Ithaca, NY, USA 
3 Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education, Hong Kong  
lubin2010@gmail.com, {chenhao, cardie}@cs.cornell.edu, btsou99@gmail.com 
 
 
Abstract 
Most previous work on multilingual sentiment 
analysis has focused on methods to adapt 
sentiment resources from resource-rich 
languages to resource-poor languages. We 
present a novel approach for joint bilingual 
sentiment classification at the sentence level 
that augments available labeled data in each 
language with unlabeled parallel data. We rely 
on the intuition that the sentiment labels for 
parallel sentences should be similar and present 
a model that jointly learns improved mono-
lingual sentiment classifiers for each language. 
Experiments on multiple data sets show that the 
proposed approach (1) outperforms the mono-
lingual baselines, significantly improving the 
accuracy for both languages by 3.44%-8.12%; 
(2) outperforms two standard approaches for 
leveraging unlabeled data; and (3) produces 
(albeit smaller) performance gains when 
employing pseudo-parallel data from machine 
translation engines. 
1 Introduction 
The field of sentiment analysis has quickly 
attracted the attention of researchers and 
practitioners alike (e.g. Pang et al, 2002; Turney, 
2002; Hu and Liu, 2004; Wiebe et al, 2005; Breck 
et al, 2007; Pang and Lee, 2008). 1 Indeed, 
sentiment analysis systems, which mine opinions 
from textual sources (e.g. news, blogs, and 
reviews), can be used in a wide variety of 
                                                          
*The work was conducted when the first author was visiting 
Cornell University. 
applications, including interpreting product 
reviews, opinion retrieval and political polling.  
Not surprisingly, most methods for sentiment 
classification are supervised learning techniques, 
which require training data annotated with the 
appropriate sentiment labels (e.g. document-level 
or sentence-level positive vs. negative polarity).  
This data is difficult and costly to obtain, and must 
be acquired separately for each language under 
consideration.  
Previous work in multilingual sentiment analysis 
has therefore focused on methods to adapt 
sentiment resources (e.g. lexicons) from resource-
rich languages (typically English) to other 
languages, with the goal of transferring sentiment 
or subjectivity analysis capabilities from English to 
other languages (e.g. Mihalcea et al (2007); Banea 
et al (2008; 2010); Wan (2008; 2009); 
Prettenhofer and Stein (2010)). In recent years, 
however, sentiment-labeled data is gradually 
becoming available for languages other than 
English (e.g. Seki et al (2007; 2008); Nakagawa et 
al. (2010); Schulz et al (2010)). In addition, there 
is still much room for improvement in existing 
monolingual (including English) sentiment 
classifiers, especially at the sentence level (Pang 
and Lee, 2008).  
This paper tackles the task of bilingual 
sentiment analysis. In contrast to previous work, 
we (1) assume that some amount of sentiment-
labeled data is available for the language pair 
under study, and (2) investigate methods to 
simultaneously improve sentiment classification 
for both languages. Given the labeled data in each 
language, we propose an approach that exploits an 
unlabeled parallel corpus with the following 
320
  
intuition: two sentences or documents that are 
parallel (i.e. translations of one another) should 
exhibit the same sentiment ? their sentiment 
labels (e.g. polarity, subjectivity, intensity) should 
be similar. The proposed maximum entropy-based 
EM approach jointly learns two monolingual 
sentiment classifiers by treating the sentiment 
labels in the unlabeled parallel text as unobserved 
latent variables, and maximizes the regularized 
joint likelihood of the language-specific labeled 
data together with the inferred sentiment labels of 
the parallel text.  Although our approach should be 
applicable at the document-level and for additional 
sentiment tasks, we focus on sentence-level 
polarity classification in this work. 
We evaluate our approach for English and 
Chinese on two dataset combinations (see Section 
4) and find that the proposed approach outperforms 
the monolingual baselines (i.e. maximum entropy 
and SVM classifiers) as well as two alternative 
methods for leveraging unlabeled data 
(transductive SVMs (Joachims, 1999b) and co-
training (Blum and Mitchell, 1998)).  Accuracy is 
significantly improved for both languages, by 
3.44%-8.12%. We furthermore find that 
improvements, albeit smaller, are obtained when 
the parallel data is replaced with a pseudo-parallel 
(i.e. automatically translated) corpus. To our 
knowledge, this is the first multilingual sentiment 
analysis study to focus on methods for 
simultaneously improving sentiment classification 
for a pair of languages based on unlabeled data 
rather than resource adaptation from one language 
to another.  
The rest of the paper is organized as follows. 
Section 2 introduces related work. In Section 3, the 
proposed joint model is described. Sections 4 and 
5, respectively, provide the experimental setup and 
results; the conclusion (Section 6) follows. 
2 Related Work 
Multilingual Sentiment Analysis.  There is a 
growing body of work on multilingual sentiment 
analysis. Most approaches focus on resource 
adaptation from one language (usually English) to 
other languages with few sentiment resources. 
Mihalcea et al (2007), for example, generate 
subjectivity analysis resources in a new language 
from English sentiment resources by leveraging a 
bilingual dictionary or a parallel corpus. Banea et 
al. (2008; 2010) instead automatically translate the 
English resources using automatic machine 
translation engines for subjectivity classification. 
Prettenhofer and Stein (2010) investigate cross-
lingual sentiment classification from the 
perspective of domain adaptation based on 
structural correspondence learning (Blitzer et al, 
2006). 
Approaches that do not explicitly involve 
resource adaptation include Wan (2009), which 
uses co-training (Blum and Mitchell, 1998) with 
English vs. Chinese features comprising the two 
independent ?views? to exploit unlabeled Chinese 
data and a labeled English corpus and thereby 
improves Chinese sentiment classification. 
Another notable approach is the work of Boyd-
Graber and Resnik (2010), which presents a 
generative model --- supervised multilingual latent 
Dirichlet alocation --- that jointly models topics 
that are consistent across languages, and employs 
them to better predict sentiment ratings. 
Unlike the methods described above, we focus 
on simultaneously improving the performance of 
sentiment classification in a pair of languages by 
developing a model that relies on sentiment-
labeled data in each language as well as unlabeled 
parallel text for the language pair. 
Semi-supervised Learning.  Another line of 
related work is semi-supervised learning, which 
combines labeled and unlabeled data to improve 
the performance of the task of interest (Zhu and 
Goldberg, 2009). Among the popular semi-
supervised methods (e.g. EM on Na?ve Bayes 
(Nigam et al, 2000), co-training (Blum and 
Mitchell, 1998), transductive SVMs (Joachims, 
1999b), and co-regularization (Sindhwani et al, 
2005; Amini et al, 2010)), our approach employs 
the EM algorithm, extending it to the bilingual 
case based on maximum entropy. We compare to 
co-training and transductive SVMs in Section 5. 
Multilingual NLP for Other Tasks. Finally, 
there exists related work using bilingual resources 
to help other NLP tasks, such as word sense 
disambiguation (e.g. Ido and Itai (1994)), parsing 
(e.g. Burkett and Klein (2008); Zhao et al (2009); 
Burkett et al (2010)), information retrieval (Gao et 
al., 2009), named entity detection (Burkett et al, 
2010); topic extraction (e.g. Zhang et al, 2010), 
text classification (e.g. Amini et al, 2010), and 
hyponym-relation acquisition (e.g. Oh et al, 2009). 
321
  
In these cases, multilingual models increase 
performance because different languages contain 
different ambiguities and therefore present 
complementary views on the shared underlying 
labels.  Our work shares a similar motivation. 
3 A Joint Model with Unlabeled Parallel 
Text 
We propose a maximum entropy-based statistical 
model. Maximum entropy (MaxEnt) models1 have 
been widely used in many NLP tasks (Berger et al, 
1996; Ratnaparkhi, 1997; Smith, 2006). The 
models assign the conditional probability of the 
label   given the observation   as follows: 
          
 
 
                                 (1) 
where    is a real-valued vector of feature weights 
and    is a feature function that maps pairs       to 
a nonnegative real-valued feature vector. Each 
feature has an associated parameter,   , which is 
called its weight; and   is the corresponding 
normalization factor.  
Maximum likelihood parameter estimation 
(training) for such a model, with a set of labeled 
examples            
  , amounts to solving the 
following optimization problem: 
  
 
                   
 
                        (2) 
3.1 Problem Definition 
Given two languages    and   , suppose we have 
two distinct (i.e. not parallel) sets of sentiment-
labeled data,    and     written in    and     
respectively. In addition, we have unlabeled (w.r.t. 
sentiment) bilingual (in    and   ) parallel data   
that are defined as follows. 
               
    
     
     
               
    
     
    
     
    
       
     
   
   
 
   
where               denotes the polarity of 
the  -th instance    (positive or negative);    and    
are respectively the numbers of labeled instances 
in    and   ;   
   and   
   are parallel instances in    
and   , respectively (i.e. they are supposed to be 
                                                          
1They are sometimes referred to as log-linear models, but also 
known as exponential models, generalized linear models, or 
logistic regression. 
translations of one another), whose labels   
   and 
  
   are unobserved, but according to the intuition 
outlined in Section 1, should be similar.  
Given the input data        and  , our task is to 
jointly learn two monolingual sentiment classifiers 
? one for    and one for   . With MaxEnt, we 
learn from the input data:  
                   
 
     
 
  
where    
   and     
 
 are the vectors of feature weights 
for    and   , respectively (for brevity we denote 
them as    and    in the remaining sections). In this 
study, we focus on sentence-level sentiment 
classification, i.e. each    is a sentence, and   
   and 
  
   are parallel sentences. 
3.2 The Joint Model  
Given the problem definition above, we now 
present a novel model to exploit the 
correspondence of parallel sentences in unlabeled 
bilingual text. The model maximizes the following 
joint likelihood with respect to    and   : 
                                        
    
    
    
    
         
        
    
     
  
   
 
     
     
     
     
     
         
 
   (3) 
where          denotes    or   ; the first term on 
the right-hand side is the likelihood of labeled data 
for both    and   ; and the second term is the 
likelihood of the unlabeled parallel data  .  
If we assume that parallel sentences are perfect 
translations, the two sentences in each pair should 
have the same polarity label, which gives us:   
    
     
     
     
          
     
    
          
    
        
                          (4) 
where   
  is the unobserved class label for the  -th 
instance in the unlabeled data. This probability 
directly models the sentiment label agreement 
between   
   and   
  . 
However, there could be considerable noise in 
real-world parallel data, i.e. the sentence pairs may 
be noisily parallel (or even comparable) instead of 
fully parallel (Munteanu and Marcu, 2005). In such 
noisy cases, the labels (positive or negative) could 
be different for the two monolingual sentences in a 
sentence pair. Although we do not know the exact 
probability that a sentence pair exhibits the same 
label, we can approximate it using their translation 
322
  
probabilities, which can be computed using word 
alignment toolkits such as Giza++ (Och and Ney, 
2003) or the Berkeley word aligner (Liang et al, 
2006). The intuition here is that if the translation 
probability of two sentences is high, the probability 
that they have the same sentiment label should be 
high as well. Therefore, by considering the noise in 
parallel data, we get: 
    
     
     
     
           
          
    
         
    
            
              
    
             
               (5)                 
where       is the translation probability of the  -th 
sentence pair in  ;2    is the opposite of   ; the first 
term models the probability that   
   and   
   have 
the same label; and the second term models the 
probability that they have different labels.  
By further considering the weight to ascribe to 
the unlabeled data vs. the labeled data (and the 
weight for the L2-norm regularization), we get the 
following regularized joint log likelihood to be 
maximized: 
                                    
 
    
         
    
    
    
         
  
 
      
  
         (6) 
where the first term on the right-hand side is the 
log likelihood of the labeled data from both    and 
    the second is the log likelihood of the 
unlabeled parallel data  , multiplied by     , a 
constant that controls the contribution of the 
unlabeled data; and      is a regularization 
constant that penalizes model complexity or large 
feature weights. When    is 0, the algorithm 
ignores the unlabeled data and degenerates to two 
MaxEnt models trained on only the labeled data. 
3.3 The EM Algorithm on MaxEnt 
To solve the optimization problem for the model, 
we need to jointly estimate the optimal parameters 
for the two monolingual classifiers by finding: 
   
    
                                      (7) 
This can be done with an EM algorithm, whose 
steps are summarized in Algorithm 1. First, the 
MaxEnt parameters,    and   , are estimated from 
                                                          
2The probability should be rescaled within the range of [0, 1], 
where 0.5 means that we are completely unsure if the 
sentences are translations of each other or not, and only those 
translation pairs with a probability larger than 0.5 are 
meaningful for our purpose. 
just the labeled data. Then, in the E-step, the 
classifiers, based on current values of     and   , 
compute          for each labeled example and 
assign probabilistically-weighted class labels to 
each unlabeled example. Next, in the M-step, the 
parameters,    and   , are updated using both the 
original labeled data (   and   ) and the newly 
labeled data  . These last two steps are iterated 
until convergence or a predefined iteration limit  . 
Algorithm 1. The MaxEnt-based EM Algorithm for 
Multilingual Sentiment Classification 
Input: Labeled data    and    
Unlabeled parallel data   
Output: 
Two monolingual MaxEnt classifiers with 
parameters   
  and   
 , respectively 
1. Train two initial monolingual models 
Train and initialize   
   
 and   
   
 on the labeled data 
2. Jointly optimize two monolingual models 
for     to   do // T: number of iterations 
       E-Step: 
Compute         for each example in    ,    and   
based on   
     
 and   
     
; 
Compute the expectation of the log likelihood with 
respect to       ; 
M-Step: 
Find    
   
 and   
   
 by maximizing the regularized 
joint log likelihood; 
Convergence: 
 If the increase of the joint log likelihood is 
sufficiently small, break; 
      end for  
3. Output    
  as   
   
s, and   
  as    
   
  
In the M-step, we can optimize the regularized 
joint log likelihood using any gradient-based 
optimization technique (Malouf, 2002). The 
gradient for Equation 3 based on Equation 4 is 
shown in Appendix A; those for Equations 5 and 6 
can be derived similarly. In our experiments, we 
use the L-BFGS algorithm (Liu et al, 1989) and 
run EM until the change in regularized joint log 
likelihood is less than 1e-5 or we reach 100 
iterations.3 
                                                          
3Since the EM-based algorithm may find a local maximum of 
the objective function, the initialization of the parameters is 
important. Our experiments show that an effective maximum 
can usually be found by initializing the parameters with those 
learned from the labeled data; performance would be much 
worse if we initialize all the parameters to 0 or 1. 
323
  
3.4 Pseudo-Parallel Labeled and Unlabeled 
Data 
We also consider the case where a parallel corpus 
is not available: to obtain a pseudo-parallel corpus 
  (i.e. sentences in one language with their 
corresponding automatic translations), we use an 
automatic machine translation system (e.g. Google 
machine translation 4 ) to translate unlabeled in-
domain data from    to    or vice versa. 
Since previous work (Banea et al, 2008; 2010; 
Wan, 2009) has shown that it could be useful to 
automatically translate the labeled data from the 
source language into the target language, we can 
further incorporate such translated labeled data into 
the joint model by adding the following component 
into Equation 6: 
           
    
      
  
   
 
                  (8) 
where    is the alternative class of  ,   
   is the 
automatically translated example from   
 ; and  
     is a constant that controls the weight of the 
translated labeled data. 
4 Experimental Setup 
4.1 Data Sets and Preprocessing 
The following labeled datasets are used in our 
experiments. 
MPQA (Labeled English Data): The Multi-
Perspective Question Answering (MPQA) corpus 
(Wiebe et al, 2005) consists of newswire 
documents manually annotated with phrase-level 
subjectivity information. We extract all sentences 
containing strong (i.e. intensity is medium or 
higher), sentiment-bearing (i.e. polarity is positive 
or negative) expressions following Choi and 
Cardie (2008). Sentences with both positive and 
negative strong expressions are then discarded, and 
the polarity of each remaining sentence is set to 
that of its sentiment-bearing expression(s). 
NTCIR-EN (Labeled English Data) and 
NTCIR-CH (Labeled Chinese Data): The 
NTCIR Opinion Analysis task (Seki et al, 2007; 
2008) provides sentiment-labeled news data in 
Chinese, Japanese and English. Only those 
sentences with a polarity label (positive or 
negative) agreed to by at least two annotators are 
extracted. We use the Chinese data from NTCIR-6 
                                                          
4http://translate.google.com/ 
as our Chinese labeled data. Since far fewer 
sentences in the English data pass the annotator 
agreement filter, we combine the English data from 
NTCIR-6 and NTCIR-7. The Chinese sentences 
are segmented using the Stanford Chinese word 
segmenter (Tseng et al, 2005). 
The number of sentences in each of these 
datasets is shown in Table 1. In our experiments, 
we evaluate two settings of the data: (1) 
MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIR-
CH. In each setting, the English labeled data 
constitutes    and the Chinese labeled data,   .  
 MPQA NTCIR-EN NTCIR-CH 
Positive 1,471 (30%) 528 (30%) 2,378 (55%) 
Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) 
Total 4,958 1,737 4,294 
Table 1: Sentence Counts for the Labeled Data 
Unlabeled Parallel Text and its Preprocessing. 
For the unlabeled parallel text, we use the ISI 
Chinese-English parallel corpus (Munteanu and 
Marcu, 2005), which was extracted automatically 
from news articles published by Xinhua News 
Agency in the Chinese Gigaword (2nd Edition) and 
English Gigaword (2nd Edition) collections. 
Because sentence pairs in the ISI corpus are quite 
noisy, we rely on Giza++ (Och and Ney, 2003) to 
obtain a new translation probability for each 
sentence pair, and select the 100,000 pairs with the 
highest translation probabilities.5  
We also try to remove neutral sentences from 
the parallel data since they can introduce noise into 
our model, which deals only with positive and 
negative examples. To do this, we train a single 
classifier from the combined Chinese and English 
labeled data for each data setting above by 
concatenating the original English and Chinese 
feature sets. We then classify each unlabeled 
sentence pair by combining the two sentences in 
each pair into one. We choose the most confidently 
predicted 10,000 positive and 10,000 negative 
pairs to constitute the unlabeled parallel corpus   
for each data setting. 
                                                          
5We removed sentence pairs with an original confidence score 
(given in the corpus) smaller than 0.98, and also removed the 
pairs that are too long (more than 60 characters in one 
sentence) to facilitate Giza++. We first obtain translation 
probabilities for both directions (i.e. Chinese to English and 
English to Chinese) with Giza++, take the log of the product 
of those two probabilities, and then divide it by the sum of 
lengths of the two sentences in each pair.  
324
  
4.2 Baseline Methods 
In our experiments, the proposed joint model is 
compared with the following baseline methods. 
MaxEnt: This method learns a MaxEnt 
classifier for each language given the monolingual 
labeled data; the unlabeled data is not used.  
SVM: This method learns an SVM classifier for 
each language given the monolingual labeled data; 
the unlabeled data is not used. SVM-light 
(Joachims, 1999a) is used for all the SVM-related 
experiments. 
Monolingual TSVM (TSVM-M): This method 
learns two transductive SVM (TSVM) classifiers 
given the monolingual labeled data and the 
monolingual unlabeled data for each language.  
Bilingual TSVM (TSVM-B): This method 
learns one TSVM classifier given the labeled 
training data in two languages together with the 
unlabeled sentences by combining the two 
sentences in each unlabeled pair into one. We 
expect this method to perform better than TSVM-
M since the combined (bilingual) unlabeled 
sentences could be more helpful than the unlabeled 
monolingual sentences. 
Co-Training with SVMs (Co-SVM): This 
method applies SVM-based co-training given both 
the labeled training data and the unlabeled parallel 
data following Wan (2009). First, two monolingual 
SVM classifiers are built based on only the 
corresponding labeled data, and then they are 
bootstrapped by adding the most confident 
predicted examples from the unlabeled data into 
the training set. We run bootstrapping for 100 
iterations. In each iteration, we select the most 
confidently predicted 50 positive and 50 negative 
sentences from each of the two classifiers, and take 
the union of the resulting 200 sentence pairs as the 
newly labeled training data. (Examples with 
conflicting labels within the pair are not included.) 
5 Results and Analysis 
In our experiments, the methods are tested in the 
two data settings with the corresponding unlabeled 
parallel corpus as mentioned in Section 4.6 We use 
                                                          
6 The results reported in this section employ Equation 4. 
Preliminary experiments showed that Equation 5 does not 
significantly improve the performance in our case, which is 
reasonable since we choose only sentence pairs with the 
highest translation probabilities to be our unlabeled data (see 
Section 4.1).      
5-fold cross-validation and report average accuracy 
(also MicroF1 in this case) and MacroF1 scores. 
Unigrams are used as binary features for all 
models, as Pang et al (2002) showed that binary 
features perform better than frequency features for 
sentiment classification. The weights for unlabeled 
data and regularization,    and   , are set to 1 
unless otherwise stated. Later, we will show that 
the proposed approach performs well with a wide 
range of parameter values.7 
5.1 Method Comparison 
We first compare the proposed joint model (Joint) 
with the baselines in Table 2. As seen from the 
table, the proposed approach outperforms all five 
baseline methods in terms of both accuracy and 
MacroF1 for both English and Chinese and in both 
of the data settings. 8  By making use of the 
unlabeled parallel data, our proposed approach 
improves the accuracy, compared to MaxEnt, by 
8.12% (or 33.27% error reduction) on English and 
3.44% (or 16.92% error reduction) on Chinese in 
the first setting, and by 5.07% (or 19.67% error 
reduction) on English and 3.87% (or 19.4% error 
reduction) on Chinese in the second setting. 
 Among the baselines, the best is Co-SVM; 
TSVMs do not always improve performance using 
the unlabeled data compared to the standalone 
SVM; and TSVM-B outperforms TSVM-M except 
for Chinese in the second setting. The MPQA data 
is more difficult in general compared to the NTCIR 
data. Without unlabeled parallel data, the 
performance on the Chinese data is better than on 
the English data, which is consistent with results 
reported in NTCIR-6 (Seki et al, 2007).  
Overall, the unlabeled parallel data improves 
classification accuracy for both languages when 
using our proposed joint model and Co-SVM. The 
joint model makes better use of the unlabeled 
parallel data than Co-SVM or TSVMs presumably 
because of its attempt to jointly optimize the two 
monolingual models via soft (probabilistic) 
assignments of the unlabeled instances to classes in 
each iteration, instead of the hard assignments in 
Co-SVM and TSVMs. Although English sentiment 
                                                          
7The code is at http://sites.google.com/site/lubin2010. 
8 Significance is tested using paired t-tests with  <0.05: ? 
denotes statistical significance compared to the corresponding 
performance of MaxEnt; * denotes statistical significance 
compared to SVM; and 
?
 denotes statistical significance 
compared to Co-SVM. 
325
  
classification alone is more difficult than Chinese 
for our datasets, we obtain greater performance 
gains for English by exploiting unlabeled parallel 
data as well as the Chinese labeled data.  
5.2 Varying the Weight and Amount of 
Unlabeled Data 
Figure 1 shows the accuracy curve of the proposed 
approach for the two data settings when varying 
the weight for the unlabeled data,   , from 0 to 1. 
When    is set to 0, the joint model degenerates to 
two MaxEnt models trained with only the labeled 
data.  
We can see that the performance gains for the 
proposed approach are quite remarkable even when 
   is set to 0.1; performance is largely stable after 
   reaches 0.4. Although MPQA is more difficult 
in general compared to the NTCIR data, we still 
see steady improvements in performance with 
unlabeled parallel data. Overall, the proposed 
approach performs quite well for a wide range of 
parameter values of   .  
Figure 2 shows the accuracy curve of the 
proposed approach for the two data settings when 
varying the amount of unlabeled data from 0 to 
20,000 instances. We see that the performance of 
the proposed approach improves steadily by adding 
more and more unlabeled data. However, even 
with only 2,000 unlabeled sentence pairs, the 
proposed approach still produces large 
performance gains.  
5.3 Results on Pseudo-Parallel Unlabeled 
Data 
As discussed in Section 3.4, we generate pseudo-
parallel data by translating the monolingual 
sentences in each setting using Google?s machine 
translation system. Figures 3 and 4 show the 
performance of our model using the pseudo-
parallel data versus the real parallel data, in the two 
settings, respectively. The EN->CH pseudo-
parallel data consists of the English unlabeled data 
and its automatic Chinese translation, and vice 
versa. 
Although not as significant as those with parallel 
data, we can still obtain improvements using the 
pseudo-parallel data, especially in the first setting. 
The difference between using parallel versus 
pseudo-parallel data is around 2-4% in Figures 3 
and 4, which is reasonable since the quality of the 
pseudo-parallel data is not as good as that of the 
parallel data. Therefore, the performance using 
pseudo-parallel data is better with a small weight 
(e.g.   = 0.1) in some cases.  
 
Setting 1: NTCIR-EN+NTCIR-CH Setting 2: MPQA+NTCIR-CH 
Accuracy MacroF1 Accuracy MacroF1 
English Chinese English Chinese English Chinese English Chinese 
MaxEnt 75.59 79.67 66.61* 79.34 74.22 79.67 65.09* 79.34 
SVM 76.34 81.02 61.12 80.75? 76.74? 81.02 61.35 80.75? 
TSVM-M 73.46 80.21 55.33 79.99 72.89 81.14 52.82 79.99 
TSVM-B 78.36 81.60? 65.53 81.42 76.42? 78.51 61.66 78.32 
Co-SVM 82.44?* 82.79? 72.61?* 82.67?* 78.18?* 82.63?* 68.03?* 82.51?* 
Joint 83.71?* 83.11?* 75.89?*? 82.97?* 79.29?*? 83.54?* 72.58?*? 83.37?* 
Table 2: Comparison of Results 
       
Figure 1. Accuracy vs. Weight of Unlabeled Data                Figure 2. Accuracy vs. Amount of Unlabeled Data 
 
0 0.2 0.4 0.6 0.8 1
72
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
0 0.5 1 1.5 2
72
74
76
78
80
82
84
86
Size of Unlabeled Data
A
c
c
u
r
a
c
y
 
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
326
  
5.4 Adding Pseudo-Parallel Labeled Data 
In this section, we investigate how adding 
automatically translated labeled data might 
influence the performance as mentioned in Section 
3.4. We use only the translated labeled data to train 
classifiers, and then directly classify the test data. 
The average accuracies in setting 1 are 66.61% and 
63.11% on English and Chinese, respectively; 
while the accuracies in setting 2 are 58.43% and 
54.07% on English and Chinese, respectively. This 
result is reasonable because of the language gap 
between the original language and the translated 
language. In addition, the class distributions of the 
English labeled data and the Chinese are quite 
different (30% vs. 55% for positive as shown in 
Table 1).  
Figures 5 and 6 show the accuracies when 
varying the weight of the translated labeled data vs. 
the labeled data, with and without the unlabeled 
parallel data. From Figure 5 for setting 1, we can 
see that the translated data can be helpful given the 
labeled data and even the unlabeled data, as long as 
   is small; while in Figure 6, the translated data 
decreases the performance in most cases for setting 
2. One possible reason is that in the first data 
setting, the NTCIR English data covers the same 
topics as the NTCIR Chinese data and thus direct 
translation is helpful, while the English and 
Chinese topics are quite different in the second 
data setting, and thus direct translation hurts the 
performance given the existing labeled data in each 
language. 
5.5 Discussion 
To further understand what contributions our 
proposed approach makes to the performance gain, 
we look inside the parameters in the MaxEnt 
models learned before and after adding the parallel 
unlabeled data. Table 3 shows the features in the 
model learned from the labeled data that have the 
largest weight change after adding the parallel data;  
     
Figure 3. Accuracy with Pseudo-Parallel Unlabeled           Figure 4. Accuracy with Pseudo-Parallel Unlabeled 
 Data in Setting 1                                                         Data in Setting 2 
        
Figure 5. Accuracy with Pseudo-Parallel Labeled              Figure 6. Accuracy with Pseudo-Parallel Labeled  
Data in Setting 1                                                      Data in Setting 2 
 
 
0 0.2 0.4 0.6 0.8 1
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
65
70
75
80
85
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
70
72
74
76
78
80
82
84
6
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
0 0.2 0.4 0.6 0.8 1
68
70
72
74
76
78
8
82
84
86
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
327
  
Positive Negative 
Word Weight Word Weight 
friendly 0.701 german 0.783 
principles 0.684 arduous 0.531 
hopes 0.630 oppose 0.511 
hoped 0.553 administrations 0.431 
cooperative 0.552 oau9 0.408 
Table 4. New Features Learned from Unlabeled Data 
and Table 4 shows the newly learned features from 
the unlabeled data with the largest weights. 
From Table 3 10  we can see that the weight 
changes of the original features are quite 
reasonable, e.g. the top words in the positive class 
are obviously positive and the proposed approach 
gives them higher weights. The new features also 
seem reasonable given the knowledge that the 
labeled and unlabeled data includes negative news 
about for specific topics (e.g. Germany, Taiwan),. 
We also examine the process of joint training by 
checking the performance on test data and the 
agreement of the two monolingual models on the 
unlabeled parallel data in both settings. The 
average agreement across 5 folds is 85.06% and 
73.87% in settings 1 and 2, respectively, before the 
joint training, and increases to 100% and 99.89%, 
respectively, after 100 iterations of joint training. 
Although the average agreement has already 
increased to 99.50% and 99.02% in settings 1 and 
2, respectively, after 30 iterations, the performance 
on the test set steadily improves in both settings 
until around 50-60 iterations, and then becomes 
relatively stable after that. 
Examination of those sentence pairs in setting 2 
for which the two monolingual models still 
                                                          
9
This is an abbreviation for the Organization of African Unity. 
10The features and weights in Tables 3 and 4 are extracted 
from the English model in the first fold of setting 1. 
disagree after 100 iterations of joint training often 
produces sentences that are not quite parallel, e.g.: 
English: The two sides attach great importance to 
international cooperation on protection and promotion of 
human rights. 
Chinese: ????,????????????????,???
??????????????(Both sides agree that double 
standards on the issue of human rights are to be avoided, and 
are opposed to using pressure on human rights issues in 
international relations.) 
Since the two sentences discuss human rights 
from very different perspectives, it is reasonable 
that the two monolingual models will classify them 
with different polarities (i.e. positive for the 
English sentence and negative for the Chinese 
sentence) even after joint training.  
6 Conclusion 
In this paper, we study bilingual sentiment 
classification and propose a joint model to 
simultaneously learn better monolingual sentiment 
classifiers for each language by exploiting an 
unlabeled parallel corpus together with the labeled 
data available for each language. Our experiments 
show that the proposed approach can significantly 
improve sentiment classification for both 
languages. Moreover, the proposed approach 
continues to produce (albeit smaller) performance 
gains when employing pseudo-parallel data from 
machine translation engines. 
In future work, we would like to apply the joint 
learning idea to other learning frameworks (e.g. 
SVMs), and to extend the proposed model to 
handle word-level parallel information, e.g. 
bilingual dictionaries or word alignment 
information. Another issue is to investigate how to 
improve multilingual sentiment analysis by 
exploiting comparable corpora. 
Acknowledgments 
We thank Shuo Chen, Long Jiang, Thorsten 
Joachims, Lillian Lee, Myle Ott, Yan Song, 
Xiaojun Wan, Ainur Yessenalina, Jingbo Zhu and 
the anonymous reviewers for many useful 
comments and discussion. This work was 
supported in part by National Science Foundation 
Grants BCS-0904822, BCS-0624277, IIS-
0968450; and by a gift from Google. Chenhao Tan 
is supported by NSF (DMS-0808864), ONR (YIP-
N000140910911), and a grant from Microsoft.  
  
 Word 
Weight 
Before After Change 
Positive 
important 0.452 1.659 1.207 
cooperation 0.325 1.492 1.167 
support 0.533 1.483 0.950 
importance 0.450 1.193 0.742 
agreed 0.347 1.061 0.714 
Negative 
difficulties 0.018 0.663 0.645 
not 0.202 0.844 0.641 
never 0.245 0.879 0.634 
germany 0.035 0.664 0.629 
taiwan 0.590 1.216 0.626 
Table 3. Original Features with Largest Weight Change 
328
  
References 
Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier. 
2010. Combining coregularization and consensus-
based self-training for multilingual text 
categorization. In Proceeding of SIGIR?10. 
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 
2010. Multilingual subjectivity: Are more languages 
better? In Proceedings of COLING?10. 
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. In Proceedings of 
EMNLP?08. 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A maximum entropy approach to 
natural language processing. Computational 
Linguistics, 22(1). 
John Blitzer, Ryan McDonald, and Fernando Pereira. 
2006. Domain adaptation with structural correspond-
dence learning. In Proceedings of EMNLP?06. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of COLT?98. 
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic 
sentiment analysis across languages: Multilingual 
supervised Latent Dirichlet Allocation. In 
Proceedings of EMNLP?10. 
Eric Breck, Yejin Choi, and Claire Cardie. 2007. 
Identifying expressions of opinion in context. In 
Proceedings of IJCAI?07.  
David Burkett, Slav Petrov, John Blitzer, and Dan 
Klein. 2010. Learning better monolingual models 
with unannotated bilingual text. In Proceedings of 
CoNLL?10. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic parsing). In 
Proceedings of EMNLP?08. 
Yejin Choi and Claire Cardie. 2008. Learning with 
compositional semantics as structural inference for 
subsentential sentiment analysis. In Proceedings of 
EMNLP?08. 
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 
2009. Exploiting bilingual information to improve 
web search. In Proceedings of ACL/IJCNLP?09. 
Minqing Hu and Bing Liu. 2004. Mining opinion 
features in customer reviews. In Proceedings of 
AAAI?04. 
Ido Dagan, and Alon Itai. 1994. Word sense 
disambiguation using a second language monolingual 
corpus, Computational Linguistics, 20(4): 563-596. 
Thorsten Joachims. 1999a. Making Large-Scale SVM 
Learning Practical. In: Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf, C. Burges, 
and A. Smola (ed.), MIT Press. 
Thorsten Joachims. 1999b. Transductive inference for 
text classification using support vector machines. In 
Proceedings of ICML?99. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by agreement. In Proceedings of 
NAACL?06. 
Dong C. Liu and  Jorge Nocedal. 1989. On the limited 
memory BFGS method for large scale optimization. 
Mathematical Programming, (45): 503?528. 
Robert Malouf. 2002. A comparison of algorithms for 
maximum entropy parameter estimation. In 
Proceedings of CoNLL?02. 
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 
2007. Learning multilingual subjective language via 
cross-lingual projections. In Proceedings of ACL?07. 
Dragos S. Munteanu and Daniel Marcu. 2005. 
Improving machine translation performance by 
exploiting non-parallel corpora. Computational 
Linguistics, 31(4): 477?504. 
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 
2010. Dependency tree-based sentiment classification 
using CRFs with hidden variables. In Proceedings of 
NAACL/HLT ?10. 
Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Machine 
Learning, 39(2): 103?134. 
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis, Foundations and Trends in 
Information Retrieval, Now Publishers. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP?02. 
Peter  Prettenhofer and  Benno Stein. 2010. Cross-
language text classification using structural 
correspondence learning. In Proceedings of ACL?10. 
Adwait Ratnaparkhi. 1997. A simple introduction to 
maximum entropy models for natural language 
processing. Technical Report 97-08, University of 
Pennsylvania. 
329
  
Julia M. Schulz, Christa Womser-Hacker, and Thomas 
Mandl. 2010. Multilingual corpus development for 
opinion mining. In Proceedings of LREC?10. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, and Noriko Kando. 2008. Overview 
of multilingual opinion analysis task at NTCIR-7. In 
Proceedings of the NTCIR-7 Workshop.  
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 
2007. Overview of opinion analysis pilot task at 
NTCIR-6. In Proceedings of the NTCIR-6 Workshop. 
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 
2005. A co-regularization approach to semi-
supervised learning with multiple views. In 
Proceedings of ICML?05. 
Noah A. Smith. 2006. Novel estimation methods for 
unsupervised discovery of latent structure in natural 
language text. Ph.D. thesis, Department of Computer 
Science, Johns Hopkins University. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel 
Jurafsky and Christopher Manning. 2005. A 
conditional random field word segmenter. In 
Proeedings of the 4th SIGHAN Workshop. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proceedings of ACL?02. 
Xiaojun Wan. 2008. Using Bilingual Knowledge and 
Ensemble Techniques for Unsupervised Chinese 
Sentiment Analysis. In Proceedings of  EMNLP?08. 
Xiaojun Wan. 2009. Co-training for cross-lingual 
sentiment classification. In Proceedings of 
ACL/AFNLP?09. 
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 
2005. Annotating expressions of opinions and 
emotions in language. Language Resources and 
Evaluation, 39(2- 3): 165-210. 
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. 
Cross-lingual latent topic extraction, In Proceedings 
of ACL?10. 
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 
2009. Cross language dependency parsing using a 
bilingual lexicon. In Proceedings of 
ACL/IJCNLP?09.  
Xiaojin Zhu and Andrew B. Goldberg. 2009. 
Introduction to Semi-Supervised Learning. Morgan 
& Claypool Publishers. 
Appendix A. Equation Deduction 
In this appendix, we derive the gradient for the objective 
function in Equation 3, which is used in parameter 
estimation. As mentioned in Section 3.3, the parameters 
can be learned by finding: 
   
    
         
      
                 
       
      
                    
       
      
                           
         
     
     
     
         
 
        
Since the first term on the right-hand side is just the 
expression for the standard MaxEnt problem, we will 
focus on the gradient for the second term, and denote 
       
     
     
     
          as ( ). 
Let         denote    or   , and   
  be the  th weight 
in the vector   . For brevity, we drop the   in the above 
notation, and write   
  to denote   
  . Then the partial 
derivative of (*) based on Equation 4 with respect to   
  
is as follows: 
    
   
  
     
    
     
 
   
     
    
       
 
     
    
         
    
       
 
                              (1) 
Further, we obtain: 
 
   
     
    
      
 
   
 
             
   
   
              
   
   
  
 
                       
 
             
   
   
              
   
   
  
 
  
    
    
     
 
             
   
   
                      
   
   
  
  
               
    
     
    
    
       
     
    
        
    
    
        
    
       
    
    
       (2) 
Merge (2) into (1), we get: 
    
   
  
 
     
    
         
    
       
 
      
    
         
    
          
   
    
    
        
    
       
    
    
         
      
    
         
    
       
    
    
        
     
    
       
    
    
             
    
    
    
       
    
         
    
          
    
           
  
330
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 175?185,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
The effect of wording on message propagation:
Topic- and author-controlled natural experiments on Twitter
Chenhao Tan
Dept. of Computer Science
Cornell University
chenhao@cs.cornell.edu
Lillian Lee
Dept. of Computer Science
Cornell University
llee@cs.cornell.edu
Bo Pang
Google Inc.
bopang42@gmail.com
Abstract
Consider a person trying to spread an
important message on a social network.
He/she can spend hours trying to craft the
message. Does it actually matter? While
there has been extensive prior work look-
ing into predicting popularity of social-
media content, the effect of wording per
se has rarely been studied since it is of-
ten confounded with the popularity of the
author and the topic. To control for these
confounding factors, we take advantage
of the surprising fact that there are many
pairs of tweets containing the same url and
written by the same user but employing
different wording. Given such pairs, we
ask: which version attracts more retweets?
This turns out to be a more difficult task
than predicting popular topics. Still, hu-
mans can answer this question better than
chance (but far from perfectly), and the
computational methods we develop can do
better than both an average human and a
strong competing method trained on non-
controlled data.
1 Introduction
How does one make a message ?successful?? This
question is of interest to many entities, including
political parties trying to frame an issue (Chong
and Druckman, 2007), and individuals attempting
to make a point in a group meeting. In the first
case, an important type of success is achieved if
the national conversation adopts the rhetoric of the
party; in the latter case, if other group members
repeat the originating individual?s point.
The massive availability of online messages,
such as posts to social media, now affords re-
searchers new means to investigate at a very large
scale the factors affecting message propagation,
also known as adoption, sharing, spread, or vi-
rality. According to prior research, important fea-
tures include characteristics of the originating au-
thor (e.g., verified Twitter user or not, author?s
messages? past success rate), the author?s social
network (e.g., number of followers), message tim-
ing, and message content or topic (Artzi et al,
2012; Bakshy et al, 2011; Borghol et al, 2012;
Guerini et al, 2011; Guerini et al, 2012; Hansen
et al, 2011; Hong et al, 2011; Lakkaraju et al,
2013; Milkman and Berger, 2012; Ma et al, 2012;
Petrovi?c et al, 2011; Romero et al, 2013; Suh et
al., 2010; Sun et al, 2013; Tsur and Rappoport,
2012). Indeed, it?s not surprising that one of the
most retweeted tweets of all time was from user
BarackObama, with 40M followers, on November
6, 2012: ?Four more years. [link to photo]?.
Our interest in this paper is the effect of alterna-
tive message wording, meaning how the message
is said, rather than what the message is about. In
contrast to the identity/social/timing/topic features
mentioned above, wording is one of the few fac-
tors directly under an author?s control when he or
she seeks to convey a fixed piece of content. For
example, consider a speaker at the ACL business
meeting who has been tasked with proposing that
Paris be the next ACL location. This person can-
not on the spot become ACL president, change the
shape of his/her social network, wait until the next
morning to speak, or campaign for Rome instead;
but he/she can craft the message to be more hu-
morous, more informative, emphasize certain as-
pects instead of others, and so on. In other words,
we investigate whether a different choice of words
affects message propagation, controlling for user
and topic: would user BarackObama have gotten
significantly more (or fewer) retweets if he had
used some alternate wording to announce his re-
election?
Although we cannot create a parallel universe
175
Table 1: Topic- and author-controlled (TAC) pairs. Topic control = inclusion of the same URL.
author tweets #retweets
natlsecuritycnn t
1
: FIRST ON CNN: After Petraeus scandal, Paula Broadwell looks to recapture ?normal life.? http://t.co/qy7GGuYW n
1
= 5
t
2
: First on CNN: Broadwell photos shared with Security Clearance as she and her family fight media portrayal of her [same URL] n
2
= 29
ABC t
1
: Workers, families take stand against Thanksgiving hours: http://t.co/J9mQHiIEqv n
1
= 46
t
2
: Staples, Medieval Times Workers Say Opening Thanksgiving Day Crosses the Line [same URL] n
2
= 27
cactus music t
1
: I know at some point you?ve have been saved from hunger by our rolling food trucks friends. Let?s help support them!
http://t.co/zg9jwA5j
n
1
= 2
t
2
: Food trucks are the epitome of small independently owned LOCAL businesses! Help keep them going! Sign the petition [same
URL]
n
2
= 13
in which BarackObama tweeted something else
1
,
fortunately, a surprising characteristic of Twitter
allows us to run a fairly analogous natural exper-
iment: external forces serendipitously provide an
environment that resembles the desired controlled
setting (DiNardo, 2008). Specifically, it turns out
to be unexpectedly common for the same user to
post different tweets regarding the same URL ?
a good proxy for fine-grained topic
2
? within a
relatively short period of time.
3
Some example
pairs are shown in Table 1; we see that the paired
tweets may differ dramatically, going far beyond
word-for-word substitutions, so that quite interest-
ing changes can be studied.
Looking at these examples, can one in fact tell
from the wording which tweet in a topic- and
author-controlled pair will be more successful?
The answer may not be a priori clear. For example,
for the first pair in the table, one person we asked
found t
1
?s invocation of a ?scandal? to be more
attention-grabbing; but another person preferred
t
2
because it is more informative about the URL?s
content and includes ?fight media portrayal?. In
an Amazon Mechanical Turk (AMT) experiment
(?4), we found that humans achieved an average
accuracy of 61.3%: not that high, but better than
chance, indicating that it is somewhat possible for
humans to predict greater message spread from
different deliveries of the same information.
Buoyed by the evidence of our AMT study that
wording effects exist, we then performed a battery
of experiments to seek generally-applicable, non-
1
Cf. the Music Lab ?multiple universes? experiment to
test the randomness of popularity (Salganik et al, 2006).
2
Although hashtags have been used as coarse-grained
topic labels in prior work, for our purposes, we have no assur-
ance that two tweets both using, say, ?#Tahrir? would be at-
tempting to express the same message but in different words.
In contrast, see the same-URL examples in Table 1.
3
Moreover, Twitter presents tweets to a reader in strict
chronological order, so that there are no algorithmic-ranking
effects to compensate for in determining whether readers saw
a tweet. And, Twitter accumulates retweet counts for the en-
tire retweet cascade and displays them for the original tweet
at the root of the propagation tree, so we can directly use
Twitter?s retweet counts to compare the entire reach of the
different versions.
Twitter-specific features of more successful phras-
ings. ?5.1 applies hypothesis testing (with Bonfer-
roni correction to ameliorate issues with multiple
comparisons) to investigate the utility of features
like informativeness, resemblance to headlines,
and conformity to the community norm in lan-
guage use. ?5.2 further validates our findings via
prediction experiments, including on completely
fresh held-out data, used only once and after an
array of standard cross-validation experiments.
4
We achieved 66.5% cross-validation accuracy and
65.6% held-out accuracy with a combination of
our custom features and bag-of-words. Our clas-
sifier fared significantly better than a number of
baselines, including a strong classifier trained on
the most- and least-retweeted tweets that was even
granted access to author and timing metadata.
2 Related work
The idea of using carefully controlled experiments
to study effective communication strategies dates
back at least to Hovland et al (1953). Recent
studies range from examining what characteris-
tics of New York Times articles correlate with high
re-sharing rates (Milkman and Berger, 2012) to
looking at how differences in description affect
the spread of content-controlled videos or images
(Borghol et al, 2012; Lakkaraju et al, 2013).
Simmons et al (2011) examined the variation of
quotes from different sources to examine how tex-
tual memes mutate as people pass them along, but
did not control for author. Predicting the ?success?
of various texts such as novels and movie quotes
has been the aim of additional prior work not al-
ready mentioned in ?1 (Ashok et al, 2013; Louis
and Nenkova, 2013; Danescu-Niculescu-Mizil et
al., 2012; Pitler and Nenkova, 2008; McIntyre and
Lapata, 2009). To our knowledge, there have been
no large-scale studies exploring wording effects in
a both topic- and author-controlled setting. Em-
ploying such controls, we find that predicting the
more effective alternative wording is much harder
than the previously well-studied problem of pre-
4
And after crossing our fingers.
176
dicting popular content when author or topic can
freely vary.
Related work regarding the features we consid-
ered is deferred to ?5.1 (features description).
3 Data
Our main dataset was constructed by first gath-
ering 1.77M topic- and author-controlled (hence-
forth TAC) tweet pairs
5
differing in more than just
spacing.
6
We accomplished this by crawling time-
lines of 236K user ids that appear in prior work
(Kwak et al, 2010; Yang and Leskovec, 2011)
via the Twitter API. This crawling process also
yielded 632K TAC pairs whose only difference
was spacing, and an additional 558M ?unpaired?
tweets; as shown later in this paper, we used these
extra corpora for computing language models and
other auxiliary information. We applied non-
obvious but important filtering ? described later
in this section ? to control for other external fac-
tors and to reduce ambiguous cases. This brought
us to a set of 11,404 pairs, with the gold-standard
labels determined by which tweet in each pair was
the one that received more retweets according to
the Twitter API. We then did a second crawl to
get an additional 1,770 pairs to serve as a held-out
dataset. The corresponding tweet IDs are available
online at http://chenhaot.com/pages/
wording-for-propagation.html. (Twit-
ter?s terms of service prohibit sharing the actual
tweets.)
Throughout, we refer to the textual content of
the earlier tweet within a TAC pair as t
1
, and of the
later one as t
2
. We denote the number of retweets
received by each tweet by n
1
and n
2
, respectively.
We refer to the tweet with higher (lower) n
i
as the
?better (worse)? tweet.
Using ?identical? pairs to determine how to
compensate for follower-count and timing ef-
fects. In an ideal setting, differences between
n
1
and n
2
would be determined solely by dif-
ferences in wording. But even with a TAC pair,
retweets might exhibit a temporal bias because of
the chronological order of tweet presentation (t
1
might enjoy a first-mover advantage (Borghol et
al., 2012) because it is the ?original?; alternatively,
5
No data collection/processing was conducted at Google.
6
The total excludes: tweets containing multiple URLs;
tweets from users posting about the same URL more than five
times (since such users might be spammers); the third, fourth,
or fifth version for users posting between three and five tweets
for the same URL; retweets (as identified by Twitter?s API or
by beginning with ?RT @?); non-English tweets.
3 6 12 18 24 36 48time lag (hours)2468
10121416D >1K f?ers>2.5K f?ers>5K f?ers>10K f?ers
(a) For identical TAC pairs,
retweet-count deviation vs.
time lag between t
1
and
t
2
, for the author follower-
counts given in the legend.
0 2 4 6 8n102
468
10
E?(n 2|n 1)
>5K f?ers,<12hrsotherwise
(b) Avg. n
2
vs. n
1
for iden-
tical TAC pairs, highlighting
our chosen time-lag and fol-
lower thresholds. Bars: stan-
dard error. Diagonal line:
p
Epn
2
|n
1
q ? n
1
.
Figure 1: (a): The ideal case where n
2
? n
1
when t
1
? t
2
is best approximated when t
2
oc-
curs within 12 hours of t
1
and the author has at
least 10,000 or 5,000 followers. (b): in our chosen
setting (blue circles), n
2
indeed tends to track n
1
,
whereas otherwise (black squares), there?s a bias
towards retweeting t
1
.
t
2
might be preferred because retweeters consider
t
1
to be ?stale?). Also, the number of followers an
author has can have complicated indirect effects
on which tweets are read (space limits preclude
discussion).
We use the 632K TAC pairs wherein t
1
and
t
2
are identical
7
to check for such confounding
effects: we see how much n
2
deviates from n
1
in such settings, since if wording were the only
explanatory factor, the retweet rates for identical
tweets ought to be equal. Figure 1(a) plots how
the time lag between t
1
and t
2
and the author?s
follower-count affect the following deviation esti-
mate:
D ?
?
0?n
1
?10
| pEpn
2
|n
1
q ? n
1
|,
where
p
Epn
2
|n
1
q is the average value of n
2
over
pairs whose t
1
is retweeted n
1
times. (Note that
the number of pairs whose t
1
is retweeted n
1
times
decays exponentially with n
1
; hence, we condi-
tion on n
1
to keep the estimate from being domi-
nated by pairs with n
1
? 0, and do not consider
n
1
? 10 because there are too few such pairs to es-
timate
p
Epn
2
|n
1
q reliably.) Figure 1(a) shows that
the setting where we (i) minimize the confound-
ing effects of time lag and author?s follower-count
and (ii) maximize the amount of data to work with
7
Identical up to spacing: Twitter prevents exact copies by
the same author appearing within a short amount of time, but
some authors work around this by inserting spaces.
177
is: when t
2
occurs within 12 hours after t
1
and
the author has more than 5,000 followers. Figure
1(b) confirms that for identical TAC pairs, our cho-
sen setting indeed results in n
2
being on average
close to n
1
, which corresponds to the desired set-
ting where wording is the dominant differentiating
factor.
8
Focus on meaningful and general changes.
Even after follower-count and time-lapse filtering,
we still want to focus on TAC pairs that (i) ex-
hibit significant/interesting textual changes (as ex-
emplified in Table 1, and as opposed to typo cor-
rections and the like), and (ii) have n
2
and n
1
suf-
ficiently different so that we are confident in which
t
i
is better at attracting retweets. To take care of
(i), we discarded the 50% of pairs whose similar-
ity was above the median, where similarity was
tf-based cosine.
9
For (ii), we sorted the remain-
ing pairs by n
2
? n
1
and retained only the top and
bottom 5%.
10
Moreover, to ensure that we do not
overfit to the idiosyncrasies of particular authors,
we cap the number of pairs contributed by each
author to 50 before we deal with (ii).
4 Human accuracy on TAC pairs
We first ran a pilot study on Amazon Mechan-
ical Turk (AMT) to determine whether humans
can identify, based on wording differences alone,
which of two topic- and author- controlled tweets
is spread more widely. Each of our 5 AMT tasks
involved a disjoint set of 20 randomly-sampled
TAC pairs (with t
1
and t
2
randomly reordered);
subjects indicated ?which tweet would other peo-
ple be more likely to retweet??, provided a short
justification for their binary response, and clicked
a checkbox if they found that their choice was a
?close call?. We received 39 judgments per pair in
aggregate from 106 subjects total (9 people com-
pleted all 5 tasks). The subjects? justifications
were of very high quality, convincing us that they
all did the task in good faith
11
. Two examples for
8
We also computed the Pearson correlation between n
1
and n
2
, even though it can be dominated by pairs with smaller
n
1
. The correlation is 0.853 for ?? 5K f?ers, ?12hrs?,
clearly higher than the 0.305 correlation for ?otherwise?.
9
Idf weighting was not employed because changes to fre-
quent words are of potential interest. Urls, hashtags, @-
mentions and numbers were normalized to [url], [hashtag],
[at], and [num] before computing similarity.
10
For our data, this meant n
2
? n
1
? 10 or ? ?15. Cf.
our median number of retweets: 30.
11
We also note that the feedback we got was quite pos-
itive, including: ?...It?s fun to make choices between close
tweets and use our subjective opinion. Thanks and best of
the third TAC pair in Table 1 were: ?[t
1
makes] the
cause relate-able to some people, therefore show-
ing more of an appeal as to why should they click
the link and support? and, expressing the opposite
view, ?I like [t
2
] more because [t
1
] starts out with
a generalization that doesn?t affect me and try to
make me look like I had that experience before?.
If we view the set of 3900 binary judgments
for our 100-TAC-pair sample as constituting in-
dependent responses, then the accuracy for this
set is 62.4% (rising to 63.8% if we exclude the
587 judgments deemed ?close calls?). However, if
we evaluate the accuracy of the majority response
among the 39 judgments per pair, the number rises
to 73%. The accuracy of the majority response
generally increases with the dominance of the ma-
jority, going above 90% when at least 80% of the
judgments agree (although less than a third of the
pairs satisfied this criterion).
Alternatively, we can consider the average ac-
curacy of the 106 subjects: 61.3%, which is bet-
ter than chance but far from 100%. (Variance was
high: one subject achieved 85% accuracy out of
20 pairs, but eight scored below 50%.) This re-
sult is noticeably lower than the 73.8%-81.2% re-
ported by Petrovi?c et al (2011), who ran a sim-
ilar experiment involving two subjects and 202
tweet pairs, but where the pairs were not topic- or
author-controlled.
12
We conclude that even though propagation pre-
diction becomes more challenging when topic
and author controls are applied, humans can
still to some degree tell which wording attracts
more retweets. Interested readers can try this
out themselves at http://chenhaot.com/
retweetedmore/quiz.
5 Experiments
We now investigate computationally what word-
ing features correspond to messages achieving a
broader reach. We start (?5.1) by introducing a set
of generally-applicable and (mostly) non-Twitter-
specific features to capture our intuitions about
what might be better ways to phrase a message.
We then use hypothesis testing (?5.1) to evaluate
the importance of each feature for message prop-
luck with your research? and ?This was very interesting and
really made me think about how I word my own tweets. Great
job on this survey!?. We only had to exclude one person (not
counted among the 106 subjects), doing so because he or she
gave the same uninformative justification for all pairs.
12
The accuracy range stems from whether author?s social
features were supplied and which subject was considered.
178
Table 2: Notational conventions for tables in ?5.1.
One-sided paired t-test for feature efficacy
????: p?1e-20 ????: p?1-1e-20
??? : p?0.001 ??? : p?0.999
?? : p?0.01 ?? : p?0.99
? : p?0.05 ? : p?0.95
?: passes our Bonferroni correction
One-sided binomial test for feature increase
(Do authors prefer to ?raise? the feature in t
2
?)
YES : t
2
has a higher feature score than t
1
, ? ? .05
NO : t
2
has a lower feature score than t
1
, ? ? .05
(x%): %pf
2
? f
1
q, if sig. larger or smaller than 50%
agation and the extent to which authors employ
it, followed by experiments on a prediction task
(?5.2) to further examine the utility of these fea-
tures.
5.1 Features: efficacy and author preference
What kind of phrasing helps message propaga-
tion? Does it work to explicitly ask people to share
the message? Is it better to be short and concise or
long and informative? We define an array of fea-
tures to capture these and other messaging aspects.
We then examine (i) how effective each feature is
for attracting more retweets; and (ii) whether au-
thors prefer applying a given feature when issuing
a second version of a tweet.
First, for each feature, we use a one-sided paired
t-test to test whether, on our 11K TAC pairs, our
score function for that feature is larger in the bet-
ter tweet versions than in the worse tweet versions,
for significance levels ? ? .05, .01, .001, 1e-20.
Given that we did 39 tests in total, there is a risk
of obtaining false positives due to multiple test-
ing (Dunn, 1961; Benjamini and Hochberg, 1995).
To account for this, we also report significance re-
sults for the conservatively Bonferroni-corrected
(?BC?) significance level ? = 0.05/39=1.28e-3.
Second, we examine author preference for ap-
plying a feature. We do so because one (but by no
means the only) reason authors post t
2
after having
already advertised the same URL in t
1
is that these
authors were dissatisfied with the amount of atten-
tion t
1
got; in such cases, the changes may have
been specifically intended to attract more retweets.
We measure author preference for a feature by the
percentage of our TAC pairs
13
where t
2
has more
?occurrences? of the feature than t
1
, which we de-
note by ?%pf
2
? f
1
q?. We use the one-sided bi-
nomial test to see whether %pf
2
? f
1
q is signifi-
cantly larger (or smaller) than 50%.
13
For our preference experiments, we added in pairs where
n
2
? n
1
was not in the top or bottom 5% (cf. ?3, meaningful
changes), since to measure author preference it?s not neces-
sary that the retweet counts differ significantly.
Table 3: Explicit requests for sharing (where only
occurrences POS-tagged as verbs count, according
to the Gimpel et al (2011) tagger).
effective? author-preferred?
rt ???? * ??
retweet ???? * YES (59%)
spread ??? ? * YES (56%)
please ??? ? * ??
pls ? ??? ??
plz ?? ?? ??
Table 4: Informativeness.
effective? author-preferred?
length (chars) ???? * YES (54%)
verb ???? * YES (56%)
noun ???? * ??
adjective ??? ? * YES (51%)
adverb ??? ? * YES (55%)
proper noun ??? ? * NO? (45%)
number ???? * NO? (48%)
hashtag ? ??? ??
@-mention ??? ? * YES (53%)
Not surprisingly, it helps to ask people to share.
(See Table 3; the notation for all tables is ex-
plained in Table 2.) The basic sanity check we
performed here was to take as features the number
of occurrences of the verbs ?rt?, ?retweet?, ?please?,
?spread?, ?pls?, and ?plz? to capture explicit re-
quests (e.g. ?please retweet?).
Informativeness helps. (Table 4) Messages that
are more informative have increased social ex-
change value (Homans, 1958), and so may be
more worth propagating. One crude approxima-
tion of informativeness is length, and we see that
length helps.
14
In contrast, Simmons et al (2011)
found that shorter versions of memes are more
likely to be popular. The difference may result
from TAC-pair changes being more drastic than
the variations that memes undergo.
A more refined informativeness measure is
counts of the parts of speech that correspond
to content. Our POS results, gathered using a
Twitter-specific tagger (Gimpel et al, 2011), echo
those of Ashok et al (2013) who looked at predict-
14
Of course, simply inserting garbage isn?t going to lead
to more retweets, but adding more information generally in-
volves longer text.
179
Table 5: Conformity to the community and one?s
own past, measured via scores assigned by various
language models.
effective? author-preferred?
twitter unigram ??? ? * YES (54%)
twitter bigram ??? ? * YES (52%)
personal unigram ??? ? * YES (52%)
personal bigram ??? NO? (48%)
ing the success of books. The diminished effect of
hashtag inclusion with respect to what has been re-
ported previously (Suh et al, 2010; Petrovi?c et al,
2011) presumably stems from our topic and author
controls.
Be like the community, and be true to yourself
(in the words you pick, but not necessarily in
how you combine them). (Table 5) Although dis-
tinctive messages may attract attention, messages
that conform to expectations might be more eas-
ily accepted and therefore shared. Prior work has
explored this tension: Lakkaraju et al (2013), in a
content-controlled study, found that the more up-
voted Reddit image titles balance novelty and fa-
miliarity; Danescu-Niculescu-Mizil et al (2012)
(henceforth DCKL?12) showed that the memora-
bility of movie quotes corresponds to higher lexi-
cal distinctiveness but lower POS distinctiveness;
and Sun et al (2013) observed that deviating from
one?s own past language patterns correlates with
more retweets.
Keeping in mind that the authors in our data
have at least 5000 followers
15
, we consider two
types of language-conformity constraints an au-
thor might try to satisfy: to be similar to what
is normal in the Twitter community, and to be
similar to what his or her followers expect. We
measure a tweet?s similarity to expectations by its
score according to the relevant language model,
1
|T |
?
xPT logpppxqq, where T refers to either all
the unigrams (unigram model) or all and only bi-
grams (bigram model).
16
We trained a Twitter-
community language model from our 558M un-
paired tweets, and personal language models from
each author?s tweet history.
Imitate headlines. (Table 6) News headlines are
often intentionally written to be both informative
and attention-getting, so we introduce the idea of
15
This is not an artificial restriction on our set of authors; a
large follower count means (in principle) that our results draw
on a large sample of decisions whether to retweet or not.
16
The tokens [at], [hashtag], [url] were ignored in the
unigram-model case to prevent their undue influence, but re-
tained in the bigram model to capture longer-range usage
(?combination?) patterns.
Table 6: LM-based resemblance to headlines.
effective? author-preferred?
headline unigram ?? ?? YES (53%)
headline bigram ???? * YES (52%)
Table 7: Retweet score.
effective? author-preferred?
rt score ?? ?? * NO? (49%)
verb rt score ???? * ??
noun rt score ??? ? * ??
adjective rt score ? ??? YES (50%)
adverb rt score ? ??? YES (51%)
proper noun rt score ??? NO? (48%)
scoring by a language model built from New York
Times headlines.
17
Use words associated with (non-paired)
retweeted tweets. (Table 7) We expect that
provocative or sensationalistic tweets are likely
to make people react. We found it difficult to
model provocativeness directly. As a rough
approximation, we check whether the changes in
t
2
with respect to t
1
(which share the same topic
and author) involve words or parts-of-speech that
are associated with high retweet rate in a very
large separate sample of unpaired tweets (retweets
and replies discarded). Specifically, for each word
w that appears more than 10 times, we compute
the probability that tweets containing w are
retweeted more than once, denoted by rspwq. We
define the rt score of a tweet as max
wPT rspwq,
where T is all the words in the tweet, and the
rt score of a particular POS tag z in a tweet as
max
wPT&tagpwq?zrspwq.
Include positive and/or negative words. (Ta-
ble 8) Prior work has found that including posi-
tive or negative sentiment increases message prop-
agation (Milkman and Berger, 2012; Godes et al,
2005; Heath et al, 2001; Hansen et al, 2011). We
measured the occurrence of positive and negative
words as determined by the connotation lexicon
of Feng et al (2013) (better coverage than LIWC).
Measuring the occurrence of both simultaneously
was inspired by Riloff et al (2013).
Refer to other people (but not your audience).
(Table 9) First-person has been found useful for
success before, but in the different domains of sci-
entific abstracts (Guerini et al, 2012) and books
(Ashok et al, 2013).
17
To test whether the results stem from similarity to news
rather than headlines per se, we constructed a NYT-text LM,
which proved less effective. We also tried using Gawker
headlines (often said to be attention-getting) but pilot studies
revealed insufficient vocabulary overlap with our TAC pairs.
180
Table 8: Sentiment (contrast is measured by pres-
ence of both positive and negative sentiments).
effective? author-preferred?
positive ??? ? * ??
negative ??? ? * ??
contrast ??? ? * ??
Table 9: Pronouns.
effective? author-preferred?
1st person singular ??? YES (51%)
1st person plural ??? YES (52%)
2nd person ??? YES (57%)
3rd person singular ?? ?? YES (55%)
3rd person plural ? ??? YES (58%)
Generality helps. (Table 10) DCKL?12 posited
that movie quotes are more shared in the culture
when they are general enough to be used in multi-
ple contexts. We hence measured the presence of
indefinite articles vs. definite articles.
The easier to read, the better. (Table 11) We
measure readability by using Flesch reading ease
(Flesch, 1948) and Flesch-Kincaid grade level
(Kincaid et al, 1975), though they are not de-
signed for short texts. We use negative grade level
so that a larger value indicates easier texts to read.
Final question: Do authors prefer to do what
is effective? Recall that we use binomial tests to
determine author preference for applying a feature
more in t
2
. Our preference statistics show that au-
thor preferences in many cases are aligned with
feature efficacy. But there are several notable ex-
ceptions: for example, authors tend to increase the
use of @-mentions and 2nd person pronouns even
though they are ineffective. On the other hand,
they did not increase the use of effective ones
like proper nouns and numbers; nor did they tend
to increase their rate of sentiment-bearing words.
Bearing in mind that changes in t
2
may not always
be intended as an effort to improve t
1
, it is still in-
teresting to observe that there are some contrasts
between feature efficacy and author preferences.
5.2 Predicting the ?better? wording
Here, we further examine the collective efficacy
of the features introduced in ?5.1 via their perfor-
mance on a binary prediction task: given a TAC
pair (t
1
, t
2
), did t
2
receive more retweets?
Our approach. We group the features introduced
in ?5.1 into 16 lexicon-based features (Table 3,
8, 9, 10), 9 informativeness features (Table 4), 6
language model features (Table 5, 6), 6 rt score
features (Table 7), and 2 readability features (Ta-
ble 11). We refer to all 39 of them together as
Table 10: Generality.
effective? author-preferred?
indefinite articles (a,an) ??? ? * ??
definite articles (the) ??? YES (52%)
Table 11: Readability.
effective? author-preferred?
reading ease ?? ?? YES (52%)
negative grade level ? ??? YES (52%)
custom features. We also consider tagged bag-of-
words (?BOW?) features, which includes all the
unigram (word:POS pair) and bigram features that
appear more than 10 times in the cross-validation
data. This yields 3,568 unigram features and 4,095
bigram features, for a total of 7,663 so-called
1,2-gram features. Values for each feature are nor-
malized by linear transformation across all tweets
in the training data to lie in the range r0, 1s.18
For a given TAC pair, we construct its feature
vector as follows. For each feature being consid-
ered, we compute its normalized value for each
tweet in the pair and take the difference as the fea-
ture value for this pair. We use L2-regularized lo-
gistic regression as our classifier, with parameters
chosen by cross validation on the training data.
(We also experimented with SVMs. The perfor-
mance was very close, but mostly slightly lower.)
A strong non-TAC alternative, with social infor-
mation and timing thrown in. One baseline re-
sult we would like to establish is whether the topic
and author controls we have argued for, while
intuitively compelling for the purposes of trying
to determine the best way for a given author to
present some fixed content, are really necessary
in practice. To test this, we consider an alterna-
tive binary L2-regularized logistic-regression clas-
sifier that is trained on unpaired data, specifically,
on the collection of 10,000 most retweeted tweets
(gold-standard label: positive) plus the 10,000
least retweeted tweets (gold-standard label: neg-
ative) that are neither retweets nor replies. Note
that this alternative thus is granted, by design,
roughly twice the training instances that our clas-
sifiers have, as a result of having roughly the same
number of tweets, since our instances are pairs.
Moreover, we additionally include the tweet au-
thor?s follower count, and the day and hour of
posting, as features. We refer to this alternative
classifier as  TAC+ff+time. (Mnemonic: ?ff? is
used in bibliographic contexts as an abbreviation
18
We also tried normalization by whitening, but it did not
lead to further improvements.
181
(a) Cross-validation and heldout accuracy for various feature sets. Blue lines inside
bars: performance when custom features are restricted to those that pass our Bon-
ferroni correction (no line for readability because no readability features passed).
Dashed vertical line:  TAC+ff+time performance.
1000 3000 5000 7000 900058%60%
62%64%66%
68%70% custom+1,2-gramcustom1,2-gramhuman
(b) Cross-validation accuracy vs data size.
Human performance was estimated from a
disjoint set of 100 pairs (see ?4).
Figure 2: Accuracy results. Pertinent significance results are as follows. In cross-validation, custom+1,2-
gram is significantly better than  TAC+ff+time (p=0) and 1,2-gram (p=3.8e-7). In heldout validation,
custom+1,2-gram is significantly better than  TAC+ff+time (p=3.4e-12) and 1,2-gram (p=0.01) but not
unigram (p=0.08), perhaps due to the small size of the heldout set.
for ?and the following?.) We apply it to a tweet
pair by computing whether it gives a higher score
to t
2
or not.
Baselines. To sanity-check whether our classifier
provides any improvement over the simplest meth-
ods one could try, we also report the performance
of the majority baseline, our request-for-sharing
features, and our character-length feature.
Performance comparison. We compare the ac-
curacy (percentage of pairs whose labels were cor-
rectly predicted) of our approach against the com-
peting methods. We report 5-fold cross validation
results on our balanced set of 11,404 TAC pairs
and on our completely disjoint heldout data
19
of
1,770 TAC pairs; this set was never examined dur-
ing development, and there are no authors in com-
mon between the two testing sets.
Figure 2(a) summarizes the main results. While
 TAC+ff+time outperforms the majority base-
line, using all the features we proposed beats
 TAC+ff+time by more than 10% in both cross-
validation (66.5% vs 55.9%) and heldout valida-
tion (65.6% vs 55.3%). We outperform the aver-
age human accuracy of 61% reported in our Ama-
zon Mechanical Turk experiments (for a different
data sample);  TAC+ff+time fails to do so.
The importance of topic and author con-
trol can be seen by further investigation of
 TAC+ff+time?s performance. First, note that
19
To construct this data, we used the same criteria as in
?3: written by authors with more than 5000 followers, posted
within 12 hours, n
2
? n
1
? 10 or ? ?15, and cosine simi-
larity threshold value the same as in ?3, cap of 50 on number
of pairs from any individual author.
it yields an accuracy of around 55% on our
alternate-version-selection task,
20
even though its
cross-validation accuracy on the larger most- and
least-retweeted unpaired tweets averages out to a
high 98.8%. Furthermore, note the superior per-
formance of unigrams trained on TAC data vs
 TAC+ff+time ? which is similar to our uni-
grams but trained on a larger but non-TAC dataset
that included metadata. Thus, TAC pairs are a use-
ful data source even for non-custom features. (We
also include individual feature comparisons later.)
Informativeness is the best-performing custom
feature group when run in isolation, and outper-
forms all baselines, as well as  TAC+ff+time;
and we can see from Figure 2(a) that this is not
due just to length. The combination of all our 39
custom features yields approximately 63% accu-
racy in both testing settings, significantly outper-
forming informativeness alone (p?0.001 in both
cases). Again, this is higher than our estimate of
average human performance.
Not surprisingly, the TAC-trained BOW fea-
tures (unigram and 1,2-gram) show impressive
predictive power in this task: many of our custom
features can be captured by bag-of-word features,
in a way. Still, the best performance is achieved
20
One might suspect that the problem is that
 TAC+ff+time learns from its training data to over-
rely on follower-count, since that is presumably a good
feature for non-TAC tweets, and for this reason suffers when
run on TAC data where follower-counts are by construction
non-informative. But in fact, we found that removing the
follower-count feature from  TAC+ff+time and re-training
did not lead to improved performance. Hence, it seems that
it is the non-controlled nature of the alternate training data
that explains the drop in performance.
182
by combining our custom and 1,2-gram features
together, to a degree statistically significantly bet-
ter than using 1,2-gram features alone.
Finally, we remark on our Bonferroni correc-
tion. Recall that the intent of applying it is to
avoid false positives. However, in our case, Fig-
ure 2(a) shows that our potentially ?false? posi-
tives ? features whose effectiveness did not pass
the Bonferroni correction test ? actually do raise
performance in our prediction tests.
Size of training data. Another interesting obser-
vation is how performance varies with data size.
For n ? 1000, 2000, . . . , 10000, we randomly
sampled n pairs from our 11,404 pairs, and com-
puted the average cross-validation accuracy on the
sampled data. Figure 2(b) shows the averages over
50 runs of the aforementioned procedure. Our cus-
tom features can achieve good performance with
little data, in the sense that for sample size 1000,
they outperform BOW features; on the other hand,
BOW features quickly surpass them. Across the
board, the custom+1,2-gram features are consis-
tently better than the 1,2-gram features alone.
Top features. Finally, we examine some of
the top-weighted individual features from our ap-
proach and from the competing  TAC+ff+time
classifier. The top three rows of Table 12 show the
best custom and best and worst unigram features
for our method; the bottom two rows show the best
and worst unigrams for  TAC+ff+time. Among
custom features, we see that community and per-
sonal language models, informativeness, retweet
scores, sentiment, and generality are represented.
As for unigram features, not surprisingly, ?rt? and
?retweet? are top features for both our approach
and  TAC+ff+time. However, the other unigrams
for the two methods seem to be a bit different in
spirit. Some of the unigrams determined to be
most poor only by our method appear to be both
surprising and yet plausible in retrospect: ?icymi?
(abbreviation for ?in case you missed it?) tends to
indicate a direct repetition of older information,
so people might prefer to retweet the earlier ver-
sion; ?thanks? and ?sorry? could correspond to
personal thank-yous and apologies not meant to
be shared with a broader audience, and similarly
@-mentioning another user may indicate a tweet
intended only for that person. The appearance of
[hashtag] in the best  TAC+ff+time unigrams is
consistent with prior research in non-TAC settings
(Suh et al, 2010; Petrovi?c et al, 2011).
Table 12: Features with largest coefficients, de-
limited by commas. POS tags omitted for clarity.
Our approach
best 15 custom twitter bigram, length (chars), rt
(the word), retweet (the word), verb, verb retweet score,
personal unigram, proper noun, number, noun, positive
words, please (the word), proper noun retweet score,
indefinite articles (a,an), adjective
best 20 unigrams rt, retweet, [num], breaking,
is, win, never, ., people, need, official, officially, are,
please, november, world, girl, !!!, god, new
worst 20 unigrams :, [at], icymi, also, comments,
half, ?, earlier, thanks, sorry, highlights, bit, point, up-
date, last, helping, peek, what, haven?t, debate
 TAC+ff+time
best 20 unigrams [hashtag], teen, fans, retweet,
sale, usa, women, butt, caught, visit, background, up-
coming, rt, this, bieber, these, each, chat, houston, book
worst 20 unigrams :, ..., boss, foundation, ?, ?,
others, john, roll, ride, appreciate, page, drive, correct,
full, ?, looks, @ (not as [at]), sales, hurts
6 Conclusion
In this work, we conducted the first large-scale
topic- and author-controlled experiment to study
the effects of wording on information propagation.
The features we developed to choose the bet-
ter of two alternative wordings posted better per-
formance than that of all our comparison algo-
rithms, including one given access to author and
timing features but trained on non-TAC data, and
also bested our estimate of average human perfor-
mance. According to our hypothesis tests, help-
ful wording heuristics include adding more infor-
mation, making one?s language align with both
community norms and with one?s prior messages,
and mimicking news headlines. Readers may
try out their own alternate phrasings at http:
//chenhaot.com/retweetedmore/ to see
what a simplified version of our classifier predicts.
In future work, it will be interesting to examine
how these features generalize to longer and more
extensive arguments. Moreover, understanding
the underlying psychological and cultural mecha-
nisms that establish the effectiveness of these fea-
tures is a fundamental problem of interest.
Acknowledgments. We thank C. Callison-Burch,
C. Danescu-Niculescu-Mizil, J. Kleinberg, P.
Mahdabi, S. Mullainathan, F. Pereira, K. Raman,
A. Swaminathan, the Cornell NLP seminar par-
ticipants and the reviewers for their comments; J.
Leskovec for providing some initial data; and the
anonymous annotators for all their labeling help.
This work was supported in part by NSF grant IIS-
0910664 and a Google Research Grant.
183
References
Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.
Predicting responses to microblog posts. In Pro-
ceedings of NAACL (short paper).
Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi.
2013. Success with style: Using writing style to
predict the success of novels. In Proceedings of
EMNLP.
Eitan Bakshy, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Everyone?s an influencer:
Quantifying influence on twitter. In Proceedings of
WSDM.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and pow-
erful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Youmna Borghol, Sebastien Ardon, Niklas Carlsson,
Derek Eager, and Anirban Mahanti. 2012. The
untold story of the clones: Content-agnostic factors
that impact YouTube video popularity. In Proceed-
ings of KDD.
Dennis Chong and James N. Druckman. 2007. Fram-
ing theory. Annual Review of Political Science,
10:103?126.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of ACL.
John DiNardo. 2008. Natural experiments and quasi-
natural experiments. In The New Palgrave Dictio-
nary of Economics. Palgrave Macmillan.
Olive Jean Dunn. 1961. Multiple comparisons among
means. Journal of the American Statistical Associa-
tion, 56(293):52?64.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of ACL.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech Tagging
for Twitter: Annotation, Features, and Experiments.
In Proceedings of NAACL (short paper).
David Godes, Dina Mayzlin, Yubo Chen, Sanjiv
Das, Chrysanthos Dellarocas, Bruce Pfeiffer, Barak
Libai, Subrata Sen, Mengze Shi, and Peeter Verlegh.
2005. The firm?s management of social interactions.
Marketing Letters, 16(3-4):415?428.
Marco Guerini, Carlo Strapparava, and G?ozde
?
Ozbal.
2011. Exploring text virality in social networks. In
Proceedings of ICWSM (poster).
Marco Guerini, Alberto Pepe, and Bruno Lepri. 2012.
Do linguistic style and readability of scientific ab-
stracts affect their virality? In Proceedings of
ICWSM (poster).
Lars Kai Hansen, Adam Arvidsson, Finn
?
Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news-affect and virality in Twitter.
Communications in Computer and Information Sci-
ence, 185:34?43.
Chip Heath, Chris Bell, and Emily Sternberg. 2001.
Emotional selection in memes: The case of urban
legends. Journal of personality and social psychol-
ogy, 81(6):1028.
George C. Homans. 1958. Social Behavior as Ex-
change. American Journal of Sociology, 63(6):597?
606.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison.
2011. Predicting popular messages in Twitter. In
Proceedings of WWW.
Carl I. Hovland, Irving L. Janis, and Harold H. Kelley.
1953. Communication and Persuasion: Psycholog-
ical Studies of Opinion Change, volume 19. Yale
University Press.
J. Peter Kincaid, Robert P. Fishburne Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (automated readability
index, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report, DTIC
Document.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a social network
or a news media? In Proceedings of WWW.
Himabindu Lakkaraju, Julian McAuley, and Jure
Leskovec. 2013. What?s in a name? Understanding
the interplay between titles, content, and communi-
ties in social media. In Proceedings of ICWSM.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? First experiments on article quality
prediction in the science journalism domain. Trans-
actions of ACL.
Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will
this #hashtag be popular tomorrow? In Proceedings
of SIGIR.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of ACL-IJCNLP.
Katherine L Milkman and Jonah Berger. 2012. What
makes online content viral? Journal of Marketing
Research, 49(2):192?205.
184
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2011. RT to win! Predicting message propagation
in Twitter. In Proceedings of ICWSM.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of EMNLP.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP.
Daniel M. Romero, Chenhao Tan, and Johan Ugander.
2013. On the interplay between social and topical
structure. In Proceedings of ICWSM.
Matthew J. Salganik, Peter Sheridan Dodds, and Dun-
can J. Watts. 2006. Experimental study of inequal-
ity and unpredictability in an artificial cultural mar-
ket. Science, 311(5762):854?856.
Matthew P. Simmons, Lada A Adamic, and Eytan Adar.
2011. Memes online: Extracted, subtracted, in-
jected, and recollected. In Proceedings of ICWSM.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.
Chi. 2010. Want to be retweeted? Large scale an-
alytics on factors impacting retweet in Twitter net-
work. In Proceedings of SocialCom.
Tao Sun, Ming Zhang, and Qiaozhu Mei. 2013. Unex-
pected relevance: An empirical study of serendipity
in retweets. In Proceedings of ICWSM.
Oren Tsur and Ari Rappoport. 2012. What?s in a hash-
tag?: Content based prediction of the spread of ideas
in microblogging communities. In Proceedings of
WSDM.
Jaewon Yang and Jure Leskovec. 2011. Patterns of
temporal variation in online media. In Proceedings
of WSDM.
185
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403?408,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Corpus of Sentence-level Revisions in Academic Writing:
A Step towards Understanding Statement Strength in Communication
Chenhao Tan
Dept. of Computer Science
Cornell University
chenhao@cs.cornell.edu
Lillian Lee
Dept. of Computer Science
Cornell University
llee@cs.cornell.edu
Abstract
The strength with which a statement is
made can have a significant impact on the
audience. For example, international rela-
tions can be strained by how the media in
one country describes an event in another;
and papers can be rejected because they
overstate or understate their findings. It is
thus important to understand the effects of
statement strength. A first step is to be able
to distinguish between strong and weak
statements. However, even this problem
is understudied, partly due to a lack of
data. Since strength is inherently relative,
revisions of texts that make claims are a
natural source of data on strength differ-
ences. In this paper, we introduce a corpus
of sentence-level revisions from academic
writing. We also describe insights gained
from our annotation efforts for this task.
1 Introduction
It is important for authors and speakers to find the
appropriate ?pitch? to convey a desired message
to the public. Indeed, sometimes heated debates
can arise around the choice of statement strength.
For instance, on March 1, 2014, an attack at Kun-
ming?s railway station left 29 people dead and
more than 140 others injured.
1
In the aftermath,
Chinese media accused Western media of ?soft-
pedaling the attack and failing to state clearly that
it was an act of terrorism?.
2
In particular, regard-
ing the statement by the US embassy that referred
to this incident as the ?terrible and senseless act
of violence in Kunming?, a Weibo user posted ?If
you say that the Kunming attack is a ?terrible and
1
http://en.wikipedia.org/wiki/2014_
Kunming_attack
2
http://sinosphere.blogs.nytimes.
com/2014/03/03/u-n-security-council-
condemns-terrorist-attack-in-kunming/
senseless act of violence?, then the 9/11 attack can
be called a ?regrettable traffic incident??.
3
This example is striking but not an isolated case,
for settings in which one party is trying to con-
vince another are pervasive; scenarios range from
court trials to conference submissions. Since the
strength and scope of an argument can be a cru-
cial factor in its success, it is important to under-
stand the effects of statement strength in commu-
nication.
A first step towards addressing this question is
to be able to distinguish between strong and weak
statements. As strength is inherently relative, it is
natural to look at revisions that change statement
strength, which we refer to as ?strength changes?.
Though careful and repeated revisions are presum-
ably ubiquitous in politics, legal systems, and jour-
nalism, it is not clear how to collect them; on the
other hand, revisions to research papers may be
more accessible, and many researchers spend sig-
nificant time on editing to convey the right mes-
sage regarding the strength of a project?s contribu-
tions, novelty, and limitations. Indeed, statement
strength in science communication matters to writ-
ers: understating contributions can affect whether
people recognize the true importance of the work;
at the same time, overclaiming can cause papers to
be rejected.
With the increasing popularity of e-print ser-
vices such as the arXiv
4
, strength changes in scien-
tific papers are becoming more readily available.
Since the arXiv started in 1991, it has become
?the standard repository for new papers in mathe-
matics, physics, statistics, computer science, biol-
ogy, and other disciplines? (Krantz, 2007). An in-
triguing observation is that many researchers sub-
mit multiple versions of the same paper on arXiv.
For instance, among the 70K papers submitted in
3
http://www.huffingtonpost.co.uk/2014/
03/03/china-kunming-911_n_4888748.html
4
http://arxiv.org/
403
ID Pairs
1
S1: The algorithm is studied in this paper .
S2: The algorithm is proposed in this paper .
2
S1: ... circadian pattern and burstiness in human communication activity .
S2: ... circadian pattern and burstiness in mobile phone communication .
3
S1: ... using minhash techniques , at a significantly lower cost and with same privacy guarantees .
S2: ... using minhash techniques , with lower costs .
4
S1: the rows and columns of the covariate matrix then have certain physical meanings ...
S2: the rows and columns of the covariate matrix could have different meanings ...
5
S1: they maximize the expected revenue of the seller but induce efficiency loss .
S2: they maximize the expected revenue of the seller but are inefficient .
Table 1: Examples of potential strength differences.
2011, almost 40% (27.7K) have multiple versions.
Many differences between these versions consti-
tute a source of valid and motivated strength dif-
ferences, as can be seen from the sentential revi-
sions in Table 1. Pair 1 makes the contribution
seem more impressive by replacing ?studied? with
?proposed?. Pair 2 downgrades ?human commu-
nication activity? to ?mobile phone communica-
tion?. Pair 3 removes ?significantly? and the em-
phasis on ?same privacy guarantees?. Pair 4 shows
an insertion of hedging, a relatively well-known
type of strength reduction. Pair 5 is an interesting
case that shows the complexity of this problem: on
the one hand, S2 claims that something is ?ineffi-
cient?, which is an absolute statement, compared
to ?efficiency loss? in S1, where the possibility of
efficiency still exists; on the other hand, S1 em-
ploys an active tone that emphasizes a causal rela-
tionship.
The main contribution of this work is to provide
the first large-scale corpus of sentence-level revi-
sions for studying a broad range of variations in
statement strength. We collected labels for a sub-
set of these revisions. Given the possibility of all
kinds of disagreement, the fair level of agreement
(Fleiss? Kappa) among our annotators was decent.
But in some cases, the labels differed from our ex-
pectations, indicating that the general public can
interpret the strength of scientific statements dif-
ferently from researchers. The participants? com-
ments may further shed light on science commu-
nication and point to better ways to define and un-
derstand strength differences.
2 Related Work and Data
Hedging, which can lead to strength differences,
has received some attention in the study of science
communication (Salager-Meyer, 2011; Lewin,
1998; Hyland, 1998; Myers, 1990). The CoNLL
2010 Shared Task was devoted to hedge detection
(Farkas et al, 2010). Hedge detection was also
used to understand scientific framing in debates
over genetically-modified organisms in food (Choi
et al, 2012).
Revisions on Wikipedia have been shown use-
ful for various applications, including spelling
correction (Zesch, 2012), sentence compression
(Yamangil and Nelken, 2008), text simplification
(Yatskar et al, 2010), paraphrasing (Max and Wis-
niewski, 2010), and textual entailment (Zanzotto
and Pennacchiotti, 2010). But none of the cat-
egories of Wikipedia revisions previously exam-
ined (Daxenberger and Gurevych, 2013; Bronner
and Monz, 2012; Mola-Velasco, 2011; Potthast et
al., 2008; Daxenberger and Gurevych, 2012) re-
late to statement strength. After all, the objective
of editing on Wikipedia is to present neutral and
objective articles.
Public datasets of science communication are
available, such as the ACL Anthology,
5
collec-
tions of NIPS papers,
6
and so on. These datasets
are useful for understanding the progress of disci-
plines or the evolution of topics. But the lack of
edit histories or revisions makes them not imme-
diately suitable for studying strength differences.
Recently, there have been experiments with open
peer review.
7
Records from open reviewing can
provide additional insights into the revision pro-
cess once enough data is collected.
5
http://aclweb.org/anthology/
6
http://nips.djvuzone.org/txt.html
7
http://openreview.net
404
title abstract intro middle conclusion0.0
100000.0200000.0300000.0
400000.0500000.0600000.0
700000.0800000.0900000.0
numberofc
hanges
57% 71% 65%
58%
62%
deletiontyporewrite
(a) Number of changes vs sections.
?middle? refers to the sections be-
tween introduction and conclusion.
math cond-mat astro-ph cs quant-ph0.0
100000.0200000.0
300000.0400000.0
500000.0600000.0
numberofc
hanges 57%
61% 67% 56% 59%
deletiontyporewrite
(b) Top 5 categories in number of
changes.
stat q-bio q-fin cs quant-ph0.0
0.10.2
0.30.4
0.5
numberofc
hangesper
sentence 54% 58% 58% 56% 59%
deletiontyporewrite
(c) Top 5 categories in number of
changes over the number of sen-
tences.
Figure 1: In all figures, different colors indicate different types of changes.
3 Dataset Description
Our main dataset was constructed from all papers
submitted in 2011 on the arXiv. We first extracted
the textual content from papers that have multiple
versions of tex source files. All mathematical en-
vironments were ignored. Section titles were not
included in the final texts but are used in align-
ment.
In order to align the first version and the fi-
nal version of the same paper, we first did macro
alignment of paper sections based on section titles.
Then, for micro alignment of sentences, we em-
ployed a dynamic programming algorithm similar
to that of Barzilay and Elhadad (2003). Instead of
cosine similarity, we used an idf-weighted longest-
common-subsequence algorithm to define the sim-
ilarity between two sentences, because changes in
word ordering can also be interesting. Formally,
the similarity score between sentence i and sen-
tence j is defined as
Simpi, jq ?
Weighted-LCSpS
i
, S
j
q
maxp
?
wPS
i
idfpwq,
?
wPS
j
idfpwqq
,
where S
i
and S
j
refer to sentence i and sentence j.
Since it is likely that a new version adds or deletes
a large sequence of sentences, we did not impose a
skip penalty. We set the mismatch penalty to 0.1.
8
In the end, there are 23K papers where the first
version was different from the last version.
9
We
8
We did not allow cross matching (i.e., i? j?1, i?1?
j), since we thought matching this case as pi ? 1, iq ? j or
i ? pj, j ? 1q can provide context for annotation purposes.
But in the end, we focused on labeling very similar pairs.
This decision had little effect.
9
This differs from the number in Section 1 because arti-
cles may not have the tex source available, or the differences
between versions may be in non-textual content.
categorize sentential revisions into the following
three types:
? Deletion: we cannot find a match in the final
version.
? Typo: all sequences in a pair of matched sen-
tences are typos, where a sequence-level typo
is one where the edit distance between the
matched sequences is less than three.
? Rewrite: matched sentences that are not ty-
pos. This type is the focus of this study.
What kinds of changes are being made? One
might initially think that typo fixes represent a
large proportion of revisions, but this is not cor-
rect, as shown in Figure 1a. Deletions represent a
substantial fraction, especially in the middle sec-
tion of a paper. But it is clear that the majority of
changes are rewrites; thus revisions on the arXiv
indeed provide a great source for potential strength
differences.
Who makes changes? Figure 1b shows that the
Math subarchive makes the largest number of
changes. This is consistent with the mathematics
community?s custom of using the arXiv to get find-
ings out early. In terms of changes per sentence
(Figure 1c), statistics and quantitative studies are
the top subareas.
Further, Figure 2 shows the effect of the number
of authors. It is interesting that both in terms of
sheer number and percentage, single-authored pa-
pers have the most changes. This could be because
a single author enjoys greater freedom and has
stronger motivation to make changes, or because
multiple authors tend to submit a more polished
initial version. This echoes the finding in Posner
405
You should mark S2 as Stronger if
? (R1) S2 strengthens the degree of some aspect of S1, for example, S1 has the word ?better?,
whereas S2 uses ?best?, or S2 removes the word ?possibly?
? (R2) S2 adds more evidence or justification (we don?t count adding details)
? (R3) S2 sounds more impressive in some other way: the authors? work is more important/novel-
/elegant/applicable/etc.
If instead S1 is stronger than S2 according to the reasons above, select Weaker. If the changes
aren?t strengthenings or weakenings according to the reason above, select No Strength Change.
If there are both strengthenings and weakenings, or you find that it is really hard to tell whether the
change is stronger or weaker, then select I can?t tell.
Table 2: Definition of labels in our labeling tasks.
1 2 3 4 5 >5number of authors46.048.050.052.0
54.056.058.060.062.0
64.0
numberofchange
s
(a) Number of changes vs
number of authors.
1 2 3 4 5 >5number of authors26%27%
28%29%
30%
percentageofcha
nges
(b) Percentage of changed
sentences vs number of au-
thors.
Figure 2: Error bars represent standard error. (a):
up until 5 authors, a larger number of authors in-
dicates a smaller number of changes. (b): per-
centage is measured over the number of sentences
in the first version; there is an interior minimum
where 2 or 3 authors make the smallest percentage
of sentence changes on a paper.
and Baecker (1992) that the collaborative writing
process differs considerably from individual writ-
ing. Also, more than 25% of the first versions are
changed, which again shows that substantive edits
are being made in these resubmissions.
4 Annotating Strength Differences
In order to study statement strength, reliable
strength-difference labels are needed. In this sec-
tion, we describe how we tried to define strength
differences, compiled labeling instructions, and
gathered labels using Amazon Mechanical Turk.
Label definition and collection procedure. We
focused on matched sentences from abstracts
and introductions to maximize the proportion of
strength differences (as opposed to factual/no
strength changes). We required pairs to have sim-
ilarity score larger than 0.5 in our labeling task to
make pairs more comparable. We also replaced
all math environments with ?[MATH]?.
10
We ob-
tained 108K pairs that satisfy the above condi-
tions, available at http://chenhaot.com/
pages/statement-strength.html. To
create the pool of pairs for labeling, we randomly
sampled 1000 pairs and then removed pairs that
we thought were processing errors.
We used Amazon Mechanical Turk. It may
initially seem surprising to have annotations of
technical statements not done by domain experts;
we did this intentionally because it is common to
communicate unfamiliar topics to the public in po-
litical and science communication (we comment
on non-expert rationales later). We use the follow-
ing set of labels: Stronger, Weaker, No Strength
Change, I can?t tell. Table 2 gives our definitions.
The instructions included 8 pairs as examples and
10 pairs to label as a training exercise. Partici-
pants were then asked to choose labels and write
mandatory comments for 50 pairs. According to
the comments written by participants, we believe
that they did the labeling in good faith.
Quantitative overview. We collected 9 labels
each for 500 pairs. Among the 500 pairs, Fleiss?
Kappa was 0.242, which indicates fair agreement
(Landis and Koch, 1977). We took a conserva-
tive approach and only considered pairs with an
absolute majority label, i.e., at least 5 of 9 label-
ers chose the same label. There are 386 pairs that
satisfy this requirement (93 weaker, 194 stronger,
99 no change). On this subset of pairs, Fleiss?
Kappa is 0.322, and 74.4% of pairs were strength
changes. Considering all the possible disagree-
ment, this result was acceptable.
Qualitative observations. We were excited
about the labels from these participants: despite
10
These decisions were made based on the results and feed-
back that we got from graduate students in an initial labeling.
406
ID Matched sentences and comments
1
S1: ... using data from numerics and experiments .
S2: ... using data sets from numerics in the point particle limit and one experimental data set .
(stronger) S2 is more specific in its description which seems stronger.
(weaker) ?one experimental data set? weakens the sentence
2
S1: we also proved that if [MATH] is sufficiently homogeneous then ...
S2: we also proved that if [MATH] is not totally disconnected and sufficiently homogeneous then ...
(stronger) We have more detail/proof in S2
(stronger) the words ?not totally disconnected? made the sentence sound more impressive.
3
S1: we also show in general that vectors of products of jack vertex operators form a basis of symmetric functions .
S2: we also show in general that the images of products of jack vertex operators form a basis of symmetric functions .
(weaker) Vectors sounds more impressive than images
(weaker) sentence one is more specific
4
S1: in the current paper we discover several variants of qd algorithms for quasiseparable matrices .
S2: in the current paper we adapt several variants of qd algorithms to quasiseparable matrices .
(stronger) in S2 Adapt is stronger than just the word discover. adapt implies more of a proactive measure.
(stronger) s2 sounds as if they?re doing something with specifics already, rather than hunting for a way to do it
Table 3: Representative examples of surprising labels, together with selected labeler comments.
the apparent difficulty of the task, we found that
many labels for the 386 pairs were reasonable.
However, in some cases, the labels were counter-
intuitive. Table 3 shows some representative ex-
amples.
First, participants tend to take details as evi-
dence even when these details are not germane to
the statement. For pair 1, while one turker pointed
out the decline in number of experiments, most
turkers simply labeled it as stronger because it was
more specific. ?Specific? turned out to be a com-
mon reason used in the comments, even though we
said in the instructions that only additional justifi-
cation and evidence matter. This echoes the find-
ing in Bell and Loftus (1989) that even unrelated
details influenced judgments of guilt.
Second, participants interpret constraints/condi-
tions not in strictly logical ways, seeming to care
little about scope at times. For instance, the ma-
jority labeled pair 2 as ?stronger?. But in S2 for
that pair, the result holds for strictly fewer pos-
sible worlds. But it should be said that there
are cases that labelers interpreted logically, e.g.,
?compelling evidence? subsumes ?compelling ex-
perimental evidence?.
Both of the above cases share the property that
they seem to be correlated with a tendency to
judge lengthier statements as stronger. Another
interesting case that does not share this character-
istic is that participants can have a different un-
derstanding of domain-specific terms. For pair 3,
the majority thought that ?vectors? sounds more
impressive than ?images?; for pair 4, the major-
ity considered ?adapt? stronger than ?discover?.
This issue is common when communicating new
topics to the public not only in science commu-
nication but also in politics and other scenarios. It
may partly explain miscommunications and misin-
terpretations of scientific studies in journalism.
11
5 Looking ahead
Our observations regarding the annotation results
raise questions regarding what is a generalizable
way to define strength differences, how to use the
labels that we collected, and how to collect la-
bels in the future. We believe that this corpus of
sentence-level revisions, together with the labels
and comments from participants, can provide in-
sights into better ways to approach this problem
and help further understand strength of statements.
One interesting direction that this enables is a
potentially new kind of learning problem. The
comments indicate features that humans think
salient. Is it possible to automatically learn new
features from the comments?
The ultimate goal of our study is to understand
the effects of statement strength on the public,
which can lead to various applications in public
communication.
Acknowledgments
We thank J. Baldridge, J. Boyd-Graber, C.
Callison-Burch, and the reviewers for helpful
comments; P. Ginsparg for providing data; and S.
Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park,
K. Raman, M. Reitblatt, S. Roy, A. Sharma, R.
Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yang
and the anonymous annotators for all their label-
ing help. This work was supported in part by NSF
grant IIS-0910664 and a Google Research Grant.
11
http://www.phdcomics.com/comics/
archive.php?comicid=1174
407
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32.
Brad E Bell and Elizabeth F Loftus. 1989. Trivial per-
suasion in the courtroom: The power of (a few) mi-
nor details. Journal of Personality and Social Psy-
chology, 56(5):669.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 356?366.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: A position paper. In Proceedings
of the Workshop on Extra-Propositional Aspects of
Meaning in Computational Linguistics, pages 70?
79.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In COLING,
pages 711?726.
Johannes Daxenberger and Iryna Gurevych. 2013.
Automatically Classifying Edit Categories in
Wikipedia Revisions. In Proceedings of the 2013
Conference on Empirical Methods in Natural
Language Processing.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12.
Ken Hyland. 1998. Hedging in scientific research
articles. John Benjamins Pub. Co., Amsterdam;
Philadelphia.
Steven G. Krantz. 2007. How to Write Your First Pa-
per. Notices of the AMS.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159?174.
Beverly A. Lewin. 1998. Hedging: Form and func-
tion in scientific research texts. In Genre Studies
in English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I.
Aurlien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedia?s Revision History. In Pro-
ceedings of The seventh international conference on
Language Resources and Evaluation.
Santiago M Mola-Velasco. 2011. Wikipedia Vandal-
ism Detection. In Proceedings of the 20th Interna-
tional Conference Companion on World Wide Web,
pages 391?396.
Greg Myers. 1990. Writing biology: Texts in the social
construction of scientific knowledge. University of
Wisconsin Press, Madison, Wis.
Ilona R Posner and Ronald M Baecker. 1992. How
people write together [groupware]. In System
Sciences, 1992. Proceedings of the Twenty-Fifth
Hawaii International Conference on, pages 127?
138.
Martin Potthast, Benno Stein, and Robert Ger-
ling. 2008. Automatic Vandalism Detection in
Wikipedia. In Advances in Information Retrieval,
pages 663?668. Springer Berlin Heidelberg.
Franc?oise Salager-Meyer. 2011. Scientific discourse
and contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sen-
tence Compression. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, pages 137?140.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on Collaboratively Constructed Se-
mantic Resources.
Torsten Zesch. 2012. Measuring Contextual Fitness
Using Error Contexts Extracted from the Wikipedia
Revision History. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 529?538.
408
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 70?79, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Hedge Detection as a Lens on Framing in the GMO Debates:
A Position Paper
Eunsol Choi?, Chenhao Tan?, Lillian Lee?, Cristian Danescu-Niculescu-Mizil? and Jennifer Spindel?
?Department of Computer Science, ?Department of Plant Breeding and Genetics
Cornell University
ec472@cornell.edu, chenhao|llee|cristian@cs.cornell.edu, jes462@cornell.edu
Abstract
Understanding the ways in which participants
in public discussions frame their arguments is
important in understanding how public opin-
ion is formed. In this paper, we adopt the po-
sition that it is time for more computationally-
oriented research on problems involving fram-
ing. In the interests of furthering that goal,
we propose the following specific, interesting
and, we believe, relatively accessible ques-
tion: In the controversy regarding the use
of genetically-modified organisms (GMOs) in
agriculture, do pro- and anti-GMO articles dif-
fer in whether they choose to adopt a more
?scientific? tone?
Prior work on the rhetoric and sociology of
science suggests that hedging may distin-
guish popular-science text from text written
by professional scientists for their colleagues.
We propose a detailed approach to studying
whether hedge detection can be used to un-
derstanding scientific framing in the GMO de-
bates, and provide corpora to facilitate this
study. Some of our preliminary analyses sug-
gest that hedges occur less frequently in scien-
tific discourse than in popular text, a finding
that contradicts prior assertions in the litera-
ture. We hope that our initial work and data
will encourage others to pursue this promising
line of inquiry.
1 Introduction
1.1 Framing, ?scientific discourse?, and GMOs
in the media
The issue of framing (Goffman, 1974; Scheufele,
1999; Benford and Snow, 2000) is of great im-
portance in understanding how public opinion is
formed. In their Annual Review of Political Science
survey, Chong and Druckman (2007) describe fram-
ing effects as occurring ?when (often small) changes
in the presentation of an issue or an event produce
(sometimes large) changes of opinion? (p. 104);
as an example, they cite a study wherein respon-
dents answered differently, when asked whether a
hate group should be allowed to hold a rally, depend-
ing on whether the question was phrased as one of
?free speech? or one of ?risk of violence?.
The genesis of our work is in a framing question
motivated by a relatively current political issue. In
media coverage of transgenic crops and the use of
genetically modified organisms (GMOs) in food, do
pro-GMO vs. anti-GMO articles differ not just with
respect to word choice, but in adopting a more ?sci-
entific? discourse, meaning the inclusion of more
uncertainty and fewer emotionally-laden words? We
view this as an interesting question from a text anal-
ysis perspective (with potential applications and im-
plications that lie outside the scope of this article).
1.2 Hedging as a sign of scientific discourse
To obtain a computationally manageable character-
ization of ?scientific discourse?, we turned to stud-
ies of the culture and language of science, a body
of work spanning fields ranging from sociology to
applied linguistics to rhetoric and communication
(Gilbert and Mulkay, 1984; Latour, 1987; Latour
and Woolgar, 1979; Halliday and Martin, 1993; Baz-
erman, 1988; Fahnestock, 2004; Gross, 1990).
One characteristic that has drawn quite a bit of
attention in such studies is hedging (Myers, 1989;
70
Hyland, 1998; Lewin, 1998; Salager-Meyer, 2011).1
Hyland (1998, pg. 1) defines hedging as the ex-
pression of ?tentativeness and possibility? in com-
munication, or, to put it another way, language cor-
responding to ?the writer withholding full commit-
ment to statements? (pg. 3). He supplies many
real-life examples from scientific research articles,
including the following:
1. ?It seems that this group plays a critical role in
orienting the carboxyl function? (emphasis Hy-
land?s)
2. ?...implies that phytochrome A is also not nec-
essary for normal photomorphogenesis, at least
under these irradiation conditions? (emphasis
Hyland?s)
3. ?We wish to suggest a structure for the salt of
deoxyribose nucleic acid (D.N.A.)? (emphasis
added)2
Several scholars have asserted the centrality of hedg-
ing in scientific and academic discourse, which cor-
responds nicely to the notion of ?more uncertainty?
mentioned above. Hyland (1998, p. 6) writes, ?De-
spite a widely held belief that professional scientific
writing is a series of impersonal statements of fact
which add up to the truth, hedges are abundant in
science and play a critical role in academic writing?.
Indeed, Myers (1989, p. 13) claims that in scien-
tific research articles, ?The hedging of claims is so
common that a sentence that looks like a claim but
has no hedging is probably not a statement of new
knowledge?.3
Not only is understanding hedges important to un-
derstanding the rhetoric and sociology of science,
but hedge detection and analysis ? in the sense of
identifying uncertain or uncertainly-sourced infor-
mation (Farkas et al, 2010) ? has important appli-
cations to information extraction, broadly construed,
and has thus become an active sub-area of natural-
language processing. For example, the CoNLL 2010
1In linguistics, hedging has been studied since the 1970s
(Lakoff, 1973).
2This example originates from Watson and Crick?s land-
mark 1953 paper. Although the sentence is overtly tentative,
did Watson and Crick truly intend to be polite and modest in
their claims? See Varttala (2001) for a review of arguments re-
garding this question.
3Note the inclusion of the hedge ?probably?.
Shared Task was devoted to this problem (Farkas
et al, 2010).
Putting these two lines of research together, we
see before us what appears to be an interesting in-
terdisciplinary and, at least in principle, straightfor-
ward research program: relying on the aforemen-
tioned rhetoric analyses to presume that hedging is
a key characteristic of scientific discourse, build a
hedge-detection system to computationally ascertain
which proponents in the GMO debate tend to use
more hedges and thus, by presumption, tend to adopt
a more ?scientific? frame.4
1.3 Contributions
Our overarching goal in this paper is to convince
more researchers in NLP and computational linguis-
tics to work on problems involving framing. We
try to do so by proposing a specific problem that
may be relatively accessible. Despite the apparent
difficulty in addressing such questions, we believe
that progress can be made by drawing on observa-
tions drawn from previous literature across many
fields, and integrating such work with movements
in the computational community toward considera-
tion of extra-propositional and pragmatic concerns.
We have thus intentionally tried to ?cover a lot of
ground?, as one referee put it, in the introductory
material just discussed.
Since framing problems are indeed difficult, we
elected to narrow our scope in the hope of making
some partial progress. Our technical goal here, at
this workshop, where hedge detection is one of the
most relevant topics to the broad questions we have
raised, is not to learn to classify texts as being pro-
vs. anti-GMO, or as being scientific or not, per se.5
Our focus is on whether hedging specifically, con-
sidered as a single feature, is correlated with these
different document classes, because of the previous
research attention that has been devoted to hedging
in particular and because of hedging being one of the
topics of this workshop. The point of this paper is
4However, this presumption that more hedges characterize a
more scientific discourse has been contested. See section 2 for
discussion and section 4.2 for our empirical investigation.
5Several other groups have addressed the problem of try-
ing to identify different sides or perspectives (Lin et al, 2006;
Hardisty et al, 2010; Beigman Klebanov et al, 2010; Ahmed
and Xing, 2010).
71
thus not to compare the efficacy of hedging features
with other types, such as bag-of-words features. Of
course, to do so is an important and interesting di-
rection for future work.
In the end, we were not able to achieve satisfac-
tory results even with respect to our narrowed goal.
However, we believe that other researchers may be
able to follow the plan of attack we outline below,
and perhaps use the data we are releasing, in order
to achieve our goal. We would welcome hearing the
results of other people?s efforts.
2 How should we test whether hedging
distinguishes scientific text?
One very important point that we have not yet ad-
dressed is: While the literature agrees on the impor-
tance of hedging in scientific text, the relative de-
gree of hedging in scientific vs. non-scientific text is
a matter of debate.
On the one side, we have assertions like those of
Fahnestock (1986), who shows in a clever, albeit
small-scale, study involving parallel texts that when
scientific observations pass into popular accounts,
changes include ?removing hedges ... thus con-
ferring greater certainty on the reported facts? (pg.
275). Similarly, Juanillo, Jr. (2001) refers to a shift
from a forensic style to a ?celebratory? style when
scientific research becomes publicized, and credits
Brown (1998) with noting that ?celebratory scien-
tific discourses tend to pay less attention to caveats,
contradictory evidence, and qualifications that are
highlighted in forensic or empiricist discourses. By
downplaying scientific uncertainty, it [sic] alludes to
greater certainty of scientific results for public con-
sumption? (Juanillo, Jr., 2001, p. 42).
However, others have contested claims that the
popularization process involves simplification, dis-
tortion, hype, and dumbing down, as Myers (2003)
colorfully puts it; he provides a critique of the rel-
evant literature. Varttala (1999) ran a corpus anal-
ysis in which hedging was found not just in pro-
fessional medical articles, but was also ?typical of
popular scientific articles dealing with similar top-
ics? (p. 195). Moreover, significant variation in use
of hedging has been found across disciplines and au-
thors? native language; see Salager-Meyer (2011) or
Varttala (2001) for a review.
To the best of our knowledge, there have been no
large-scale empirical studies validating the hypoth-
esis that hedges appear more or less frequently in
scientific discourse.
Proposed procedure Given the above, our first
step must be to determine whether hedges are more
or less prominent in ?professional scientific? (hence-
forth ?prof-science??) vs. ?public science? (hence-
forth ?pop-science?) discussions of GMOs. Of
course, for a large-scale study, finding hedges re-
quires developing and training an effective hedge de-
tection algorithm.
If the first step shows that hedges can indeed be
used to effectively distinguish prof-science vs. pop-
science discourse on GMOs, then the second step is
to examine whether the use of hedging in pro-GMO
articles follows our inferred ?scientific? occurrence
patterns to a greater extent than the hedging in anti-
GMO articles.
However, as our hedge classifier trained on the
CoNLL dataset did not perform reliably on the dif-
ferent domain of prof-science vs. pop-science dis-
cussions of GMOs, we focus the main content of this
paper on the first step. We describe data collection
for the second step in the appendix.
3 Data
To accomplish the first step of our proposed pro-
cedure outlined above, we first constructed a prof-
science/pop-science corpus by pulling text from
Web of Science for prof-science examples and from
LexisNexis for pop-science examples, as described
in Section 3.1. Our corpus will be posted online
at https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
As noted above, computing the degree of hedg-
ing in the aforementioned corpus requires access to
a hedge-detection algorithm. We took a supervised
approach, taking advantage of the availability of the
CoNLL 2010 hedge-detection training and evalua-
tion corpora, described in Section 3.2
3.1 Prof-science/pop-science data: LEXIS and
WOS
As mentioned previously, a corpus of prof-science
and pop-science articles is required to ascertain
whether hedges are more prevalent in one or the
72
Dataset Doc type # docs # sentences Avg sentence length Flesch reading ease
Prof-science/pop-science corpus
WOS abstracts 648 5596 22.35 23.39
LEXIS (short) articles 928 36795 24.92 45.78
Hedge-detection corpora
Bio (train) abstracts, articles 1273, 9 14541 (18% uncertain) 29.97 20.77
Bio (eval) articles 15 5003 (16% uncertain) 31.30 30.49
Wiki (train) paragraphs 2186 11111 (22% uncertain) 23.07 35.23
Wiki (eval) paragraphs 2346 9634 (23% uncertain) 20.82 31.71
Table 1: Basic descriptive statistics for the main corpora we worked with. We created the first two. Higher Flesch
scores indicate text that is easier to read.
other of these two writing styles. Since our ultimate
goal is to look at discourse related to GMOs, we re-
strict our attention to documents on this topic.
Thomson Reuter?s Web of Science (WOS), a
database of scientific journal and conference arti-
cles, was used as a source of prof-science samples.
We chose to collect abstracts, rather than full scien-
tific articles, because intuition suggests that the lan-
guage in abstracts is more high-level than that in the
bodies of papers, and thus more similar to the lan-
guage one would see in a public debate on GMOs.
To select for on-topic abstracts, we used the phrase
?transgenic foods? as a search keyword and dis-
carded results containing any of a hand-selected list
of off-topic filtering terms (e.g., ?mice? or ?rats?).
We then made use of domain expertise to manually
remove off-topic texts. The process yielded 648 doc-
uments for a total of 5596 sentences.
Our source of pop-science articles was Lexis-
Nexis (LEXIS). On-topic documents were collected
from US newspapers using the search keywords ?ge-
netically modified foods? or ?transgenic crops? and
then imposing the additional requirement that at
least two terms on a hand-selected list7 be present
in each document. After the removal of duplicates
and texts containing more than 2000 words to delete
excessively long articles, our final pop-science sub-
corpus was composed of 928 documents.
7The term list: GMO, GM, GE, genetically modified, ge-
netic modification, modified, modification, genetic engineer-
ing, engineered, bioengineered, franken, transgenic, spliced,
G.M.O., tweaked, manipulated, engineering, pharming, aqua-
culture.
3.2 CoNLL hedge-detection training data 8
As described in Farkas et al (2010), the motivation
behind the CoNLL 2010 shared task is that ?distin-
guishing factual and uncertain information in texts is
of essential importance in information extraction?.
As ?uncertainty detection is extremely important for
biomedical information extraction?, one component
of the dataset is biological abstracts and full arti-
cles from the BioScope corpus (Bio). Meanwhile,
the chief editors of Wikipedia have drawn the at-
tention of the public to specific markers of uncer-
tainty known as weasel words9: they are words or
phrases ?aimed at creating an impression that some-
thing specific and meaningful has been said?, when,
in fact, ?only a vague or ambiguous claim, or even
a refutation, has been communicated?. An example
is ?It has been claimed that ...?: the claimant has not
been identified, so the source of the claim cannot be
verified. Thus, another part of the dataset is a set
of Wikipedia articles (Wiki) annotated with weasel-
word information. We view the combined Bio+Wiki
corpus (henceforth the CoNLL dataset) as valuable
for developing hedge detectors, and we attempt to
study whether classifiers trained on this data can be
generalized to other datasets.
3.3 Comparison
Table 1 gives the basic statistics on the main datasets
we worked with. Though WOS and LEXIS differ in
the total number of sentences, the average sentence
length is similar. The average sentence length in Bio
is longer than that in Wiki. The articles in WOS
are markedly more difficult to read than the articles
8http://www.inf.u-szeged.hu/rgai/conll2010st/
9http://en.wikipedia.org/wiki/Weasel word
73
in LEXIS according to Flesch reading ease (Kincaid
et al, 1975).
4 Hedging to distinguish scientific text:
Initial annotation
As noted in Section 1, it is not a priori clear whether
hedging distinguishes scientific text or that more
hedges correspond to a more ?scientific? discourse.
To get an initial feeling for how frequently hedges
occur in WOS and LEXIS, we hand-annotated a
sample of sentences from each. In Section 4.1, we
explain the annotation policy of the CoNLL 2010
Shared Task and our own annotation method for
WOS and LEXIS. After that, we move forward in
Section 4.2 to compare the percentage of uncertain
sentences in prof-science vs. pop-science text on
this small hand-labeled sample, and gain some ev-
idence that there is indeed a difference in hedge oc-
currence rates, although, perhaps surprisingly, there
seem to be more hedges in the pop-science texts.
As a side benefit, we subsequently use the
hand-labeled sample we produce to investigate the
accuracy of an automatic hedge detector in the
WOS+LEXIS domain; more on this in Section 5.
4.1 Uncertainty annotation
CoNLL 2010 Shared Task annotation policy As
described in Farkas et al (2010, pg. 4), the data an-
notation polices for the CoNLL 2010 Shared Task
were that ?sentences containing at least one cue
were considered as uncertain, while sentences with
no cues were considered as factual?, where a cue
is a linguistic marker that in context indicates un-
certainty. A straightforward example of a sentence
marked ?uncertain? in the Shared Task is ?Mild blad-
der wall thickening raises the question of cystitis.?
The annotated cues are not necessarily general, par-
ticularly in Wiki, where some of the marked cues
are as specific as ?some of schumann?s best choral
writing?, ?people of the jewish tradition?, or ?certain
leisure or cultural activities?.
Note that ?uncertainty? in the Shared Task def-
inition also encompassed phrasing that ?creates an
impression that something important has been said,
but what is really communicated is vague, mislead-
ing, evasive or ambiguous ... [offering] an opinion
without any backup or source?. An example of such
Dataset % of uncertain sentences
WOS (estimated from 75-sentence sample) 20
LEXIS (estimated from 78-sentence sample) 28
Bio 17
Wiki 23
Table 2: Percentages of uncertain sentences.
a sentence, drawn from Wikipedia and marked ?un-
certain? in the Shared Task, is ?Some people claim
that this results in a better taste than that of other diet
colas (most of which are sweetened with aspartame
alone).?; Farkas et al (2010) write, ?The ... sentence
does not specify the source of the information, it is
just the vague term ?some people? that refers to the
holder of this opinion?.
Our annotation policy We hand-annotated 200
randomly-sampled sentences, half from WOS and
half from LEXIS10, to gauge the frequency with
which hedges occur in each corpus. Two annota-
tors each followed the rules of the CoNLL 2010
Shared Task to label sentences as certain, uncertain,
or not a proper sentence.11 The annotators agreed on
153 proper sentences of the 200 sentences (75 from
WOS and 78 from LEXIS). Cohen?s Kappa (Fleiss,
1981) was 0.67 on the annotation, which means that
the consistency between the two annotators was fair
or good. However, there were some interesting cases
where the two annotators could not agree. For ex-
ample, in the sentence ?Cassava is the staple food of
tropical Africa and its production, averaged over 24
countries, has increased more than threefold from
1980 to 2005 ... ?, one of the annotators believed
that ?more than? made the sentence uncertain. These
borderline cases indicate that the definition of hedg-
ing should be carefully delineated in future studies.
4.2 Percentages of uncertain sentences
To validate the hypothesis that prof-science articles
contain more hedges, we computed the percentage
10We took steps to attempt to hide from the annotators any
explicit clues as to the source of individual sentences: the sub-
set of authors who did the annotation were not those that col-
lected the data, and the annotators were presented the sentences
in random order.
11The last label was added because of a few errors in scraping
the data.
74
of uncertain sentences in our labeled data. As shown
in Table 2, we observed a trend contradicting ear-
lier studies. Uncertain sentences were more frequent
in LEXIS than in WOS, though the difference was
not statistically significant12 (perhaps not surprising
given the small sample size). The same trend was
seen in the CoNLL dataset: there, too, the percent-
age of uncertain sentences was significantly smaller
in Bio (prof-science articles) than in Wiki. In order
to make a stronger argument about prof-science vs
pop-science, however, more annotation on the WOS
and LEXIS datasets is needed.
5 Experiments
As stated in Section 1, our proposal requires devel-
oping an effective hedge detection algorithm. Our
approach for the preliminary work described in this
paper is to re-implement Georgescul?s (2010) algo-
rithm; the experimental results on the Bio+Wiki do-
main, given in Section 5.1, are encouraging. Then
we use this method to attempt to validate (at a larger
scale than in our manual pilot annotation) whether
hedges can be used to distinguish between prof-
science and pop-science discourse on GMOs. Un-
fortunately, our results, given in Section 5.2, are
inconclusive, since our trained model could not
achieve satisfactory automatic hedge-detection ac-
curacy on the WOS+LEXIS domain.
5.1 Method
We adopted the method of Georgescul (2010): Sup-
port Vector Machine classification based on a Gaus-
sian Radial Basis kernel function (Vapnik, 1998; Fan
et al, 2005), employing n-grams from annotated cue
phrases as features, as described in more detail be-
low. This method achieved the top performance in
the CoNLL 2010 Wikipedia hedge-detection task
(Farkas et al, 2010), and SVMs have been proven
effective for many different applications. We used
the LIBSVM toolkit in our experiments13.
As described in Section 3.2, there are two separate
datasets in the CoNLL dataset. We experimented on
them separately (Bio, Wiki). Also, to make our clas-
sifier more generalizable to different datasets, we
12Throughout, ?statistical significance? refers to the student
t-test with p < .05.
13http://www.csie.ntu.edu.tw/?cjlin/libsvm/
also trained models based on the two datasets com-
bined (Bio+Wiki). As for features, we took advan-
tage of the observation in Georgescul (2010) that the
bag-of-words model does not work well for this task.
We used different sets of features based on hedge
cue words that have been annotated as part of the
CoNLL dataset distribution14. The basic feature set
was the frequency of each hedge cue word from the
training corpus after removing stop words and punc-
tuation and transforming words to lowercase. Then,
we extracted unigrams, bigrams and trigrams from
each hedge cue phrase. Table 3 shows the number
of features in different settings. Notice that there are
many more features in Wiki. As mentioned above,
in Wiki, some cues are as specific as ?some of schu-
mann?s best choral writing?, ?people of the jewish
tradition?, or ? certain leisure or cultural activities?.
Taking n-grams from such specific cues can cause
some sentences to be classified incorrectly.
Feature source #features
Bio 220
Bio (cues + bigram + trigram) 340
Wiki 3740
Wiki (cues + bigram + trigram) 10603
Table 3: Number of features.
Best cross-validation performance
Dataset (C, ?) P R F
Bio (40, 2?3) 84.0 92.0 87.8
Wiki (30, 2?6) 64.0 76.3 69.6
Bio+Wiki (10, 2?4) 66.7 78.3 72.0
Table 4: Best 5-fold cross-validation performance for Bio
and/or Wiki after parameter tuning. As a reminder, we
repeat that our intended final test set is the WOS+LEXIS
corpus, which is disjoint from Bio+Wiki.
We adopted several techniques from Georgescul
(2010) to optimize performance through cross vali-
dation. Specifically, we tried different combinations
of feature sets (the cue phrases themselves, cues +
14For the Bio model, we used cues extracted from Bio. Like-
wise, the Wiki model used cues from Wiki, and the Bio+Wiki
model used cues from Bio+Wiki.
75
Evaluation set Model P R F
WOS+LEXIS Bio 54 68 60
WOS+LEXIS Wiki 38 54 45
WOS+LEXIS Bio+Wiki 21 93 34
Sub-corpus performance of the model based on Bio
WOS Bio 58 73 65
LEXIS Bio 52 64 57
Table 5: The upper part shows the performance on WOS
and LEXIS based on models trained on the CoNLL
dataset. The lower part gives the sub-corpus results for
Bio, which provided the best performance on the full
WOS+LEXIS corpus.
unigram, cues + bigram, cues + trigram, cues + uni-
gram + bigram + trigram, cues + bigram + trigram).
We tuned the width of the RBF kernel (?) and the
regularization parameter (C) via grid search over the
following range of values: {2?9, 2?8, 2?7, . . . , 24}
for ?, {1, 10, 20, 30, . . . , 150} for C. We also tried
different weighting strategies for negative and pos-
itive classes (i.e., either proportional to the number
of positive instances, or uniform). We performed 5-
fold cross validation for each possible combination
of experimental settings on the three datasets (Bio,
Wiki, Bio+Wiki).
Table 4 shows the best performance on all three
datasets and the corresponding parameters. In the
three datasets, cue+bigram+trigram provided the
best performance, and the weighted model con-
sistently produced superior results to the uniform
model. The F1 measure for Bio was 87.8, which
was satisfactory, while the F1 results for Wiki were
69.6, which were the worst of all the datasets.
This resonates with our observation that the task on
Wikipedia is more subtly defined and thus requires
a more sophisticated approach than counting the oc-
currences of bigrams and trigrams.
5.2 Results on WOS+LEXIS
Next, we evaluated whether our best classifier
trained on the CoNLL dataset can be generalized to
other datasets, in particular, the WOS and LEXIS
corpus. Performance was measured on the 153 sen-
tences on which our annotators agreed, a dataset
that was introduced in Section 4.1. Table 5 shows
how the best models trained on Bio, Wiki, and
Evaluation set (C, ?) P R F
WOS + LEXIS (50, 2?9) 68 62 65
WOS (50, 2?9) 85 73 79
LEXIS (50, 2?9) 57 54 56
Table 6: Best performance after parameter tuning
based on the 153 labeled WOS+LEXIS sentences; this
gives some idea of the upper-bound potential of our
Georgescul-based method. The training set is Bio, which
gave the best performance in Table 5.
Bio+Wiki, respectively, performed on the 153 la-
beled sentences. First, we can see that the perfor-
mance degraded significantly compared to the per-
formance for in-domain cross validation. Second, of
the three different models, Bio showed the best per-
formance. Bio+Wiki gave the worst performance,
which hints that combining two datasets and cue
words may not be a promising strategy: although
Bio+Wiki shows very good recall, this can be at-
tributed to its larger feature set, which contains all
available cues and perhaps as a result has a very high
false-positive rate. We further investigated and com-
pared performance on LEXIS and WOS for the best
model (Bio). Not surprisingly, our classifier works
better in WOS than in LEXIS.
It is clear that there exist domain differences be-
tween the CoNLL dataset and WOS+LEXIS. To bet-
ter understand the poor cross-domain performance
of the classifier, we tuned another model based on
the performance on the 153 labeled sentences us-
ing Bio as training data. As we can see in Table
6, the performance on WOS improved significantly,
while the performance on LEXIS decreased. This
is probably caused by the fact that WOS is a col-
lection of scientific paper abstracts, which is more
similar to the training corpus than LEXIS, which is
a collection of news media articles15. Also, LEXIS
articles are hard to classify even with the tuned
model, which challenges the effectiveness of a cue-
words frequency approach beyond professional sci-
entific texts. Indeed, the simplicity of our reim-
plementation of Georgescul?s algorithm seems to
cause longer sentences to be classified as uncer-
tain, because cue phrases (or n-grams extracted from
15The Wiki model performed better on LEXIS than on WOS.
Though the performance was not good, this result further rein-
forces the possibility of a domain-dependence problem.
76
cue phrases) are more likely to appear in lengthier
sentences. Analysis of the best performing model
shows that the false-positive sentences are signifi-
cantly longer than the false-negative ones.16
Dataset Model % classified uncertain
WOS Bio 16
LEXIS Bio 19
WOS Tuned 15
LEXIS Tuned 14
Table 7: For completeness, we report here the percentage
of uncertain sentences in WOS and LEXIS according to
our trained classifiers, although we regard these results as
unreliable since those classifiers have low accuracy. Bio
refers to the best model trained on Bio only in Section 5.1,
while Tuned refers to the model in Table 6 that is tuned
based on the 153 labeled sentences in WOS+LEXIS.
While the cross-domain results were not reliable,
we produced preliminary results on whether there
exist fewer hedges in scientific text. We can see that
the relative difference in certain/uncertain ratios pre-
dicted by the two different models (Bio, Tuned) are
different in Table 7. In the tuned model, the differ-
ence between LEXIS and WOS in terms of the per-
centage of uncertain sentences was not statistically
significant, while in the Bio model, their difference
was statistically significant. Since the performance
of our hedge classifier on the 153 hand-annotated
WOS+LEXIS sentences was not reliable, though,
we must abstain from making conclusive statements
here.
6 Conclusion and future work
In this position paper, we advocated that researchers
apply hedge detection not only to the classic moti-
vation of information-extraction problems, but also
to questions of how public opinion forms. We pro-
posed a particular problem in how participants in de-
bates frame their arguments. Specifically, we asked
whether pro-GMO and anti-GMO articles differ in
adopting a more ?scientific? discourse. Inspired by
earlier studies in social sciences relating hedging to
texts aimed at professional scientists, we proposed
16Average length of true positive sentences : 28.6, false pos-
itive sentences 35.09, false negative sentences: 22.0.
addressing the question with automatic hedge de-
tection as a first step. To develop a hedge clas-
sifier, we took advantage of the CoNLL dataset
and a small annotated WOS and LEXIS dataset.
Our preliminary results show there may exist a gap
which indicates that hedging may, in fact, distin-
guish prof-science and pop-science documents. In
fact, this computational analysis suggests the possi-
bility that hedges occur less frequently in scientific
prose, which contradicts several prior assertions in
the literature.
To confirm the argument that pop-science tends
to use more hedging than prof-science, we need
a hedge classifier that performs more reliably in
the WOS and LEXIS dataset than ours does. An
interesting research direction would be to develop
transfer-learning techniques to generalize hedge
classifiers for different datasets, or to develop a gen-
eral hedge classifier relatively robust to domain dif-
ferences. In either case, more annotated data on
WOS and LEXIS is needed for better evaluation or
training.
Another strategy would be to bypass the first step,
in which we determine whether hedges are more
or less prominent in scientific discourse, and pro-
ceed directly to labeling and hedge-detection in pro-
GMO and anti-GMO texts. However, this will not
answer the question of whether advocates in debates
other than on GMO-related topics employ a more
scientific discourse. Nonetheless, to aid those who
wish to pursue this alternate strategy, we have col-
lected two sets of opinionated articles on GMO (pro-
and anti-); see appendix for more details.
Acknowledgments We thank Daniel Hopkins and
Bonnie Webber for reference suggestions, and the
anonymous reviewers for helpful and thoughtful
comments. This paper is based upon work sup-
ported in part by US NSF grants IIS-0910664 and
IIS-1016099, a US NSF graduate fellowship to JS,
Google, and Yahoo!
References
Amr Ahmed and Eric P Xing. Staying informed: su-
pervised and semi-supervised multi-view topical
analysis of ideological perspective. In EMNLP,
pages 1140?1150, 2010.
Charles Bazerman. Shaping Written Knowledge:
77
The Genre and Activity of the Experimental Ar-
ticle in Science. University of Wisconsin Press,
Madison, Wis., 1988.
Beata Beigman Klebanov, Eyal Beigman, and
Daniel Diermeier. Vocabulary choice as an indi-
cator of perspective. In ACL Short Papers, pages
253?257, Stroudsburg, PA, USA, 2010. Associa-
tion for Computational Linguistics.
Robert D. Benford and David A. Snow. Framing
processes and social movements: An overview
and assessment. Annual Review of Sociology, 26:
611?639, 2000.
Richard Harvey Brown. Toward a democratic sci-
ence: Scientific narration and civic communica-
tion. Yale University Press, New Haven, 1998.
Dennis Chong and James N. Druckman. Framing
theory. Annual Review of Political Science, 10:
103?126, 2007.
Jeanne Fahnestock. Accommodating Science. Writ-
ten Communication, 3(3):275?296, 1986.
Jeanne Fahnestock. Preserving the figure: Consis-
tency in the presentation of scientific arguments.
Written Communication, 21(1):6?31, 2004.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
Working set selection using second order in-
formation for training support vector machines.
JMLR, 6:1889?1918, December 2005. ISSN
1532-4435.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra,
Ja?nos Csirik, and Gyo?rgy Szarvas. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12, 2010.
Joseph L. Fleiss. Statistical Methods for Rates
and Proportions. Wiley series in probability and
mathematical statistics. John Wiley & Sons, New
York, second edition, 1981.
Maria Georgescul. A hedgehop over a max-margin
framework using hedge cues. In CONLL?
Shared-Task, pages 26?31, 2010.
G. Nigel Gilbert and Michael Joseph Mulkay. Open-
ing Pandora?s box: A sociological analysis of sci-
entists? discourse. CUP Archive, 1984.
Erving Goffman. Frame analysis: An essay on the
organization of experience. Harvard University
Press, 1974.
Alan G. Gross. The rhetoric of science. Harvard
University Press, Cambridge, Mass., 1990.
Michael Alexander Kirkwood Halliday and
James R. Martin. Writing science: Literacy and
discursive power. Psychology Press, London
[u.a.], 1993.
Eric A Hardisty, Jordan Boyd-Graber, and Philip
Resnik. Modeling perspective using adaptor
grammars. In EMNLP, pages 284?292, 2010.
Ken Hyland. Hedging in scientific research articles.
John Benjamins Pub. Co., Amsterdam; Philadel-
phia, 1998.
Napoleon K. Juanillo, Jr. Frames for Public Dis-
course on Biotechnology. In Genetically Modified
Food and the Consumer: Proceedings of the 13th
meeting of the National Agricultural Biotechnol-
ogy Council, pages 39?50, 2001.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. Derivation of new
readability formulas for navy enlisted personnel.
Technical report, National Technical Information
Service, Springfield, Virginia, February 1975.
George Lakoff. Hedges: A study in meaning cri-
teria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508, 1973.
Bruno Latour. Science in action: How to follow sci-
entists and engineers through society. Harvard
University Press, Cambridge, Mass., 1987.
Bruno Latour and Steve Woolgar. Laboratory life:
The social construction of scientific facts. Sage
Publications, Beverly Hills, 1979.
Beverly A. Lewin. Hedging: Form and function
in scientific research texts. In Genre Studies in
English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I, 1998.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In CoNLL, 2006.
Greg Myers. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35,
1989.
78
Greg Myers. Discourse studies of scientific popular-
ization: Questioning the boundaries. Discourse
Studies, 5(2):265?279, 2003.
Franc?oise Salager-Meyer. Scientific discourse and
contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37, 2011.
Dietram A. Scheufele. Framing as a theory of media
effects. Journal of Communication, 49(1):103?
122, 1999.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Teppo Varttala. Remarks on the communicative
functions of hedging in popular scientific and spe-
cialist research articles on medicine. English for
Specific Purposes, 18(2):177?200, 1999.
Teppo Varttala. Hedging in scientifically oriented
discourse: Exploring variation according to dis-
cipline and intended audience. PhD thesis, Uni-
versity of Tampere, 2001.
7 Appendix: pro- vs. anti-GMO dataset
Here, we describe the pro- vs. anti-GMO dataset we
collected, in the hopes that this dataset may prove
helpful in future research regarding the GMO de-
bates, even though we did not use the corpus in the
project described in this paper.
The second step of our overall procedure out-
lined in the introduction ? that step being to ex-
amine whether the use of hedging in pro-GMO arti-
cles corresponds with our inferred ?scientific? oc-
currence patterns more than that in anti-GMO ar-
ticles ? requires a collection of opinionated arti-
cles on GMOs. Our first attempt to use news me-
dia articles (LEXIS) was unsatisfying, as we found
many articles attempt to maintain a neutral position.
This led us to collect documents from more strongly
opinionated organizational websites such as Green-
peace (anti-GMO), Non GMO Project (anti-GMO),
or Why Biotechnology (pro-GMO). Articles were
collected from 20 pro-GMO and 20 anti-GMO or-
ganizational web sites.
After the initial collection of data, near-duplicates
and irrelevant articles were filtered through cluster-
ing, keyword searches and distance between word
vectors at the document level. We have collected
762 ?anti? documents and 671 ?pro? documents.
We reduced this to a 404 ?pro? and 404 ?con?
set as follows. Each retained ?document? con-
sists of only the first 200 words after excluding the
first 50 words of documents containing over 280
words. This was done to avoid irrelevant sections
such as Educators have permission to reprint arti-
cles for classroom use; other users, please contact
editor@actionbioscience.org for reprint permission.
See reprint policy.
The data will be posted online at
https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
79
