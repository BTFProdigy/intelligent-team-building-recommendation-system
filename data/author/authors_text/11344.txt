Proceedings of the 12th Conference of the European Chapter of the ACL, pages 16?23,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
On the use of Comparable Corpora to improve SMT performance
Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans, FRANCE
Sadaf.Abdul-Rauf@lium.univ-lemans.fr
Abstract
We present a simple and effective method
for extracting parallel sentences from
comparable corpora. We employ a sta-
tistical machine translation (SMT) system
built from small amounts of parallel texts
to translate the source side of the non-
parallel corpus. The target side texts are
used, along with other corpora, in the lan-
guage model of this SMT system. We
then use information retrieval techniques
and simple filters to create French/English
parallel data from a comparable news cor-
pora. We evaluate the quality of the ex-
tracted data by showing that it signifi-
cantly improves the performance of an
SMT systems.
1 Introduction
Parallel corpora have proved be an indispens-
able resource in Statistical Machine Translation
(SMT). A parallel corpus, also called bitext, con-
sists in bilingual texts aligned at the sentence level.
They have also proved to be useful in a range of
natural language processing applications like au-
tomatic lexical acquisition, cross language infor-
mation retrieval and annotation projection.
Unfortunately, parallel corpora are a limited re-
source, with insufficient coverage of many lan-
guage pairs and application domains of inter-
est. The performance of an SMT system heav-
ily depends on the parallel corpus used for train-
ing. Generally, more bitexts lead to better per-
formance. Current resources of parallel corpora
cover few language pairs and mostly come from
one domain (proceedings of the Canadian or Eu-
ropean Parliament, or of the United Nations). This
becomes specifically problematic when SMT sys-
tems trained on such corpora are used for general
translations, as the language jargon heavily used in
these corpora is not appropriate for everyday life
translations or translations in some other domain.
One option to increase this scarce resource
could be to produce more human translations, but
this is a very expensive option, in terms of both
time and money. In recent work less expensive but
very productive methods of creating such sentence
aligned bilingual corpora were proposed. These
are based on generating ?parallel? texts from al-
ready available ?almost parallel? or ?not much
parallel? texts. The term ?comparable corpus? is
often used to define such texts.
A comparable corpus is a collection of texts
composed independently in the respective lan-
guages and combined on the basis of similarity
of content (Yang and Li, 2003). The raw mate-
rial for comparable documents is often easy to ob-
tain but the alignment of individual documents is a
challenging task (Oard, 1997). Multilingual news
reporting agencies like AFP, Xinghua, Reuters,
CNN, BBC etc. serve to be reliable producers
of huge collections of such comparable corpora.
Such texts are widely available from LDC, in par-
ticular the Gigaword corpora, or over the WEB
for many languages and domains, e.g. Wikipedia.
They often contain many sentences that are rea-
sonable translations of each other, thus potential
parallel sentences to be identified and extracted.
There has been considerable amount of work on
bilingual comparable corpora to learn word trans-
lations as well as discovering parallel sentences.
Yang and Lee (2003) use an approach based on
dynamic programming to identify potential paral-
lel sentences in title pairs. Longest common sub
sequence, edit operations and match-based score
functions are subsequently used to determine con-
fidence scores. Resnik and Smith (2003) pro-
pose their STRAND web-mining based system
and show that their approach is able to find large
numbers of similar document pairs.
Works aimed at discovering parallel sentences
16
French: Au total, 1,634 million d?e?lecteurs doivent de?signer les 90 de?pute?s de la prochaine le?gislature
parmi 1.390 candidats pre?sente?s par 17 partis, dont huit sont repre?sente?s au parlement.
Query: In total, 1,634 million voters will designate the 90 members of the next parliament among 1.390
candidates presented by 17 parties, eight of which are represented in parliament.
Result: Some 1.6 million voters were registered to elect the 90 members of the legislature from 1,390
candidates from 17 parties, eight of which are represented in parliament, several civilian organisations
and independent lists.
French: ?Notre implication en Irak rend possible que d?autres pays membres de l?Otan, comme
l?Allemagne par exemple, envoient un plus gros contingent? en Afghanistan, a estime? M.Belka au cours
d?une confe?rence de presse.
Query: ?Our involvement in Iraq makes it possible that other countries members of NATO, such
as Germany, for example, send a larger contingent in Afghanistan, ?said Mr.Belka during a press
conference.
Result: ?Our involvement in Iraq makes it possible for other NATO members, like Germany for
example, to send troops, to send a bigger contingent to your country, ?Belka said at a press conference,
with Afghan President Hamid Karzai.
French: De son co?te?, Mme Nicola Duckworth, directrice d?Amnesty International pour l?Europe et
l?Asie centrale, a de?clare? que les ONG demanderaient a` M.Poutine de mettre fin aux violations des
droits de l?Homme dans le Caucase du nord.
Query: For its part, Mrs Nicole Duckworth, director of Amnesty International for Europe and Central
Asia, said that NGOs were asking Mr Putin to put an end to human rights violations in the northern
Caucasus.
Result: Nicola Duckworth, head of Amnesty International?s Europe and Central Asia department, said
the non-governmental organisations (NGOs) would call on Putin to put an end to human rights abuses
in the North Caucasus, including the war-torn province of Chechnya.
Figure 1: Some examples of a French source sentence, the SMT translation used as query and the poten-
tial parallel sentence as determined by information retrieval. Bold parts are the extra tails at the end of
the sentences which we automatically removed.
include (Utiyama and Isahara, 2003), who use
cross-language information retrieval techniques
and dynamic programming to extract sentences
from an English-Japanese comparable corpus.
They identify similar article pairs, and then, treat-
ing these pairs as parallel texts, align their sen-
tences on a sentence pair similarity score and use
DP to find the least-cost alignment over the doc-
ument pair. Fung and Cheung (2004) approach
the problem by using a cosine similarity measure
to match foreign and English documents. They
work on ?very non-parallel corpora?. They then
generate all possible sentence pairs and select the
best ones based on a threshold on cosine simi-
larity scores. Using the extracted sentences they
learn a dictionary and iterate over with more sen-
tence pairs. Recent work by Munteanu and Marcu
(2005) uses a bilingual lexicon to translate some
of the words of the source sentence. These trans-
lations are then used to query the database to find
matching translations using information retrieval
(IR) techniques. Candidate sentences are deter-
mined based on word overlap and the decision
whether a sentence pair is parallel or not is per-
formed by a maximum entropy classifier trained
on parallel sentences. Bootstrapping is used and
the size of the learned bilingual dictionary is in-
creased over iterations to get better results.
Our technique is similar to that of (Munteanu
and Marcu, 2005) but we bypass the need of the
bilingual dictionary by using proper SMT transla-
tions and instead of a maximum entropy classifier
we use simple measures like the word error rate
(WER) and the translation error rate (TER) to de-
cide whether sentences are parallel or not. Using
the full SMT sentences, we get an added advan-
tage of being able to detect one of the major errors
of this technique, also identified by (Munteanu and
Marcu, 2005), i.e, the cases where the initial sen-
tences are identical but the retrieved sentence has
17
a tail of extra words at sentence end. We try to
counter this problem as detailed in 4.1.
We apply this technique to create a parallel cor-
pus for the French/English language pair using the
LDC Gigaword comparable corpus. We show that
we achieve significant improvements in the BLEU
score by adding our extracted corpus to the already
available human-translated corpora.
This paper is organized as follows. In the next
section we first describe the baseline SMT system
trained on human-provided translations only. We
then proceed by explaining our parallel sentence
selection scheme and the post-processing. Sec-
tion 4 summarizes our experimental results and
the paper concludes with a discussion and perspec-
tives of this work.
2 Baseline SMT system
The goal of SMT is to produce a target sentence
e from a source sentence f . Among all possible
target language sentences the one with the highest
probability is chosen:
e? = arg max
e
Pr(e|f) (1)
= arg max
e
Pr(f |e) Pr(e) (2)
where Pr(f |e) is the translation model and
Pr(e) is the target language model (LM). This ap-
proach is usually referred to as the noisy source-
channel approach in SMT (Brown et al, 1993).
Bilingual corpora are needed to train the transla-
tion model and monolingual texts to train the tar-
get language model.
It is today common practice to use phrases as
translation units (Koehn et al, 2003; Och and
Ney, 2003) instead of the original word-based ap-
proach. A phrase is defined as a group of source
words f? that should be translated together into a
group of target words e?. The translation model in
phrase-based systems includes the phrase transla-
tion probabilities in both directions, i.e. P (e?|f?)
and P (f? |e?). The use of a maximum entropy ap-
proach simplifies the introduction of several addi-
tional models explaining the translation process :
e? = arg maxPr(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (3)
The feature functions hi are the system mod-
els and the ?i weights are typically optimized to
maximize a scoring function on a development
SMT baseline
system
phrase
table
3.3G
4?gram
LM
Fr En
automatic
translations
En
words
words
275M
up to
Fr En
human translations
words
116M
up to
Figure 2: Using an SMT system used to translate
large amounts of monolingual data.
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty, and a target language
model.
The system is based on the Moses SMT
toolkit (Koehn et al, 2007) and constructed as fol-
lows. First, Giza++ is used to perform word align-
ments in both directions. Second, phrases and
lexical reorderings are extracted using the default
settings of the Moses SMT toolkit. The 4-gram
back-off target LM is trained on the English part
of the bitexts and the Gigaword corpus of about
3.2 billion words. Therefore, it is likely that the
target language model includes at least some of
the translations of the French Gigaword corpus.
We argue that this is a key factor to obtain good
quality translations. The translation model was
trained on the news-commentary corpus (1.56M
words)1 and a bilingual dictionary of about 500k
entries.2 This system uses only a limited amount
of human-translated parallel texts, in comparison
to the bitexts that are available in NIST evalua-
tions. In a different versions of this system, the
Europarl (40M words) and the Canadian Hansard
corpus (72M words) were added.
In the framework of the EuroMatrix project, a
test set of general news data was provided for the
shared translation task of the third workshop on
1Available at http://www.statmt.org/wmt08/
shared-task.html
2The different conjugations of a verb and the singular and
plural form of adjectives and nouns are counted as multiple
entries.
18
EN
SMT
FR
used as queries
per day articles
candidate sentence pairs parallel 
sentences
+?5 day articles
from English Gigaword
English
translations Gigaword
French
174M words
133M words
tail
removal
sentences with
extra words at ends
+
24.3M words
parallel 
number / table
comparison
      length  
removing
WER/TER
26.8M words
Figure 3: Architecture of the parallel sentence extraction system.
SMT (Callison-Burch et al, 2008), called new-
stest2008 in the following. The size of this cor-
pus amounts to 2051 lines and about 44 thousand
words. This data was randomly split into two parts
for development and testing. Note that only one
reference translation is available. We also noticed
several spelling errors in the French source texts,
mainly missing accents. These were mostly auto-
matically corrected using the Linux spell checker.
This increased the BLEU score by about 1 BLEU
point in comparison to the results reported in the
official evaluation (Callison-Burch et al, 2008).
The system tuned on this development data is used
translate large amounts of text of French Gigaword
corpus (see Figure 2). These translations will be
then used to detect potential parallel sentences in
the English Gigaword corpus.
3 System Architecture
The general architecture of our parallel sentence
extraction system is shown in figure 3. Start-
ing from comparable corpora for the two lan-
guages, French and English, we propose to trans-
late French to English using an SMT system as de-
scribed above. These translated texts are then used
to perform information retrieval from the English
corpus, followed by simple metrics like WER and
TER to filter out good sentence pairs and even-
tually generate a parallel corpus. We show that a
parallel corpus obtained using this technique helps
considerably to improve an SMT system.
We shall also be trying to answer the following
question over the course of this study: do we need
to use the best possible SMT systems to be able to
retrieve the correct parallel sentences or any ordi-
nary SMT system will serve the purpose ?
3.1 System for Extracting Parallel Sentences
from Comparable Corpora
LDC provides large collections of texts from mul-
tilingual news reporting agencies. We identified
agencies that provided news feeds for the lan-
guages of our interest and chose AFP for our
study.3
We start by translating the French AFP texts to
English using the SMT systems discussed in sec-
tion 2. In our experiments we considered only
the most recent texts (2002-2006, 5.5M sentences;
about 217M French words). These translations are
then treated as queries for the IR process. The de-
sign of our sentence extraction process is based on
the heuristic that considering the corpus at hand,
we can safely say that a news item reported on
day X in the French corpus will be most proba-
bly found in the day X-5 and day X+5 time pe-
riod. We experimented with several window sizes
and found the window size of ?5 days to be the
most accurate in terms of time and the quality of
the retrieved sentences.
Using the ID and date information for each sen-
tence of both corpora, we first collect all sentences
from the SMT translations corresponding to the
same day (query sentences) and then the corre-
sponding articles from the English Gigaword cor-
3LDC corpora LDC2007T07 (English) and LDC2006T17
(French).
19
pus (search space for IR). These day-specific files
are then used for information retrieval using a ro-
bust information retrieval system. The Lemur IR
toolkit (Ogilvie and Callan, 2001) was used for
sentence extraction. The top 5 scoring sentences
are returned by the IR process. We found no evi-
dence that retrieving more than 5 top scoring sen-
tences helped get better sentences. At the end of
this step, we have for each query sentence 5 po-
tentially matching sentences as per the IR score.
The information retrieval step is the most time
consuming task in the whole system. The time
taken depends upon various factors like size of the
index to search in, length of the query sentence
etc. To give a time estimate, using a ?5 day win-
dow required 9 seconds per query vs 15 seconds
per query when a ?7 day window was used. The
number of results retrieved per sentence also had
an impact on retrieval time with 20 results tak-
ing 19 seconds per query, whereas 5 results taking
9 seconds per query. Query length also affected
the speed of the sentence extraction process. But
with the problem at we could differentiate among
important and unimportant words as nouns, verbs
and sometimes even numbers (year, date) could be
the keywords. We, however did place a limit of
approximately 90 words on the queries and the in-
dexed sentences. This choice was motivated by the
fact that the word alignment toolkit Giza++ does
not process longer sentences.
A Krovetz stemmer was used while building the
index as provided by the toolkit. English stop
words, i.e. frequently used words, such as ?a? or
?the?, are normally not indexed because they are
so common that they are not useful to query on.
The stop word list provided by the IR Group of
University of Glasgow4 was used.
The resources required by our system are min-
imal : translations of one side of the comparable
corpus. We will be showing later in section 4.2
of this paper that with an SMT system trained on
small amounts of human-translated data we can
?retrieve? potentially good parallel sentences.
3.2 Candidate Sentence Pair Selection
Once we have the results from information re-
trieval, we proceed on to decide whether sentences
are parallel or not. At this stage we choose the
best scoring sentence as determined by the toolkit
4http://ir.dcs.gla.ac.uk/resources/
linguistic utils/stop words
and pass the sentence pair through further filters.
Gale and Church (1993) based their align program
on the fact that longer sentences in one language
tend to be translated into longer sentences in the
other language, and that shorter sentences tend to
be translated into shorter sentences. We also use
the same logic in our initial selection of the sen-
tence pairs. A sentence pair is selected for fur-
ther processing if the length ratio is not more than
1.6. A relaxed factor of 1.6 was chosen keeping
in consideration the fact that French sentences are
longer than their respective English translations.
Finally, we discarded all sentences that contain a
large fraction of numbers. Typically, those are ta-
bles of sport results that do not carry useful infor-
mation to train an SMT.
Sentences pairs conforming to the previous cri-
teria are then judged based on WER (Levenshtein
distance) and translation error rate (TER). WER
measures the number of operations required to
transform one sentence into the other (insertions,
deletions and substitutions). A zero WER would
mean the two sentences are identical, subsequently
lower WER sentence pairs would be sharing most
of the common words. However two correct trans-
lations may differ in the order in which the words
appear, something that WER is incapable of tak-
ing into account as it works on word to word ba-
sis. This shortcoming is addressed by TER which
allows block movements of words and thus takes
into account the reorderings of words and phrases
in translation (Snover et al, 2006). We used both
WER and TER to choose the most suitable sen-
tence pairs.
4 Experimental evaluation
Our main goal was to be able to create an addi-
tional parallel corpus to improve machine transla-
tion quality, especially for the domains where we
have less or no parallel data available. In this sec-
tion we report the results of adding these extracted
parallel sentences to the already available human-
translated parallel sentences.
We conducted a range of experiments by adding
our extracted corpus to various combinations of al-
ready available human-translated parallel corpora.
We experimented with WER and TER as filters to
select the best scoring sentences. Generally, sen-
tences selected based on TER filter showed better
BLEU and TER scores than their WER counter
parts. So we chose TER filter as standard for
20
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 22
 0  2  4  6  8  10  12  14  16
B
LE
U
 sc
or
e
French words for training [M]
newsbitexts only
TER filter
 WER 
Figure 4: BLEU scores on the Test data using an
WER or TER filter.
our experiments with limited amounts of human
translated corpus. Figure 4 shows this WER vs
TER comparison based on BLEU and TER scores
on the test data in function of the size of train-
ing data. These experiments were performed with
only 1.56M words of human-provided translations
(news-commentary corpus).
4.1 Improvement by sentence tail removal
Two main classes of errors common in such
tasks: firstly, cases where the two sentences share
many common words but actually convey differ-
ent meaning, and secondly, cases where the two
sentences are (exactly) parallel except at sentence
ends where one sentence has more information
than the other. This second case of errors can be
detected using WER as we have both the sentences
in English. We detected the extra insertions at the
end of the IR result sentence and removed them.
Some examples of such sentences along with tails
detected and removed are shown in figure 1. This
resulted in an improvement in the SMT scores as
shown in table 1.
This technique worked perfectly for sentences
having TER greater than 30%. Evidently these
are the sentences which have longer tails which
result in a lower TER score and removing them
improves performance significantly. Removing
sentence tails evidently improved the scores espe-
cially for larger data, for example for the data size
of 12.5M we see an improvement of 0.65 and 0.98
BLEU points on dev and test data respectively and
1.00 TER points on test data (last line table 1).
The best BLEU score on the development data
is obtained when adding 9.4M words of automat-
ically aligned bitexts (11M in total). This corre-
Limit Word BLEU BLEU TER
TER tail Words Dev Test Test
filter removal (M) data data data
0 1.56 19.41 19.53 63.17
10 no 1.58 19.62 19.59 63.11yes 19.56 19.51 63.24
20 no 1.7 19.76 19.89 62.49yes 19.81 19.75 62.80
30 no 2.1 20.29 20.32 62.16yes 20.16 20.22 62.02
40 no 3.5 20.93 20.81 61.80yes 21.23 21.04 61.49
45 no 4.9 20.98 20.90 62.18yes 21.39 21.49 60.90
50 no 6.4 21.12 21.07 61.31yes 21.70 21.70 60.69
55 no 7.8 21.30 21.15 61.23yes 21.90 21.78 60.41
60 no 9.8 21.42 20.97 61.46yes 21.96 21.79 60.33
65 no 11 21.34 21.20 61.02yes 22.29 21.99 60.10
70 no 12.2 21.21 20.84 61.24yes 21.86 21.82 60.24
Table 1: Effect on BLEU score of removing extra
sentence tails from otherwise parallel sentences.
sponds to an increase of about 2.88 points BLEU
on the development set and an increase of 2.46
BLEU points on the test set (19.53 ? 21.99) as
shown in table 2, first two lines. The TER de-
creased by 3.07%.
Adding the dictionary improves the baseline
system (second line in Table 2), but it is not nec-
essary any more once we have the automatically
extracted data.
Having had very promising results with our pre-
vious experiments, we proceeded onto experimen-
tation with larger human-translated data sets. We
added our extracted corpus to the collection of
News-commentary (1.56M) and Europarl (40.1M)
bitexts. The corresponding SMT experiments
yield an improvement of about 0.2 BLEU points
on the Dev and Test set respectively (see table 2).
4.2 Effect of SMT quality
Our motivation for this approach was to be able
to improve SMT performance by ?creating? paral-
lel texts for domains which do not have enough
or any parallel corpora. Therefore only the news-
21
total BLEU score TER
Bitexts words Dev Test Test
News 1.56M 19.41 19.53 63.17
News+Extracted 11M 22.29 21.99 60.10
News+dict 2.4M 20.44 20.18 61.16
News+dict+Extracted 13.9M 22.40 21.98 60.11
News+Eparl+dict 43.3M 22.27 22.35 59.81
News+Eparl+dict+Extracted 51.3M 22.47 22.56 59.83
Table 2: Summary of BLEU scores for the best systems on the Dev-data with the news-commentary
corpus and the bilingual dictionary.
 19
 19.5
 20
 20.5
 21
 21.5
 22
 22.5
 2  4  6  8  10  12  14
B
LE
U
 sc
or
e
French words for training [M]
news + extractedbitexts only
dev
 test
Figure 5: BLEU scores when using news-
commentary bitexts and our extracted bitexts fil-
tered using TER.
commentary bitext and the bilingual dictionary
were used to train an SMT system that produced
the queries for information retrieval. To investi-
gate the impact of the SMT quality on our sys-
tem, we built another SMT system trained on large
amounts of human-translated corpora (116M), as
detailed in section 2. Parallel sentence extrac-
tion was done using the translations performed by
this big SMT system as IR queries. We found
no experimental evidence that the improved au-
tomatic translations yielded better alignments of
the comaprable corpus. It is however interesting to
note that we achieve almost the same performance
when we add 9.4M words of autoamticallly ex-
tracted sentence as with 40M of human-provided
(out-of domain) translations (second versus fifth
line in Table 2).
5 Conclusion and discussion
Sentence aligned parallel corpora are essential for
any SMT system. The amount of in-domain paral-
lel corpus available accounts for the quality of the
translations. Not having enough or having no in-
domain corpus usually results in bad translations
for that domain. This need for parallel corpora,
has made the researchers employ new techniques
and methods in an attempt to reduce the dire need
of this crucial resource of the SMT systems. Our
study also contributes in this regard by employing
an SMT itself and information retrieval techniques
to produce additional parallel corpora from easily
available comparable corpora.
We use automatic translations of comparable
corpus of one language (source) to find the cor-
responding parallel sentence from the comparable
corpus in the other language (target). We only
used a limited amount of human-provided bilin-
gual resources. Starting with about a total 2.6M
words of sentence aligned bilingual data and a
bilingual dictionary, large amounts of monolin-
gual data are translated. These translations are
then employed to find the corresponding match-
ing sentences in the target side corpus, using infor-
mation retrieval methods. Simple filters are used
to determine whether the retrieved sentences are
parallel or not. By adding these retrieved par-
allel sentences to already available human trans-
lated parallel corpora we were able to improve the
BLEU score on the test set by almost 2.5 points.
Almost one point BLEU of this improvement was
obtained by removing additional words at the end
of the aligned sentences in the target language.
Contrary to the previous approaches as in
(Munteanu and Marcu, 2005) which used small
amounts of in-domain parallel corpus as an initial
resource, our system exploits the target language
side of the comparable corpus to attain the same
goal, thus the comparable corpus itself helps to
better extract possible parallel sentences. The Gi-
gaword comparable corpora were used in this pa-
per, but the same approach can be extended to ex-
22
tract parallel sentences from huge amounts of cor-
pora available on the web by identifying compara-
ble articles using techniques such as (Yang and Li,
2003) and (Resnik and Y, 2003).
This technique is particularly useful for lan-
guage pairs for which very little parallel corpora
exist. Other probable sources of comparable cor-
pora to be exploited include multilingual ency-
clopedias like Wikipedia, encyclopedia Encarta
etc. There also exist domain specific compara-
ble corpora (which are probably potentially par-
allel), like the documentations that are done in the
national/regional language as well as English, or
the translations of many English research papers in
French or some other language used for academic
proposes.
We are currently working on several extensions
of the procedure described in this paper. We will
investigate whether the same findings hold for
other tasks and language pairs, in particular trans-
lating from Arabic to English, and we will try to
compare our approach with the work of Munteanu
and Marcu (2005). The simple filters that we are
currently using seem to be effective, but we will
also test other criteria than the WER and TER. Fi-
nally, another interesting direction is to iterate the
process. The extracted additional bitexts could be
used to build an SMT system that is better opti-
mized on the Gigaword corpus, to translate again
all the sentence from French to English, to per-
form IR and the filtering and to extract new, po-
tentially improved, parallel texts. Starting with
some million words of bitexts, this process may
allow to build at the end an SMT system that
achieves the same performance than we obtained
using about 40M words of human-translated bi-
texts (news-commentary + Europarl).
6 Acknowledgments
This work was partially supported by the Higher
Education Commission, Pakistan through the
HEC Overseas Scholarship 2005 and the French
Government under the project INSTAR (ANR
JCJC06 143038). Some of the baseline SMT sys-
tems used in this work were developed in a coop-
eration between the University of Le Mans and the
company SYSTRAN.
References
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Third Workshop on SMT, pages 70?106.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and em. In Dekang
Lin and Dekai Wu, editors, EMNLP, pages 57?63,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Douglas W. Oard. 1997. Alternative approaches for
cross-language text retrieval. In In AAAI Sympo-
sium on Cross-Language Text and Speech Retrieval.
American Association for Artificial Intelligence.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Philip Resnik and Noah A. Smith Y. 2003. The web
as a parallel corpus. Computational Linguistics,
29:349?380.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In ACL.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Erhard Hinrichs and Dan
Roth, editors, ACL, pages 72?79.
Christopher C. Yang and Kar Wing Li. 2003. Auto-
matic construction of English/Chinese parallel cor-
pora. J. Am. Soc. Inf. Sci. Technol., 54(8):730?742.
23
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130?134,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
SMT and SPE Machine Translation Systems for WMT?09
Holger Schwenk and Sadaf Abdul-Rauf and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
schwenk,abdul,barrault@lium.univ-lemans.fr
Jean Senellart
SYSTRAN SA
92044 Paris La De?fense cedex, FRANCE
senellart@systran.fr
Abstract
This paper describes the development of
several machine translation systems for
the 2009 WMT shared task evaluation.
We only consider the translation between
French and English. We describe a sta-
tistical system based on the Moses de-
coder and a statistical post-editing sys-
tem using SYSTRAN?s rule-based system.
We also investigated techniques to auto-
matically extract additional bilingual texts
from comparable corpora.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labo-
ratory at the University of Le Mans (LIUM) for
the 2009 WMT shared task evaluation. This work
was performed in cooperation with the company
SYSTRAN. We only consider the translation be-
tween French and English (in both directions).
The main differences to the previous year?s system
(Schwenk et al, 2008) are as follows: better us-
age of SYSTRAN?s bilingual dictionary in the sta-
tistical system, less bilingual training data, addi-
tional language model training data (news-train08
as distributed by the organizers), usage of com-
parable corpora to improve the translation model,
and development of a statistical post-editing sys-
tem (SPE). These different components are de-
scribed in the following.
2 Used Resources
In the frame work of the 2009 WMT shared trans-
lation task many resources were made available.
The following sections describe how they were
used to train the translation and language models
of the systems.
2.1 Bilingual data
The latest version of the French/English Europarl
and news-commentary corpus were used. We re-
alized that the first corpus contains parts with for-
eign languages. About 1200 such lines were ex-
cluded.1 Additional bilingual corpora were avail-
able, namely the Canadian Hansard corpus (about
68M English words) and an UN corpus (about
198M English words). In several initial exper-
iments, we found no evidence that adding this
data improves the overall system and they were
not used in the final system, in order to keep
the phrase-table small. We also performed ex-
periments with the provided so-called bilingual
French/English Gigaword corpus (575M English
words in release 3). Again, we were not able
to achieve any improvement by adding this data
to the training material of the translation model.
These findings are somehow surprising since it
was eventually believed by the community that
adding large amounts of bitexts should improve
the translation model, as it is usually observed for
the language model (Brants et al, 2007).
In addition to these human generated bitexts,
we also integrated a high quality bilingual dictio-
nary from SYSTRAN. The entries of the dictio-
nary were directly added to the bitexts. This tech-
nique has the potential advantage that the dictio-
nary words could improve the alignments of these
words when they also appear in the other bitexts.
However, it is not guaranteed that multi-word ex-
pressions will be correctly aligned by GIZA++
and that only meaningful translations will actually
appear in the phrase-table. A typical example is
fire engine ? camion de pompiers, for which the
individual constituent words are not good trans-
lations of each other. The use of a dictionary to
improve an SMT system was also investigated by
1Lines 580934?581316 and 599839?600662.
130
EN
SMT
FR
used as queries
per day articles
candidate sentence pairs parallel 
sentences
+?5 day articles
from English Gigaword
English
translations Gigaword
French
174M words
133M words
tail
removal
sentences with
extra words at ends
+
9.3M words
parallel 
number / table
comparison
      length  
removing
WER
10.3M words
Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009).
(Brown et al, 1993).
In comparison to our previous work (Schwenk
et al, 2008), we also included all verbs in the
French subjonctif and passe? simple tense. In fact,
those tenses seem to be frequently used in news
material. In total about 10,000 verbs, 1,500 adjec-
tives/adverbs and more than 100,000 noun forms
were added.
2.2 Use of Comparable corpora
Available human translated bitexts such as the UN
and the Hansard corpus seem to be out-of domain
for this task, as mentioned above. Therefore, we
investigated a new method to automatically extract
and align parallel sentences from comparable in-
domain corpora. In this work we used the AFP
news texts since there are available in the French
and English LDC Gigaword corpora.
The general architecture of our parallel sentence
extraction system is shown in figure 1. We first
translate 174M words from French into English
using an SMT system. These English sentences
are then used to search for translations in the En-
glish AFP texts of the Gigaword corpus using in-
formation retrieval techniques. The Lemur toolkit
(Ogilvie and Callan, 2001) was used for this pur-
pose. Search was limited to a window of ?5 days
of the date of the French news text. The retrieved
candidate sentences were then filtered using the
word error rate with respect to the automatic trans-
lations. In this study, sentences with an error rate
below 32% were kept. Sentences with a large
length difference (French versus English) or con-
taining a large fraction of numbers were also dis-
carded. By these means, about 9M words of ad-
ditional bitexts were obtained. An improved ver-
sion of this algorithm using TER instead of the
word error rate is described in detail in (Rauf and
Schwenk, 2009).
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. We
realized that the news-train08 corpora contained
some foreign texts, in particular in German. We
tried to filter those lines using simple regular ex-
pressions. We also discarded lines with a large
fraction of numerical expressions. In addition,
LDC?s Gigaword collection, the Hansard corpus
and the UN corpus were used for both languages.
Finally, about 30M words crawled from the WEB
were used for the French LM. All this data pre-
dated the evaluation period.
2.4 Development data
All development was done on news-dev2009a and
news-dev2009b was used as internal test set. The
default Moses tokenization was used. All our
models are case sensitive and include punctuation.
The BLEU scores reported in this paper were cal-
culated with the NIST tool and are case sensitive.
3 Language Modeling
Language modeling plays an important role in
SMT systems. 4-gram back-off language models
(LM) were used in all our systems. The word list
contains all the words of the bitext used to train
the translation model and all words that appear at
least ten times in the news-train08 corpus. Sep-
arate LMs were build on each data source with
the SRI LM toolkit (Stolcke, 2002) and then lin-
early interpolated, optimizing the coefficients with
an EM procedure. The perplexities of these LMs
131
Corpus # Fr words Dev09a Dev09b Test09
SMT system
Eparl+NC 46.5M 22.44 22.38 25.60
Eparl+NC+dict 48.5M 22.60 22.55 26.01
Eparl+NC+dict+AFP 57.8M 22.82 22.63? 26.18
SPE system
SYSTRAN - 17.76 18.13 19.98
Eparl+NC 45.5M 22.84 22.59# 25.59
Eparl+NC+AFP 54.4M 22.72 21.96 25.40
Table 1: Case sensitive NIST BLEU scores for the French-English systems. ?NC? denotes the news-
commentary bitexts, ?dict? SYSTRAN?s bilingual dictionary and ?AFP? the automatically aligned news
texts (?=primary, #=contrastive system)
are given in Table 2. Adding the new news-train08
monolingual data had an important impact on the
quality of the LM, even when the Gigaword data
is already included.
Data French English
Vocabulary size 407k 299k
Eparl+news 248.8 416.7
+ LDC Gigaword 142.2 194.9
+ Hansard and UN 137.5 187.5
news-train08 alone 165.0 245.9
all 120.6 174.8
Table 2: Perplexities on the development data of
various language models.
4 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
translation process:
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to max-
imize a scoring function on a development set
(Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).2 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words. This previously
caused problems when adding the entries of the
bilingual dictionary to the bitexts.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses are tuned on news-dev2009a,
using the cmert tool. The basic architecture of
the system is identical to the one used in the
2008 WMT evaluation (Schwenk et al, 2008),
but we did not use two pass decoding and n-best
list rescoring with a continuous space language
model.
The results of the SMT systems are summarized
in the upper part of Table 1 and 3. The dictionary
and the additional automatically produced AFP bi-
texts achieved small improvements when translat-
ing from French to English. In the opposite trans-
lation direction, the systems that include the addi-
tional AFP texts exhibit a bad generalisation be-
havior. We provide also the performance of the
different systems on the official test set, calculated
after the evaluation. In most of the cases, the ob-
served improvements carry over on the test set.
5 Architecture of the SPE system
During the last years statistical post-editing sys-
tems have shown to achieve very competitive per-
formance (Simard et al, 2007; Dugast et al,
2007). The main idea of this techniques is to use
2The source is available at http://www.cs.cmu.
edu/
?
qing/
132
Corpus # En words Dev09a Dev09b Test09
SMT system
Eparl+NC 41.6M 21.89 21.78 23.80
Eparl+NC+dict 44.0M 22.28 22.35# 24.13
Eparl+NC+dict+AFP 51.7M 22.21 21.43 23.88
SPE system
SYSTRAN - 18.68 18.84 20.29
Eparl+NC 44.2M 23.03 23.15 24.36
Eparl+NC+AFP 53.3M 22.95 23.15? 24.62
Table 3: Case sensitive NIST BLEU scores for the English-French systems. ?NC? denotes the news-
commentary bitexts, ?dict? denotes SYSTRAN?s bilingual dictionary and ?AFP? the automatically
aligned news texts (?=primary, #=contrastive system)
an SMT system to correct the errors of a rule-
based translation system. In this work, SYSTRAN
server version 6, followed by an SMT system
based on Moses were used. The post-editing sys-
tems uses exactly the same language models than
the above described stand-alone SMT systems.
The translation model was trained on the Europarl,
the news-commentary and the extracted AFP bi-
texts. The results of these SPE systems are sum-
marized in the lower part of Table 1 and 3. SYS-
TRAN?s rule-based system alone already achieves
remarkable BLEU scores although it was not op-
timized or adapted to this task. This could be sig-
nificantly improved using statistical post-editing.
The additional AFP texts were not useful when
translating form French to English, but helped to
improve the generalisation behavior for the En-
glish/French systems.
When translating from English to French (Ta-
ble 3), the SPE system is clearly better than the
carefully optimized SMT system. Consequently,
it was submitted as primary system and the SMT
system as contrastive one.
6 Conclusion and discussion
We described the development of two comple-
mentary machine translation systems for the 2009
WMT shared translation task: an SMT and an SPE
system. The last one is based on SYSTRAN?s
rule-based system. Interesting findings of this re-
search include the fact that the SPE system out-
performs the SMT system when translating into
French. This system has also obtained the best
scores in the human evaluation.
With respect to the SMT system, we were
not able to improve the translation model by
adding large amounts of bitexts, although different
sources were available (Canadian Hansard, UN
or WEB data). Eventually these corpora are too
noisy or out-of-domain. On the other hand, the
integration of a high quality bilingual dictionary
was helpful, as well as the automatic alignment of
news texts from comparable corpora.
Future work will concentrate on the integration
of previously successful techniques, in particu-
lar continuous space language models and lightly-
supervised training (Schwenk, 2008). We also be-
lieve that the tokenization could be improved, in
particular for the French sources texts. Numbers,
dates and other numerical expressions could be
translated by a rule-based system.
System combination has recently shown to pro-
vide important improvements of translation qual-
ity. We are currently working on a combination of
the SMT and SPE system. It may be also interest-
ing to add a third (hierarchical) MT system.
7 Acknowledgments
This work has been partially funded by the French
Government under the project INSTAR (ANR
JCJC06 143038) and the by the Higher Education
Commission, Pakistan through the HEC Overseas
Scholarship 2005.
133
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP, pages 858?
867.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the
workshop on Human Language Technology, pages
202?205, Princeton, New Jersey.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Second Workshop on SMT,
pages 179?182.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In EACL, page to be published.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation sys-
tem. In Third Workshop on SMT, pages 119?122.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Second
Workshop on SMT, pages 203?206.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
134
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 46?54,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploiting Comparable Corpora with TER and TERp
Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans, FRANCE
Sadaf.Abdul-Rauf@lium.univ-lemans.fr
Abstract
In this paper we present an extension of a
successful simple and effective method for
extracting parallel sentences from com-
parable corpora and we apply it to an
Arabic/English NIST system. We exper-
iment with a new TERp filter, along with
WER and TER filters. We also report a
comparison of our approach with that of
(Munteanu and Marcu, 2005) using ex-
actly the same corpora and show perfor-
mance gain by using much lesser data.
Our approach employs an SMT system
built from small amounts of parallel texts
to translate the source side of the non-
parallel corpus. The target side texts are
used, along with other corpora, in the lan-
guage model of this SMT system. We then
use information retrieval techniques and
simple filters to create parallel data from
a comparable news corpora. We evaluate
the quality of the extracted data by show-
ing that it significantly improves the per-
formance of an SMT systems.
1 Introduction
Parallel corpora, a requisite resource for Statistical
Machine Translation (SMT) as well as many other
natural language processing applications, remain
a sparse resource due to the huge expense (human
as well as monetary) required for their creation.
A parallel corpus, also called bitext, consists in
bilingual texts aligned at the sentence level. SMT
systems use parallel texts as training material and
monolingual corpora for target language model-
ing. Though enough monolingual data is available
for most language pairs, it is the parallel corpus
that is a sparse resource.
The performance of an SMT system heavily
depends on the parallel corpus used for train-
ing. Generally, more bitexts lead to better perfor-
mance. The existing resources of parallel corpora
cover a few language pairs and mostly come from
one domain (proceedings of the Canadian or Eu-
ropean Parliament, or of the United Nations). The
language jargon used in such corpora is not very
well suited for everyday life translations or transla-
tions of some other domain, thus a dire need arises
for more parallel corpora well suited for everyday
life and domain adapted translations.
One option to increase this scarce resource
could be to produce more human translations, but
this is a very expensive option, in terms of both
time and money. Crowd sourcing could be an-
other option, but this has its own costs and thus
is not very practical for all cases. The world
wide web can also be crawled for potential ?par-
allel sentences?, but most of the found bilingual
texts are not direct translations of each other and
not very easy to align. In recent works less ex-
pensive but very productive methods of creating
such sentence aligned bilingual corpora were pro-
posed. These are based on generating ?parallel?
texts from already available ?almost parallel? or
?not much parallel? texts. The term ?comparable
corpus? is often used to define such texts.
A comparable corpus is a collection of texts
composed independently in the respective lan-
guages and combined on the basis of similarity of
content (Yang and Li, 2003). The raw material for
comparable documents is often easy to obtain but
the alignment of individual documents is a chal-
lenging task (Oard, 1997). Potential sources of
comparable corpora are multilingual news report-
ing agencies like AFP, Xinhua, Al-Jazeera, BBC
etc, or multilingual encyclopedias like Wikipedia,
Encarta etc. Such comparable corpora are widely
available from LDC, in particular the Gigaword
corpora, or over the WEB for many languages
and domains, e.g. Wikipedia. They often contain
many sentences that are reasonable translations of
46
each other. Reliable identification of these pairs
would enable the automatic creation of large and
diverse parallel corpora.
The ease of availability of these comparable
corpora and the potential for parallel corpus as
well as dictionary creation has sparked an interest
in trying to make maximum use of these compa-
rable resources, some of these works include dic-
tionary learning and identifying word translations
(Rapp, 1995), named entity recognition (Sproat
et al, 2006), word sense disambiguation (Kaji,
2003), improving SMT performance using ex-
tracted parallel sentences (Munteanu and Marcu,
2005), (Rauf and Schwenk, 2009). There has been
considerable amount of work on bilingual compa-
rable corpora to learn word translations as well
as discovering parallel sentences. Yang and Lee
(2003) use an approach based on dynamic pro-
gramming to identify potential parallel sentences
in title pairs. Longest common sub sequence, edit
operations and match-based score functions are
subsequently used to determine confidence scores.
Resnik and Smith (2003) propose their STRAND
web-mining based system and show that their ap-
proach is able to find large numbers of similar doc-
ument pairs.
Works aimed at discovering parallel sentences
include (Utiyama and Isahara, 2003), who use
cross-language information retrieval techniques
and dynamic programming to extract sentences
from an English-Japanese comparable corpus.
They identify similar article pairs, and then, treat-
ing these pairs as parallel texts, align their sen-
tences on a sentence pair similarity score and use
DP to find the least-cost alignment over the doc-
ument pair. Fung and Cheung (2004) approach
the problem by using a cosine similarity measure
to match foreign and English documents. They
work on ?very non-parallel corpora?. They then
generate all possible sentence pairs and select the
best ones based on a threshold on cosine simi-
larity scores. Using the extracted sentences they
learn a dictionary and iterate over with more sen-
tence pairs. Recent work by Munteanu and Marcu
(2005) uses a bilingual lexicon to translate some
of the words of the source sentence. These trans-
lations are then used to query the database to find
matching translations using information retrieval
(IR) techniques. Candidate sentences are deter-
mined based on word overlap and the decision
whether a sentence pair is parallel or not is per-
formed by a maximum entropy classifier trained
on parallel sentences. Bootstrapping is used and
the size of the learned bilingual dictionary is in-
creased over iterations to get better results.
Our technique is similar to that of (Munteanu
and Marcu, 2005) but we bypass the need of the
bilingual dictionary by using proper SMT transla-
tions and instead of a maximum entropy classifier
we use simple measures like the word error rate
(WER) and the translation edit rate (TER) to de-
cide whether sentences are parallel or not. We
also report an extension of our work (Rauf and
Schwenk, 2009) by experimenting with an addi-
tional filter TERp, and building a named entity
noun dictionary using the unknown words from
the SMT (section 5.2). TERp has been tried en-
couraged by the outperformance of TER in our
previous study on French-English. We have ap-
plied our technique on a different language pair
Arabic-English, versus French-English that we re-
ported the technique earlier on. Our use of full
SMT sentences, gives us an added advantage of
being able to detect one of the major errors of
these approaches, also identified by (Munteanu
and Marcu, 2005), i.e, the cases where the initial
sentences are identical but the retrieved sentence
has a tail of extra words at sentence end. We dis-
cuss this problem as detailed in section 5.1.
We apply our technique to create a parallel cor-
pus for the Arabic/English language pair. We
show that we achieve significant improvements
in the BLEU score by adding our extracted cor-
pus to the already available human-translated cor-
pora. We also perform a comparison of the data
extracted by our approach and that by (Munteanu
and Marcu, 2005) and report the results in Sec-
tion 5.3.
This paper is organized as follows. In the next
section we first describe the baseline SMT system
trained on human-provided translations only. We
then proceed by explaining our parallel sentence
selection scheme and the post-processing. Sec-
tion 5 summarizes our experimental results and
the paper concludes with a discussion and perspec-
tives of this work.
2 Task Description
In this paper, we consider the translation from
Arabic into English, under the same conditions as
the official NIST 2008 evaluation. The used bi-
47
texts include various news wire translations1 as
well as some texts from the GALE project.2 We
also added the 2002 to 2005 test data to the paral-
lel training data (using all reference translations).
This corresponds to a total of about 8M Arabic
words. Our baseline system is trained on these bi-
texts only.
We use the 2006 NIST test data as development
data and the official NIST 2008 test data as in-
ternal test set. All case sensitive BLEU scores
are calculated with the NIST scoring tool with re-
spect to four reference translations. Both data sets
include texts from news wires as well as news-
groups.
LDC provides large collections of monolingual
data, namely the LDC Arabic and English Giga-
word corpora. There are two text sources that do
exist in Arabic and English: the AFP and XIN col-
lection. It is likely that each corpora contains sen-
tences which are translations of the other. We aim
to extract those. We have used the XIN corpus
for all of our reported results and the collection
of the AFP and XIN for comparison with ISI. Ta-
ble 1 summarizes the characteristics of the corpora
used. Note that the English part is much larger
than the Arabic one (we found the same to be the
case for French-English AFP comparable corpora
that we used in our previous study). The number
of words are given after tokenization.
Source Arabic English
AFP 138M 527M
XIN 51M 140M
Table 1: Characteristics of the available compara-
ble Gigaword corpora for the Arabic-English task
(number of words).
3 Baseline SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f . It is today common practice to use phrases
as translation units (Koehn et al, 2003; Och and
Ney, 2003) and a log linear framework in order
to introduce several models explaining the transla-
tion process:
e? = argmax p(e|f)
1LDC2003T07, 2004E72, T17, T18, 2005E46 and
2006E25.
2LDC2005E83, 2006E24, E34, E85 and E92.
= argmax
e
{exp(?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to max-
imize a scoring function on a development set
(Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, Giza++ is used to perform word alignments
in both directions. Second, phrases and lexical re-
orderings are extracted using the default settings
of the Moses SMT toolkit. The target 4-gram
back-off language model is trained on the English
part of all bitexts as well as the whole English Gi-
gaword corpus.
4 System Architecture
The general architecture of our parallel sentence
extraction system is shown in figure 1. Starting
from comparable corpora for the two languages,
Arabic and English, we first translate Arabic to
English using an SMT system as described in the
above sections. These translated texts are then
used to perform information retrieval from the
English corpus, followed by simple metrics like
WER, TER or TERp to filter out good sentence
pairs and eventually generate a parallel corpus.
We show that a parallel corpus obtained using this
technique helps considerably to improve an SMT
system.
4.1 System for Extracting Parallel Sentences
from Comparable Corpora
We start by translating the Arabic XIN and AFP
texts to English using the SMT systems discussed
in section 2. In our experiments we considered
only the most recent texts (2001-2006, 1.7M sen-
tences; about 65.M Arabic words for XIN ). For
our experiments on effect on SMT quality we use
only the XIN corpus. We use the combination
of AFP and XIN for comparison of sentences ex-
tracted by our approach with that of (Munteanu
and Marcu, 2005). These translations are then
treated as queries for the IR process. The design
of our sentence extraction process is based on the
heuristic that considering the corpus at hand, we
48
ENSMT
used as queries per day articles candidate sentence pairs parallel sentences
+?5 day articlesfrom English Gigaword
English
translations 
tail
removal
sentences with
extra words at ends
+
parallel 
number / table
comparison      length  
removing
Arabic
comparable
corpus
AR
WER/TER/TERp
Figure 1: Architecture of the parallel sentence extraction system.
can safely say that a news item reported on day X
in the Arabic corpus will be most probably found
in the day X-5 and day X+5 time period. We ex-
perimented with several window sizes and found
the window size of is to be the most accurate in
terms of time and the quality of the retrieved sen-
tences. (Munteanu and Marcu, 2005) have also
worked with a ?5 day window.
Using the ID and date information for each sen-
tence of both corpora, we first collect all sentences
from the SMT translations corresponding to the
same day (query sentences) and then the corre-
sponding articles from the English Gigaword cor-
pus (search space for IR). These day-specific files
are then used for information retrieval using a ro-
bust information retrieval system. The Lemur IR
toolkit (Ogilvie and Callan, 2001) was used for
sentence extraction.
The information retrieval step is the most time
consuming task in the whole system. The time
taken depends upon various factors like size of the
index to search in, length of the query sentence
etc. To give a time estimate, using a ?5 day win-
dow required 9 seconds per query vs 15 seconds
per query when a ?7 day window was used. We
placed a limit of approximately 90 words on the
queries and the indexed sentences. This choice
was motivated by the fact that the word alignment
toolkit Giza++ does not process longer sentences.
A Krovetz stemmer was used while building the
index as provided by the toolkit. English stop
words, i.e. frequently used words, such as ?a? or
?the?, are normally not indexed because they are
so common that they are not useful to query on.
The stop word list provided by the IR Group of
University of Glasgow3 was used.
The resources required by our system are min-
imal : translations of one side of the comparable
corpus. It has already been demonstrated in (Rauf
and Schwenk, 2009) that when using translations
as queries, the quality of the initial SMT is not
a factor for better sentence retrieval and that an
SMT system trained on small amounts of human-
translated data can ?retrieve? potentially good par-
allel sentences.
4.2 Candidate Sentence Pair Selection
The information retrieval process gives us the po-
tential parallel sentences per query sentence, the
decision of their being parallel or not needs to be
made about them. At this stage we choose the
best scoring sentence as determined by the toolkit
and pass the sentence pair through further filters.
Gale and Church (1993) based their align program
on the fact that longer sentences in one language
tend to be translated into longer sentences in the
other language, and that shorter sentences tend to
be translated into shorter sentences. We initially
used the same logic in our selection of the candi-
date sentence pairs. However our observation was
that the filters that we use, WER, TER and TERp
implicitly place a penalty when the length differ-
3http://ir.dcs.gla.ac.uk/resources/
linguistic_utils/stop_words
49
ence between two sentences is too large. Thus us-
ing this inherent property, we did not apply any
explicit sentence length filtering.
The candidate sentences pairs are then judged
based on simple filters. Our choice of filters
in accordance to the task in consideration were
the WER (Levenshtein distance), Translation Edit
Rate (TER) and the relatively new Translation Edit
Rate plus (TERp). WER measures the number
of operations required to transform one sentence
into the other (insertions, deletions and substitu-
tions). A zero WER would mean the two sen-
tences are identical, subsequently lower WER sen-
tence pairs would be sharing most of the common
words. However two correct translations may dif-
fer in the order in which the words appear, some-
thing that WER is incapable of taking into ac-
count. This shortcoming is addressed by TER
which allows block movements of words and thus
takes into account the reorderings of words and
phrases in translation (Snover et al, 2006). TERp
is an extension of Translation Edit Rate and was
one of the top performing metrics at the NIST
Metric MATR workshop 4. It had the highest ab-
solute correlation, as measured by the Pearson cor-
relation coefficient, with human judgments in 9
of the 45 test conditions. TERp tries to address
the weaknesses of TER through the use of para-
phrases, morphological stemming, and synonyms,
as well as edit costs that are optimized to corre-
late better with various types of human judgments
(Snover et al, 2009). The TER filter allows shifts
if the two strings (the word sequence in the trans-
lated and the IR retrieved sentence) match exactly,
however TERp allows shifts if the words being
shifted are exactly the same, are synonyms, stems
or paraphrases of each other, or any such combi-
nation. This allows better sentence comparison
by incorporation of sort of linguistic information
about words.
5 Experimental evaluation
Our main goal was to be able to create an addi-
tional parallel corpus to improve machine transla-
tion quality, especially for the domains where we
have less or no parallel data available. In this sec-
tion we report the results of adding these extracted
parallel sentences to the already available human-
translated parallel sentences.
4http://www.itl.nist.gov/iad/mig/
/tests/metricsmatr/2008/
#words BLEU
Bitexts Arabic Eval06 Eval08
Baseline 5.8M 42.64 39.35
+WER-10 5.8M 42.73 39.70
+WER-40 7.2M 43.34 40.59
+WER-60 14.5M 43.95 41.20
+WER-70 20.4M 43.58 41.18
+TER-30 6.5M 43.41 40.08
+TER-50 12.5M 43.90 41.45
+TER-60 17.3M 44.30 41.73
+TER-75 24.1M 43.79 41.21
+TERp-10 5.8M 42.69 39.80
+TERp-40 10.2M 43.89 41.44
+TERp-60 20.8M 43.94 41.25
+TERp-80 27.7M 43.90 41.58
Table 2: Summary of BLEU scores for the best
systems selected based on various thresholds of
WER, TER and TERp filters
We conducted a range of experiments by adding
our extracted corpus to various combinations of
already available human-translated parallel cor-
pora. For our experiments on effect on SMT qual-
ity we use only the XIN extracted corpus. We
experimented with WER, TER and TERp as fil-
ters to select the best scoring sentences. Table 2
shows some of the scores obtained based on BLEU
scores on the Dev and test data as a function of
the size of the added extracted corpus. The name
of the bitext indicates the filter threshold used, for
example, TER-50 means sentences selected based
on TER filter threshold of 50. Generally, sen-
tences selected based on TER filter showed bet-
ter BLEU scores on NIST06 than their WER and
TERp counter parts up to almost 21M words. Also
for the same filter threshold TERp selected longer
sentences, followed by TER and then WER, this
fact is evident from table 2, where for the fil-
ter threshold of 60, TERp and TER select 20.8M
and 17.3 words respectively, whereas WER selects
14.5M words.
Figure 2 shows the trend obtained in function
of the number of words added. These experiments
were performed by adding our extracted sentences
to only 5.8M words of human-provided transla-
tions. Our best results are obtained when 11.5M
of our extracted parallel sentences based on TER
filter are added to 5.8M of News wire and gale par-
allel corpora. We gain an improvement of 1.66
BLEU points on NIST06 and 2.38 BLEU points
50
 41.5
 42
 42.5
 43
 43.5
 44
 44.5
 45
 5  10  15  20  25  30
BL
EU
 sc
ore
 on
 nis
t06
Arabic words for training [M]
baseline
TERTERpWER
 38.5
 39
 39.5
 40
 40.5
 41
 41.5
 42
 5  10  15  20  25  30
BL
 sc
ore
 on
 nis
t08
Arabic words for training [M]
baseline
TERTERpWER
Figure 2: BLEU scores on the NIST06 (Dev,
top) and NIST08 (test, bottom) data using an
WER,TER or TERp filter as a function of the num-
ber of extracted Arabic words added.
on NIST08 (TER-60 in table 2 ).
An interesting thing to notice in figure 2 is that
no filter was able to clearly outperform the others,
which is contradictory to our experiments with the
French-English language pair (Rauf and Schwenk,
2009), where the TER filter clearly outperformed
the WER filter. WER is worse than TER but less
evident here than for our previous experiments for
the French-English language pair. This perfor-
mance gain by using the TER filter for French-
English was our main motivation for trying TERp.
We expected TERp to get better results compared
to WER and TER, but TER filter seems the better
one among the three filters. Note that all condi-
tions in all the experiments were identical. This
gives a strong hint of language pair dependency,
making the decision of suitability of a particular
filter dependent on the language pair in considera-
tion.
5.1 Sentence tail removal
Two main classes of errors are known when ex-
tracting parallel sentences from comparable cor-
pora: firstly, cases where the two sentences share
many common words but actually convey differ-
ent meaning, and secondly, cases where the two
sentences are (exactly) parallel except at sentence
ends where one sentence has more information
than the other. This second case of errors can
be detected using WER as we have the advan-
tage of having both the sentences in English. We
detected the extra insertions at the end of the IR
result sentence and removed them. Some exam-
ples of such sentences along with tails detected
and removed are shown in figure 3. Since this
gives significant improvement in the SMT scores
we used it for all our extracted sentences (Rauf
and Schwenk, 2009). However, similar to our ob-
servations in the last section, the tails were much
shorter as compared to our previous experiments
with French-English, also most of the tails in this
Arabic-English data were of type as shown in last
line figure 3. This is a factor dependent on re-
porting agency and its scheme for reporting, i.e,
whether it reports an event independently in each
language or uses the translation from one language
to the other .
5.2 Dictionary Creation
In our translations, we keep the unknown words as
they are, i.e. in Arabic (normally a flag is used so
that Moses skips them). This enables us to build a
dictionary. Consider the case with translation with
one unknown word in Arabic, if all the other words
around align well with the English sentence that
we found with IR, we could conclude the trans-
lation of the unknown Arabic word, see figure 3
line 5. We were able to make a dictionary us-
ing this scheme which was comprised mostly of
proper nouns often not found in Arabic-English
dictionaries. Our proper noun dictionary com-
prises of about 244K words, some sample words
are shown in figure 4. Adding the proper nouns
found by this technique to the initial SMT sys-
tem should help improve translations for new sen-
tences, as these words were before unknown to the
system. However, the impact of addition of these
words on translation quality is to be evaluated at
the moment.
51
Arabic:                   	
 
      Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LIUM SMT Machine Translation System for WMT 2010
Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French ma-
chine translation systems for the 2010
WMT shared task evaluation. These sys-
tems were standard phrase-based statisti-
cal systems based on the Moses decoder,
trained on the provided data only. Most
of our efforts were devoted to the choice
and extraction of bilingual data used for
training. We filtered out some bilingual
corpora and pruned the phrase table. We
also investigated the impact of adding two
types of additional bilingual texts, ex-
tracted automatically from the available
monolingual data. We first collected bilin-
gual data by performing automatic trans-
lations of monolingual texts. The second
type of bilingual text was harvested from
comparable corpora with Information Re-
trieval techniques.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labora-
tory at the University of Le Mans (LIUM) for the
2010 WMT shared task evaluation. We only con-
sidered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Schwenk
et al, 2009) are as follows: restriction to the data
recommended for the workshop, usage of the (fil-
tered) French?English gigaword bitext, pruning of
the phrase table, and usage of automatic trans-
lations of the monolingual news corpus to im-
prove the translation model. We also used a larger
amount of bilingual data extracted from compara-
ble corpora than was done in 2009. These different
points are described in the rest of the paper, to-
gether with a summary of the experimental results
showing the impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used
to train the translation and language models of the
system.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations may be used
directly with the source texts to build additional
bitexts, or as queries of an Information Retrieval
(IR) system to extract new bitexts from compara-
ble corpora. In a second stage, these additional
bilingual data were incorporated to the system (see
Section 4 and Tables 1 and 2).
The latest version of the News-Commentary
(NC) corpus, of the Europarl (Eparl) corpus (ver-
sion 5), and of the United Nations (UN) corpus
were used. We also took as training data a sub-
set of the French?English Gigaword (109) cor-
pus. Since a significant part of the data was
crawled from the web, we thought that many sen-
tence pairs may be only approximate translations
of each other. We applied a lexical filter to dis-
card them. Furthermore, some sentences of this
corpus were extracted from web page menus and
are not grammatical. Although we could have
used a part of the menu items as a dictionary, for
simplicity we applied an n-gram language model
(LM) filter to remove all non-grammatical sen-
tences. Thanks to this filter, sentences out of the
language model domain (in this case, mainly the
news domain), may also have been discarded be-
cause they contain many unknown or unfrequent
n-grams. The lexical filter was based on the IBM
model 1 cost (Brown et al, 1993) of each side of
a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter
121
was trained on a corpus composed of Eparl, NC,
and UN data. The language model filter was an
n-gram LM cost of the target sentence (see Sec-
tion 3), normalised with respect to its length. This
filter was trained with all monolingual resources
available except the 109 data. We generated a first
subset, 1091, selecting sentence pairs with a lexi-
cal cost inferior to 4 and an LM cost inferior to
2.3. The corpus selected in this way contains 115
million words in the English side (out of 580 mil-
lion in the original corpus). Close to the evaluation
deadline we decided to generate a second corpus
(1092) by raising the LM cost threshold to 2.6. The
1092 corpus contains 232 million words on the En-
glish side (twice as much as in the 1091 corpus).
In the French side of the bilingual corpora, for
the French?English direction only, the contrac-
tions ?du? (?of the?), ?au? and ?aux? (?to the? singu-
lar and plural) were substituted by their expanded
forms (?de le?, ?a` le? and ?a` les?).
2.2 Use of Automatic Translations and
Comparable corpora
Available human translated bitexts such as the UN
corpus seem to be out-of domain for this task.
We used two types of automatically extracted re-
sources to adapt our system to the task domain.
First, we generated automatic translations of the
French News corpus provided (231M words), and
selected the sentences with a normalised transla-
tion cost (returned by the decoder) inferior to a
threshold. The resulting bitext has no new words
in the English side, since all words of the transla-
tion output come from the translation model, but
it contains new combinations (phrases) of known
words, and reinforces the probability of some
phrase pairs (Schwenk, 2008).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. This year
we used the AFP and APW news texts since there
are available in the French and English LDC Gi-
gaword corpora. The general architecture of our
parallel sentence extraction system is described in
detail by Abdul-Rauf and Schwenk (2009). We
first translated 91M words from French into En-
glish using our first stage SMT system. These En-
glish sentences were then used to search for trans-
lations in the English AFP and APW texts of the
Gigaword corpus using information retrieval tech-
niques. The Lemur toolkit (Ogilvie and Callan,
2001) was used for this purpose. Search was lim-
ited to a window of ?5 days of the date of the
French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER be-
low 65% for the French?English system and 75%
for the English?French system were kept. Sen-
tences with a large length difference (French ver-
sus English) or containing a large fraction of num-
bers were also discarded. By these means, about
15M words of additional bitexts were obtained to
include in the French?English system, and 21M
words to include in the English?French system.
Note that these additional bitexts do not depend
on the translation direction. The most suitable
amount of additional data was just different in
the French?English and English?French transla-
tion directions.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the devel-
opment and test periods were removed from the
Gigaword collections.
2.4 Development data
All development was done on news-test2008, and
newstest2009 was used as internal test set. For all
corpora except the French side of the bitexts used
to train the French?English system (see above),
the default Moses tokenization was used. How-
ever, we added abbreviations for the French tok-
enizer. All our models are case sensitive and in-
clude punctuation. The BLEU scores reported in
this paper were calculated with the multi-bleu.perl
tool and are case sensitive. The BLEU score
was one of metrics with the best correlation with
human ratings in last year evaluation (Callison-
Burch et al, 2009) for the French?English and
English?French directions.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
122
translation process:
e? = argmax
e
p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system mod-
els and the ?i weights are typically optimized to
maximize a scoring function on a development
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words.
Phrases and lexical reorderings are extracted
using the default settings of the Moses toolkit.
The parameters of Moses were tuned on news-
test2008, using the ?new? MERT tool. We repeated
the training process three times, each with a differ-
ent seed value for the optimisation algorithm. In
this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word
list contains all the words of the bitext used to
train the translation model and all words that ap-
pear at least ten times in the monolingual corpora.
Words of the monolingual corpora containing spe-
cial characters or sequences of uppercase charac-
ters were not included in the word list. Separate
LMs were build on each data source with the SRI
LM toolkit (Stolcke, 2002) and then linearly in-
terpolated, optimizing the coefficients with an EM
procedure. The perplexities of these LMs were
103.4 for French and 149.2 for English.
4 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 1 and 2, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
1The source is available at http://www.cs.cmu.
edu/?qing/
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is
not significant. The reverse is not true. Note that
most of the improvements shown in the tables are
small and not significant. However many of the
gains are cumulative and the sum of several small
gains makes a significant difference.
Phrase-table Pruning
We tried to prune the phrase-table as proposed by
Johnson et. al. (2007), and available in moses
(?sigtest-filter?). We used the ? ?  filter2. As
lines 3 and 4 of Table 1, and lines 3 and 4 of Ta-
ble 2 reveal, in addition to the reduction 43% of
the phrase-table, a small gain in BLEU score (0.15
and 0.11 respectively) was obtained with the prun-
ing.
Baseline French?English System
The first section of Table 1 (lines 1 to 5) shows re-
sults of the development of the baseline SMT sys-
tem, used to generate automatic translations. Al-
though being out-of-domain data, the introduction
of the UN corpus yields an improvement of one
BLEU point with respect to Eparl+NC. Adding the
1091 corpus, we gain 0.7 BLEU point more. Ac-
tually, we obtained the same score with the 1091
added directly to Eparl+NC (line 5). However, we
choose to include the UN corpus to generate trans-
lations to have a larger vocabulary. The system
highlighted in bold (line 4) is the one we choose
to generate our English translations.
Although no French translations were gener-
ated, we did similar experiments in the English?
French direction (lines 1 to 4 of Table 2). In this
direction, the 1091 corpus is still more valuable than
the UN corpus when added to Eparl+NC, but with
less difference in terms of BLEU score. In this di-
2The p-value of two-by-two contingency tables (describ-
ing the degree of association between a source and a target
phrase) is calculated with Fisher exact test. This probability
is interpreted as the probability of observing by chance an as-
sociation that is at least as strong as the given one, and hence
as its significance. An important special case of a table oc-
curs when a phrase pair occurs exactly once in the corpus,
and each of the component phrases occurs exactly once in its
side of the parallel corpus (1-1-1 phrase pairs). In this case
the negative log of the p-value is ? = logN (N is number of
sentence pairs in the corpus). ? ?  is the largest threshold
that results in all of the 1-1-1 phrase pairs being included.
123
rection, we obtain a gain by adding the UN corpus
to Eparl+NC+1091.
Filtering the 109 Corpus
Lines 5 to 7 of Table 1 show the impact of filtering
the 109 corpus. The system trained on the full 109
corpus added to Eparl+NC achieves a BLEU score
of 26.83. Substituting the full 109 corpus by 1091 (5
times smaller), i.e. using the first filtering settings,
we gain 0.13 BLEU point. Using 1092 instead of
1091, we gain another 0.16 BLEU point, that is 0.3
in total. With respect to not using the 109 data at
all (as we did last year), we gain 0.8 BLEU point.
Impact of the Additional Bitexts
With the baseline French?English SMT system
(see above), we translated the French News cor-
pus to generated an additional bitext (News). We
also translated some parts of the French LDC Gi-
gaword corpus, to serve as queries to our IR sys-
tem (see section 2.2). The resulting additional bi-
text is referred to as IR. Lines 8 to 13 of Table 1
and lines 6 to 12 of Table 2 summarize the system
development including the additional bitexts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate
the automatic translations, but with less than 30%
of the data. This holds in both translation direc-
tions. Adding the News corpus to a larger corpus,
such as Eparl+NC+1091, has less impact but still
yields some improvement: 0.15 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. Note
that the number of additional phrase-table entries
per additional running word is twice as high for
the News bitext than for the other corpora. For
example, with respect to Eparl+NC+UN+1091 (Ta-
ble 2), Eparl+NC+UN+1091+News has 56M more
words and 116M more entries in the phrase-table,
thus the ratio is more than 2. For all other cor-
pora, the ratio is equal to 1 or less. This is un-
expected, particularly in this case where the News
bitext has no new English vocabulary with respect
to the Eparl+NC+UN+1091 corpus, from which its
English side was generated.
With the IR additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the system trained on Eparl+NC+UN,
while the IR bitext is 10 times smaller than the
UN corpus. Added to Eparl+NC+1091+News, the
IR bitext allows gains of 0.13 and 0.2 BLEU point
respectively in the French?English and English?
French directions.
Comparing the systems trained on
Eparl+NC+1091 or Eparl+NC+10
9
2 to the sys-
tems trained on the same corpora plus News+IR,
we can estimate the cumulative impact of the
additional bitexts. The gain is around 0.3 BLEU
point for French?English and around 0.5 BLEU
point for English?French.
Final System
In both translation directions our best system was
the one trained on Eparl+NC+1092+News+IR. We
further achieved small improvements (0.3 BLEU
point) by pruning the phrase-table (as above) and
by using a language model with no cut-off together
with increasing the beam size and/or the maxi-
mum number of translation table entries per input
phrase. Note that the English LM with cut-off had
a size of 6G, and the one with no cut-off had a
size of 29G. It was too much to fit in our 72G
machines so we pruned it with the SRILM prun-
ing tool down to a size of 19G. The French LM
with cut-off had a size of 2G and the one with
no cut-off had a size of 9G. These sizes corre-
spond to the binary format. Taking as example the
French?English direction, the running time went
from 8600 seconds for the system of line 14 (with
a threshold pruning coefficient of 0.4 and a LM
with cut-off) to 28200 seconds for the system sub-
mitted (with the LM without cut-off pruned by the
SRILM tool and a threshold pruning coefficient of
0.00001).
5 Conclusions and Further Work
We presented the development of our machine
translation system for the French?English and
English?French 2010 WMT shared task. Our sys-
tem was actually a standard phrase-based SMT
system based on the Moses decoder. Its original-
ity mostly lied in the choice and extraction of the
training data used.
We decided to use a part of the 109 French?
English corpus. We found this resource useful,
even without filtering. We nevertheless gained 0.3
BLEU point by selecting sentences based on an
IBM Model 1 filter and a language model filter.
We pruned the phrase table with the ?sigtest-
filter? distributed in Moses, yielding improve-
124
Bitext #Fr Words P-table Mem news-test2008 newstest2009
(M) size (M) (G) BLEU BLEU
1 Eparl+NC 52 66 19.3 22.80 (0.03) 25.31 (0.2)
2 Eparl+NC+UN 275 250 22.8 23.38 (0.1) 26.30 (0.2)
3 Eparl+NC+UN+1091 406 376 25.1 23.81 (0.05) 27.0 (0.2)
4 Eparl+NC+UN+1091 pruned 406 215 21.4 23.96 (0.1) 27.15 (0.18)
5 Eparl+NC+1091 183 198 22.1 23.83 (0.07) 26.96 (0.04)
6 Eparl+NC+1092 320 319 24.1 23.95 (0.03) 27.12 (0.1)
7 Eparl+NC+109 733 580 29.5 23.65 (0.09) 26.83 (0.2)
8 Eparl+NC+News 111 188 19.5 23.46 (0.1) 26.95 (0.2)
9 Eparl+NC+1091+News 242 317 22.5 23.77 (0.04) 27.11 (0.04)
10 Eparl+NC+IR 68 78 19.5 22.97 (0.03) 26.20 (0.1)
11 Eparl+NC+News+IR 127 198 20.1 23.62 (0.01) 27.04 (0.06)
12 Eparl+NC+1091+News+IR 258 327 22.8 23.75 (0.05) 27.24 (0.05)
13 Eparl+NC+1092+News+IR 395 441 24.4 23.87 (0.03) 27.43 (0.08)
14 Eparl+NC+1092+News+IR pruned 395 285 62.5 24.04 27.72
(+larger beam, +no-cutoff LM)
Table 1: French?English results: number of French words (in million), number of entries in the phrase-
table (in million), memory needed during decoding (in gigabytes) and BLEU scores in the development
(news-test2008) and internal test (newstest2009) sets for the different systems developped. The BLEU
scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.)
ments of 0.1 to 0.2 BLEU point for a 43% reduc-
tion of the phrase-table size.
We used additional bitexts extracted automati-
cally from the available monolingual corpora. The
first type of additional bitext is generated with au-
tomatic translations of the monolingual data with
a baseline SMT system. The second one is ex-
tracted from comparable corpora, with Informa-
tion Retrieval techniques. With the additional bi-
texts we gained 0.3 and 0.5 BLEU point for the
French?English and English?French systems, re-
spectively.
Next year we want to perform an improved se-
lection of parallel training data with re-sampling
techniques. We also want to use a continuous
space language model (Schwenk, 2007) in an n-
best list rescoring step after decoding. Finally, we
plan to train different types of systems (such as
a hierarchical SMT system and a Statistical Post-
Editing system) and combine their outputs with
the MANY open source system combination soft-
ware (Barrault, 2010).
Acknowledgments
This work has been partially funded by
the European Union under the EuroMatrix
Plus project ? Bringing Machine Transla-
tion for European Languages to the User ?
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720).
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, Athens, Greece.
Lo??c Barrault. 2010. MANY : Open source machine
translation system combination. Prague Bulletin
of Mathematical Linguistics, Special Issue on Open
Source Tools for Machine Translation, 93:147?155.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the ACL Fourth Workshop on Sta-
tistical Machine Translation, pages 1?28, Athens,
Greece.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
125
Bitext #En Words Phrase-table news-test2008 newstest2009
(M) size (M) BLEU BLEU
1 Eparl+NC+UN 242 258 24.21 (0.01) 25.29 (0.12)
2 Eparl+NC+1091 163 203 24.24 (0.06) 25.51 (0.13)
3 Eparl+NC+UN+1091 357 385 24.46 (0.08) 25.73 (0.20)
4 Eparl+NC+UN+1091 pruned 357 221 24.42 (0.1) 25.84 (0.05)
5 Eparl+NC+1092 280 330 24.43 (0.04) 25.68 (0.12)
6 Eparl+NC+News 103 188 24.27 (0.2) 25.70 (0.15)
7 Eparl+NC+1091+News 218 321 24.51 (0.05) 25.83 (0.05)
8 Eparl+NC+UN+1091+News 413 501 24.70 (0.1) 25.86 (0.14)
9 Eparl+NC+IR 69 81 24.14 (0.05) 25.17 (0.2)
10 Eparl+NC+News+IR 124 201 24.32 (0.12) 25.84 (0.17)
11 Eparl+NC+1091+News+IR 239 333 24.54 (0.1) 26.03 (0.15)
12 Eparl+NC+1092+News+IR 356 453 24.68 (0.04) 26.19 (0.05)
13 Eparl+NC+1092+News+IR pruned 356 293 25.06 26.53
(+larger beam, +no-cutoff LM)
Table 2: English?French results: number of English words (in million), number of entries in the phrase-
table (in million) and BLEU scores in the development (news-test2008) and internal test (newstest2009)
sets for the different systems developped. The BLEU scores and the number in parentheses are the
average and standard deviation over 3 values (see Section 3.)
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Holger Schwenk, Sadaf Abdul Rauf, Lo??c Barrault,
and Jean Senellart. 2009. SMT and SPE machine
translation systems for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 130?134, Athens, Greece. Association
for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
126
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 284?293,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Investigations on Translation Model Adaptation Using Monolingual Data
Patrik Lambert, Holger Schwenk, Christophe Servan and Sadaf Abdul-Rauf
LIUM, University of Le Mans
72085 Le Mans, France
FirstName.LastName@lium.univ-lemans.fr
Abstract
Most of the freely available parallel data to
train the translation model of a statistical ma-
chine translation system comes from very spe-
cific sources (European parliament, United
Nations, etc). Therefore, there is increasing
interest in methods to perform an adaptation
of the translation model. A popular approach
is based on unsupervised training, also called
self-enhancing. Both only use monolingual
data to adapt the translation model. In this pa-
per we extend the previous work and provide
new insight in the existing methods. We report
results on the translation between French and
English. Improvements of up to 0.5 BLEU
were observed with respect to a very com-
petitive baseline trained on more than 280M
words of human translated parallel data.
1 Introduction
Adaptation of a statistical machine translation sys-
tem (SMT) is a topic of increasing interest during
the last years. Statistical (n-gram) language models
are used in many domains and several approaches to
adapt such models were proposed in the literature,
for instance in the framework of automatic speech
recognition. Many of these approaches were suc-
cessfully used to adapt the language model of an
SMT system. On the other hand, it seems more chal-
lenging to adapt the other components of an SMT
system, namely the translation and reordering mod-
els. In this work we consider the adaptation of the
translation model of a phrase-based SMT system.
While rule-based machine translation rely on
rules and linguistic resources built for that purpose,
SMT systems can be developed without the need of
any language-specific expertise and are only based
on bilingual sentence-aligned data (?bitexts?) and
large monolingual texts. However, while monolin-
gual data are usually available in large amounts and
for a variety of tasks, bilingual texts are a sparse re-
source for most language pairs.
Current parallel corpora mostly come from one
domain (proceedings of the Canadian or European
Parliament, or of the United Nations). This is prob-
lematic when SMT systems trained on such corpora
are used for general translations, as the language jar-
gon heavily used in these corpora is not appropriate
for everyday life translations or translations in some
other domain. This problem could be attacked by ei-
ther searching for more in-domain training data, e.g.
by exploring comparable corpora or the WEB, or by
adapting the translation model to the task. In this
work we consider translation model adaptation with-
out using additional bilingual data. One can dis-
tinguish two types of translation model adaptation:
first, adding new source words or/and new transla-
tions to the model; and second, modifying the prob-
abilities of the existing model to better fit the topic
of the task. These two directions are complementary
and could be simultaneously applied. In this work
we focus on the second type of adaptation.
In this work, we focus on statistical phrase-
based machine translations systems (PBSMT), but
the methods could be also applied to hierarchical
systems. In PBSMT, the translation model is rep-
resented by a large list of all known source phrases
and their translations. Each entry is weighted us-
ing several probabilities, e.g. the popular Moses
284
system uses phrase translation probabilities in the
forward and backward direction, as well as lexical
probabilities in both directions. The entries of the
phrase-table are automatically extracted from sen-
tence aligned parallel data and they are usually quite
noisy. It is not uncommon to encounter several hun-
dreds, or even thousands of possible translations of
frequent source phrases. Many of these automati-
cally extracted translations are probably wrong and
are never used since their probabilities are (fortu-
nately) small in comparison to better translations.
Therefore, several approaches were proposed to fil-
ter these phrase-tables, reducing considerably their
size without any loss of the quality, or even achiev-
ing improved performance (Johnson et al, 2007).
Given these observations, adaptation of the trans-
lation model of PBSMT systems could be performed
by modifying the probability distribution of the ex-
isting phrases without necessarily modifying the en-
tries. The idea is of course to increase the prob-
abilities of translations that are appropriate to the
task and to decrease the probabilities of the other
ones. Ideally, we should also add new translations or
source phrase, but this seems to be more challenging
without any additional parallel data.
A common way to modify a statistical model is to
use a mixture model and to optimize the coefficients
to the adaptation domain. This was investigated in
the framework of SMT by several authors, for in-
stance for word alignment (Civera and Juan, 2007),
for language modeling (Zhao et al, 2004; Koehn
and Schroeder, 2007) and to a lesser extent for the
translation model (Foster and Kuhn, 2007; Chen et
al., 2008). This mixture approach has the advan-
tage that only few parameters need to be modified,
the mixture coefficients. On the other hand, many
translation probabilities are modified at once and it
is not possible to selectively modify the probabilities
of particular phrases.
Another direction of research is self-enhancing of
the translation model. This was first proposed by
Ueffing (2006). The idea is to translate the test data,
to filter the translations with help of a confidence
score and to use the most reliable ones to train an
additional small phrase table that is jointly used with
the generic phrase table. This could be also seen as a
mixture model with the in-domain component being
build on-the-fly for each test set. In practice, such
an approach is probably only feasible when large
amounts of test data are collected and processed at
once, e.g. a typical evaluation set up with a test set of
about 50k words. This method of self-enhancing the
translation model seems to be more difficult to apply
for on-line SMT, e.g. a WEB service, since often the
translation of some sentences only is requested. In
follow up work, this approach was refined (Ueffing
et al, 2007). Domain adaptation was also performed
simultaneously for the translation, language and re-
ordering model (Chen et al, 2008).
A somehow related approach was named lightly-
supervised training (Schwenk, 2008). In that work
an SMT system is used to translate large amounts of
monolingual texts, to filter them and to add them to
the translation model training data. This approach
was reported to obtain interesting improvements
in the translations quality (Schwenk and Senellart,
2009; Bertoldi and Federico, 2009). In comparison
to self enhancing as proposed by Ueffing (2006),
lightly-supervised training does not adapt itself to
the test data, but large amounts of monolingual train-
ing data are translated and a completely new model
is built. This model can be applied to any test data,
including a WEB service.
In this paper we propose to extend this approach
in several ways. First, we argue that the automatic
translations should not be performed from the source
to the target language, but in the opposite direction.
Second, we propose to use the segmentation ob-
tained during translation instead of performing word
alignments with GIZA++ (Och and Ney, 2003) of
the automatic translations. Finally, we propose to
enrich the vocabulary of the adapted system by de-
tecting untranslated words and automatically infer-
ring possible translations from the stemmed form
and the existing translations in the phrase table.
This paper is organized as follows. In the next
section we first describe our approach in detail. Sec-
tion 3 describes the considered task, the available
resources and the baseline PBSMT system. Results
are summarized in section 4 and the paper concludes
with a discussion and perspectives of this work.
2 Architecture of the approach
In this paper we propose to extend in several ways
the translation model adaptation by unsupervised
285
training as proposed by Schwenk (2008). In that
paper the authors propose to first build a PBSMT
system using all available human translated bi-
texts. This system is then used to translate large
amounts of monolingual data in the source language.
These automatic translations are filtered using the
sentence-length normalized log score of Moses, i.e.
the sum of the log-scores of all feature functions.
Putting a threshold on this score, only the most re-
liable translations are kept. This threshold was de-
termined experimentally. The automatic translations
were added to the parallel training data and a new
PBSMT model was build, performing the complete
pipeline of word alignment with GIZA++, phrase
extraction and scoring and tuning the system on
development data with MERT. In Schwenk (2009)
significant improvement were obtained by this ap-
proach when translating from Arabic to French.
2.1 Choice of the translation direction
First, we argue that it should be better to translate
monolingual data in the opposite translation direc-
tion of the system that we want to improve, i.e. from
the target into the source language. When translat-
ing large amounts of monolingual data, the system
will of course produce some wrong translations with
respect to choice of the vocabulary, to word order,
to morphology, etc. If we translate from the source
to the target language, these wrong translations are
added to the phrase table and may be used in future
translations performed by the adapted system. When
we add the automatic translations performed in the
opposite direction to the training data, the possibly
wrong translations will appear on the source side of
the entries in the adapted phrase table. PBSMT sys-
tems segment the source sentence according to the
available entries in the phrase table. Since the source
sentence is usually grammatically and semantically
correct, with the eventual exception of speech trans-
lation, it is unlikely that the wrong entries in the
phrase table will be ever used, e.g. phrases with bad
word choice or wrong morphology.
The question of the choice of the translation di-
rection was already raised by Bertoldi and Fed-
erico (2009). However, when data in the source
language is available they adapt only the translation
model (TM), while they adapt the TM and the lan-
guage model (LM) when data in the target language
is given. Of course the system with adapted LM is
much better, but this doesn?t prove that target mono-
lingual data are better than source monolingual data
for TM adaptation. In our paper, we use the same,
best, LM for all systems and we adapt the baseline
system with bitexts synthesized from source or tar-
get monolingual data.
2.2 Word alignment
In the work of Schwenk (2008), the filtered auto-
matic translation were added to the parallel training
data and the full pipeline to build a PBSMT sys-
tem was performed again, including word alignment
with GIZA++. Word alignment of bitexts of several
hundreds of millions of words is a very time con-
suming step. Therefore we propose to use the seg-
mentation into phrases and words obtained implic-
itly during the translation of the monolingual data
with the moses toolkit. These alignments are simply
added to the previously calculated alignments of the
human translated bitexts and a new phrase table is
built.
This new procedure does not only speed-up the
overall processing, but there are also investigations
that these alignments obtained by decoding are more
suitable to extract phrases than the symmetrized
word alignments produced by GIZA++. For in-
stance, Wuebker et al (2010) proposed to trans-
late the training data, using forced alignment and
a leave-one-out technique, and to use the induced
alignments to extract phrases. They have observed
improvements with respect to word alignment ob-
tained by GIZA++. On the other hand, Bertoldi and
Federico (2009) adapted an SMT system with au-
tomatic translations and trained the translation and
reordering models on the word alignment used by
moses. They reported a very small drop in per-
formance with respect to training word alignments
with GIZA++. Similar ideas were also used in pivot
translation. Bertoldi et al (2008) translated from the
pivot language to the source language to create par-
allel training data for the direct translation.
2.3 Treatment of unknown words
Statistical machine translation systems have some
trouble dealing with morphologically rich lan-
guages. It can happen, in function of the avail-
able training data, that translations of words are only
286
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
known in some forms and not in others. For in-
stance, for a user of MT technology it is quite dif-
ficult to understand why the system can translate
the French word ?je pense?1, but not ?tu penses?2.
There have been attempts in the literature to address
this problem, for instance by Habash (2008) to deal
with the Arabic language. It is actually possible to
automatically infer possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
stemmed les travaux sont fini
segment les travaux sont <n translation=?finished||ended?
proposed prob=?0.008||0.0001?>finis</n>
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
1I think
2you think
comparing the n-grams contained in the phrase ta-
ble and the source segment in order to detect iden-
tical words. Once the unknown word is selected,
we are looking for its stemmed form in the dictio-
nary and propose some translations for the unknown
word based on lexical score of the phrase table (see
Table 2 for some examples). The stemmer used is
the snowball stemmer3. Then the different hypothe-
sis are evaluated with the target language model.
This kind of processing could be done either be-
fore running the Moses decoder, i.e. using the
XML mark-up of Moses, or after decoding by post-
processing the untranslated words. In both cases, we
are unable to differentiate the possible translations
of the same source phrase with meaningful transla-
tion probabilities, and they won?t be added to the
phrase-table, nor put into a context with other words
that may trigger their use.
Therefore, we propose to use this technique to re-
place unknown words during the translation of the
monolingual data that we use to adapt the transla-
tion model. By these means, the automatically in-
duced translations of previously unknown morpho-
logical forms will be put into a context and actually
appear in the new adapted phrase-table. The corre-
sponding translation probabilities will be those cor-
responding to their frequency in the monolingual in-
domain data.
This procedure has been implemented, but we
were not able to obtain improvements in the BLEU
score. However, one can ask if automatic metrics,
evaluated on a test corpus of limited size, are the best
choice to judge this technique. In fact, in our setting
we have observed that less than 0.2% of the words
in the test set are unknown. We argue that the ability
to complement the phrase-table with many morpho-
logical forms of other wise known words, can only
improve the usability of SMT systems.
3 Task Description and resources
In this paper, we consider the translation of news
texts between French and English, in both direc-
tions. In order to allow comparisons, we used ex-
actly the same data as those allowed for the inter-
national evaluation organized in the framework of
the sixth workshop on SMT, to be held in Edinburgh
3http://snowball.tartarus.org/
287
Parallel data Size English/French French/English
[M words] Dev Test Dev Test
Eparl + nc 54 26.20 (0.06) 28.06 (0.2) 26.70 (0.06) 27.41 (0.2)
Eparl + nc + crawled1 168 26.84 (0.09) 29.08 (0.1) 27.96 (0.09) 28.20 (0.04)
Eparl + nc + crawled2 286 26.95 (0.04) 29.29 (0.03) 28.20 (0.03) 28.57 (0.1)
Eparl + nc + un 379 26.57 28.52 - -
Eparl + nc + crawled1 + un 514 26.87 28.99 - -
Eparl + nc + crawled2 + un 631 26.99 29.26 - -
Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News
Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts).
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
United Nations 344M 393M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 3: Available training data for the translation be-
tween French and English for the translation evaluation
at WMT?11 (number of words after tokenisation).
in July 2011. Preliminary results of this evaluation
are available on the Internet.4 Table 3 summarizes
the available training and development data. We op-
timized our systems on newstest2009 and used
newstest2010 as internal test set. For both cor-
pora, only one reference translations is available.
Scoring was performed with NIST?s implementation
of the BLEU score (?mt-eval? version 13).
3.1 Baseline system
The baseline system is a standard phrase-based SMT
system based on the the Moses SMT toolkit (Koehn
et al, 2007). It uses fourteen features functions
for translation, namely phrase and lexical translation
probabilities in both directions, seven features for
the lexicalized distortion model, a word and a phrase
penalty, and a target language model. It is con-
4http://matrix.statmt.org
structed as follows. First, word alignments in both
directions are calculated. We used a multi-threaded
version of the GIZA++ tool (Gao and Vogel, 2008).
Phrases and lexical reorderings are extracted using
the default settings of the Moses toolkit. All the bi-
texts were concatenated. The parameters of Moses
are tuned on the development data using the MERT
tool. For most of the runs, we performed three op-
timizations using different starting points and report
average results. English and French texts were to-
kenised using a modified version of the tools of the
Moses suite. Punctuation and case were preserved.
The language models were trained on all the avail-
able data, i.e. the target side of the bitexts, the whole
Gigaword corpus and the crawled monolingual data.
We build 4-gram back-off LMs with the SRI LM
toolkit using Modified Kneser-Ney and no cut-off
on all the n-grams. Past experience has shown that
keeping all n-grams slightly improves the perfor-
mance although this produces quite huge models
(10G and 30G of disk space for French and English
respectively).
Table 4 gives the baseline results using various
amounts of bitexts. Starting with the Europarl and
the News Commentary corpora, various amounts of
human translated data were added. The organizers
of the evaluation provide the so called 109 French-
English parallel corpus which contains almost 800
million words of data crawled from Canadian and
European Internet pages. Following works from the
2010 WMT evaluation (Lambert et al, 2010), we
filtered this data using IBM-1 probabilities and lan-
guage model scores to keep only the most reliable
translations. Two subsets were built with 115M and
232M English words respectively (using two differ-
288
alignment Dev Test
BLEU BLEU TER
giza 27.34 (0.01) 29.80 (0.06) 55.34 (0.06)
reused giza 27.40 (0.05) 29.82 (0.10) 55.30 (0.02)
reused moses 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
Table 5: Results for systems trained via different word alignment configurations. The values are the average over
3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation of these three
values. Translation was performed from English to French, adding 45M words of automatic translations (translated
from French to English) to the baseline system ?eparl+nc+crawled2?.
ent settings of the filter thresholds). They are re-
ferred to as ?crawled1? and ?crawled2? respectively.
Adding this data improved the BLEU score of al-
most 1 BLEU point (28.30 ? 29.27). This is our
baseline system to be improved by translation model
adaptation. Using the UN data gave no significant
improvement despite its huge size. This is probably
a typical example that it is not necessarily useful to
use all available parallel training data, in particular
when a very specific (out-of domain) jargon is used.
Consequently, the UN data was not used in the sub-
sequent experiments.
We were mainly working on the translation from
English to French. Therefore only one baseline sys-
tem was build for the reverse translation direction.
4 Experimental Evaluation
The system trained on Europarl, News Commen-
tary and the sub-sampled version of the 109 bitexts
(?eparl+nc+crawled2?, in the third line of Table 3),
was used to translate parts of the crawled news in
French and English. Statistics on the translated data
are given in Table 6.
We focused on the most recent data since the
time period of our development and test data was
end of 2008 and 2009 respectively. In the future
we will translate all the available monolingual data
and make it available to the community in order to
ease the widespread use of this kind of translation
model adaptation methods. These automatic trans-
lations were filtered using the sentence normalized
log-score of the decoder, as proposed by (Schwenk,
2008). However, we did not perform systematic ex-
periments to find the optimal threshold on this score,
but simply used a value which seems to be a good
compromise of quality and quantity of the transla-
tions. This gave us about 45M English words of
Corpus French (fe) English (ef)
available filtered available filtered
2009 92 31 121 45
2010 43 12 112 49
2011 8 2 15 6
total 219 45 177 100
Table 6: Monolingual data used to adapt the systems,
given in millions of English words. Under ?French (fe)?,
we indicated the number of translated English words
from French, and under ?English (ef)? we reported the
number of source English words translated into French.
Thus ?fe? and ?ef? refer respectively to French?English
and English?French translation direction of monolingual
data. In the experiments we used the 100M English?
French (ef) filtered monolingual data, as well as a 45M-
word subset (in order to have the same amount of data as
for French?English) and a 65M-word subset.
automatic translations from French, as well as the
translations into French of 100M English words, to
be used to adapt the baseline systems.
4.1 Word alignment
In order to build a phrase table with the translated
data, we re-used the word alignment obtained dur-
ing the translation with the moses toolkit. We com-
pared the system trained via these alignments to
the systems built by running GIZA++ on all the
data. When word alignments of the baseline corpus
(not adapted) are trained together with the translated
data, they could be affected by phrase pairs com-
ing from incorrect translations. To measure this ef-
fect, we trained an additional system, for which the
alignments of the baseline corpus are those trained
without the translated data. For the translated data,
we re-use the GIZA++ alignments trained on all the
data. Results for these three alignment configura-
289
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.20 (0.06) 28.06 (0.22) 56.85 (0.09)
news fe 45M 27.18 (0.09) 29.03 (0.07) 55.97 (0.07)
news ef 45M 26.15 (0.04) 28.44 (0.09) 56.56 (0.11)
Eparl + nc + crawled2 - 26.95 (0.04) 29.29 (0.03) 55.77 (0.19)
news fe 45M 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
news ef 45M 26.75 (0.04) 28.88 (0.10) 56.06 (0.05)
Table 7: Translation results of the English?French systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M refers to the number of English running words.
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.70 (0.06) 27.41 (0.24) 55.07 (0.17)
news fe 45M 27.47 (0.08) 27.77 (0.23) 54.84 (0.13)
news ef 45M 27.55 (0.05) 28.51 (0.10) 54.12 (0.09)
news ef 65M 27.58 (0.03) 28.70 (0.09) 54.06 (0.17)
news ef 100M 27.63 (0.06) 28.68 (0.06) 54.02 (0.06)
Eparl + nc + crawled2 - 28.20 (0.03) 28.54 (0.12) 54.17 (0.15)
news fe 45M 28.02 (0.11) 28.40 (0.10) 54.45 (0.06)
news ef 45M 28.24 (0.06) 28.93 (0.22) 53.90 (0.08)
news ef 65M 28.16 (0.19) 28.75 (0.06) 54.03 (0.14)
news ef 100M 28.28 (0.09) 28.96 (0.03) 53.79 (0.09)
Table 8: Translation results of the French?English systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M/65M/100M refers to the number of English running
words.
tions are presented in Table 5. In these systems
French sources and English translations (45 mil-
lion words) were added to the ?eparl+nc+crawled2?
baseline corpus. According to BLEU and TER met-
rics, reusing Moses alignments to build the adapted
phrase table has no significant impact on the system
performance. We repeated the experiment without
the 109 corpus and with the smaller selection of 109
(crawled1) and arrived to the same conclusion.
However, the re-use of Moses alignments saves time
and resources. On the larger baseline corpus, the
mGiza process lasted 46 hours with two jobs of 4
thread running and a machine with two Intel X5650
quad-core processors.
4.2 Choice of the translation direction
A second point under study in this work is the effect
of the translation direction of the monolingual data
used to adapt the translation model. Tables 7 and
8 present results for, respectively, English?French
and French?English systems adapted with news data
translated from English to French (ef) and French
to English (fe). The experiment was repeated with
two baseline corpora. The results show clearly
that target to source translated data are more use-
ful than source to target translated data. The im-
provement in terms of BLEU score due to the use of
target-to-source translated data instead of source-to-
target translated data ranges from 0.5 to 0.9 for the
French?English and English?French systems. For
instance, when translating from English to French
(Table 7), the baseline system ?eparl+nc? achieves
a BLEU score of 28.06 on the test set. This could
be improved to 29.03 using automatic translations
in the reverse direction (French to English), while
we only achieve a BLEU score of 28.44 when us-
ing automatic translation performed in the same di-
rection as the system to be adapted. The effect is
even clearer when we try to adapt the large system
290
?eparl+nc+crawled2?. Adding automatic transla-
tions translated from English-to-French did actually
lead to a lower BLEU score (29.29 ? 28.88) while
we observe an improvement of nearly 0.5 BLEU in
the other case.
With target-to-source translated news data,
the gain with respect to the baseline corpus
for English-French systems (Table 7) is nearly
1 BLEU for ?Eparl+nc? and 0.5 BLEU for
?Eparl+nc+crawled2?. With the same amount
of translated data (45 million English words),
approximately the same gains are observed in
French?English systems. Due to the larger avail-
ability of English news data, we were able to use
larger sets of target-to-source translated data for
French-English systems, as can be seen in Table 8.
With a bitext containing additionally 20 million
English words, we get a further improvement of
0.2 BLEU for ?Eparl+nc? (28.51 ? 28.70), but no
improvement for ?Eparl+nc+crawled2? (the BLEU
score is even lower, but the scores lie within the
error interval). No further gain on the test data is
achieved if we add again 35 million English words
(total of 100M words) to the system ?Eparl+nc?.
With the ?Eparl+nc+crawle2? baseline, no sig-
nificant improvement is observed if we adapt the
system with 100M words instead of only 45M.
4.3 Result analysis
To get more insight into what happens to the model
when we add the automatic translations, we cal-
culated some statistics of the phrase table, pre-
sented in Table 9. Namely, we calculated the
number of entries in the phrase table, the aver-
age number of translation options of each source
phrase, the average entropy for each source phrase,
the average source phrase length (in words) and
the average target phrase length. The entropy is
calculated over the probabilities of all translation
options for each source phrase. Comparing the
baseline with ?Eparl+nc? and the baseline with
?Eparl+nc+crawl2?, we can observe that the aver-
age number of translation options was nearly mul-
tiplied by 3 with the addition of 230 million words
of human translated bitexts. As a consequence the
average entropy was increased from 1.84 to 2.08.
On the contrary, adding 100 million words of in-
domain automatic translations, the average num-
ber of translation options increased by only 5%
for the ?Eparl+nc? baseline, and decreased for the
?Eparl+nc+crawl2? baseline. A decrease may occur
if new source phrases with less translation options
than the average are added. Furthermore, with the
addition of 45 million words of in-domain data, the
average entropy dropped from 1.84 to 1.33 or 1.60
for the ?Eparl+nc? baseline, and from 2.08 to 1.81 or
1.96 for the ?Eparl+nc+crawl2? baseline. With both
baselines, the more translations are added to the sys-
tem, the lower the entropy, although in some case
the number of translation options increases (this is
the case when we pass from 65M to 100M words
of synthetic data). These results illustrate the fact
that the automatic translations only reinforce some
probabilities in the model, with the subsequent de-
crease in entropy, while human translations add new
vocabulary. Note also that in the corpus using au-
tomatic translations, new words can only occur in
the source side. Thus when translating from French
to English, automatic translations from English to
French are expected to yield more translation op-
tions and a higher entropy than the automatic trans-
lations from French to English. This is what is ef-
fectively observed in Table 9.
5 Conclusion
Unsupervised training is widely used in other ar-
eas, in particular large vocabulary speech recogni-
tion. The statistical models in speech recognition
use a generative approach based on small units, usu-
ally triphones. Each triphone is modeled by a hid-
den Markov model and Gaussian mixture probabil-
ity distributions (plus many improvements like pa-
rameter tying etc). Many methods were developed
to adapt such models. The corresponding model
in statistical machine translation is the phrase table,
a long list of known words with their translations
and probabilities. It seems much more challenging
to adapt this kind of statistical model with unsuper-
vised training, i.e. monolingual data. Nevertheless,
we believe that unsupervised training can be also
very useful in SMT. To the best of our knowledge,
work in this area is very recent and only in its begin-
nings. This paper tries to give additional insights in
this promising method.
Our work is based on the approach initially pro-
291
baseline translated bitexts entries (M) translations entropy src size trg size
Eparl + nc - 7.16 83.83 1.84 1.80 2.81
news fe 45M 7.42 70.00 1.33 1.83 2.80
news ef 45M 8.24 81.58 1.60 1.86 2.79
news ef 65M 8.42 81.58 1.55 1.88 2.79
news ef 100M 9.21 85.93 1.54 1.90 2.79
Eparl + nc + crawl2 - 25.42 235.16 2.08 1.76 2.93
news fe 45M 25.54 217.21 1.81 1.77 2.93
news ef 45M 26.09 228.07 1.96 1.78 2.93
news ef 65M 26.21 226.45 1.91 1.78 2.93
news ef 100M 26.79 227.08 1.89 1.79 2.93
Table 9: Phrase table statistics for French?English systems augmented with bitexts built via automatic translations.
Only the entries useful to translate the development set were present in the considered phrase table.
posed in (Schwenk, 2008): build a first SMT sys-
tem, use it to translate large amounts of monolingual
data, filter the obtained translations, add them to the
bitexts and build a new system from scratch.
We proposed several extensions to this technique
which seem to improve the translations quality in
our experiments. First of all, we have observed that
it is clearly better to add automatically translated
texts to the translations model training data which
were translated from the target to the source lan-
guage. This seems to ensure that potentially wrong
translations are not used in the new model.
Second, we were able to skip the process of per-
forming word alignment of this additional parallel
data without any significant loss in the BLEU score.
Performing word alignments with GIZA++ can eas-
ily take several days when several hundred millions
of bitexts are available. Instead, we directly used the
word alignments produced by Moses when translat-
ing the monolingual data. This resulted in an appre-
ciable speed-up of the procedure, but has also inter-
esting theoretical aspects. Reusing the word align-
ment from the translation process is expected to re-
sult in a phrase extraction process that is more con-
sistent with the use of the phrases.
Finally, we outlined a method to automatically
add new translations without any additional parallel
training data. In fact, when translating from a mor-
phologically rich language to an easier one, in our
case from French to English, it is often possible to
infer the translations of unobserved morphological
forms of nouns, verbs or adjectives. This is obtained
by looking up the stemmed form in an automati-
cally constructed dictionary. This kind of approach
could be also applied to a classical PBSMT system,
by adding various forms to the phrase table, but it
is not obvious to come up with reasonable transla-
tions probabilities for these new entries. In our ap-
proach, the unknown word forms are processed in
large amounts of monolingual data and the induced
translations will appear in the context of complete
sentences. Wrong translations can be blocked by the
language model and the new translations can appear
in phrases of various lengths.
This paper provided a detailed experimental eval-
uation of these methods. We considered the trans-
lation between French and English using the same
data than was made available for the 2011 WMT
evaluation. Improvement of up to 0.5 BLEU were
observed with respect to an already competitive sys-
tem trained on more than 280M words of human
translated parallel data.
Acknowledgments
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004.) and the European Commis-
sion under the project EUROMATRIXPLUS (ICT-
2007.2.2-FP7-231720).
292
References
Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation. In Forth
Workshop on SMT, pages 182?189.
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,
and Roldano Cattoni. 2008. Phrase-based statistical
machine translation with pivot languages. In IWSLT,
pages 143?149.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In ACL, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Second Workshop on SMT, pages 177?180,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In EMNLP, pages 128?135.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In EMNLP, pages
967?975, Prague, Czech Republic.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Second Workshop on SMT, pages 224?227, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Workshop on SMT, pages 121?126.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an Arabic/French news trans-
lation system by lightly-supervised training. In MT
Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL, pages 25?32.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In IWSLT,
pages 174?181.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL, pages 475?484, Uppsala, Sweden, July.
Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Coling.
293
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
