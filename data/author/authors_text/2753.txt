MUP: The UIC Standoff Markup Tool
Michael Glass and Barbara Di Eugenio
CS. Dept. M/C 152, University of Illinois at Chicago
851. S. Morgan
Chicago, IL, 60607-7053
fmglass|bdieugeng@cs.uic.edu
Abstract
Recently developed markup tools for di-
alogue work are quite sophisticated and
require considerable knowledge and over-
head, but older tools do not support XML
standoff markup, the current annotation
style of choice. For the DIAG-NLP
project we have created a ?lightweight?
but modern markup tool that can be con-
figured and used by the working NLP re-
searcher.
Introduction
Speech and text corpora augmented with linguis-
tic annotations have become essential to everyday
NLP. In the realm of discourse-related annotation,
which we are interested in, linguistic annotation is
still mostly a manual effort. Thus, the availability of
coding tools that facilitate a human coder?s task has
become paramount. In this paper we present MUP,
a coding tool for standoff markup which is sophisti-
cated enough to allow for a variety of different mark-
ings to be applied, but which is also simple enough
to use that it does not require a sizable set up effort.
Other coding tools have been developed, and
some of them do in fact target discourse phenomena.
Tools specifically developed to code for discourse
phenomena include Nb (Flammia and Zue, 1995),
DAT (Allen and Core, 1997), MATE (McKelvie et
al., 2001), and the Alembic Workbench (Day et al,
1997). MUP differs from all of them because it is
standoff (contrary to Nb and DAT), allows tagging
of discontinuous constituents (contrary to Nb), and
is simple to set up and use (contrary to MATE).
We developed MUP within the DIAG-NLP
project (Di Eugenio et al, 2002), which is grafting
an NLG component onto a tutorial program written
in the VIVIDS (Munro, 1994) and DIAG (Towne,
1997) ITS authoring environment. MUP is targeted
to written or transcribed text. Phenomena such as
intonation contours and overlapping speech have no
opportunity to occur in our transcripts. Thus MUP
lacks features that spoken-language phenomena re-
quire of annotation tools, e.g. layers of annotation to
repair disfluencies, the representation of simultane-
ous speakers, and interfaces to speech tools.
Requirements and Alternatives
Our fundamental requirements for a markup tool are
that it 1) use standoff markup, 2) represent source
documents, annotations, and control files in sim-
ple XML, 3) have a simple graphical annotation in-
terface, 4) provide control over element names, at-
tribute names, and attribute values, enforcing consis-
tency in the final markup, and 5) can be configured
and employed by everyday computational linguists
without much effort or training.
In standoff markup (Thompson and McKelvie,
1997) the source text is inviolate and the annota-
tions are kept physically separate, usually in other
files. Annotatable items in the source text contain
labels, while the physically separate annotations re-
fer to these labels. Since annotations are themselves
labeled, complex structures of linked annotated con-
stitutents pointing to each other are representable.
Thompson and McKelvie list three advantages to
standoff markup: 1) the source document might be
read-only or unwieldy, 2) the annotations can devi-
ate from the strictly tree-structured hierarchies that
in-line XML demands, 3) annotation files can be
       Philadelphia, July 2002, pp. 37-41.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
distributed without distributing the source text. We
note a few more advantages of the standoff style: 4)
discontinuous segments of text can be combined in
a single annotation, 5) independent parallel coders
produce independent parallel annotation files, aiding
the determination of inter-coder reliability, 6) dif-
ferent annotation files can contain different layers
of information, 7) when the source text is regener-
ated from primary sources (for example, to incorpo-
rate more information) existing annotations are pre-
served.
Several years before Thompson and McKelvie,
the Tipster project evolved a similar standoff archi-
tecture for similar reasons (Grishman, 1995). Two
notable differences between Tipster and our own ar-
chitecture are that Tipster annotations refer to abso-
lute byte numbers within an unlabeled source docu-
ment file, and the Tipster architecture does not use
XML or SGML but instead supports its own class
library and internal representation. Other markup
projects, for example the Alembic Workbench, have
taken their cue from Tipster and implemented the
same standoff idea.
We specified XML for an annotation language be-
cause it is a lingua franca: the vocabulary is quite
commonly known, there is a host of XML process-
ing software, people can inspect it, and XML pro-
vides a rich ability to add attributes to elements.
The ATLAS.ti (Muhr, 2002) and the NUD*IST
annotation packages (QSR Corp., 2002) have both
been marketed for many years to researchers in
the ?soft? sciences for computer-assisted qualitative
analysis of texts. Their emphasis is on visually il-
lustrating the various codes attached to parts of the
document so the researcher can observe patterns.
Fairly complicated relationships between the anno-
tation tags can be created and visualized. An impres-
sive characteristic of these packages is that ordinary
researchers in non-technical fields can create their
own tags and commence annotating. However the
annotations in these packages point to sections of the
plain-text source document by absolute byte num-
ber. Thus the annotations are not readily available
for inspection or machine processing outside of the
programs? own interfaces and existing XML-tagged
data cannot readily become a source document for
further markup. These packages are not useful for
the analysis usually needed by NLP projects. It is
<xscript id="t12">
...
<tutor-resp id="t12_turn_3">
<w id="t12_t19">to</w>
<w id="t12_t20">see</w>
<w id="t12_t21">if</w>
<w id="t12_t22">the</w>
<w id="t12_t23">oil</w>
...
Figure 1: Source Document: ?to see if the oil...?
interesting to note that the most recent versions of
ATLAS.ti and NUD*IST have been adding the abil-
ity to import and export structured documents and
annotations, in XML and Rich Text Format respec-
tively.
At another extreme are markup tools like DAT,
an annotator for DAMSL (Allen and Core, 1997), a
dense multi-layered annotation scheme rich with at-
tributes. DAT is extremely convenient for the coder,
but it seems to require expert reprogramming when
DAMSL?s tag set changes.
A Taste of MUP
Running MUP requires a source document, a DTD-
like document describing a tag set, a style file con-
trolling how source text and annotations are dis-
played to the user, and optionally an existing annota-
tion file. The coder can then mark up the document
and save the results.
Source Document Figure 1 shows an extract from
a DIAG-NLP project source document. These
source document elements have special meaning:
word elements in line-wrapped text are tagged
<word> or <w>, and formatted lines of text are
tagged with <line> to be displayed without wrap-
ping. These elements must be labeled with XML
ID atributes to be the target of annotations. Other
XML elements may appear in the source document
as desired. All source document elements can be
optionally revealed or hidden for the coder, styled
according to the style file.
Tag Descriptions Each tag is an empty XML el-
ement, described in the tags description file by an
<!ATTLIST> declaration. We omit <!ELEMENT>
declarations as superfluous, so this file is not fully
a DTD. The pop-up dialogue the coder uses for
entering and editing attributes is driven by the
<!ATTLIST> description. Figure 2 illustrates the
description of a tag named indicator. The id,
idrefs, and comment attributes are standard for
all MUP markup tags. This description is inter-
preted, in part, as follows:
 idrefs will contain the IDs of ranges of tar-
get elements for this annotation, selected by the
coder by painting the source text with a cursor.
 The comment attribute, being CDATA, con-
tains arbitrary text typed by the coder.
 The directness, senseref and indi-
cator name attributes, being enumerated
lists, will present a drop-down list of values
to the coder. Notice the indicator name
is specified by entity substitution for conve-
nience.
Snapshot of MUP at Work Figure 3 shows a
snapshot of MUP at work. A control window has
a list of available markup tags plus it shows which
source document elements are displayed or hidden,
while the source document text is in a separate
window. The style file controls the display of the
source document by selecting which elements and
attributes to show/hide, picking colors for highlight-
ing, and inserting some bracketing text before and
after. In the snapshot, we have elected to display
the data from the <date> tag, the <consult>
element with its attributes but not the data, and the
<tutor-resp> element and data with a separa-
tor after it, while suppressing all other elements. We
have shown the text ?the blue box that lets you see if
the oil is flowing? being annotated as an indica-
tor via a pop-up dialogue.
Discussion
We believe a simplified easy-to-configure and run
tool will have wider applicability beyond our own
project. Other projects that manually code large
quantities of typed text, e.g. the RST dialogue
markup project (Marcu et al, 1999), have found
it desirable to create their own markup tools. The
CIRCSIM-Tutor project, with well over a hundred
transcripts of typed dialogue averaging an hour each,
has been coding in SGML (Freedman et al, 1998;
Kim, 1999) with general-purpose text editors.
The MATE workbench (McKelvie et al, 2001)
is a full-featured dialogue markup tool, however we
found it to be complex and difficult to use. We saw
an opportunity to borrow some of the ideas from
MATE and realize them with a simpler annotation
tool. MATE envisions three levels of user: coders,
researchers for whom the coding task is performed
and who need to view and manipulate the results,
and experts who are able to configure the software
(Carletta and Isard, 1999). It is this last group that
can perform the manipulations necessary for adding
new tags to the tag set and controlling how they
are displayed. MATE permits programmatic con-
trol over the coding interface by means of an XSL
style sheet customized for a particular application.
It is possible to split windows, intercept cursor op-
erations, provide linking operations between text in
different windows, and so on. This kind of flexibil-
ity is useful in annotated speech, for example in sep-
arately displaying and linking two speech streams
and having several related windows update simul-
taneously in response to coder actions. In our ex-
perience the MATE style sheets were quite difficult
to write and debug, and for our application we did
not need the flexibility, so we dispensed with them
and created our own, simpler, mechanism to control
the display of text. One consequence of the lessened
flexibility in MUP is that it presents a consistent cod-
ing interface using familiar single-dialog GUI con-
ventions.
Brooks (1975) estimates that the difference be-
tween a working program and a usable system with
ancillary utilities, shell scripts, etc. is three times the
original effort, and producing a distributable prod-
uct requires another factor of three effort. The core
MUP program works well enough that we have been
using it for several months. Our highest priority next
enhancement is to add a utility for inter-rater com-
parison, featuring some control over how parallel
annotations are compared (e.g., by selecting which
of the element?s attributes must match), and auto-
matically computing  statistics. MUP runs on So-
laris and Linux. We will make it available to re-
searchers as it matures.
Acknowledgments
This work is supported by grant N00014-00-1-0640 from the
ONR Cognitive, Neural and Biomolecular S&T Division and
<!ENTITY % indlist "( current-temp-gauge | sight-hole | water_temp_gauge ...)" >
<!ATTLIST indicator
id ID #required
idrefs IDREFS #required
comment CDATA #implied
indicator_name %indlist; ?unspecified?
directness ( explicit | implicit | summary | unspecified ) ?unspecified?
senseref ( sense | reference | unspecified ) ?unspecified? >
Figure 2: Description of indicator tag
Research Infrastructure grant EIA-9802090 from NSF. Thanks
also to Maarika Traat and Heena Raval, who have been most
helpful in the DIAG-NLP markup effort.
References
James Allen and Mark Core. 1997. Draft of
DAMSL: Dialog Act Markup in Several Lay-
ers. http://www.cs.rochester.edu/
research/cisd/resources/damsl/.
Frederick P. Brooks. 1975. The Mythical Man-Month:
Essays on Software Engineering. Addison-Wesley,
Reading, MA.
Jean Carletta and Amy Isard. 1999. The MATE an-
notation workbench: User requirements. In Marilyn
Walker, editor, Towards Standards and Tools for Dis-
course Tagging: Proceedings of the Workshop, Col-
lege Park MD, pages 11?17, New Brunswick, NJ. As-
sociation for Computational Linguistics.
David Day, John Aberdeen, Lynette Hirschmann, Robyn
Kozierok, Patricia Robinson, and Marc Vilain. 1997.
Mixed-initiative development of language processing
systems. In Fifth Conference on Applied Natural
Language Processing ANLP-97, pages 348?355, New
Brunswick, NJ. Association for Computational Lin-
guistics.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural language
generation for intelligent tutoring systems. In Second
International Natural Language Generation Confer-
ence INLG ?02, Harriman, NY. To appear.
Giovanni Flammia and Victor Zue. 1995. Empirical
evaluation of human performance and agreement in
parsing discourse constituents in spoken dialogue. In
Proc. Eurospeech-95, Fourth European Conference on
Speech Communication and Technology, pages 1965?
1968.
Reva Freedman, Yujian Zhou, Jung Hee Kim, Michael
Glass, and Martha W. Evens. 1998. SGML-based
markup as a step toward improving knowledge acqui-
sition for text generation. In AAAI Spring Symposium
on Applying Machine Learning to Discourse Process-
ing, pages 114?117.
Ralph Grishman. 1995. Tipster phase II architecture
design document (Tinman architecture). Technical
report, New York University. http://cs.nyu.
edu/pub/nlp/tipster/152.ps.
Jung Hee Kim. 1999. A manual for SGML markup
of tutoring transcripts. Technical report, CIRCSIM-
Tutor Project, Illinois Institute of Technology. http:
//www.cs.iit.edu/?circsim/.
Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a corpus
of discourse trees. In Marilyn Walker, editor, Towards
Standards and Tools for Discourse Tagging: Proceed-
ings of the Workshop, College Park MD, pages 48?57,
New Brunswick, NJ. Association for Computational
Linguistics.
Dave McKelvie, Amy Isard, Andreas Mengel,
Morten Braun M?ller, Michael Grosse, and Mar-
ion Klein. 2001. The MATE workbench ?
an annotation tool for XML coded speech cor-
pora. Speech Communication, 33(1?2):97?112.
http://www.cogsci.ed.ac.uk/?dmck/
Papers/speechcomm00.ps.
Thomas Muhr. 2002. ATLAS.ti home page. http:
//www.atlasti.de/atlasneu.html.
Allen Munro. 1994. Authoring interactive graphical
models. In T. de Jong, D. M. Towne, and H. Spada,
editors, The Use of Computer Models for Explication,
Analysis and Experiential Learning. Springer Verlag.
QSR Corp. 2002. NUD*IST home page. http://
www.qsr.com.au/.
Henry Thompson and David McKelvie. 1997. Hy-
perlink semantics for standoff markup of read-only
documents. In SGML Europe 97, Barcelona.
http://www.infoloom.com/gcaconfs/
WEB/TOC/barcelona97toc.HTM.
Douglas Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal of AI in Education, 8:262?283.
<   >
see if the oil is flowing.  If it
--end resp--
problem.
is, you have solved the
to the oil burner view and click
To see if the oil is flowing, go 
on the blue box that lets you
<consult prob="1" type="RU">
<tutor-resp id="t12_turn_3">
--end resp--
see if the oil is flowing properly
that was clogged.  Check to
You have replaced the oil filter
<tutor-resp id="t12_turn_2">
<consult prob="2" type= ...>
Document
Tags
Markup
Style
./tlogs2001/t12.xml
./styles_master.
./mglass/t12a.xml
./tags.dtd
QUITSAVERUN
File Selections
Markup Tags
indication
indicator
operationality
ru
related
aggregate_object
consult
date
diag-data
log
tutor-resp
Source Document Tags
t12.xmlMUP Standoff Markup
indicator
Nice cross-modal referring exp
the blue box t...e oil is flowing
oil-flow-indicator
explicit
sense
comment
senseref
directness
indicator_name
indicator_20id
OK CANCEL REMOVE
Edit Markup Tag
<   >  
03/23/01 F 10:00
actionlog
Figure 3: MUP in Action: Control Panel, Text Window, and Edit Tag Window
c? 2004 Association for Computational Linguistics
Squibs and Discussions
The Kappa Statistic: A Second Look
Barbara Di Eugenio? Michael Glass?
University of Illinois at Chicago Valparaiso University
In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating
intercoder agreement for tagging tasks. In this squib, we highlight issues that affect ? and that
the community has largely neglected. First, we discuss the assumptions underlying different
computations of the expected agreement component of ?. Second, we discuss how prevalence and
bias affect the ? measure.
In the last few years, coded corpora have acquired an increasing importance in ev-
ery aspect of human-language technology. Tagging for many phenomena, such as
dialogue acts (Carletta et al 1997; Di Eugenio et al 2000), requires coders to make
subtle distinctions among categories. The objectivity of these decisions can be as-
sessed by evaluating the reliability of the tagging, namely, whether the coders reach
a satisfying level of agreement when they perform the same coding task. Currently,
the de facto standard for assessing intercoder agreement is the ? coefficient, which
factors out expected agreement (Cohen 1960; Krippendorff 1980). ? had long been
used in content analysis and medicine (e.g., in psychiatry to assess how well stu-
dents? diagnoses on a set of test cases agree with expert answers) (Grove et al 1981).
Carletta (1996) deserves the credit for bringing ? to the attention of computational
linguists.
? is computed as P(A)? P(E)1? P(E) , where P(A) is the observed agreement among the
coders, and P(E) is the expected agreement, that is, P(E) represents the probabil-
ity that the coders agree by chance. The values of ? are constrained to the inter-
val [?1, 1]. A ? value of one means perfect agreement, a ? value of zero means
that agreement is equal to chance, and a ? value of negative one means ?perfect?
disagreement.
This squib addresses two issues that have been neglected in the computational
linguistics literature. First, there are two main ways of computing P(E), the expected
agreement, according to whether the distribution of proportions over the categories
is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel
and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different
conceptualizations of the problem. We believe the distinction between the two is often
glossed over because in practice the two computations of P(E) produce very similar
outcomes in most cases, especially for the highest values of ?. However, first, we
will show that they can indeed result in different values of ?, that we will call ?Co
(Cohen 1960) and ?S&C (Siegel and Castellan 1988). These different values can lead
to contradictory conclusions on intercoder agreement. Moreover, the assumption of
? Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail:
bdieugen@uic.edu.
? Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@
valpo.edu.
96
Computational Linguistics Volume 30, Number 1
equal distributions over the categories masks the exact source of disagreement among
the coders. Thus, such an assumption is detrimental if such systematic disagreements
are to be used to improve the coding scheme (Wiebe, Bruce, and O?Hara 1999).
Second, ? is affected by skewed distributions of categories (the prevalence prob-
lem) and by the degree to which the coders disagree (the bias problem). That is, for
a fixed P(A), the values of ? vary substantially in the presence of prevalence, bias, or
both.
We will conclude by suggesting that ?Co is a better choice than ?S&C in those studies
in which the assumption of equal distributions underlying ?S&C does not hold: the vast
majority, if not all, of discourse- and dialogue-tagging efforts. However, as ?Co suffers
from the bias problem but ?S&C does not, ?S&C should be reported too, as well as a third
measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993).
1. The Computation of P(E)
P(E) is the probability of agreement among coders due to chance. The literature de-
scribes two different methods for estimating a probability distribution for random
assignment of categories. In the first, each coder has a personal distribution, based
on that coder?s distribution of categories (Cohen 1960). In the second, there is one
distribution for all coders, derived from the total proportions of categories assigned
by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1
We now illustrate the computation of P(E) according to these two methods. We
will then show that the resulting ?Co and ?S&C may straddle one of the significant
thresholds used to assess the raw ? values.
The assumptions underlying these two methods are made tangible in the way
the data are visualized, in a contingency table for Cohen, and in what we will call
an agreement table for the others. Consider the following situation. Two coders2
code 150 occurrences of Okay and assign to them one of the two labels Accept or
Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Ac-
cept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels
as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contin-
gency table on the left (labeled Example 1) and the agreement table on the right. The
contingency table directly mirrors our description. The agreement table is an N ? m
matrix, where N is the number of items in the data set and m is the number of labels
that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij
is the number of codings of label j to item i. The agreement table in Figure 1 shows
that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through
125 as Ack by both coders, and 126 to 150 differ in their labels.
1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan?s to produce a
statistic called alpha. Krippendorff computes P(E) (called 1 ? De in his terminology) with a
sampling-without-replacement methodology. The computations of P(E) and of 1 ? De show that the
difference is negligible:
P(E) =
?
j
(
?
i
nij
Nk
)2
(Siegel and Castellan)
1 ? De =
?
j
(
?
i
nij
Nk
)(
[
?
i
nij
]
?1
Nk?1
)
(Krippendorff)
2 Both ?S&C (Scott 1955) and ?Co (Cohen 1960) were originally devised for two coders. Each has been
extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter
(1976). Thus, without loss of generality, our examples involve two coders.
97
Di Eugenio and Glass Kappa: A Second Look
Example 1
Coder 2
Coder 1 Accept Ack
Accept 70 25 95
Ack 0 55 55
70 80 150
Example 2
Coder 2
Coder 1 Accept Ack
Accept 70 15 85
Ack 10 55 65
80 70 150
Accept Ack
Okay1 2 0
...
Okay70 2 0
Okay71 0 2
...
Okay125 0 2
Okay126 1 1
...
Okay150 1 1
165 135
Figure 1
Cohen?s contingency tables (left) and Siegel and Castellan?s agreement table (right).
Agreement tables lose information. When the coders disagree, we cannot recon-
struct which coder picked which category. Consider Example 2 in Figure 1. The two
coders still disagree on 25 occurrences of Okay. However, one coder now labels 10
of those as Accept and the remaining 15 as Ack, whereas the other labels the same
10 as Ack and the same 15 as Accept. The agreement table does not change, but the
contingency table does.
Turning now to computing P(E), Figure 2 shows, for Example 1, Cohen?s com-
putation of P(E) on the left, and Siegel and Castellan?s computation on the right. We
include the computations of ?Co and ?S&C as the last step. For both Cohen and Siegel
and Castellan, P(A) = 125/150 = 0.8333. The observed agreement P(A) is computed
as the proportion of items the coders agree on to the total number of items; N is the
number of items, and k the number of coders (N = 150 and k = 2 in our example).
Both ?Co and ?S&C are highly significant at the p = 0.5 ? 10?5 level (significance is
computed for ?Co and ?S&C according to the formulas in Cohen [1960] and Siegel and
Castellan [1988], respectively).
The difference between ?Co and ?S&C in Figure 2 is just under 1%, however, the
results of the two ? computations straddle the value 0.67, which for better or worse
has been adopted as a cutoff in computational linguistics. This cutoff is based on the
assessment of ? values in Krippendorff (1980), which discounts ? < 0.67 and allows
tentative conclusions when 0.67 ? ? < 0.8 and definite conclusions when ? ? 0.8.
Krippendorff?s scale has been adopted without question, even though Krippendorff
himself considers it only a plausible standard that has emerged from his and his
colleagues? work. In fact, Carletta et al (1997) use words of caution against adopting
Krippendorff?s suggestion as a standard; the first author has also raised the issue of
how to assess ? values in Di Eugenio (2000).
If Krippendorff?s scale is supposed to be our standard, the example just worked
out shows that the different computations of P(E) do affect the assessment of inter-
coder agreement. If less-strict scales are adopted, the discrepancies between the two
? computations play a larger role, as they have a larger effect on smaller values of ?.
For example, Rietveld and van Hout (1993) consider 0.20 < ? ? 0.40 as indicating fair
agreement, and 0.40 < ? ? 0.60 as indicating moderate agreement. Suppose that two
coders are coding 100 occurrences of Okay. The two coders label 40 occurrences as
Accept and 25 as Ack. The remaining 35 are labeled as Ack by one coder and as Accept
by the other (as in Example 6 in Figure 4); ?Co = 0.418, but ?S&C = 0.27. These two
values are really at odds.
98
Computational Linguistics Volume 30, Number 1
Assumption of different distributions among
coders (Cohen)
Step 1. For each category j, compute the overall
proportion pj,l of items assigned to j by each coder
l. In a contingency table, each row and column
total divided by N corresponds to one such pro-
portion for the corresponding coder.
pAccept,1 = 95/150, pAck,1 = 55/150,
pAccept,2 = 70/150, pAck,2 = 80/150
Assumption of equal distributions among coders
(Siegel and Castellan)
Step 1. For each category j, compute pj, the overall
proportion of items assigned to j. In an agreement
table, the column totals give the total counts for
each category j, hence:
pj =
1
Nk ?
?
i nij
pAccept = 165/300 = 0.55, pAck = 135/300 = 0.45
Step 2. For a given item, the likelihood of both
coders? independently agreeing on category j by
chance, is pj,1 ? pj,2.
pAccept,1 ? pAccept,2 = 95/150 ? 70/150 = 0.2956
pAck,1 ? pAck,2 = 55/150 ? 80/150 = 0.1956
Step 2. For a given item, the likelihood of both
coders? independently agreeing on category j by
chance is p2j .
p2Accept = 0.3025
p2Ack = 0.2025
Step 3. P(E), the likelihood of coders? accidentally
assigning the same category to a given item, is
?
j pj,1 ? pj,2 = 0.2956 + 0.1956 = 0.4912
Step 3. P(E), the likelihood of coders? accidentally
assigning the same category to a given item, is
?
j p
2
j = 0.3025 + 0.2025 = 0.5050
Step 4.
?Co= (0.8333 ? 0.4912)/(1 ? 0.4912) =
.3421/.5088=0.6724
Step 4.
?S&C= (0.8333 ? 0.5050)/(1 ? 0.5050) =
.3283/.4950 = 0.6632
Figure 2
The computation of P(E) and ? according to Cohen (left) and to Siegel and Castellan (right).
2. Unpleasant Behaviors of Kappa: Prevalence and Bias
In the computational linguistics literature, ? has been used mostly to validate cod-
ing schemes: Namely, a ?good? value of ? means that the coders agree on the cate-
gories and therefore that those categories are ?real.? We noted previously that assess-
ing what constitutes a ?good? value for ? is problematic in itself and that different
scales have been proposed. The problem is compounded by the following obvious
effect on ? values: If P(A) is kept constant, varying values for P(E) yield vary-
ing values of ?. What can affect P(E) even if P(A) is constant are prevalence and
bias.
The prevalence problem arises because skewing the distribution of categories in
the data increases P(E). The minimum value P(E) = 1/m occurs when the labels are
equally distributed among the m categories (see Example 4 in Figure 3). The maximum
value P(E) = 1 occurs when the labels are all concentrated in a single category. But
for a given value of P(A), the larger the value of P(E), the lower the value of ?.
Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100
occurrences of Okay, that is, P(A) = 0.9. However, ? ranges from ?0.048 to 0.80, and
from not significant to significant (the values of ?S&C for Examples 3 and 4 are the
same as the values of ?Co).3 The differences in ? are due to the difference in the relative
prevalence of the two categories Accept and Ack. In Example 3, the distribution is
skewed, as there are 190 Accepts but only 10 Acks across the two coders; in Example 4,
the distribution is even, as there are 100 Accepts and 100 Acks, respectively. These
results do not depend on the size of the sample; that is, they are not due to the fact
3 We are not including agreement tables for the sake of brevity.
99
Di Eugenio and Glass Kappa: A Second Look
Example 3
Coder 2
Coder 1 Accept Ack
Accept 90 5 95
Ack 5 0 5
95 5 100
P(A) = 0.90, P(E) = 0.905
?Co = ?S&C = ?0.048, p = 1
Example 4
Coder 2
Coder 1 Accept Ack
Accept 45 5 50
Ack 5 45 50
50 50 100
P(A) = 0.90, P(E) = 0.5
?Co = ?S&C = 0.80, p = 0.5 ? 10?5
Figure 3
Contingency tables illustrating the prevalence effect on ?.
Example 5
Coder 2
Coder 1 Accept Ack
Accept 40 15 55
Ack 20 25 45
60 40 100
P(A) = 0.65, P(E) = 0.52
?Co = 0.27, p = 0.005
Example 6
Coder 2
Coder 1 Accept Ack
Accept 40 35 75
Ack 0 25 25
40 60 100
P(A) = 0.65, P(E) = 0.45
?Co = 0.418, p = 0.5 ? 10?5
Figure 4
Contingency tables illustrating the bias effect on ?Co.
Example 3 and Example 4 are small. As the computations of P(A) and P(E) are based
on proportions, the same distributions of categories in a much larger sample, say,
10,000 items, will result in exactly the same ? values. Although this behavior follows
squarely from ??s definition, it is at odds with using ? to assess a coding scheme.
From both Example 3 and Example 4 we would like to conclude that the two coders
are in substantial agreement, independent of the skewed prevalence of Accept with
respect to Ack in Example 3. The role of prevalence in assessing ? has been subject
to heated discussion in the medical literature (Grove et al 1981; Berry 1992; Goldman
1992).
The bias problem occurs in ?Co but not ?S&C. For ?Co, P(E) is computed from
each coder?s individual probabilities. Thus, the less two coders agree in their overall
behavior, the fewer chance agreements are expected. But for a given value of P(A),
decreasing P(E) will increase ?Co, leading to the paradox that ?Co increases as the
coders become less similar, that is, as the marginal totals diverge in the contingency
table. Consider two coders coding the usual 100 occurrences of Okay, according to
the two tables in Figure 4. In Example 5, the proportions of each category are very
similar among coders, at 55 versus 60 Accept, and 45 versus 40 Ack. However, in
Example 6 coder 1 favors Accept much more than coder 2 (75 versus 40 occurrences)
and conversely chooses Ack much less frequently (25 versus 60 occurrences). In both
cases, P(A) is 0.65 and ?S&C is stable at 0.27, but ?Co goes from 0.27 to 0.418. Our
initial example in Figure 1 is also affected by bias. The distribution in Example 1
yielded ?Co = 0.6724 but ?S&C = 0.6632. If the bias decreases as in Example 2, ?Co
becomes 0.6632, the same as ?S&C.
3. Discussion
The issue that remains open is which computation of ? to choose. Siegel and
Castellan?s ?S&C is not affected by bias, whereas Cohen?s ?Co is. However, it is
100
Computational Linguistics Volume 30, Number 1
questionable whether the assumption of equal distributions underlying ?S&C is ap-
propriate for coding in discourse and dialogue work. In fact, it appears to us that it
holds in few if any of the published discourse- or dialogue-tagging efforts for which
? has been computed. It is, for example, appropriate in situations in which item i may
be tagged by different coders than item j (Fleiss 1971). However, ? assessments for
discourse and dialogue tagging are most often performed on the same portion of the
data, which has been annotated by each of a small number of annotators (between
two and four). In fact, in many cases the analysis of systematic disagreements among
annotators on the same portion of the data (i.e., of bias) can be used to improve the
coding scheme (Wiebe, Bruce, and O?Hara 1999).
To use ?Co but to guard against bias, Cicchetti and Feinstein (1990) suggest that ?Co
be supplemented, for each coding category, by two measures of agreement, positive
and negative, between the coders. This means a total of 2m additional measures, which
we believe are too many to gain a general insight into the meaning of the specific ?Co
value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability
be reported as three numbers: ?Co and two adjustments of ?Co, one with bias removed,
the other with prevalence removed. The value of ?Co adjusted for bias turns out to
be . . . ?S&C. Adjusted for prevalence, ?Co yields a measure that is equal to 2P(A) ? 1.
The results for Example 1 should then be reported as ?Co = 0.6724, ?S&C = 0.6632,
2P(A)?1 = 0.6666; those for Example 6 as ?Co = 0.418, ?S&C = 0.27, and 2P(A)?1 = 0.3.
For both Examples 3 and 4, 2P(A)? 1 = 0.8. Collectively, these three numbers appear
to provide a means of better judging the meaning of ? values. Reporting both ?
and 2P(A) ? 1 may seem contradictory, as 2P(A) ? 1 does not correct for expected
agreement. However, when the distribution of categories is skewed, this highlights
the effect of prevalence. Reporting both ?Co and ?S&C does not invalidate our previous
discussion, as we believe ?Co is more appropriate for discourse- and dialogue-tagging
in the majority of cases, especially when exploiting bias to improve coding (Wiebe,
Bruce, and O?Hara 1999).
Acknowledgments
This work is supported by grant
N00014-00-1-0640 from the Office of Naval
Research. Thanks to Janet Cahn and to the
anonymous reviewers for comments on
earlier drafts.
References
Allen, James and Mark Core. 1997. DAMSL:
Dialog act markup in several layers;
Coding scheme developed by the
participants at two discourse tagging
workshops, University of Pennsylvania,
March 1996, and Schlo? Dagstuhl,
February 1997. Draft.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Berry, Charles C. 1992. The ? statistic [letter
to the editor]. Journal of the American
Medical Association, 268(18):2513?2514.
Byrt, Ted, Janet Bishop, and John B. Carlin.
1993. Bias, prevalence, and kappa. Journal
of Clinical Epidemiology, 46(5):423?429.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The Kappa statistic.
Computational Linguistics, 22(2):249?254.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Lingustics, 23(1):13?31.
Cicchetti, Domenic V. and Alvan R.
Feinstein. 1990. High agreement but low
kappa: II. Resolving the paradoxes.
Journal of Clinical Epidemiology,
43(6):551?558.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Di Eugenio, Barbara. 2000. On the usage of
Kappa to evaluate agreement on coding
tasks. In LREC2000: Proceedings of the
Second International Conference on Language
Resources and Evaluation, pages 441?444,
Athens.
Di Eugenio, Barbara, Pamela W. Jordan,
Richmond H. Thomason, and Johanna D.
Moore. 2000. The agreement process: An
101
Di Eugenio and Glass Kappa: A Second Look
empirical investigation of human-human
computer-mediated collaborative
dialogues. International Journal of Human
Computer Studies, 53(6):1017?1076.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Goldman, Ronald L. 1992. The ? statistic
[letter to the editor (in reply)]. Journal of
the American Medical Association,
268(18):2513?2514.
Grove, William M., Nancy C. Andreasen,
Patricia McDonald-Scott, Martin B. Keller,
and Robert W. Shapiro. 1981. Reliability
studies of psychiatric diagnosis: Theory
and practice. Archives of General Psychiatry,
38:408?413.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Beverly Hills, CA.
Rietveld, Toni and Roeland van Hout. 1993.
Statistical Techniques for the Study of
Language and Language Behaviour. Mouton
de Gruyter, Berlin.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale
coding. Public Opinion Quarterly,
19:127?141.
Siegel, Sidney and N. John Castellan, Jr.
1988. Nonparametric statistics for the
behavioral sciences. McGraw Hill, Boston.
Wiebe, Janyce M., Rebecca F. Bruce, and
Thomas P. O?Hara. 1999. Development
and use of a gold-standard data set for
subjectivity classifications. In ACL99:
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 246?253, College Park, MD.
Latent Semantic Analysis for dialogue act classification
Riccardo Serafin
Computer Science
University of Illinois
Chicago, IL, USA
rseraf1@uic.edu
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@uic.edu
Michael Glass
Mathematics and Computer Science
Valparaiso University
Valparaiso, IN, USA
Michael.Glass@valpo.edu
Abstract
This paper presents our experiments in apply-
ing Latent Semantic Analysis (LSA) to dia-
logue act classification. We employ both LSA
proper and LSA augmented in two ways. We
report results on DIAG, our own corpus of tu-
toring dialogues, and on the CallHome Spanish
corpus. Our work has the theoretical goal of as-
sessing whether LSA, an approach based only
on raw text, can be improved by using addi-
tional features of the text.
1 Introduction
Dialogue systems need to perform dialog act classifica-
tion, in order to understand the role the user?s utterance
plays in the dialog (e.g., a question for information or a
request to perform an action), and to generate an appro-
priate next turn. In recent years, a variety of empirical
techniques have been used to train the dialogue act clas-
sifier (Reithinger and Maier, 1995; Stolcke et al, 2000;
Walker et al, 2001).
In this paper, we propose Latent Semantic Analysis
(LSA) as a method to train the dialogue act classifier.
LSA can be thought as representing the meaning of a
word as a kind of average of the meanings of all the pas-
sages in which it appears, and the meaning of a passage
as a kind of average of the meaning of all the words it
contains (Landauer et al, 1998). LSA learns from co-
occurrence of words in collections of texts. It builds a se-
mantic space where words and passages are represented
as vectors. Their similarity is measured by the cosine of
their contained angle in the semantic space. LSA is based
on Single Value Decomposition (SVD), a mathematical
technique that causes the semantic space to be arranged
so as to reflect the major associative patterns in the data,
and ignores the smaller, less important influences.
LSA has been successfully applied to many tasks: e.g,
to assess the quality of student essays (Foltz et al, 1999)
and to interpret the student?s input in an Intelligent Tutor-
ing system (Graesser et al, 2000). However, there is no
research on applying LSA to dialogue act classification.
LSA is an attractive method because it is relatively
straightforward to train and use. More importantly, al-
though it is a statistical theory, it has been shown to mimic
a number of aspects of human competence / performance
(Landauer et al, 1998). Thus, it appears to somehow cap-
ture and represent important components of meanings.
We also have a theoretical goal in investigating LSA.
A common criticism of LSA is that its ?bag of words? ap-
proach ignores any other linguistic information that may
be available, e.g. order and syntactic information: to
LSA, man bites dog is identical to dog bites man. We
suggest that an LSA semantic space can be built from the
co-occurrence of arbitrary textual features. We propose
to place in the bag of words other features that co-occur
in the same text. We are calling LSA augmented with
features ?FLSA? (for ?feature LSA?). The only relevant
prior work is (Wiemer-Hastings, 2001), that adds part of
speech tags and some syntactic information to LSA.
This paper describes the corpora and the methods we
used, and the results we obtained. To summarize, plain
LSA seems to perform well on large corpora and classi-
fication tasks. Augmented LSA seems to perform better
on smaller corpora and target classifications.
2 Corpora
We report experiments on two corpora, DIAG and Span-
ish CallHome.
DIAG is a corpus of computer mediated tutoring dia-
logues between a tutor and a student who is diagnosing a
fault in a mechanical system with the DIAG tutoring sys-
tem (Towne, 1997). The student?s input is via menu, the
tutor is in a different room and answers via a text window.
The DIAG corpus comprises 23 dialogues for a total of
607 different words and 660 dialogue acts. It has been an-
notated for a variety of features, including four dialogue
acts1 (Glass et al, 2002): problem solving, the tutor gives
problem solving directions; judgement, the tutor evalu-
ates the student?s actions or diagnosis; domain knowl-
edge, the tutor imparts domain knowledge; and other,
when none of the previous three applies.
The Spanish CallHome corpus (Levin et al, 1998;
Ries, 1999) comprises 128 unrestricted phone calls in
Spanish, for a total of 12066 different words and 44628
dialogue acts. The Spanish CallHome annotation aug-
ments a basic tag such as statement along several dimen-
sions, such as whether the statement describes a psycho-
logical state of the speaker. This results in 232 differ-
ent dialogue act tags, many with very low frequencies.
In this sort of situations, tag categories are often col-
lapsed when running experiments so as to get meaningful
frequencies (Stolcke et al, 2000). In CallHome37, we
collapsed statements and backchannels, obtaining 37 dif-
ferent tags. CallHome37 maintains some subcategoriza-
tions, e.g. whether a question is yes/no or rhetorical. In
CallHome10, we further collapse these categories. Call-
Home10 is reduced to 8 dialogue acts proper (eg state-
ment, question, answer) plus the two tags ??%?? for
abandoned sentences and ??x?? for noise.
3 Methods
We have experimented with four methods: LSA proper,
which we call plain LSA; two versions of clustered LSA,
in which we ?cluster? the document dimension in the
Word-Document matrix; FLSA, in which we incorporate
features other than words to train LSA (specifically, we
used the preceding n dialogue acts).
Plain LSA. The input to LSA is a Word-Document ma-
trix with a row for each word, and a column for each
document (for us, a document is a unit such as a sen-
tence or paragraph tagged with a dialogue act). Cell
c(i; j) contains the frequency with which word
i
appears
in document
j
. Clearly, this w*d matrix will be very
sparse. Next, LSA applies SVD to the Word-Document
matrix, obtaining a representation of each document in a
k dimensional space: crucially, k is much smaller than the
dimension of the original space. As a result, words that
did not appear in certain documents now appear, as an
estimate of their correlation to the meaning of those doc-
uments. The number of dimensions k retained by LSA
is an empirical question. The results we report below are
for the best k we experimented with.
To choose the best tag for a document in the test set, we
compare the vector representing the new document with
the vector of each document in the training set. The tag of
1They should be more appropriately termed tutor moves.
the document which has the highest cosine with our test
vector is assigned to the new document.
Clustered LSA. Instead of building the Word-
Document matrix we build a Word-Tag matrix, where the
columns refer to all the possible dialog act types (tags).
The cell c(i; j) will tell us how many times word
i
is
used in documents that have tag
j
. The Word-Tag matrix
is w*t instead of w*d. We then apply Plain LSA to the
Word-Tag matrix.
SemiClustered LSA. In Clustered LSA we lose the
distribution of words in the documents. Moreover, if the
number of tags is small, such as for DIAG, SVD loses its
meaning. SemiClustered LSA tries to remedy these prob-
lems. We still produce the k-dimensional space apply-
ing SVD to the Word-Document matrix. We then reduce
the Word-Tag matrix to the k dimensional space using a
transformation based on the SVD of the Word-Document
matrix. Note that both Clustered and SemiClustered LSA
are much faster at test time than plain LSA, as the test
document needs to be compared only with t tag vectors,
rather than with d document vectors (t << d).
Feature LSA (FLSA). We add extra features to plain
LSA. Specifically, we have experimented with the se-
quence of the previous n dialogue acts. We compute
the input WordTag-Document matrix by computing the
Word-Document matrix, computing the Tag-Document
matrix and then concatenating them vertically to get the
(w+t)*d final matrix. Otherwise, the method is the same
as Plain LSA.
4 Results
Table 1 reports the best results we obtained for each cor-
pus and method. In parentheses, we include the k di-
mension, and, for FLSA, the number of previous tags we
considered.
In all cases, we can see that Plain LSA performs much
better than baseline, where baseline is computed as pick-
ing the most frequent dialogue act in each corpus. As
concerns DIAG, we can also see that SemiClustered LSA
improves on Plain LSA by 3%, but no other method does.
As regards CallHome, first, the results with plain LSA
are comparable to published ones, even if the comparison
is not straightforward, because it is often unclear what
the target classification and features used are. For exam-
ple, (Ries, 1999) reports 76.2% accuracy by using neural
networks augmented with the sequence of the n previous
speech acts. However, (Ries, 1999) does not mention the
target classification; the reported baseline appears com-
patible with both CallHome37 and CallHome10. The
training features in (Ries, 1999) include part-of-speech
(POS) tags for words, which we do not have. This may
Corpus Plain Clustered SemiClustered FLSA
Diag (43.64%) 75.73% (50) 71.91% (3) 78.78% (50) 74.26% (1,150)
CallHome37 (42.69%) 65.36% (50) 22.08% (10) 31.39% (300) 62.59% (1, 50)
CallHome10 (42.69%) 68.91% (25) 61.64% (5) 58.38% (300) 66.57% (1, 100)
Table 1: Result Summary
explain the higher performance. Including POS tags into
our FLSA method is left for future work.
No variation on LSA performs better than plain LSA in
our CallHome experiments. In fact, clustered and semi-
clustered LSA perform vastly worse on the larger clas-
sification problem in CallHome37. It appears that, the
smaller the corpus and target classification are, the better
clustered and semiclustered LSA perform. In fact, semi-
clustered LSA outperforms plain LSA on DIAG.
Our experiments with FLSA do not support the hy-
pothesis that adding features different from words to LSA
helps with classification. (Wiemer-Hastings, 2001) re-
ports mixed results when augmenting LSA: adding POS
tags did not improve performance, but adding some syn-
tactic information did. Note that, in our experiments,
adding more than one previous speech act did not help.
5 Future work
Our experiments show that LSA can be effectively used
to train a dialogue act classifier. On the whole, plain LSA
appears to perform well. Even if our experiments with
extensions to plain LSA were mostly unsuccessful, they
are not sufficient to conclude that plain LSA cannot be
improved. Thus, we will pursue the following directions.
1) Further investigate the correlation of the performance
of (semi)clustered LSA with the size of the corpus and /
or of the target classification. 2) Include other features in
FLSA, e.g. syntactic roles. 3) Redo our experiments on
other corpora, such as Map Task (Carletta et al, 1997).
Map Task is appropriate because besides dialogue acts it
is annotated for syntactic information, while CallHome
is not. 4) Experiment with FLSA on other tasks, such as
assessing text coherence.
Acknowledgements
This work is supported by grant N00014-00-1-0640 from the
Office of Naval Research.
References
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. 1997. The reliability
of a dialogue structure coding scheme. Computational
Linguistics, 23(1):13?31.
P. W. Foltz, D. Laham, and T. K. Landauer. 1999. The
intelligent essay assessor: Applications to educational
technology. Interactive Multimedia Electronic Journal
of Computer-Enhanced Learning, 1(2).
M. Glass, H. Raval, B. Di Eugenio, and M. Traat. 2002.
The DIAG-NLP dialogues: coding manual. Technical
Report UIC-CS 02-03, University of Illinois - Chicago.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, R. Kreuz, and the Tutoring Research Group.
2000. Autotutor: A simulation of a human tutor. Jour-
nal of Cognitive Systems Research.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Intro-
duction to Latent Semantic Analysis. Discourse Pro-
cesses, 25:259?284.
L. Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, and K.
Zechner. 1998. A discourse coding scheme for con-
versational Spanish. In Proceedings ICSLP.
N. Reithinger and E. Maier. 1995. Utilizing statistical
dialogue act processing in Verbmobil. In ACL95, Pro-
ceedings of the 33rd Annual Meeting of the Association
for Computational Linguistics.
K. Ries. 1999. HMM and Neural Network Based Speech
Act Detection. In Proceedings of ICASSP 99.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
D. M. Towne. 1997. Approximate reasoning techniques
for intelligent diagnostic instruction. International
Journal of Artificial Intelligence in Education,8.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001.
Qualitative and quantitative evaluation of DARPA
communicator dialogue systems. In ACL01, Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics.
P. Wiemer-Hastings. 2001. Rules for syntax, vectors for
semantics. In CogSci01, Proceedings of the Twenty-
Third Annual Meeting of the Cognitive Science Society.
Proceedings of the 43rd Annual Meeting of the ACL, pages 50?57,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Aggregation improves learning:
experiments in natural language generation for intelligent tutoring systems
Barbara Di Eugenio and Davide Fossati and Dan Yu
University of Illinois
Chicago, IL, 60607, USA
{bdieugen,dfossa1,dyu6}@uic.edu
Susan Haller
University of Wisconsin - Parkside
Kenosha, WI 53141, USA
haller@cs.uic.edu
Michael Glass
Valparaiso University
Valparaiso, IN, 46383, USA
Michael.Glass@valpo.edu
Abstract
To improve the interaction between students
and an intelligent tutoring system, we devel-
oped two Natural Language generators, that we
systematically evaluated in a three way com-
parison that included the original system as
well. We found that the generator which intu-
itively produces the best language does engen-
der the most learning. Specifically, it appears
that functional aggregation is responsible for
the improvement.
1 Introduction
The work we present in this paper addresses three
issues: evaluation of Natural Language Generation
(NLG) systems, the place of aggregation in NLG,
and NL interfaces for Intelligent Tutoring Systems.
NLG systems have been evaluated in various
ways, such as via task efficacy measures, i.e., mea-
suring how well the users of the system perform on
the task at hand (Young, 1999; Carenini and Moore,
2000; Reiter et al, 2003). We also employed task
efficacy, as we evaluated the learning that occurs
in students interacting with an Intelligent Tutoring
System (ITS) enhanced with NLG capabilities. We
focused on sentence planning, and specifically, on
aggregation. We developed two different feedback
generation engines, that we systematically evaluated
in a three way comparison that included the orig-
inal system as well. Our work is novel for NLG
evaluation in that we focus on one specific com-
ponent of the NLG process, aggregation. Aggrega-
tion pertains to combining two or more of the mes-
sages to be communicated into one sentence (Reiter
and Dale, 2000). Whereas it is considered an es-
sential task of an NLG system, its specific contri-
butions to the effectiveness of the text that is even-
tually produced have rarely been assessed (Harvey
and Carberry, 1998). We found that syntactic aggre-
gation does not improve learning, but that what we
call functional aggregation does. Further, we ran a
controlled data collection in order to provide a more
solid empirical base for aggregation rules than what
is normally found in the literature, e.g. (Dalianis,
1996; Shaw, 2002).
As regards NL interfaces for ITSs, research on the
next generation of ITSs (Evens et al, 1993; Litman
et al, 2004; Graesser et al, 2005) explores NL as
one of the keys to bridging the gap between cur-
rent ITSs and human tutors. However, it is still not
known whether the NL interaction between students
and an ITS does in fact improve learning. We are
among the first to show that this is the case.
We will first discuss DIAG, the ITS shell we are
using, and the two feedback generators that we de-
veloped, DIAG-NLP1and DIAG-NLP2 . Since the
latter is based on a corpus study, we will briefly de-
scribe that as well. We will then discuss the formal
evaluation we conducted and our results.
2 Natural Language Generation for DIAG
DIAG (Towne, 1997) is a shell to build ITSs based
on interactive graphical models that teach students to
troubleshoot complex systems such as home heating
and circuitry. A DIAG application presents a student
with a series of troubleshooting problems of increas-
ing difficulty. The student tests indicators and tries
to infer which faulty part (RU) may cause the abnor-
mal states detected via the indicator readings. RU
stands for replaceable unit, because the only course
of action for the student to fix the problem is to re-
place faulty components in the graphical simulation.
50
Figure 1: The furnace system
Fig. 1 shows the furnace, one subsystem of the home
heating system in our DIAG application. Fig. 1 in-
cludes indicators such as the gauge labeled Water
Temperature, RUs, and complex modules (e.g., the
Oil Burner) that contain indicators and RUs. Com-
plex components are zoomable.
At any point, the student can consult the tutor
via the Consult menu (cf. the Consult button in
Fig. 1). There are two main types of queries: Con-
sultInd(icator) and ConsultRU. ConsultInd queries
are used mainly when an indicator shows an ab-
normal reading, to obtain a hint regarding which
RUs may cause the problem. DIAG discusses the
RUs that should be most suspected given the symp-
toms the student has already observed. ConsultRU
queries are mainly used to obtain feedback on the di-
agnosis that a certain RU is faulty. DIAG responds
with an assessment of that diagnosis and provides
evidence for it in terms of the symptoms that have
been observed relative to that RU.
The original DIAG system (DIAG-orig) uses very
simple templates to assemble the text to present to
the student. The top parts of Figs. 2 and 3 show the
replies provided by DIAG-orig to a ConsultInd on
the Visual Combustion Check, and to a ConsultRu
on the Water Pump.
The highly repetitive feedback by DIAG-orig
screams for improvements based on aggregation
techniques. Our goal in developing DIAG-NLP1
and DIAG-NLP2 was to assess whether simple,
rapidly deployable NLG techniques would lead to
measurable improvements in the student?s learning.
Thus, in both cases it is still DIAG that performs
content determination, and provides to DIAG-NLP1
and DIAG-NLP2 a file in which the facts to be com-
municated are written ? a fact is the basic unit of
information that underlies each of the clauses in a
reply by DIAG-orig . The only way we altered the
interaction between student and system is the ac-
tual language that is presented in the output win-
dow. In DIAG-NLP1 we mostly explored using syn-
tactic aggregation to improve the feedback, whereas
DIAG-NLP2 is corpus-based and focuses on func-
tional aggregation. In both DIAG-NLP1 and DIAG-
NLP2 , we use EXEMPLARS (White and Cald-
well, 1998), an object-oriented, rule-based genera-
tor. The rules (called exemplars) are meant to cap-
ture an exemplary way of achieving a communica-
tive goal in a given context. EXEMPLARS selects
rules by traversing the exemplar specialization hi-
erarchy and evaluating the applicability conditions
associated with each exemplar.
The visual combustion check is igniting which is abnormal
(normal is combusting).
Oil Nozzle always
produces this abnormality when it fails.
Oil Supply Valve always
produces this abnormality when it fails.
Oil pump always
produces this abnormality when it fails.
Oil Filter always
produces this abnormality when it fails.
System Control Module sometimes
produces this abnormality when it fails.
Ignitor Assembly never
produces this abnormality when it fails.
Burner Motor always
produces this abnormality when it fails.
The visual combustion check indicator is igniting.
This is abnormal.
Normal is combusting.
Within the furnace system,
this is sometimes caused if
the System Control Module has failed.
Within the Oil Burner
this is never caused if
the Ignitor Assembly has failed.
In contrast, this is always caused if
the Burner Motor, Oil Filter, Oil Pump,
Oil Supply Valve, or Oil Nozzle has failed.
The combustion is abnormal.
In the oil burner, check the units along the path of the oil and
the burner motor.
Figure 2: Answers to ConsultInd by DIAG-orig ,
DIAG-NLP1and DIAG-NLP2
51
Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
Water pump sound was normal.
This normal indication never results when this unit fails.
Visual combustion check was igniting.
This abnormal indication never results when this unit fails.
Burner Motor RMP Gauge was 525.
This normal indication never results when this unit fails.
The Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
The following indicators never display normally
when this unit fails.
Within the furnace system,
the Burner Motor RMP Gauge is 525.
Within the water pump and safety cutoff valve,
the water pump sound indicator is normal.
The following indicators never display abnormally
when this unit fails.
Within the fire door sight hole,
the visual combustion check indicator is igniting.
The water pump is a poor suspect since the water pump
sound is ok.
You have seen that the combustion is abnormal.
Check the units along the path of the oil and the electrical
devices.
Figure 3: Answers to ConsultRu by DIAG-orig ,
DIAG-NLP1 and DIAG-NLP2
2.1 DIAG-NLP1 : Syntactic aggregation
DIAG-NLP1 1 (i) introduces syntactic aggregation
(Dalianis, 1996; Huang and Fiedler, 1996; Reape
and Mellish, 1998; Shaw, 2002) and what we call
structural aggregation, namely, grouping parts ac-
cording to the structure of the system; (ii) gener-
ates some referring expressions; (iii) models a few
rhetorical relations; and (iv) improves the format of
the output.
The middle parts of Figs. 2 and 3 show the revised
output produced by DIAG-NLP1 . E.g., in Fig. 2 the
RUs of interest are grouped by the system modules
that contain them (Oil Burner and Furnace System),
and by the likelihood that a certain RU causes the
observed symptoms. In contrast to the original an-
swer, the revised answer highlights that the Ignitor
Assembly cannot cause the symptom.
In DIAG-NLP1 , EXEMPLARS accesses the
SNePS Knowledge Representation and Reasoning
System (Shapiro, 2000) for static domain informa-
tion.2 SNePS makes it easy to recognize structural
1DIAG-NLP1 actually augments and refines the first feed-
back generator we created for DIAG, DIAG-NLP0 (Di Eugenio
et al, 2002). DIAG-NLP0 only covered (i) and (iv).
2In DIAG, domain knowledge is hidden and hardly acces-
similarities and use shared structures. Using SNePS,
we can examine the dimensional structure of an ag-
gregation and its values to give preference to aggre-
gations with top-level dimensions that have fewer
values, to give summary statements when a dimen-
sion has many values that are reported on, and to
introduce simple text structuring in terms of rhetor-
ical relations, inserting relations like contrast and
concession to highlight distinctions between dimen-
sional values (see Fig. 2, middle).
DIAG-NLP1 uses the GNOME algorithm (Kib-
ble and Power, 2000) to generate referential expres-
sions. Importantly, using SNePS propositions can
be treated as discourse entities, added to the dis-
course model and referred to (see This is ... caused
if ... in Fig. 2, middle). Information about lexical
realization, and choice of referring expression is en-
coded in the appropriate exemplars.
2.2 DIAG-NLP2 : functional aggregation
In the interest of rapid prototyping, DIAG-NLP1
was implemented without the benefit of a corpus
study. DIAG-NLP2 is the empirically grounded
version of the feedback generator. We collected
23 tutoring interactions between a student using the
DIAG tutor on home heating and two human tutors,
for a total of 272 tutor turns, of which 235 in re-
ply to ConsultRU and 37 in reply to ConsultInd (the
type of student query is automatically logged). The
tutor and the student are in different rooms, sharing
images of the same DIAG tutoring screen. When
the student consults DIAG, the tutor sees, in tabular
form, the information that DIAG would use in gen-
erating its advice ? the same ?fact file? that DIAG
gives to DIAG-NLP1and DIAG-NLP2? and types
a response that substitutes for DIAG?s. The tutor is
presented with this information because we wanted
to uncover empirical evidence for aggregation rules
in our domain. Although we cannot constrain the tu-
tor to mention only the facts that DIAG would have
communicated, we can analyze how the tutor uses
the information provided by DIAG.
We developed a coding scheme (Glass et al,
2002) and annotated the data. As the annotation was
performed by a single coder, we lack measures of
intercoder reliability. Thus, what follows should be
taken as observations rather than as rigorous find-
ings ? useful observations they clearly are, since
sible. Thus, in both DIAG-NLP1 and DIAG-NLP2 we had to
build a small knowledge base that contains domain knowledge.
52
DIAG-NLP2 is based on these observations and its
language fosters the most learning.
Our coding scheme focuses on four areas. Fig. 4
shows examples of some of the tags (the SCM is the
System Control Module). Each tag has from one to
five additional attributes (not shown) that need to be
annotated too.
Domain ontology. We tag objects in the domain
with their class indicator, RU and their states, de-
noted by indication and operationality, respectively.
Tutoring actions. They include (i) Judgment. The
tutor evaluates what the student did. (ii) Problem
solving. The tutor suggests the next course of ac-
tion. (iii) The tutor imparts Domain Knowledge.
Aggregation. Objects may be functional aggre-
gates, such as the oil burner, which is a system com-
ponent that includes other components; linguistic
aggregates, which include plurals and conjunctions;
or a summary over several unspecified indicators or
RUs. Functional/linguistic aggregate and summary
tags often co-occur, as shown in Fig. 4.
Relation to DIAG?s output. Contrary to all other
tags, in this case we annotate the input that DIAG
gave the tutor. We tag its portions as included / ex-
cluded / contradicted, according to how it has been
dealt with by the tutor.
Tutors provide explicit problem solving directions
in 73% of the replies, and evaluate the student?s ac-
tion in 45% of the replies (clearly, they do both in
28% of the replies, as in Fig. 4). As expected, they
are much more concise than DIAG, e.g., they never
mention RUs that cannot or are not as likely to cause
a certain problem, such as, respectively, the ignitor
assembly and the SCM in Fig. 2.
As regards aggregation, 101 out of 551 RUs, i.e.
18%, are labelled as summary; 38 out of 193 indica-
tors, i.e. 20%, are labelled as summary. These per-
centages, though seemingly low, represent a consid-
erable amount of aggregation, since in our domain
some items have very little in common with others,
and hence cannot be aggregated. Further, tutors ag-
gregate parts functionally rather than syntactically.
For example, the same assemblage of parts, i.e., oil
nozzle, supply valve, pump, filter, etc., can be de-
scribed as the other items on the fuel line or as the
path of the oil flow.
Finally, directness ? an attribute on the indica-
tor tag ? encodes whether the tutor explicitly talks
about the indicator (e.g., The water temperature
gauge reading is low), or implicitly via the object
to which the indicator refers (e.g., the water is too
cold). 110 out of 193 indicators, i.e. 57%, are
marked as implicit, 45, i.e. 41%, as explicit, and 2%
are not marked for directness (the coder was free to
leave attributes unmarked). This, and the 137 occur-
rences of indication, prompted us to refer to objects
and their states, rather than to indicators (as imple-
mented by Steps 2 in Fig. 5, and 2(b)i, 3(b)i, 3(c)i in
Fig. 6, which generate The combustion is abnormal
and The water pump sound is OK in Figs. 2 and 3).
2.3 Feedback Generation in DIAG-NLP2
In DIAG-NLP1 the fact file provided by DIAG is
directly processed by EXEMPLARS. In contrast, in
DIAG-NLP2 a planning module manipulates the in-
formation before passing it to EXEMPLARS. This
module decides which information to include ac-
cording to the type of query the system is respond-
ing to, and produces one or more Sentence Structure
objects. These are then passed to EXEMPLARS
that transforms them into Deep Syntactic Structures.
Then, a sentence realizer, RealPro (Lavoie and Ram-
bow, 1997), transforms them into English sentences.
Figs. 5 and 6 show the control flow in DIAG-
NLP2 for feedback generation for ConsultInd and
ConsultRU. Step 3a in Fig. 5 chooses, among all
the RUs that DIAG would talk about, only those
that would definitely result in the observed symp-
tom. Step 2 in the AGGREGATE procedure in Fig. 5
uses a simple heuristic to decide whether and how to
use functional aggregation. For each RU, its possi-
ble aggregators and the number n of units it covers
are listed in a table (e.g., electrical devices covers
4 RUs, ignitor, photoelectric cell, transformer and
burner motor). If a group of REL-RUs contains k
units that a certain aggregator Agg covers, if k < n2 ,
Agg will not be used; if n2 ? k < n, Agg preceded
by some of will be used; if k = n, Agg will be used.
DIAG-NLP2 does not use SNePS, but a relational
database storing relations, such as the ISA hierarchy
(e.g., burner motor IS-A RU), information about ref-
erents of indicators (e.g., room temperature gauge
REFERS-TO room), and correlations between RUs
and the indicators they affect.
3 Evaluation
Our empirical evaluation is a three group, between-
subject study: one group interacts with DIAG-orig ,
53
[judgment [replaceable?unit the ignitor] is a poor suspect] since [indication combustion is working] during startup. The problem is
that the SCM is shutting the system off during heating.
[domain?knowledge The SCM reads [summary [linguistic?aggregate input signals from sensors]] and uses the signals to determine
how to control the system.]
[problem?solving Check the sensors.]
Figure 4: Examples of a coded tutor reply
1. IND? queried indicator
2. Mention the referent of IND and its state
3. IF IND reads abnormal,
(a) REL-RUs? choose relevant RUs
(b) AGGR-RUs? AGGREGATE(REL-RUs)
(c) Suggest to check AGGR-RUs
AGGREGATE(RUs)
1. Partition REL-RUs into subsets by system structure
2. Apply functional aggregation to subsets
Figure 5: DIAG-NLP2 : Feedback generation for
ConsultInd
one with DIAG-NLP1 , one with DIAG-NLP2 . The
75 subjects (25 per group) were all science or engi-
neering majors affiliated with our university. Each
subject read some short material about home heat-
ing, went through one trial problem, then continued
through the curriculum on his/her own. The curricu-
lum consisted of three problems of increasing dif-
ficulty. As there was no time limit, every student
solved every problem. Reading materials and cur-
riculum were identical in the three conditions.
While a subject was interacting with the system,
a log was collected including, for each problem:
whether the problem was solved; total time, and time
spent reading feedback; how many and which in-
dicators and RUs the subject consults DIAG about;
how many, and which RUs the subject replaces. We
will refer to all the measures that were automatically
collected as performance measures.
At the end of the experiment, each subject was ad-
ministered a questionnaire divided into three parts.
The first part (the posttest) consists of three ques-
tions and tests what the student learned about the
domain. The second part concerns whether subjects
remember their actions, specifically, the RUs they
replaced. We quantify the subjects? recollections in
terms of precision and recall with respect to the log
that the system collects. We expect precision and re-
call of the replaced RUs to correlate with transfer,
namely, to predict how well a subject is able to ap-
ply what s/he learnt about diagnosing malfunctions
1. RU? queried RU
REL-IND? indicator associated to RU
2. IF RU warrants suspicion,
(a) state RU is a suspect
(b) IF student knows that REL-IND is abnormal
i. remind him of referent of REL-IND and
its abnormal state
ii. suggest to replace RU
(c) ELSE suggest to check REL-IND
3. ELSE
(a) state RU is not a suspect
(b) IF student knows that REL-IND is normal
i. use referent of REL-IND and its normal state
to justify judgment
(c) IF student knows of abnormal indicators OTHER-INDs
i. remind him of referents of OTHER-INDs
and their abnormal states
ii. FOR each OTHER-IND
A. REL-RUs? RUs associated with OTHER-IND
B. AGGR-RUs? AGGREGATE(REL-RUs)
? AGGR-RUs
iii. Suggest to check AGGR-RUs
Figure 6: DIAG-NLP2 : Feedback generation for
ConsultRU
to new problems. The third part concerns usability,
to be discussed below.
We found that subjects who used DIAG-NLP2
had significantly higher scores on the posttest, and
were significantly more correct (higher precision)
in remembering what they did. As regards perfor-
mance measures, there are no so clear cut results.
As regards usability, subjects prefer DIAG-NLP1 /2
to DIAG-orig , however results are mixed as regards
which of the two they actually prefer.
In the tables that follow, boldface indicates sig-
nificant differences, as determined by an analysis of
variance performed via ANOVA, followed by post-
hoc Tukey tests.
Table 1 reports learning measures, average across
the three problems. DIAG-NLP2 is significantly
better as regards PostTest score (F = 10.359, p =
0.000), and RU Precision (F = 4.719, p =
0.012). Performance on individual questions in the
54
DIAG-orig DIAG-NLP1 DIAG-NLP2
PostTest 0.72 0.69 0.90
RU Precision 0.78 0.70 0.91
RU Recall .53 .47 .40
Table 1: Learning Scores
Figure 7: Scores on PostTest questions
PostTest3 is illustrated in Fig. 7. Scores in DIAG-
NLP2 are always higher, significantly so on ques-
tions 2 and 3 (F = 8.481, p = 0.000, and F =
7.909, p = 0.001), and marginally so on question 1
(F = 2.774, p = 0.069).4
D-Orig D-NLP1 D-NLP2
Total Time 30?17? 28?34? 34?53?
RU Replacements 8.88 11.12 11.36
ConsultInd 22.16 6.92 28.16
Avg. Reading Time 8? 14? 2?
ConsultRU 63.52 45.68 52.12
Avg. Reading Time 5? 4? 5?
Table 2: Performance Measures
Table 2 reports performance measures, cumula-
tive across the three problems, other than average
reading times. Subjects don?t differ significantly in
the time they spend solving the problems, or in the
number of RU replacements they perform. DIAG?s
assumption (known to the subjects) is that there is
only one broken RU per problem, but the simula-
tion allows subjects to replace as many as they want
without any penalty before they come to the correct
solution. The trend on RU replacements is opposite
what we would have hoped for: when repairing a
real system, replacing parts that are working should
clearly be kept to a minimum, and subjects replace
3The three questions are: 1. Describe the main subsystems
of the furnace. 2. What is the purpose of (a) the oil pump (b)
the system control module? 3. Assume the photoelectric cell is
covered with enough soot that it could not detect combustion.
What impact would this have on the system?
4The PostTest was scored by one of the authors, following
written guidelines.
fewer parts in DIAG-orig .
The next four entries in Table 2 report the number
of queries that subjects ask, and the average time it
takes subjects to read the feedback. The subjects
ask significantly fewer ConsultInd in DIAG-NLP1
(F = 8.905, p = 0.000), and take significantly less
time reading ConsultInd feedback in DIAG-NLP2
(F = 15.266, p = 0.000). The latter result is
not surprising, since the feedback in DIAG-NLP2 is
much shorter than in DIAG-orig and DIAG-NLP1 .
Neither the reason not the significance of subjects
asking many fewer ConsultInd of DIAG-NLP1 are
apparent to us ? it happens for ConsultRU as well,
to a lesser, not significant degree.
We also collected usability measures. Although
these are not usually reported in ITS evaluations,
in a real setting students should be more willing to
sit down with a system that they perceive as more
friendly and usable. Subjects rate the system along
four dimensions on a five point scale: clarity, useful-
ness, repetitiveness, and whether it ever misled them
(the scale is appropriately arranged: the highest clar-
ity but the lowest repetitiveness receive 5 points).
There are no significant differences on individual
dimensions. Cumulatively, DIAG-NLP2 (at 15.08)
slightly outperforms the other two (DIAG-orig at
14.68 and DIAG-NLP1 at 14.32), however, the dif-
ference is not significant (highest possible rating is
20 points).
prefer neutral disprefer
DIAG-NLP1 to DIAG-orig 28 5 17
DIAG-NLP2 to DIAG-orig 34 1 15
DIAG-NLP2 to DIAG-NLP1 24 1 25
Table 3: User preferences among the three systems
prefer neutral disprefer
Consult Ind. 8 1 16
Consult RU 16 0 9
Table 4: DIAG-NLP2 versus DIAG-NLP1
natural concise clear contentful
DIAG-NLP1 4 8 10 23
DIAG-NLP2 16 8 11 12
Table 5: Reasons for system preference
Finally,5 on paper, subjects compare two pairs of
versions of feedback: in each pair, the first feedback
5Subjects can also add free-form comments. Only few did
55
is generated by the system they just worked with,
the second is generated by one of the other two sys-
tems. Subjects say which version they prefer, and
why (they can judge the system along one or more
of four dimensions: natural, concise, clear, content-
ful). The first two lines in Table 3 show that subjects
prefer the NLP systems to DIAG-orig (marginally
significant, ?2 = 9.49, p < 0.1). DIAG-NLP1
and DIAG-NLP2 receive the same number of pref-
erences; however, a more detailed analysis (Table 4)
shows that subjects prefer DIAG-NLP1 for feed-
back to ConsultInd, but DIAG-NLP2 for feedback
to ConsultRu (marginally significant, ?2 = 5.6, p <
0.1). Finally, subjects find DIAG-NLP2 more nat-
ural, but DIAG-NLP1 more contentful (Table 5,
?2 = 10.66, p < 0.025).
4 Discussion and conclusions
Our work touches on three issues: aggregation, eval-
uation of NLG systems, and the role of NL inter-
faces for ITSs.
In much work on aggregation (Huang and Fiedler,
1996; Horacek, 2002), aggregation rules and heuris-
tics are shown to be plausible, but are not based on
any hard evidence. Even where corpus work is used
(Dalianis, 1996; Harvey and Carberry, 1998; Shaw,
2002), the results are not completely convincing be-
cause we do not know for certain the content to be
communicated from which these texts supposedly
have been aggregated. Therefore, positing empir-
ically based rules is guesswork at best. Our data
collection attempts at providing a more solid em-
pirical base for aggregation rules; we found that tu-
tors exclude significant amounts of factual informa-
tion, and use high degrees of aggregation based on
functionality. As a consequence, while part of our
rules implement standard types of aggregation, such
as conjunction via shared participants, we also intro-
duced functional aggregation (see conceptual aggre-
gation (Reape and Mellish, 1998)).
As regards evaluation, NLG systems have been
evaluated e.g. by using human judges to assess the
quality of the texts produced (Coch, 1996; Lester
and Porter, 1997; Harvey and Carberry, 1998); by
comparing the system?s performance to that of hu-
mans (Yeh and Mellish, 1997); or through task ef-
ficacy measures, i.e., measuring how well the users
so, and the distribution of topics and of evaluations is too broad
to be telling.
of the system perform on the task at hand (Young,
1999; Carenini and Moore, 2000; Reiter et al,
2003). The latter kind of studies generally contrast
different interventions, i.e. a baseline that does not
use NLG and one or more variations obtained by pa-
rameterizing the NLG system. However, the evalu-
ation does not focus on a specific component of the
NLG process, as we did here for aggregation.
Regarding the role of NL interfaces for ITSs, only
very recently have the first few results become avail-
able, to show that first of all, students do learn when
interacting in NL with an ITS (Litman et al, 2004;
Graesser et al, 2005). However, there are very few
studies like ours, that evaluate specific features of
the NL interaction, e.g. see (Litman et al, 2004). In
our case, we did find that different features of the NL
feedback impact learning. Although we contend that
this effect is due to functional aggregation, the feed-
back in DIAG-NLP2 changed along other dimen-
sions, mainly using referents of indicators instead of
indicators, and being more strongly directive in sug-
gesting what to do next. Of course, we cannot ar-
gue that our best NL generator is equivalent to a hu-
man tutor ? e.g., dividing the number of ConsultRU
and ConsultInd reported in Sec. 2.2 by the number
of dialogues shows that students ask about 10 Con-
sultRus and 1.5 ConsultInd per dialogue when in-
teracting with a human, many fewer than those they
pose to the ITSs (cf. Table 2) (regrettably we did not
administer a PostTest to students in the human data
collection). We further discuss the implications of
our results for NL interfaces for ITSs in a compan-
ion paper (Di Eugenio et al, 2005).
The DIAG project has come to a close. We are
satisfied that we demonstrated that even not overly
sophisticated NL feedback can make a difference;
however, the fact that DIAG-NLP2 has the best lan-
guage and engenders the most learning prompts us
to explore more complex language interactions. We
are pursuing new exciting directions in a new do-
main, that of basic data structures and algorithms.
We are investigating what distinguishes expert from
novice tutors, and we will implement our findings
in an ITS that tutors in this domain.
Acknowledgments. This work is supported by the Office
of Naval Research (awards N00014-99-1-0930 and N00014-00-
1-0640), and in part by the National Science Foundation (award
IIS 0133123). We are grateful to CoGenTex Inc. for making
EXEMPLARS and RealPro available to us.
56
References
Giuseppe Carenini and Johanna D. Moore. 2000. An em-
pirical study of the influence of argument conciseness
on argument effectiveness. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, Hong Kong.
Jose? Coch. 1996. Evaluating and comparing three text-
production techniques. In COLING96, Proceedings of
the Sixteenth International Conference on Computa-
tional Linguistics, pages 249?254
Hercules Dalianis. 1996. Concise Natural Language
Generation from Formal Specifications. Ph.D. thesis,
Department of Computer and Systems Science, Sto-
cholm University. Technical Report 96-008.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural Lan-
guage Generation for Intelligent Tutoring Systems. In
INLG02, The Third International Natural Language
Generation Conference, pages 120?127.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Natural language
generation for intelligent tutoring systems: a case
study. In AIED 2005, the 12th International Confer-
ence on Artificial Intelligence in Education.
M. W. Evens, J. Spitkovsky, P. Boyle, J. A. Michael, and
A. A. Rovick. 1993. Synthesizing tutorial dialogues.
In Proceedings of the Fifteenth Annual Conference of
the Cognitive Science Society, pages 137?140.
Michael Glass, Heena Raval, Barbara Di Eugenio, and
Maarika Traat. 2002. The DIAG-NLP dialogues: cod-
ing manual. Technical Report UIC-CS 02-03, Univer-
sity of Illinois - Chicago.
A.C. Graesser, N. Person, Z. Lu, M.G. Jeon, and B. Mc-
Daniel. 2005. Learning while holding a conversation
with a computer. In L. PytlikZillig, M. Bodvarsson,
and R. Brunin, editors, Technology-based education:
Bringing researchers and practitioners together. Infor-
mation Age Publishing.
Terrence Harvey and Sandra Carberry. 1998. Inte-
grating text plans for conciseness and coherence. In
ACL/COLING 98, Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 512?518.
Helmut Horacek. 2002. Aggregation with strong regu-
larities and alternatives. In International Conference
on Natural Language Generation.
Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing
and aggregating argumentative text using text struc-
ture. In Proceedings of the 8th International Workshop
on Natural Language Generation, pages 21?30.
Rodger Kibble and Richard Power. 2000. Nominal gen-
eration in GNOME and ICONOCLAST. Technical re-
port, Information Technology Research Institute, Uni-
versity of Brighton, Brighton, UK.
Beno??t Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing.
James C. Lester and Bruce W. Porter. 1997. Developing
and empirically evaluating robust explanation genera-
tors: the KNIGHT experiments. Computational Lin-
guistics, 23(1):65?102.
D. J. Litman, C. P. Rose?, K. Forbes-Riley, K. VanLehn,
D. Bhembe, and S. Silliman. 2004. Spoken versus
typed human and computer dialogue tutoring. In Pro-
ceedings of the Seventh International Conference on
Intelligent Tutoring Systems, Maceio, Brazil.
Mike Reape and Chris Mellish. 1998. Just what is ag-
gregation anyway? In Proceedings of the European
Workshop on Natural Language Generation.
Ehud Reiter and Robert Dale. 2000. Building Natu-
ral Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
S. C. Shapiro. 2000. SNePS: A logic for natural lan-
guage understanding and commonsense reasoning. In
L. M. Iwanska and S. C. Shapiro, editors, Natural
Language Processing and Knowledge Representation.
AAAI Press/MIT Press.
James Shaw. 2002. A corpus-based analysis for the or-
dering of clause aggregation operators. In COLING02,
Proceedings of the 19th International Conference on
Computational Linguistics.
Douglas M. Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal of Artificial Intelligence in Education.
Michael White and Ted Caldwell. 1998. Exemplars: A
practical, extensible framework for dynamic text gen-
eration. In Proceedings of the Ninth International
Workshop on Natural Language Generation, pages
266?275, Niagara-on-the-Lake, Canada.
Ching-Long Yeh and Chris Mellish. 1997. An empir-
ical study on the generation of anaphora in Chinese.
Computational Linguistics, 23(1):169?190.
R. Michael Young. 1999. Using Grice?s maxim of quan-
tity to select the content of plan descriptions. Artificial
Intelligence, 115:215?256.
57
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 610?614,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Lexical Substitution for the Medical Domain
Martin Riedl
1
Michael R. Glass
2
Alfio Gliozzo
2
(1) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
(2) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
riedl@cs.tu-darmstadt.de, {mrglass,gliozzo}@us.ibm.com
Abstract
In this paper we examine the lexical substitu-
tion task for the medical domain. We adapt
the current best system from the open domain,
which trains a single classifier for all instances
using delexicalized features. We show sig-
nificant improvements over a strong baseline
coming from a distributional thesaurus (DT).
Whereas in the open domain system, features
derived from WordNet show only slight im-
provements, we show that its counterpart for
the medical domain (UMLS) shows a signif-
icant additional benefit when used for feature
generation.
1 Introduction
The task of lexical substitution (McCarthy and Navigli,
2009) deals with the substitution of a target term within
a sentence with words having the same meaning. Thus,
the task divides into two subtasks:
? Identification of substitution candidates, i.e.
terms that are, for some contexts, substitutable for
a given target term.
? Ranking the substitution candidates according to
their context
Such a substitution system can help for semantic text
similarity (B?ar et al., 2012), textual entailment (Dagan
et al., 2013) or plagiarism detection (Chong and Specia,
2011).
Datasets provided by McCarthy and Navigli (2009)
and Biemann (2012) offer manually annotated substi-
tutes for a given set of target words within a context
(sentence). Contrary to these two datasets in Kremer et
al. (2014) a dataset is offered where all words have are
annotated with substitutes. All the datasets are suited
for the open domain.
But a system performing lexical substitution is not
only of interest for the open domain, but also for the
medical domain. Such a system could then be applied
to medical word sense disambiguation, entailment or
question answering tasks. Here we introduce a new
dataset and adapt the lexical substitution system, pro-
vided by Szarvas et al. (2013), to the medical domain.
Additionally, we do not make use of WordNet (Miller,
1995) to provide similar terms, but rather employ a Dis-
tributional Thesaurus (DT), computed on medical texts.
2 Related Work
For the general domain, the lexical substitution task
was initiated by a Semeval-2007 Task (McCarthy and
Navigli, 2009). This task was won by an unsupervised
method (Giuliano et al., 2007), which uses WordNet for
the substitution candidate generation and then relies on
the Google Web1T n-grams (Brants and Franz, 2006)
1
to rank the substitutes.
The currently best system, to our knowledge, is pro-
posed by Szarvas et al. (2013). This is a supervised ap-
proach, where a single classifier is trained using delex-
icalized features for all substitutes and can thus be ap-
plied even to previously unseen substitutes. Although
there have been many approaches for solving the task
for the general domain, only slight effort has been done
in adapting it to different domains.
3 Method
To perform lexical substitution, we follow the delex-
icalization framework of Szarvas et al. (2013). We
automatically build Distributional Thesauri (DTs) for
the medical domain and use features from the Uni-
fied Medical Language System (UMLS) ontology. The
dataset for supervised lexical substitution consists of
sentences, containing an annotated target word t. Con-
sidering the sentence being the context for the target
word, the target word might have different meanings.
Thus annotated substitute candidates s
g
1
. . . s
g
n
? s
g
,
need to be provided for each context. The negative ex-
amples are substitute candidates that either are incor-
rect for the target word, do not fit into the context or
both. We will refer to these substitutes as false substi-
tute candidates s
f
1
. . . s
f
m
? s
f
with s
f
? s
g
= ?.
For the generation of substitute candidates we do not
use WordNet, as done in previous works (Szarvas et al.,
2013), but use only substitutes from a DT. To train a
single classifier, features that distinguishing the mean-
ing of words in different context need to be considered.
Such features could be e.g. n-grams, features from dis-
tributional semantics or features which are extracted
1
http://catalog.ldc.upenn.edu/
LDC2006T13
610
relative to the target word, such as the ratio between
frequencies of the substitute candidate and the target
word. After training, we apply the algorithm to un-
seen substitute candidates and rank them according to
their positive probabilities, given by the classifier. Con-
trary to Szarvas et al. (2013), we do not use any weight-
ing in the training if a substitute has been supplied by
many annotators, as we could not observe any improve-
ments. Additionally, we use logistic regression (Fan et
al., 2008) as classifier
2
.
4 Resources
For the substitutes and for the generation of delexical-
ized features, we rely on DTs, the UMLS and Google
Web1T.
4.1 Distributional thesauri (DTs)
We computed two different DTs using the framework
proposed in Biemann and Riedl (2013)
3
.
The first DT is computed based on Medline
4
ab-
stracts. This thesaurus uses the left and the right word
as context features. To include multi-word expressions,
we allow the number of tokens that form a term to be
up to the length of three.
The second DT is based on dependencies as context
features from a English Slot Grammar (ESG) parser
(McCord et al., 2012) modified to handle medical data.
The ESG parser is also capable of finding multi-word
expressions. As input data we use 3.3 GB of texts
from medical textbooks, encyclopedias and clinical ref-
erence material as well as selected journals. This DT is
also used for the generation of candidates supplied to
annotators when creating the gold standard and there-
fore is the main resource to provide substitute candi-
dates.
4.2 UMLS
The Unified Medical Language System (UMLS) is an
ontology for the medical domain. In contrast to Szarvas
et al. (2013), which uses WordNet (Miller, 1995) to
generate substitute candidates and also for generating
features, we use UMLS solely for feature generation.
4.3 Google Web1T
We use the Google Web1T to generate n-gram features
as we expect this open domain resource to have consid-
erable coverage for most specific domains as well. For
accessing the resource, we use JWeb1T
5
(Giuliano et
al., 2007).
2
We use a Java port of LIBLINEAR (http://www.
csie.ntu.edu.tw/
?
cjlin/liblinear/) available
from http://liblinear.bwaldvogel.de/
3
We use Lexicographer?s Mutual Information (LMI) (Ev-
ert, 2005) as significance measure and consider only the top
1000 (p = 1000) features per term.
4
http://www.nlm.nih.gov/bsd/licensee/
2014_stats/baseline_med_filecount.html
5
https://code.google.com/p/jweb1t/
5 Lexical Substitution dataset
Besides the lexical substitution data sets for the open
domain (McCarthy and Navigli, 2009; Biemann, 2012;
Kremer et al., 2014) there is no dataset available that
can be used for the medical domain. Therefore, we
constructed an annotation task for the medical domain
using a medical corpus and domain experts.
In order to provide the annotators with a clear task,
we presented a question, and a passage that contains
the correct answer to the question. We restricted this to
a subset of passages that were previously annotated as
justifying the answer to the question. This is related to
a textual entailment task, essentially the passage entails
the question with the answer substituted for the focus of
the question. We instructed the annotators to first iden-
tify the terms that were relevant for the entailment rela-
tion. For each relevant term we randomly extracted 10
terms from the ESG-based DT within the top 100 most
similar terms. Using this list of distributionally similar
terms, the annotators selected those terms that would
preserve the entailment relation if substituted. This re-
sulted in a dataset of 699 target terms with substitutes.
On average from the 10 terms 0.846 are annotated as
correct substitutes. Thus, the remaining terms can be
used as false substitute candidates.
The agreement on this task by Fleiss Kappa was
0.551 indicating ?moderate agreement? (Landis and
Koch, 1977). On the metric of pairwise agreement,
as defined in the SemEval lexical substitution task, we
achieve 0.627. This number is not directly comparable
to the pairwise agreement score of 0.277 for the Se-
mEval lexical substitution task (McCarthy and Navigli,
2009) since in our task the candidates are given. How-
ever, it shows promise that subjectivity may be reduced
by casting lexical substitution into a task of maintain-
ing entailment.
6 Evaluation
For the evaluation we use a ten-fold cross validation
and report P@1 (also called Average Precision (AP) at
1) and Mean Average Precision (MAP) (Buckley and
Voorhees, 2004) scores. The P@1 score indicates how
often the first substitute of the system matches the gold
standard. The MAP score is the mean of all AP from 1
to the number of all substitutes.
? Google Web 1T:
We use the same Google n-gram features, as
used in Giuliano et al. (2007) and Szarvas et al.
(2013). These are frequencies of n-grams formed
by the substitute candidate s
i
and the left and right
words, taken from the context sentence, normal-
ized by the frequency of the same context n-gram
with the target term t. Additionally, we add the
same features, normalized by the frequency sum
of all n-grams of the substitute candidates. An-
other feature is generated using the frequencies
where t and s are listed together using the words
611
and, or and ?,? as separator and also add the left
and right words of that phrase as context. Then we
normalize this frequency by the frequency of the
context occurring only with t.
? DT features:
To characterize if t and s
i
have similar words
in common, and therefore are similar, we com-
pute the percentage of words their thesauri en-
tries share, considering the top n words in each
entry with n = 1, 5, 20, 50, 100, 200. During
the DT calculation we also calculate the signif-
icances between each word and its context fea-
tures (see Section 4.1). Using this information,
we compute if the words in the sentences also
occur as context features for the substitute can-
didate. A third feature group relying on DTs
is created by the overlapping context features
for the top m entries of t and s
i
with m =
1, 5, 20, 50, 100, 1000, which are ranked regard-
ing their significance score. Whereas, the simi-
larities between the trigram-based and the ESG-
based DT are similar, the context features are dif-
ferent. Both feature types can be applied to the
two DTs. Additionally, we extract the thesaurus
entry for the target word t and generate a feature
indicating whether the substitute s
i
is within the
top k entries with k = 1, 5, 10, 20, 100 entries
6
.
? Part-of-speech n-grams:
To identify the context of the word we use the
POS-tag (only the first letter) of s
i
and t as feature
and POS-tag combinations of up to three neigh-
boring words.
? UMLS:
Considering UMLS we look up all concept unique
identifiers (CUIs) for s
i
and t. The first two fea-
tures are the number of CUIs for s
i
and t. The next
features compute the number of CUIs that s
i
and t
share, starting from the minimal to the maximum
number of CUIs. Additionally, we use a feature
indicating that s
i
and t do not share any CUI.
6.1 Substitute candidates
The candidates for the substitution are taken from the
ESG based DT. For each target term we use the gold
substitute candidates as correct instances and add all
possible substitutes for the same target term occurring
in a different context and do not have been annotated
as valid in the present context as false instances.
7 Results
Running the experiment, we get the results as shown
in Table 1. As baseline system we use the ranking of
6
Whereas in Szarvas et al. (2013) only k = 100 is used,
we gained an improvement in performance when also adding
smaller values of k.
the ESG-based DT. As can be seen, the baseline is al-
ready quite high, which can be attributed to the fact
that this resource was used to generate substitutes und
thus contains all positive instances. Using the super-
vised approach, we can beat the baseline by 0.10 for
the MAP score and by 0.176 for the P@1 score, which
is a significant improvement (p < 0.0001, using a two
tailed permutation test). To get insights of the contri-
System MAP P@1
Baseline 0.6408 0.5365
ALL 0.7048 0.6366
w/o DT 0.5798 0.4835
w/o UMLS 0.6618 0.5651
w/o Ngrams 0.7009 0.6252
w/o POS 0.7027 0.6323
Table 1: Results for the evaluation using substitute can-
didates from the DT.
bution of individual feature types, we perform an abla-
tion test. We observe that the most prominent features
are coming from the two DTs as we only achieve re-
sults below the baseline, when removing DT features.
We still obtain significant improvements over the base-
line when removing other feature groups. The second
most important feature comes from the UMLS. Fea-
tures coming from the Google n-grams improve the
system only slightly. The lowest improvement is de-
rived from the part-of-speech features. This leads us
to summarize that a hybrid approach for feature gen-
eration using manually created resources (UMLS) and
unsupervised features (DTs) leads to the best result for
lexical substitution for the medical domain.
8 Analysis
For a better insight into the lexical substitution we ana-
lyzed how often we outperform the baseline, get equal
results or get decreased scores. According to Table 2 in
performance # of instances Avg. ? MAP
decline 180 -0.16
equal 244 0
improvements 275 0.26
Table 2: Error analysis for the task respectively to the
MAP score.
around 26% of the cases we observe a decreased MAP
score, which is on average 0.16 smaller then the scores
achieved with the baseline. On the other hand, we see
improvements in around 39% of the cases: an average
improvements of 0.26, which is much higher then the
loss. For the remaining 25% of cases we observe the
same score.
Looking inside the data, the largest error class is
caused by antonyms. A sub-class of this error are
multi-word expressions having an adjective modifier.
This problems might be solved by additional features
using the UMLS resource. An example is shown in
Figure 1.
612
Figure 1: Example sentence for the target term mild
thrombocytopenia. The system returns a wrong rank-
ing, as the adjective changes the meaning and turns the
first ranked term into an antonym.
For feature generation, we currently lookup multi-
word expressions as one term, both in the DT and the
UMLS resource and do not split them into their sin-
gle tokens. This error also suggests considering the
single words inside the multi-word expression, espe-
cially adjectives, and looking them up in a resource
(e.g. UMLS) to detect synonymy and antonymy.
Figure 2 shows the case, where the ranking is per-
formed correctly, but the precise substitute is not an-
notated as a correct one. The term nail plate might be
even more precise in the context as the manual anno-
tated term nail bed. Due to the missing annotation the
Figure 2: Example sentence for the target term nails.
Here the ranking from the system is correct, but the first
substitute from the system was not annotated as such.
baseline gets better scores then the result from the sys-
tem.
9 Conclusion
In summary, we have examined the lexical substitution
task for the medical domain and could show that a sys-
tem for open domain text data can be applied to the
medical domain. We can show that following a hybrid
approach using features from UMLS and distributional
semantics leads to the best results. In future work, we
will work on integrating DTs using other context fea-
tures, as we could see an impact of using two different
DTs. Furthermore, we want to incorporate features us-
ing n-grams computed on a corpus from the domain
and include co-occurrence features.
Acknowledgments
We thank Adam Lally, Eric Brown, Edward A. Epstein,
Chris Biemann and Faisal Chowdhury for their helpful
comments.
References
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation, held
in conjunction with the 1st Joint Conference on Lex-
ical and Computational Semantics, pages 435?440,
Montreal, Canada.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! A Framework for Lexical Expansion with Con-
textual Similarity. Journal of Language Modelling,
1(1):55?95.
Chris Biemann. 2012. Turk bootstrap word sense in-
ventory 2.0: A large-scale resource for lexical sub-
stitution. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram corpus version 1. Technical report, Google Re-
search.
Chris Buckley and Ellen M. Voorhees. 2004. Re-
trieval evaluation with incomplete information. In
Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?04, pages 25?32,
Sheffield, United Kingdom.
Miranda Chong and Lucia Specia. 2011. Lexical gen-
eralisation for word-level matching in plagiarism de-
tection. In Recent Advances in Natural Language
Processing, pages 704?709, Hissar, Bulgaria.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio M.
Zanzotto. 2013. Recognizing Textual Entailment:
Models and Applications. Synthesis Lectures on Hu-
man Language Technologies, 6(4):1?220.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Institut f?ur maschinelle Sprachverarbeitung, Univer-
sity of Stuttgart.
613
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 145?148,
Prague, Czech Republic.
Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and Ste-
fan Thater. 2014. What Substitutes Tell Us - Anal-
ysis of an ?All-Words? Lexical Substitution Corpus.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2014), pages 540?549, Gothen-
burg, Sweden.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Michael C. McCord, J. William Murdock, and Bran-
imir K. Boguraev. 2012. Deep Parsing in Watson.
IBM J. Res. Dev., 56(3):264?278.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised All-Words Lexical Substitution
using Delexicalized Features. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2013),
pages 1131?1141, Atlanta, GA, USA.
614
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1522?1531,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Word Semantic Representations using Bayesian Probabilistic Tensor
Factorization
Jingwei Zhang and Jeremy Salwen
Columbia University
Computer Science
New York, NY 10027, USA
{jz2541,jas2312}@columbia.edu
Michael Glass and Alfio Gliozzo
IBM T.J. Waston Research
Yorktown Heights, NY 10598, USA
{mrglass,gliozzo}@us.ibm.com
Abstract
Many forms of word relatedness have been
developed, providing different perspec-
tives on word similarity. We introduce
a Bayesian probabilistic tensor factoriza-
tion model for synthesizing a single word
vector representation and per-perspective
linear transformations from any number
of word similarity matrices. The result-
ing word vectors, when combined with the
per-perspective linear transformation, ap-
proximately recreate while also regulariz-
ing and generalizing, each word similarity
perspective.
Our method can combine manually cre-
ated semantic resources with neural word
embeddings to separate synonyms and
antonyms, and is capable of generaliz-
ing to words outside the vocabulary of
any particular perspective. We evaluated
the word embeddings with GRE antonym
questions, the result achieves the state-of-
the-art performance.
1 Introduction
In recent years, vector space models (VSMs)
have been proved successful in solving various
NLP tasks including named entity recognition,
part-of-speech tagging, parsing, semantic role-
labeling and answering synonym or analogy ques-
tions (Turney et al., 2010; Collobert et al., 2011).
Also, VSMs are reported performing well on
tasks involving the measurement of word related-
ness (Turney et al., 2010). Many existing works
are distributional models, based on the Distribu-
tional Hypothesis, that words occurring in simi-
lar contexts tend to have similar meanings (Har-
ris, 1954). The limitation is that word vectors de-
veloped from distributional models cannot reveal
word relatedness if its information does not lie in
word distributions. For instance, they are believed
to have difficulty distinguishing antonyms from
synonyms, because the distribution of antonymous
words are close, since the context of antonymous
words are always similar to each other (Moham-
mad et al., 2013). Although some research claims
that in certain conditions there do exist differ-
ences between the contexts of different antony-
mous words (Scheible et al., 2013), the differences
are subtle enough that it can hardly be detected by
such language models, especially for rare words.
Another important class of lexical resource for
word relatedness is a lexicon, such as Word-
Net (Miller, 1995) or Roget?s Thesaurus (Kipfer,
2009). Manually producing or extending lexi-
cons is much more labor intensive than generat-
ing VSM word vectors using a corpus. Thus, lex-
icons are sparse with missing words and multi-
word terms as well as missing relationships be-
tween words. Considering the synonym / antonym
perspective as an example, WordNet answers less
than 40% percent of the the GRE antonym ques-
tions provided by Mohammad et al. (2008) di-
rectly. Moreover, binary entries in lexicons do not
indicate the degree of relatedness, such as the de-
gree of lexical contrast between happy and sad or
happy and depressed. The lack of such informa-
tion makes it less fruitful when adopted in NLP
applications.
In this work, we propose a Bayesian tensor fac-
torization model (BPTF) for synthesizing a com-
posite word vector representation by combining
multiple different sources of word relatedness.
The input is a set of word by word matrices, which
may be sparse, providing a number indicating the
presence or degree of relatedness. We treat word
relatedness matrices from different perspectives as
slices, forming a word relatedness tensor. Then the
composite word vectors can be efficiently obtained
by performing BPTF. Furthermore, given any two
words and any trained relatedness perspective, we
1522
can create or recreate the pair-wise word related-
ness with regularization via per-perspective linear
transformation.
This method allows one set of word vectors to
represent word relatednesses from many different
perspectives (e.g. LSA for topic relatedness / cor-
pus occurrences, ISA relation and YAGO type) It
is able to bring the advantages from both word re-
latedness calculated by distributional models, and
manually created lexicons, since the former have
much more vocabulary coverage and many varia-
tions, while the latter covers word relatedness that
is hard to detect by distributional models. We can
use information from distributional perspectives to
create (if does not exist) or re-create (with regular-
ization) word relatedness from the lexicon?s per-
spective.
We evaluate our model on distinguishing syn-
onyms and antonyms. There are a number of re-
lated works (Lin and Zhao, 2003; Turney, 2008;
Mohammad et al., 2008; Mohammad et al., 2013;
Yih et al., 2012; Chang et al., 2013). A number of
sophisticated methods have been applied, produc-
ing competitive results using diverse approaches.
We use the GRE antonym questions (Mohammad
et al., 2008) as a benchmark, and answer these
questions by finding the most contrasting choice
according to the created or recreated synonym /
antonym word relatedness. The result achieves
state-of-the-art performance.
The rest of this paper is organized as fol-
lows. Section 2 describes the related work of
word vector representations, the BPTF model and
antonymy detection. Section 3 presents our BPTF
model and the sampling method. Section 4 shows
the experimental evaluation and results with Sec-
tion 5 providing conclusion and future work.
2 Related Work
2.1 Word Vector Representations
Vector space models of semantics have a long his-
tory as part of NLP technologies. One widely-
used method is deriving word vectors using la-
tent semantic analysis (LSA) (Deerwester et al.,
1990), for measuring word similarities. This pro-
vides a topic based perspective on word simi-
larity. In recent years, neural word embeddings
have proved very effective in improving various
NLP tasks (e.g. part-of-speech tagging, chunking,
named entity recognition and semantic role label-
ing) (Collobert et al., 2011). The proposed neural
models have a large number of variations, such as
feed-forward networks (Bengio et al., 2003), hi-
erarchical models (Mnih and Hinton, 2008), re-
current neural networks (Mikolov, 2012), and re-
cursive neural networks (Socher et al., 2011).
Mikolov et al. (2013) reported their vector-space
word representation is able to reveal linguistic
regularities and composite semantics using sim-
ple vector addition and subtraction. For example,
?King?Man+Woman? results in a vector very
close to ?Queen?. Luong et al. (2013) proposed
a recursive neural networks model incorporating
morphological structure, and has better perfor-
mance for rare words.
Some non-VSM models
1
also generate word
vector representations. Yih et al. (2012) apply po-
larity inducing latent semantic analysis (PILSA)
to a thesaurus to derive the embedding of words.
They treat each entry of a thesaurus as a docu-
ment giving synonyms positive term counts, and
antonyms negative term counts, and preform LSA
on the signed TF-IDF matrix In this way, syn-
onyms will have cosine similarities close to one
and antonyms close to minus one.
Chang et al. (2013) further introduced Multi-
Relational LSA (MRLSA), as as extension of
LSA, that performs Tucker decomposition over a
three-way tensor consisting of multiple relations
(document-term like matrix) between words as
slices, to capture lexical semantics. The purposes
of MRLSA and our model are similar, but the dif-
ferent factorization techniques offer different ad-
vantages. In MRLSA, the k-th slice of tensor W
is approximated by
W
:,:,k
? X
:,:,k
= US
:,:,k
V
T
,
where U and V are both for the same word list
but are not guaranteed (or necessarily desired) to
be the same. Thus, this model has the ability to
capture asymmetric relations, but this flexibility is
a detriment for symmetric relatedness. In order to
expand word relatedness coverage, MRLSA needs
to choose a pivot slice (e.g. the synonym slice),
thus there always must existence such a slice, and
the model performance depends on the quality of
this pivot slice. Also, while non-completeness is
a pervasive issue in manually created lexicons,
MRLSA is not flexible enough to treat the un-
known entries as missing. Instead it just sets them
1
As defined by Turney et al. (2010), VSM must be derived
from event frequencies.
1523
to zero at the beginning and uses the pivot slice
to re-calculate them. In contrast, our method of
BPTF is well suited to symmetric relations with
many unknown relatedness entries.
2.2 BPTF Model
Salakhutdinov and Mnih (2008) introduced a
Bayesian Probabilistic Matrix Factorization
(BPMF) model as a collaborative filtering algo-
rithm. Xiong et al. (2010) proposed a Bayesian
Probabilistic Tensor Factorization (BPTF) model
which further extended the original model to
incorporate temporal factors. They modeled latent
feature vector for users and items, both can be
trained efficiently using Markov chain Monte
Carlo methods, and they obtained competitive
results when applying their models on real-world
recommendation data sets.
2.3 Antonomy Detection
There are a number of previous works in detect-
ing antonymy. Lin and Zhao (2003) identifies
antonyms by looking for pre-identified phrases in
corpus datasets. Turney (2008) proposed a su-
pervised classification method for handling analo-
gies, then apply it to antonyms by transforming
antonym pairs into analogy relations. Mohammad
et al. (Mohammad et al., 2008; Mohammad et
al., 2013) proposed empirical approaches consid-
ering corpus co-occurrence statistics and the struc-
ture of a published thesaurus. Based on the as-
sumption that the strongly related words of two
words in a contrasting pair are also often antony-
mous, they use affix patterns (e.g. ?un-?, ?in-? and
?im-?) and a thesaurus as seed sets to add con-
trast links between word categories. Their best
performance is achieved by further manually an-
notating contrasting adjacent categories. This ap-
proach relies on the Contrast Hypothesis, which
will increase false positives even with a carefully
designed methodology. Furthermore, while this
approach can expand contrast relationships in a
lexicon, out-of-vocabulary words still pose a sub-
stancial challenge.
Yih et al. (2012) and Chang et al. (2013) also
applied their vectors on antonymy detection, and
Yih et al. achieves the state-of-the-art performance
in answering GRE antonym questions. In addition
to the word vectors generated from PILSA, they
use morphology and k-nearest neighbors from dis-
tributional word vector spaces to derive the em-
beddings for out-of-vocabulary words. The latter
is problematic since both synonyms and antonyms
are distributionally similar. Their approach is two
stage: polarity inducing LSA from a manually
created thesaurus, then falling back to morphol-
ogy and distributional similarity when the lexicon
lacks coverage. In contrast, we focus on fusing
the information from thesauruses and automati-
cally induced word relatedness measures during
the word vector space creation. Then prediction
is done in a single stage, from the latent vectors
capturing all word relatedness perspectives and the
appropriate per-perspective transformation vector.
3 Methods
3.1 The Bayesian Probabilistic Tensor
Factorization Model
Our model is a variation of the BPMF model
(Salakhutdinov and Mnih, 2008), and is similar
to the temporal BPTF model (Xiong et al., 2010).
To model word relatedness from multiple perspec-
tives, we denote the relatedness between word i
and word j from perspective k as R
k
ij
. Then we
can organize these similarities to form a three-way
tensor R ? R
N?N?K
.
Table 1 shows an example, the first slice of the
tensor is a N ? N matrix consists of 1/-1 corre-
sponding to the synonym/antonym entries in the
Roget?s thesaurus, and the second slice is aN?N
matrix consists of the cosine similarity from neural
word embeddings created by Luong et al. (2013),
where N is the number of words in the vocabu-
lary. Note that in our model the entries missing
in Table 1a do not necessarily need to be treated
as zero. Here we use the indicator variable I
k
ij
to denote if the entry R
k
ij
exists (I
k
ij
= 1) or not
(I
k
ij
= 0). If K = 1, the BPTF model becomes to
BPMF. Hence the key difference between BPTF
and BPMF is that the former combines multi-
ple complementary word relatedness perspectives,
while the later only smooths and generalizes over
one.
We assume the relatedness R
k
ij
to be Gaussian,
and can be expressed as the inner-product of three
D-dimensional latent vectors:
R
k
ij
|V
i
, V
j
, P
k
? N (< V
i
, V
j
, P
k
>,?
?1
),
where< ?, ?, ? > is a generalization of dot product:
< V
i
, V
j
, P
k
>?
D
?
d=1
V
(d)
i
V
(d)
j
P
(d)
k
,
1524
happy joyful lucky sad depressed
happy 1 1 -1 -1
joyful 1 -1
lucky 1 -1
sad -1 -1 -1 1
depressed -1 1
(a) The first slice: synonym & antonym relatedness
happy joyful lucky sad depressed
happy .03 .61 .65 .13
joyful .03 .25 .18 .23
lucky .61 .25 .56 .31
sad .65 .18 .56 -.01
depressed .13 .23 .31 -.01
(b) The second slice: distributional similarity
Table 1: Word Relatedness Tensor
and ? is the precision, the reciprocal of the vari-
ance. V
i
and V
j
are the latent vectors of word i and
word j, and P
k
is the latent vector for perspective
k.
We follow a Bayesian approach, adding Gaus-
sian priors to the variables:
V
i
? N (?
V
,?
?1
V
),
P
i
? N (?
P
,?
?1
P
),
where ?
V
and ?
P
are D dimensional vectors and
?
V
and ?
P
are D-by-D precision matrices.
Furthermore, we model the prior distribution of
hyper-parameters as conjugate priors (following
the model by (Xiong et al., 2010)):
p(?) =W(?|
?
W
0
, ?
0
),
p(?
V
,?
V
) = N (?
V
|?
0
, (?
0
?
V
)
?1
)W(?
V
|W
0
, ?
0
),
p(?
P
,?
P
) = N (?
P
|?
0
, (?
0
?
P
)
?1
)W(?
P
|W
0
, ?
0
),
where W(W
0
, ?
0
) is the Wishart distribution of
degree of freedom ? and a D-by-D scale matrix
W , and
?
W
0
is a 1-by-1 scale matrix for ?. The
graphical model is shown in Figure 1 (with ?
0
set
to 1). After choosing the hyper-priors, the only re-
maining parameter to tune is the dimension of the
latent vectors.
Due to the existence of prior distributions, our
model can capture the correlation between dif-
ferent perspectives during the factorization stage,
then create or re-create word relatedness using this
correlation for regularization and generalization.
This advantage is especially useful when such cor-
relation is too subtle to be captured by other meth-
ods. On the other hand, if perspectives (let?s say k
and l) are actually unrelated, our model can handle
it as well by making P
k
and P
l
orthogonal to each
other.
3.2 Inference
To avoid calculating intractable distributions, we
use a numerical method to approximate the re-
sults. Here we use the Gibbs sampling algorithm
R
k
ij
P
k
?
P
?
P
?
0
W
0
, ?
0
?
V
i
V
j
?
V
?
V
W
0
, ?
0
?
0
? ? ? ? ? ?? ? ?
k = 1, ..., K
I
k
i,j
= 1
i 6= j
i, j = 1, ..., N
Figure 1: The graphical model for BPTF.
to perform the Markov chain Monte Carlo method.
When sampling a block of parameters, all other
parameters are fixed, and this procedure is re-
peated many times until convergence. The sam-
pling algorithm is shown in Algorithm 1.
With conjugate priors, and assuming I
k
i,i
=
0, ?i, k (we do not consider a word?s relatedness
to itself), the posterior distributions for each block
of parameters are:
p(?|R,V,P) =W(
?
W
0
?
, ??
0
?
) (1)
Where:
??
?
0
= ??
0
+
2?
k=1
N?
i,j=1
I
k
ij
,
(
?
W
?
0
)
?1
=
?
W
?1
0
+
2?
k=1
N?
i,j=1
I
k
ij
(R
k
ij
? < V
i
, V
j
, P
k
>)
2
1525
p(?
V
,?
V
|V) = N (?
V
|?
?
0
, (?
?
0
?
V
)
?1
)W(?
V
|W
?
0
, ?
?
0
)
(2)
Where:
?
?
0
=
?
0
?
0
+N
?
V
?
0
+N
, ?
?
0
= ?
0
+N, ?
?
0
= ?
0
+N,
(W
?
0
)
?1
= W
?1
0
+N
?
S +
?
0
N
?
0
+N
(?
0
?
?
V )(?
0
?
?
V )
T
,
?
V =
1
N
N?
i=1
V
i
,
?
S =
1
N
N?
i=1
(V
i
?
?
V )(V
i
?
?
V )
T
p(?
P
,?
P
|P) = N (?
P
|?
?
0
, (?
?
0
?
P
)
?1
)W(?
P
|W
?
0
, ?
?
0
)
(3)
Which has the same form as p(?
V
,?
V
|V).
p(V
i
|R,V
?i
,P, ?
V
,?
V
, ?) = N (?
?
i
, (?
?
i
)
?1
) (4)
Where:
?
?
i
= (?
?
i
)
?1
(?
V
?
V
+ ?
2?
k=1
N?
j=1
I
k
ij
R
k
ij
Q
jk
),
?
?
i
= ?
V
+ ?
2?
k=1
N?
j=1
I
k
ij
Q
jk
Q
T
jk
,
Q
jk
= V
j
 P
k
 is the element-wise product.
p(P
i
|R,V,P
?i
, ?
P
,?
P
, ?) = N (?
?
i
, (?
?
i
)
?1
) (5)
Where:
?
?
k
= (?
?
k
)
?1
(?
P
?
P
+ ?
N?
i,j=1
I
k
ij
R
k
ij
X
ij
),
?
?
k
= ?
P
+ ?
N?
i,j=1
I
k
ij
X
ij
X
T
ij
,
X
ij
= V
i
 V
j
The influence each perspective k has on the la-
tent word vectors is roughly propotional to the
number of non-empty entries n
k
=
?
i,j
I
k
i,j
. If
one wants to adjust the weight of each slices, this
can easily achieved by adjusting (e.g. down sam-
pling) the number of entries of each slice sampled
at each iteration.
3.2.1 Out-of-Vocabulary words
It often occurs that some of the perspectives have
greater word coverage than the others. For ex-
ample, hand-labeled word relatedness usually has
much less coverage than automatically acquired
similarities. Of course, it is typically for the hand-
labeled perspectives that the generalization is most
Algorithm 1 Gibbs Sampling for BPTF
Initialize the parameters.
repeat
Sample the hyper-parameters ?, ?
V
, ?
V
, ?
P
,
?
P
(Equation 1, 2, 3)
for i = 1 to N do
Sample V
i
(Equation 4)
end for
for k = 1 to 2 do
Sample P
k
(Equation 5)
end for
until convergence
desired. In this situation, our model can generalize
word relatedness for the sparse perspective. For
example, assume perspective k has larger vocabu-
lary coverageN
k
, while perspective l has a smaller
coverage N
l
.
There are two options for using the high vocab-
ulary word relation matrix to generalize over the
perspective with lower coverage. The most direct
way simply considers the larger vocabulary in the
BPTF R ? R
N
k
?N
k
?K
directly. A more efficient
method trains on a tensor using the smaller vocab-
ulary R ? R
N
l
?N
l
?K
, then samples the N
k
?N
l
word vectors using Equation 4.
3.3 Predictions
With MCMC method, we can approximate the
word relatedness distribution easily by averaging
over a number of samples (instead of calculating
intractable marginal distribution):
p(
?
R
k
ij
|R) ?
1
M
M
?
m=1
p(
?
R
k
ij
|V
m
i
, V
m
j
, P
m
k
, ?
m
),
wherem indicate parameters sampled from differ-
ent sampling iterations.
3.4 Scalability
The time complexity of training our model is
roughly O(n?D
2
), where n is the number of ob-
served entries in the tensor. If one is only inter-
ested in creating and re-creating word relatedness
of one single slice rather than synthesizing word
vectors, then entries in other slices can be down-
sampled at every iteration to reduce the training
time. In our model, the vector length D is not
sensitive and does not necessarily need to be very
long. Xiong et al. (2010) reported in their collab-
orative filtering experiment D = 10 usually gives
satisfactory performance.
1526
4 Experimental Evaluation
In this section, we evaluate our model by answer-
ing antonym questions. This task is especially
suitable for evaluating our model since the perfor-
mance of straight-forward look-up from the the-
sauruses we considered is poor. There are two ma-
jor limitations:
1. The thesaurus usually only contains antonym
information for word pairs with a strong con-
trast.
2. The vocabulary of the antonym entries in the
thesaurus is limited, and does not contain
many words in the antonym questions.
On the other hand, distributional similarities can
be trained from large corpora and hence have a
large coverage for words. This implies that we can
treat the thesaurus data as the first slice, and the
distributional similarities as the second slice, then
use our model to create / recreate word relatedness
on the first slice to answer antonym questions.
4.1 The GRE Antonym Questions
There are several publicly available test datasets
to measure the correctness of our word embed-
dings. In order to be able to compare with pre-
vious works, we follow the widely-used GRE test
dataset provided by (Mohammad et al., 2008),
which has a development set (consisting of 162
questions) and a test set (consisting of 950 ques-
tions). The GRE test is a good benchmark because
the words are relatively rare (19% of the words in
Mohammad?s test are not in the top 50,000 most
frequent words from Google Books (Goldberg and
Orwant, 2013)), thus it is hard to lookup answers
from a thesaurus directly with high recall. Below
is an example of the GRE antonym question:
adulterate: a. renounce b. forbid
c. purify d. criticize e. correct
The goal is to choose the most opposite word from
the target, here the correct answer is purify.
4.2 Data Resources
In our tensor model, the first slice (k = 1) con-
sists of synonyms and antonyms from public the-
sauruses, and the second slice (k = 2) consists of
cosine similarities from neural word embeddings
(example in Table 1)
4.2.1 Thesaurus
Two popular thesauruses used in other research are
the Macquarie Thesaurus and the Encarta The-
saurus. Unfortunately, their electronic versions
are not publicly available. In this work we use two
alternatives:
WordNet Words in WordNet (version 3.0) are
grouped into sense-disambiguated synonym sets
(synsets), and synsets have links between each
other to express conceptual relations. Previ-
ous works reported very different look-up perfor-
mance using WordNet (Mohammad et al., 2008;
Yih et al., 2012), we consider this difference
as different understanding of the WordNet struc-
ture. By extending ?indirect antonyms? defined in
WordNet to nouns, verbs and adverbs that similar
words share the antonyms,we achieve a look-up
performance close to Yih et al. (2012). Using this
interpretation of WordNet synonym and antonym
structure we obtain a thesaurus containing 54,239
single-token words. Antonym entries are present
for 21,319 of them with 16.5 words per entry on
average, and 52,750 of them have synonym entries
with 11.7 words per entry on average.
Roget?s Only considering single-token words,
the Roget?s Thesaurus (Kipfer, 2009) contains
47,282 words. Antonym entries are present for
8,802 of them with 4.2 words per entry on av-
erage, and 22,575 of them have synonym entries
with 20.7 words per entry on average. Although
the Roget?s Thesaurus has a less coverage on both
vocabulary and antonym pairs, it has better look-
up precision in the GRE antonym questions.
4.2.2 Distributional Similarities
We use cosine similarity of the morphRNN word
representations
2
provided by Luong et al. (2013)
as a distributional word relatedness perspective.
They used morphological structure in training re-
cursive neural networks and the learned mod-
els outperform previous works on word similarity
tasks, especially a task focused on rare words. The
vector space models were initialized from exist-
ing word embeddings trained on Wikipedia. We
use word embeddings adapted from Collobert et
al. (2011). This advantage complements the weak-
ness of the thesaurus perspective ? that it has less
coverage on rare words. The word vector data con-
tains 138,218 words, and it covers 86.9% of the
words in the GRE antonym questions. Combining
the two perspectives, we can cover 99.8% of the
1527
Dev. Set Test Set
Prec. Rec. F
1
Prec. Rec. F
1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60
Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78
Encarta PILSA + S2Net + Emebed 0.88 0.87 0.87 0.81 0.80 0.81
W&E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79
WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49
WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44
WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62
Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47
Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45
Roget BPTF 0.80 0.80 0.80 0.76 0.75 0.76
W&R lookup* 1.00 0.48 0.64 0.98 0.45 0.62
W&R lookup 0.62 0.54 0.58 0.59 0.51 0.55
W&R BPMF 0.59 0.59 0.59 0.52 0.52 0.52
W&R BPTF 0.88 0.88 0.88 0.82 0.82 0.82
Table 2: Development and test results on the GRE antonym questions. *Note: to allow comparison, in
look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word
is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random
guessing.
GRE antonym question words. Further using mor-
phology information from WordNet, the coverage
achieves 99.9%.
4.3 Tests
To answer the GRE questions, we calculateR
1
ij
for
word pair (i, j), where i is the target word and j
is one of the question?s candidates. The candidate
with the smallest similarity is then the predicted
answer. If a target word is missing in the vocabu-
lary, that question will not be answered, while if a
choice is missing, that choice will be ignored.
We first train on a tensor from a subset consist-
ing of words with antonym entries, then add all
other words using the out-of-vocabulary method
described in Section 3. During each iteration, ze-
ros are randomly added into the first slice to keep
the model from overfitting. In the meantime, the
second slice entries is randomly downsampled to
match the number of non-empty entries in the first
slice. This ensures each perspective has approxi-
mately equal influence on the latent word vectors.
We sample the parameters iteratively, and
choose the burn-in period and vector length D ac-
cording to the development set. We choose the
vector length D = 40, the burn-in period starting
from the 30
th
iterations, then averaging the relat-
edness over 200 runs. The hyper-priors used are
?
0
= 0, ?
0
= ??
0
= D, ?
0
= 1 and W
0
=
?
W
0
= I
(not tuned). Note that Yih et al. (2012) use a vec-
tor length of 300, which means our embeddings
save considerable storage space and running time.
Our model usually takes less than 30 minutes to
meet the convergence criteria (on a machine with
an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In
contrast, the MRLSA requires about 3 hours for
tensor decomposition (Chang et al., 2013).
4.4 Results
The results are summarized in Table 2. We list the
results of previous works (Yih et al., 2012; Chang
et al., 2013) at the top of the table, where the
best performance is achieved by PILSA on Encarta
with further discriminative training and embed-
ding. For comparison, we adopt the standard first
used by (Mohammad et al., 2008), where preci-
sion is the number of questions answered correctly
2
http://www-nlp.stanford.edu/ lmthang/morphoNLM/
1528
20 40 60 80 100 120 140
Number of Iterations
0.0
0.5
1.0
1.5
2.0
2.5
R
M
S
E
BPMF
BPTF
Figure 2: Convergence curves of BPMF and BPTF
in training the W&R dataset. MAE is the mean
absolute error over the synonym & antonym slice
in the training tensor.
divided by the number of questions answered. Re-
call is the number of questions answered correctly
divided by the total number of questions. BPMF
(Bayesian Probabilistic Matrix Factorization) re-
sult is derived by only keeping the synonym &
antonym slice in our BPTF model.
By using Roget?s and WordNet together, our
method increases the baseline look-up recall from
51% to 82% on the test set, while Yih?s method
increases the recall of Encarta from 56% to 80%.
This state-of-the-art performance is achieved with
the help of a neural network for fine tuning and
multiple schemes of out-of-vocabulary embed-
ding, while our method has inherent and straight-
forward ?out-of-vocabulary embedding?. While
MRLSA, which has this character as well, only
has a recall 77% when combining WordNet and
Encarta together.
WordNet records less antonym relations for
nouns, verbs and adverbs, while the GRE antonym
questions has a large coverage of them. Al-
though by extending these antonym relations us-
ing the ?indirect antonym? concept achieves better
look-up performance than Roget?s, in contrast, the
BPTF performance is actually much lower. This
implies Roget?s has better recording of antonym
relations. Mohammad et al. (2008) reproted a 23%
F-score look-up performance of WordNet which
support this claim as well. Combining WordNet
and Roget?s together can improve the look-up per-
formance further to 59% precision and 51% recall
(still not as good as Encarta look-up).
Notably, if we strictly follow our BPTF ap-
proach but only use the synonym & antonym slice
(i.e. a matrix factorization model instead of ten-
sor factorization model), this single-slice model
BPMF has performance that is only slightly bet-
ter than look-up. Meanwhile Figure 1 shows the
convergence curves of BPMF and BPTF. BPMF
actually has lower MAE after convergence. Such
behavior is caused by overfitting of BPMF on the
training data. While known entries were recreated
well, empty entries were not filled correctly. On
the other hand, note that although our BPTF model
has a higher MAE, it has much better performance
in answering the GRE antonym questions. We in-
terpret this as the regularization and generalization
effect from other slice(s). Instead of focusing on
one-slice training data, our model fills the missing
entries with the help of inter-slice relations.
We also experimented with a linear metric
learning method over the generated word vectors
(to learn a metric matrix A to measure the word
relatedness via V
T
i
AV
j
) using L-BFGS. By op-
timizing the mean square error on the synonym
& antonym slice, we can reduce 8% of the mean
square error on a held out test set, and improve
the F-score by roughly 0.5% (of a single iteration).
Although this method doesn?t give a significant
improvement, it is general and has the potential
to boost the performance in other scenarios.
5 Conclusion
In this work, we propose a method to map words
into a metric space automatically using thesaurus
data, previous vector space models, or other word
relatedness matrices as input, which is capable
of handling out-of-vocabulary words of any par-
ticular perspective. This allows us to derive the
relatedness of any given word pair and any per-
spective by the embedded word vectors with per-
perspective linear transformation. We evaluated
the word embeddings with GRE antonym ques-
tions, and the result achieves the state-of-the-art
performance.
For future works, we will extend the model and
its applications in three main directions. First, in
this model we only use a three-way tensor with
two slices, while more relations may be able to
add into it directly. Possible additional perspec-
tive slices include LSA for topic relatedness, and
corpus occurrences in engineered or induced se-
mantic patterns.
Second, we will apply the method to other tasks
that require completing a word relatedness matrix.
We evaluated the performance of our model on
1529
creating / recreating one perspective of word re-
latedness: antonymy. Perhaps using vectors gen-
erated from many kinds of perspectives would im-
prove the performance on other NLP tasks, such
as term matching employed by textual entailment
and machine translation metrics.
Third, if our model does learn the relation be-
tween semantic similarities and distributional sim-
ilarities, there may be fruitful information con-
tained in the vectors V
i
and P
k
that can be ex-
plored. One straight-forward idea is that the dot
product of perspective vectors P
k
? P
l
should be a
measurement of correlation between perspectives.
Also, a straightforward adaptation of our model
has the potential ability to capture asymmet-
ric word relatedness as well, by using a per-
perspective matrix instead of vector for the asym-
metric slices (i.e. use V
T
i
A
k
V
j
instead of
?
D
d=1
V
(d)
i
P
(d)
k
V
(d)
j
for calculating word related-
ness, where A
k
is a square matrix).
Acknowledgments
We thank Christopher Kedzie for assisting the
Semantic Technologies in IBM Watson seminar
course in which this work has been carried out,
and Kai-Wei Chang for giving detailed explana-
tion of the evaluation method in his work.
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
EMNLP.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
JASIS, 41(6):391?407.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241?247.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Barbara Ann Kipfer. 2009. Roget?s 21st Century The-
saurus, Third Edition. Philip Lief Group.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In In
Proceedings of IJCAI-03, page 14921493.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL, Sofia, Bulgaria.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751. The Association for Computational Linguistics.
Tom Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
NIPS, pages 1081?1088.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982?991. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics, 39(3):555?590.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
markov chain monte carlo. In ICML, pages 880?
887. ACM.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. International Joint Conference on
Natural Language Processing, pages 489?497.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
129?136.
Peter D. Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. Col-
ing, pages 905?912, August.
1530
Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G.
Schneider, and Jaime G. Carbonell. 2010. Tempo-
ral collaborative filtering with bayesian probabilis-
tic tensor factorization. In SDM, volume 10, pages
211?222. SIAM.
Wen-tau Yih, Geoffrey Zweig, and John C. Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP-CoNLL, pages 1212?1222. Association for
Computational Linguistics.
1531
Proceedings of the TextGraphs-8 Workshop, pages 6?10,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
JoBimText Visualizer:
A Graph-based Approach to Contextualizing Distributional Similarity
Alfio Gliozzo1 Chris Biemann2 Martin Riedl2
Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1
(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com
{biem,riedl}@cs.tu-darmstadt.de
Abstract
We introduce an interactive visualization com-
ponent for the JoBimText project. JoBim-
Text is an open source platform for large-scale
distributional semantics based on graph rep-
resentations. First we describe the underly-
ing technology for computing a distributional
thesaurus on words using bipartite graphs of
words and context features, and contextualiz-
ing the list of semantically similar words to-
wards a given sentential context using graph-
based ranking. Then we demonstrate the ca-
pabilities of this contextualized text expan-
sion technology in an interactive visualization.
The visualization can be used as a semantic
parser providing contextualized expansions of
words in text as well as disambiguation to
word senses induced by graph clustering, and
is provided as an open source tool.
1 Introduction
The aim of the JoBimText1 project is to build a
graph-based unsupervised framework for computa-
tional semantics, addressing problems like lexical
ambiguity and variability, word sense disambigua-
tion and lexical substitutability, paraphrasing, frame
induction and parsing, and textual entailment. We
construct a semantic analyzer able to self-adapt to
new domains and languages by unsupervised learn-
ing of semantics from large corpora of raw text. At
the moment, this analyzer encompasses contextual-
ized similarity, sense clustering, and a mapping of
senses to existing knowledge bases. While its pri-
mary target application is functional domain adap-
tation of Question Answering (QA) systems (Fer-
1http://sf.net/projects/jobimtext/
rucci et al, 2013), output of the semantic analyzer
has been successfully utilized for word sense disam-
biguation (Miller et al, 2012) and lexical substitu-
tion (Szarvas et al, 2013). Rather than presenting
the different algorithms and technical solutions cur-
rently implemented by the JoBimText community in
detail, in this paper we will focus on available func-
tionalities and illustrate them using an interactive vi-
sualization.
2 Underlying Technologies
While distributional semantics (de Saussure, 1959;
Harris, 1951; Miller and Charles, 1991) and the
computation of distributional thesauri (Lin, 1998)
has been around for decades, its full potential has yet
to be utilized in Natural Language Processing (NLP)
tasks and applications. Structural semantics claims
that meaning can be fully defined by semantic oppo-
sitions and relations between words. In order to per-
form a reliable knowledge acquisition process in this
framework, we gather statistical information about
word co-occurrences with syntactic contexts from
very large corpora. To avoid the intrinsic quadratic
complexity of the similarity computation, we have
developed an optimized process based on MapRe-
duce (Dean and Ghemawat, 2004) that takes advan-
tage of the sparsity of contexts, which allows scal-
ing the process through parallelization. The result of
this computation is a graph connecting the most dis-
criminative contexts to terms and explicitly linking
the most similar terms. This graph represents local
models of semantic relations per term rather than a
model with fixed dimensions. This representation
departs from the vector space metaphor (Schu?tze,
1993; Erk and Pado?, 2008; Baroni and Zamparelli,
6
2010), commonly employed in other frameworks for
distributional semantics such as LSA (Deerwester et
al., 1990) or LDA (Blei et al, 2003).
The main contribution of this paper is to de-
scribe how we operationalize semantic similarity in
a graph-based framework and explore this seman-
tic graph using an interactive visualization. We de-
scribe a scalable and flexible computation of a dis-
tributional thesaurus (DT), and the contextualization
of distributional similarity for specific occurrences
of language elements (i.e. terms). For related works
on the computation of distributional similarity, see
e.g. (Lin, 1998; Lin and Dyer, 2010).
2.1 Holing System
To keep the framework flexible and abstract with re-
spect to the pre-processing that identifies structure
in language material, we introduce the holing op-
eration, cf. (Biemann and Riedl, 2013). It is ap-
plied to observations over the structure of text, and
splits these observations into a pair of two parts,
which we call the ?Jo? and the ?Bim?2. All JoBim
pairs are maintained in the bipartite First-Order Jo-
Bim graph TC(T,C,E) with T set of terms (Jos),
C set of contexts (Bims), and e(t, c, f) ? E edges
between t ? T , c ? C with frequency f . While
these parts can be thought of as language elements
referred to as terms, and their respective context fea-
tures, splits over arbitrary structures are possible (in-
cluding pairs of terms for Jos), which makes this
formulation more general than similar formulations
found e.g. in (Lin, 1998; Baroni and Lenci, 2010).
These splits form the basis for the computation of
global similarities and for their contextualization. A
Holing System based on dependency parses is illus-
trated in Figure 1: for each dependency relation, two
JoBim pairs are generated.
2.2 Distributed Distributional Thesaurus
Computation
We employ the Apache Hadoop MapReduce Fram-
work3, and Apache Pig4, for parallelizing and dis-
tributing the computation of the DT. We describe
this computation in terms of graph transformations.
2arbitrary names to emphasize the generality, should be
thought of as ?term? and ?context?
3http://hadoop.apache.org
4http://pig.apache.org/
Figure 1: Jos and Bims generated applying a dependency
parser (de Marneffe et al, 2006) to the sentence I suffered
from a cold and took aspirin. The @@ symbolizes the
hole.
Staring from the JoBim graph TC with counts as
weights, we first apply a statistical test5 to com-
pute the significance of each pair (t, c), then we only
keep the p most significant pairs per t. This consti-
tutes our first-order graph for Jos FOJO. In analogy,
when keeping the p most significant pairs per c, we
can produce the first-order graph for Bims FOBIM .
The second order similarity graph for Jos is defined
as SOJO(T,E) with Jos t1, t2 ? T and undirected
edges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?
FOJO, e(t2, c) ? FOJO}|, which defines similar-
ity between Jos as the number of salient features
two Jos share. SOJO defines a distributional the-
saurus. In analogy, SOBIM is defined over the
shared Jos for pairs of Bims and defines similar-
ity of contexts. This method, which can be com-
puted very efficiently in a few MapReduce steps, has
been found superior to other measures for very large
datasets in semantic relatedness evaluations in (Bie-
mann and Riedl, 2013), but could be replaced by any
other measure without interfering with the remain-
der of the system.
2.3 Contextualization with CRF
While the distributional thesaurus provides the sim-
ilarity between pairs of terms, the fidelity of a par-
ticular expansion depends on the context. From the
term-context associations gathered in the construc-
tion of the distributional thesaurus we effectively
have a language model, factorized according to the
holing operation. As with any language model,
smoothing is critical to performance. There may be
5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,
2004)
7
many JoBim (term-context) pairs that are valid and
yet under represented in the corpus. Yet, there may
be some similar term-context pair that is attested in
the corpus. We can find similar contexts by expand-
ing the term arguments with similar terms. However,
again we are confronted with the fact that the simi-
larity of these terms depends on the context.
This suggests some technique of joint inference
to expand terms in context. We use marginal in-
ference in a conditional random field (CRF) (Laf-
ferty et al, 2001). A particular world, x is defined
as single, definite sequence of either original or ex-
panded words. The weight of the world, w(x) de-
pends on the degree to which the term-context as-
sociations present in this sentence are present in the
corpus and the general out-of-context similarity of
each expanded term to the corresponding term in the
original sentence. Therefore the probability associ-
ated with any expansion t for any position xi is given
by Equation 1. Where Z is the partition function, a
normalization constant.
P (xi = t) =
1
Z
?
{x | xi=t}
ew(x) (1)
The balance between the plausibility of an ex-
panded sentence according to the language model,
and its per-term similarity to the original sentence is
an application specific tuning parameter.
2.4 Word Sense Induction, Disambiguation
and Cluster Labeling
The contextualization described in the previous sub-
section performs implicit word sense disambigua-
tion (WSD) by ranking contextually better fitting
similar terms higher. To model this more explicitly,
and to give rise to linking senses to taxonomies and
domain ontologies, we apply a word sense induction
(WSI) technique and use information extracted by
IS-A-patterns (Hearst, 1992) to label the clusters.
Using the aggregated context features of the clus-
ters, the word cluster senses are assigned in con-
text. The DT entry for each term j as given in
SOJO(J,E) induces an open neighborhood graph
Nj(Vj , Ej) with Vj = {j?|e(j, j?, s) ? E) and Ej
the projection of E regarding Vj , consisting of sim-
ilar terms to j and their similarities, cf. (Widdows
and Dorow, 2002).
We cluster this graph using the Chinese Whispers
graph clustering algorithm (Biemann, 2010), which
finds the number of clusters automatically, to ob-
tain induced word senses. Running shallow, part-
of-speech-based IS-A patterns (Hearst, 1992) over
the text collection, we obtain a list of extracted IS-
A relationships between terms, and their frequency.
For each of the word clusters, consisting of similar
terms for the same target term sense, we aggregate
the IS-A information by summing the frequency of
hypernyms, and multiplying this sum by the number
of words in the cluster that elicited this hypernym.
This results in taxonomic information for labeling
the clusters, which provides an abstraction layer for
terms in context6. Table 1 shows an example of this
labeling from the model described below. The most
similar 200 terms for ?jaguar? have been clustered
into the car sense and the cat sense and the high-
est scoring 6 hypernyms provide a concise descrip-
tion of these senses. This information can be used
to automatically map these cluster senses to senses
in an taxonomy or ontology. Occurrences of am-
biguous words in context can be disambiguated to
these cluster senses comparing the actual context
with salient contexts per sense, obtained by aggre-
gating the Bims from the FOJO graph per cluster.
sense IS-A labels similar terms
jaguar
N.0
car, brand,
company,
automaker,
manufacturer,
vehicle
geely, lincoln-mercury,
tesla, peugeot, ..., mit-
subishi, cadillac, jag, benz,
mclaren, skoda, infiniti,
sable, thunderbird
jaguar
N.1
animal, species,
wildlife, team,
wild animal, cat
panther, cougar, alligator,
tiger, elephant, bull, hippo,
dragon, leopard, shark,
bear, otter, lynx, lion
Table 1: Word sense induction and cluster labeling exam-
ple for ?jaguar?. The shortened cluster for the car sense
has 186 members.
3 Interactive Visualization
3.1 Open Domain Model
The open domain model used in the current vi-
sualization has been trained from newspaper cor-
6Note that this mechanism also elicits hypernyms for unam-
biguous terms receiving a single cluster by the WSI technique.
8
Figure 2: Visualization GUI with prior expansions for
?cold?. Jobims are visualized on the left, expansions on
the right side.
pora using 120 million sentences (about 2 Giga-
words), compiled from LCC (Richter et al, 2006)
and the Gigaword (Parker et al, 2011) corpus. We
constructed a UIMA (Ferrucci and Lally, 2004)
pipeline, which tokenizes, lemmatizes and parses
the data using the Stanford dependency parser (de
Marneffe et al, 2006). The last annotator in the
pipeline annotates Jos and Bims using the collapsed
dependency relations, cf. Fig. 1. We define the lem-
matized forms of the terminals including the part-
of-speech as Jo and the lemmatized dependent word
and the dependency relation name as Bim.
3.2 Interactive Visualization Features
Evaluating the impact of this technology in applica-
tions is an ongoing effort. However, in the context
of this paper, we will show a visualization of the ca-
pabilities allowed by this flavor of distributional se-
mantics. The visualization is a GUI as depicted in
Figure 2, and exemplifies a set of capabilities that
can be accessed through an API. It is straightfor-
ward to include all shown data as features for seman-
tic preprocessing. The input is a sentence in natural
language, which is processed into JoBim pairs as de-
scribed above. All the Jos can be expanded, showing
their paradigmatic relations with other words.
We can perform this operation with and without
taking the context into account (cf. Sect. 2.3). The
latter performs an implicit disambiguation by rank-
ing similar words higher if they fit the context. In
the example, the ?common cold? sense clearly dom-
inates in the prior expansions. However, ?weather?
and ?chill? appear amongst the top-similar prior ex-
pansions.
We also have implemented a sense view, which
displays sense clusters for the selected word, see
Figure 3. Per sense, a list of expansions is pro-
vided together with a list of possible IS-A types. In
this example, the algorithm identified two senses of
?cold? as a temperature and a disease (not all clus-
ter members shown). Given the JoBim graph of the
context (as displayed left in Fig. 2), the particular
occurrence of ?cold? can be disambiguated to Clus-
ter 0 in Fig. 3, since its Bims ?amod(@@,nasty)?
and ?-dobj(catch, @@)? are found in FOJO for far
more members of cluster 0 than for members of clus-
ter 1. Applications of this type of information in-
clude knowledge-based word sense disambiguation
(Miller et al, 2012), type coercion (Kalyanpur et al,
2011) and answer justification in question answering
(Chu-Carroll et al, 2012).
4 Conclusion
In this paper we discussed applications of the Jo-
BimText platform and introduced a new interactive
visualization which showcases a graph-based unsu-
pervised technology for semantic processing. The
implementation is operationalized in a way that it
can be efficiently trained ?off line? using MapRe-
duce, generating domain and language specific mod-
els for distributional semantics. In its ?on line? use,
those models are used to enhance parsing with con-
textualized text expansions of terms. This expansion
step is very efficient and runs on a standard laptop,
so it can be used as a semantic text preprocessor. The
entire project, including pre-computed data models,
is available in open source under the ASL 2.0, and
allows computing contextualized lexical expansion
on arbitrary domains.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Comp. Ling., 36(4):673?721.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: representing adjective-noun
constructions in semantic space. In Proc. EMNLP-
2010, pages 1183?1193, Cambridge, Massachusetts.
C. Biemann and M. Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual simi-
larity. Journal of Language Modelling, 1(1):55?95.
C. Biemann. 2010. Co-occurrence cluster features for
lexical substitutions in context. In Proceedings of
TextGraphs-5, pages 55?59, Uppsala, Sweden.
9
Figure 3: Senses induced for the term ?cold?.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022,
March.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: search and candidate generation. IBM
J. Res. Dev., 56(3):300?311.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. LREC-2006, Genova,
Italy.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. OSDI
?04, San Francisco, CA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proc. EMNLP-
2008, pages 897?906, Honolulu, Hawaii.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
D. Ferrucci and A. Lally. 2004. UIMA: An Architectural
Approach to Unstructured Information Processing in
the Corporate Research Environment. In Nat. Lang.
Eng. 2004, pages 327?348.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.
Mueller. 2013. Watson: Beyond Jeopardy! Artificial
Intelligence, 199-200:93?105.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING-
1992, pages 539?545, Nantes, France.
A. Kalyanpur, J.W. Murdock, J. Fan, and C. Welty. 2011.
Leveraging community-built knowledge for type co-
ercion in question answering. In Proc. ISWC 2011,
pages 144?156. Springer.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML 2001, pages 282?289, San Francisco, CA, USA.
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan & Claypool Publishers, San
Rafael, CA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768?774,
Montre?al, Quebec, Canada.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych. 2012.
Using distributional similarity for lexical expansion
in knowledge-based word sense disambiguation. In
Proc. COLING-2012, pages 1781?1796, Mumbai, In-
dia.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proc. IS-LTC 2006, Ljubljana, Slovenia.
H. Schu?tze. 1993. Word space. In Advances in Neu-
ral Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.
G. Szarvas, C. Biemann, and I. Gurevych. 2013. Super-
vised all-words lexical substitution using delexicalized
features. In Proc. NAACL-2013, Atlanta, GA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING-
2002, pages 1?7, Taipei, Taiwan.
10
