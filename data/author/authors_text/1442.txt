Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 907?916,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning Graph Walk Based Similarity Measures for Parsed Text
Einat Minkov?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
einat@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
wcohen@cs.cmu.edu
Abstract
We consider a parsed text corpus as an in-
stance of a labelled directed graph, where
nodes represent words and weighted directed
edges represent the syntactic relations be-
tween them. We show that graph walks, com-
bined with existing techniques of supervised
learning, can be used to derive a task-specific
word similarity measure in this graph. We also
propose a new path-constrained graph walk
method, in which the graph walk process is
guided by high-level knowledge about mean-
ingful edge sequences (paths). Empirical eval-
uation on the task of named entity coordinate
term extraction shows that this framework is
preferable to vector-based models for small-
sized corpora. It is also shown that the path-
constrained graph walk algorithm yields both
performance and scalability gains.
1 Introduction
Graph-based similarity measures have been used
for a variety of language processing applications.
In this paper we assume directed graphs, where
typed nodes denote entities and labelled directed
and weighted edges denote the relations between
them. In this framework, graph walks can be ap-
plied to draw a measure of similarity between the
graph nodes. Previous works have applied graph
walks to draw a notion of semantic similarity over
such graphs that were carefully designed and man-
ually tuned, based on WordNet reations (Toutanova
?Current address: Nokia Research Center Cambridge, Cam-
bridge, MA 02142, USA.
et al, 2004; Collins-Thompson and Callan, 2005;
Hughes and Ramage, 2007).
While these and other researchers have used
WordNet to evaluate similarity between words, there
has been much interest in extracting such a measure
from text corpora (e.g., (Snow et al, 2005; Pado?
and Lapata, 2007)). In this paper, we suggest pro-
cessing dependency parse trees within the general
framework of directed labelled graphs. We construct
a graph that directly represents a corpus of struc-
tured (parsed) text. In the suggested graph scheme,
nodes denote words and weighted edges represent
the dependency relations between them. We apply
graph walks to derive an inter-word similarity mea-
sure. We further apply learning techniques, adapted
to this framework, to improve the derived corpus-
based similarity measure.
The learning methods applied include existing
learning techniques, namely edge weight tuning,
where weights are associated with the edge types,
and discriminative reranking of graph nodes, using
features that describe the possible paths between a
graph node and the initial ?query nodes? (Minkov
and Cohen, 2007).
In addition, we outline in this paper a novel
method for learning path-constrained graph walks.
While reranking allows use of high-level features
that describe properties of the traversed paths, the
suggested algorithm incorporates this information in
the graph walk process. More specifically, we allow
the probability flow in the graph to be conditioned
on the history of the walk. We show that this method
results in improved performance as it directs proba-
bility flow to meaningful paths. In addition, it leads
907
to substantial gains in terms of runtime performance.
The graph representation and the set of learning
techniques suggested are empirically evaluated on
the task of coordinate term extraction1 from small
to moderately sized corpora, where we compare
them against vector-based models, including a state-
of-the-art syntactic distributional similarity method
(Pado? and Lapata, 2007). It is shown that the graph
walk based approach gives preferable results for the
smaller datasets (and comparable otherwise), where
learning yields significant gains in accuracy.
There are several contributions of this paper.
First, we represent dependency-parsed corpora
within a general graph walk framework, and derive
inter-word similarity measures using graph walks
and learning techniques available in this framework.
To our knowledge, the application of graph walks to
parsed text in general, and to the extraction of coor-
dinate terms in particular, is novel. Another main
contribution of this paper is the path-constrained
graph walk variant, which is a general learning tech-
nique for calculating the similarity between graph
nodes in directed and labelled graphs.
Below we first outline our proposed scheme for
representing a dependency-parsed text corpus as a
graph, and provide some intuitions about the asso-
ciated similarity metric (Section 2). We then give
an overview of the graph-walk based similarity met-
ric (Section 3), as well as the known edge weight
tuning and reranking learning techniques (Section
4). We next present the proposed algorithm of path-
constrained graph walks (Section 5). The paper pro-
ceeds with a review of related work (Section 6), a
discussion of the coordinate term extraction task,
empirical evaluation and our conclusions (Sections
7-9).
2 Representing a Corpus as a Graph
A typed dependency parse tree consists of directed
links between words, where dependencies are la-
belled with the relevant grammatical relation (e.g.,
nominal subject, indirect object etc.). We suggest
representing a text corpus as a connected graph
of dependency structures, according to the scheme
shown in Figure 1. The graph shown in the figure
1In particular, we focus on the extraction of named entity
classes.
Figure 1: The suggested graph schema, demonstrated for
a two-sentence corpus.
includes the dependency analysis of two sentences:
?boys like playing with all kinds of cars?, and ?girls
like playing with dolls?. In the graph, each word
mention is represented as a node, which includes the
index of the sentence in which it appears, as well
as its position within the sentence. Word mentions
are marked as circles in the figure. The ?type? of
each word ? henceforth a term node ? is denoted by
a square in the figure. Each word mention is linked
to the corresponding term; for example, the nodes
?like1? and ?like2? represent distinct word mentions
and both nodes are linked to the term ?like?. For
every edge in the graph, we add another edge in the
opposite direction (not shown in the figure); for ex-
ample, an inverse edge exists from ?like1? to ?girls1?
with an edge labelled as ?nsubj-inv?. The resulting
graph is highly interconnected and cyclic.
We will apply graph walks to derive an extended
measure of similarity, or relatedness, between word
terms (as defined above). For example, starting from
the term ?girls?, we will reach the semantically re-
lated term ?boys? via the following two paths:
(1) girls mention?? girls1 nsubj?? like1 as?term?? like mention??
like2 nsubj?inverse?? boys2 as?term?? boys
(2) girls mention?? girls1 nsubj?? like1 partmod?? playing1
as?term?? playing mention?? playing2 partmod?inverse?? like2
nsubj?inverse?? boys2 as?term?? boys .
Intuitively, in a graph representing a large cor-
pus, terms that are more semantically related will
be linked by a larger number of connecting paths. In
addition, shorter connecting paths may be in general
more meaningful. In the next section we show that
908
the graph walk paradigm addresses both of these re-
quirements. Further, different edge types, as well as
the paths traversed, are expected to have varying im-
portance in different types of word similarity (for ex-
ample, verbs and nouns are associated with different
connectivity patterns). These issues are addressed
using learning.
3 Graph Walks and Similarity Queries
This section provides a quick overview of the graph
walk induced similarity measure. For details, the
reader is referred to previous publications (e.g.,
(Toutanova et al, 2004; Minkov and Cohen, 2007)).
In summary, similarity between two nodes in the
graph is defined by a weighted graph walk process,
where an edge of type ` is assigned an edge weight,
?`, determined by its type.2 The transition proba-
bility of reaching node y from node x over a single
time step, Pr(x ?? y), is defined as the weight
of their connecting edge, ?l, normalized by the to-
tal outgoing weight from x. Given these transition
probabilities, and starting from an initial distribu-
tion Vq of interest (a query), we perform a graph
walk for a finite number of steps K. Further, at each
step of the walk, a proportion ? of the probability
mass at every node is emitted. Thus, this model ap-
plies exponential decay on path length. The final
probability distribution of this walk over the graph
nodes, which we denote as R, is computed as fol-
lows: R = ?Ki=1 ?iVqMi, where M is the transi-
tion matrix.3 The answer to a query, Vq, is a list of
nodes, ranked by the scores in the final distribution
R. In this multi-step walk, nodes that are reached
from the query nodes by many shorter paths will be
assigned a higher score than nodes connected over
fewer longer paths.
4 Learning
We consider a supervised setting, where we are
given a dataset of example queries and labels over
the graph nodes, indicating which nodes are relevant
to which query. For completeness, we describe here
two methods previously described by Minkov and
2In this paper, we consider either uniform edge weights; or,
learn the set of weights ? from examples.
3We tune K empirically and set ? = 0.5, as in (Minkov and
Cohen, 2007).
Cohen (Minkov and Cohen, 2007): a hill-climbing
method that tunes the graph weights; and a reranking
method. We also specify the feature set to be used by
the reranking method in the domain of parsed text.
4.1 Weight Tuning
There are several motivations for learning the graph
weights ? in this domain. First, some dependency
relations ? foremost, subject and object ? are in gen-
eral more salient than others (Lin, 1998; Pado? and
Lapata, 2007). In addition, dependency relations
may have varying importance per different notions
of word similarity (e.g., noun vs. verb similarity
(Resnik and Diab, 2000)). Weight tuning allows the
adaption of edge weights to each task (i.e., distribu-
tion of queries).
The weight tuning method implemented in this
work is based on an error backpropagation hill
climbing algorithm (Diligenti et al, 2005). The al-
gorithm minimizes the following cost function:
E = 1N
?
z?N
ez =
1
N
?
z?N
1
2(pz ? p
Opt
z )2
where ez is the error for a target node z defined as the
squared difference between the final score assigned
to z by the graph walk, pz , and some ideal score ac-
cording to the example?s labels, pOptz .4 Specifically,
pOptz is set to 1 in case that the node z is relevant
or 0 otherwise. The error is averaged over a set of
example instantiations of size N . The cost function
is minimized by gradient descent where the deriva-
tive of the error with respect to an edge weight ?`
is derived by decomposing the walk into single time
steps, and considering the contribution of each node
traversed to the final node score.
4.2 Node Reranking
Reranking of the top candidates in a ranked list
has been successfully applied to multiple NLP tasks
(Collins, 2002; Collins and Koo, 2005). In essence,
discriminative reranking allows the re-ordering of
results obtained by methods that perform some form
of local search, using features that encode higher
level information.
4For every example query, a handful of the retrieved nodes
are considered, including both relevant and irrelevant nodes.
909
A number of features describing the set of paths
from Vq can be conveniently computed in the pro-
cess of executing the graph walk, and it has been
shown that reranking using these features can im-
prove results significantly. It has also been shown
that reranking is complementary to weight tuning
(Minkov and Cohen, 2007), in the sense that the
two techniques can be usefully combined by tuning
weights, and then reranking the results.
In the reranking approach, for every training ex-
ample i (1 ? i ? N ), the reranking algorithm is
provided with the corresponding output ranked list
of li nodes. Let zij be the output node ranked at rank
j in li, and let pzij be the probability assigned to zij
by the graph walk. Each output node zij is repre-
sented through m features, which are computed by
pre-defined feature functions f1, . . . , fm. The rank-
ing function for node zij is defined as:
F (zij , ??) = ?0log(pzij ) +
m
?
k=1
?kfk(zij)
where ?? is a vector of real-valued parameters. Given
a new test example, the output of the model is the
output node list reranked by F (zij , ??). To learn the
parameter weights ??, we here applied a boosting
method (Collins and Koo, 2005) (see also (Minkov
et al, 2006)).
4.2.1 Features
We evaluate the following feature templates.
Edge label sequence features indicate whether a par-
ticular sequence of edge labels `i occurred, in a
particular order, within the set of paths leading to
the target node zij . Lexical unigram feature indi-
cate whether a word mention whose lexical value
is tk was traversed in the set of paths leading to
zij . Finally, the Source-count feature indicates the
number of different source query nodes that zij was
reached from. The intuition behind this last fea-
ture is that nodes linked to multiple query nodes,
where applicable, are more relevant. For exam-
ple, for the query term ?girl? in the graph depicted
in Figure 1, the target node ?boys? is described
by the features (denoted as feature-name.feature-
value): sequence.nsubj.nsubj-inv (where mention
and as-term edges are omitted) , lexical.??like? etc.
In this work, the features encoded are binary.
However, features can be assugned numeric weights
that corresponds to the probability of the indicator
being true for any path between x and zij (Cohen
and Minkov, 2006).
5 Path-Constrained Graph Walk
While node reranking allows the incorporation of
high-level features that describe the traversed paths,
it is desirable to incorporate such information ear-
lier in the graph walk process. In this paper, we
suggest a variant of a graph-walk, which is con-
strained by path information. Assume that prelim-
inary knowledge is available that indicates the prob-
ability of reaching a relevant node after following a
particular edge type sequence (path) from the query
distribution Vq to some node x. Rather than fix the
edge weights ?, we can evaluate the weights of the
outgoing edges from node x dynamically, given the
history of the walk (the path) up to this node. This
should result in gains in accuracy, as paths that lead
mostly to irrelevant nodes can be eliminated in the
graph walk process. In addition, scalability gains
are expected, for the same reason.
We suggest a path-constrained graph walk algo-
rithm, where path information is maintained in a
compact path-tree structure constructed based on
training examples. Each vertex in the path tree de-
notes a particular walk history. In applying the graph
walk, the nodes traversed are represented as a set of
node pairs, comprised of the graph node and the cor-
responding vertices in the path tree. The outgoing
edge weights from each node pair will be estimated
according to the respective vertex in the path tree.
This approach needs to address two subtasks: learn-
ing of the path-tree; and updating of the graph walk
paradigm to co-sample from the graph and the path
tree. We next describe these two components in de-
tail.
The Path-Tree
We construct a path-tree T using a training set
of N example queries. Let a path p be a sequence
of k < K edge types (where K is the maximum
number of graph walk steps). For each training ex-
ample, we recover all of the connecting paths lead-
ing to the top M (correct and incorrect) nodes. We
consider only acyclic paths. Let each path p be as-
sociated with its count, within the paths leading to
the correct nodes, denoted as C+p . Similarly, the
910
Figure 2: An example path-tree.
count within paths leading to the negatively labelled
nodes is denoted C?p . The full set of paths observed
is then represented as a tree.5 The leaves of the
tree are assigned a Laplace-smoothed probability:
Pr(p) = C
+
p +1
C+p +C?p +2
.
Given path probabilities, they are propagated
backwards to all tree vertices, applying the MAX
operator.6 Consider the example given in Figure 2.
The path-tree in the figure includes three paths (con-
structed from edge types k, l,m, n). The top part
of the figure gives the paths? associated counts, and
the bottom part of the figure gives the derived outgo-
ing edge probabilities at each vertex. This path-tree
specifies, for example, that given an edge of type l
was traversed from the root, the probability of reach-
ing a correct target node is 0.9 if an edge of type n
is followed, whereas the respective probability if an
edge of type m is followed is estimated at a lower
0.2.
A Concurrent Graph-walk
Given a generated path tree, we apply path-
constrained graph walks that adhere both to the
topology of the graph G, and to the path tree T .
Walk histories of each node x visited in the walk
are compactly represented as pairs < t, x >, where
t denotes the relevant vertex in the path tree. For
example, suppose that after one walk step, the main-
tained node-history pairs include < T (l), x1 > and
< T (m), x2 >. If x3 is reached in the next walk step
5The conversion to a tree is straight-forward, where identical
path prefixes are merged.
6Another possibility is to average the downstream cumula-
tive counts at each vertex. The MAX operation gave better re-
sults in our experiments.
Given: graph G, path-tree T , query distribution V0,
number of steps K
Initialize: for each xi ? V0, assign a pair
< root(T ), xi >
Repeat for steps k = 0 to K:
For each < ti, xi >? Vk:
Let L be the set of outgoing edge labels from xi, in G.
For each lm ? L:
For each xj ? G s.t., xi lm?? xj , add < tj , xj > to
Vk+1, where tj ? T , s.t. ti lm?? tj , with probability
Pr(xi|Vk) ? Pr(lm|ti, T ). (The latter probabilities
should be normalized with respect to xi.)
If ti is a terminal node in T , emit xi with probability
Pr(xi|Vk)? Pr(ti|T ).
Figure 3: Pseudo-code for path-constrained graph walk
from both x1 and x2 over paths included in the path-
tree, it will be represented by multiple node pairs,
e.g., < T (l ? n), x3 > and < T (m ? l, x3 >.
A pseudo-code for a path-constrained graph walk is
given in Figure 3. It is straight-forward to discard
paths in T that are associated with a lower proba-
bility than some threshold. A threshold of 0.5, for
example, implies that only paths that led to a major-
ity of positively labelled nodes in the training set are
followed.
6 Related Work
Graph walks over typed graphs have been applied
to derive semantic similarity for NLP problems us-
ing WordNet as a primary information source. For
instance, Hughes and Ramage (2007) constructed a
graph which represented various types of word re-
lations from WordNet, and compared random-walk
similarity to similarity assessments from human-
subject trials. Random-walk similarity has also been
used for lexical smoothing for prepositional word
attachment (Toutanova et al, 2004) and query ex-
pansion (Collins-Thompson and Callan, 2005). In
contrast to these works, our graph representation de-
scribes parsed text and has not been (consciously)
engineered for a particular task. Instead, we in-
clude learning techniques to optimize the graph-
walk based similarity measure. The learning meth-
ods described in this paper can be readily applied to
911
other directed and labelled entity-relation graphs.7
The graph representation described in this paper
is perhaps most related to syntax-based vector space
models, which derive a notion of semantic similar-
ity from statistics associated with a parsed corpus
(Grefenstette, 1994; Lin, 1998; Pado? and Lapata,
2007). In most cases, these models construct vectors
to represent each word wi, where each element in the
vector for wi corresponds to particular ?context? c,
and represents a count or an indication of whether
wi occurred in context c. A ?context? can refer to
simple co-occurrence with another word wj , to a
particular syntactic relation to another word (e.g., a
relation of ?direct object? to wj), etc. Given these
word vectors, inter-word similarity is evaluated us-
ing some appropriate similarity measure for the vec-
tor space, such as cosine vector similarity, or Lin?s
similarity (Lin, 1998).
Recently, Pado? and Lapata (Pado? and Lapata,
2007) have suggested an extended syntactic vector
space model called dependency vectors, in which
rather than simple counts, the components of a
word vector of contexts consist of weighted scores,
which combine both co-occurrence frequency and
the importance of a context, based on properties of
the connecting dependency paths. They considered
two different weighting schemes: a length weight-
ing scheme, assigning lower weight to longer con-
necting paths; and an obliqueness weighting hierar-
chy (Keenan and Comrie, 1977), assigning higher
weight to paths that include grammatically salient
relations. In an evaluation of word pair similar-
ity based on statistics from a corpus of about 100
million words, they show improvements over sev-
eral previous vector space models. Below we will
compare our framework to that of Pado? and Lap-
ata. One important difference is that while Pado? and
Lapata make manual choices (regarding the set of
paths considered and the weighting scheme), we ap-
ply learning to adjust the analogous parameters.
7 Extraction of Coordinate Terms
We evaluate the text representation schema and the
proposed set of graph-based similarity measures on
the task of coordinate term extraction. In particular,
7We refer the reader to the TextGraph workshop proceed-
ings, http://textgraphs.org.
we evaluate the extraction of named entities, includ-
ing city names and person names from newswire
data, using word similarity measures. Coordinate
terms reflect a particular type of word similarity
(relatedness), and are therefore an appropriate test
case for our framework. While coordinate term ex-
traction is often addressed by a rule-based (tem-
plates) approach (Hearst, 1992), this approach was
designed for very large corpora such as the Web,
where the availability of many redundant documents
allows use of high-precision and low-recall rules.
In this paper we focus on relatively small corpora.
Small limited text collections may correspond to
documents residing on a personal desktop, email
collections, discussion groups and other specialized
sets of documents.
The task defined in the experiments is to retrieve
a ranked list of city or person names given a small
set of seeds. This task is implemented in the graph
as a query, where we let the query distribution Vq be
uniform over the given seeds (and zero elsewhere).
Ideally, the resulting ranked list will be populated
with many additional city, or person, names.
We compare graph walks to dependency vec-
tors (DV) (Pado? and Lapata, 2007),8 as well as to
a vector-based bag-of-words co-occurrence model.
DV is a state-of-the-art syntactic vector-based model
(see Section 6). The co-occurrence model represents
a more traditional approach, where text is processed
as a stream rather than syntactic structures. In ap-
plying the vector-space based methods, we compute
a similarity score between every candidate from the
corpus and each of the query terms, and then aver-
age these scores (as the query distributions are uni-
form) to construct a ranked list. For efficiency, in
the vector-based models we limit the considered set
of candidates to named entities. Similarly, the graph
walk results are filtered to include named entities.9
Corpora. As the experimental corpora, we use
the training set portion of the MUC-6 dataset (MUC,
1995) as well as articles from the Associated Press
(AP) extracted from the AQUAINT corpus (Bilotti
8We used the code from http://www.coli.uni-
saarland.de/ pado/dv.html, and converted the underlying
syntactic patterns to the Stanford dependency parser conven-
tions.
9In general, graph walk results can be filtered by various
word properties, e.g., capitalization pattern, or part-of-speech.
912
Corpus words nodes edges unique NEs
MUC 140K 82K 244K 3K
MUC+AP 2,440K 1,030K 3,550K 36K
Table 1: Corpus statistics
et al, 2007), all parsed using the Stanford depen-
dency parser (de Marneffe et al, 2006).10 The MUC
corpus provides true named entity tags, while the
AQUAINT corpus includes automatically generated,
noisy, named entity tags. Statistics on the experi-
mental corpora and their corresponding graph rep-
resentation are detailed in Table 1. As shown, the
MUC corpus contains about 140 thousand words,
whereas the MUC+AP experimental corpus is sub-
stantially larger, containing about 2.5 million words.
We generated 10 queries, each comprised of 4 city
names selected randomly according to the distribu-
tion of city name mentions in MUC-6. Similarly,
we generated a set of 10 queries that include 4 per-
son names selected randomly from the MUC corpus.
(The MUC corpus was appended to AP, so that the
same query sets are applicable in both cases.) For
each task, we use 5 queries for training and tuning
and the remaining queries for testing.
8 Experimental Results
Experimental setup. We evaluated cross-validation
performance over the training queries in terms of
mean average precision for varying walk lengths K.
We found that beyond K = 6 improvements were
small (and in fact deteriorated for K = 9). We there-
fore set K = 6. Weight tuning was trained using
the training queries and two dozens of target nodes
overall. In reranking, we set a feature count cutoff
of 3, in order to avoid over-fitting. Reranking was
applied to the top 200 ranked nodes output by the
graph walk using the tuned edge weights. Finally,
path-trees were constructed using the top 20 correct
nodes and 20 incorrect nodes retrieved by the uni-
formly weighted graph walk. In the experiments,
we apply a threshold of 0.5 to the path constrained
graph walk method.
We note that for learning, true labels were used for
the fully annotated MUC corpus (we hand labelled
all of the named entities of type location in the cor-
pus as to whether they were city names). However,
10http://nlp.stanford.edu/software/lex-parser.shtml; sen-
tences longer than 70 words omitted.
noisy negative examples were considered for the
larger automatically annotated AP corpus. (Specif-
ically, for cities, we only considered city names in-
cluded in the MUC corpus as correct answers.)
A co-occurrence vector-space model was applied
using a window of two tokens to the right and to
the left of the focus word. Inter-word similarity
was evaluated in this model using cosine similar-
ity, where the underlying co-occurrence counts were
normalized by log-likelihood ratio (Pado? and Lap-
ata, 2007). The parameters of the DV method were
set based on a cross validation evaluation (using the
MUC+AP corpus). The medium set of dependency
paths and the oblique edge weighting scheme were
found to perform best. We experimented with co-
sine as well as Lin similarity measure in combina-
tion with the dependency vectors representation. Fi-
nally, given the large number of candidates in the
MUC+AP corpus (Table 1), we show the results of
applying the considered vector-space models to the
top, high-quality, entities retrieved with reranking
for this corpus.11
Test set results. Figure 4 gives results for the city
name (top) and the person name (bottom) extraction
tasks. The left part of the figure shows results us-
ing the MUC corpus, and its right part ? using the
MUC+AP corpus. The curves show precision as a
function of rank in the ranked list, up to rank 100.
(For this evaluation, we hand-labeled all the top-
ranked results as to whether they are city names or
person names.) Included in the figure are the curves
of the graph-walk method with uniform weights
(G:Uw), learned weights (G:Lw), graph-walk with
reranking (Rerank) and a path-constrained graph-
walk (PCW). Also given are the results of the co-
occurrence model (CO), and the syntactic vector-
space DV model, using the Lin similarity measure
(DV:Lin). Performance of the DV model using co-
sine similarity was found comparable or inferior to
using the Lin measure, and is omitted from the fig-
ure for clarity.
Several trends can be observed from the results.
With respect to the graph walk methods, the graph
walk using the learned edge weights consistently
outperforms the graph walk with uniform weights.
Reranking and the path-constrained graph walk,
11We process the union of the top 200 results per each query.
913
MUC MUC+AP
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
G:Uw
G:Lw
CO
DV:Lin
PCW
Rerank
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Figure 4: Test results: Precision at the top 100 ranks, for the city name extraction task (top) and person name extraction
task (bottom).
however, yield superior results. Both of these learn-
ing methods utilize a richer set of features than the
graph walk and weight tuning, which can consider
only local information. In particular, while the graph
walk paradigm assigns lower importance to longer
connecting paths (as described in Section 3), rerank-
ing and the path-constrained walker allow to dis-
card short yet irrelevant paths, and by that eliminate
noise at the top ranks of the retrieved list. In gen-
eral, the results show that edge sequences carry ad-
ditional meaning compared with the individual edge
label segments traversed.
Out of the vector-based models, the co-
occurrence model is preferable for the city name
extraction task, and the syntactic dependency vec-
tors model gives substantially better performance
for person name extraction. We conjecture that city
name mentions are less structured in the underlying
text. In addition, the syntactic weighting scheme of
the DV model is probably not optimal for the case of
city names. For example, a conjunction relation was
found highly indicative for city names (see below).
However, this relation is not emphasized by the DV
weighting schema. As expected, the performance of
the vector-based models improves for larger corpora
(Terra and Clarke, 2003). These models demonstrate
good performance for the larger MUC+AP corpus,
but only mediocre performance for the smaller MUC
corpus.
Contrasting the graph-based methods with the
vector-based models, the difference in performance
in favor of reranking and PCW, especially for the
smaller corpus, can be attributed to two factors. The
first factor is learning, which optimizes performance
for the underlying data. A second factor is the incor-
poration of non-local information, encoding proper-
ties of the traversed paths.
Models. Following is a short description of the
models learned by the different methods and tasks.
Weight tuning assigned high weights to edge types
such as conj-and, prep-in and prep-from, nn, ap-
pos and amod for the city extraction task. For per-
914
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  1  2  3  4  5  6Nu
m
be
r o
f g
ra
ph
 n
od
es
 v
isi
te
d 
[lo
g_
2]
Walk steps
G:U
PCW:0
PCW:0.5
PCW:0.8
Figure 5: The graph walk exponential spread is bounded
by the path constrained walk.
son extraction, prominent edge types included subj,
obj, poss and nn. (The latter preferences are sim-
ilar to the linguistically motivated weights of DV.)
High weight features assigned by reranking for city
name extraction included, for example, lexical fea-
tures such as ?based? and ?downtown?, and edge bi-
grams such as ?prep-in-Inverse?conj-and? or ?nn-
Inverse?nn?. Positive highly predictive paths in
the constructed path tree included many symmetric
paths, such as ...?conj andInverse...?.conj and...,
...?prep inInverse...?.prep in..., for the city name
extraction task.
Scalability. Figure 5 shows the number of graph
nodes maintained in each step of the graph walk
(logarithm scale) for a typical city extraction query
and the MUC+AP corpus. As shown by the solid
line, the number of graph nodes visited using the
weighted graph walk paradigm grows exponentially
with the length of the walk. Applying a path-
constrained walk with a threshold of 0 (PCW:0) re-
duces the maximal number of nodes expanded (as
paths not observed in the training set are discarded).
As shown, increasing the threshold leads to signifi-
cant gains in scalability. Overall, query processing
time averaged at a few minutes, using a commodity
PC.
9 Conclusion and Future Directions
In this paper we make several contributions. First,
we have explored a novel but natural representation
for a corpus of dependency-parsed text, as a labelled
directed graph. We have evaluated the task of coor-
dinate term extraction using this representation, and
shown that this task can be performed using similar-
ity queries in a general-purpose graph-walk based
query language. Further, we have successfully ap-
plied learning techniques that tune weights assigned
to different dependency relations, and re-score can-
didates using features derived from the graph walk.
Another orthogonal contribution of this paper is
a path-constrained graph walk variant, where the
graph walk is guided by high level knowledge about
meaningful paths, learned from training examples.
This method was shown to yield improved perfor-
mance for the suggested graph representation, and
improved scalability compared with the local graph
walk. The method is general, and can be readily ap-
plied in similar settings.
Empirical evaluation of the coordinate term ex-
traction task shows that the graph-based framework
performs better than vector-space models for the
smaller corpus, and comparably otherwise. Over-
all, we find that the suggested model is suitable for
deep (syntactic) processing of small specialized cor-
pora. In preliminary experiments where we evalu-
ated this framework on the task of extracting general
word synonyms, using a relatively large corpus of
15 million words, we found the graph-walk perfor-
mance to be better than DV using cosine similarity
measures, but second to DV using Lin?s similarity
measure. While this set of results is incomplete, we
find that it is consistent with the results reported in
this paper.
The framework presented can be enhanced in sev-
eral ways. For instance, WordNet edges and mor-
phology relations can be readily encoded in the
graph. We believe that this framework can be ap-
plied for the extraction of more specialized no-
tions of word relatedness, as in relation extraction
(Bunescu and Mooney, 2005). The path-constrained
graph walk method proposed may be enhanced by
learning edge probabilities, using a rich set of fea-
tures. We are also interested in exploring a possi-
ble relation between the path-constrained walk ap-
proach and reinforcement learning.
Acknowledgments
The authors wish to thank the anonymous reviewers
and Hanghang Tong for useful advice. This material
is based upon work supported by Yahoo! Research.
915
References
Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric
Nyberg. 2007. Structured retrieval for question an-
swering. In SIGIR.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT-EMNLP.
William W. Cohen and Einat Minkov. 2006. A graph-
search framework for associating gene identifiers with
documents. BMC Bioinformatics, 7(440).
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In
CIKM.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Michelangelo Diligenti, Marco Gori, and Marco Mag-
gini. 2005. Learning web page scores by error back-
propagation. In IJCAI.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Dordrecht.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In EMNLP.
Edward Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and universal grammar. Linguis-
tic Inquiry, 8.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Einat Minkov and William W. Cohen. 2007. Learning to
rank typed graph walks: Local and global approaches.
In WebKDD/KDD-SNA workshop.
Einat Minkov, William W. Cohen, and Andrew Y. Ng.
2006. Contextual search and name disambiguation in
email using graphs. In SIGIR.
1995. Proceedings of the sixth message understanding
conference (muc-6). In Morgan Kaufmann Publish-
ers, Inc. Columbia, Maryland.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In CogSci.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Egidio Terra and C. L. A. Clarke. 2003. Frequency
estimates for statistical word similarity measures. In
NAACL.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In ICML.
916
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 947?954,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic Set Expansion for List Question Answering
Richard C. Wang, Nico Schlaefer, William W. Cohen, and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh PA 15213
{rcwang,nico,wcohen,ehn}@cs.cmu.edu
Abstract
This paper explores the use of set expan-
sion (SE) to improve question answering (QA)
when the expected answer is a list of entities
belonging to a certain class. Given a small
set of seeds, SE algorithms mine textual re-
sources to produce an extended list including
additional members of the class represented
by the seeds. We explore the hypothesis that
a noise-resistant SE algorithm can be used to
extend candidate answers produced by a QA
system and generate a new list of answers that
is better than the original list produced by the
QA system. We further introduce a hybrid ap-
proach which combines the original answers
from the QA system with the output from the
SE algorithm. Experimental results for several
state-of-the-art QA systems show that the hy-
brid system performs better than the QA sys-
tems alone when tested on list question data
from past TREC evaluations.
1 Introduction
Question answering (QA) systems are designed to
retrieve precise answers to questions posed in nat-
ural language. A list question expects a list as its
answer, e.g. Name the coffee-producing countries in
South America. The ability to answer list questions
has been tested as part of the yearly TREC QA eval-
uation (Dang et al, 2006; Dang et al, 2007). This
paper focuses on the use of set expansion to improve
list question answering. A set expansion (SE) algo-
rithm receives as input a few members of a class or
set, and mines various textual resources (e.g. web
pages) to produce an extended list including addi-
tional members of the class or set that are not in the
input. A well-known online SE system is Google
Sets1. This system is publicly accessible, but since it
is a proprietary system that might be changed at any
time, its results cannot be replicated reliably. We ex-
plore the hypothesis that a SE algorithm, when care-
fully designed to handle noisy inputs, can be applied
to the output from a QA system to produce an overall
list of answers for a given question that is better than
the answers produced by the QA system itself. We
propose to exploit large, redundant sources of struc-
tured and/or semi-structured data and use linguistic
analysis to seed a shallow analysis of these sources.
This is a hard problem since the linguistic evidence
used as seeds is noisy. More precisely, we combine
the QA system Ephyra (Schlaefer et al, 2007) with
the SE system SEAL (Wang and Cohen, 2007) to
create a hybrid approach that performs better than
either system by itself when tested on data from the
TREC 13-15 evaluations. In addition, we apply our
SE algorithm to answers generated by the five QA
systems that performed the best on the list questions
in the TREC 15 evaluation and report improvements
in F1 scores for four of these systems.
Section 2 of the paper gives an overview of the
QA and SE systems used for our experiments. Sec-
tion 3 describes how the SE system was adapted to
deal with noisy seeds produced by QA systems, and
Section 4 presents the details of the experimental de-
sign. Experimental results are discussed in Section
5, and the paper concludes in Section 6 with a dis-
cussion of planned future work.
1http://labs.google.com/sets
947
2 System Overview
2.1 Ephyra Question Answering System
Ephyra (Schlaefer et al, 2006; Schlaefer et al,
2007) is a QA system that has been evaluated in
the TREC QA track (Dang et al, 2006; Dang et al,
2007). The system combines three answer extrac-
tion techniques for factoid and list questions: (1) an
answer type classification approach; (2) a syntactic
pattern learning and matching approach; and (3) a
semantic extractor that uses a semantic role label-
ing system. The answer type based extractor clas-
sifies questions by their answer types and extracts
candidates of the expected types. The Ephyra pat-
tern matching approach learns textual patterns that
relate question key terms to possible answers and
applies these patterns to candidate sentences to ex-
tract factoid answers. The semantic approach gener-
ates a semantic representation of the question that is
based on predicate-argument structures and extracts
answer candidates from similar structures in the cor-
pus. The source code of the answer extractors is in-
cluded in OpenEphyra, an open source release of the
system.2
The answer candidates from these extractors are
combined and ranked by a statistical answer selec-
tion framework (Ko et al, 2007), which estimates
the probability of an answer based on a number of
answer validation and similarity features. Valida-
tion features use resources such as gazetteers and
Wikipedia to verify an answer, whereas similarity
features measure the syntactic and semantic simi-
larity to other candidates, e.g. using string distance
measures and WordNet relations.
2.2 Set Expander for Any Language (SEAL)
Set expansion (SE) refers to expanding a given par-
tial set of objects into a more complete set. SEAL3
(Wang and Cohen, 2007) is a SE system which ac-
cepts input elements (seeds) of some target set St
and automatically finds other probable elements of
St in semi-structured documents such as web pages.
SEAL also works on unstructured text, but its ex-
traction mechanism benefits from structuring ele-
ments such as HTML tags. The algorithm is in-
dependent of the human language from which the
2http://www.ephyra.info/
3http://rcwang.com/seal
Figure 1: Examples of SEAL?s input and output. English
entities are reality TV shows, Chinese entities are popular
Taiwanese food, and Japanese entities are famous cartoon
characters.
Figure 2: An example graph constructed by SEAL. Every
edge from node x to y actually has an inverse relation
edge from node y to x that is not shown here (e.g. m1 is
extracted by w1).
seeds are taken, and also independent of the markup
language used to annotate the documents. Examples
of SEAL?s input and output are shown in Figure 1.
In more detail, SEAL comprises three major com-
ponents: the Fetcher, the Extractor, and the Ranker.
The Fetcher focuses on retrieving web pages. The
URLs of the web pages come from top results re-
trieved from Google and Yahoo! using the concate-
nation of all seeds as the query. The Extractor au-
tomatically constructs page-specific extraction rules,
or wrappers, for each page that contains the seeds.
Every wrapper is defined by two character strings,
which specify the left-context and right-context nec-
essary for an entity to be extracted from a page.
These strings are chosen to be maximally-long con-
texts that bracket at least one occurrence of every
seed string on a page. Most of the wrappers con-
948
tain HTML tags, which illustrates the importance
of structuring information in the source documents.
All entity mentions bracketed by these contextual
strings derived from a particular page are extracted
from the same page. Finally, the Ranker builds a
graph, and then ranks the extracted mentions glob-
ally based on the weights computed by performing a
random graph walk.
An example graph is shown in Figure 2, where
each node di represents a document, wi a wrapper,
and mi an extracted entity mention. The graph mod-
els the relationship between documents, wrappers,
and mentions. In order to measure the relative im-
portance of each node within the graph, the Ranker
performs a graph walk until all node weights con-
verge. The idea is that nodes are weighted higher
if they are connected to many other highly weighted
nodes.
We apply this SE algorithm to answer candidates
for list questions generated by Ephyra and other
TREC QA systems to find additional instances of
correct answers that were not in the original candi-
date set.
3 Proposed Approach
SEAL was originally designed to handle only rele-
vant input seeds. When provided with a mixture of
relevant and irrelevant answers from a QA system,
the performance would suffer. In this section, we
propose three modifications to SEAL to improve its
ability to handle noisy input seeds.
3.1 Aggressive Fetcher
For each expansion, SEAL?s fetcher concatenates all
seeds and sends them as one query to the search
engines. However, when the seeds are noisy, the
documents fetched are constrained by the irrele-
vant seeds, which decreases the chance of finding
good documents. To overcome this problem, we
designed an aggressive fetcher (AF) that increases
the chance of composing queries containing only
relevant seeds. It sends a two-seed query for ev-
ery possible pair of seeds to the search engines. If
there are n input seeds, then the total number of
queries sent would be (n2
). For example, suppose
SEAL is given a set of noisy seeds: Boston, Seattle
and Carnegie-Mellon (assuming Carnegie-Mellon is
irrelevant), then by using AF, one query will contain
only relevant seeds (as shown in Table 1). The docu-
ments are then collected and sent to SEAL?s extrac-
tor for learning wrappers.
Queries Quality
-AF #1: Boston Seattle Carnegie-Mellon Low
+AF
#1: Boston Seattle High
#2: Boston Carnegie-Mellon Low
#3: Seattle Carnegie-Mellon Low
Table 1: Example queries and their quality given
the seeds Boston, Seattle and Carnegie-Mellon, where
Carnegie-Mellon is assumed to be irrelevant.
3.2 Lenient Extractor
SEAL?s extractor requires the longest common
contexts to bracket at least one instance of every
seed per web page. However, when seeds are noisy,
such common contexts usually do not exist or
are too short to be useful. To solve this problem,
we propose a lenient extractor (LE) which only
requires the contexts to bracket at least one in-
stance of a minimum of two seeds, instead of every
seed. This increases the chance of finding longest
common contexts that bracket only relevant seeds.
For instance, suppose SEAL is given the seeds
from the previous example (Boston, Seattle and
Carnegie-Mellon) and the passage below. Then the
extractor would learn the wrappers shown in Table 2.
?While attending a hearing in Boston City
Hall, Alan, a professor at Boston University,
met Tina, his former student at Seattle Univer-
sity, who is studying at Carnegie-Mellon University
Art School and will be working in Seattle City Hall.?
Learned Wrappers
-LE #1: at [...] University
+LE #1: at [...] University#2: in [...] City Hall
Table 2: Wrappers learned by SEAL?s extractor when
given the passage in Section 3.2 and the seeds Boston,
Seattle and Carnegie-Mellon.
949
As illustrated, with lenient extraction, SEAL is
now able to learn the second wrapper because it
brackets one instance of at least two seeds (Boston
and Seattle). This can be very helpful if the list
question is asking for city names rather than univer-
sity names. The extractor then uses these wrappers
to extract additional answer candidates, by search-
ing for other strings that fit into the placeholders of
the wrappers. Note that the example was simplified
for ease of presentation. The wrappers are actually
character-based (as opposed to word-based) and are
likely to contain HTML tags when generated from
real web pages.
3.3 Hinted Expander
Most QA systems use keywords from the question to
guide the retrieval of relevant documents and the ex-
traction of answer candidates. We believe these key-
words are also important for SEAL to identify ad-
ditional instances of correct answers. For example,
if the seeds are George Washington, John Adams,
and Thomas Jefferson, then without using any con-
text from the question, SEAL would output a mix-
ture of founding fathers and presidents of the U.S.A.
To solve this problem, we devised a hinted expan-
sion (HE) technique that utilizes the context given
in the question to constrain SEAL?s search space on
the Web. This is achieved by appending keywords
from the question to every query that is sent to the
search engines. The rationale is that the retrieved
documents will also match the keywords, which may
increase the chance of finding those documents that
contain our desired set of answers.
4 Experimental Design
We conducted experiments in two phases. In the
first phase, we evaluated the SE approach by apply-
ing SEAL to answers generated by Ephyra. In the
second phase, we evaluated the approach by apply-
ing SEAL to the output from QA systems that per-
formed the best on the list questions in the TREC 15
evaluation. In both phases, the answers found by
SEAL were retrieved from the Web instead of the
AQUAINT newswire corpus used in the TREC eval-
uations. However, we rejected answers if they could
only be found in the Web and not in the AQUAINT
corpus to avoid an unfair advantage over the QA
systems: TREC participants were allowed to extract
candidates from the Web (or any other source), but
they had to identify a supporting document in the
AQUAINT corpus for each answer and thus could
not return answers that were not covered by the cor-
pus.
Preliminary experiments showed that we can ob-
tain a good balance between the amount and quality
of the documents fetched by using only rare ques-
tion terms as hint words. In particular, we select the
three question words that occur least frequently in a
sample of the AQUAINT corpus as hints. The can-
didate answers were evaluated by using the answer
keys, composed of regular expression patterns, ob-
tained from the TREC website. We did not extend
the patterns with additional correct answers found in
our experiments. These answer keys were not offi-
cially used in the TREC evaluation; thus the baseline
scores we computed for Ephyra and other QA sys-
tems in our experiments are slightly different from
those officially reported.
4.1 Ephyra
We evaluated our SE approach on Ephyra using the
list questions from TREC 13, 14, and 15 (55, 93, and
89 questions, respectively). For each question, the
top four answer candidates from Ephyra were given
as input seeds to SEAL. Initial experiments showed
that by adding additional seeds, the effectiveness of
our approach can be improved at the expense of a
longer runtime.
We report both mean average precision (MAP)
and F1 scores. For the F1 scores, we drop answer
candidates with low confidence scores by applying
a relative cut-off threshold: an answer candidate is
dropped if the ratio of its confidence score and the
score of the top answer is below a threshold. An
optimal threshold for a question is a threshold that
maximizes the F1 score for that particular question.
For each TREC dataset, we conducted three ex-
periments: (1) evaluation of answer candidates us-
ing MAP; (2) evaluation using average F1 with an
optimal threshold for each question; and (3) eval-
uation using average F1 with thresholds trained by
5-fold cross validation. For each of those 5-fold val-
idations, only one threshold was determined for all
questions in the training folds.
950
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 25.95% 21.39% 23.76% 31.43% 34.22% 35.26%
TREC 14 14.45% 8.71% 14.47% 17.04% 16.58% 18.82%
TREC 15 13.42% 9.02% 13.17% 16.87% 17.12% 18.95%
Table 3: Mean average precision of Ephyra, its top four answers, and various SEAL configurations, where LE is
Lenient Extractor, AF is Aggressive Fetcher, and HE is Hinted Expander.
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 35.74% 26.29% 30.53% 36.47% 40.08% 40.80%
TREC 14 22.83% 14.05% 20.62% 22.81% 22.66% 24.88%
TREC 15 22.42% 14.57% 19.88% 23.30% 24.04% 25.65%
Table 4: Average F1 of Ephyra, its top four answers, and various SEAL configurations when using an optimal threshold
for each question.
4.2 Top QA Systems
We evaluated two SE approaches, SEAL and Google
Sets, on the five QA systems that performed the best
on the list questions in TREC 15. For each question,
the top four answer candidates4 from those systems
were given as input seeds to SEAL and Google Sets.
Unlike the candidates found by Ephyra, these can-
didates were provided without confidence scores;
hence, we assumed they all have a score of 1.0. In
our experiments with SEAL, we first determined a
single threshold that optimizes the average of the F1
scores of the top five systems in both TREC 13 and
14. We then obtained evaluation results for the top
systems in TREC 15 by using this trained threshold.
When performing hinted expansion, the keywords
(or hint words) for each question were extracted by
Ephyra?s question analysis component. In our exper-
iments with Google Sets, we requested Small Sets of
items and again measured the performance in terms
of F1 scores. We also tried requesting Large Sets but
the results were worse.
5 Results and Discussion
In Tables 3 and 4, we present evaluation results for
all answers from Ephyra, only the top four answers,
and various configurations of SEAL using the top
four answers as seeds. Table 3 shows the MAP for
4Obtained from http://trec.nist.gov/results
each dataset (TREC 13, 14, and 15), and Table 4
shows for each dataset the average F1 score when
using optimal per-question thresholds. The results
indicate that SEAL achieves the best performance
when configured with all three proposed extensions.
In terms of MAP, the best-configured SEAL im-
proves the quality of the input answers (relatively)
by 65%, 116%, 110% for each dataset respectively,
and improves Ephyra?s overall performance by 36%,
30%, 41%. In terms of optimal F1, SEAL improves
the quality of the input answers by 55%, 77%, 76%
and Ephyra?s overall performance by 14%, 9%, 14%
respectively. These results illustrate that a SE sys-
tem is capable of improving a QA system?s perfor-
mance on list questions, if we know how to select
good thresholds.
In practice, the thresholds are unknown and must
be estimated from a training set. Table 5 shows eval-
uation results using 5-fold cross validation for each
dataset (TREC 13, 14, and 15) independently, and
the combination of all three datasets (All). For each
validation, we determine the threshold that maxi-
mizes the F1 score on the training folds, and we
also determine the F1 score on the test fold by ap-
plying the trained threshold. We repeat this valida-
tion for each of the five test folds and present the av-
erage threshold and F1 score for each configuration
and dataset. The F1 scores give an estimate of the
performance on unseen data and allow a fair com-
951
Ephyra SEAL+LE+AF+HE Hybrid
Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold
TREC 13 25.55% 0.3808 30.71% 0.3257 29.04% 0.0796
TREC 14 15.78% 0.2636 15.60% 0.1889 17.13% 0.0108
TREC 15 15.19% 0.1192 15.64% 0.2581 16.47% 0.0123
All 18.03% 0.2883 19.15% 0.2606 19.59% 0.0164
Table 5: Average F1 of Ephyra, the best-configured SEAL, and the hybrid system, along with thresholds trained by
5-fold cross validation.
TREC 15 Baseline Top 4 Ans. Google Sets SEAL+LE+AF+HE Hybrid
QA Systems Avg. F1 Avg. F1 Avg. F1 ?F1 Avg. F1 ?F1 Avg. F1 ?F1
lccPA06 44.96% 32.67% 37.89% -15.72% 40.00% -11.04% 45.30% 0.76%
cuhkqaepisto 18.27% 17.02% 15.96% -12.68% 19.75% 8.08% 19.13% 4.70%
NUSCHUAQA1 18.40% 14.99% 16.70% -9.21% 18.74% 1.86% 18.06% -1.81%
FDUQAT15A 19.71% 14.32% 18.79% -4.63% 19.78% 0.38% 20.61% 4.57%
QACTIS06C 17.52% 15.22% 17.05% -2.72% 18.45% 5.26% 18.38% 4.85%
Average 23.77% 18.84% 21.28% -10.49% 23.34% -1.81% 24.30% 2.20%
Table 6: Average F1 of the QA systems, their top four answers, Google Sets, the best-configured SEAL, the hybrid
system, and their relative improvements over the QA systems.
parison across systems. Here, we also introduce a
hybrid system (Hybrid) that intersects the answers
found by both systems by multiplying their proba-
bilistic scores.
Tables 3, 4, and 5 show that the effectiveness of
the SE approach depends on the quality of the initial
answer candidates. The improvements are most ap-
parent for the TREC 13 dataset, where Ephyra has
a much higher performance compared to TREC 14
and 15. However, the best-configured SEAL did not
improve the F1 score on TREC 14, as reported in
Table 5. We suspect that this is due to the compar-
atively low quality of Ephyra?s top four answers for
this dataset. The experiments also illustrate that by
intersecting the answer candidates found by Ephyra
and SEAL, we can eliminate poor answer candi-
dates and partially compensate for the low preci-
sion of Ephyra on the harder TREC datasets. How-
ever, this comes at the expense of a lower recall,
which slightly hurts the performance on the compar-
atively easier TREC 13 questions. We also evaluated
Google Sets on top four answers from Ephyra for
TREC 13-15 and obtained F1 scores of 12%, 11%,
and 9% respectively (compared to 29%, 17%, and
16% for our hybrid approach with trained thresh-
olds).
Table 6 shows F1 scores for the SE approach
applied to the output from the five QA systems
with the highest performance on the list questions
in TREC 15. Again, Hybrid intersects the answers
found by the QA system and SEAL by multiplying
their confidence scores. Two thresholds were trained
separately on the top five systems in both TREC 13
and 14; one for SEAL (0.2376) and another for Hy-
brid (0.2463). As shown, the performance of Google
Sets is worse than SEAL and Hybrid, but better than
the top four answers on average. We believe our SE
system outperforms Google Sets because we have
methods to handle noisy inputs (i.e. AF, LE) and a
method for guiding the SE algorithm to search in the
right space on the Web (i.e. HE).
The results show that both SEAL and Hybrid are
capable of improving four out of the five systems.
We observed that one reason why SEAL did not im-
prove ?lccPA06? was the incompleteness of the an-
swer keys. Table 7 shows one of many examples
where SEAL was penalized for finding additional
correct answers. As illustrated, Hybrid improved
all systems except ?NUSCHUAQA1?. The reason
is that even though SEAL improved the baseline,
their overlapping answer set is too small; thus hurt-
ing the recall of Hybrid substantially. Unfortunately,
952
Question 154.6: Name titles of movies, other than ?Superman? movies, that
Christopher Reeve acted in.
lccPA06 (F1: 75%) SEAL+LE+AF+HE (F1: 40%)
+Rear Window +Rear Window
+The Remains of the Day +The Remains of the Day
+Snakes and Ladders -The Bostonians
-Superman -Somewhere in Time
-Village of the Damned
-In the Gloaming
Table 7: Example of SEAL being penalized for finding correct answers (all are correct except the last one). Answers
found in the answer keys are marked with ?+?. All four answers from ?lccPA06? were used as seeds.
Question 170.6: What are the titles of songs written by John Prine?
NUSCHUAQA1 (F1: 25%) SEAL+LE+AF+HE (F1: 44%)
+I Just Want to Dance With You +I Just Want to Dance With You
-Titled In Spite of Ourselves +Christmas in Prison
+Christmas in Prison +Sam Stone
-Grammy - Winning -Grandpa was a Carpenter
-Sabu Visits the Twin Cities Alone
+Angel from Montgomery
Table 8: Example demonstrating SEAL?s ability to handle noisy input seeds. All four answers from ?NUSCHUAQA1?
were used as seeds. Again, SEAL is penalized for finding correct answers (all answers are correct).
for the top TREC 15 systems we only had access to
the answers that were actually submitted by the par-
ticipants, whereas for Ephyra we could utilize the
entire list of generated answer candidates, includ-
ing those that fell below the cutoff threshold for list
questions. Nevertheless, the hybrid approach could
improve the baseline by more than 2% on average
in terms of F1 score. Table 8 shows that the best-
configured SEAL is capable of expanding only the
relevant seeds even when given a set of noisy seeds.
Neither Google Sets nor the original SE algorithm
without the proposed extensions could expand these
seeds with additional candidates.
On average, SEAL required about 5 seconds for
querying the search engines, 10 seconds for crawl-
ing the Web, 20 seconds for extracting answer can-
didates from the web pages, and 5 seconds for rank-
ing the candidates. Note that the SE system has not
been optimized extensively. The runtime of the web
page retrieval step and much of the search is due to
network latency and can be reduced if the search is
performed locally.
6 Conclusion and Future Work
We have shown that our SE approach is capable of
improving the performance of QA systems on list
questions by utilizing only their top four answer can-
didates as seeds. We have also illustrated a feasible
and effective method for integrating a SE approach
into any QA system. We would like to emphasize
that for each of the experiments we conducted, all
that the SE system received as input were the top
four noisy answers from a QA system and three key-
words from the TREC questions. We have shown
that higher quality candidates support more effec-
tive set expansion. In the future, we will investigate
how to utilize more answer candidates from the QA
system and determine the minimal quality of those
candidates required for SE approach to make an im-
provement.
We have also shown that, in terms of F1 scores
with trained thresholds, the hybrid method improves
the Ephyra QA system on all datasets and also im-
proves four out of the five systems that performed
953
the best on the list questions in TREC 15. How-
ever, the final list of answers only comprises candi-
dates found by both the QA system and the SE al-
gorithm. In future experiments, we will investigate
other methods of merging answer candidates, such
as taking the union of answers from both systems.
We expect further improvements from adding can-
didates that are found only by the QA system, but
it is unclear how the confidence measures from the
two systems can be combined effectively.
We would also like to emphasize that the SE ap-
proach is entirely language independent, and thus
can be readily applied to answer candidates in other
languages. In future experiments, we will investi-
gate its performance on question answering tasks in
languages such as Chinese and Japanese.
As pointed out previously, the performance of the
SE approach highly depends on the accuracy of the
seeds. However, QA systems are usually not op-
timized to provide few high-precision results, but
treat precision and recall as equally important. This
leaves room for further improvements, e.g. by ap-
plying stricter answer validation techniques to the
seeds used for SE.
We also plan to analyze the effectiveness of our
approach across different question types and evalu-
ate it on more complex questions such as the rigid
list questions in the new TAC QA evaluation, which
ask for opinion holders and subjects.
Acknowledgements
This work was supported in part by the Google Re-
search Awards program, IBM Open Collaboration
Agreement #W0652159, and the Defense Advanced
Research Projects Agency (DARPA) under Contract
No. NBCHD030010.
References
H.T. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. Proceedings of
the Fifteenth Text REtrieval Conference.
H.T. Dang, D. Kelly, and J. Lin. 2007. Overview of the
TREC 2007 question answering track. Proceedings of
the Sixteenth Text REtrieval Conference.
J. Ko, L. Si, and E. Nyberg. 2007. A probabilistic frame-
work for answer selection in question answering. Pro-
ceedings of NAACL-HLT.
N. Schlaefer, P. Gieselmann, and G. Sautter. 2006. The
Ephyra QA system at TREC 2006. Proceedings of the
Fifteenth Text REtrieval Conference.
N. Schlaefer, G. Sautter, J. Ko, J. Betteridge, M. Pathak,
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system in TREC 2007. To appear in: Pro-
ceedings of the Sixteenth Text REtrieval Conference.
R.C. Wang and W.W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. Proceedings of IEEE International Conference
on Data Mining.
954
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1503?1512,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Character-level Analysis of Semi-Structured Documents for Set Expansion
Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
rcwang@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
Set expansion refers to expanding a par-
tial set of ?seed? objects into a more com-
plete set. One system that does set ex-
pansion is SEAL (Set Expander for Any
Language), which expands entities auto-
matically by utilizing resources from the
Web in a language-independent fashion.
In this paper, we illustrated in detail the
construction of character-level wrappers
for set expansion implemented in SEAL.
We also evaluated several kinds of wrap-
pers for set expansion and showed that
character-based wrappers perform better
than HTML-based wrappers. In addition,
we demonstrated a technique that extends
SEAL to learn binary relational concepts
(e.g., ?x is the mayor of the city y?) from
only two seeds. We also show that the
extended SEAL has good performance on
our evaluation datasets, which includes
English and Chinese, thus demonstrating
language-independence.
1 Introduction
SEAL
1
(Set Expander for Any Language) is a
set expansions system that accepts input ele-
ments (seeds) of some target set S and automat-
ically finds other probable elements of S in semi-
structured documents such as web pages. SEAL
is a research system that has shown good perfor-
mance in previously published results (Wang and
Cohen, 2007). By using only three seeds and
top one hundred documents returned by Google,
SEAL achieved 90% in mean average precision
(MAP), averaged over 36 datasets from three lan-
guages: English, Chinese, and Japanese. Un-
like other published research work (Etzioni et al,
2005), SEAL focuses on finding small closed sets
1
http://rcwang.com/seal
of items (e.g., Disney movies) rather than large
and more open sets (e.g., scientists).
In this paper, we explore the impact on perfor-
mance of one of the innovations in SEAL, specif-
ically, the use of character-level techniques to de-
tect candidate regular structures, or wrappers, in
web pages. Although some early systems for
web-page analysis induce rules at character-level
(e.g., such as WIEN (Kushmerick et al, 1997) and
DIPRE (Brin, 1998)), most recent approaches for
set expansion have used either tokenized and/or
parsed free-text (Carlson et al, 2009; Talukdar et
al., 2006; Snow et al, 2006; Pantel and Pennac-
chiotti, 2006), or have incorporated heuristics for
exploiting HTML structures that are likely to en-
code lists and tables (Nadeau et al, 2006; Etzioni
et al, 2005).
In this paper, we experimentally evaluate
SEAL?s performance under two settings: 1) us-
ing the character-level page analysis techniques
of the original SEAL, and 2) using page analy-
sis techniques constrained to identify only HTML-
related wrappers. Our conjecture is that the less
constrained character-level methods will produce
more candidate wrappers than HTML-based tech-
niques. We also conjecture that a larger number of
candidate wrappers will lead to better performance
overall, due to SEAL?s robust methods for ranking
candidate wrappers.
The experiments in this paper largely vali-
date this conjecture. We show that the HTML-
restricted version of SEAL performs less well,
losing 13 points in MAP on a dozen Chinese-
language benchmark problems, 8 points in MAP
on a dozen English-language problems, and 2
points in MAP on a dozen Japanese-language
problems.
SEAL currently only handles unary relation-
ships (e.g., ?x? is a mayor). In this paper, we
show that SEAL?s character-level analysis tech-
niques can, like HTML-based methods, be read-
1503
ily extended to handle binary relationships. We
then demonstrate that this extension of SEAL can
learn binary concepts (e.g., ?x is the mayor of
the city y?) from a small number of seeds, and
show that, as with unary relationships, MAP per-
formance is 26 points lower when wrappers are
restricted to be HTML-related. Furthermore, we
also illustrate that the learning of binary concepts
can be bootstrapped to improve its performance.
Section 2.1 explains how SEAL constructs
wrappers and rank candidate items for unary re-
lations. Section 3 describes the experiments and
results for unary relations. Section 4 presents the
method for extending SEAL to handle binary re-
lationships, as well as their experimental results.
Related work is discussed in Section 5, and the
paper concludes in Section 6.
2 SEAL
2.1 Identifying Wrappers for Unary
Relations
When SEAL performs set expansion, it accepts a
small number of seeds from the user (e.g., ?Ford?,
?Nissan?, and ?Toyota?). It then uses a web
search engine to retrieve some documents that
contain these instances, and then analyzes these
documents to find candidate wrappers (i.e., regu-
lar structures on a page that contain the seed in-
stances). Strings that are extracted by a candidate
wrapper (but are not equivalent to any seed) are
called candidate instances. SEAL then statisti-
cally ranks the candidate instances (and wrappers),
using the techniques outlined below, and outputs a
ranked list of instances to the user.
One key step in this process is identifying can-
didate wrappers. In SEAL, a candidate wrapper is
defined by a pair of left and right character strings,
` and r. A wrapper ?extracts? items from a partic-
ular document by locating all strings in the docu-
ment that are bracketed by the wrapper?s left and
right strings, but do not contain either of the two
strings. In SEAL, wrappers are always learned
from, and applied to, a single document.
Table 1 illustrates some candidate wrappers
learned by SEAL. (Here, a wrapper is written as
`[...]r, with the [...] to be filled by an extracted
string.) Notice that the instances extracted by
wrappers can and do appear in surprising places,
such as embedded in URLs or in HTML tag at-
tributes. Our experience with these character-
based wrappers lead us to conjecture that exist-
ing heuristics for identifying structure in HTML
are fundamentally limited, in that many potentially
useful structures will not be identified by analyz-
ing HTML structure only.
SEAL uses these rules to find wrappers. Each
candidate wrapper `, r is a maximally long pair of
strings that bracket at least one occurrence of ev-
ery seed in a document: in other words, for each
pair `, r, the set of strings C extracted by `, r has
the properties that:
1. For every seed s, there exists some c ? C that
is equivalent to s; and
2. There are no strings `
?
, r
?
that satisfy property
(1) above such that ` is a proper suffix of `
?
and r is a proper prefix of r
?
.
SEAL?s wrappers can be found quite efficiently.
The algorithm we use has been described previ-
ously (Wang and Cohen, 2007), but will be ex-
plained again here for completeness. As an ex-
ample, below shows a mock document, written in
an unknown mark-up language, that has the seeds:
Ford, Nissan, and Toyota located (and boldfaced).
There are two other car makers hidden inside this
document (can you spot them?). In this section,
we will show you how to automatically construct
wrappers that reveal them.
GtpKxHnIsSaNxjHJglekuDialcLBxKHforDxkrpW
NaCMwAAHOFoRduohdEXocUvaGKxHaCuRAxjHjnOx
oTOyOTazxKHAUdIxkrOyQKxHToYotAxjHCRdmLxa
puRAPprtqOVKxHfoRdxjHaJAScRFrlaFoRDofwNL
WxKHtOYotaxkrHxQKlacXlGEKtxKHNisSanxkrEq
Given a set of seeds and a semi-structured doc-
ument, the wrapper construction algorithm starts
by locating all strings equivalent to a seed in the
document; these strings are called seed instances
below. (In SEAL, we always use case-insensitive
string matching, so a string is ?equivalent to? any
case variant of itself.) The algorithm then inserts
all the instances into a list and assigns a unique id
to each of them by its index in the list (i.e., the id
of an instance is its position in the list.)
For every seed instance in the document, its
immediate left character string (starting from the
first character of the document) and right charac-
ter string (ending at the last character of the docu-
ment) are extracted and inserted into a left-context
trie and a right-context trie respectively, where the
left context is inserted in reversed character or-
der. (Here, we implemented a compact trie called
1504
URL: http://www.shopcarparts.com/
Wrapper: .html" CLASS="shopcp">[...] Parts</A> <br>
Content: acura, audi, bmw, buick, cadillac, chevrolet, chevy, chrysler, daewoo, daihatsu, dodge, eagle, ford, ...
URL: http://www.allautoreviews.com/
Wrapper: </a><br> <a href="auto reviews/[...]/
Content: acura, audi, bmw, buick, cadillac, chevrolet, chrysler, dodge, ford, gmc, honda, hyundai, infiniti, isuzu, ...
URL: http://www.hertrichs.com/
Wrapper: <li class="franchise [...]"> <h4><a href="#">
Content: buick, chevrolet, chrysler, dodge, ford, gmc, isuzu, jeep, lincoln, mazda, mercury, nissan, pontiac, scion, ...
URL: http://www.metacafe.com/watch/1872759/2009 nissan maxima performance/
Wrapper: videos">[...]</a> <a href="/tags/
Content: avalon, cars, carscom, driving, ford, maxima, nissan, performance, speed, toyota
URL: http://www.worldstyling.com/
Wrapper: ?>[...] Accessories</option><option value=?
Content: chevy, ford, isuzu, mitsubishi, nissan, pickup, stainless steel, suv, toyota
Table 1: Examples of wrappers constructed from web pages given the seeds: Ford, Nissan, Toyota.
Patricia trie where every node stores a substring.)
Every node in the left-context trie maintains a list
of ids for keeping track of the seed instances that
follow the string associated with that node. Same
thing applies to the right-context trie symmetri-
cally. Figure 1 shows the two context tries and
the list of seed instances when provided the mock
document with the seeds: Ford, Nissan, and Toy-
ota.
Provided that the left and right context tries are
populated with all the contextual strings of ev-
ery seed instance, the algorithm then finds maxi-
mally long contextual strings that bracket at least
one seed instance of every seed. The pseudo-code
for finding these strings for building wrappers is
illustrated in Table 2, where Seeds is the set of
input seeds and ` is the minimum length of the
strings. We observed that longer strings produce
higher precision but lower recall. This is an in-
teresting parameter that is worth exploring, but
for this paper, we consider and use only a min-
imum length of one throughout the experiments.
The basic idea behind the pseudo-code is to first
find all the longest possible strings from one trie
given some constraints, then for every such string
s, find the longest possible string s
?
from another
trie such that s and s
?
bracket at least one occur-
rence of every given seed in a document.
The wrappers constructed as well as the items
extracted given the mock document and the exam-
ple seeds are shown below. Notice that Audi and
Acura are uncovered (did you spot them?).
Wrapper: xKH[...]xkr
Content: audi, ford, nissan, toyota
Wrapper: KxH[...]xjH
Content: acura, ford, nissan, toyota
Wrappers MakeWrappers(Trie `, Trie r)
Return Wraps(l, r) ?Wraps(r, l)
Wrappers Wraps(Trie t
1
, Trie t
2
)
For each n
1
? TopNodes(t
1
, `)
For each n
2
? BottomNodes(t
2
, n
1
)
For each n
1
? BottomNodes(t
1
, n
2
)
Construct a new Wrapper(Text(n
1
), Text(n
2
))
Return a union of all wrappers constructed
Nodes BottomNodes(Trie t
1
, Node n
?
)
Find node n ? t
1
such that:
(1) NumCommonSeeds(n, n
?
) == |Seeds|, and
(2) All children nodes of n (if exist) fail on (1)
Return a union of all nodes found
Nodes TopNodes(Trie t, int `)
Find node n ? t such that:
(1) Text(n).length ? `, and
(2) Parent node of n (if exist) fails on (1)
Return a union of all nodes found
String Text(Node n)
Return the textual string represented by the
path from root to n in the trie containing n
Integer NumCommonSeeds(Node n
1
, Node n
2
)
For each index i ? Intersect(n
1
, n
2
):
Find the seed at index i of seed instance list
Return the size of the union of all seeds found
Integers Intersect(Node n
1
, Node n
2
)
Return n
1
.indexes ? n
2
.indexes
Table 2: Pseudo-code for constructing wrappers.
Table 1 shows examples of wrappers con-
structed from real web documents. We have also
observed items extracted from plain text (.txt),
comma/tab-separated text (.csv/.tsv), latex (.tex),
and even Word documents (.doc) of which the
wrappers have binary character strings. These ob-
servations support our claim that the algorithm is
independent of mark-up language. In our experi-
mental results, we will show that it is independent
of human language as well.
1505
Figure 1: The context tries and the seed instance list constructed given the mock document presented in
Section 2.1 and the seeds: Ford, Nissan and Toyota.
2.2 Ranking Wrappers and Candidate
Instances
In previous work (Wang and Cohen, 2007), we
presented a graph-walk based technique that is
effective for ranking sets and wrappers. This
model encapsulates the relations between docu-
ments, wrappers, and extracted instances (entity
mentions). Similarly, our graph also consists of
a set of nodes and a set of labeled directed edges.
Figure 2 shows an example graph where each node
d
i
represents a document, w
i
a wrapper, and m
i
an extracted entity mention. A directed edge con-
nects a node d
i
to a w
i
if d
i
contains w
i
, a w
i
to a
m
i
ifw
i
extractsm
i
, and a d
i
to am
i
if d
i
contains
m
i
. Although not shown in the figure, every edge
from node x to y actually has an inverse relation
edge from node y to x (e.g., m
i
is extracted by w
i
)
to ensure that the graph is cyclic.
We will use letters such as x, y, and z to denote
nodes, and x
r
?? y to denote an edge from x to
y with labeled relation r. Each node represents an
object (document, wrapper, or mention), and each
edge x
r
?? y asserts that a binary relation r(x, y)
holds. We want to find entity mention nodes that
are similar to the seed nodes. We define the sim-
ilarity between two nodes by random walk with
restart (Tong et al, 2006). In this algorithm, to
walk away from a source node x, one first chooses
an edge relation r; then given r, one picks a target
node y such that x
r
?? y. When given a source
node x, we assume that the probability of picking
an edge relation r is uniformly distributed among
the set of all r, where there exist a target node y
such that x
r
?? y. More specifically,
Figure 2: Example graph built by Random Walk.
P (r|x) =
1
|r : ?y x
r
?? y|
(1)
We also assume that once an edge relation r is
chosen, a target node y is picked uniformly from
the set of all y such that x
r
?? y. More specifi-
cally,
P (y|r, x) =
1
|y : x
r
?? y|
(2)
In order to perform random walk, we will build
a transition matrix M where each entry at (x, y)
represents the probability of traveling one step
from a source node x to a target node y, or more
specifically,
M
xy
=
?
r
P (r|x)P (y|r, x) (3)
We will also define a state vector ~v
t
which rep-
resents the probability at each node after iterating
through the entire graph t times, where one itera-
tion means to walk one step away from every node.
The state vector at t+ 1 iteration is defined as:
~v
t+1
= ?~v
0
+ (1? ?)M~v
t
(4)
1506
Since we want to start our walk from the seeds,
we initialize v
0
to have probabilities uniformly
distributed over the seed nodes. In each step of
our walk, there is a small probability ? of tele-
porting back to the seed nodes, which prevents us
from walking too far away from the seeds. We
iterate our graph until the state vector converges,
and rank the extracted mentions by their probabil-
ities in the final state vector. We use a constant ?
of 0.01 in the experiments below.
2.3 Bootstrapping Candidate Instances
Bootstrapping refers to iterative unsupervised set
expansion. This process requires minimal super-
vision, but is very sensitive to the system?s perfor-
mance because errors can easily propagate from
one iteration to another. As shown in previous
work (Wang and Cohen, 2008), carefully designed
seeding strategies can minimize the propagated er-
rors. Below, we show the pseudo-code for our
bootstrapping strategy.
stats? ?, used? inputs
for i = 1 to M do
m = min(3, |used|)
seeds? select
m
(used) ? top(list)
stats? expand(seeds, stats)
list? rank(stats)
used? used ? seeds
end for
where M is the total number of iterations, inputs
are the two initial input seeds, select
m
(S) ran-
domly selects m different seeds from the set S,
used is a set that contains previously expanded
seeds, top(list) returns an item that has the high-
est rank in list, expand(seeds, stats) expands
the selected seeds using stats and outputs accu-
mulated statistics, and rank(stats) applies Ran-
dom Walk described in Section 2.2 on the accu-
mulated stats to produce a list of items. This
strategy dumps the highest-ranked item into the
used bucket after every iteration. It starts by ex-
panding two input seeds. For the second iteration,
it expands three seeds: two used plus one from
last iteration. For every successive iteration, it ex-
pands four seeds: three randomly selected used
ones plus one from last iteration.
3 Experiments with Unary Relations
We would like to determine whether character-
based or HTML-based wrappers are more suited
for the task of set expansion. In order to do that,
# L. Context [...]R. Context Eng Jap Chi Avg
1 .+[...].+ 87.6 96.9 95.4 93.3
2 .
*
[<>].
*
[...].
*
[<>].
*
85.7 96.8 90.7 91.1
3 .
*
>[...]<.
*
85.7 96.7 90.7 91.0
4 .
*
<.+?>.
*
[...].
*
<.+?>.
*
80.1 95.8 83.7 86.5
5 .
*
<.+?>[...]<.+?>.
*
79.6 94.9 82.4 85.6
Table 3: The performance (MAP) of various types
of wrappers on semi-structured web pages.
we introduce five types of wrappers, as illustrated
in Table 3. The first type is the character-based
wrapper that does not have any restriction on the
alphabets of its characters. Starting from the sec-
ond type, the allowable alphabets in a wrapper be-
come more restrictive. The fifth type requires that
an item must be tightly bracketed by two complete
HTML tags in order to be extracted.
All pure HTML-based wrappers are type 5, pos-
sibly with additional restrictions imposed (Nadeau
et al, 2006; Etzioni et al, 2005). SEAL cur-
rently does not use an HTML parser (or any other
kinds of parser), so restrictions cannot be easily
imposed. As far as we know, there isn?t an agree-
ment on what restrictions make the most sense
or work the best. Therefore, we evaluate perfor-
mance for varying wrapper constraints from type
1 (most general) to type 5 (most strict) in our ex-
periments.
For set expansion, we use the same evaluation
set as in (Wang and Cohen, 2007) which contains
36 manually constructed lists across three differ-
ent languages: English, Chinese, and Japanese (12
lists per language). Each list contains all instances
of a particular semantic class in a certain language,
and each instance contains a set of synonyms (e.g.,
USA, America).
Since the output of our system is a ranked list
of extracted instances, we choose mean average
precision (MAP) as our evaluation metric. MAP
is commonly used in the field of Information Re-
trieval for evaluating ranked lists because it is sen-
sitive to the entire ranking and it contains both re-
call and precision-oriented aspects. The MAP for
multiple ranked lists is simply the mean value of
average precisions calculated separately for each
ranked list. We define the average precision of a
single ranked list as:
AvgPrec(L) =
|L|
?
r=1
Prec(r)? isFresh(r)
Total # of Correct Instances
1507
where L is a ranked list of extracted instances, r
is the rank ranging from 1 to |L|, Prec(r) is the
precision at rank r, or the percentage of correct
synonyms above rank r (inclusively). isFresh(r)
is a binary function for ensuring that, if a list con-
tains multiple synonyms of the same instance (or
instance pair), we do not evaluate that instance (or
instance pair) more than once. More specifically,
the function returns 1 if a) the synonym at r is cor-
rect, and b) it is the highest-ranked synonym of its
instance in the list; it returns 0 otherwise.
We evaluate the performance of each type of
wrapper by conducting set expansion on the 36
datasets across three languages. For each dataset,
we randomly select two seeds, expand them by
bootstrapping ten iterations (where each iteration
retrieves at most 200 web pages only), and evalu-
ate the final result. We repeat this process three
times for every dataset and report the average
MAP for English, Japanese, and Chinese in Ta-
ble 3. As illustrated, the more restrictive a wrapper
is, the worse it performs. As a result, this indicates
that further restrictions on wrappers of type 5 will
not improve performance.
4 Set Expansion for Binary Relations
4.1 Identifying Wrappers for Binary
Relations
We extend the wrapper construction algorithm de-
scribed in Section 2.1 to support relational set ex-
pansion. The major difference is that we introduce
a third type of context called the middle context
that occurs between the left and right contexts of
a wrapper for separating any two items. We ex-
ecute the same algorithm as before, except that a
seed instance in the algorithm is now a seed in-
stance pair bracketing some middle context (i.e.,
?s
1
?middle? s
2
?).
Given some seed pairs (e.g., Ford and USA),
the algorithm first locates the seeds in some given
documents. For every pair of seeds located, it ex-
tracts their left, middle, and right contexts. The
left and right contexts are inserted into their corre-
sponding tries, while the middle context is inserted
into a list. Every middle context is assigned a flag
indicating whether the two instances bracketing it
were found in the same or reversed order as the
input seed pairs. Every entry in the seed instance
list described previously now stores a pair of in-
stances as one single string (e.g. ?Ford/USA?). An
id stored in a node now matches the index of a pair
of instances as well as a middle context.
Shown below is a mock example document of
which the seed pairs: Ford and USA, Nissan and
Japan, Toyota and Japan are located (and bold-
faced).
GtpKxHnIsSaNoKpjaPaNxjHJgleTuoLpBlcLBxKH
forDEFcuSAxkrpWNapnIkAAHOFoRdawHDaUSauoh
deQsKxHaCuRAoKpJapANxjHdIjWnOxoTOyOTaVaq
jApaNzxKHAUdIEFcgErmANyxkrOyQKxHToYotAoK
pJApaNxjHCRdmtqOVKxHfoRdoKpusAxjHaJASzEi
nSfrlaFoRDLMmpuSaofwNLWxKHtOYotaEFcjAPan
xkrHxQKzrHpoKdGEKtxKHNisSanEFcJApAnxkrEq
After performing the abovementioned proce-
dures on this mock document, we now have con-
text tries that are much more complicated than
those illustrated in Figure 1, as well as a list of
middle contexts similar to the one shown below:
id Seed Pairs r Middle Context
0 Nissan/Japan No oKp
1 Nissan/Japan No EFc
2 Nissan/Japan Yes xkrHxQKzrHpoKd...
4 Toyota/Japan No oKp
6 Toyota/Japan Yes xjHdIjWnOxo
9 Ford/USA No EFc
13 Ford/USA Yes xkrpWNapnIkAAHO
where r indicates if the two instances bracketing
the middle context were found in the reversed or-
der as the input seed pairs. In order to find the
maximally long contextual strings, the ?Intersect?
function in the set expansion pseudo-code pre-
sented in Table 2 needs to be replaced with the
following:
Integers Intersect(Node n
1
, Node n
2
)
Define S = n
1
.indexes ? n
2
.indexes
Return the largest subset s of S such that:
Every index ? s corresponds to same middle context
which returns those seed pairs that are bracketed
by the strings associated with the two input nodes
with the same middle context. A wrapper for re-
lational set expansion, or relational wrapper, is
defined by the left, middle, and right contextual
strings. The relational wrappers constructed from
the mock document given the example seed pairs
are shown below. Notice that Audi/Germany and
Acura/Japan are discovered.
Wrapper: xKH[.1.]EFc[.2.]xkr
Content: audi/germany, ford/usa, nissan/japan,
toyota/japan
Wrapper: KxH[.1.]oKp[.2.]xjH
Content: acura/japan, ford/usa, nissan/japan,
toyota/japan
1508
Dataset ID Item #1 vs. Item #2 Lang. #1 Lang. #2 Size Complete?
US Governor US State/Territory vs. Governor English English 56 Yes
Taiwan Mayor Taiwanese City vs. Mayor Chinese Chinese 26 Yes
NBA Team NBA Team vs. NBA Team Chinese English 30 Yes
Fed. Agency US Federal Agency Acronym vs. Full Name English English 387 No
Car Maker Car Manufacturer vs. Headquartered Country English English 122 No
Table 4: The five relational datasets for evaluating relational set expansion.
Mean Avg. Precision Precision@100
Datasets 1 2 3 4 5 1 2 3 4 5
US Governor 97.4 89.3 89.2 89.3 89.2 55 50 51 50 50
Taiwan Mayor 99.8 95.6 94.3 91.3 90.8 25 25 24 23 23
NBA Team 100.0 99.9 99.9 99.9 99.2 30 30 30 30 30
Fed. Agency 43.7 14.5 5.2 11.1 5.2 96 55 20 40 20
Car Maker 61.7 0.0 0.0 0.0 0.0 74 0 0 0 0
Average 80.5 59.9 57.7 58.3 56.9 56 32 25 29 25
Table 5: Performance of various types of wrappers on the five relational datasets after first iteration.
Mean Avg. Precision Precision@100
Datasets 1 2 3 4 5 1 2 3 4 5
US Governor 98.9 97.0 95.3 94.1 93.9 55 55 54 53 53
Taiwan Mayor 99.8 98.3 96.9 93.8 94.3 25 25 25 24 24
NBA Team 100.0 100.0 99.2 98.4 98.6 30 30 30 30 30
Fed. Agency 65.5 54.5 27.9 55.3 30.0 97 97 61 95 69
Car Maker 81.6 0.0 0.0 0.0 0.0 90 0 0 0 0
Average 89.2 70.0 63.9 68.3 63.4 59 41 34 40 35
Table 6: Performance of various types of wrappers on the five relational datasets after 10
th
iteration.
4.2 Experiments with Binary Relations
For binary relations, we performed the same ex-
periment as with unary relations described in Sec-
tion 3. A relational wrapper is of type t if the
wrapper?s left and right context match t?s con-
straint for left and right respectively, and also
that the wrapper?s middle context match both con-
straints.
For choosing the evaluation datasets for rela-
tional set expansion, we surveyed and obtained a
dozen relationships, from which we randomly se-
lected five of them and present in Table 4. Each
dataset was then manually constructed. For the
last two datasets, since there are too many items,
we tried our best to make the lists as exhaustive as
possible.
To evaluate relational wrappers, we performed
relational set expansion on randomly selected
seeds from the five relational datasets. For every
dataset, we select two seeds randomly and boot-
strap the relational set expansion ten times. The
results after the first iteration are shown in Table 5
and after the tenth iteration in Table 6. When com-
puting precision at 100 for each resulting list, we
kept only the top-most-ranked synonym of every
instance and remove all other synonyms from the
list; this ensures that every instance is unique. No-
tice that for the ?Car Maker? dataset, there exists
no wrappers of types 2 to 5; thus resulting in zero
performance for those wrapper types. In each ta-
ble, the results indicate that character-based wrap-
pers perform the best, while those HTML-based
wrappers that require tight HTML bracketing of
items (type 3 and 5) perform the worse.
In addition, the results illustrate that bootstrap-
ping is effective for expanding relational pairs of
items. As illustrated in Table 6, the result of find-
ing translation pairs of NBA team names is per-
fect, and it is almost perfect for finding pairs of
U.S. states/territories and governors, as well as
Taiwanese cities and mayors. In finding pairs of
acronyms and full names of federal agencies, the
precision at top 100 is nearly perfect (97%). The
results for finding pairs of car makers and coun-
tries is good as well, with a high precision of
90%. For the last two datasets, we believe that
MAP could be improved by increasing the number
of bootstrapping iterations. Table 7 shows some
example wrappers constructed and instances ex-
tracted for wrappers of type 1.
1509
Seeds: kentucky / steve beshear, north dakota / john hoeven
URL: http://wikifoia.pbworks.com/Alaska-Governor-Sarah-Palin
Wrapper: Governor [.2.]">[.1.] Governor
URL: http://blogs.suntimes.com/sweet/2008/02/sweet state dinner for governo.html
Wrapper: <br /> <br /> The Honorable [.2.], Governor of [.1.] <br /> <br />
URL: http://en.wikipedia.org/wiki/United States Senate elections, 2010
Wrapper: " title="Governor of [.1.]">Governor</a> <a href="/wiki/[.2.]" title="
URL: http://ballotbox.governing.com/2008/07/index.html
Wrapper: , [.1.]?s [.2.],
Content: alabama / bob riley, alaska / sarah palin, arizona / janet napolitano, arkansas / mike huckabee, california /
arnold schwarzenegger, colorado / bill ritter, connecticut / mary jodi rell, delaware / ruth ann minner, florida
/ charlie crist, georgia / sonny perdue, hawaii / linda lingle, idaho / butch otter, illinois / rod blagojevich. . .
Seeds: cia / central intelligence agency, usps / united states postal service
URL: http://www1.american.edu/dccampus/links/whitehouse.html
Wrapper: <a href="http://www.[.1.].gov" class="Links2nd">[.2.]</a><span class="Links2nd">
URL: http://www.usembassy.at/en/us/gov.htm
Wrapper: /" target=" blank">[.2.] ([.1.])</a> -
URL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agencies
Wrapper: The [.2.] ([.1.]) is
URL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agencies
Wrapper: </li> <li>[.1.]- <a href="/encyclopedia/[.2.]" onmouseover="pv(event, 2
Content: achp / advisory council on historic preservation, arc / appalachian regional commission, cftc / commod-
ity futures trading commission, cia / central intelligence agency, cms / centers for medicare and medicaid
services, exim bank / export import bank of the united states, ntrc / national transportation research center. . .
Seeds: mazda / japan, venturi / france
URL: http://www.jrfilters.com/filtres/index.php?lng=en
Wrapper: &page=filtres&lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.])</option><option value="index.php?
URL: http://www.jrfilters.com/suspensions/index.php?famille=1&lng=en
Wrapper: &lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.])</option><option value="index.php?famille=1&rubrique1
URL: http://www.street-car.net/forums/forumdisplay.php?f=10
Wrapper: "><strong>[.1.]</strong></a> </div> <div class="smallfont">Country of origin:[.2.].
URL: http://www.allcarcentral.com/
Wrapper: file.html">[.1.],[.2.]</a><br />
Content: abarth / italy, acura / japan, alfa romeo / italy, aston martin / england, auburn / usa, audi / germany, austin
healey / england, austin / england, auto union / germany, balwin / usa, bandini / italy, bentley / england, bmw
/ germany, brabham / england, bricklin / usa, bristol / england, brm / england, bucciali / france. . .
Table 7: Examples of (type 1) wrappers constructed and instances (contents) extracted.
1510
5 Related Work
In recent years, many research has been done
on extracting relations from free text (e.g., (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000; Snow et al, 2006)); however, al-
most all of them require some language-dependent
parsers or taggers for English, which restrict
the language of their extractions to English only
(or languages that have these parsers). There
has also been work done on extracting relations
from HTML-structured tables (e.g., (Etzioni et al,
2005; Nadeau et al, 2006; Cafarella et al, 2008));
however, they all incorporated heuristics for ex-
ploiting HTML structures; thus, they cannot han-
dle documents written in other mark-up languages.
Extracting relations at character-level from
semi-structured documents has been proposed
(e.g., (Kushmerick et al, 1997),(Brin, 1998)).
In particular, Brin?s approach (DIPRE) is the
most similar to ours in terms of expanding rela-
tional items. One difference is that it requires
maximally-long contextual strings to bracket al
seed occurrences. This technique has been experi-
mentally illustrated to perform worse than SEAL?s
approach on unary relations (Wang and Cohen,
2007). Brin presented five seed pairs of author
names and book titles that he used in the exper-
iment (unfortunately, he did not provide detailed
results). We input the top two seed pairs listed in
his paper into the relational SEAL, performed ten
bootstrapping iterations (took about 3 minutes),
and obtained 26,000 author name/book title pairs
of which the precision at 100 is perfect (100%).
6 Conclusions
In this paper, we have described in detail an al-
gorithm for constructing document-specific wrap-
pers automatically for set expansion. In the exper-
imental results, we have illustrated that character-
based wrappers are better suited than HTML-
based wrappers for the task of set expansion. We
also presented a method that utilizes an additional
middle context for constructing relational wrap-
pers. We also showed that our relational set ex-
pansion approach is language-independent; it can
be applied to non-English and even cross-lingual
seeds and documents. Furthermore, we have il-
lustrated that bootstrapping improves the perfor-
mance of relational set expansion. In the future,
we will explore automatic mining of binary con-
cepts given only the relation (e.g., ?mayor of?).
7 Acknowledgments
This work was supported by the Google Research
Awards program.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In In Proceedings of the 5th ACM Inter-
national Conference on Digital Libraries, pages 85?
94.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT98, pages 172?183.
Michael J. Cafarella, Alon Y. Halevy, Daisy Z. Wang,
Eugene W. 0002, and Yang Zhang. 2008. Webta-
bles: exploring the power of tables on the web.
PVLDB, 1(1):538?549.
A. Carlson, J. Betteridge, E.R. Hruschka Junior, and
T.M. Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL HLT
Workshop on Semi-supervised Learning for Natural
Language Processing, pages 1?9. Association for
Computational Linguistics.
Oren Etzioni, Michael J. Cafarella, Doug Downey,
Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artif. Intell.,
165(1):91?134.
N. Kushmerick, D. Weld, and B. Doorenbos. 1997.
Wrapper induction for information extraction. In
Proc. Int. Joint Conf. Artificial Intelligence.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Luc Lamontagne and Mario Marchand, editors,
Canadian Conference on AI, volume 4013 of Lec-
ture Notes in Computer Science, pages 266?277.
Springer.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 113?120, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL ?06: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the ACL, pages 801?
808, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
1511
Partha P. Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern
induction method for named entity extraction. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its appli-
cations. In ICDM, pages 613?622. IEEE Computer
Society.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
Richard C. Wang and William W. Cohen. 2008. Iter-
ative set expansion of named entities using the web.
In ICDM, pages 1091?1096. IEEE Computer Soci-
ety.
1512
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 443?450, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Extracting Personal Names from Email: Applying Named Entity
Recognition to Informal Text
Einat Minkov and Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15217
{einat,rcwang}@cs.cmu.edu
William W. Cohen
Ctr for Automated Learning & Discovery
Carnegie Mellon University
Pittsburgh, PA 15217
wcohen@cs.cmu.edu
Abstract
There has been little prior work on Named
Entity Recognition for ?informal? docu-
ments like email. We present two meth-
ods for improving performance of per-
son name recognizers for email: email-
specific structural features and a recall-
enhancing method which exploits name
repetition across multiple documents.
1 Introduction
Named entity recognition (NER), the identification
of entity names in free text, is a well-studied prob-
lem. In most previous work, NER has been applied
to news articles (e.g., (Bikel et al, 1999; McCal-
lum and Li, 2003)), scientific articles (e.g., (Craven
and Kumlien, 1999; Bunescu and Mooney, 2004)),
or web pages (e.g., (Freitag, 1998)). These genres of
text share two important properties: documents are
written for a fairly broad audience, and writers take
care in preparing documents. Important genres that
do not share these properties include instant messag-
ing logs, newsgroup postings and email messages.
We refer to these genres as ?informal? text.
Informal text is harder to process automatically.
Informal documents do not obey strict grammatical
conventions. They contain grammatical and spelling
errors. Further, since the audience is more restricted,
informal documents often use group- and task-
specific abbreviations and are not self-contained.
Because of these differences, existing NER methods
may require modifications to perform well on infor-
mal text.
In this paper, we investigate NER for informal
text with an experimental study of the problem of
recognizing personal names in email?a task that is
both useful and non-trivial. An application of in-
terest is corpus anonymization. Automatic or semi-
automatic email anonymization should allow using
large amounts of informal text for research purposes,
for example, of medical files. Person-name extrac-
tion and other NER tasks are helpful for automatic
processing of informal text for a large variety of ap-
plications (Culotta et al, 2004; Cohen et al, 2005).
We first present four corpora of email text, anno-
tated with personal names, each roughly compara-
ble in size to the MUC-6 corpus1. We experimen-
tally evaluate the performance of conditional ran-
dom fields (CRF) (Lafferty et al, 2001), a state-
of-the art machine-learning based NER methods on
these corpora. We then turn to examine the special
attributes of email text (vs. newswire) and suggest
venues for improving extraction performance. One
important observation is that email messages often
include some structured, easy-to-recognize names,
such as names within a header, names appearing in
automatically-generated phrases, as well as names in
signature files or sign-offs. We therefore suggest a
set of specialized structural features for email; these
features are shown to significantly improve perfor-
mance on our corpora.
We also present and evaluate a novel method for
exploiting repetition of names in a test corpus. Tech-
niques for exploiting name repetition within docu-
ments have been recently applied to newswire text
1Two of these are publicly available. The others can not be
distributed due to privacy considerations.
443
(e.g., (Humphreys et al, 1998)), scientific abstracts
(e.g., (Bunescu and Mooney, 2004)) and seminar an-
nouncements (Sutton and Mccallum, 2004); how-
ever, these techniques rely on either NP analysis or
capitalization information to pre-identify candidate
coreferent name mentions, features which are not re-
liable in email. Furthermore, we argue that name
repetition in email should be inferred by examining
multiple documents in a corpus, which is not com-
mon practice. We therefore present an alternative
efficient scheme for increasing recall in email, us-
ing the whole corpus. This technique is shown to
always improve recall substantially, and to almost
always improve F1 performance.
2 Corpora
Two email corpora used in our experiments were
extracted from the CSpace email corpus (Kraut et
al., 2004), which contains email messages collected
from a management course conducted at Carnegie
Mellon University in 1997. In this course, MBA stu-
dents, organized in teams of four to six members,
ran simulated companies in different market scenar-
ios. We believe this corpus to be quite similar to
the work-oriented mail of employees of a small or
medium-sized company. This text corpus contains
three header fields: ?From?, ?Subject?, and ?Time?.
Mgmt-Game is a subcorpora consisting of all emails
written over a five-day period. In the experiments,
the first day worth of email was used as a training
set, the fourth for tuning and the fifth day as a test
set. Mgmt-Teams forms another split of this data,
where the training set contains messages between
different teams than in the test set; hence in Mgmt-
Teams, the person names appearing in the test set
are generally different than those that appear in the
training set.
The next two collections of email were extracted
from the Enron corpus (Klimt and Yang, 2004). The
first subset, Enron-Meetings, consists of messages in
folders named ?meetings? or ?calendar?2 . Most but
not all of these messages are meeting-related. The
second subset, Enron-Random, was formed by re-
peatedly sampling a user name (uniformly at random
among 158 users), and then sampling an email from
2with two exceptions: (a) six very large files were removed,
and (b) one very large ?calendar? folder was excluded.
that user (uniformly at random).
Annotators were instructed to include nicknames
and misspelled names, but exclude person names
that are part of an email address and names that are
part of a larger entity name like an organization or
location (e.g., ?David Tepper School of Business?).
The sizes of the corpora are given in Table 1. We
limited training size to be relatively small, reflecting
a real-world scenario.
Corpus # Documents #Words #NamesTrain Tune Test x1000
Mgmt-Teams 120 82 83 105 2,792
Mgmt-Game 120 216 264 140 2,993
Enron-Meetings 244 242 247 204 2,868
Enron-Random 89 82 83 286 5,059
Table 1: Summary of the corpora used in the experiments.
The number of words and names refer to the whole annotated
corpora.
3 Existing NER Methods
In our first set of experiments we apply CRF, a
machine-learning based probabilistic approach to la-
beling sequences of examples, and evaluate it on the
problem of extracting personal names from email.
Learning reduces NER to the task of tagging (i.e.,
classifying) each word in a document. We use a set
of five tags, corresponding to (1) a one-token entity,
(2) the first token of a multi-token entity, (3) the last
token of a multi-token entity, (4) any other token of
a multi-token entity and (5) a token that is not part
of an entity.
The sets of features used are presented in Table
2. All features are instantiated for the focus word, as
well as for a window of 3 tokens to the left and to the
right of the focus word. The basic features include
the lower-case value of a token t, and its capital-
ization pattern, constructed by replacing all capital
letters with the letter ?X?, all lower-case letters with
?x?, all digits with ?9? and compressing runs of the
same letter with a single letter. The dictionary fea-
tures define various categories of words including
common words, first names, last names 3 and ?roster
names? 4 (international names list, where first and
3We used US Census? lists of the most com-
mon first and last names in the US, available from
http://www.census.gov/genealogy/www/freqnames.html
4A dictionary of 16,623 student names across the country,
obtained as part of the RosterFinder project (Sweeney, 2003)
444
Basic Features
t, lexical value, lowercase (binary form, e.g. f(t=?hello?)=1)
capitalization pattern of t (binary form, e.g. f(t.cap=x+)=1)
Dictionary Features
inCommon: t in common words dictionary
inFirst: t in first names dictionary
inLast: t in last names dictionary
inRoster: t in roster names dictionary
First: inFirst ? ?isLast ? ?inCommon
Last: ?inFirst ? inLast ? ?inCommon
Name: (First ? Last ? inRoster) ? ? inCommon
Title: t in a personal prefixes/suffixes dictionary
Org: t in organization suffixes dictionary
Loc: t in location suffixes dictionary
Email Features
t appears in the header
t appears in the ?from? field
t is a probable ?signoff?
(? after two line breaks and near end of message)
t is part of an email address (regular expression)
does the word starts a new sentence
(? capitalized after a period, question or exclamation mark)
t is a probable initial (X or X.)
t followed by the bigram ?and I?
t capitalized and followed by a pronoun within 15 tokens
Table 2: Feature sets
last names are mixed.) In addition, we constructed
some composite dictionary features, as specified in
Table 2: for example, a word that is in the first-name
dictionary and is not in the common-words or last-
name dictionaries is designated a ?sure first name?.
The common-words dictionary used consists of
base forms, conjugations and plural forms of com-
mon English words, and a relatively small ad-hoc
dictionary representing words especially common in
email (e.g., ?email?, ?inbox?). We also use small
manually created word dictionaries of prefixes and
suffixes indicative of persons (e.g., ?mr?, ?jr?), loca-
tions (e.g., ?ave?) and organizations (e.g., ?inc?).
Email structure features: We perform a simplified
document analysis of the email message and use this
to construct some additional features. One is an in-
dicator as to whether a token t is equal to some to-
ken in the ?from? field. Another indicates whether
a token t in the email body is equal to some token
appearing in the whole header. An indicator feature
based on a regular expression is used to mark tokens
that are part of a probable ?sign-off? (i.e., a name at
the end of a message). Finally, since the annotation
rules do not consider email addresses to be names,
we added an indicator feature for tokens that are in-
side an email address.
l.2.mr l.1.president
l.2.mrs l.2.dr
l.1.jr r.2.who
l.1.judge r.2.jr
r.3.staff l.3.by
l.2.ms r.3.president
r.2.staff l.3.by
r.1.family l.3.rep
l.3.says l.2.rep
r.3.reporter r.1.administration
l.1.by r.2.home
l.2.by r.1.or
l.3.name l.1.with
l.2.name l.1.thanks
l.3.by r.1.picked
r.3.his l.3.meet
r.1.ps r.1.started
r.3.home r.1.told
r.1.and l.2.prof
l.1.called l.2.email
Figure 1: Predictive contexts for personal-name words for
MUC-6 (left) and Mgmt-Game (right) corpora. A features is
denoted by its direction comparing to the focus word (l/r), offset
and lexical value.
We experimented with features derived from POS
tags and NP-chunking of the email, but found the
POS assignment too noisy to be useful. We did in-
clude some features based on approximate linguistic
rules. One rule looks for capitalized words that are
not common words and are followed by a pronoun
within a distance of up to 15 tokens. (As an exam-
ple, consider ?Contact Puck tomorrow. He should be
around.?). Another rule looks for words followed by
the bigram ?and I?. As is common for hand-coded
NER rules, both these rules have high precision and
low recall.
3.1 Email vs Newswire
In order to explore some of the differences between
email and newswire NER problems, we stripped all
header fields from the Mgmt-Game messages, and
trained a model (using basic features only) from the
resulting corpus of email bodies. Figure 1 shows the
features most indicative of a token being part of a
name in the models trained for the Mgmt-Game and
MUC-6 corpora. To make the list easier to interpret,
it includes only the features corresponding to tokens
surrounding the focus word.
As one might expect, the important features from
the MUC-6 dataset are mainly formal name titles
such as ?mr?, ?mrs?, and ?jr?, as well as job ti-
tles and other pronominal modifiers such as ?pres-
ident? and ?judge?. However, for the Mgmt-Game
corpus, most of the important features are related
to email-specific structure. For example, the fea-
tures ?left.1.by? and ?left.2.by? are often associated
with a quoted excerpt from another email message,
which in the Mgmt-Game corpus is often marked
by mailers with text like ?Excerpts from mail: 7-
445
Sep-97 Re: paper deadline by Richard Wang?. Sim-
ilarly, features like ?left.1.thanks? and ?right.1.ps?
indicate a ?signoff? section of an email, as does
?right.2.home? (which often indicates proximity to
a home phone number appearing in a signature).
3.2 Experimental Results
We now turn to evaluate the usefulness of the fea-
ture sets described above. Table 3 gives entity-level
F1 performance 5 for CRF trained models for all
datasets, using the basic features alone (B); the ba-
sic and email-tailored features (B+E); the basic and
dictionary features (B+D); and, all of the feature sets
combined (B+D+E). All feature sets were tuned us-
ing the Mgmt-Game validation subset. The given
results relate to previously unseen test sets.
Dataset B B+E B+D B+D+E
Mgmt-Teams 68.1 75.7 82.0 87.9
Mgmt-Game 79.2 84.2 90.7 91.9
Enron-Meetings 59.0 71.5 78.6 76.9
Enron-Random 68.1 70.2 72.9 76.2
Table 3: F1 entity-leavel performance for the sets of features,
across all datasets, with CRF training.
The results show that the email-specific features
are very informative. In addition, they show that
the dictionary features are especially useful. This
can be explained by the relatively weak contextual
evidence in email. While dictionaries are useful in
named entities extraction in general, they are in fact
more essential when extracting names from email
text, where many name mentions are part of headers,
names lists etc. Finally, the results for the combined
feature set are superior in most cases to any subset
of the features.
Overall the level of performance using all fea-
tures is encouraging, considering the limited training
set size. Performance on Mgmt-Teams is somewhat
lower than for Mgmt-Game mainly because (by de-
sign) there is less similarity between training and
test sets with this split. Enron emails seem to be
harder than Mgmt-Game emails, perhaps because
they include fewer structured instances of names.
Enron-Meetings emails also contain a number of
constructs that were not encountered in the Mgmt-
Game corpus, notably lists (e.g., of people attending
a meeting), and also include many location and or-
5No credit awarded for partially correct entity boundaries.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 5  10  15  20  25  30  35
Mgmt Game
Enron-Meetings
Enron-Random
MUC-6
Figure 2: Cumulative percentage of person-name tokens w
that appear in at most K distinct documents as a function of K.
ganization names, which are rare in Mgmt-Game. A
larger set of dictionaries might improve performance
for the Enron corpora.
4 Repetition of named entities in email
In the experiments described above, the extractors
have high precision, but relatively low recall. This
typical behavior suggests that some sort of recall-
enhancing procedure might improve overall perfor-
mance.
One family of recall-enhancing techniques are
based on looking for multiple occurrences of names
in a document, so that names which occur in am-
biguous contexts will be more likely to be recog-
nized. It is an intuitive assumption that the ways in
which names repeat themselves in a corpus will be
different in email and newswire text. In news stories,
one would expect repetitions within a single docu-
ment to be common, as a means for an author to es-
tablish a shared context with the reader. In an email
corpus, one would expect names to repeat more fre-
quently across the corpus, in multiple documents?
at least when the email corpus is associated with a
group that works together closely. In this section we
support this conjecture with quantitative analysis.
In a first experiment, we plotted the percentage
of person-name tokens w that appear in at most
K distinct documents as a function of K. Figure
2 shows this function for the Mgmt-Game, MUC-
6, Enron-Meetings, and Enron-Random datasets.
There is a large separation between MUC-6 and
Mgmt-Game, the most workgroup-oriented email
corpus. In MUC-6, for instance, almost 80% of the
446
Single-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
SDR
CRF
SDR+CRF
(a) SDR
Multiple-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
MDR
CRF
MDR+CRF
(b) MDR
Figure 3: Upper bounds on recall and recall improvements
associated with methods that look for terms that re-occur within
a single document (SDR) or across multiple documents (MDR).
names appear in only a single document, while in
Mgmt-Game, only 30% of the names appear in only
a single document. At the other extreme, in MUC-6,
only 1.3% of the names appear in 10 or more docu-
ments, while in Mgmt-Game, almost 20% do. The
Enron-Random and Enron-Meetings datasets show
distributions of names that are intermediate between
Mgmt-Game and MUC-6.
As a second experiment, we implemented two
very simple extraction rules. The single document
repetition (SDR) rule marks every token that oc-
curs more than once inside a single document as a
name. Adding tokens marked by the SDR rule to
the tokens marked by the learned extractor generates
a new extractor, which we will denote SDR+CRF.
Thus, the recall of SDR+CRF serves as an upper
bound on the token recall6 of any recall-enhancing
6Token level recall is recall on the task of classifying tokens
as inside or outside an entity name.
method that improves the extractor by exploiting
repetition within a single document. Analogously,
the multiple document repetition (MDR) rule marks
every token that occurs in more than one document
as a name. Again, the token recall of MDR+CRF
rule is an upper bound on the token recall of any
recall-enhancing method that exploits token repeti-
tion across multiple documents.
The left bars in Figure 3 show the recall obtained
by the SDR (top) and the MDR rule (bottom). The
MDR rule has highest recall for the two Mgmt cor-
pora, and lowest recall for the MUC-6 corpus. Con-
versely, for the SDR rule, the highest recall level
obtained is for MUC-6. The middle bars show the
token recall obtained by the CRF extractor, using
all features. The right bars show the token recall
of the SDR+CRF and MDR+CRF extractors. Com-
paring them to the other bars, we see that the maxi-
mal potential recall gain from a SDR-like method is
on MUC-6. For MDR-like methods, there are large
potential gains on the Mgmt corpora as well as on
Enron-Meetings and Enron-Random to a lesser de-
gree. This probably reflects the fact that the Enron
corpora are from a larger and more weakly interact-
ing set of users, compared to the Mgmt datasets.
These results demonstrate the importance of ex-
ploiting repetition of names across multiple docu-
ments for entity extraction from email.
5 Improving Recall With Inferred
Dictionaries
Sequential learners of the sort used here classify to-
kens from each document independently; moreover,
the classification of a word w is independent of the
classification of other occurrences of w elsewhere in
the document. That is, the fact that a word w has ap-
peared somewhere in a context that clearly indicates
that it is a name does not increase the probability that
it will be classified as a name in other, more ambigu-
ous contexts.
Recently, sequential learning methods have been
extended to directly utilize information about name
co-occurrence in learning the sequential classifier.
This approach provides an elegant solution to mod-
eling repetition within a single document. However,
it requires identifying candidate related entities in
advance, applying some heuristic. Thus, Bunescu &
447
Mooney (2004) link between similar NPs (requiring
their head to be identical), and Sutton and Mccallum
(2004) connect pairs of identical capitalized words.
Given that in email corpora capitalization patterns
are not followed to a large extent, there is no ad-
equate heuristic that would link candidate entities
prior to extraction. Further, it is not clear if a col-
lective classification approach can scale to modeling
multiple-document repetition.
We suggest an alternative approach of recall-
enhancing name matching, which is appropriate for
email. Our approach has points of similarity to
the methods described by Stevenson and Gaizauskas
(2000), who suggest matching text against name dic-
tionaries, filtering out names that are also common
words or appear as non-names in high proportion
in the training data. The approach described here
is more systematic and general. In a nutshell, we
suggest applying the noisy dictionary of predicted
names over the test corpus, and use the approximate
(predicted) name to non-name proportions over the
test set itself to filter out ambiguous names. There-
fore, our approach does not require large amount of
annotated training data. It also does not require word
distribution to be similar between train and test data.
We will now describe our approach in detail.
5.1 Matching names from dictionary
First, we construct a dictionary comprised of all
spans predicted as names by the learned model. For
personal names, we suggest expanding this dictio-
nary further, using a transformation scheme. Such a
scheme would construct a family of possible varia-
tions of a name n: as an example, Figure 4 shows
name variations created for the name span ?Ben-
jamin Brown Smith?. Once a dictionary is formed,
a single pass is made through the corpus, and ev-
ery longest match to some name-variation is marked
as a name7. It may be that a partial name span n1
identified by the extractor is subsumed by the full
name span n2 identified by the dictionary-matching
scheme. In this case, entity-level precision is in-
creased, having corrected the entity?s boundaries.
7Initials-only variants of a name, e.g., ?bs? in Figure 4 are
marked as a name only if the ?inSignoff? feature holds?i.e., if
they appear near the end of a message in an apparent signature.
benjamin brown smith benjamin-brown-s. b. brown s. bbs
benjamin-brown smith benjamin-b. s. b. b. smith bs
benjamin brown-smith benjamin-smith b. brown-s.
benjamin-brown-smith benjamin smith benjamin
benjamin brown s. b. brown smith brown
benjamin-b. smith benjamin b. s. smith
benjamin b. smith b. brown-smith b. smith
benjamin brown-s. benjamin-s. b. b. s
benjamin-brown s. benjamin s. b. s.
Figure 4: Names variants created from the name ?Benjamin
Brown Smith?
5.2 Dictionary-filtering schemes
The noisy dictionary-matching scheme is suscepti-
ble to false positives. That is, some words predicted
by the extractor to be names are in fact non-names.
Presumably, these non-names could be removed by
simply eliminating low-confidence predictions of
the extractor; however, ambiguous words ?that are
not exclusively personal names in the corpus? may
need to be identified and removed as well. We note
that ambiguity better be evaluated in the context of
the corpus. For example, ?Andrew? is a common
first name, and may be confidently (and correctly)
recognized as one by the extractor. However, in the
Mgmt-Game corpus, ?Andrew? is also the name of
an email server, and most of the occurrences of this
name in this corpus are not personal names. The
high frequency of the word ?Andrew? in the cor-
pus, coupled with the fact that it is only sometimes a
name, means that adding this word to the dictionary
leads to a substantial drop in precision.
We therefore suggest a measure for filtering the
dictionary. This measure combines two metrics. The
first metric, predicted frequency (PF), estimates the
degree to which a word appears to be used consis-
tently as a name throughout the corpus:
PF (w) ? cpf(w)ctf(w)
where cpf(w) denotes the number of times that a
word w is predicted as part of a name by the extrac-
tor, and ctf(w) is the number of occurrences of the
word w in the entire test corpus (we emphasize that
estimating this statistic based on test data is valid, as
it is fully automatic ?blind? procedure).
Predicted frequency does not assess the likely cost
of adding a word to a dictionary: as noted above,
ambiguous or false dictionary terms that occur fre-
quently will degrade accuracy. A number of statis-
tics could be used here; for instance, practitioners
448
sometimes filter a large dictionary by simply dis-
carding all words that occur more than k times in a
test corpus. We elected to use the inverse document
frequency (IDF) of w to measure word frequency:
IDF (w) ?
log(N+0.5df(w) )
log(N + 1)
Here df(w) is the number of documents that contain
a word w, and N is the total number of documents
in the corpus. Inverse document frequency is often
used in the field of information retrieval (Allan et al,
1998), and the formula above has the virtue of being
scaled between 0 and 1 (like our PF metric) and of
including some smoothing. In addition to bounding
the cost of a dictionary entry, the IDF formula is in
itself a sensible filter, since personal names will not
appear as frequently as common English words.
The joint filter combines these two multiplica-
tively, with equal weights:
PF.IDF (w) : PF (w) ? IDF (w)
PF.IDF takes into consideration both the probability
of a word being a name, and how common it is in
the entire corpus. Words that get low PF.IDF scores
are therefore either words that are highly ambiguous
in the corpus (as derived from the extractors? pre-
dictions) or are common words, which were inaccu-
rately predicted as names by the extractor.
In the MDR method of Figure 3, we imposed
an artificial requirement that words must appear in
more than one document. In the method described
here, there is no such requirement: indeed, words
that appear in a small number of documents are
given higher weights, due to the IDF factor. Thus
this approach exploits both single-document and
multiple-document repetitions.
In a set of experiments that are not described here,
the PF.IDF measure was found to be robust to pa-
rameter settings, and also preferable to its separate
components in improving recall at minimal cost in
precision. As described, the PF.IDF values per word
range between 0 and 1. One can vary the threshold,
under which a word is to be removed from the dic-
tionary, to control the precision-recall trade-off. We
tuned the PF.IDF threshold using the validation sub-
sets, optimizing entity-level F1 (a threshold of 0.16
was found optimal).
In summary, our recall-enhancing strategy is as
follows:
1. Learn an extractor E from the training corpus Ctrain .
2. Apply the extractor E to a test corpus Ctest to assign a
preliminary labeling.
3. Build a dictionary S?? including the names n such that
(a) n is extracted somewhere in the preliminary label-
ing of the test corpus, or is derived from an extracted
name applying the name transformation scheme and (b)
PF.IDF (n) > ??.
4. Apply the dictionary-matching scheme of Section 5.1, us-
ing the dictionary S?? to augment the preliminary label-
ing, and output the result.
5.3 Experiments with inferred dictionaries
Table 4 shows results using the method described
above. We consider all of the email corpora and the
CRF learner, trained with the full feature set. The
results are given in terms of relative change, com-
pared to the baseline results generated by the extrac-
tors (scoreresult/scorebaseline ? 1) and final value.
As expected, recall is always improved. Entity-
level F1 is increased as well, as recall is increased
more than precision is decreased. The largest im-
provements are for the Mgmt corpora ?the two e-
mail datasets shown to have the largest potential im-
provement from MDR-like methods in Figure 3. Re-
call improvements are more modest for the Enron
datasets, as was anticipated by the MDR analysis.
Another reason for the gap is that extractor baseline
performance is lower for the Enron datasets, so that
the Enron dictionaries are noisier.
As detailed in Section 2, the Mgmt-Teams dataset
was constructed so that the names in the training
and test set have only minimal overlap. The perfor-
mance improvement on this dataset shows that rep-
etition of mostly-novel names can be detected using
our method. This technique is highly effective when
names are novel, or dense, and is optimal when ex-
tractor baseline precision is relatively high.
Dataset Precision Recall F1
Mgmt-Teams -0.9% / 92.9 +8.5% / 89.8 +3.9% / 91.3
Mgmt-Game -0.8% / 94.5 +8.4% / 96.2 +3.8% / 95.4
Enron-Meetings -2.5% / 81.1 +4.7% / 74.9 +1.2% / 77.9
Enron-Random -3.8% / 79.2 +4.9% / 74.3 +0.7% / 76.7
Table 4: Entity-level relative improvement and final result,
applying name-matching on models trained with CRF and the
full feature set (F1 baseline given in Table 3).
449
6 Conclusion
This work applies recently-developed sequential
learning methods to the task of extraction of named
entities from email. This problem is of interest as an
example of NER from informal text?text that has
been prepared quickly for a narrow audience.
We showed that informal text has different char-
acteristics from formal text such as newswire. Anal-
ysis of the highly-weighted features selected by the
learners showed that names in informal text have
different (and less informative) types of contextual
evidence. However, email also has some structural
regularities which make it easier to extract personal
names. We presented a detailed description of a set
of features that address these regularities and signif-
icantly improve extraction performance on email.
In the second part of this paper, we analyzed
the way in which names repeat in different types
of corpora. We showed that repetitions within a
single document are more common in newswire
text, and that repetitions that span multiple docu-
ments are more common in email corpora. Addi-
tional analysis confirms that the potential gains in
recall from exploiting multiple-document repetition
is much higher than the potential gains from exploit-
ing single-document repetition.
Based on this insight, we introduced a simple and
effective method for exploiting multiple-document
repetition to improve an extractor. One drawback of
the recall-enhancing approach is that it requires the
entire test set to be available: however, our test sets
are of only moderate size (83 to 264 documents),
and it is likely that a similar-size sample of unlabeled
data would be available in many practical applica-
tions. The approach substantially improves recall
and often improves F1 performance; furthermore, it
can be easily used with any NER method.
Taken together, extraction performance is sub-
stantially improved by this approach. The improve-
ments seem to be strongest for email corpora col-
lected from closely interacting groups. On the
Mgmt-Teams dataset, which was designed to reduce
the value of memorizing specific names appearing
in the training set, F1 performance is improved from
68.1% for the out-of-the-box system (or 82.0% for
the dictionary-augmented system) to 91.3%. For the
less difficult Mgmt-Game dataset, F1 performance
is improved from 79.2% for an out-of-the-box CRF-
based NER system (or 90.7% for a CRF-based sys-
tem that uses several large dictionaries) to 95.4%.
As future work, experiments should be expanded to
include additional entity types and other types of in-
formal text, such as blogs and forum postings.
References
J. Allan, J. Callan, W.B. Croft, L. Ballesteros, D. Byrd,
R. Swan, and J. Xu. 1998. Inquery does battle with trec-
6. In TREC-6.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine Learning,
34:211?231.
R. Bunescu and R. J. Mooney. 2004. Relational markov net-
works for collective information extraction. In ICML-2004
Workshop on Statistical Relational Learning.
W. W. Cohen, E. Minkov, and A. Tomasic. 2005. Learning to
undertand website update requests. In IJCAI-05.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from text
sources. In ISMB-99.
A. Culotta, R. Bekkerman, and A. McCallum. 2004. Extracting
social networks and contact information from email and the
web. In CEAS-04.
D. Freitag. 1998. Information extraction from html: applica-
tion of a general machine learning approach. In AAAI-98.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 1998. Descrip-
tion of the LASIE-II system as used for MUC-7.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS-04.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. Espinosa. 2004.
Coordination in teams: evi-dence from a simulated manage-
ment game. To appear in the Journal of Organizational Be-
havior.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-01.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CoNLL-2003.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-derived
names lists for named entities recognition. In NAACL-2000.
C. Sutton and A. Mccallum. 2004. Collective segmentation
and labeling of distant entities in information extraction. In
ICML workshop on Statistical Relational Learning.
L. Sweeney. 2003. Finding lists of people on the web.
Technical Report CMU-CS-03-168, CMU-ISRI-03-104.
http://privacy.cs.cmu.edu/dataprivacy/ projects/rosterfinder/.
450
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 93?96,
New York, June 2006. c?2006 Association for Computational Linguistics
NER Systems that Suit User?s Preferences: Adjusting the Recall-Precision
Trade-off for Entity Extraction
Einat Minkov, Richard C. Wang
Language Technologies
Institute
Carnegie Mellon University
einat,rcwang@cs.cmu.edu
Anthony Tomasic
Inst. for Software Research
International
Carnegie Mellon University
tomasic@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
We describe a method based on ?tweak-
ing? an existing learned sequential classi-
fier to change the recall-precision tradeoff,
guided by a user-provided performance
criterion. This method is evaluated on
the task of recognizing personal names in
email and newswire text, and proves to be
both simple and effective.
1 Introduction
Named entity recognition (NER) is the task of iden-
tifying named entities in free text?typically per-
sonal names, organizations, gene-protein entities,
and so on. Recently, sequential learning methods,
such as hidden Markov models (HMMs) and con-
ditional random fields (CRFs), have been used suc-
cessfully for a number of applications, including
NER (Sha and Pereira, 2003; Pinto et al, 2003; Mc-
callum and Lee, 2003). In practice, these methods
provide imperfect performance: precision and re-
call, even for well-studied problems on clean well-
written text, reach at most the mid-90?s. While
performance of NER systems is often evaluated in
terms of F1 measure (a harmonic mean of preci-
sion and recall), this measure may not match user
preferences regarding precision and recall. Further-
more, learned NER models may be sub-optimal also
in terms of F1, as they are trained to optimize other
measures (e.g., loglikelihood of the training data for
CRFs).
Obviously, different applications of NER have
different requirements for precision and recall. A
system might require high precision if it is designed
to extract entities as one stage of fact-extraction,
where facts are stored directly into a database. On
the other hand, a system that generates candidate ex-
tractions which are passed to a semi-automatic cu-
ration system might prefer higher recall. In some
domains, such as anonymization of medical records,
high recall is essential.
One way to manipulate an extractor?s precision-
recall tradeoff is to assign a confidence score to each
extracted entity and then apply a global threshold to
confidence level. However, confidence thresholding
of this sort cannot increase recall. Also, while confi-
dence scores are straightforward to compute in many
classification settings, there is no inherent mecha-
nism for computing confidence of a sequential ex-
tractor. Culotta and McCallum (2004) suggest sev-
eral methods for doing this with CRFs.
In this paper, we suggest an alternative simple
method for exploring and optimizing the relation-
ship between precision and recall for NER systems.
In particular, we describe and evaluate a technique
called ?extractor tweaking? that optimizes a learned
extractor with respect to a specific evaluation met-
ric. In a nutshell, we directly tweak the threashold
term that is part of any linear classifier, including se-
quential extractors. Though simple, this approach
has not been empirically evaluated before, to our
knowledge. Further, although sequential extractors
such as HMMs and CRFs are state-of-the-art meth-
ods for tasks like NER, there has been little prior re-
search about tuning these extractors? performance to
suit user preferences. The suggested algorithm op-
timizes the system performance per a user-provided
93
evaluation criterion, using a linear search procedure.
Applying this procedure is not trivial, since the un-
derlying function is not smooth. However, we show
that the system?s precision-recall rate can indeed be
tuned to user preferences given labelled data using
this method. Empirical results are presented for a
particular NER task?recognizing person names, for
three corpora, including email and newswire text.
2 Extractor tweaking
Learning methods such as VP-HMM and CRFs op-
timize criteria such as margin separation (implicitly
maximized by VP-HMMs) or log-likelihood (ex-
plicitly maximized by CRFs), which are at best indi-
rectly related to precision and recall. Can such learn-
ing methods be modified to more directly reward a
user-provided performance metric?
In a non-sequential classifier, a threshold on confi-
dence can be set to alter the precision-recall tradeoff.
This is nontrivial to do for VP-HMMs and CRFs.
Both learners use dynamic programming to find the
label sequence y = (y1, . . . , yi, . . . , yN ) for a word
sequence x = (x1, . . . , xi, . . . , xN ) that maximizes
the function W ?
?
i f(x, i, yi?1, yi) , where W is
the learned weight vector and f is a vector of fea-
tures computed from x, i, the label yi for xi, and the
previous label yi?1. Dynamic programming finds
the most likely state sequence, and does not output
probability for a particular sub-sequence. (Culotta
and McCallum, 2004) suggest several ways to gen-
erate confidence estimation in this framework. We
propose a simpler approach for directly manipulat-
ing the learned extractor?s precision-recall ratio.
We will assume that the labels y include one label
O for ?outside any named entity?, and let w0 be the
weight for the feature f0, defined as follows:
f0(x, i, yi?1, yi) ?
{
1 if yi = O
0 else
If no such feature exists, then we will create one.
The NER based on W will be sensitive to the value
of w0: large negative values will force the dynamic
programming method to label tokens as inside enti-
ties, and large positive values will force it to label
fewer entities1.
1We clarify that w0 will refer to feature f0 only, and not to
other features that may incorporate label information.
We thus propose to ?tweak? a learned NER by
varying the single parameter w0 systematically so as
to optimize some user-provided performance metric.
Specifically, we tune w0 using a a Gauss-Newton
line search, where the objective function is itera-
tively approximated by quadratics.2 We terminate
the search when two adjacent evaluation results are
within a 0.01% difference3.
A variety of performance metrics might be imag-
ined: for instance, one might wish to optimize re-
call, after applying some sort of penalty for pre-
cision below some fixed threshold. In this paper
we will experiment with performance metrics based
on the (complete) F-measure formula, which com-
bines precision and recall into a single numeric value
based on a user-provided parameter ?:
F (?, P,R) = (?
2 + 1)PR
?2P +R
A value of ? > 1 assigns higher importance to re-
call. In particular, F2 weights recall twice as much
as precision. Similarly, F0.5 weights precision twice
as much as recall.
We consider optimizing both token- and entity-
level F? ? awarding partial credit for partially ex-
tracted entities and no credit for incorrect entity
boundaries, respectively. Performance is optimized
over the dataset on which W was trained, and tested
on a separate set. A key question our evaluation
should address is whether the values optimized for
the training examples transfer well to unseen test ex-
amples, using the suggested approximate procedure.
3 Experiments
3.1 Experimental Settings
We experiment with three datasets, of both email
and newswire text. Table 1 gives summary statis-
tics for all datasets. The widely-used MUC-6 dataset
includes news articles drawn from the Wall Street
Journal. The Enron dataset is a collection of emails
extracted from the Enron corpus (Klimt and Yang,
2004), where we use a subcollection of the mes-
sages located in folders named ?meetings? or ?cal-
endar?. The Mgmt-Groups dataset is a second email
2from http://billharlan.com/pub/code/inv.
3In the experiments, this is usually within around 10 itera-
tions. Each iteration requires evaluating a ?tweaked? extractor
on a training set.
94
collection, extracted from the CSpace email cor-
pus, which contains email messages sent by MBA
students taking a management course conducted at
Carnegie Mellon University in 1997. This data was
split such that its test set contains a different mix of
entity names comparing to training exmaples. Fur-
ther details about these datasets are available else-
where (Minkov et al, 2005).
# documents # names
Train Test # tokens per doc.
MUC-6 347 30 204,071 6.8
Enron 833 143 204,423 3.0
Mgmt-Groups 631 128 104,662 3.7
Table 1: Summary of the corpora used in the experiments
We used an implementation of Collins? voted-
percepton method for discriminatively training
HMMs (henceforth, VP-HMM) (Collins, 2002) as
well as CRF (Lafferty et al, 2001) to learn a NER.
Both VP-HMM and CRF were trained for 20 epochs
on every dataset, using a simple set of features such
as word identity and capitalization patterns for a
window of three words around each word being clas-
sified. Each word is classified as either inside or out-
side a person name.4
3.2 Extractor tweaking Results
Figure 1 evaluates the effectiveness of the optimiza-
tion process used by ?extractor tweaking? on the
Enron dataset. We optimized models for F? with
different values of ?, and also evaluated each op-
timized model with different F? metrics. The top
graph shows the results for token-level F? , and the
bottom graph shows entity-level F? behavior. The
graph illustates that the optimized model does in-
deed roughly maximize performance for the target
? value: for example, the token-level F? curve for
the model optimized for ? = 0.5 indeed peaks at
? = 0.5 on the test set data. The optimization is
only roughly accurate5 for several possible reasons:
first, there are differences between train and test sets;
in addition, the line search assumes that the perfor-
mance metric is smooth and convex, which need
not be true. Note that evaluation-metric optimiza-
tion is less successful for entity-level performance,
4This problem encoding is basic. However, in the context of
this paper we focus on precision-recall trade-off in the general
case, avoiding settings? optimization.
5E.g, the token-level F2 curve peaks at ? = 5.
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
Figure 1: Results of token-level (top) and entity-level (bot-
tom) optimization for varying values of ?, for the Enron dataset,
VP-HMM. The y-axis gives F in terms of ?. ? (x-axis) is given
in a logarithmic scale.
which behaves less smoothly than token-level per-
formance.
Token Entity
? Prec Recall Prec Recall
Baseline 93.3 76.0 93.6 70.6
0.2 100 53.2 98.2 57.0
0.5 95.3 71.1 94.4 67.9
1.0 88.6 79.4 89.2 70.9
2.0 81.0 83.9 81.8 70.9
5.0 65.8 91.3 69.4 71.4
Table 2: Sample optimized CRF results, for the MUC-6
dataset and entity-level optimization.
Similar results were obtained optimizing baseline
CRF classifiers. Sample results (for MUC-6 only,
due to space limitations) are given in Table 2, opti-
mizing a CRF baseline for entity-level F? . Note that
as ? increases, recall monotonically increases and
precision monotonically falls.
The graphs in Figure 2 present another set of re-
sults with a more traditional recall-precision curves.
The top three graphs are for token-level F? opti-
mization, and the bottom three are for entity-level
optimization. The solid lines show the token-level
and entity-level precision-recall tradeoff obtained by
95
MUC-6 Enron M.Groups
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
Figure 2: Results for the evaluation-metric model optimization. The top three graphs are for token-level F (?) optimization,
and the bottom three are for entity-level optimization. Each graph shows the baseline learned VP-HMM and evaluation-metric
optimization for different values of ?, in terms of both token-level and entity-level performance.
varying6 ? and optimizing the relevant measure for
F? ; the points labeled ?baseline? show the precision
and recall in token and entity level of the baseline
model, learned by VP-HMM. These graphs demon-
strate that extractor ?tweaking? gives approximately
smooth precision-recall curves, as desired. Again,
we note that the resulting recall-precision trade-
off for entity-level optimization is generally less
smooth.
4 Conclusion
We described an approach that is based on mod-
ifying an existing learned sequential classifier to
change the recall-precision tradeoff, guided by a
user-provided performance criterion. This approach
not only allows one to explore a recall-precision
tradeoff, but actually allows the user to specify a
performance metric to optimize, and optimizes a
learned NER system for that metric. We showed
that using a single free parameter and a Gauss-
Newton line search (where the objective is itera-
tively approximated by quadratics), effectively op-
timizes two plausible performance measures, token-
6We varied ? over the values 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 3
and 5
level F? and entity-level F? . This approach is in
fact general, as it is applicable for sequential and/or
structured learning applications other than NER.
References
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP.
A. Culotta and A. McCallum. 2004. Confidence estimation for
information extraction. In HLT-NAACL.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML.
A. Mccallum and W. Lee. 2003. early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CONLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting
personal names from emails: Applying named entity recog-
nition to informal text. In HLT-EMNLP.
D. Pinto, A. Mccallum, X. Wei, and W. B. Croft. 2003. table
extraction using conditional random fields. In ACM SIGIR.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In HLT-NAACL.
96
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477?485,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Predicting Response to Political Blog Posts with Topic Models
Tae Yano William W. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{taey,wcohen,nasmith}@cs.cmu.edu
Abstract
In this paper we model discussions in online
political blogs. To do this, we extend Latent
Dirichlet Allocation (Blei et al, 2003), in var-
ious ways to capture different characteristics
of the data. Our models jointly describe the
generation of the primary documents (posts)
as well as the authorship and, optionally, the
contents of the blog community?s verbal reac-
tions to each post (comments). We evaluate
our model on a novel comment prediction task
where the models are used to predict which
blog users will leave comments on a given
post. We also provide a qualitative discussion
about what the models discover.
1 Introduction
Web logging (blogging) and its social impact have
recently attracted considerable public and scientific
interest. One use of blogs is as a community dis-
cussion forum, especially for political discussion
and debate. Blogging has arguably opened a new
channel for huge numbers of people to express their
views with unprecedented speed and to unprece-
dented audiences. Their collective behavior in the
blogosphere has already been noted in the Ameri-
can political arena (Adamic and Glance, 2005). In
this paper we attempt to deliver a framework useful
for analyzing text in blogs quantitatively as well as
qualitatively. Better blog text analysis could lead to
better automated recommendation, organization, ex-
traction, and retrieval systems, and might facilitate
data-driven research in the social sciences.
Apart from the potential social utility of text pro-
cessing for this domain, we believe blog data is wor-
thy of scientific study in its own right. The sponta-
neous, reactive, and informal nature of the language
in this domain seems to defy conventional analytical
approaches in NLP such as supervised text classifi-
cation (Mullen and Malouf, 2006), yet the data are
rich in argumentative, topical, and temporal struc-
ture that can perhaps be modeled computationally.
We are especially interested in the semi-causal struc-
ture of blog discussions, in which a post ?spawns?
comments (or fails to do so), which meander among
topics and asides and show the personality of the
participants and the community.
Our approach is to develop probabilistic mod-
els for the generation of blog posts and comments
jointly within a blog site. The model is an extension
of Latent Dirichlet Allocation (Blei et al, 2003).
Unsupervised topic models can be applied to collec-
tions of unannotated documents, requiring very lit-
tle corpus engineering. They can be easily adapted
to new problems by altering the graphical model,
then applying standard probabilistic inference algo-
rithms. Different models can be compared to ex-
plore the ramifications of different hypotheses about
the data. For example, we will explore whether the
contents of posts a user has commented on in the
past and the words she has used can help predict
which posts she will respond to in the future.
The paper is organized as follows. In ?2 we re-
view prior work on topic modeling for document
collections and studies of social media like political
blogs. We then provide a qualitative characterization
of political blogs, highlighting some of the features
we believe a computational model should capture
and discuss our new corpus of political blogs (?3).
We present several different candidate topic models
that aim to capture these ideas in ?4. ?5 shows our
empirical evaluation on a new comment prediction
task and a qualitative analysis of the models learned.
2 Related Work
Network analysis, including citation analysis, has
been applied to document collections on the Web
(Cohn and Hofmann, 2001). Adamic and Glance
(2005) applied network analysis to the political bl-
477
ogosphere. The study modeled the large, complex
structure of the political blogosphere as a network
of hyperlinks among the blog sites, demonstrated the
viability of link structure for information discovery,
though their analysis of text content was less exten-
sive. In contrast, the text seems to be of interest
to social scientists studying blogs as an artifact of
the political process. Although attempts to quanti-
tatively analyze the contents of political texts have
been made, results from classical, supervised text
classification experiments are mixed (Mullen and
Malouf, 2006; Malouf and Mullen, 2007). Also, a
consensus on useful, reliable annotation or catego-
rization schemes for political texts, at any level of
granularity, has yet to emerge.
Meanwhile, latent topic modeling has become a
widely used unsupervised text analysis tool. The ba-
sic aim of those models is to discover recurring pat-
terns of ?topics? within a text collection. LDA was
introduced by Blei et al (2003) and has been espe-
cially popular because it can be understood as a gen-
erative model and because it discovers understand-
able topics in many scenarios (Steyvers and Grif-
fiths, 2007). Its declarative specification makes it
easy to extend for new kinds of text collections. The
technique has been applied to Web document collec-
tions, notably for community discovery in social net-
works (Zhang et al, 2007), opinion mining in user
reviews (Titov and McDonald, 2008), and sentiment
discovery in free-text annotations (Branavan et al,
2008). Dredze et al (2008) applied LDA to a collec-
tion of email for summary keyword extraction. The
authors evaluated the model with proxy tasks such as
recipient prediction. More closely related to the data
considered in this work, Lin et al (2008) applied a
variation of LDA to ideological discourse.
A notable trend in the recent research is to aug-
ment the models to describe non-textual evidence
alongside the document collection. Several such
studies are especially relevant to our work. Blei and
Jordan (2003) were one of the earliest results in this
trend. The concept was developed into more general
framework by Blei and McAuliffe (2008). Steyvers
et al (2004) and Rosen-Zvi et al (2004) first ex-
tended LDA to explicitly model the influence of au-
thorship, applying the model to a collection of aca-
demic papers from CiteSeer. The model combined
the ideas from the mixture model proposed by Mc-
Callum (1999) and LDA. In this model, an abstract
notion ?author? is associated with a distribution over
topics. Another approach to the same document col-
lection based on LDA was used for citation network
analysis. Erosheva et al (2004), following Cohn and
Hofmann (2001), defined a generative process not
only for each word in the text, but also its citation
to other documents in the collection, thereby cap-
turing the notion of relations between the document
into one generative process. Nallapati and Cohen
(2008) introduced the Link-PLSA-LDA model, in
which the contents of the citing document and the
?influences? on the document (its citations to exist-
ing literature), as well as the contents of the cited
documents, are modeled together. They further ap-
plied the Link-PLSA-LDA model to a blog corpus
to analyze its cross citation structure via hyperlinks.
In this work, we aim to model the data within blog
conversations, focusing on comments left by a blog
community in response to a blogger?s post.
3 Political Blog Data
We discuss next the dataset used in our experiments.
3.1 Corpus
We have collected blog posts and comments from
40 blog sites focusing on American politics during
the period November 2007 to October 2008, con-
temporaneous with the presidential elections. The
discussions on these blogs focus on American poli-
tics, and many themes appear: the Democratic and
Republican candidates, speculation about the results
of various state contests, and various aspects of
international and (more commonly) domestic poli-
tics. The sites were selected to have a variety of
political leanings. From this pool we chose five
blogs which accumulated a large number of posts
during this period: Carpetbagger (CB),1 Daily Kos
(DK),2 Matthew Yglesias (MY),3 Red State (RS),4
and Right Wing News (RWN).5 CB and MY ceased
as independent bloggers in August 2008.6 Because
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com
3http://matthewyglesias.theatlantic.com
4http://www.redstate.com
5http://www.rightwingnews.com
6The authors of those blogs now write for larger on-
line media, CB for Washingon Monthly at http://www.
478
MY RWN CB RS DK
Time span (from 11/11/07) ?8/2/08 ?10/10/08 ?8/25/08 ?6/26/08 ?4/9/08
# training posts 1607 1052 1080 2045 2146
# words (total) 110,788 194,948 183,635 321,699 221,820
(on average per post) (68) (185) (170) (157) (103)
# comments 56,507 34,734 34,244 59,687 425,494
(on average per post) (35) (33) (31) (29) (198)
(unique commenters, on average) (24) (13) (24) (14) (93)
# words in comments (total) 2,287,843 1,073,726 1,411,363 1,675,098 8,359,456
(on average per post) (1423) (1020) (1306) (819) (3895)
(on average per comment) (41) (31) (41) (27) (20)
Post vocabulary size 6,659 9,707 7,579 12,282 10,179
Comment vocabulary size 33,350 22,024 24,702 25,473 58,591
Size of user pool 7,341 963 5,059 2,789 16,849
# test posts 183 113 121 231 240
Table 1: Details of the blog data used in this paper.
our focus in this paper is on blog posts and their
comments, we discard posts on which no one com-
mented within six days. We also remove posts with
too few words: specifically, we retain a post only
if it has at least five words in the main entry, and
at least five words in the comment section. All
posts are represented as text only (images, hyper-
links, and other non-text contents are ignored). To
standardize the texts, we remove from the text 670
commonly used stop words, non-alphabet symbols
including punctuation marks, and strings consisting
of only symbols and digits. We also discard infre-
quent words from our dataset: for each word in a
post?s main entry, we kept it only if it appears at
least one more time in some main entry. We ap-
ply the same word pruning to the comment section
as well. The corpus size and the vocabulary size of
the five datasets are listed in Table 1. In addition,
each user?s handle is replaced with a unique inte-
ger. The dataset is available for download at http:
//www.ark.cs.cmu.edu/blog-data.
3.2 Qualitative Properties of Blogs
We believe that readers? reactions to blog posts are
an integral part of blogging activity. Often com-
ments are much more substantial and informative
than the post. While circumspective articles limit
themselves to allusions or oblique references, read-
ers? comments may point to heart of the matter more
washingtonmonthly.com and MY for Think Progress
athttp://yglesias.thinkprogress.org.
boldly. Opinions are expressed more blatantly in
comments. Comments may help a human (or au-
tomated) reader to understand the post more clearly
when the main text is too terse, stylized, or technical.
Although the main entry and its comments are
certainly related and at least partially address similar
topics, they are markedly different in several ways.
First of all, their vocabulary is noticeably different.
Comments are more casual, conversational, and full
of jargon. They are less carefully edited and there-
fore contain more misspellings and typographical er-
rors. There is more diversity among comments than
within the single-author post, both in style of writing
and in what commenters like to talk about. Depend-
ing on the subjects covered in a blog post, different
types of people are inspired to respond. We believe
that analyzing a piece of text based on the reaction
it causes among those who read it is a fascinating
problem for NLP.
Blog sites are also quite distinctive from each
other. Their language, discussion topics, and col-
lective political orientations vary greatly. Their vol-
umes also vary; multi-author sites (such as DK, RS)
may consistently produce over twenty posts per day,
while single-author sites (such asMY, CB) may have
a day with only one post. Single author sites also
tend to have a much smaller vocabulary and range
of interests. The sites are also culturally different
in commenting styles; some sites are full of short
interjections, while others have longer, more analyt-
ical comments. On some sites, users appear to be
479
??
z z?
u
?
w
?
D
N M
1
?
?
z z?
w? u
? ? ?
w
?
D
MN
1
Figure 1: Left:
LinkLDA (Erosheva
et al, 2004), with
variables reassigned.
Right:
CommentLDA. In
training, w, u, and
(in CommentLDA)
w? are observed. D is
the number of blog
posts, and N and M
are the word counts
in the post and the all
of its comments,
respectively. Here we
?count by verbosity.?
close-knit, while others have high turnover.
In the next section, we describe how we apply
topic models to political blogs, and how these prob-
abilistic models can put to use to make predictions.
4 Generative Models
The first model we consider is LinkLDA, which is
analogous to the model of Erosheva et al (2004),
though the variables are given different meanings
here.7 The graphical model is depicted in Fig. 1
(left). As in LDA and its many variants, this model
postulates a set of latent ?topic? variables, where
each topic k corresponds to a multinomial distribu-
tion ?k over the vocabulary. In addition to gener-
ating the words in the post from its topic mixture,
this model also generates a bag of users who respond
to the post, according to a distribution ? over users
given topics. In this model, the topic distribution ?
is all that determines the text content of the post and
which users will respond to the post.
LinkLDA models which users are likely to re-
spond to a post, but it does not model what they
will write. Our new model, CommentLDA, gen-
erates the contents of the comments (see Fig. 1,
right). In order to capture the differences in lan-
guage style between posts and comments, however,
we use a different conditional distribution over com-
ment words given topics, ??. The post text, comment
text, and commenter distributions are all interdepen-
dent through the (latent) topic distribution ?, and a
topic k is defined by:
7Instead of blog commenters, they modeled citations.
? A multinomial distribution ?k over post words;
? A multinomial distribution ??k over comment
words; and
? A multinomial distribution ?k over blog com-
menters who might react to posts on the topic.
Formally, LinkLDA and CommentLDA generate
blog data as follows: For each blog post (1 to D):
1. Choose a distribution ? over topics according
to Dirichlet distribution ?.
2. For i from 1 to Ni (the length of the post):
(a) Choose a topic zi according to ?.
(b) Choose a word wi according to the topic?s
post word distribution ?zi .
3. For j from 1 to Mi (the length of the comments
on the post, in words):
(a) Choose a topic z?j .
(b) Choose an author uj from the topic?s com-
menter distribution ?z?j .
(c) (CommentLDA only) Choose a word w?j
according to the topic?s comment word
distribution ??z?j .
4.1 Variations on Counting Users
As described, CommentLDA associates each com-
ment word token with an independent author. In
both LinkLDA and CommentLDA, this ?counting
by verbosity? will force ? to give higher probabil-
ity to users who write longer comments with more
480
words. We consider two alternative ways to count
comments, applicable to both LinkLDA and Com-
mentLDA. These both involve a change to step 3 in
the generative process.
Counting by response (replaces step 3): For j from
1 to Ui (the number of users who respond to the
post): (a) and (b) as before. (c) (CommentLDA only)
For ` from 1 to `i,j (the number of words in uj?s
comments), choose w?` according to the topic?s com-
ment word distribution ??z?j . This model collapses allcomments by a user into a single bag of words on a
single topic.8
Counting by comments (replaces step 3): For j
from 1 to Ci (the number of comments on the post):
(a) and (b) as before. (c) (CommentLDA only) For `
from 1 to `i,j (the number of words in comment j),
choose w?` according to the topic?s comment word
distribution ??z?j . Intuitively, each comment has atopic, a user, and a bag of words.
The three variations?counting users by ver-
bosity, response, or comments?correspond to dif-
ferent ways of thinking about topics in political blog
discourse. Counting by verbosity will let garrulous
users define the topics. Counting by response is
more democratic, letting every user who responds
to a blog post get an equal vote in determining what
the post is about, no matter how much that user says.
Counting by comments gives more say to users who
engage in the conversation repeatedly.
4.2 Implementation
We train our model using empirical Bayesian esti-
mation. Specifically, we fix ? = 0.1, and we learn
the values of word distributions ? and ?? and user
distribution ? by maximizing the likelihood of the
training data:
p(w,w?,u | ?, ?, ??, ?) (1)
(Obviously, ?? is not present in the LinkLDA mod-
els.) This requires an inference step that marginal-
izes out the latent variables, ?, z, and z?, for which
we use Gibbs sampling as implemented by the Hier-
archical Bayes Compiler (Daume?, 2007). The Gibbs
8The counting-by-response models are deficient, since they
assume each user will only be chosen once per blog post, though
they permit the same user to be chosen repeatedly.
sampling inference algorithm for LDA was first in-
troduced by Griffiths and Steyvers (2004) and has
since been used widely.
5 Empirical Evaluation
We adopt a typical NLP ?train-and-test? strategy that
learns the model parameters on a training dataset
consisting of a collection of blog posts and their
commenters and comments, then considers an un-
seen test dataset from a later time period. Many
kinds of predictions might be made about the test
set and then evaluated against the true comment re-
sponse. For example, the likelihood of a user to
comment on the post, given knowledge of ? can be
estimated as:9
p(u | wN1 , ?, ?) =
K?
z=1
p(u | z, ?)p(z | wN1 , ?)
=
K?
z=1
?z,u ? ?z (2)
The latter is in a sense a ?guessing game,? a pre-
diction on who is going to comment on a new blog
post. A similar task was used by Nallapati and Co-
hen (2008) for assessing the performance of Link-
PLSA-LDA: they predicted the presence or absence
of citation links between documents. We report the
performance on this prediction task using our six
blog topic models (LinkLDA and CommentLDA,
with three counting variations each).
Our aim is to explore and compare the effective-
ness of the different models in discovering topics
that are useful for a practical task. We also give a
qualitative analysis of topics learned.
5.1 Comment Prediction
For each political blog, we trained the three varia-
tions each of LinkLDA and CommentLDA. Model
parameters ?, ?, and (in CommentLDA) ?? were
learned by maximizing likelihood, with Gibbs sam-
pling for inference, as described in ?4.2. The num-
ber of topics K was fixed at 15.
A simple baseline method makes a post-
independent prediction that ranks users by their
comment frequency. Since blogs often have a ?core
constituency? of users who post frequently, this is a
9Another approach would attempt to integrate out ?.
481
n=5 n=10 n=20 n=30 oracle
MY
Freq. 23.93 18.68 14.20 11.65 13.18
NB 25.13 19.28 14.20 11.63 13.54
Link-v 20.10 14.04 11.17 9.23 11.32
Link-r 26.77 18.63 14.64 12.47 14.03
Link-c 25.13 18.85 14.61 11.91 13.84
Com-v 22.84 17.15 12.75 10.69 12.77
Com-r 27.54 20.54 14.61 12.45 14.35
Com-c 22.40 18.50 14.83 12.56 14.20
Max 94.75 89.89 73.63 58.76 92.60
RWN
Freq. 32.56 30.17 22.61 19.7 27.19
NB 25.63 34.86 27.61 22.03 18.28
Link-v 28.14 21.06 17.34 14.51 19.81
Link-r 32.92 29.29 22.61 18.96 26.32
Link-c 32.56 27.43 21.15 17.43 25.09
Com-v 29.02 24.07 19.07 16.04 22.71
Com-r 36.10 29.64 23.8 19.26 25.97
Com-c 32.03 27.43 19.82 16.25 23.88
Max 90.97 76.46 52.56 37.05 96.16
CB
Freq. 33.38 28.84 24.17 20.99 21.63
NB 36.36 31.15 25.08 21.40 23.22
Link-v 32.06 26.11 19.79 17.43 18.31
Link-r 37.02 31.65 24.62 20.85 22.34
Link-c 36.03 32.06 25.28 21.10 23.44
Com-v 32.39 26.36 20.95 18.26 19.85
Com-r 35.53 29.33 24.33 20.22 22.02
Com-c 33.71 29.25 23.80 19.86 21.68
Max 99.66 98.34 88.88 72.53 95.58
RS
Freq. 25.45 16.75 11.42 9.62 17.15
NB 22.07 16.01 11.60 9.76 16.50
Link-v 14.63 11.9 9.13 7.76 11.38
Link-r 25.19 16.92 12.14 9.82 17.98
Link-c 24.50 16.45 11.49 9.32 16.76
Com-v 14.97 10.51 8.46 7.37 11.3 0
Com-r 15.93 11.42 8.37 6.89 10.97
Com-c 17.57 12.46 8.85 7.34 12.14
Max 80.77 62.98 40.95 29.03 91.86
DK
Freq. 24.66 19.08 15.33 13.34 9.64
NB 35.00 27.33 22.25 19.45 13.97
Link-v 20.58 19.79 15.83 13.88 10.35
Link-r 33.83 27.29 21.39 19.09 13.44
Link-c 28.66 22.16 18.33 16.79 12.60
Com-v 22.16 18.00 16.54 14.45 10.92
Com-r 33.08 25.66 20.66 18.29 12.74
Com-c 26.08 20.91 17.47 15.59 11.82
Max 100.00 100.00 100.00 99.09 98.62
Table 2: Comment prediction results on 5 blogs. See text.
strong baseline. We also compared to a Na??ve Bayes
classifier (with word counts in the post?s main en-
try as features). To perform the prediction task with
our models, we took the following steps. First, we
removed the comment section (both the words and
the authorship information) from the test data set.
Then, we ran a Gibbs sampler with the partial data,
fixing the model parameters to their learned values
and the blog post words to their observed values.
This gives a posterior topic mixture for each post (?
in the above equations).10 We then computed each
user?s comment prediction score for each post as in
Eq. 2. Users are ordered by their posterior probabil-
ities. Note that these posteriors have different mean-
ings for different variations:
? When counting by verbosity, the value is the prob-
ability that the next (or any) comment word will
be generated by the user, given the blog post.
? When counting by response, the value is the prob-
ability that the user will respond at all, given the
blog post. (Intuitively, this approach best matches
the task at hand.)
? When counting by comments, the value is the
probability that the next (or any) comment will be
generated by the user, given the blog post.
We compare our commenter ranking-by-
likelihood with the actual commenters in the test
set. We report in Tab. 2 the precision (macro-
averaged across posts) of our predictions at various
cut-offs (n). The oracle column is the precision
where it is equal to the recall, equivalent to the
situation when the true number of commenters
is known. (The performance of random guessing
is well below 1% for all sites at cut-off points
shown.) ?Freq.? and ?NB? refer to our baseline
methods. ?Link? refers to LinkLDA and ?Com? to
CommentLDA. The suffixes denote the counting
methods: verbosity (?-v?), response (?-r?), and
comments (?-c?). Recall that we considered only
the comments by the users seen at least once in the
training set, so perfect precision, as well as recall, is
impossible when new users comment on a post; the
Max row shows the maximum performance possible
given the set of commenters recognizable from the
training data.
10For a few cases we checked the stability of the sampler and
found results varied by less than 1% precision across ten runs.
482
Our results suggest that, if asked to guess 5 peo-
ple who would comment on a new post given some
site history, we will get 25?37% of them right, de-
pending on the site, given the content of a new post.
We achieved some improvement over both the
baseline and Na??ve Bayes for some cut-offs on three
of the five sites, though the gains were very small
for and RS and CB. LinkLDA usually works slightly
better than CommentLDA, except for MY, where
CommentLDA is stronger, and RS, where Com-
mentLDA is extremely poor. Differences in com-
menting style are likely to blame: MY has relatively
long comments in comparison to RS, as well as DK.
MY is the only site where CommentLDA variations
consistently outperformed LinkLDA variations, as
well as Na??ve Bayes classifiers. This suggests that
sites with more terse comments may be too sparse
to support a rich model like CommentLDA.
In general, counting by response works best,
though counting by comments is a close rival in
some cases. We observe that counting by response
tends to help LinkLDA, which is ignorant of the
word contents of the comment, more than it helps
CommentLDA. Varying the counting method can
bring as much as 10% performance gain.
Each of the models we have tested makes differ-
ent assumptions about the behavior of commenters.
Our results suggest that commenters on different
sites behave differently, so that the same modeling
assumptions cannot be made universally. In future
work, we hope to permit blog-specific properties
to be automatically discovered during learning, so
that, for example, the comment words can be ex-
ploited when they are helpful but assumed indepen-
dent when they are not. Of course, improved per-
formance might also be obtained with more topics,
richer priors over topic distributions, or models that
take into account other cues, such as the time of the
post, pages it links to, etc. It is also possible that bet-
ter performance will come from more sophisticated
supervised models that do not use topics.
5.2 Qualitative Evaluation
Aside from prediction tasks such as above, the
model parameters by themselves can be informative.
? defines which words are likely to occur in the post
body for a given topic. ?? tells which words are
likely to appear in the collective response to a partic-
ular topic. Similarity or divergence of the two dis-
tributions can tell us about differences in language
used by bloggers and their readers. ? expresses
users? topic preferences. A pair or group of par-
ticipants may be seen as ?like-minded? if they have
similar topic preferences (perhaps useful in collabo-
rative filtering).
Following previous work on LDA and its exten-
sions, we show words most strongly associated with
a few topics, arguing that some coherent clusters
have been discovered. Table 3 shows topics discov-
ered in MY using CommentLDA (counting by com-
ments). This is the blog site where our models most
consistently outperformed the Na??ve Bayes classi-
fiers and LinkLDA, therefore we believe the model
was a good fit for this dataset.
Since the site is concentrated on American pol-
itics, many of the topics look alike. Table 3 shows
the most probable words in the posts, comments, and
both together for five hand-picked topics that were
relatively transparent. The probabilistic scores of
those words are computed with the scoring method
suggested by Blei and Lafferty (in press).
The model clustered words into topics pertain-
ing to religion and domestic policy (first and last
topics in Table 3) quite reasonably. Some of the
religion-related words make sense in light of cur-
rent affairs.11 Some words in the comment sec-
tion are slightly off-topic from the issue of religion,
such as dawkins12 or wright,13 but are relevant in
the context of real-world events. Notice those words
rank highly only in the comment section, showing
differences between discussion in the post and the
comments. This is also noticeable, for example, in
the ?primary? topic (second in Table 3), where the
Republican primary receives more discussion in the
main post, and in the ?Iraq war? and ?energy? top-
ics, where bloggers discuss strategy and commenters
11Mitt Romney was a candidate for the Republican nomi-
nation in 2008 presidential election. He is a member of The
Church of Jesus Christ of Latter-Day Saints. Another candi-
date, Mike Huckabee, is an ordained Southern Baptist minister.
Moktada al-Sadr is an Iraqi theologian and political activist, and
John Hagee is an influential televangelist.
12Richard Dawkins is a well known evolutionary biologist
who is a vocal critic of intelligent design.
13We believe this is a reference to Rev. Jeremiah Wright of
Trinity United Church of Christ, whose inflammatory rhetoric
was negatively associated with then-candidate Barack Obama.
483
religion; in both: people, just, american, church, believe, god, black, jesus, mormon, faith, jews, right, say,
mormons, religious, point
in posts: romney, huckabee, muslim, political, hagee, cabinet, mitt, consider, true, anti, problem,
course, views, life, real, speech, moral, answer, jobs, difference, muslims, hardly, going,
christianity
in comments: religion, think, know, really, christian, obama, white, wright, way, said, good, world, science,
time, dawkins, human, man, things, fact, years, mean, atheists, blacks, christians
primary; in both: obama, clinton, mccain, race, win, iowa, delegates, going, people, state, nomination, primary,
hillary, election, polls, party, states, voters, campaign, michigan, just
in posts: huckabee, wins, romney, got, percent, lead, barack, point, majority, ohio, big, victory, strong,
pretty, winning, support, primaries, south, rules
in comments: vote, think, superdelegates, democratic, candidate, pledged, delegate, independents, votes,
white, democrats, really, way, caucuses, edwards, florida, supporters, wisconsin, count
Iraq war; in
both:
american, iran, just, iraq, people, support, point, country, nuclear, world, power, military,
really, government, war, army, right, iraqi, think
in posts: kind, united, forces, international, presence, political, states, foreign, countries, role, need,
making, course, problem, shiite, john, understand, level, idea, security, main
in comments: israel, sadr, bush, state, way, oil, years, time, going, good, weapons, saddam, know, maliki,
want, say, policy, fact, said, shia, troops
energy; in both: people, just, tax, carbon, think, high, transit, need, live, going, want, problem, way, market,
money, income, cost, density
in posts: idea, public, pretty, course, economic, plan, making, climate, spending, economy, reduce,
change, increase, policy, things, stimulus, cuts, low, fi nancial, housing, bad, real
in comments: taxes, fuel, years, time, rail, oil, cars, car, energy, good, really, lot, point, better, prices, pay,
city, know, government, price, work, technology
domestic policy;
in both:
people, public, health, care, insurance, college, schools, education, higher, children, think,
poor, really, just, kids, want, school, going, better
in posts: different, things, point, fact, social, work, large, article, getting, inequality, matt, simply,
percent, tend, hard, increase, huge, costs, course, policy, happen
in comments: students, universal, high, good, way, income, money, government, class, problem, pay, amer-
icans, private, plan, american, country, immigrants, time, know, taxes, cost
Table 3: The most probable words for some CommentLDA topics (MY).
focus on the tangible (oil, taxes, prices, weapons).
While our topic-modeling approach achieves
mixed results on the prediction task, we believe it
holds promise as a way to understand and summa-
rize the data. Without CommentLDA, we would not
be able to easily see the differences noted above in
blogger and commenter language. In future work,
we plan to explore models with weaker indepen-
dence assumptions among users, among blog posts
over time, and even across blogs. This line of re-
search will permit a more nuanced understanding
of language in the blogosphere and in political dis-
course more generally.
6 Conclusion
In this paper we applied several probabilistic topic
models to discourse within political blogs. We in-
troduced a novel comment prediction task to assess
these models in an objective evaluation with possi-
ble practical applications. The results show that pre-
dicting political discourse behavior is challenging,
in part because of considerable variation in user be-
havior across different blog sites. Our results show
that using topic modeling, we can begin to make rea-
sonable predictions as well as qualitative discoveries
about language in blogs.
Acknowledgments
This research was supported by a gift from Microsoft
Research and NSF IIS-0836431. The authors appreciate
helpful comments from the anonymous reviewers, Ja-Hui
Chang, Hal Daume?, and Ramesh Nallapati. We thank
Shay Cohen for his help with inference algorithms and
the members of the ARK group for reviewing this paper.
484
References
L. Adamic and N. Glance. 2005. The political blogo-
sphere and the 2004 U.S. election: Divided they blog.
In Proceedings of the 2nd Annual Workshop on the We-
blogging Ecosystem: Aggregation, Analysis and Dy-
namics.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in
Informaion Retrieval.
D. Blei and J. Lafferty. In press. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Franci.
D. Blei and J. McAuliffe. 2008. Supervised topic mod-
els. In Advances in Neural Information Processing
Systems 20.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
ACL-08: HLT.
D. Cohn and T. Hofmann. 2001. The missing link?a
probabilistic model of document content and hyper-
text connectivity. In Neural Information Processing
Systems 13.
H. Daume?. 2007. HBC: Hierarchical Bayes compiler.
http://www.cs.utah.edu/?hal/HBC.
M. Dredze, H. M. Wallach, D. Puller, and F. Pereira.
2008. Generating summary keywords for emails us-
ing topics. In Proceedings of the 13th International
Conference on Intelligent User Interfaces.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences, pages
5220?5227, April.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101 Suppl. 1:5228?5235, April.
W.-H. Lin, E. Xing, and A. Hauptmann. 2008. A joint
topic and perspective model for ideological discourse.
In Proceedings of 2008 European Conference on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Databases.
R. Malouf and T. Mullen. 2007. Graph-based user clas-
sification for informal online political discourse. In
Proceedings of the 1st Workshop on Information Cred-
ibility on the Web.
A. McCallum. 1999. Multi-label text classification with
a mixture model trained by EM. In AAAI Workshop on
Text Learning.
T. Mullen and R. Malouf. 2006. A preliminary investi-
gation into sentiment analysis of informal political dis-
course. In Proceedings of AAAI-2006 Spring Sympo-
sium on Computational Approaches to Analyzing We-
blogs.
R. Nallapati and W. Cohen. 2008. Link-PLSA-LDA: A
new unsupervised model for topics and influence of
blogs. In Proceedings of the 2nd International Con-
ference on Weblogs and Social Media.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth.
2004. The author-topic model for authors and docu-
ments. In Proceedings of the 20th Conference on Un-
certainty in Artificial Intelligence.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent Semantic
Analysis. Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Grif-
fiths. 2004. Probabilistic author-topic models for in-
formation discovery. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
Proceedings of ACL-08: HLT.
H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen.
2007. An LDA-based community structure discovery
approach for large-scale social networks. In Proceed-
ings of the IEEE International Conference on Intelli-
gence and Security Informatics.
485
Proceedings of ACL-08: HLT, pages 245?253,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploiting Feature Hierarchy for Transfer Learning in Named Entity
Recognition
Andrew Arnold, Ramesh Nallapati and William W. Cohen
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA
{aarnold, nmramesh, wcohen}@cs.cmu.edu
Abstract
We present a novel hierarchical prior struc-
ture for supervised transfer learning in named
entity recognition, motivated by the common
structure of feature spaces for this task across
natural language data sets. The problem of
transfer learning, where information gained in
one learning task is used to improve perfor-
mance in another related task, is an important
new area of research. In the subproblem of do-
main adaptation, a model trained over a source
domain is generalized to perform well on a re-
lated target domain, where the two domains?
data are distributed similarly, but not identi-
cally. We introduce the concept of groups
of closely-related domains, called genres, and
show how inter-genre adaptation is related to
domain adaptation. We also examine multi-
task learning, where two domains may be re-
lated, but where the concept to be learned in
each case is distinct. We show that our prior
conveys useful information across domains,
genres and tasks, while remaining robust to
spurious signals not related to the target do-
main and concept. We further show that our
model generalizes a class of similar hierarchi-
cal priors, smoothed to varying degrees, and
lay the groundwork for future exploration in
this area.
1 Introduction
1.1 Problem definition
Consider the task of named entity recognition
(NER). Specifically, you are given a corpus of news
articles in which all tokens have been labeled as ei-
ther belonging to personal name mentions or not.
The standard supervised machine learning problem
is to learn a classifier over this training data that will
successfully label unseen test data drawn from the
same distribution as the training data, where ?same
distribution? could mean anything from having the
train and test articles written by the same author to
having them written in the same language. Having
successfully trained a named entity classifier on this
news data, now consider the problem of learning to
classify tokens as names in e-mail data. An intuitive
solution might be to simply retrain the classifier, de
novo, on the e-mail data. Practically, however, large,
labeled datasets are often expensive to build and this
solution would not scale across a large number of
different datasets.
Clearly the problems of identifying names in
news articles and e-mails are closely related, and
learning to do well on one should help your per-
formance on the other. At the same time, however,
there are serious differences between the two prob-
lems that need to be addressed. For instance, cap-
italization, which will certainly be a useful feature
in the news problem, may prove less informative in
the e-mail data since the rules of capitalization are
followed less strictly in that domain.
These are the problems we address in this paper.
In particular, we develop a novel prior for named
entity recognition that exploits the hierarchical fea-
ture space often found in natural language domains
(?1.2) and allows for the transfer of information
from labeled datasets in other domains (?1.3). ?2
introduces the maximum entropy (maxent) and con-
ditional random field (CRF) learning techniques em-
ployed, along with specifications for the design and
training of our hierarchical prior. Finally, in ?3 we
present an empirical investigation of our prior?s per-
formance against a number of baselines, demonstrat-
ing both its effectiveness and robustness.
1.2 Hierarchical feature trees
In many NER problems, features are often con-
structed as a series of transformations of the input
training data, performed in sequence. Thus, if our
task is to identify tokens as either being (O)utside or
(I)nside person names, and we are given the labeled
245
sample training sentence:
O O O O O I
Give the book to Professor Caldwell
(1)
one such useful feature might be: Is the token one
slot to the left of the current token Professor?
We can represent this symbolically as L.1.Professor
where we describe the whole space of useful features
of this form as: {direction = (L)eft, (C)urrent,
(R)ight}.{distance = 1, 2, 3, ...}.{value = Pro-
fessor, book, ...}. We can conceptualize this struc-
ture as a tree, where each slot in the symbolic name
of a feature is a branch and each period between slots
represents another level, going from root to leaf as
read left to right. Thus a subsection of the entire fea-
ture tree for the token Caldwell could be drawn
as in Figure 1 (zoomed in on the section of the tree
where the L.1.Professor feature resides).
direction
L
C
R
distance
1 2 ...
... ...
value
Professor
book
...
... ...
true false ...
Figure 1: Graphical representation of a hierarchical fea-
ture tree for token Caldwell in example Sentence 1.
Representing feature spaces with this kind of tree,
besides often coinciding with the explicit language
used by common natural language toolkits (Cohen,
2004), has the added benefit of allowing a model to
easily back-off, or smooth, to decreasing levels of
specificity. For example, the leaf level of the fea-
ture tree for our sample Sentence 1 tells us that the
word Professor is important, with respect to la-
beling person names, when located one slot to the
left of the current word being classified. This may
be useful in the context of an academic corpus, but
might be less useful in a medical domain where the
word Professor occurs less often. Instead, we
might want to learn the related feature L.1.Dr. In
fact, it might be useful to generalize across multiple
domains the fact that the word immediately preced-
ing the current word is often important with respect
LeftToken.*
LeftToken.IsWord.*
LeftToken.IsWord.IsTitle.*
LeftToken.IsWord.IsTitle.equals.*
LeftToken.IsWord.IsTitle.equals.mr
Table 1: A few examples of the feature hierarchy
to the named entity status of the current word. This
is easily accomplished by backing up one level from
a leaf in the tree structure to its parent, to represent
a class of features such as L.1.*. It has been shown
empirically that, while the significance of particular
features might vary between domains and tasks, cer-
tain generalized classes of features retain their im-
portance across domains (Minkov et al, 2005). By
backing-off in this way, we can use the feature hier-
archy as a prior for transferring beliefs about the sig-
nificance of entire classes of features across domains
and tasks. Some examples illustrating this idea are
shown in table 1.
1.3 Transfer learning
When only the type of data being examined is al-
lowed to vary (from news articles to e-mails, for
example), the problem is called domain adapta-
tion (Daume? III and Marcu, 2006). When the task
being learned varies (say, from identifying person
names to identifying protein names), the problem
is called multi-task learning (Caruana, 1997). Both
of these are considered specific types of the over-
arching transfer learning problem, and both seem
to require a way of altering the classifier learned
on the first problem (called the source domain, or
source task) to fit the specifics of the second prob-
lem (called the target domain, or target task).
More formally, given an example x and a class
label y, the standard statistical classification task
is to assign a probability, p(y|x), to x of belong-
ing to class y. In the binary classification case the
labels are Y ? {0, 1}. In the case we examine,
each example xi is represented as a vector of bi-
nary features (f1(xi), ? ? ? , fF (xi)) where F is the
number of features. The data consists of two dis-
joint subsets: the training set (Xtrain, Ytrain) =
{(x1, y1) ? ? ? , (xN , yN )}, available to the model for
its training and the test set Xtest = (x1, ? ? ? , xM ),
upon which we want to use our trained classifier to
make predictions.
246
In the paradigm of inductive learning,
(Xtrain, Ytrain) are known, while both Xtest and
Ytest are completely hidden during training time. In
this cases Xtest and Xtrain are both assumed to have
been drawn from the same distribution, D. In the
setting of transfer learning, however, we would like
to apply our trained classifier to examples drawn
from a distribution different from the one upon
which it was trained. We therefore assume there
are two different distributions, Dsource and Dtarget,
from which data may be drawn. Given this notation
we can then precisely state the transfer learning
problem as trying to assign labels Y targettest to test
data Xtargettest drawn from Dtarget, given training
data (Xsourcetrain , Y sourcetrain ) drawn from Dsource.
In this paper we focus on two subproblems of
transfer learning:
? domain adaptation, where we assume Y (the set
of possible labels) is the same for both Dsource
and Dtarget, while Dsource and Dtarget them-
selves are allowed to vary between domains.
? multi-task learning (Ando and Zhang, 2005;
Caruana, 1997; Sutton and McCallum, 2005;
Zhang et al, 2005) in which the task (and label
set) is allowed to vary from source to target.
Domain adaptation can be further distinguished by
the degree of relatedness between the source and tar-
get domains. For example, in this work we group
data collected in the same medium (e.g., all anno-
tated e-mails or all annotated news articles) as be-
longing to the same genre. Although the specific
boundary between domain and genre for a particu-
lar set of data is often subjective, it is nevertheless a
useful distinction to draw.
One common way of addressing the transfer
learning problem is to use a prior which, in conjunc-
tion with a probabilistic model, allows one to spec-
ify a priori beliefs about a distribution, thus bias-
ing the results a learning algorithm would have pro-
duced had it only been allowed to see the training
data (Raina et al, 2006). In the example from ?1.1,
our belief that capitalization is less strict in e-mails
than in news articles could be encoded in a prior that
biased the importance of the capitalization
feature to be lower for e-mails than news articles.
In the next section we address the problem of how
to come up with a suitable prior for transfer learning
across named entity recognition problems.
2 Models considered
2.1 Basic Conditional Random Fields
In this work, we will base our work on Condi-
tional Random Fields (CRF?s) (Lafferty et al, 2001),
which are now one of the most preferred sequential
models for many natural language processing tasks.
The parametric form of the CRF for a sentence of
length n is given as follows:
p?(Y = y|x) =
1
Z(x) exp(
n
?
i=1
F
?
j=1
fj(x, yi)?j)
(2)
where Z(x) is the normalization term. CRF learns a
model consisting of a set of weights ? = {?1...?F }
over the features so as to maximize the conditional
likelihood of the training data, p(Ytrain|Xtrain),
given the model p?.
2.2 CRF with Gaussian priors
To avoid overfitting the training data, these ??s are
often further constrained by the use of a Gaussian
prior (Chen and Rosenfeld, 1999) with diagonal co-
variance, N (?, ?2), which tries to maximize:
argmax
?
N
?
k=1
(
log p?(yk|xk)
)
? ?
F
?
j
(?j ? ?j)2
2?2j
where ? > 0 is a parameter controlling the amount
of regularization, and N is the number of sentences
in the training set.
2.3 Source trained priors
One recently proposed method (Chelba and Acero,
2004) for transfer learning in Maximum Entropy
models 1 involves modifying the ??s of this Gaussian
prior. First a model of the source domain, ?source,
is learned by training on {Xsourcetrain , Y sourcetrain }. Then a
model of the target domain is trained over a limited
set of labeled target data
{
Xtargettrain , Y
target
train
}
, but in-
stead of regularizing this ?target to be near zero (i.e.
setting ? = 0), ?target is instead regularized to-
wards the previously learned source values ?source
(by setting ? = ?source, while ?2 remains 1) and
thus minimizing (?target ? ?source)2.
1Maximum Entropy models are special cases of CRFs that
use the I.I.D. assumption. The method under discussion can
also be extended to CRF directly.
247
Note that, since this model requires Y targettrain in or-
der to learn ?target, it, in effect, requires two distinct
labeled training datasets: one on which to train the
prior, and another on which to learn the model?s fi-
nal weights (which we call tuning), using the previ-
ously trained prior for regularization. If we are un-
able to find a match between features in the training
and tuning datasets (for instance, if a word appears
in the tuning corpus but not the training), we back-
off to a standard N (0, 1) prior for that feature.
3
y
x i
i
(1)
(1)
(1)M
w (1)1
y
x i
i
(
M
y
x i
i
(
M
(2)
2)
(2)
(3)
3)
(3)
w w (1) w (1) w1 w w w1 w(1)2 3 4 (2) (2) (2)2 3 (3) (3)2
z z
z
1 2
Figure 2: Graphical representation of the hierarchical
transfer model.
2.4 New model: Hierarchical prior model
In this section, we will present a new model that
learns simultaneously from multiple domains, by
taking advantage of our feature hierarchy.
We will assume that there are D domains on
which we are learning simultaneously. Let there be
Md training data in each domain d. For our experi-
ments with non-identically distributed, independent
data, we use conditional random fields (cf. ?2.1).
However, this model can be extended to any dis-
criminative probabilistic model such as the MaxEnt
model. Let ?(d) = (?(d)1 , ? ? ? , ?
(d)
Fd ) be the param-
eters of the discriminative model in the domain d
where Fd represents the number of features in the
domain d.
Further, we will also assume that the features of
different domains share a common hierarchy repre-
sented by a tree T , whose leaf nodes are the features
themselves (cf. Figure 1). The model parameters
?(d), then, form the parameters of the leaves of this
hierarchy. Each non-leaf node n ? non-leaf(T ) of
the tree is also associated with a hyper-parameter zn.
Note that since the hierarchy is a tree, each node n
has only one parent, represented by pa(n). Simi-
larly, we represent the set of children nodes of a node
n as ch(n).
The entire graphical model for an example con-
sisting of three domains is shown in Figure 2.
The conditional likelihood of the entire training
data (y,x) = {(y(d)1 ,x
(d)
1 ), ? ? ? , (y
(d)
Md ,x
(d)
Md)}
D
d=1 is
given by:
P (y|x,w, z) =
{ D
?
d=1
Md
?
k=1
P (y(d)k |x
(d)
k ,?(d))
}
?
?
?
?
D
?
d=1
Fd
?
f=1
N (?(d)f |zpa(f (d)), 1)
?
?
?
?
?
?
?
?
n?Tnonleaf
N (zn|zpa(n), 1)
?
?
?
(3)
where the terms in the first line of eq. (3) represent
the likelihood of data in each domain given their cor-
responding model parameters, the second line repre-
sents the likelihood of each model parameter in each
domain given the hyper-parameter of its parent in the
tree hierarchy of features and the last term goes over
the entire tree T except the leaf nodes. Note that in
the last term, the hyper-parameters are shared across
the domains, so there is no product over d.
We perform a MAP estimation for each model pa-
rameter as well as the hyper-parameters. Accord-
ingly, the estimates are given as follows:
?(d)f =
Md
?
i=1
?
??(d)f
(
logP (ydi |x(d)i ,?(d))
)
+ zpa(f (d))
zn =
zpa(n) +
?
i?ch(n)(?|z)i
1 + |ch(n)| (4)
where we used the notation (?|z)i because node i,
the child node of n, could be a parameter node or
a hyper-parameter node depending on the position
of the node n in the hierarchy. Essentially, in this
model, the weights of the leaf nodes (model param-
eters) depend on the log-likelihood as well as the
prior weight of its parent. Additionally, the weight
248
of each hyper-parameter node in the tree is com-
puted as the average of all its children nodes and its
parent, resulting in a smoothing effect, both up and
down the tree.
2.5 An approximate Hierarchical prior model
The Hierarchical prior model is a theoretically well
founded model for transfer learning through feature
heirarchy. However, our preliminary experiments
indicated that its performance on real-life data sets is
not as good as expected. Although a more thorough
investigation needs to be carried out, our analysis in-
dicates that the main reason for this phenomenon is
over-smoothing. In other words, by letting the infor-
mation propagate from the leaf nodes in the hierar-
chy all the way to the root node, the model loses its
ability to discriminate between its features.
As a solution to this problem, we propose an
approximate version of this model that weds ideas
from the exact heirarchical prior model and the
Chelba model.
As with the Chelba prior method in ?2.3, this ap-
proximate hierarchical method also requires two dis-
tinct data sets, one for training the prior and another
for tuning the final weights. Unlike Chelba, we
smooth the weights of the priors using the feature-
tree hierarchy presented in ?1.1, like the hierarchical
prior model.
For smoothing of each feature weight, we chose to
back-off in the tree as little as possible until we had a
large enough sample of prior data (measured as M ,
the number of subtrees below the current node) on
which to form a reliable estimate of the mean and
variance of each feature or class of features. For
example, if the tuning data set is as in Sentence
1, but the prior contains no instances of the word
Professor, then we would back-off and compute
the prior mean and variance on the next higher level
in the tree. Thus the prior for L.1.Professor would
be N (mean(L.1.*), variance(L.1.*)), where mean()
and variance() of L.1.* are the sample mean and
variance of all the features in the prior dataset that
match the pattern L.1.* ? or, put another way, all the
siblings of L.1.Professor in the feature tree. If fewer
than M such siblings exist, we continue backing-off,
up the tree, until an ancestor with sufficient descen-
dants is found. A detailed description of the approx-
imate hierarchical algorithm is shown in table 2.
Input: Dsource = (Xsourcetrain , Y sourcetrain )
Dtarget = (Xtargettrain , Y
target
train );
Feature sets Fsource, F target;
Feature HierarchiesHsource,Htarget
Minimum membership size M
Train CRF using Dsource to obtain
feature weights ?source
For each feature f ? F target
Initialize: node n = f
While (n /? Hsource
or |Leaves(Hsource(n))| ?M)
and n 6= root(Htarget)
n? Pa(Htarget(n))
Compute ?f and ?f using the sample
{?sourcei | i ? Leaves(Hsource(n))}
Train Gaussian prior CRF using Dtarget as data
and {?f} and {?f} as Gaussian prior parameters.
Output:Parameters of the new CRF ?target.
Table 2: Algorithm for approximate hierarchical prior:
Pa(Hsource(n)) is the parent of node n in feature hierar-
chy Hsource; |Leaves(Hsource(n))| indicates the num-
ber of leaf nodes (basic features) under a node n in the
hierarchyHsource.
It is important to note that this smoothed tree is
an approximation of the exact model presented in
?2.4 and thus an important parameter of this method
in practice is the degree to which one chooses to
smooth up or down the tree. One of the benefits
of this model is that the semantics of the hierarchy
(how to define a feature, a parent, how and when
to back-off and up the tree, etc.) can be specified
by the user, in reference to the specific datasets and
tasks under consideration. For our experiments, the
semantics of the tree are as presented in ?1.1.
The Chelba method can be thought of as a hier-
archical prior in which no smoothing is performed
on the tree at all. Only the leaf nodes of the
prior?s feature tree are considered, and, if no match
can be found between the tuning and prior?s train-
ing datasets? features, a N (0, 1) prior is used in-
stead. However, in the new approximate hierarchical
model, even if a certain feature in the tuning dataset
does not have an analog in the training dataset, we
can always back-off until an appropriate match is
found, even to the level of the root.
Henceforth, we will use only the approximate hi-
erarchical model in our experiments and discussion.
249
Table 3: Summary of data used in experiments
Corpus Genre Task
UTexas Bio Protein
Yapex Bio Protein
MUC6 News Person
MUC7 News Person
CSPACE E-mail Person
3 Investigation
3.1 Data, domains and tasks
For our experiments, we have chosen five differ-
ent corpora (summarized in Table 3). Although
each corpus can be considered its own domain (due
to variations in annotation standards, specific task,
date of collection, etc), they can also be roughly
grouped into three different genres. These are: ab-
stracts from biological journals [UT (Bunescu et al,
2004), Yapex (Franze?n et al, 2002)]; news articles
[MUC6 (Fisher et al, 1995), MUC7 (Borthwick et
al., 1998)]; and personal e-mails [CSPACE (Kraut
et al, 2004)]. Each corpus, depending on its genre,
is labeled with one of two name-finding tasks:
? protein names in biological abstracts
? person names in news articles and e-mails
We chose this array of corpora so that we could
evaluate our hierarchical prior?s ability to generalize
across and incorporate information from a variety of
domains, genres and tasks.
In each case, each item (abstract, article or e-mail)
was tokenized and each token was hand-labeled as
either being part of a name (protein or person) or
not, respectively. We used a standard natural lan-
guage toolkit (Cohen, 2004) to compute tens of
thousands of binary features on each of these to-
kens, encoding such information as capitalization
patterns and contextual information from surround-
ing words. This toolkit produces features of the type
described in ?1.2 and thus was amenable to our hi-
erarchical prior model. In particular, we chose to
use the simplest default, out-of-the-box feature gen-
erator and purposefully did not use specifically en-
gineered features, dictionaries, or other techniques
commonly employed to boost performance on such
tasks. The goal of our experiments was to see to
what degree named entity recognition problems nat-
urally conformed to hierarchical methods, and not
just to achieve the highest performance possible.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 20 40 60 80 100
F1
Percent of target-domain data used for tuning
Intra-genre transfer performance evaluated on MUC6
(a) GAUSS: tuned on MUC6
(b) CAT: tuned on MUC6+7
(c) HIER: MUC6+7 prior, tuned on MUC6
(d) CHELBA: MUC6+7 prior, tuned on MUC6
Figure 3: Adding a relevant HIER prior helps compared
to the GAUSS baseline ((c) > (a)), while simply CAT?ing
or using CHELBA can hurt ((d) ? (b) < (a), except with
very little data), and never beats HIER ((c) > (b) ? (d)).
3.2 Experiments & results
We evaluated the performance of various transfer
learning methods on the data and tasks described
in ?3.1. Specifically, we compared our approximate
hierarchical prior model (HIER), implemented as a
CRF, against three baselines:
? GAUSS: CRF model tuned on a single domain?s
data, using a standard N (0, 1) prior
? CAT: CRF model tuned on a concatenation of
multiple domains? data, using a N (0, 1) prior
? CHELBA: CRF model tuned on one domain?s
data, using a prior trained on a different, related
domain?s data (cf. ?2.3)
We use token-level F1 as our main evaluation mea-
sure, combining precision and recall into one metric.
3.2.1 Intra-genre, same-task transfer learning
Figure 3 shows the results of an experiment in
learning to recognize person names in MUC6 news
articles. In this experiment we examined the effect
of adding extra data from a different, but related do-
main from the same genre, namely, MUC7. Line
a shows the F1 performance of a CRF model tuned
only on the target MUC6 domain (GAUSS) across a
range of tuning data sizes. Line b shows the same
experiment, but this time the CRF model has been
tuned on a dataset comprised of a simple concate-
nation of the training MUC6 data from (a), along
with a different training set from MUC7 (CAT). We
can see that adding extra data in this way, though
250
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
F1
Percent of target-domain data used for tuning
Inter-genre transfer performance evaluated on MUC6
(e) HIER: MUC6+7 prior, tuned on MUC6
(f) CAT: tuned on all domains
(g) HIER: all domains prior, tuned on MUC6
(h) CHELBA: all domains prior, tuned on MUC6
Figure 4: Transfer aware priors CHELBA and HIER ef-
fectively filter irrelevant data. Adding more irrelevant
data to the priors doesn?t hurt ((e) ? (g) ? (h)), while
simply CAT?ing it, in this case, is disastrous ((f) << (e).
the data is closely related both in domain and task,
has actually hurt the performance of our recognizer
for training sizes of moderate to large size. This is
most likely because, although the MUC6 and MUC7
datasets are closely related, they are still drawn from
different distributions and thus cannot be intermin-
gled indiscriminately. Line c shows the same com-
bination of MUC6 and MUC7, only this time the
datasets have been combined using the HIER prior.
In this case, the performance actually does improve,
both with respect to the single-dataset trained base-
line (a) and the naively trained double-dataset (b).
Finally, line d shows the results of the CHELBA
prior. Curiously, though the domains are closely re-
lated, it does more poorly than even the non-transfer
GAUSS. One possible explanation is that, although
much of the vocabulary is shared across domains,
the interpretation of the features of these words may
differ. Since CHELBA doesn?t model the hierarchy
among features like HIER, it is unable to smooth
away these discrepancies. In contrast, we see that
our HIER prior is able to successfully combine the
relevant parts of data across domains while filtering
the irrelevant, and possibly detrimental, ones. This
experiment was repeated for other sets of intra-genre
tasks, and the results are summarized in ?3.2.3.
3.2.2 Inter-genre, multi-task transfer learning
In Figure 4 we see that the properties of the hi-
erarchical prior hold even when transferring across
tasks. Here again we are trying to learn to recognize
person names in MUC6 e-mails, but this time, in-
stead of adding only other datasets similarly labeled
with person names, we are additionally adding bi-
ological corpora (UT & YAPEX), labeled not with
person names but with protein names instead, along
with the CSPACE e-mail and MUC7 news article
corpora. The robustness of our prior prevents a
model trained on all five domains (g) from degrading
away from the intra-genre, same-task baseline (e),
unlike the model trained on concatenated data (f ).
CHELBA (h) performs similarly well in this case,
perhaps because the domains are so different that al-
most none of the features match between prior and
tuning data, and thus CHELBA backs-off to a stan-
dard N (0, 1) prior.
This robustness in the face of less similarly related
data is very important since these types of transfer
methods are most useful when one possesses only
very little target domain data. In this situation, it
is often difficult to accurately estimate performance
and so one would like assurance than any transfer
method being applied will not have negative effects.
3.2.3 Comparison of HIER prior to baselines
Each scatter plot in Figure 5 shows the relative
performance of a baseline method against HIER.
Each point represents the results of two experi-
ments: the y-coordinate is the F1 score of the base-
line method (shown on the y-axis), while the x-
coordinate represents the score of the HIER method
in the same experiment. Thus, points lying be-
low the y = x line represent experiments for which
HIER received a higher F1 value than did the base-
line. While all three plots show HIER outperform-
ing each of the three baselines, not surprisingly,
the non-transfer GAUSS method suffers the worst,
followed by the naive concatenation (CAT) base-
line. Both methods fail to make any explicit dis-
tinction between the source and target domains and
thus suffer when the domains differ even slightly
from each other. Although the differences are
more subtle, the right-most plot of Figure 5 sug-
gests HIER is likewise able to outperform the non-
hierarchical CHELBA prior in certain transfer sce-
narios. CHELBA is able to avoid suffering as much
as the other baselines when faced with large differ-
ence between domains, but is still unable to capture
251
0.2
.4
.6
.8
1
0 .2 .4 .6 .8 1
G
AU
SS
(F
1)
HIER (F1)
0
.2
.4
.6
.8
1
0 .2 .4 .6 .8 1
CA
T
(F
1)
HIER (F1)
.4
.6
.8
.4 .6 .8
CH
EL
BA
(F
1)
HIER (F1)
?
y = x
MUC6@3%
MUC6@6%
MUC6@13%
MUC6@25%
MUC6@50%
MUC6@100%
CSPACE@3%
CSPACE@6%
CSPACE@13%
CSPACE@25%
CSPACE@50%
CSPACE@100%
Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nine
prior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets. Points
below the y = x line indicate HIER outperforming baselines.
as many dependencies between domains as HIER.
4 Conclusions, related & future work
In this work we have introduced hierarchical feature
tree priors for use in transfer learning on named en-
tity extraction tasks. We have provided evidence that
motivates these models on intuitive, theoretical and
empirical grounds, and have gone on to demonstrate
their effectiveness in relation to other, competitive
transfer methods. Specifically, we have shown that
hierarchical priors allow the user enough flexibil-
ity to customize their semantics to a specific prob-
lem, while providing enough structure to resist un-
intended negative effects when used inappropriately.
Thus hierarchical priors seem a natural, effective
and robust choice for transferring learning across
NER datasets and tasks.
Some of the first formulations of the transfer
learning problem were presented over 10 years
ago (Thrun, 1996; Baxter, 1997). Other techniques
have tried to quantify the generalizability of cer-
tain features across domains (Daume? III and Marcu,
2006; Jiang and Zhai, 2006), or tried to exploit the
common structure of related problems (Ben-David
et al, 2007; Scho?lkopf et al, 2005). Most of
this prior work deals with supervised transfer learn-
ing, and thus requires labeled source domain data,
though there are examples of unsupervised (Arnold
et al, 2007), semi-supervised (Grandvalet and Ben-
gio, 2005; Blitzer et al, 2006), and transductive ap-
proaches (Taskar et al, 2003).
Recent work using so-called meta-level priors to
transfer information across tasks (Lee et al, 2007),
while related, does not take into explicit account the
hierarchical structure of these meta-level features of-
ten found in NLP tasks. Daume? allows an extra de-
gree of freedom among the features of his domains,
implicitly creating a two-level feature hierarchy with
one branch for general features, and another for do-
main specific ones, but does not extend his hierar-
chy further (Daume? III, 2007)). Similarly, work on
hierarchical penalization (Szafranski et al, 2007) in
two-level trees tries to produce models that rely only
on a relatively small number of groups of variable,
as structured by the tree, as opposed to transferring
knowledge between branches themselves.
Our future work is focused on designing an al-
gorithm to optimally choose a smoothing regime
for the learned feature trees so as to better exploit
the similarities between domains while neutralizing
their differences. Along these lines, we are working
on methods to reduce the amount of labeled target
domain data needed to tune the prior-based mod-
els, looking forward to semi-supervised and unsu-
pervised transfer methods.
252
References
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. In JMLR 6, pages 1817 ? 1853.
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2007. A comparative study of methods for trans-
ductive transfer learning. In Proceedings of the IEEE
International Conference on Data Mining (ICDM)
2007 Workshop on Mining and Management of Bio-
logical Data.
Jonathan Baxter. 1997. A Bayesian/information theo-
retic model of learning to learn via multiple task sam-
pling. Machine Learning, 28(1):7?39.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In NIPS 20, Cambridge, MA. MIT
Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, Sydney, Australia.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. NYU: Description of the MENE named entity
system as used in MUC-7.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney,
A. Ramani, and Y. Wong. 2004. Comparative experi-
ments on learning information extractors for proteins
and their interactions. In Journal of AI in Medicine.
Data from ftp://ftp.cs.utexas.edu/pub/mooney/bio-
data/proteins.tar.gz.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP
2004, pages 285?292. ACL.
S. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artificial
Intelligence Research 26, pages 101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. Descrip-
tion of the UMass system as used for MUC-6.
Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, Lars
Asker, Per Lidn, and Joakim Co?ster. 2002. Protein
names and how to find them. In International Journal
of Medical Informatics.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In CAP,
Nice, France.
Jing Jiang and ChengXiang Zhai. 2006. Exploiting do-
main structure for named entity recognition. In Hu-
man Language Technology Conference, pages 74 ? 81.
R. Kraut, S. Fussell, F. Lerch, and J. Espinosa. 2004. Co-
ordination in teams: evidence from a simulated man-
agement game.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller.
2007. Learning a meta-level prior for feature relevance
from multiple related tasks. In Proceedings of Interna-
tional Conference on Machine Learning (ICML).
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: Ap-
plying named entity recognition to informal text. In
HLT/EMNLP.
Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Transfer learning by constructing informative priors.
In ICML 22.
Bernhard Scho?lkopf, Florian Steinke, and Volker Blanz.
2005. Object correspondence as a machine learning
problem. In ICML ?05: Proceedings of the 22nd inter-
national conference on Machine learning, pages 776?
783, New York, NY, USA. ACM.
Charles Sutton and Andrew McCallum. 2005. Composi-
tion of conditional random fields for transfer learning.
In HLT/EMLNLP.
M. Szafranski, Y. Grandvalet, and P. Morizet-
Mahoudeaux. 2007. Hierarchical penalization.
In Advances in Neural Information Processing
Systems 20. MIT press.
B. Taskar, M.-F. Wong, and D. Koller. 2003. Learn-
ing on the test data: Leveraging ?unseen? features. In
Proc. Twentieth International Conference on Machine
Learning (ICML).
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS, volume 8,
pages 640?646. MIT.
J. Zhang, Z. Ghahramani, and Y. Yang. 2005. Learning
multiple related tasks using latent independent compo-
nent analysis.
253
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 441?449,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic Set Instance Extraction using the Web
Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
rcwang@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
An important and well-studied problem is
the production of semantic lexicons from
a large corpus. In this paper, we present
a system named ASIA (Automatic Set In-
stance Acquirer), which takes in the name
of a semantic class as input (e.g., ?car
makers?) and automatically outputs its in-
stances (e.g., ?ford?, ?nissan?, ?toyota?).
ASIA is based on recent advances in web-
based set expansion - the problem of find-
ing all instances of a set given a small
number of ?seed? instances. This ap-
proach effectively exploits web resources
and can be easily adapted to different
languages. In brief, we use language-
dependent hyponym patterns to find a
noisy set of initial seeds, and then use a
state-of-the-art language-independent set
expansion system to expand these seeds.
The proposed approach matches or outper-
forms prior systems on several English-
language benchmarks. It also shows ex-
cellent performance on three dozen addi-
tional benchmark problems from English,
Chinese and Japanese, thus demonstrating
language-independence.
1 Introduction
An important and well-studied problem is the pro-
duction of semantic lexicons for classes of in-
terest; that is, the generation of all instances of
a set (e.g., ?apple?, ?orange?, ?banana?) given
a name of that set (e.g., ?fruits?). This task is
often addressed by linguistically analyzing very
large collections of text (Hearst, 1992; Kozareva
et al, 2008; Etzioni et al, 2005; Pantel and
Ravichandran, 2004; Pasca, 2004), often using
hand-constructed or machine-learned shallow lin-
guistic patterns to detect hyponym instances. A hy-
ponym is a word or phrase whose semantic range
Figure 1: Examples of SEAL?s input and output.
English entities are reality TV shows, Chinese en-
tities are popular Taiwanese foods, and Japanese
entities are famous cartoon characters.
is included within that of another word. For exam-
ple, x is a hyponym of y if x is a (kind of) y. The
opposite of hyponym is hypernym.
In this paper, we evaluate a novel approach to
this problem, embodied in a system called ASIA1
(Automatic Set Instance Acquirer). ASIA takes a
semantic class name as input (e.g., ?car makers?)
and automatically outputs instances (e.g., ?ford?,
?nissan?, ?toyota?). Unlike prior methods, ASIA
makes heavy use of tools for web-based set ex-
pansion. Set expansion is the task of finding all
instances of a set given a small number of exam-
ple (seed) instances. ASIA uses SEAL (Wang and
Cohen, 2007), a language-independent web-based
system that performed extremely well on a large
number of benchmark sets ? given three correct
seeds, SEAL obtained average MAP scores in the
high 90?s for 36 benchmark problems, including a
dozen test problems each for English, Chinese and
Japanese. SEAL works well in part because it can
efficiently find and process many semi-structured
web documents containing instances of the set be-
ing expanded. Figure 1 shows some examples of
SEAL?s input and output.
SEAL has been recently extended to be robust
to errors in its initial set of seeds (Wang et al,
1http://rcwang.com/asia
441
2008), and to use bootstrapping to iteratively im-
prove its performance (Wang and Cohen, 2008).
These extensions allow ASIA to extract instances
of sets from the Web, as follows. First, given a
semantic class name (e.g., ?fruits?), ASIA uses a
small set of language-dependent hyponym patterns
(e.g., ?fruits such as ?) to find a large but noisy
set of seed instances. Second, ASIA uses the ex-
tended version of SEAL to expand the noisy set of
seeds.
ASIA?s approach is motivated by the conjecture
that for many natural classes, the amount of infor-
mation available in semi-structured documents on
the Web is much larger than the amount of infor-
mation available in free-text documents; hence, it
is natural to attempt to augment search for set in-
stances in free-text with semi-structured document
analysis. We show that ASIA performs extremely
well experimentally. On the 36 benchmarks used
in (Wang and Cohen, 2007), which are relatively
small closed sets (e.g., countries, constellations,
NBA teams), ASIA has excellent performance
for both recall and precision. On four additional
English-language benchmark problems (US states,
countries, singers, and common fish), we com-
pare to recent work by Kozareva, Riloff, and Hovy
(Kozareva et al, 2008), and show comparable or
better performance on each of these benchmarks;
this is notable because ASIA requires less infor-
mation than the work of Kozareva et al(their sys-
tem requires a concept name and a seed). We also
compare ASIA on twelve additional benchmarks
to the extended Wordnet 2.1 produced by Snow
et al(Snow et al, 2006), and show that for these
twelve sets, ASIA produces more than five times
as many set instances with much higher precision
(98% versus 70%).
Another advantage of ASIA?s approach is that it
is nearly language-independent: since the underly-
ing set-expansion tools are language-independent,
all that is needed to support a new target language
is a new set of hyponym patterns for that lan-
guage. In this paper, we present experimental re-
sults for Chinese and Japanese, as well as English,
to demonstrate this language-independence.
We present related work in Section 2, and ex-
plain our proposed approach for ASIA in Sec-
tion 3. Section 4 presents the details of our ex-
periments, as well as the experimental results. A
comparison of results are illustrated in Section 5,
and the paper concludes in Section 6.
2 Related Work
There has been a significant amount of research
done in the area of semantic class learning (aka
lexical acquisition, lexicon induction, hyponym
extraction, or open-domain information extrac-
tion). However, to the best of our knowledge, there
is not a system that can perform set instance ex-
traction in multiple languages given only the name
of the set.
Hearst (Hearst, 1992) presented an approach
that utilizes hyponym patterns for extracting can-
didate instances given the name of a semantic set.
The approach presented in Section 3.1 is based on
this work, except that we extended it to two other
languages: Chinese and Japanese.
Pantel et al(Pantel and Ravichandran, 2004)
presented an algorithm for automatically inducing
names for semantic classes and for finding their
instances by using ?concept signatures? (statistics
on co-occuring instances). Pasca (Pasca, 2004)
presented a method for acquiring named entities in
arbitrary categories using lexico-syntactic extrac-
tion patterns. Etzioni et al(Etzioni et al, 2005)
presented the KnowItAll system that also utilizes
hyponym patterns to extract class instances from
the Web. All the systems mentioned rely on either
a English part-of-speech tagger, a parser, or both,
and hence are language-dependent.
Kozareva et al(Kozareva et al, 2008) illustrated
an approach that uses a single hyponym pattern
combined with graph structures to learn semantic
class from the Web. Section 5.1 shows that our
approach is competitive experimentally; however,
their system requires more information, as it uses
the name of the semantic set and a seed instance.
Pasca (Pas?ca, 2007b; Pas?ca, 2007a) illustrated
a set expansion approach that extracts instances
from Web search queries given a set of input seed
instances. This approach is similar in flavor to
SEAL but, addresses a different task from that ad-
dressed here: for ASIA the user provides no seeds,
but instead provides the name of the set being ex-
panded. We compare to Pasca?s system in Sec-
tion 5.2.
Snow et al(Snow et al, 2006) use known hyper-
nym/hyponym pairs to generate training data for a
machine-learning system, which then learns many
lexico-syntactic patterns. The patterns learned are
based on English-language dependency parsing.
We compare to Snow et als results in Section 5.3.
442
3 Proposed Approach
ASIA is composed of three main components: the
Noisy Instance Provider, the Noisy Instance Ex-
pander, and the Bootstrapper. Given a semantic
class name, the Provider extracts a initial set of
noisy candidate instances using hand-coded pat-
terns, and ranks the instances by using a sim-
ple ranking model. The Expander expands and
ranks the instances using evidence from semi-
structured web documents, such that irrelevant
ones are ranked lower in the list. The Bootstrap-
per enhances the quality and completeness of the
ranked list by using an unsupervised iterative tech-
nique. Note that the Expander and Bootstrap-
per rely on SEAL to accomplish their goals. In
this section, we first describe the Noisy Instance
Provider, then we briefly introduce SEAL, fol-
lowed by the Noisy Instance Expander, and finally,
the Bootstrapper.
3.1 Noisy Instance Provider
Noisy Instance Provider extracts candidate in-
stances from free text (i.e., web snippets) us-
ing the methods presented in Hearst?s early work
(Hearst, 1992). Hearst exploited several patterns
for identifying hyponymy relation (e.g., such au-
thor as Shakespeare) that many current state-of-
the-art systems (Kozareva et al, 2008; Pantel and
Ravichandran, 2004; Etzioni et al, 2005; Pasca,
2004) are using. However, unlike all of those sys-
tems, ASIA does not use any NLP tool (e.g., parts-
of-speech tagger, parser) or rely on capitalization
for extracting candidates (since we wanted ASIA
to be as language-independent as possible). This
leads to sets of instances that are noisy; however,
we will show that set expansion and re-ranking can
improve the initial sets dramatically. Below, we
will refer to the initial set of noisy instances ex-
tracted by the Provider as the initial set.
In more detail, the Provider first constructs a
few queries of hyponym phrase by using a se-
mantic class name and a set of pre-defined hy-
ponym patterns. For every query, the Provider re-
trieves a hundred snippets from Yahoo!, and splits
each snippet into multiple excerpts (a snippet of-
ten contains multiple continuous excerpts from its
web page). For each excerpt, the Provider extracts
all chunks of characters that would then be used
as candidate instances. Here, we define a chunk
as a sequence of characters bounded by punctua-
tion marks or the beginning and end of an excerpt.
Figure 2: Hyponym patterns in English, Chinese,
and Japanese. In each pattern, <C> is a place-
holder for the semantic class name and <I> is a
placeholder for its instances.
Lastly, the Provider ranks each candidate instance
x based on its weight assigned by the simple rank-
ing model presented below:
weight(x) =
sf (x,S)
|S|
?
ef (x,E)
|E|
?
wcf (x,E)
|C|
where S is the set of snippets, E is the set of ex-
cerpts, and C is the set of chunks. sf (x,S) is
the snippet frequency of x (i.e., the number of
snippets containing x) and ef (x,E) is the excerpt
frequency of x. Furthermore, wcf (x,E) is the
weighted chunk frequency of x, which is defined
as follows:
wcf (x,E) =
?
e?E
?
x?e
1
dist(x, e) + 1
where dist(x, e) is the number of characters be-
tween x and the hyponym phrase in excerpt e.
This model weights every occurrence of x based
on the assumption that chunks closer to a hyponym
phrase are usually more important than those fur-
ther away. It also heavily rewards frequency, as
our assumption is that the most common instances
will be more useful as seeds for SEAL.
Figure 2 shows the hyponym patterns we use
for English, Chinese, and Japanese. There are two
types of hyponym patterns: The first type are the
ones that require the class name C to precede its
instance I (e.g., C such as I), and the second type
are the opposite ones (e.g., I and other C). In
order to reduce irrelevant chunks, when excerpts
were extracted, the Provider drops all characters
preceding the hyponym phrase in excerpts that
contain the first type, and also drops all charac-
ters following the hyponym phrase in excerpts that
contain the second type. For some semantic class
names (e.g., ?cmu buildings?), there are no web
443
documents containing any of the hyponym-phrase
queries that were constructed using the name. In
this case, the Provider turns to a back-off strategy
which simply treats the semantic class name as the
hyponym phrase and extracts/ranks all chunks co-
occurring with the class name in the excerpts.
3.2 Set Expander - SEAL
In this paper, we rely on a set expansion system
named SEAL (Wang and Cohen, 2007), which
stands for Set Expander for Any Language. The
system accepts as input a few seeds of some target
set S (e.g., ?fruits?) and automatically finds other
probable instances (e.g., ?apple?, ?banana?) of S
in web documents. As its name implies, SEAL
is independent of document languages: both the
written (e.g., English) and the markup language
(e.g., HTML). SEAL is a research system that
has shown good performance in published results
(Wang and Cohen, 2007; Wang et al, 2008; Wang
and Cohen, 2008). Figure 1 shows some examples
of SEAL?s input and output.
In more detail, SEAL contains three major com-
ponents: the Fetcher, Extractor, and Ranker. The
Fetcher is responsible for fetching web docu-
ments, and the URLs of the documents come from
top results retrieved from the search engine us-
ing the concatenation of all seeds as the query.
This ensures that every fetched web page contains
all seeds. The Extractor automatically constructs
?wrappers? (i.e. page-specific extraction rules) for
each page that contains the seeds. Every wrap-
per comprises two character strings that specify
the left and right contexts necessary for extract-
ing candidate instances. These contextual strings
are maximally-long contexts that bracket at least
one occurrence of every seed string on a page. All
other candidate instances bracketed by these con-
textual strings derived from a particular page are
extracted from the same page.
After the candidates are extracted, the Ranker
constructs a graph that models all the relations
between documents, wrappers, and candidate in-
stances. Figure 3 shows an example graph where
each node di represents a document, wi a wrapper,
and mi a candidate instance. The Ranker performs
Random Walk with Restart (Tong et al, 2006) on
this graph (where the initial ?restart? set is the
set of seeds) until all node weights converge, and
then ranks nodes by their final score; thus nodes
are weighted higher if they are connected to many
Figure 3: An example graph constructed by
SEAL. Every edge from node x to y actually has
an inverse relation edge from node y to x that is
not shown here (e.g., m1 is extracted by w1).
seed nodes by many short, low fan-out paths. The
final expanded set contains all candidate instance
nodes, ranked by their weights in the graph.
3.3 Noisy Instance Expander
Wang (Wang et al, 2008) illustrated that it is feasi-
ble to perform set expansion on noisy input seeds.
The paper showed that the noisy output of any
Question Answering system for list questions can
be improved by using a noise-resistant version of
SEAL (An example of a list question is ?Who
were the husbands of Heddy Lamar??). Since the
initial set of candidate instances obtained using
Hearst?s method are noisy, the Expander expands
them by performing multiple iterations of set ex-
pansion using the noise-resistant SEAL.
For every iteration, the Expander performs set
expansion on a static collection of web pages. This
collection is pre-fetched by querying Google and
Yahoo! using the input class name and words such
as ?list?, ?names?, ?famous?, and ?common? for
discovering web pages that might contain lists of
the input class. In the first iteration, the Expander
expands instances with scores of at least k in the
initial set. In every upcoming iteration, it expands
instances obtained in the last iteration that have
scores of at least k and that also exist in the ini-
tial set. We have determined k to be 0.4 based on
our development set2. This process repeats until
the set of seeds for ith iteration is identical to that
of (i? 1)th iteration.
There are several differences between the origi-
nal SEAL and the noise-resistant SEAL. The most
important difference is the Extractor. In the origi-
2A collection of closed-set lists such as planets, Nobel
prizes, and continents in English, Chinese and Japanese
444
nal SEAL, the Extractor requires the longest com-
mon contexts to bracket at least one instance of ev-
ery seed per web page. However, when seeds are
noisy, such common contexts usually do not ex-
ist. The Extractor in noise-resistant SEAL solves
this problem by requiring the contexts to bracket
at least one instance of a minimum of two seeds,
rather than every seed. This is implemented using
a trie-based method described briefly in the origi-
nal SEAL paper (Wang and Cohen, 2007). In this
paper, the Expander utilizes a slightly-modified
version of the Extractor, which requires the con-
texts to bracket as many seed instances as possible.
This idea is based on the assumption that irrelevant
instances usually do not have common contexts;
whereas relevant ones do.
3.4 Bootstrapper
Bootstrapping (Etzioni et al, 2005; Kozareva,
2006; Nadeau et al, 2006) is an unsupervised iter-
ative process in which a system continuously con-
sumes its own outputs to improve its own perfor-
mance. Wang (Wang and Cohen, 2008) showed
that it is feasible to bootstrap the results of set ex-
pansion to improve the quality of a list. The pa-
per introduces an iterative version of SEAL called
iSEAL, which expands a list in multiple iterations.
In each iteration, iSEAL expands a few candi-
dates extracted in previous iterations and aggre-
gates statistics. The Bootstrapper utilizes iSEAL
to further improve the quality of the list returned
by the Expander.
In every iteration, the Bootstrapper retrieves 25
web pages by using the concatenation of three
seeds as query to each of Google and Yahoo!.
In the first iteration, the Bootstrapper expands
randomly-selected instances returned by the Ex-
pander that exist in the initial set. In every upcom-
ing iteration, the Bootstrapper expands randomly-
selected unsupervised instances obtained in the
last iteration that also exist in the initial set. This
process terminates when all possible seed com-
binations have been consumed or five iterations3
have been reached, whichever comes first. No-
tice that from iteration to iteration, statistics are
aggregated by growing the graph described in Sec-
tion 3.2. We perform Random Walk with Restart
(Tong et al, 2006) on this graph to determine the
final ranking of the extracted instances.
3To keep the overall runtime minimal.
4 Experiments
4.1 Datasets
We evaluated our approach using the evaluation
set presented in (Wang and Cohen, 2007), which
contains 36 manually constructed lists across
three different languages: English, Chinese, and
Japanese (12 lists per language). Each list contains
all instances of a particular semantic class in a cer-
tain language, and each instance contains a set of
synonyms (e.g., USA, America). There are a total
of 2515 instances, with an average of 70 instances
per semantic class. Figure 4 shows the datasets
and their corresponding semantic class names that
we use in our experiments.
4.2 Evaluation Metric
Since the output of ASIA is a ranked list of ex-
tracted instances, we choose mean average pre-
cision (MAP) as our evaluation metric. MAP is
commonly used in the field of Information Re-
trieval for evaluating ranked lists because it is sen-
sitive to the entire ranking and it contains both re-
call and precision-oriented aspects. The MAP for
multiple ranked lists is simply the mean value of
average precisions calculated separately for each
ranked list. We define the average precision of a
single ranked list as:
AvgPrec(L) =
|L|?
r=1
Prec(r)? isFresh(r)
Total # of Correct Instances
where L is a ranked list of extracted instances, r
is the rank ranging from 1 to |L|, Prec(r) is the
precision at rank r. isFresh(r) is a binary function
for ensuring that, if a list contains multiple syn-
onyms of the same instance, we do not evaluate
that instance more than once. More specifically,
the function returns 1 if a) the synonym at r is cor-
rect, and b) it is the highest-ranked synonym of its
instance in the list; it returns 0 otherwise.
4.3 Experimental Results
For each semantic class in our dataset, the
Provider first produces a noisy list of candidate in-
stances, using its corresponding class name shown
in Figure 4. This list is then expanded by the Ex-
pander and further improved by the Bootstrapper.
We present our experimental results in Table 1.
As illustrated, although the Provider performs
badly, the Expander substantially improves the
445
Figure 4: The 36 datasets and their semantic class names used as inputs to ASIA in our experiments.
English Dataset NP Chinese Dataset NP Japanese Dataset NP
NP NP +NE NP NP +NE NP NP +NE
# NP +BS +NE +BS # NP +BS +NE +BS # NP +BS +NE +BS
1. 0.22 0.83 0.82 0.87 13. 0.09 0.75 0.80 0.80 25. 0.20 0.63 0.71 0.76
2. 0.31 1.00 1.00 1.00 14. 0.08 0.99 0.80 0.89 26. 0.20 0.40 0.90 0.96
3. 0.54 0.99 0.99 0.98 15. 0.29 0.66 0.84 0.91 27. 0.16 0.96 0.97 0.96
4. 0.48 1.00 1.00 1.00 *16. 0.09 0.00 0.93 0.93 *28. 0.01 0.00 0.80 0.87
5. 0.54 1.00 1.00 1.00 17. 0.21 0.00 1.00 1.00 29. 0.09 0.00 0.95 0.95
6. 0.64 0.98 1.00 1.00 *18. 0.00 0.00 0.19 0.23 *30. 0.02 0.00 0.73 0.73
7. 0.32 0.82 0.98 0.97 19. 0.11 0.90 0.68 0.89 31. 0.20 0.49 0.83 0.89
8. 0.41 1.00 1.00 1.00 20. 0.18 0.00 0.94 0.97 32. 0.09 0.00 0.88 0.88
9. 0.81 1.00 1.00 1.00 21. 0.64 1.00 1.00 1.00 33. 0.07 0.00 0.95 1.00
*10. 0.00 0.00 0.00 0.00 22. 0.08 0.00 0.67 0.80 34. 0.04 0.32 0.98 0.97
11. 0.11 0.62 0.51 0.76 23. 0.47 1.00 1.00 1.00 35. 0.15 1.00 1.00 1.00
12. 0.01 0.00 0.30 0.30 24. 0.60 1.00 1.00 1.00 36. 0.20 0.90 1.00 1.00
Avg. 0.37 0.77 0.80 0.82 Avg. 0.24 0.52 0.82 0.87 Avg. 0.12 0.39 0.89 0.91
Table 1: Performance of set instance extraction for each dataset measured in MAP. NP is the Noisy
Instance Provider, NE is the Noisy Instance Expander, and BS is the Bootstrapper.
quality of the initial list, and the Bootstrapper then
enhances it further more. On average, the Ex-
pander improves the performance of the Provider
from 37% to 80% for English, 24% to 82% for
Chinese, and 12% to 89% for Japanese. The Boot-
strapper then further improves the performance of
the Expander to 82%, 87% and 91% respectively.
In addition, the results illustrate that the Bootstrap-
per is also effective even without the Expander; it
directly improves the performance of the Provider
from 37% to 77% for English, 24% to 52% for
Chinese, and 12% to 39% for Japanese.
The simple back-off strategy seems to be effec-
tive as well. There are five datasets (marked with *
in Table 1) of which their hyponym phrases return
zero web documents. For those datasets, ASIA au-
tomatically uses the back-off strategy described in
Section 3.1. Considering only those five datasets,
the Expander, on average, improves the perfor-
mance of the Provider from 2% to 53% and the
Bootstrapper then improves it to 55%.
5 Comparison to Prior Work
We compare ASIA?s performance to the results
of three previously published work. We use the
best-configured ASIA (NP+NE+BS) for all com-
parisons, and we present the comparison results in
this section.
5.1 (Kozareva et al, 2008)
Table 2 shows a comparison of our extraction per-
formance to that of Kozareva (Kozareva et al,
2008). They report results on four tasks: US
states, countries, singers, and common fish. We
evaluated our results manually. The results in-
dicate that ASIA outperforms theirs for all four
datasets that they reported. Note that the input
to their system is a semantic class name plus one
seed instance; whereas, the input to ASIA is only
the class name. In terms of system runtime, for
each semantic class, Kozareva et alreported that
their extraction process usually finished overnight;
however, ASIA usually finished within a minute.
446
N Kozareva ASIA N Kozareva ASIA
US States Countries
25 1.00 1.00 50 1.00 1.00
50 1.00 1.00 100 1.00 1.00
64 0.78 0.78 150 1.00 1.00
200 0.90 0.93
300 0.61 0.67
323 0.57 0.62
Singers Common Fish
10 1.00 1.00 10 1.00 1.00
25 1.00 1.00 25 1.00 1.00
50 0.97 1.00 50 1.00 1.00
75 0.96 1.00 75 0.93 1.00
100 0.96 1.00 100 0.84 1.00
150 0.95 0.97 116 0.80 1.00
180 0.91 0.96
Table 2: Set instance extraction performance com-
pared to Kozareva et al We report our precision
for all semantic classes and at the same ranks re-
ported in their work.
5.2 (Pas?ca, 2007b)
We compare ASIA to Pasca (Pas?ca, 2007b) and
present comparison results in Table 3. There are
ten semantic classes in his evaluation dataset, and
the input to his system for each class is a set of
seed entities rather than a class name. We evaluate
every instance manually for each class. The results
show that, on average, ASIA performs better.
However, we should emphasize that for the
three classes: movie, person, and video game,
ASIA did not initially converge to the correct in-
stance list given the most natural concept name.
Given ?movies?, ASIA returns as instances strings
like ?comedy?, ?action?, ?drama?, and other kinds
of movies. Given ?video games?, it returns ?PSP?,
?Xbox?, ?Wii?, etc. Given ?people?, it returns
?musicians?, ?artists?, ?politicians?, etc. We ad-
dressed this problem by simply re-running ASIA
with a more specific class name (i.e., the first one
returned); however, the result suggests that future
work is needed to support automatic construction
of hypernym hierarchy using semi-structured web
documents.
5.3 (Snow et al, 2006)
Snow (Snow et al, 2006) has extended the Word-
Net 2.1 by adding thousands of entries (synsets)
at a relatively high precision. They have made
several versions of extended WordNet available4.
For comparison purposes, we selected the version
(+30K) that achieved the best F-score in their ex-
periments.
4http://ai.stanford.edu/?rion/swn/
Precision @
Target Class System 25 50 100 150 250
Cities Pasca 1.00 0.96 0.88 0.84 0.75
ASIA 1.00 1.00 0.97 0.98 0.96
Countries Pasca 1.00 0.98 0.95 0.82 0.60
ASIA 1.00 1.00 1.00 1.00 0.79
Drugs Pasca 1.00 1.00 0.96 0.92 0.75
ASIA 1.00 1.00 1.00 1.00 0.98
Food Pasca 0.88 0.86 0.82 0.78 0.62
ASIA 1.00 1.00 0.93 0.95 0.90
Locations Pasca 1.00 1.00 1.00 1.00 1.00
ASIA 1.00 1.00 1.00 1.00 1.00
Newspapers Pasca 0.96 0.98 0.93 0.86 0.54
ASIA 1.00 1.00 0.98 0.99 0.85
Universities Pasca 1.00 1.00 1.00 1.00 0.99
ASIA 1.00 1.00 1.00 1.00 1.00
Movies Pasca 0.92 0.90 0.88 0.84 0.79
Comedy Movies ASIA 1.00 1.00 1.00 1.00 1.00
People Pasca 1.00 1.00 1.00 1.00 1.00
Jazz Musicians ASIA 1.00 1.00 1.00 0.94 0.88
Video Games Pasca 1.00 1.00 0.99 0.98 0.98
PSP Games ASIA 1.00 1.00 1.00 0.99 0.97
Pasca 0.98 0.97 0.94 0.90 0.80
Average ASIA 1.00 1.00 0.99 0.98 0.93
Table 3: Set instance extraction performance com-
pared to Pasca. We report our precision for all se-
mantic classes and at the same ranks reported in
his work.
For the experimental comparison, we focused
on leaf semantic classes from the extended Word-
Net that have many hypernyms, so that a mean-
ingful comparison could be made: specifically, we
selected nouns that have at least three hypernyms,
such that the hypernyms are the leaf nodes in the
hypernym hierarchy of WordNet. Of these, 210
were extended by Snow. Preliminary experiments
showed that (as in the experiments with Pasca?s
classes above) ASIA did not always converge to
the intended meaning; to avoid this problem, we
instituted a second filter, and discarded ASIA?s re-
sults if the intersection of hypernyms from ASIA
and WordNet constituted less than 50% of those
in WordNet. About 50 of the 210 nouns passed
this filter. Finally, we manually evaluated preci-
sion and recall of a randomly selected set of twelve
of these 50 nouns.
We present the results in Table 4. We used a
fixed cut-off score5 of 0.3 to truncate the ranked
list produced by ASIA, so that we can compute
precision. Since only a few of these twelve nouns
are closed sets, we cannot generally compute re-
call; instead, we define relative recall to be the
ratio of correct instances to the union of correct
instances from both systems. As shown in the re-
sults, ASIA has much higher precision, and much
higher relative recall. When we evaluated Snow?s
extended WordNet, we assumed all instances that
5Determined from our development set.
447
Snow?s Wordnet (+30k) Relative ASIA Relative
Class Name # Right # Wrong Prec. Recall # Right # Wrong Prec. Recall
Film Directors 4 4 0.50 0.01 457 0 1.00 1.00
Manias 11 0 1.00 0.09 120 0 1.00 1.00
Canadian Provinces 10 82 0.11 1.00 10 3 0.77 1.00
Signs of the Zodiac 12 10 0.55 1.00 12 0 1.00 1.00
Roman Emperors 44 4 0.92 0.47 90 0 1.00 0.96
Academic Departments 20 0 1.00 0.67 27 0 1.00 0.90
Choreographers 23 10 0.70 0.14 156 0 1.00 0.94
Elected Officials 5 102 0.05 0.31 12 0 1.00 0.75
Double Stars 11 1 0.92 0.46 20 0 1.00 0.83
South American Countries 12 1 0.92 1.00 12 0 1.00 1.00
Prizefighters 16 4 0.80 0.23 63 1 0.98 0.89
Newspapers 20 0 1.00 0.23 71 0 1.00 0.81
Average 15.7 18.2 0.70 0.47 87.5 0.3 0.98 0.92
Table 4: Set instance extraction performance compared to Snow et al
Figure 5: Examples of ASIA?s input and out-
put. Input class for Chinese is ?holidays? and for
Japanese is ?dramas?.
were in the original WordNet are correct. The
three incorrect instances of Canadian provinces
from ASIA are actually the three Canadian terri-
tories.
6 Conclusions
In this paper, we have shown that ASIA, a SEAL-
based system, extracts set instances with high pre-
cision and recall in multiple languages given only
the set name. It obtains a high MAP score (87%)
averaged over 36 benchmark problems in three
languages (Chinese, Japanese, and English). Fig-
ure 5 shows some real examples of ASIA?s in-
put and output in those three languages. ASIA?s
approach is based on web-based set expansion
using semi-structured documents, and is moti-
vated by the conjecture that for many natural
classes, the amount of information available in
semi-structured documents on the Web is much
larger than the amount of information available
in free-text documents. This conjecture is given
some support by our experiments: for instance,
ASIA finds 457 instances of the set ?film direc-
tor? with perfect precision, whereas Snow et als
state-of-the-art methods for extraction from free
text extract only four correct instances, with only
50% precision.
ASIA?s approach is also quite language-
independent. By adding a few simple hyponym
patterns, we can easily extend the system to sup-
port other languages. We have also shown that
Hearst?s method works not only for English, but
also for other languages such as Chinese and
Japanese. We note that the ability to construct
semantic lexicons in diverse languages has obvi-
ous applications in machine translation. We have
also illustrated that ASIA outperforms three other
English systems (Kozareva et al, 2008; Pas?ca,
2007b; Snow et al, 2006), even though many of
these use more input than just a semantic class
name. In addition, ASIA is also quite efficient,
requiring only a few minutes of computation and
couple hundreds of web pages per problem.
In the future, we plan to investigate the pos-
sibility of constructing hypernym hierarchy auto-
matically using semi-structured documents. We
also plan to explore whether lexicons can be con-
structed using only the back-off method for hy-
ponym extraction, to make ASIA completely lan-
guage independent. We also wish to explore
whether performance can be improved by simul-
taneously finding class instances in multiple lan-
guages (e.g., Chinese and English) while learning
translations between the extracted instances.
7 Acknowledgments
This work was supported by the Google Research
Awards program.
448
References
Oren Etzioni, Michael J. Cafarella, Doug Downey,
Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artif. Intell.,
165(1):91?134.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings
of the 14th International Conference on Computa-
tional Linguistics, pages 539?545.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056, Columbus, Ohio,
June. Association for Computational Linguistics.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In EACL. The Association for Computer Lin-
guistics.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Luc Lamontagne and Mario Marchand, editors,
Canadian Conference on AI, volume 4013 of Lec-
ture Notes in Computer Science, pages 266?277.
Springer.
Marius Pas?ca. 2007a. Organizing and searching the
world wide web of facts ? step two: harnessing the
wisdom of the crowds. In WWW ?07: Proceedings
of the 16th international conference on World Wide
Web, pages 101?110, New York, NY, USA. ACM.
Marius Pas?ca. 2007b. Weakly-supervised discovery
of named entities using web search queries. In
CIKM ?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 683?690, New York, NY,
USA. ACM.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321?328, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ?04: Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
137?145, New York, NY, USA. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL ?06: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the ACL, pages 801?
808, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its appli-
cations. In ICDM, pages 613?622. IEEE Computer
Society.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
Richard C. Wang and William W. Cohen. 2008. Iter-
ative set expansion of named entities using the web.
In ICDM, pages 1091?1096. IEEE Computer Soci-
ety.
Richard C. Wang, Nico Schlaefer, William W. Co-
hen, and Eric Nyberg. 2008. Automatic set ex-
pansion for list question answering. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 947?954, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
449
Learning to Classify Email into "Speech Acts"  
 
William W. Cohen1 Vitor R. Carvalho2 Tom M. Mitchell1,2 
1Center for Automated Learning & Discovery 
 Carnegie Mellon University 
  Pittsburgh, PA 15213 
2Language Technology Institute  
Carnegie Mellon University  
   Pittsburgh, PA 15213 
Abstract 
It is often useful to classify email accord-
ing to the intent of the sender (e.g., "pro-
pose a meeting", "deliver information"). 
We present experimental results in learn-
ing to classify email in this fashion, 
where each class corresponds to a verb-
noun pair taken from a predefined ontol-
ogy describing typical ?email speech 
acts?.   We demonstrate that, although 
this categorization problem is quite dif-
ferent from ?topical? text classification, 
certain categories of messages can none-
theless be detected with high precision 
(above 80%) and reasonable recall (above 
50%) using existing text-classification 
learning methods. This result suggests 
that useful task-tracking tools could be 
constructed based on automatic classifi-
cation into this taxonomy.  
1 Introduction 
In this paper we discuss using machine learn-
ing methods to classify email according to the 
intent of the sender.  In particular, we classify 
emails according to an ontology of verbs (e.g., 
propose, commit, deliver) and nouns (e.g., infor-
mation, meeting, task), which jointly describe the 
?email speech act? intended by the email sender.   
A method for accurate classification of email 
into such categories would have many potential 
benefits. For instance, it could be used to help an 
email user track the status of ongoing joint activi-
ties.  Delegation and coordination of joint tasks is 
a time-consuming and error-prone activity, and 
the cost of errors is high: it is not uncommon that 
commitments are forgotten, deadlines are missed, 
and opportunities are wasted because of a failure 
to properly track, delegate, and prioritize sub-
tasks. The classification methods we consider  
 
methods which could be used to partially auto-
mate this sort of activity tracking. A hypothetical 
example of an email assistant that works along 
these lines is shown in Figure 1. 
Bill,
Do you have any sample 
scheduling-related email we 
could use as data?  -Steve
Assistant announces:  ?new 
email request, priority 
unknown.?
Sure, I?ll put some together 
shortly. -Bill
Assistant:  ?should I add this 
new commitment to your to-
do list??
Fred, can you collect the msgs
from the CSPACE corpora 
tagged w/ the  ?meeting?
noun, ASAP? -Bill
Assistant:  notices outgoing
request, may take action if no 
answer is received promptly.
Yes, I can get to that in the 
next few days.  Is next 
Monday ok? -Fred
Assistant:  notices incoming 
commitment. ?Should I send 
Fred a reminder on Monday??
 
Figure 1 - Dialog with a hypothetical email assistant 
that automatically detects email speech acts.  Dashed 
boxes indicate outgoing messages.  (Messages have 
been edited for space and anonymity.) 
2 Related Work 
Our research builds on earlier work defining il-
locutionary points of speech acts (Searle, 1975), 
and relating such speech acts to email and work-
flow tracking (Winograd, 1987, Flores & Lud-
low, 1980, Weigant et al 2003). Winograd 
suggested that research explicating the speech-act 
based ?language-action perspective? on human 
communication could be used to build more use-
ful tools for coordinating joint activities.  The 
Coordinator (Winograd, 1987) was one such sys-
tem, in which users augmented email messages 
with additional annotations indicating intent. 
While such systems have been useful in lim-
ited contexts, they have also been criticized as 
cumbersome: by forcing users to conform to a 
particular formal system, they constrain commu-
nication and make it less natural (Schoop, 2001); 
in short, users often prefer unstructured email 
interactions (Camino et al 1998). We note that 
these difficulties are avoided if messages can be 
automatically annotated by intent, rather than 
soliciting a statement of intent from the user. 
Murakoshi et al (1999) proposed an email an-
notation scheme broadly similar to ours, called a 
?deliberation tree?, and an algorithm for con-
structing deliberation trees automatically, but 
their approach was not quantitatively evaluated. 
The approach is based on recognizing a set of 
hand-coded linguistic ?clues?.  A limitation of 
their approach is that these hand-coded linguistic 
?clues? are language-specific (and in fact limited 
to Japanese text.) 
Prior research on machine learning for text 
classification has primarily considered classifica-
tion of documents by topic (Lewis, 1992; Yang, 
1999), but also has addressed sentiment detection 
(Pang et al, 2002;  Weibe et al, 2001) and au-
thorship attribution (e.g., Argamon et al 2003).   
There has been some previous use of machine 
learning to classify email messages (Cohen 1996; 
Sahami et al, 1998; Rennie, 2000; Segal & 
Kephart, 2000).  However, to our knowledge, 
none of these systems has investigated learning 
methods for assigning email speech acts. Instead, 
email is generally classified into folders (i.e., ac-
cording to topic) or according to whether or not it 
is ?spam?. Learning systems have been previ-
ously used to automatically detect acts in 
conversational speech (e.g. Finke et al, 1998). 
3 An Ontology of Email Acts 
Our ontology of nouns and verbs covering some 
of the possible speech acts associated with emails 
is summarized in Figure 2.  We assume that a 
single email message may contain multiple acts, 
and that each act is described by a verb-noun pair 
drawn from this ontology (e.g., "deliver data").   
The underlined nodes in the figure indicate the 
nouns and verbs for which we have trained clas-
sifiers (as discussed in subsequent sections). 
To define the noun and verb ontology of 
Figure 2, we first examined email from several 
corpora (including our own inboxes) to find regu-
larities, and then performed a more detailed 
analysis of one corpus. The ontology was further 
refined in the process of labeling the corpora de-
scribed below. 
In refining this ontology, we adopted several 
principles. First, we believe that it is more impor-
tant for the ontology to reflect observed linguistic 
behavior than to reflect any abstract view of the 
space of possible speech acts. As a consequence, 
the taxonomy of verbs contains concepts that are 
atomic linguistically, but combine several illocu-
tionary points. (For example, the linguistic unit 
"let's do lunch" is both directive, as it requests the 
receiver, and commissive, as it implicitly com-
mits the sender. In our taxonomy this is a single 
'propose' act.) Also, acts which are abstractly 
possible but not observed in our data are not rep-
resented (for instance, declarations). 
 
Noun 
Activity Information 
Meeting 
Logistics 
Data 
Opinion Ongoing 
Activity 
Data Single 
Event 
Meeting Other   
Short Term 
Task 
Other 
Data Committee 
<Verb><Noun> 
Verb 
Remind 
Propose 
Deliver 
Commit 
Request 
Amend 
Refuse 
Greet 
Other Negotiate 
Initiate Conclude 
 
Figure 2 ? Taxonomy  
Second, we believe that the taxonomy must re-
flect common non-linguistic uses of email, such 
as the use of email as a mechanism to deliver 
files. We have grouped this with the linguistically 
similar speech act of delivering information. 
The verbs in Figure 1 are defined as follows.  
A request asks (or orders) the recipient to per-
form some activity. A question is also considered 
a request (for delivery of information).  
A propose message proposes a joint activity, 
i.e., asks the recipient to perform some activity 
and commits the sender as well, provided the re-
cipient agrees to the request.  A typical example 
is an email suggesting a joint meeting.  
An amend message amends an earlier proposal. 
Like a proposal, the message involves both a 
commitment and a request.  However, while a 
proposal is associated with a new task, an 
amendment is a suggested modification of an 
already-proposed task. 
A commit message commits the sender to 
some future course of action, or confirms the 
senders' intent to comply with some previously 
described course of action.   
A deliver message delivers something, e.g., 
some information, a PowerPoint presentation,  
the URL of a website, the answer to a question, a 
message sent "FYI?, or an opinion. 
The refuse, greet, and remind verbs occurred 
very infrequently in our data, and hence we did 
not attempt to learn classifiers for them (in this 
initial study). The primary reason for restricting 
ourselves in this way was our expectation that 
human annotators would be slower and less reli-
able if given a more complex taxonomy.  
The nouns in Figure 2 constitute possible ob-
jects for the email speech act verbs. The nouns 
fall into two broad categories. 
Information nouns are associated with email 
speech acts described by the verbs Deliver, Re-
mind and Amend, in which the email explicitly 
contains information. We also associate informa-
tion nouns with the verb Request, where the 
email contains instead a description of the needed 
information (e.g., "Please send your birthdate." 
versus "My birthdate is ?".  The request act is 
actually for a 'deliver information' activity). In-
formation includes data believed to be fact as 
well as opinions, and also attached data files. 
Activity nouns are generally associated with 
email speech acts described by the verbs Pro-
pose, Request, Commit, and Refuse.  Activities 
include meetings, as well as longer term activities 
such as committee memberships.   
Notice every email speech act is itself an ac-
tivity.  The <verb><noun> node in Figure 1 indi-
cates that any email speech act can also serve as 
the noun associated with some other email 
speech act.  For example, just as (deliver infor-
mation) is a legitimate speech act, so is (commit 
(deliver information)). Automatically construct-
ing such nested speech acts is an interesting and 
difficult topic; however, in the current paper we 
consider only the problem of determining top-
level the verb for such compositional speech acts. 
For instance, for a message containing a (commit 
(deliver information)) our goal would be to 
automatically detect the commit verb but not the 
inner (deliver information) compound noun. 
4 Categorization Results 
4.1 Corpora 
Although email is ubiquitous, large and realis-
tic email corpora are rarely available for research 
purposes.  The limited availability is largely due 
to privacy issues: for instance, in most US aca-
demic institutions, a users? email can only be dis-
tributed to researchers if all senders of the email 
also provided explicit written consent. 
The email corpora used in our experiments 
consist of four different email datasets collected 
from working groups who signed agreements to 
make their email accessible to researchers. The 
first three datasets, N01F3, N02F2, and N03F2 
are annotated subsets of a larger corpus, the 
CSpace email corpus, which contains approxi-
mately 15,000 email messages collected from a 
management course at Carnegie Mellon Univer-
sity. In this course, 277 MBA students, organized 
in approximately 50 teams of four to six mem-
bers, ran simulated companies in different market 
scenarios over a 14-week period (Kraut et al). 
N02F2, N01F3 and N03F2 are collections of all 
email messages written by participants from three 
different teams, and contain 351, 341 and 443 
different email messages respectively.  
The fourth dataset, the PW CALO corpus, was 
generated during a four-day exercise conducted 
at SRI specifically to generate an email corpus. 
During this time a group of six people assumed 
different work roles (e.g. project leader, finance 
manager, researcher, administrative assistant, etc) 
and performed a number of group activities.  
There are 222 email messages in this corpus. 
These email corpora are all task-related, and 
associated with a small working group, so it is 
not surprising that they contain many instances of 
the email acts described above?for instance, the 
CSpace corpora contain an average of about 1.3 
email verbs per message. Informal analysis of 
other personal inboxes suggests that this sort of 
email is common for many university users. We 
believe that negotiation of shared tasks is a cen-
tral use of email in many work environments.  
All messages were preprocessed by removing 
quoted material, attachments, and non-subject 
header information.  This preprocessing was per-
formed manually, but was limited to operations 
which can be reliably automated. The most diffi-
cult step is removal of quoted material, which we 
address elsewhere (Carvalho & Cohen, 2004). 
4.2 Inter-Annotator Agreement  
Each message may be annotated with several 
labels, as it may contain several speech acts.   To 
evaluate inter-annotator agreement, we double-
labeled N03F2 for the verbs Deliver, Commit, 
Request, Amend, and Propose, and the noun, 
Meeting, and computed the kappa statistic (Car-
letta, 1996) for each of these, defined as 
R
RA
?
?
=
1
?
 
where A is the empirical probability of agreement 
on a category, and R is the probability of agree-
ment for two annotators that label documents at 
random (with the empirically observed frequency 
of each label). Hence kappa ranges from -1 to +1. 
The results in Table 1 show that agreement is 
good, but not perfect. 
 
Email Act Kappa 
Meeting 0.82 
Deliver 0.75 
Commit 0.72 
Request 0.81 
Amend 0.83 
Propose 0.72 
Table 1 - Inter-Annotator Agreement on N03F2. 
We also took doubly-annotated messages 
which had only a single verb label and con-
structed the 5-class confusion matrix for the two 
annotators shown in Table 2. Note kappa values 
are somewhat higher for the shorter one-act mes-
sages. 
 
            Req Prop Amd Cmt Dlv kappa 
Req 55 0 0 0 0 0.97 
Prop 1 11 0 0 1 0.77 
Amd 0 1 15 0 0 0.87 
Cmt 1 3 1 24 4 0.78 
Dlv 1 0 2 3 135 0.91 
Table 2 - Inter-annotator agreement on documents 
with only one category. 
4.3 Learnability of Categories 
Representation of documents. To assess the 
types of message features that are most important 
for prediction, we adopted Support Vector Ma-
chines (Joachims, 2001) as our baseline learning 
method, and a TFIDF-weighted bag-of-words as 
a baseline representation for messages.  We then 
conducted a series of experiments with the 
N03F2 corpus only to explore the effect of dif-
ferent representations.   
NF032 Cmt Dlv Directive 
Baseline SVM 25.0 49.8 75.2 
no tfidf  47.3 58.4 74.6 
+bigrams 46.1 66.1 76.0 
+times 43.6 60.1 73.2 
+POSTags 48.6 61.8 75.4 
+personPhrases 41.2 61.1 73.4 
 
NF02F2 and NF01F3 Cmt Dlv Directive 
Baseline SVM 10.1 56.3 66.1 
All ?useful? features 42.0 64.0 73.3 
Table 3 ? F1 for different feature sets. 
 
We noted that the most discriminating words 
for most of these categories were common words, 
not the low-to-intermediate frequency words that 
are most discriminative in topical classification. 
This suggested that the TFIDF weighting was 
inappropriate, but that a bigram representation 
might be more informative. Experiments showed 
that adding bigrams to an unweighted bag of 
words representation slightly improved perform-
ance, especially on Deliver. These results are 
shown in Table 4 on the rows marked ?no tfidf? 
and ?bigrams?. (The TFIDF-weighted SVM is 
shown in the row marked ?baseline?, and the ma-
jority classifier in the row marked ?default?; all 
numbers are F1 measures on 10-fold cross-
validation.) Examination of messages suggested 
other possible improvements. Since much nego-
tiation involves timing, we ran a hand-coded ex-
tractor for time and date expressions on the data, 
and extracted as features the number of time ex-
pressions in a message, and the words that oc-
curred near a time (for instance, one such feature 
is ?the word ?before? appears near a time?). 
These results appear in the row marked ?times?.  
Similarly, we ran a part of speech (POS) tagger 
and added features for words appearing near a 
pronoun or proper noun (?personPhrases? in the 
table), and also added POS counts. 
To derive a final representation for each cate-
gory, we pooled all features that improved per-
formance over ?no tfidf? for that category.  We 
then compared performance of these document 
representations to the original TFIDF bag of 
words baseline on the (unexamined) N02F2 and 
N01F3 corpora.  As Table 3 shows, substantial 
improvement with respect to F1 and kappa was 
obtained by adding these additional features over 
the baseline representation. This result contrasts 
with previous experiments with bigrams for topi-
cal text classification (Scott & Matwin, 1999)  
and sentiment detection (Pang et al, 2002).  The 
difference is probably that in this task, more in-
formative words are potentially ambiguous: for 
instance, ?will you? and ?I will? are correlated 
with requests and commitments, respectively, but 
the individual words in these bigrams are less 
predictive. 
Learning methods.  In another experiment, 
we fixed the document representation to be un-
weighted word frequency counts and varied the 
learning algorithm. In these experiments, we 
pooled all the data from the four corpora, a total 
of 9602 features in the 1357 messages, and since 
the nouns and verbs are not mutually exclusive, 
we formulated the task as a set of several binary 
classification problems, one for each verb. 
The following learners were used from the 
Based on the MinorThird toolkit (Cohen, 2004). 
VP is an implementation of the voted perceptron 
algorithm (Freund & Schapire, 1999). DT is a 
simple decision tree learning system, which 
learns trees of depth at most five, and chooses 
splits to maximize the function ( )00112
?+?+ + WWWW  suggested by Schapire and 
Singer (1999) as an appropriate objective for 
?weak learners?. AB is an implementation of the 
confidence-rated boosting method described by 
Singer and Schapire (1999), used to boost the DT 
algorithm 10 times.  SVM is a support vector ma-
chine with a linear kernel (as used above). 
 
Act 
 VP AB SVM  DT 
Request 
(450/907) 
Error 
F1 
0.25 
0.58 
0.22 
0.65 
0.23 
0.64 
0.20 
0.69 
Proposal 
(140/1217) 
Error 
F1 
0.11 
0.19 
0.12 
0.26 
0.12 
0.44 
0.10 
0.13 
Delivery 
(873/484) 
Error 
F1 
0.26 
0.80 
0.28 
0.78 
0.27 
0.78 
0.30 
0.76 
Commit-
ment 
(208/1149) 
Error 
F1 
0.15 
0.21 
0.14 
0.44 
0.17 
0.47 
0.15 
0.11 
Directive 
(605/752) 
Error 
F1 
0.25 
0.72 
0.23 
0.73 
0.23 
0.73 
0.19 
0.78 
Commis-
sive 
(993/364) 
Error 
F1 
0.23 
0.84 
0.23 
0.84 
0.24 
0.83 
0.22 
0.85 
Meet 
(345/1012) 
Error 
F1 
0.187 
0.573 
0.17 
0.62 
0.14 
0.72 
0.18
0.60 
Table 4 ? Learning on the entire corpus. 
Table 4 reports the results on the most common 
verbs, using 5-fold cross-validation to assess ac-
curacy. One surprise was that DT (which we had 
intended merely as a base learner for AB) works 
surprisingly well for several verbs, while AB sel-
dom improves much over DT.  We conjecture 
that the bias towards large-margin classifiers that 
is followed by SVM, AB, and VP (and which has 
been so successful in topic-oriented text classifi-
cation) may be less appropriate for this task, per-
haps because positive and negative classes are 
not clearly separated (as suggested by substantial 
inter-annotator disagreement). 
Class:
 Commisive
 (Total: 1357 msgs)
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n Voted Perceptron
AdaBoost
SVM
Decision Tree
 
Figure 3 - Precision/Recall for Commissive act 
Further results are shown in Figure 3-5, which 
provide precision-recall curves for many of these 
classes. The lowest recall level in these graphs 
corresponds to the precision of random guessing. 
These graphs indicate that high-precision predic-
tions can be made for the top-level of the verb 
hierarchy, as well as verbs Request and Deliver, 
if one is willing to slightly reduce recall.  
Class:  Directive
(Total: 1357 msgs)
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
ci
s
io
n VotedPerceptron
AdaBoost
SVM
DecisionTree
 
Figure 4 - Precision/Recall for Directive act 
 
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n
Meet
Dlv
Req
AdaBoost Learner
(Total: 1357
 
messages)
 
Figure 5 - Precision/Recall of 3 different classes 
using AdaBoost 
 
 
Transferability. One important question in-
volves the generality of these classifiers: to what 
range of corpora can they be accurately applied?  
Is it possible to train a single set of email-act 
classifiers that work for many users, or is it nec-
essary to train individual classifiers for each 
user? To explore this issue we trained a DT clas-
sifier for Directive emails on the NF01F3 corpus, 
and tested it on the NF02F2 corpus; trained the 
same classifier on NF02F2 and tested it on 
NF01F3; and also performed a 5-fold cross-
validation experiment within each corpus.   
(NF02F2 and NF01F3 are for disjoint sets of us-
ers, but are approximately the same size.)  We 
then performed the same experiment with VP for 
Deliver verbs and SVM for Commit verbs (in 
each case picking the top-performing learner with 
respect to F1).  The results are shown in Table 5. 
  
 Test Data 
DT/Directive 1f3 2f2 
Train Data Error F1 Error F1 
1f3 25.1 71.6 16.4 72.8 
2f2 20.1 68.8 18.8 71.2 
VP/Deliver  
1f3 30.1 55.1 21.1 56.1 
2f2 35.0 25.4 21.1 35.7 
SVM/Commit  
1f3 23.4 39.7 15.2 31.6 
2f2 31.9 27.3 16.4 15.1 
Table 5 - Transferability of classifiers 
 
If learned classifiers were highly specific to a 
particular set of users, one would expect that the 
diagonal entries of these tables (the ones based 
on cross-validation within a corpus) would ex-
hibit much better performance than the off-
diagonal entries.  In fact, no such pattern is 
shown. For Directive verbs, performance is simi-
lar across all table entries, and for Deliver and 
Commit, it seems to be somewhat better to train 
on NF01F3 regardless of the test set. 
4.4 Future Directions 
None of the algorithms or representations dis-
cussed above take into account the context of an 
email message, which intuitively is important in 
detecting implicit speech acts.  A plausible notion 
of context is simply the preceding message in an 
email thread. 
Exploiting this context is non-trivial for sev-
eral reasons.  Detecting threads is difficult; al-
though email headers contain a ?reply-to? field, 
users often use the ?reply? mechanism to start 
what is intuitively a new thread.  Also, since 
email is asynchronous, two or more users may 
reply simultaneously to a message, leading to a 
thread structure which is a tree, rather than a se-
quence.  Finally, most sequential learning models 
assume a single category is assigned to each in-
stance?e.g., (Ratnaparkhi, 1999)?whereas our 
scheme allows multiple categories. 
Classification of emails according to our verb-
noun ontology constitutes a special case of a gen-
eral family of learning problems we might call 
factored classification problems, as the classes 
(email speech acts) are factored into two features 
(verbs and nouns) which jointly determine this 
class. A variety of real-world text classification 
problems can be naturally expressed as factored 
problems, and from a theoretical viewpoint, the 
additional structure may allow construction of 
new, more effective algorithms.   
For example, the factored classes provide a 
more elaborate structure for generative probabil-
istic models, such as those assumed by Na?ve 
Bayes. For instance, in learning email acts, one 
might assume words were drawn from a mixture 
distribution with one mixture component pro-
duces words conditioned on the verb class factor, 
and a second mixture component generates words 
conditioned on the noun (see Blei et al(2003) for 
a related mixture model).  Alternatively, models 
of the dependencies between the different factors 
(nouns and verbs) might also be used to improve 
classification accuracy, for instance by building 
into a classifier the knowledge that some nouns 
and verbs are incompatible.  
The fact that an email can contain multiple 
email speech acts almost certainly makes learn-
ing more difficult: in fact, disagreement between 
human annotators is generally higher for longer 
messages.  This problem could be addressed by 
more detailed annotation: rather than annotating 
each message with all the acts it contains, human 
annotators could label smaller message segments 
(say, sentences or paragraphs). An alternative to 
more detailed (and expensive) annotation would 
be to use learning algorithms that implicitly seg-
ment a message. As an example, another mixture 
model formulation might be used, in which each 
mixture component corresponds to a single verb 
category.    
5 Concluding Remarks 
We have presented an ontology of ?email 
speech acts? that is designed to capture some im-
portant properties of a central use of email: nego-
tiating and coordinating joint activities. Unlike 
previous attempts to combine speech act theory 
with email (Winograd, 1987; Flores and Ludlow, 
1980), we propose a system which passively ob-
serves email and automatically classifies it by 
intention. This reduces the burden on the users of 
the system, and avoids sacrificing the flexibility 
and socially desirable aspects of informal, natural 
language communication. 
This problem also raises a number of interest-
ing research issues. We showed that entity ex-
traction and part of speech tagging improves 
classifier performance, but leave open the ques-
tion of whether other types of linguistic analysis 
would be useful. Predicting speech acts requires 
context, which makes classification an inherently 
sequential task, and the labels assigned to mes-
sages have non-trivial structure; we also leave 
open the question of whether these properties can 
be effectively exploited. 
  Our experiments show that many categories 
of messages can be detected, with high precision 
and moderate recall, using existing text-
classification learning methods. This suggests 
that useful task-tracking tools could be con-
structed based on automatic classifiers?a poten-
tially important practical application. 
References 
S. Argamon, M. ?ari? and S. S. Stein. (2003). Style 
mining of electronic messages for multiple authorship 
discrimination: first results. Proceedings of the 9th 
ACM SIGKDD, Washington, D.C. 
 
V. Bellotti, N. Ducheneaut, M. Howard and I. Smith. 
(2003). Taking email to task: the design and evalua-
tion of a task management centered email tool. Pro-
ceedings of the Conference on Human Factors in 
Computing Systems, Ft. Lauderdale, Florida.  
 
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 
(2003).  Hierarchical topic models and the nested Chi-
nese restaurant process.  Advances in Neural Informa-
tion Processing Systems, 16, MIT Press. 
 
B. M. Camino, A. E. Millewski, D. R. Millen and T. 
M. Smith. (1998). Replying to email with structured 
responses. International Journal of Human-Computer 
Studies. Vol. 48, Issue 6, pp 763 ? 776. 
 
J. Carletta. (1996). Assessing Agreement on Classifi-
cation Tasks: The Kappa Statistic. Computational 
Linguistics, Vol. 22, No. 2, pp 249-254. 
 
V. R. Carvalho & W. W. Cohen (2004). Learning to 
Extract Signature and Reply Lines from Email.  To 
appear in Proc. of the 2004 Conference on Email and 
Anti-Spam. Mountain View, California. 
 
W. W. Cohen. (1996). Learning Rules that Classify E-
Mail. Proceedings of the 1996 AAAI Spring Sympo-
sium on Machine Learning and Information Access, 
Palo Alto, California. 
 
W. W. Cohen. (2004). Minorthird: Methods for Identi-
fying Names and Ontological Relations in Text using 
Heuristics for Inducing Regularities from Data, 
http://minorthird.sourceforge.net. 
 
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
fieldTomokiyo, T. Polzin, K. Ries, A. Waibel and K. 
Zechner. (1998). CLARITY: Inferring Discourse 
Structure from Speech. In Applying Machine Learn-
ing to Discourse Processing, AAAI'98. 
 
F. Flores, and J.J. Ludlow. (1980). Doing and Speak-
ing in the Office. In: G. Fick, H. Spraque Jr. (Eds.). 
Decision Support Systems: Issues and Challenges, 
Pergamon Press, New York, pp. 95-118. 
 
Y. Freund and R. Schapire. (1999). Large Margin 
Classification using the Perceptron Algorithm.  Ma-
chine Learning 37(3), 277?296. 
 
T. Joachims. (2001). A Statistical Learning Model of 
Text Classification with Support Vector Machines. 
Proc. of the Conference on Research and Develop-
ment in Information Retrieval (SIGIR), ACM, 2001. 
 
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. 
Espinosa. (under review). Coordination in teams: evi-
dence from a simulated management game. To appear 
in the Journal of Organizational Behavior. 
 
D. D. Lewis. (1992). Representation and Learning in 
Information Retrieval. PhD Thesis, No. 91-93, Com-
puter Science Dept., Univ of Mass at Amherst 
 
A. McCallum, D. Freitag and F. Pereira. (2000). 
Maximum Entropy Markov Models for Information 
Extraction and Segmentation. Proc. of the 17th Int?l 
Conf. on Machine Learning, Nashville, TN. 
 
B. Pang, L. Lee and S. Vaithyanathan. (2002). 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques. Proc. of the 2002 Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP), pp 79-86. 
 
A. E. Milewski and T. M. Smith. (1997). An Experi-
mental System For Transactional Messaging. Proc. of 
the international ACM  SIGGROUP conference on 
Supporting group work: the integration challenge, pp. 
325-330. 
 
H. Murakoshi, A. Shimazu and K. Ochimizu. (1999). 
Construction of Deliberation Structure in Email 
Communication,. Pacific Association for Computa-
tional Linguistics, pp. 16-28, Waterloo, Canada. 
 
A. Ratnaparkhi. (1999). Learning to Parse Natural 
Language with Maximum Entropy Models. Machine 
Learning, Vol. 34, pp. 151-175. 
  
J. D. M. Rennie. (2000). Ifile: An Application of Ma-
chine Learning to Mail Filtering. Proc. of the KDD-
2000 Workshop on Text Mining, Boston, MA. 
 
M. Sahami, S. Dumais, D. Heckerman and E. Horvitz. 
(1998). A Bayesian Approach to Filtering Junk E-
Mail. AAAI'98 Workshop on Learning for Text Cate-
gorization. Madison, WI. 
 
M. Schoop. (2001). An introduction to the language-
action perspective. SIGGROUP Bulletin, Vol. 22, No. 
2, pp 3-8. 
 
S. Scott and S. Matwin. (1999). Feature engineering 
for text classification. Proc. of 16th International Con-
ference on Machine Learning, Bled, Slovenia. 
 
J. R. Searle. (1975). A taxonomy of illocutionary acts.  
In K. Gunderson (Ed.), Language, Mind and Knowl-
edge, pp. 344-369.  Minneapolis: University of Min-
nesota Press. 
 
R. B. Segal and J. O. Kephart. (2000). Swiftfile: An 
intelligent assistant for organizing e-mail. In AAAI 
2000 Spring Symposium on Adaptive User Interfaces, 
Stanford, CA. 
 
Y. Yang. (1999). An Evaluation of Statistical Ap-
proaches to Text Categorization. Information Re-
trieval, Vol. 1, Numbers 1-2, pp 69-90. 
 
S. Wermter and M. L?chel. (1996). Learning dialog 
act processing. Proc. of the International Conference 
on Computational Linguistics, Kopenhagen, Denmark. 
 
R. E. Schapire and Y. Singer. (1998). Improved boost-
ing algorithms using confidence-rated predictions. The 
11th Annual Conference on Computational Learning 
Theory, Madison, WI. 
 
H. Wiegend, G. Goldkuhl, and A. de Moor. (2003). 
Proc. of the Eighth Annual Working Conference on 
Language-Action Perspective on Communication 
Modelling (LAP 2003), Tilburg, The Netherlands. 
 
J. Wiebe, R. Bruce, M. Bell, M. Martin and T. Wilson. 
(2001). A Corpus Study of Evaluative and Speculative 
Language.  Proceedings of the 2nd ACL SIGdial 
Workshop on Discourse and Dialogue. Aalborg, 
Denmark. 
 
T. Winograd. 1987. A Language/Action Perspective 
on the Design of Cooperative Work. Human-
Computer Interactions, 3:1, pp. 3-30. 
 
S. Whittaker, Q. Jones, B. Nardi, M. Creech, L. 
Terveen, E. Isaacs and J. Hainsworth. (in press). Us-
ing Personal Social Networks to Organize Communi-
cation in a Social desktop. To appear in Transactions 
on Human Computer Interaction. 
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 93?95,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Graph-Search Framework for GeneId Ranking
(Extended Abstract)
William W. Cohen
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA 15213
wcohen@cs.cmu.edu
1 Introduction
One step in the curation process is geneId finding?
the task of finding the database identifier of every
gene discussed in an article. GeneId-finding was
studied experimentally in the BioCreatIvE challenge
(Hirschman et al, 2005), which developed testbed
problems for each of three model organisms (yeast,
mice, and fruitflies). Here we consider geneId rank-
ing, a relaxation of geneId-finding in which the sys-
tem provides a ranked list of genes that might be
discussed by the document. We show how multi-
ple named entity recognition (NER) methods can
be combined into a single high-performance geneId-
ranking system.
2 Methods and Results
We focused on the mouse dataset, which was the
hardest for the BioCreatIvE participants. This
dataset consists of several parts. The gene synonym
list consists of 183,142 synonyms for 52,594 genes;
the training data consists of 100 mouse-relevant
Medline abstracts, associated with the MGI geneId?s
for those genes that are mentioned in the abstract;
the evaluation data consists of an additional 50
mouse-relevant Medline abstracts, also associated
with the MGI geneId?s as above; the test data con-
sists of an additional 250 mouse-relevant Medline
abstracts, again associated with MGI geneId?s; fi-
nally the historical data consists of 5000 mouse-
relevant Medline abstracts, each of which is associ-
ated with the MGI geneId?s for all genes which are
(a) associated with the article according to the MGI
database, and (b) mentioned in the abstract, as deter-
mined by an automated procedure based on the gene
synonym list.1 We also annotated the evaluation-
data for NER evaluation.
We used two closely related gene-protein NER
systems in our experiments, both trained using
Minorthird (Min, 2004) on the YAPEX corpus
(Franze?n et al, 2002). The likely-protein extractor
was designed to have high precision and lower re-
call, and the possible-protein extractor was designed
to have high recall and lower precision. As shown in
Table 1, the likely-protein extractor performs well
on the YAPEX test set, but neither system performs
well on the mouse evaluation data?here, they per-
form only comparably to exact matching against the
synonym dictionary. This performance drop is typ-
ical when learning-based NER systems are tested
on data from a statistical distribution different from
their training set.
As a baseline for geneId-ranking, we used a string
similarity metric called soft TFIDF, as implemented
in the SecondString open-source software package
(Cohen and Ravikumar, 2003), and soft-matched ex-
tracted gene names against the synonym list. Ta-
ble 2 shows the mean average precision on the eval-
uation data. Note that the geneId ranker based on
possible-protein performs statistically significantly
better2 than the one based on likely-protein, even
though possible-protein has a lower F score.
To combine these two NER systems, we represent
all information as a labeled directed graph which in-
1The training data and evaluation data are subsets of the
BioCreatIvE ?devtest? set. The historical data was called ?train-
ing data? in the BioCreatIvE publications. The test data is the
same as the blind test set used in BioCreatIvE.
2With z = 3.1, p > 0.995 using a two-tailed paired test.
93
Precis. Recall F
mouse eval
likely-prot 0.667 0.268 0.453
possible-prot 0.304 0.566 0.396
dictionary 0.245 0.439 0.314
YAPEX test
likely-prot 0.872 0.621 0.725
YAPEX system 0.678 0.664 0.671
Table 1: Performance of the NER systems on the
mouse evaluation corpus and the YAPEX test cor-
pus.
Mean Average
Precision (MAP)
mouse evaluation data
likely-prot + softTFIDF 0.450
possible-prot + softTFIDF 0.626
graph-based ranking 0.513
+ extra links 0.730
+ extra links & learning 0.807
Table 2: Mean average precision of several geneId-
ranking methods on the 50 abstracts from the mouse
evaluation dataset.
cludes the test abstracts, the extracted names, the
synonym list, and the historical data. We then use
proximity in a graph for ranking. The graph used
is illustrated in Figure 1. Nodes in this graph can
be either files, strings, terms, or user-defined types.
Abstracts and gene synonyms are represented as file
and string nodes, respectively. Files are linked to
the terms (i.e., the words) that they contain, and
terms are linked to the files that contain them.3 File
nodes are also linked to string nodes corresponding
to the output of an NER system on that file. (String
nodes are simply short files.) The graph also con-
tains geneId nodes and synonym string nodes cre-
ated from the dictionary, and for each historical-data
abstract, we include links to its associated geneId
nodes.
Given this graph, gene identifiers for an abstract
are generated by traversing the graph away from the
abstract node, and looking for geneId nodes that are
?close? to the abstract according to a certain proxim-
3In fact, all edges have inverses in the graph.
Figure 1: Part of a simplified version of the graph
used for geneId ranking.
ity measure for nodes. Similarity between two nodes
is defined by a lazy walk process, similar to PageR-
ank with decay. The details of this are described in
the full paper and elsewhere (Minkov et al, 2006).
Intuitively, however, this measures the similarity of
two nodes by the weighted sum of all paths that con-
nect the nodes, where shorter paths will be weighted
exponentially higher than longer paths. One conse-
quence of this measure is that information associ-
ated with paths like the one on the left-hand side of
the graph?which represents a soft-match between a
likely-protein and a synonym?can be reinforced by
other types of paths, like the one on the right-hand
side of the figure.
As shown in Table 2, the graph-based approach
has performance intermediate between the two base-
line systems. However, the baseline approaches in-
clude some information which is not available in the
graph, e.g., the softTFIDF distances, and the implicit
knowledge of the ?importance? of paths from an ab-
stract to a synonym via an NER-extracted string. To
include this information, we inserted extra edges la-
beled proteinToSynonym between the extracted pro-
tein strings x and comparable synonyms y, and also
?short-cut? edges in the graph that directly link ab-
stracts x to geneId nodes reachable via one of the
?important? paths described above.
As Table 2 shows, graph search with the aug-
mented graph does indeed improve MAP perfor-
mance on the mouse evaluation data: performance
is better than the simple graph, and also better than
94
MAP Avg Max F
mouse test data
likely-prot + softTFIDF 0.368 0.421
possible-prot + softTFIDF 0.611 0.672
graph-based ranking 0.640 0.695
+ extra links & learning 0.711 0.755
Table 3: Mean average precision of several geneId-
ranking methods on the 250 abstracts from the
mouse test dataset.
either of the baseline methods described above.
Finally we extended the lazy graph walk to pro-
duce, for each node x reached on the walk, a feature
vector summarizing the walk. Intuitively, the fea-
ture vector records certain features of each edge in
the graph, weighting these features according to the
probability of traversing the edge. We then use a
learning-to-rank method (Collins and Duffy, 2002)
to rerank the top 100 nodes. Table 2 shows that
learning improves performance. In combination, the
techniques described have improved MAP perfor-
mance to 0.807, an improvement of nearly 80% over
the most natural baseline (i.e., soft-matching the dic-
tionary to the NER method with the best F measure).
As a final prospective test, we applied these meth-
ods to the 250-abstract mouse test data. We com-
pared their performance to the graph-based search
method combined with a reranking postpass learned
from the 100-abstract mouse training data. The per-
formance of these methods is summarized in Ta-
ble 3. The somewhat lower performance is proba-
bly due to variation in the two samples.4 We also
computed the maximal F-measure (over any thresh-
old) of each ranked list produced, and then averaged
these measures over all queries. This is compara-
ble to the best F1 scores in the BioCreatIvE work-
shop, although the averaging for BioCreatIvE was
done differently.
3 Conclusion
We evaluate several geneId-ranking systems, in
which an article is associated with a ranked list of
possible gene identifiers. We find that, when used
4For instance, the test-set abstracts contain somewhat more
proteins on average (2.2 proteins/abstract) than the evaluation-
set abstracts (1.7 proteins/abstract).
in the most natural manner, the F-measure perfor-
mance of an NER systems does not correlate well
with MAP of the geneId-ranker based on it: rather,
the NER system with higher recall, but lower overall
performance, has significantly better performance
when used for geneId-ranking.
We also present a graph-based scheme for com-
bining NER systems, which allows many types of
information to be combined. Combining this sys-
tem with learning produces performance much bet-
ter than either NER system can achieve alone. On
average, 68% of the correct proteins will be found in
the top two elements of the list, 84% will be found
in the top five elements, and more than 90% will
be found in the top ten elements. This level of per-
formance is probably good enough to be of use in
curation.
Acknowledgement
The authors with to thank the organizers of BioCre-
atIvE, Bob Murphy, Tom Mitchell, and Einat
Minkov. The work described here is supported by
NIH K25 grant DA017357-01.
References
William W. Cohen and Pradeep Ravikumar. 2003. Sec-
ondString: An open-source Java toolkit of approxi-
mate string-matching techniques. Project web page,
http://secondstring.sourceforge.net.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of the ACL.
Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, Lars
Asker Per Lide?n, and Joakim Coster. 2002. Protein names
and how to find them. International Journal of Medical In-
formatics, 67(1-3):49?61.
Lynette Hirschman, Alexander Yeh, Christian Blaschke, and
Alfonso Valencia. 2005. Overview of BioCreAtIvE: criti-
cal assessment of information extraction for biology. BMC
Bioinformatics, 6(S1).
2004. Minorthird: Methods for identifying names and ontolog-
ical relations in text using heuristics for inducing regularities
from data. http://minorthird.sourceforge.net.
Einat Minkov, William Cohen, and Andrew Ng. 2006. A graph
framework for contextual search and name disambiguation
in email. In SIGIR ?06: Proceedings of the 29th annual in-
ternational ACM SIGIR conference on research and devel-
opment in information retrieval, August. To appear.
95
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 35?41,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Improving ?Email Speech Acts? Analysis via N-gram Selection
Vitor R. Carvalho
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA
vitor@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA
wcohen@cs.cmu.edu
Abstract
In email conversational analysis, it is of-
ten useful to trace the the intents behind
each message exchange. In this paper,
we consider classification of email mes-
sages as to whether or not they contain
certain intents or email-acts, such as ?pro-
pose a meeting? or ?commit to a task?.
We demonstrate that exploiting the con-
textual information in the messages can
noticeably improve email-act classifica-
tion. More specifically, we describe a
combination of n-gram sequence features
with careful message preprocessing that is
highly effective for this task. Compared
to a previous study (Cohen et al, 2004),
this representation reduces the classifica-
tion error rates by 26.4% on average. Fi-
nally, we introduce Ciranda: a new open
source toolkit for email speech act predic-
tion.
1 Introduction
One important use of work-related email is negoti-
ating and delegating shared tasks and subtasks. To
provide intelligent email automated assistance, it is
desirable to be able to automatically detect the intent
of an email message?for example, to determine if
the email contains a request, a commitment by the
sender to perform some task, or an amendment to an
earlier proposal. Successfully adding such a seman-
tic layer to email communication is still a challenge
to current email clients.
In a previous work, Cohen et al (2004) used text
classification methods to detect ?email speech acts?.
Based on the ideas from Speech Act Theory (Searle,
1975) and guided by analysis of several email cor-
pora, they defined a set of ?email acts? (e.g., Re-
quest, Deliver, Propose, Commit) and then classified
emails as containing or not a specific act. Cohen et
al. (2004) showed that machine learning algorithms
can learn the proposed email-act categories reason-
ably well. It was also shown that there is an accept-
able level of human agreement over the categories.
A method for accurate classification of email into
such categories would have many potential appli-
cations. For instance, it could be used to help
users track the status of ongoing joint activities, im-
proving task delegation and coordination. Email
speech acts could also be used to iteratively learn
user?s tasks in a desktop environment (Khoussainov
and Kushmerick, 2005). Email acts classification
could also be applied to predict hierarchy positions
in structured organizations or email-centered teams
(Leusky, 2004); predicting leadership positions can
be useful to analyze behavior in teams without an
explicitly assigned leader.
By using only single words as features, Cohen et
al. (2004) disregarded a very important linguistic as-
pect of the speech act inference task: the textual
context. For instance, the specific sequence of to-
kens ?Can you give me? can be more informative to
detect a Request act than the words ?can?, ?you?,
?give? and ?me? separately. Similarly, the word se-
quence ?I will call you? may be a much stronger in-
dication of a Commit act than the four words sep-
arately. More generally, because so many specific
35
sequence of words (or n-grams) are inherently as-
sociated with the intent of an email message, one
would expect that exploiting this linguistic aspect
of the messages would improve email-act classifi-
cation.
In the current work we exploit the linguistic as-
pects of the problem by a careful combination of n-
gram feature extraction and message preprocessing.
After preprocessing the messages to detect entities,
punctuation, pronouns, dates and times, we gener-
ate a new feature set by extracting all possible term
sequences with a length of 1, 2, 3, 4 or 5 tokens.
Using this n-gram based representation in classi-
fication experiments, we obtained a relative average
drop of 26.4% in error rate when compared to the
original Cohen et al (2004) paper. Also, ranking
the most ?meaningful? n-grams based on Informa-
tion Gain score (Yang and Pedersen, 1997) revealed
an impressive agreement with the linguistic intuition
behind the email speech acts.
We finalize this work introducing Ciranda: an
open source package for Email Speech Act predic-
tion. Among other features, Ciranda provides an
easy interface for feature extraction and feature se-
lection, outputs the prediction confidence, and al-
lows retraining using several learning algorithms.
2 ?Email-Acts? Taxonomy and
Applications
A taxonomy of speech acts applied to email com-
munication (email-acts) is described and motivated
in (Cohen et al, 2004). The taxonomy was divided
into verbs and nouns, and each email message is rep-
resented by one or more verb-noun pairs. For exam-
ple, an email proposing a meeting and also request-
ing a project report would have the labels Propose-
Meeting and Request-Data.
The relevant part of the taxonomy is shown in Fig-
ure 1. Very briefly, a Request asks the recipient to
perform some activity; a Propose message proposes
a joint activity (i.e., asks the recipient to perform
some activity and commits the sender); a Commit
message commits the sender to some future course
of action; Data is information, or a pointer to infor-
mation, delivered to the recipient; and a Meeting is a
joint activity that is constrained in time and (usually)
space.
Several possible verbs/nouns were not considered
here (such as Refuse, Greet, and Remind), either be-
cause they occurred very infrequently in the corpus,
or because they did not appear to be important for
task-tracking. The most common verbs found in the
labeled datasets were Deliver, Request, Commit, and
Propose, and the most common nouns were Meet-
ing and deliveredData (abbreviated as dData hence-
forth).
In our modeling, a single email message may have
multiple verbs-nouns pairs.
Figure 1: Taxonomy of email-acts used in experi-
ments. Shaded nodes are the ones for which a clas-
sifier was constructed.
Cohen et al (2004) showed that machine learn-
ing algorithms can learn the proposed email-act cat-
egories reasonably well. It was also shown that
there is an acceptable level of human agreement
over the categories. In experiments using different
human annotators, Kappa values between 0.72 and
0.85 were obtained. The Kappa statistic (Carletta,
1996) is typically used to measure the human inter-
rater agreement. Its values ranges from -1 (com-
plete disagreement) to +1 (perfect agreement) and
it is defined as (A-R)/(1-R), where A is the empiri-
cal probability of agreement on a category, and R is
the probability of agreement for two annotators that
36
label documents at random (with the empirically ob-
served frequency of each label).
3 The Corpus
The CSpace email corpus used in this paper con-
tains approximately 15,000 email messages col-
lected from a management course at Carnegie Mel-
lon University. This corpus originated from work-
ing groups who signed agreements to make certain
parts of their email accessible to researchers. In this
course, 277 MBA students, organized in approxi-
mately 50 teams of four to six members, ran sim-
ulated companies in different market scenarios over
a 14-week period (Kraut et al, ). The email tends to
be very task-oriented, with many instances of task
delegation and negotiation.
Messages were mostly exchanged with members
of the same team. Accordingly, we partitioned the
corpus into subsets according to the teams. The 1F3
team dataset has 351 messages total, while the 2F2,
3F2, 4F4 and 11F1 teams have, respectively, 341,
443, 403 and 176 messages. All 1716 messages
were labeled according to the taxonomy in Figure
1.
4 N-gram Features
In this section we detail the preprocessing step and
the feature selection applied to all email acts.
4.1 Preprocessing
Before extracting the n-grams features, a sequence
of preprocessing steps was applied to all email mes-
sages in order to emphasize the linguistic aspects of
the problem. Unless otherwise mentioned, all pre-
processing procedures were applied to all acts.
Initially, forwarded messages quoted inside email
messages were deleted. Also, signature files and
quoted text from previous messages were removed
from all messages using a technique described else-
where (Carvalho and Cohen, 2004). A similar clean-
ing procedure was executed by Cohen et al (2004).
Some types of punctuation marks (?,;:.)(][?) were
removed, as were extra spaces and extra page
breaks. We then perform some basic substitutions
such as: from ??m? to ? am?, from ??re? to ? are?,
from ??ll? to ? will?, from ?won?t? to ?will not?,
from ?doesn?t? to ?does not? and from ??d? to ?
would?.
Any sequence of one or more numbers was re-
placed by the symbol ?[number]?. The pattern
?[number]:[number]? was replaced with ?[hour]?.
The expressions ?pm or am? were replaced by
?[pm]?. ?[wwhh]? denoted the words ?why, where,
who, what or when?. The words ?I, we, you, he,
she or they? were replaced by ?[person]?. Days
of the week (?Monday, Tuesday, ..., Sunday?) and
their short versions (i.e., ?Mon, Tue, Wed, ..., Sun?)
were replaced by ?[day]?. The words ?after, before
or during? were replaced by ?[aaafter]?. The pro-
nouns ?me, her, him, us or them? were substituted by
?[me]?. The typical filename types ?.doc, .xls, .txt,
.pdf, .rtf and .ppt? were replaced by ?.[filetype]?. A
list with some of these substitutions is illustrated in
Table 1.
Symbol Pattern
[number] any sequence of numbers
[hour] [number]:[number]
[wwhh] ?why, where, who, what, or when?
[day] the strings ?Monday, Tuesday, ..., or Sunday?
[day] the strings ?Mon, Tue, Wed, ..., or Sun?
[pm] the strings ?P.M., PM, A.M. or AM?
[me] the pronouns ?me, her, him, us or them?
[person] the pronouns ?I, we, you, he, she or they?
[aaafter] the strings ?after, before or during?
[filetype] the strings ?.doc, .pdf, .ppt, .txt, or .xls?
Table 1: Some PreProcessing Substitution Patterns
For the Commit act only, references to the first
person were removed from the symbol [person] ?
i.e., [person] was used to replace ?he, she or they?.
The rationale is that n-grams containing the pronoun
?I? are typically among the most meaningful for this
act (as shall be detailed in Section 4.2).
4.2 Most Meaningful N-grams
After preprocessing the 1716 email messages, n-
gram sequence features were extracted. In this pa-
per, n-gram features are all possible sequences of
length 1 (unigrams or 1-gram), 2 (bigram or 2-
gram), 3 (trigram or 3-gram), 4 (4-gram) and 5 (5-
gram) terms. After extracting all n-grams, the new
dataset had more than 347500 different features. It
would be interesting to know which of these n-grams
are the ?most meaningful? for each one of email
speech acts.
37
1-gram 2-gram 3-gram 4-gram 5-gram
? do [person] [person] need to [wwhh] do [person] think [wwhh] do [person] think ?
please ? [person] [wwhh] do [person] do [person] need to let [me] know [wwhh] [person]
[wwhh] could [person] let [me] know and let [me] know a call [number]-[number]
could [person] please would [person] call [number]-[number] give [me] a call [number]
do ? thanks do [person] think would be able to please give give [me] a call
can are [person] are [person] meeting [person] think [person] need [person] would be able to
of can [person] could [person] please let [me] know [wwhh] take a look at it
[me] need to do [person] need do [person] think ? [person] think [person] need to
Table 2: Request Act:Top eight N-grams Selected by Information Gain.
One possible way to accomplish this is using
some feature selection method. By computing the
Information Gain score (Forman, 2003; Yang and
Pedersen, 1997) of each feature, we were able to
rank the most ?meaningful? n-gram sequence for
each speech act. The final rankings are illustrated
in Tables 2 and 3.
Table 2 shows the most meaningful n-grams for
the Request act. The top features clearly agree with
the linguistic intuition behind the idea of a Request
email act. This agreement is present not only in
the frequent 1g features, but also in the 2-grams,
3-grams, 4-grams and 5-grams. For instance, sen-
tences such as ?What do you think ?? or ?let me
know what you ...? can be instantiations of the top
two 5-grams, and are typically used indicating a re-
quest in email communication.
Table 3 illustrates the top fifteen 4-grams for all
email speech acts selected by Information Gain. The
Commit act reflects the general idea of agreeing to
do some task, or to participate in some meeting. As
we can see, the list with the top 4-grams reflects the
intuition of commitment very well. When accepting
or committing to a task, it is usual to write emails
using ?Tomorrow is good for me? or ?I will put the
document under your door? or ?I think I can finish
this task by 7? or even ?I will try to bring this to-
morrow?. The list even has some other interesting
4-grams that can be easily associated to very specific
commitment situations, such as ?I will bring copies?
and ?I will be there?.
Another act in Table 3 that visibly agrees with
its linguistic intuition is Meeting. The 4-grams
listed are usual constructions associated with ei-
ther negotiating a meeting time/location (?[day] at
[hour][pm]?), agreeing to meet (?is good for [me]?)
or describing the goals of the meeting (?to go over
the?).
The top features associated with the dData act in
Table 3 are also closely related to its general intu-
ition. Here the idea is delivering or requesting some
data: a table inside the message, an attachment, a
document, a report, a link to a file, a url, etc. And
indeed, it seems to be exactly the case in Table 3:
some of the top 4-grams indicate the presence of an
attachment (e.g., ?forwarded message begins here?),
some features suggest the address or link where a file
can be found (e.g., ?in my public directory? or ?in
the etc directory?), some features request an action
to access/read the data (e.g., ?please take a look?)
and some features indicate the presence of data in-
side the email message, possibly formatted as a table
(e.g., ?[date] [hour] [number] [number]? or ?[date]
[day] [number] [day]?).
From Table 3, the Propose act seems closely re-
lated to the Meeting act. In fact, by checking the
labeled dataset, most of the Proposals were associ-
ated with Meetings. Some of the features that are not
necessarily associated with Meeting are ? [person]
would like to?, ?please let me know? and ?was hop-
ing [person] could?.
The Deliver email speech act is associated with
two large sets of actions: delivery of data and deliv-
ery of information in general. Because of this gener-
ality, is not straightforward to list the most meaning-
ful n-grams associated with this act. Table 3 shows
a variety of features that can be associated with a
Deliver act. As we shall see in Section 5, the De-
liver act has the highest error rate in the classifica-
tion task.
In summary, selecting the top n-gram features
via Information Gain revealed an impressive agree-
ment with the linguistic intuition behind the differ-
ent email speech acts.
38
Request Commit Meeting
[wwhh] do [person] think is good for [me] [day] at [hour] [pm]
do [person] need to is fine with [me] on [day] at [hour]
and let [me] know i will see [person] [person] can meet at
call [number]-[number] i think i can [person] meet at [hour]
would be able to i will put the will be in the
[person] think [person] need i will try to is good for [me]
let [me] know [wwhh] i will be there to meet at [hour]
do [person] think ? will look for [person] at [hour] in the
[person] need to get $[number] per person [person] will see [person]
? [person] need to am done with the meet at [hour] in
a copy of our at [hour] i will [number] at [hour] [pm]
do [person] have any [day] is fine with to go over the
[person] get a chance each of us will [person] will be in
[me] know [wwhh] i will bring copies let?s plan to meet
that would be great i will do the meet at [hour] [pm]
dData Propose Deliver
? forwarded message begins [person] would like to forwarded message begins here
forwarded message begins here would like to meet [number] [number] [number] [number]
is in my public please let [me] know is good for [me]
in my public directory to meet with [person] if [person] have any
[person] have placed the [person] meet at [hour] if fine with me
please take a look would [person] like to in my public directory
[day] [hour] [number] [number] [person] can meet tomorrow [person] will try to
[number] [day] [number] [hour] an hour or so is in my public
[date] [day] [number] [day] meet at [hour] in will be able to
in our game directory like to get together just wanted to let
in the etc directory [hour] [pm] in the [pm] in the lobby
the file name is [after] [hour] or [after] [person] will be able
is in our game [person] will be available please take a look
fyi ? forwarded message think [person] can meet can meet in the
just put the file was hoping [person] could [day] at [hour] is
my public directory under do [person] want to in the commons at
Table 3: Top 4-grams Selected by Information Gain
5 Experiments
Here we describe how the classification experiments
on the email speech acts dataset were carried out.
Using all n-gram features, we performed 5-fold
crossvalidation tests over the 1716 email messages.
Linear SVM1 was used as classifier. Results are il-
lustrated in Figure 2.
Figure 2 shows the test error rate of four dif-
ferent experiments (bars) for all email acts. The
first bar denotes the error rate obtained by Cohen
et al (2004) in a 5-fold crossvalidation experiment,
also using linear SVM. Their dataset had 1354 email
messages, and only 1-gram features were extracted.
The second bar illustrates the error rate obtained
using only 1-gram features with additional data. In
this case, we used 1716 email messages. The third
bar represents the the same as the second bar (1-
1We used the LIBSVM implementation (Chang and Lin,
2001) with default parameters.
gram features with 1716 messages), with the differ-
ence that the emails went through the preprocessing
procedure previously described.
The fourth bar shows the error rate when all 1-
gram, 2-gram and 3-gram features are used and the
1716 messages go through the preprocessing proce-
dure. The last bar illustrates the error rate when all
n-gram features (i.e., 1g+2g+3g+4g+5g) are used in
addition to preprocessing in all 1716 messages.
In all acts, a consistent improvement in 1-gram
performance is observed when more data is added,
i.e., a drop in error rate from the first to the sec-
ond bar. Therefore, we can conclude that Cohen et
al. (2004) could have obtained better results if they
had used more labeled data.
A comparison between the second and third bars
reveals the extent to which preprocessing seems to
help classification based on 1-grams only. As we
can see, no significant performance difference can
be observed: for most acts the relative difference is
39
Figure 2: Error Rate 5-fold Crossvalidation Experiment
very small, and in one or maybe two acts some small
improvement can be noticed.
A much larger performance improvement can be
seen between the fourth and third bars. This reflects
the power of the contextual features: using all 1-
grams, 2-grams and 3-grams is considerably more
powerful than using only 1-gram features. This
significant difference can be observed in all acts.
Compared to the original values from (Cohen et
al., 2004), we observed a relative error rate drop of
24.7% in the Request act, 33.3% in the Commit act,
23.7% for the Deliver act, 38.3% for the Propose
act, 9.2% for Meeting and 29.1% in the dData act.
In average, a relative improvement of 26.4% in error
rate.
We also considered adding the 4-gram and 5-gram
features to the best system. As pictured in the last
bar of Figure 2, this addition did not seem to im-
prove the performance and, in some cases, even a
small increase in error rate was observed. We be-
lieve this was caused by the insufficient amount of
labeled data in these tests; and the 4-gram and 5-
gram features are likely to improve the performance
of this system if more labeled data becomes avail-
able.
Precision versus recall curves of the Request act
classification task are illustrated in Figure 3. The
curve on the top shows the Request act performance
when the preprocessing step cues and n-grams pro-
posed in Section 4 are applied. For the bottom curve,
only 1g features were used. These two curves corre-
spond to the second bar (bottom curve) and forth bar
(top curve) in Figure 2. Figure 3 clearly shows that
both recall and precision are improved by using the
contextual features.
To summarize, these results confirm the intuition
that contextual information (n-grams) can be very
effective in the task of email speech act classifica-
tion.
40
Figure 3: Precision versus Recall of the Request Act
Classification
6 The Ciranda Package
Ciranda is an open source package for Email Speech
Act prediction built on the top of the Minorthird
package (Cohen, 2004). Among other features,
Ciranda allows customized feature engineering, ex-
traction and selection. Email Speech Act classi-
fiers can be easily retrained using any learning al-
gorithm from the Minorthird package. Ciranda is
currently available from http://www.cs.cmu.
edu/?vitor.
7 Conclusions
In this work we considered the problem of automat-
ically detecting the intents behind email messages
using a shallow semantic taxonomy called ?email
speech acts? (Cohen et al, 2004). We were in-
terested in the task of classifying whether or not
an email message contains acts such as ?propose a
meeting? or ?deliver data?.
By exploiting contextual information in emails
such as n-gram sequences, we were able to notice-
ably improve the classification performance on this
task. Compared to the original study (Cohen et al,
2004), this representation reduced the classification
error rates by 26.4% on average. Improvements of
more than 30% were observed for some acts (Pro-
pose and Commit).
We also showed that the selection of the top n-
gram features via Information Gain revealed an im-
pressive agreement with the linguistic intuition be-
hind the different email speech acts.
References
[Carletta1996] Jean Carletta. 1996. Assessing agreement
on classification tasks: The kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
[Carvalho and Cohen2004] Vitor R. Carvalho and
William W. Cohen. 2004. Learning to extract signa-
ture and reply lines from email. In Proceedings of the
Conference on Email and Anti-Spam, Palo Alto, CA.
[Chang and Lin2001] Chih-Chung Chang and Chih-
Jen Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Cohen et al2004] William W. Cohen, Vitor R. Carvalho,
and Tom M. Mitchell. 2004. Learning to classify
email into ?speech acts?. In Proceedings of Empiri-
cal Methods in Natural Language Processing, pages
309?316, Barcelona, Spain, July.
[Cohen2004] William W. Cohen, 2004. Minorthird:
Methods for Identifying Names and Ontological Re-
lations in Text using Heuristics for Inducing Reg-
ularities from Data. http://minorthird.
sourceforge.net.
[Forman2003] George Forman. 2003. An extensive em-
pirical study of feature selection metrics for text classi-
fication. The Journal of Machine Learning Research,
3:1289?1305.
[Khoussainov and Kushmerick2005] Rinat Khoussainov
and Nicholas Kushmerick. 2005. Email task man-
agement: An iterative relational learning approach. In
Conference on Email and Anti-Spam (CEAS?2005).
[Kraut et al] R.E. Kraut, S.R. Fussell, F.J. Lerch, and
A. Espinosa. Coordination in teams: Evidence from
a simulated management game. To appear in the Jour-
nal of Organizational Behavior.
[Leusky2004] Anton Leusky. 2004. Email is a stage:
Discovering people roles from email archives. In ACM
Conference on Research and Development in Informa-
tion Retrieval (SIGIR).
[Searle1975] J. R. Searle. 1975. A taxonomy of illo-
cutionary acts. In In K. Gunderson (Ed.), Language,
Mind and Knowledge., pages 344?369, Minneapolis,
MN. University of Minnesota Press.
[Yang and Pedersen1997] Yiming Yang and Jan O. Peder-
sen. 1997. A comparative study on feature selection in
text categorization. In Proceedings of ICML-97, 14th
International Conference on Machine Learning, pages
412?420.
41
Workshop on TextGraphs, at HLT-NAACL 2006, pages 1?8,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Graphical Framework for Contextual Search and Name Disambiguation
in Email
Einat Minkov
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
einatm@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Andrew Y. Ng
Computer Science Dept.
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
Similarity measures for text have histor-
ically been an important tool for solving
information retrieval problems. In this pa-
per we consider extended similarity met-
rics for documents and other objects em-
bedded in graphs, facilitated via a lazy
graph walk. We provide a detailed in-
stantiation of this framework for email
data, where content, social networks and
a timeline are integrated in a structural
graph. The suggested framework is evalu-
ated for the task of disambiguating names
in email documents. We show that rerank-
ing schemes based on the graph-walk sim-
ilarity measures often outperform base-
line methods, and that further improve-
ments can be obtained by use of appropri-
ate learning methods.
1 Introduction
Many tasks in information retrieval can be per-
formed by clever application of textual similarity
metrics. In particular, The canonical IR problem of
ad hoc retrieval is often formulated as the task of
finding documents ?similar to? a query. In modern
IR settings, however, documents are usually not iso-
lated objects: instead, they are frequently connected
to other objects, via hyperlinks or meta-data. (An
email message, for instance, is connected via header
information to other emails in the same thread and
also to the recipient?s social network.) Thus it is
important to understand how text-based document
similarity measures can be extended to documents
embedded in complex structural settings.
Our similarity metric is based on a lazy graph
walk, and is closely related to the well-known
PageRank algorithm (Page et al, 1998). PageRank
and its variants are based on a graph walk of infi-
nite length with random resets. In a lazy graph walk,
there is a fixed probability of halting the walk at each
step. In previous work (Toutanova et al, 2004), lazy
walks over graphs were used for estimating word
dependency distributions: in this case, the graph
was one constructed especially for this task, and the
edges in the graph represented different flavors of
word-to-word similarity. Other recent papers have
also used walks over graphs for query expansion (Xi
et al, 2005; Collins-Thompson and Callan, 2005).
In these tasks, the walk propagates similarity to a
start node through edges in the graph?incidentally
accumulating evidence of similarity over multiple
connecting paths.
In contrast to this previous work, we consider
schemes for propogating similarity across a graph
that naturally models a structured dataset like an
email corpus: entities correspond to objects includ-
ing email addresses and dates, (as well as the usual
types of documents and terms), and edges corre-
spond to relations like sent-by. We view the simi-
larity metric as a tool for performing search across
this structured dataset, in which related entities that
are not directly similar to a query can be reached via
multi-step graph walk.
In this paper, we formulate and evaluate this ex-
tended similarity metric. The principal problem we
1
consider is disambiguating personal names in email,
which we formulate as the task of retrieving the per-
son most related to a particular name mention. We
show that for this task, the graph-based approach im-
proves substantially over plausible baselines. After
retrieval, learning can be used to adjust the ranking
of retrieved names based on the edges in the paths
traversed to find these names, which leads to an ad-
ditional performance improvement. Name disam-
biguation is a particular application of the suggested
general framework, which is also applicable to any
real-world setting in which structural data is avail-
able as well as text.
This paper proceeds as follows. Sections 2 and
3 formalize the general framework and its instanti-
ation for email. Section 4 gives a short summary
of the learning approach. Section 5 includes experi-
mental evaluation, describing the corpora and results
for the person name disambiguation task. The paper
concludes with a review of related work, summary
and future directions.
2 Email as a Graph
A graph G consists of a set of nodes, and a set of la-
beled directed edges. Nodes will be denoted by let-
ters like x, y, or z, and we will denote an edge from
x to y with label ` as x `?? y. Every node x has
a type, denoted T (x), and we will assume that there
are a fixed set of possible types. We will assume for
convenience that there are no edges from a node to
itself (this assumption can be easily relaxed.)
We will use these graphs to represent real-world
data. Each node represents some real-world entity,
and each edge x `?? y asserts that some binary
relation `(x, y) holds. The entity types used here
to represent an email corpus are shown in the left-
most column of Table 1. They include the tradi-
tional types in information retrieval systems, namely
file and term. In addition, however, they include the
types person, email-address and date. These enti-
ties are constructed from a collection of email mes-
sages in the obvious way?for example, a recipient of
?Einat Minkov <einat@cs.cmu.edu>? indicates the
existence of a person node ?Einat Minkov? and an
email-address node ?einat@cs.cmu.edu?. (We as-
sume here that person names are unique identifiers.)
The graph edges are directed. We will assume
that edge labels determine the source and target
node types: i.e., if x `?? z and w `?? y then
T (w) = T (x) and T (y) = T (z). However, mul-
tiple relations can hold between any particular pair
of nodes types: for instance, it could be that x `?? y
or x `
?
?? y, where ` 6= `?. (For instance, an email
message x could be sent-from y, or sent-to y.) Note
also that edges need not denote functional relations:
for a given x and `, there may be many distinct nodes
y such that x `?? y. For instance, for a file x, there
are many distinct terms y such that x has-term?? y holds.
In representing email, we also create an inverse
label `?1 for each edge label (relation) `. Note that
this means that the graph will definitely be cyclic.
Table 1 gives the full set of relations used in our
email represention scheme.
3 Graph Similarity
3.1 Edge weights
Similarity between two nodes is defined by a lazy
walk process, and a walk on the graph is controlled
by a small set of parameters ?. To walk away from
a node x, one first picks an edge label `; then, given
`, one picks a node y such that x `?? y. We assume
that the probability of picking the label ` depends
only on the type T (x) of the node x, i.e., that the
outgoing probability from node x of following an
edge type ` is:
Pr(` | x) = Pr(` | Ti) ? ?`,Ti
Let STi be the set of possible labels for an edge leav-
ing a node of type Ti. We require that the weights
over all outgoing edge types given the source node
type form a probability distribution, i.e., that
?
`?STi
?`,Ti = 1
In this paper, we will assume that once ` is picked,
y is chosen uniformly from the set of all y such that
x `?? y. That is, the weight of an edge of type l
connecting source node x to node y is:
Pr(x `?? y | `) = ?`,Ti
| y : x `?? y |
This assumption could easily be generalized, how-
ever: for instance, for the type T (x) = file and
2
source type edge type target type
file sent-from person
sent-from-email email-address
sent-to person
sent-to-email email-address
date-of date
has-subject-term term
has-term term
person sent-from inv. file
sent-to?1 file
alias email-address
has-term term
email-address sent-to-email?1 file
sent-from-email?1 file
alias-inverse person
is-email?1 term
term has-term?1 file
has subject-term?1 file
is-email email-address
has-term?1 person
date date-of?1 file
Table 1: Graph structure: Node and relation types
` = has-term, weights for terms y such that x `?? y
might be distributed according to an appropriate lan-
guage model (Croft and Lafferty, 2003).
3.2 Graph walks
Conceptually, the edge weights above define the
probability of moving from a node x to some other
node y. At each step in a lazy graph walk, there
is also some probability ? of staying at x. Putting
these together, and denoting byMxy the probability
of being at node y at time t + 1 given that one is at
x at time t in the walk, we define
Mxy =
{
(1 ? ?)
?
` Pr(x
`?? y|`) ? Pr(`|T (x)) x 6= y
? x = y
If we associate nodes with integers, and makeM
a matrix indexed by nodes, then a walk of k steps
can then be defined by matrix multiplication: specif-
ically, if V0 is some initial probability distribution
over nodes, then the distribution after a k-step walk
is proportional to Vk = V0Mk. Larger values of ?
increase the weight given to shorter paths between
x and y. In the experiments reported here, we con-
sider small values of k, and this computation is car-
ried out directly using sparse-matrix multiplication
methods.1 If V0 gives probability 1 to some node x0
1We have also explored an alternative approach based on
sampling; this method scales better but introduces some addi-
tional variance into the procedure, which is undesirable for ex-
perimentation.
and probability 0 to all other nodes, then the value
given to y in Vk can be interpreted as a similarity
measure between x and y.
In our framework, a query is an initial distribu-
tion Vq over nodes, plus a desired output type Tout ,
and the answer is a list of nodes y of type Tout ,
ranked by their score in the distribution Vk. For in-
stance, for an ordinary ad hoc document retrieval
query (like ?economic impact of recycling tires?)
would be an appropriate distribution Vq over query
terms, with Tout = file . Replacing Tout with person
would find the person most related to the query?
e.g., an email contact heavily associated with the
retread economics. Replacing Vq with a point dis-
tribution over a particular document would find the
people most closely associated with the given docu-
ment.
3.3 Relation to TF-IDF
It is interesting to view this framework in compar-
ison to more traditional IR methods. Suppose we
restrict ourselves to two types, terms and files, and
allow only in-file edges. Now consider an initial
query distribution Vq which is uniform over the two
terms ?the aardvark?. A one-step matrix multiplica-
tion will result in a distribution V1, which includes
file nodes. The common term ?the? will spread
its probability mass into small fractions over many
file nodes, while the unusual term ?aardvark? will
spread its weight over only a few files: hence the
effect will be similar to use of an IDF weighting
scheme.
4 Learning
As suggested by the comments above, this graph
framework could be used for many types of tasks,
and it is unlikely that a single set of parameter val-
ues will be best for all tasks. It is thus important to
consider the problem of learning how to better rank
graph nodes.
Previous researchers have described schemes for
adjusting the parameters ? using gradient descent-
like methods (Diligenti et al, 2005; Nie et al, 2005).
In this paper, we suggest an alternative approach of
learning to re-order an initial ranking. This rerank-
ing approach has been used in the past for meta-
search (Cohen et al, 1999) and also several natural-
3
language related tasks (Collins and Koo, 2005). The
advantage of reranking over parameter tuning is that
the learned classifier can take advantage of ?global?
features that are not easily used in walk.
Note that node reranking, while can be used as
an alternative to weight manipulation, it is better
viewed as a complementary approach, as the tech-
niques can be naturally combined by first tuning the
parameters ?, and then reranking the result using a
classifier which exploits non-local features. This hy-
brid approach has been used successfully in the past
on tasks like parsing (Collins and Koo, 2005).
We here give a short overview of the reranking ap-
proach, that is described in detail elsewhere (Collins
and Koo, 2005). The reranking algorithm is pro-
vided with a training set containing n examples. Ex-
ample i (for 1 ? i ? n) includes a ranked list of
li nodes. Let wij be the jth node for example i,
and let p(wij) be the probability assigned to wij by
the graph walk. A candidate node wij is represented
through m features, which are computed by m fea-
ture functions f1, . . . , fm. We will require that the
features be binary; this restriction allows a closed
form parameter update. The ranking function for
node x is defined as:
F (x, ??) = ?0L(x) +
m
?
k=1
?kfk(x)
where L(x) = log(p(x)) and ?? is a vector of real-
value parameters. Given a new test example, the out-
put of the model is the given node list re-ranked by
F (x, ??).
To learn the parameter weights ??, we use a boost-
ing method (Collins and Koo, 2005), which min-
imizes the following loss function on the training
data:
ExpLoss(??) =
?
i
li
?
j=2
e?(F (xi,1,??)?F (xi,j ,??))
where xi,1 is, without loss of generality, a correct
target node. The weights for the function are learned
with a boosting-like method, where in each itera-
tion the feature fk that has the most impact on the
loss function is chosen, and ?k is modified. Closed
form formulas exist for calculating the optimal ad-
ditive updates and the impact per feature (Schapire
and Singer, 1999).
5 Evaluation
We experiment with three separate corpora.
The Cspace corpus contains email messages col-
lected from a management course conducted at
Carnegie Mellon University in 1997 (Minkov et
al., 2005). In this course, MBA students, orga-
nized in teams of four to six members, ran simu-
lated companies in different market scenarios. The
corpus we used here includes the emails of all
teams over a period of four days. The Enron cor-
pus is a collection of mail from the Enron cor-
pus that has been made available for the research
community (Klimt and Yang, 2004). Here, we
used the saved email of two different users.2 To
eliminate spam and news postings we removed
email files sent from email addresses with suf-
fix ?.com? that are not Enron?s; widely distributed
email files (sent from ?enron.announcement?, to
?all.employees@enron.com? etc.). Text from for-
warded messages, or replied-to messages were also
removed from the corpus.
Table 2 gives the size of each processed corpus,
and the number of nodes in the graph representation
of it. In deriving terms for the graph, terms were
Porter-stemmed and stop words were removed. The
processed Enron corpora are available from the first
author?s home page.
corpus Person set
files nodes train test
Cspace 821 6248 26 80
Sager-E 1632 9753 11 51
Shapiro-R 978 13174 11 49
Table 2: Corpora Details
5.1 Person Name Disambiguation
5.1.1 Task definition
Consider an email message containing a common
name like ?Andrew?. Ideally an intelligent mailer
would, like the user, understand which person ?An-
drew? refers to, and would rapidly perform tasks like
retrieving Andrew?s prefered email address or home
page. Resolving the referent of a person name is also
an important complement to the ability to perform
named entity extraction for tasks like social network
analysis or studies of social interaction in email.
2Specifially, we used the ?all documents? folder, including
both incoming and outgoing files.
4
However, although the referent of the name is
unambiguous to the recipient of the email, it can
be non-trivial for an automated system to find out
which ?Andrew? is indicated. Automatically de-
termining that ?Andrew? refers to ?Andrew Y. Ng?
and not ?Andrew McCallum? (for instance) is espe-
cially difficult when an informal nickname is used,
or when the mentioned person does not appear in the
email header. As noted above, we model this prob-
lem as a search task: based on a name-mention in an
email message m, we formulate query distribution
Vq, and then retrieve a ranked list of person nodes.
5.1.2 Data preparation
Unfortunately, building a corpus for evaluating
this task is non-trivial, because (if trivial cases are
eliminated) determining a name?s referent is often
non-trivial for a human other than the intended re-
cipient. We evaluated this task using three labeled
datasets, as detailed in Table 2.
The Cspace corpus has been manually annotated
with personal names (Minkov et al, 2005). Addi-
tionally, with the corpus, there is a great deal of
information available about the composition of the
individual teams, the way the teams interact, and
the full names of the team members. Using this
extra information it is possible to manually resolve
name mentions. We collected 106 cases in which
single-token names were mentioned in the the body
of a message but did not match any name from the
header. Instances for which there was not suffi-
cient information to determine a unique person en-
tity were excluded from the example set. In addition
to names that refer to people that are simply not in
the header, the names in this corpus include people
that are in the email header, but cannot be matched
because they are referred to using: initials?this is
commonly done in the sign-off to an email; nick-
names, including common nicknames (e.g., ?Dave?
for ?David?), unusual nicknames (e.g., ?Kai? for
?Keiko?); or American names adopted in place of
a foreign name (e.g., ?Jenny? for ?Qing?).
For Enron, two datasets were generated automat-
ically. We collected name mentions which corre-
spond uniquely a names that is in the email ?Cc?
header line; then, to simulate a non-trivial matching
task, we eliminate the collected person name from
the email header. We also used a small dictionary of
16 common American nicknames to identify nick-
names that mapped uniquely to full person names
on the ?Cc? header line.
For each dataset, some examples were picked ran-
domly and set aside for learning and evaluation pur-
poses.
initials nicknames other
Cspace 11.3% 54.7% 34.0%
Sager-E - 10.2% 89.8%
Shapiro-R - 15.0% 85.0%
Table 3: Person Name Disambiguation Datasets
5.2 Results for person name disambiguation
5.2.1 Evaluation details
All of the methods applied generate a ranked list
of person nodes, and there is exactly one correct an-
swer per example.3 Figure 1 gives results4 for two
of the datasets as a function of recall at rank k, up
to rank 10. Table 4 shows the mean average preci-
sion (MAP) of the ranked lists as well as accuracy,
which we define as the percentage of correct answers
at rank 1 (i.e., precision at rank 1.)
5.2.2 Baseline method
To our knowledge, there are no previously re-
ported experiments for this task on email data. As a
baseline, we apply a reasonably sophisticated string
matching method (Cohen et al, 2003). Each name
mention in question is matched against all of the per-
son names in the corpus. The similarity score be-
tween the name term and a person name is calculated
as the maximal Jaro similarity score (Cohen et al,
2003) between the term and any single token of the
personal name (ranging between 0 to 1). In addition,
we incorporate a nickname dictionary5, such that if
the name term is a known nickname of a name, the
similarity score of that pair is set to 1.
The results are shown in Figure 1 and Table 4. As
can be seen, the baseline approach is substantially
less effective for the more informal Cspace dataset.
Recall that the Cspace corpus includes many cases
such as initials, and also nicknames that have no
literal resemblance to the person?s name (section
3If a ranking contains a block of items with the same score,
a node?s rank is counted as the average rank of the ?block?.
4Results refer to test examples only.
5The same dictionary that was used for dataset generation.
5
5.1.2), which are not handled well by the string sim-
ilarity approach. For the Enron datasets, the base-
line approach perfoms generally better (Table 4). In
all the corpora there are many ambiguous instances,
e.g., common names like ?Dave? or ?Andy? that
match many people with equal strength.
5.2.3 Graph walk methods
We perform two variants of graph walk, corre-
sponding to different methods of forming the query
distribution Vq. Unless otherwise stated, we will use
a uniform weighting of labels?i.e., ?`,T = 1/ST ;
? = 1/2; and a walk of length 2.
In the first variant, we concentrate all the prob-
ability in the query distribution on the name term.
The column labeled term gives the results of the
graph walk from this probability vector. Intuitively,
using this variant, the name term propagates its
weight to the files in which it appears. Then, weight
is propagated to person nodes which co-occur fre-
quently with these files. Note that in our graph
scheme there is a direct path between terms to per-
son names, so that they recieve weight as well.
As can be seen in the results, this leads to very
effective performance: e.g., it leads to 61.3% vs.
41.3% accuracy for the baseline approach on the
CSpace dataset. However, it does not handle am-
biguous terms as well as one would like, as the query
does not include any information of the context in
which the name occurred: the top-ranked answer for
ambiguous name terms (e.g., ?Dave?) will always
be the same person. To solve this problem, we also
used a file+term walk, in which the query Vq gives
equal weight to the name term node and the file in
which it appears.
We found that adding the file node to Vq provides
useful context for ambiguous instances?e.g., the
correct ?David? would in general be ranked higher
than other persons with this same name. On the
other hand, though, adding the file node reduces
the the contribution of the term node. Although the
MAP and accuracy are decreased, file+term has bet-
ter performance than term at higher recall levels, as
can be seen in Figure 1.
5.2.4 Reranking the output of a walk
We now examine reranking as a technique for im-
proving the results. After some preliminary exper-
imentation, we adopted the following types of fea-
tures f for a node x. The set of features are fairly
generic. Edge unigram features indicate, for each
edge label `, whether ` was used in reaching x from
Vq. Edge bigram features indicate, for each pair of
edge labels `1, `2, whether `1 and `2 were used (in
that order) in reaching x from Vq. Top edge bigram
features are similar but indicate if `1, `2 were used
in one of the two highest-scoring paths between Vq
and x (where the ?score? of a path is the product of
Pr(y `?? z) for all edges in the path.)
We believe that these features could all be com-
puted using dynamic programming methods. Cur-
rently, however, we compute features by using a
method we call path unfolding, which is simi-
lar to the back-propagation through time algorithm
(Haykin, 1994; Diligenti et al, 2005) used in train-
ing recurrent neural networks. Graph unfolding is
based on a backward breadth-first visit of the graph,
starting at the target node at time step k, and expand-
ing the unfolded paths by one layer per each time
step. This procedure is more expensive, but offers
more flexibility in choosing alternative features, and
was useful in determining an optimal feature set.
In addition, we used for this task some addi-
tional problem-specific features. One feature indi-
cates whether the set of paths leading to a node orig-
inate from one or two nodes in Vq. (We conjecture
that in the file+term walk, nodes are connected to
both the source term and file nodes are more rele-
vant comparing to nodes that are reached from the
file node or term node only.) We also form features
that indicate whether the given term is a nickname of
the person name, per the nicknames dictionary; and
whether the Jaro similarity score between the term
and the person name is above 0.8. This information
is similar to that used by the baseline method.
The results (for the test set, after training on the
train set) are shown in Table 4 and (for two represen-
tative cases) Figure 1. In each case the top 10 nodes
were reranked. Reranking substantially improves
performance, especially for the file+term walk. The
accuracy rate is higher than 75% across all datasets.
The features that were assigned the highest weights
by the re-ranker were the literal similarity features
and the source count feature.
6
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20
Cu
m
ul
at
ive
 R
at
e
CSPACE
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25
Cu
m
ul
at
ive
 R
at
e
Rank
SHAPIRO-R
baseline
term
term reranked
file+term
file+term re-ranked
Figure 1: Person name disambiguation results: Re-
call at rank k
6 Related Work
As noted above, the similarity measure we use is
based on graph-walk techniques which have been
adopted by many other researchers for several dif-
ferent tasks. In the information retrieval commu-
nity, infinite graph walks are prevalent for deter-
mining document centrality (e.g., (Page et al, 1998;
Diligenti et al, 2005; Kurland and Lee, 2005)). A
related venue of research is of spreading activa-
tion over semantic or association networks, where
the underlying idea is to propagate activation from
source nodes via weighted links through the network
(Berger et al, 2004; Salton and Buckley, 1988).
The idea of representing structured data as a
graph is widespread in the data mining community,
which is mostly concerned with relational or semi-
structured data. Recently, the idea of PageRank
MAP Accuracy
Cspace
Baseline 49.0 41.3
Graph - term 72.6 61.3
Graph - file+term 66.3 48.8
Reranking - term 85.6 72.5
Reranking - file+term 89.0 83.8
Sager-E
Baseline 67.5 39.2
Graph - term 82.8 66.7
Graph - file+term 61.7 41.2
Reranking - term 83.2 68.6
Reranking - file+term 88.9 80.4
Shapiro-R
Baseline 60.8 38.8
Graph - term 84.1 63.3
Graph - file+term 56.5 38.8
Reranking - term 87.9 65.3
Reranking - file+term 85.5 77.6
Table 4: Person Name Disambiguation Results
has been applied to keyword search in structured
databases (Balmin et al, 2004). Analysis of inter-
object relationships has been suggested for entity
disambiguation for entities in a graph (Kalashnikov
et al, 2005), where edges are unlabelled. It has been
suggested to model similarity between objects in re-
lational data in terms of structural-context similarity
(Jeh and Widom, 2002).
We propose the use of learned re-ranking schemes
to improve performance of a lazy graph walk.
Earlier authors have considered instead using hill-
climbing approaches to adjust the parameters of a
graph-walk (Diligenti et al, 2005). We have not
compared directly with such approaches; prelimi-
nary experiments suggest that the performance gain
of such methods is limited, due to their inability to
exploit the global features we used here6. Related
research explores random walks for semi supervised
learning (Zhu et al, 2003; Zhou et al, 2005).
The task of person disambiguation has been stud-
ied in the field of social networks (e.g., (Malin et
al., 2005)). In particular, it has been suggested to
perform name disambiguation in email using traf-
fic information, as derived from the email headers
(Diehl et al, 2006). Our approach differs in that it
allows integration of email content and a timeline in
addition to social network information in a unified
6For instance, re-ranking using a set of simple locally-
computable features only modestly improved performance of
the ?random? weight set for the CSpace threading task.
7
framework. In addition, we incorporate learning to
tune the system parameters automatically.
7 Conclusion
We have presented a scheme for representing a cor-
pus of email messages with a graph of typed entities,
and an extension of the traditional notions of docu-
ment similarity to documents embedded in a graph.
Using a boosting-based learning scheme to rerank
outputs based on graph-walk related, as well as other
domain-specific, features provides an additional per-
formance improvement. The final results are quite
strong: for the explored name disambiguation task,
the method yields MAP scores in the mid-to-upper
80?s. The person name identification task illustrates
a key advantage of our approach?that context can
be easily incorporated in entity disambiguation.
In future work, we plan to further explore the
scalability of the approach, and also ways of inte-
grating this approach with language-modeling ap-
proaches for document representation and retrieval.
An open question with regard to contextual (multi-
source) graph walk in this framework is whether it is
possible to further focus probability mass on nodes
that are reached from multiple source nodes. This
may prove beneficial for complex queries.
References
Andrey Balmin, Vagelis Hristidis, and Yannis Papakonstanti-
nou. 2004. ObjectRank: Authority-based keyword search in
databases. In VLDB.
Helmut Berger, Michael Dittenbach, and Dieter Merkl. 2004.
An adaptive information retrieval system. based on associa-
tive networks. In APCCM.
William W. Cohen, Robert E. Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial Intelli-
gence Research (JAIR), 10:243?270.
William W. Cohen, Pradeep Ravikumar, and Stephen Fienberg.
2003. A comparison of string distance metrics for name-
matching tasks. In IIWEB.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query ex-
pansion using random walk models. In CIKM.
W. Bruce Croft and John Lafferty. 2003. Language Modeling
for Information Retrieval. Springer.
Christopher P. Diehl, Lise Getoor, and Galileo Namata. 2006.
Name reference resolution in organizational email archives.
In SIAM.
Michelangelo Diligenti, Marco Gori, and Marco Maggini.
2005. Learning web page scores by error back-propagation.
In IJCAI.
Simon Haykin. 1994. Neural Networks. Macmillan College
Publishing Company.
Glen Jeh and Jennifer Widom. 2002. Simrank: A measure of
structural-context similarity. In SIGKDD.
Dmitri Kalashnikov, Sharad Mehrotra, and Zhaoqi Chen. 2005.
Exploiting relationship for domain independent data clean-
ing. In SIAM.
Brown Klimt and Yiming Yang. 2004. The enron corpus: A
new dataset for email classification research. In ECML.
Oren Kurland and Lillian Lee. 2005. Pagerank without hyper-
links: Structural re-ranking using links induced by language
models. In SIGIR.
Bradely Malin, Edoardo M. Airoldi, and Kathleen M. Carley.
2005. A social network analysis model for name disam-
biguation in lists. Journal of Computational and Mathemat-
ical Organization Theory, 11(2).
Einat Minkov, Richard Wang, and William Cohen. 2005. Ex-
tracting personal names from emails: Applying named entity
recognition to informal text. In HLT-EMNLP.
Zaiqing Nie, Yuanzhi Zhang, Ji-Rong Wen, and Wei-Ying Ma.
2005. Object-level ranking: Bringing order to web objects.
In WWW.
Larry Page, Sergey Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the web. In
Technical Report, Computer Science department, Stanford
University.
Gerard Salton and Chris Buckley. 1988. On the use of spread-
ing activation methods in automatic information retrieval. In
SIGIR.
Robert E. Schapire and Yoram Singer. 1999. Improved boost-
ing algorithms using confidence-rated predictions. Machine
Learning, 37(3):297?336.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In ICML.
Wensi Xi, Edward Allan Fox, Weiguo Patrick Fan, Benyu
Zhang, Zheng Chen, Jun Yan, and Dong Zhuang. 2005.
Simfusion: Measuring similarity using unified relationship
matrix. In SIGIR.
Dengyong Zhou, Bernhard Scholkopf, and Thomas Hofmann.
2005. Semi-supervised learning on directed graphs. In
NIPS.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003.
Semi-supervised learning using gaussian fields and harmonic
functions. In ICML.
8
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529?539,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Random Walk Inference and Learning in A Large Scale Knowledge Base
Ni Lao
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
nlao@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Abstract
We consider the problem of performing learn-
ing and inference in a large scale knowledge
base containing imperfect knowledge with
incomplete coverage. We show that a soft
inference procedure based on a combination
of constrained, weighted, random walks
through the knowledge base graph can be
used to reliably infer new beliefs for the
knowledge base. More specifically, we
show that the system can learn to infer
different target relations by tuning the weights
associated with random walks that follow
different paths through the graph, using a
version of the Path Ranking Algorithm (Lao
and Cohen, 2010b). We apply this approach to
a knowledge base of approximately 500,000
beliefs extracted imperfectly from the web
by NELL, a never-ending language learner
(Carlson et al, 2010). This new system
improves significantly over NELL?s earlier
Horn-clause learning and inference method:
it obtains nearly double the precision at rank
100, and the new learning method is also
applicable to many more inference tasks.
1 Introduction
Although there is a great deal of recent research
on extracting knowledge from text (Agichtein and
Gravano, 2000; Etzioni et al, 2005; Snow et
al., 2006; Pantel and Pennacchiotti, 2006; Banko
et al, 2007; Yates et al, 2007), much less
progress has been made on the problem of drawing
reliable inferences from this imperfectly extracted
knowledge. In particular, traditional logical
inference methods are too brittle to be used to make
complex inferences from automatically-extracted
knowledge, and probabilistic inference methods
(Richardson and Domingos, 2006) suffer from
scalability problems. This paper considers the
problem of constructing inference methods that can
scale to large knowledge bases (KB?s), and that are
robust to imperfect knowledge. The KB we consider
is a large triple store, which can be represented as a
labeled, directed graph in which each entity a is a
node, each binary relation R(a, b) is an edge labeled
R between a and b, and unary concepts C(a) are
represented as an edge labeled ?isa? between the
node for the entity a and a node for the concept
C. We present a trainable inference method that
learns to infer relations by combining the results of
different random walks through this graph, and show
that the method achieves good scaling properties and
robust inference in a KB containing over 500,000
triples extracted from the web by the NELL system
(Carlson et al, 2010).
1.1 The NELL Case Study
To evaluate our approach experimentally, we study
it in the context of the NELL (Never Ending
Language Learning) research project, which is an
effort to develop a never-ending learning system that
operates 24 hours per day, for years, to continuously
improve its ability to read (extract structured facts
from) the web (Carlson et al, 2010). NELL began
operation in January 2010. As of March 2011,
NELL had built a knowledge base containing several
million candidate beliefs which it had extracted from
the web with varying confidence. Among these,
529
NELL had fairly high confidence in approximately
half a million, which we refer to as NELL?s
(confident) beliefs. NELL had lower confidence in a
few million others, which we refer to as its candidate
beliefs.
NELL is given as input an ontology that defines
hundreds of categories (e.g., person, beverage,
athlete, sport) and two-place typed relations among
these categories (e.g., atheletePlaysSport(?athlete?,
?sport?)), which it must learn to extract from the
web. It is also provided a set of 10 to 20 positive
seed examples of each such category and relation,
along with a downloaded collection of 500 million
web pages from the ClueWeb2009 corpus (Callan
and Hoy, 2009) as unlabeled data, and access to
100,000 queries each day to Google?s search engine.
Each day, NELL has two tasks: (1) to extract
additional beliefs from the web to populate its
growing knowledge base (KB) with instances of the
categories and relations in its ontology, and (2) to
learn to perform task 1 better today than it could
yesterday. We can measure its learning competence
by allowing it to consider the same text documents
today as it did yesterday, and recording whether it
extracts more beliefs, more accurately today.1
NELL uses a large-scale semi-supervised multi-
task learning algorithm that couples the training
of over 1500 different classifiers and extraction
methods (see (Carlson et al, 2010)). Although
many of the details of NELL?s learning method
are not central to this paper, two points should
be noted. First, NELL is a multistrategy learning
system, with components that learn from different
?views? of the data (Blum and Mitchell, 1998): for
instance, one view uses orthographic features of
a potential entity name (like ?contains capitalized
words?), and another uses free-text contexts in
which the noun phrase is found (e.g., ?X frequently
follows the bigram ?mayor of? ?). Second, NELL
is a bootstrapping system, which self-trains on its
growing collection of confident beliefs.
1.2 Knowledge Base Inference: Horn Clauses
Although NELL has now grown a sizable knowl-
edge base, its ability to perform inference over this
1NELL?s current KB is available online at
http://rtw.ml.cmu.edu.
Eli Manning Giants
AthletePlays
ForTeam
HinesWard Steelers
AthletePlays
ForTeam NFL
TeamPlays
InLeague
MLBTeamPlays
InLeague
TeamPlays
InLeague
Figure 1: An example subgraph.
knowledge base is currently very limited. At present
its only inference method beyond simple inheritance
involves applying first order Horn clause rules to
infer new beliefs from current beliefs. For example,
it may use a Horn clause such as
AthletePlaysForTeam(a, b) (1)
? TeamPlaysInLeague(b, c)
? AthletePlaysInLeague(a,c)
to infer that AthletePlaysInLeague(HinesWard,NFL),
if it has already extracted the beliefs in the
preconditions of the rule, with variables a, b and c
bound to HinesWard, PittsburghSteelers and NFL
respectively as shown in Figure 1. NELL currently
has a set of approximately 600 such rules, which
it has learned by data mining its knowledge base
of beliefs. Each learned rule carries a conditional
probability that its conclusion will hold, given that
its preconditions are satisfied.
NELL learns these Horn clause rules using
a variant of the FOIL algorithm (Quinlan and
Cameron-Jones, 1993), henceforth N-FOIL.
N-FOIL takes as input a set of positive and
negative examples of a rule?s consequent
(e.g., +AthletePlaysInLeague(HinesWard,NFL),
?AthletePlaysInLeague(HinesWard,NBA)), and
uses a ?separate-and-conquer? strategy to learn a
set of Horn clauses that fit the data well. Each
Horn clause is learned by starting with a general
rule and progressively specializing it, so that it
still covers many positive examples but covers few
negative examples. After a clause is learned, the
examples covered by that clause are removed from
the training set, and the process repeats until no
positive examples remain.
Learning first-order Horn clauses is computation-
ally expensive?not only is the search space large,
but some Horn clauses can be costly to evaluate
(Cohen and Page, 1995). N-FOIL uses two tricks
to improve its scalability. First, it assumes that
the consequent predicate is functional?e.g., that
530
each Athlete plays in at most one League. This
means that explicit negative examples need not
be provided (Zelle et al, 1995): e.g., if Ath-
letePlaysInLeague(HinesWard,NFL) is a positive
example, then AthletePlaysInLeague(HinesWard,c?)
for any other value of c? is negative. In general,
this constraint guides the search algorithm toward
Horn clauses that have fewer possible instantiations,
and hence are less expensive to match. Second,
N-FOIL uses ?relational pathfinding? (Richards
and Mooney, 1992) to produce general rules?i.e.,
the starting point for a predicate R is found
by looking at positive instances R(a, b) of the
consequent, and finding a clause that corresponds
to a bounded-length path of binary relations that
link a to b. In the example above, a start clause
might be the clause (1). As in FOIL, the clause
is then (potentially) specialized by greedily adding
additional conditions (like ProfessionalAthlete(a))
or by replacing variables with constants (eg,
replacing c with NFL).
For each N-FOIL rule, an estimated conditional
probability P? (conclusion|preconditions) is calcu-
lated using a Dirichlet prior according to
P? = (N+ +m ? prior)/(N+ +N? +m) (2)
where N+ is the number of positive instances
matched by this rule in the FOIL training data,
N? is the number of negative instances matched,
m = 5 and prior = 0.5. As the results below
show, N-FOIL generally learns a small number of
high-precision inference rules. One important role
of these inference rules is that they contribute to
the bootstrapping procedure, as inferences made by
N-FOIL increase either the number of candidate
beliefs, or (if the inference is already a candidate)
improve NELL?s confidence in candidate beliefs.
1.3 Knowledge Base Inference: Graph
Random Walks
In this paper, we consider an alternative approach,
based on the Path Ranking Algorithm (PRA) of Lao
and Cohen (2010b), described in detail below. PRA
learns to rank graph nodes b relative to a query
node a. PRA begins by enumerating a large set of
bounded-length edge-labeled path types, similar to
the initial clauses used in NELL?s variant of FOIL.
These path types are treated as ranking ?experts?,
each performing a random walk through the graph,
constrained to follow that sequence of edge types,
and ranking nodes b by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? using logistic
regression to predict the probability that the relation
R(a, b) is satisfied.
As an example, consider a path from a to b via
the sequence of edge types isa, isa?1 (the inverse of
isa), and AthletePlaysInLeague, which corresponds
to the Horn clause
isa(a, c) ? isa?1(c, a?) (3)
? AthletePlaysInLeague(a?, b)
? AthletePlaysInLeague(a, b)
Suppose a random walk starts at a query node a
(say a=HinesWard). If HinesWard is linked to the
single concept node ProfessionalAthlete via isa, the
walk will reach that node with probability 1 after
one step. If A is the set of ProfessionalAthlete?s
in the KB, then after two steps, the walk will have
probability 1/|A| of being at any a? ? A. If L is
the set of athletic leagues and ` ? L, let A` be the
set of athletes in league `: after three steps, the walk
will have probability |A`|/|A| of being at any point
b ? L. In short, the ranking associated with this
path gives the prior probability of a value b being an
athletic league for a?which is useful as a feature in
a combined ranking method, although not by itself a
high-precision inference rule.
Note that the rankings produced by this ?expert?
will change as the knowledge base evolves?for
instance, if the system learns about proportionally
more soccer players than hockey players over time,
then the league rankings for the path of clause (3)
will change. Also, the ranking is specific to the
query node a. For instance, suppose the KB contains
facts which reflect the ambiguity of the team name
?Giants?2 as in Figure 1. Then the path for clause (1)
above will give lower weight to b = NFL for a =
EliManning than to b = NFL for a = HinesWard.
The main contribution of this paper is to introduce
and evaluate PRA as an algorithm for making
probabilistic inference in large KBs. Compared to
Horn clause inference, the key characteristics of this
new inference method are as follows:
2San Francisco?s Major-League Baseball and New York?s
National Football League teams are both called the ?Giants?.
531
? The evidence in support of inferring a relation
instance R(a, b) is based on many existing
paths between a and b in the current KB,
combined using a learned logistic function.
? The confidence in an inference is sensitive to
the current state of the knowledge base, and the
specific entities being queried (since the paths
used in the inference have these properties).
? Experimentally, the inference method yields
many more moderately-confident inferences
than the Horn clauses learned by N-FOIL.
? The learning and inference are more efficient
than N-FOIL, in part because we can exploit
efficient approximation schemes for random
walks (Lao and Cohen, 2010a). The resulting
inference is as fast as 10 milliseconds per query
on average.
The Path Ranking Algorithm (PRA) we use is
similar to that described elsewhere (Lao and Cohen,
2010b), except that to achieve efficient model
learning, the paths between a and b are determined
by the statistics from a population of training
queries rather than enumerated completely. PRA
uses random walks to generate relational features
on graph data, and combine them with a logistic
regression model. Compared to other relational
models (e.g. FOIL, Markov Logic Networks), PRA
is extremely efficient at link prediction or retrieval
tasks, in which we are interested in identifying top
links from a large number of candidates, instead of
focusing on a particular node pair or joint inferences.
1.4 Related Work
The TextRunner system (Cafarella et al, 2006)
answers list queries on a large knowledge base
produced by open domain information extrac-
tion. Spreading activation is used to measure
the closeness of any node to the query term
nodes. This approach is similar to the random
walk with restart approach which is used as a
baseline in our experiment. The FactRank system
(Jain and Pantel, 2010) compares different ways of
constructing random walks, and combining them
with extraction scores. However, the shortcoming
of both approaches is that they ignore edge type
information, which is important for achieving high
accuracy predictions.
The HOLMES system (Schoenmackers et al,
2008) derives new assertions using a few manually
written inference rules. A Markov network
corresponding to the grounding of these rules to
the knowledge base is constructed for each query,
and then belief propagation is used for inference.
In comparison, our proposed approach discovers
inference rules automatically from training data.
Similarly, the Markov Logic Networks (Richard-
son and Domingos, 2006) are Markov networks
constructed corresponding to the grounding of rules
to knowledge bases. In comparison, our proposed
approach is much more efficient by avoiding the
harder problem of joint inferences and by leveraging
efficient random walk schemes (Lao and Cohen,
2010a).
Below we describe our approach in greater detail,
provide experimental evidence of its value for
performing inference in NELL?s knowledge base,
and discuss implications of this work and directions
for future research.
2 Approach
In this section, we first describe how we formulate
link (relation) prediction on a knowledge base as
a ranking task. Then we review the Path Ranking
Algorithm (PRA) introduced by Lao and Cohen
(2010b; 2010a). After that, we describe two
improvements to the PRA method to make it more
suitable for the task of link prediction in knowledge
bases. The first improvement helps PRA deal
with the large number of relations typical of large
knowledge bases. The second improvement aims at
improving the quality of inference by applying low
variance sampling.
2.1 Learning with NELL?s Knowledge Base
For each relationR in the knowledge base we train a
model for the link prediction task: given a concept a,
find all other concepts b which potentially have the
relationR(a, b). This prediction is made based on an
existing knowledge base extracted imperfectly from
the web. Although a model can potentially benefit
from predicting multiple relations jointly, such joint
inference is beyond the scope of this work.
532
To ensure a reasonable number of training
instances, we generate labeled training example
queries from 48 relations which have more than
100 instances in the knowledge base. We create
two tasks for each relation?i.e., predicting b given
a and predicting a given b? yielding 96 tasks in
all. Each node a which has relation R in the
knowledge base with any other node is treated as a
training query, the actual nodes b in the knowledge
base known to satisfy R(a, b) are treated as labeled
positive examples, and any other nodes are treated
as negative examples.
2.2 Path Ranking Algorithm Review
We now review the Path Ranking Algorithm
introduced by Lao and Cohen (2010b). A relation
path P is defined as a sequence of relations
R1 . . . R`, and in order to emphasize the types
associated with each step, P can also be written as
T0 R1??? . . . R`?? T`, where Ti = range(Ri) =
domain(Ri+1), and we also define domain(P ) ?
T0, range(P ) ? T`. In the experiments in this
paper, there is only one type of node which we call
a concept, which can be connected through different
types of relations. In this notation, relations like ?the
team a certain player plays for?, and ?the league a
certain player?s team is in? can be expressed by the
paths below (respectively):
P1 : concept
AtheletePlayesForTeam??????????????? concept
P2 : concept
AtheletePlayesForTeam??????????????? concept
TeamPlaysInLeagure?????????????? concept
For any relation path P = R1 . . . R` and a
seed node s ? domain(P ), a path constrained
random walk defines a distribution hs,P recursively
as follows. If P is the empty path, then define
hs,P (e) =
{
1, if e = s
0, otherwise (4)
If P = R1 . . . R` is nonempty, then let P ? =
R1 . . . R`?1, and define
hs,P (e) =
?
e??range(P ?)
hs,P ?(e?) ? P (e|e?;R`), (5)
where P (e|e?;R`) = R`(e?,e)|R`(e?,?)| is the probability ofreaching node e from node e? with a one step random
walk with edge type R`. R(e?, e) indicates whether
there exists an edge with type R that connect e? to e.
More generally, given a set of paths P1, . . . , Pn,
one could treat each hs,Pi(e) as a path feature for
the node e, and rank nodes by a linear model
?1hs,P1(e) + ?2hs,P2(e) + . . . ?nhs,Pn(e)
where ?i are appropriate weights for the paths. This
gives a ranking of nodes e related to the query node
s by the following scoring function
score(e; s) =
?
P?P`
hs,P (e)?P , (6)
where P` is the set of relation paths with length? `.
Given a relation R and a set of node pairs
{(si, ti)} for which we know whether R(si, ti) is
true or not, we can construct a training dataset
D = {(xi, yi)}, where xi is a vector of all the
path features for the pair (si, ti)?i.e., the j-th
component of xi is hsi,Pj (ti), and where yi is a
boolean variable indicating whetherR(si, ti) is true.
We then train a logistic function to predict the
conditional probability P (y|x; ?). The parameter
vector ? is estimated by maximizing a regularized
form of the conditional likelihood of y given x. In
particular, we maximize the objective function
O(?) =
?
i
oi(?)? ?1|?|1 ? ?2|?|2, (7)
where ?1 controls L1-regularization to help struc-
ture selection, and ?2 controls L2-regularization
to prevent overfitting. oi(?) is the per-instance
weighted log conditional likelihood given by
oi(?) = wi[yi ln pi + (1? yi) ln(1? pi)], (8)
where pi is the predicted probability p(yi =
1|xi; ?) = exp(?
Txi)
1+exp(?Txi) , and wi is an importanceweight to each example. A biased sampling
procedure selects only a small subset of negative
samples to be included in the objective (see (Lao and
Cohen, 2010b) for detail).
2.3 Data-Driven Path Finding
In prior work with PRA, P` was defined as all
relation paths of length at most `. When the number
of edge types is small, one can generate P` by
533
Table 1: Number of paths in PRA models of maximum
path length 3 and 4. Averaged over 96 tasks.
`=3 `=4
all paths up to length L 15, 376 1, 906, 624
+query support? ? = 0.01 522 5016
+ever reach a target entity 136 792
+L1 regularization 63 271
enumeration; however, for domains with a large
number of edge types (e.g., a knowledge base), it is
impractical to enumerate all possible relation paths
even for small `. For instance, if the number of
edge types related to each node type is 100, even
the number of length three paths types easily reaches
millions. For other domains like parsed natural
language sentences, useful relation paths can be as
long as ten relations (Minkov and Cohen, 2008). In
this case, even with smaller number of possible edge
types, the total number of relation paths is still too
large for systematic enumeration.
In order to apply PRA to these domains, we
modify the path generation procedure in PRA to
produce only relation paths which are potentially
useful for the task. Define a query s to be supporting
a path P if hs,P (e) 6= 0 for any entity e. We require
that any path node created during path finding needs
to be supported by at least a fraction ? of the training
queries si, as well as being of length no more than
` (In the experiments, we set ? = 0.01) We also
require that in order for a relation path to be included
in the PRA model, it must retrieve at least one target
entity ti in the training set. As we can see from
Table 1, together these two constraints dramatically
reduce the number of relation paths that need to be
considered, relative to systematically enumerating
all possible relation paths. L1 regularization reduces
the size of the model even more.
The idea of finding paths that connects nodes in a
graph is not new. It has been embodied previously in
first-order learning systems (Richards and Mooney,
1992) as well as N-FOIL, and relational database
searching systems (Bhalotia et al, 2002). These
approaches consider a single query during path
finding. In comparison, the data-driven path finding
method we described here uses statistics from a
population of queries, and therefore can potentially
determine the importance of a path more reliably.
Table 2: Comparing PRA with RWR models. MRRs and
training times are averaged over 96 tasks.
`=2 `=3
MRR Time MRR Time
RWR(no train) 0.271 0.456
RWR 0.280 3.7s 0.471 9.2s
PRA 0.307 5.7s 0.516 15.4s
2.4 Low-Variance Sampling
Lao and Cohen (2010a) previously showed that
sampling techniques like finger printing and particle
filtering can significantly speedup random walk
without sacrificing retrieval quality. However, the
sampling procedures can induce a loss of diversity
in the particle population. For example, consider a
node in the graph with just two out links with equal
weights, and suppose we are required to generate
two walkers starting from this node. A disappointing
result is that with 50 percent chance both walkers
will follow the same branch, and leave the other
branch with no probability mass.
To overcome this problem, we apply a technique
called Low-Variance Sampling (LVS) (Thrun et
al., 2005), which is commonly used in robotics
to improve the quality of sampling. Instead of
generating independent samples from a distribution,
LVS uses a single random number to generate all
samples, which are evenly distributed across the
whole distribution. Note that given a distribution
P (x), any number r in [0, 1] points to exactly one
x value, namely x = argminj
?
m=1..j P (m) ?
r. Suppose we want to generate M samples from
P (x). LVS first generates a random number r in
the interval [0,M?1]. Then LVS repeatedly adds
the fixed amount M?1 to r and chooses x values
corresponding to the resulting numbers.
3 Results
This section reports empirical results of applying
random walk inference to NELL?s knowledge base
after the 165th iteration of its learning process. We
first investigate PRA?s behavior by cross validation
on the training queries. Then we compare PRA and
N-FOIL?s ability to reliably infer new beliefs, by
leveraging the Amazon Mechanical Turk service.
534
3.1 Cross Validation on the Training Queries
Random Walk with Restart (RWR) (also called
personalized PageRank (Haveliwala, 2002)) is a
general-purpose graph proximity measure which
has been shown to be fairly successful for many
types of tasks. We compare PRA to two versions
of RWR on the 96 tasks of link prediction with
NELL?s knowledge base. The two baseline methods
are an untrained RWR model and a trained RWR
model as described by Lao and Cohen (2010b). (In
brief, in the trained RWR model, the walker will
probabilistically prefer to follow edges associated
with different labels, where the weight for each edge
label is chosen to minimize a loss function, such as
Equation 7. In the untrained model, edge weights
are uniform.) We explored a range of values for
the regularization parameters L1 and L2 using cross
validation on the training data, and we fix both
L1 and L2 parameters to 0.001 for all tasks. The
maximum path length is fixed to 3.3
Table 2 compares the three methods using
5-fold cross validation and the Mean Reciprocal
Rank (MRR)4 measure, which is defined as the
inverse rank of the highest ranked relevant result
in a set of results. If the the first returned
result is relevant, then MRR is 1.0, otherwise,
it is smaller than 1.0. Supervised training can
significantly improve retrieval quality (p-value=9 ?
10?8 comparing untrained and trained RWR), and
leveraging path information can produce further
improvement (p-value=4? 10?4 comparing trained
RWR with PRA). The average training time for a
predicate is only a few seconds.
We also investigate the effect of low-variance
sampling on the quality of prediction. Figure 2 com-
pares independent and low variance sampling when
applied to finger printing and particle filtering (Lao
and Cohen, 2010a). The horizontal axis corresponds
to the speedup of random walk compared with
exact inference, and the vertical axis measures the
quality of prediction by MRR with three fold cross
validation on the training query set. Low-variance
3Results with maximum length 4 are not reported here.
Generally models with length 4 paths produce slightly better
results, but are 4-5 times slower to train
4For a set of queries Q,
MRR = 1|Q|
?
q?Q
1rank of the first correct answer for q
 
0.4
0.5
0 1 2 3 4 5
M
R
R
Random Walk Speedup
Exact
Independent Fingerprinting
Low Variance Fingerprinting
Independent Filtering
Low Variance Filtering
10k
1k
100
10k
1k
100k
Figure 2: Compare inference speed and quality over 96
tasks. The speedup is relative to exact inference, which is
on average 23ms per query.
sampling can improve prediction for both finger
printing and particle filtering. The numbers on the
curves indicate the number of particles (or walkers).
When using a large number of particles, the particle
filtering methods converge to the exact inference.
Interestingly, when using a large number of walkers,
the finger printing methods produce even better
prediction quality than exact inference. Lao and
Cohen noticed a similar improvement on retrieval
tasks, and conjectured that it is because the sampling
inference imposes a regularization penalty on longer
relation paths (2010a).
3.2 Evaluation by Mechanical Turk
The cross-validation result above assumes that the
knowledge base is complete and correct, which
we know to be untrue. To accurately compare
PRA and N-FOIL?s ability to reliably infer new
beliefs from an imperfect knowledge base, we
use human assessments obtained from Amazon
Mechanical Turk. To limit labeling costs, and
since our goal is to improve the performance of
NELL, we do not include RWR-based approaches
in this comparison. Among all the 24 functional
predicates, N-FOIL discovers confident rules for
8 of them (it produces no result for the other 16
predicates). Therefore, we compare the quality
of PRA to N-FOIL on these 8 predicates only.
Among all the 72 non-functional predicates?which
535
Table 3: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules. c stands for concept.
ID PRA Path (Comment)
athletePlaysForTeam
1 c athletePlaysInLeague???????????????? c leaguePlayers?????????? c athletePlaysForTeam???????????????? c (teams with many players in the athlete?s league)
2 c athletePlaysInLeague???????????????? c leagueTeams?????????? c teamAgainstTeam????????????? c (teams that play against many teams in the athlete?s league)
athletePlaysInLeague
3 c athletePlaysSport????????????? c players?????? c athletePlaysInLeague???????????????? c (the league that players of a certain sport belong to)
4 c isa??? c isa?1????? c athletePlaysInLeague???????????????? c (popular leagues with many players)
athletePlaysSport
5 c isa??? c isa?1????? c athletePlaysSport????????????? c (popular sports of all the athletes)
6 c athletePlaysInLeague???????????????? c superpartOfOrganization?????????????????? c teamPlaysSport???????????? c (popular sports of a certain league)
stadiumLocatedInCity
7 c stadiumHomeTeam?????????????? c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same team)
8 c latitudeLongitude????????????? c latitudeLongitudeOf??????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same location)
teamHomeStadium
9 c teamPlaysInCity????????????? c cityStadiums?????????? c (stadiums located in the same city with the query team)
10 c teamMember?????????? c athletePlaysForTeam???????????????? c teamHomeStadium?????????????? c (home stadium of teams which share players with the query)
teamPlaysInCity
11 c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the team?s home stadium)
12 c teamHomeStadium?????????????? c stadiumHomeTeam?????????????? c teamPlaysInCity????????????? c (city of teams with the same home stadium as the query)
teamPlaysInLeague
13 c teamPlaysSport???????????? c players?????? c athletePlaysInLeague???????????????? c (the league that the query team?s members belong to)
14 c teamPlaysAgainstTeam????????????????? c teamPlaysInLeague?????????????? c (the league that the query team?s competing team belongs to)
teamPlaysSport
15 c isa??? c isa?1????? c teamPlaysSport???????????? c (sports played by many teams)
16 c teamPlaysInLeague?????????????? c leagueTeams?????????? c teamPlaysSport???????????? c (the sport played by other teams in the league)
Table 4: Amazon Mechanical Turk evaluation for the promoted knowledge. Using paired t-test at task level, PRA is
not statistically different from N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003)
PRA N-FOIL
Task Pmajority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000
athletePlaysForTeam 0.07 125 0.4 0.46 0.66 1(+1) 7 0.6 0.08 0.01
athletePlaysInLeague 0.60 15 1.0 0.84 0.80 3(+30) 332 0.9 0.80 0.24
athletePlaysSport 0.73 34 1.0 0.78 0.70 2(+30) 224 1.0 0.82 0.18
stadiumLocatedInCity 0.05 18 0.9 0.62 0.54 1(+0) 25 0.7 0.16 0.00
teamHomeStadium 0.02 66 0.3 0.48 0.34 1(+0) 2 0.2 0.02 0.00
teamPlaysInCity 0.10 29 1.0 0.86 0.62 1(+0) 60 0.9 0.56 0.06
teamPlaysInLeague 0.26 36 1.0 0.70 0.64 4(+151) 30 0.9 0.18 0.02
teamPlaysSport 0.42 21 0.7 0.60 0.62 4(+86) 48 0.9 0.42 0.02
average 0.28 43 0.79 0.668 0.615 91 0.76 0.38 0.07
teamMember 0.01 203 0.8 0.64 0.48
companiesHeadquarteredIn 0.05 42 0.6 0.54 0.60
publicationJournalist 0.02 25 0.7 0.70 0.64
producedBy 0.19 13 0.5 0.58 0.68 N-FOIL does not produce results
competesWith 0.19 74 0.6 0.56 0.72 for non-functional predicates
hasOfficeInCity 0.03 262 0.9 0.84 0.60
teamWonTrophy 0.24 56 0.5 0.50 0.46
worksFor 0.13 62 0.6 0.60 0.74
average 0.11 92 0.650 0.620 0.615
536
N-FOIL cannot be applied to?PRA exhibits a wide
range of performance in cross-validation. The are 43
tasks for which PRA obtains MRR higher than 0.4
and builds a model with more than 10 path features.
We randomly sampled 8 of these predicates to be
evaluated by Amazon Mechanical Turk.
Table 3 shows the top two weighted PRA features
for each task on which N-FOIL can successfully
learn rules. These PRA rules can be categorized into
broad coverage rules which behave like priors over
correct answers (e.g. 1-2, 4-6, 15), accurate rules
which leverage specific relation sequences (e.g. 9,
11, 14), rules which leverage information about the
synonyms of the query node (e.g. 7-8, 10, 12),
and rules which leverage information from a local
neighborhood of the query node (e.g. 3, 12-13, 16).
The synonym paths are useful, because an entity
may have multiple names on the web. We find
that all 17 general rules (no specialization) learned
by N-FOIL can be expressed as length two relation
paths such as path 11. In comparison, PRA explores
a feature space with many length three paths.
For each relation R to be evaluated, we generate
test queries s which belong to domain(R). Queries
which appear in the training set are excluded. For
each query node s, we applied a trained model
(either PRA or N-FOIL) to generate a ranked list
of candidate t nodes. For PRA, the candidates
are sorted by their scores as in Eq. (6). For
N-FOIL, the candidates are sorted by the estimated
accuracies of the rules as in Eq. (2) (which generate
the candidates). Since there are about 7 thousand
(and 13 thousand) test queries s for each functional
(and non-functional) predicate R, and there are
(potentially) thousands of candidates t returned for
each query s, we cannot evaluate all candidates of
all queries. Therefore, we first sort the queries s for
each predicate R by the scores of their top ranked
candidate t in descending order, and then calculate
precisions at top 10, 100 and 1000 positions for the
list of result R(sR,1, tR,11 ), R(sR,2, tR,21 ), ..., where
sR,1 is the first query for predicate R, tR,11 is its first
candidate, sR,2 is the second query for predicate R,
tR,21 is its first candidate, so on and so forth. To
reduce the labeling load, we judge all top 10 queries
for each predicate, but randomly sample 50 out of
the top 100, and randomly sample 50 out of the
Table 5: Comparing Mechanical Turk workers? voted
assessments with our gold standard labels based on 100
samples.
AMT=F AMT=T
Gold=F 25% 15%
Gold=T 11% 49%
top 1000. Each belief is evaluated by 5 workers
at Mechanical Turk, who are given assertions like
?Hines Ward plays for the team Steelers?, as well
as Google search links for each entity, and the
combination of both entities. Statistics shows
that the workers spend on average 25 seconds to
judge each belief. We also remove some workers?
judgments which are obviously incorrect5. We
sampled 100 beliefs, and compared their voted result
to gold-standard labels produced by one author of
this paper. Table 5 shows that 74% of the time the
workers? voted result agrees with our judgement.
Table 4 shows the evaluation result. The
Pmajority column shows for each predicate the
accuracy achieved by the majority prediction: given
a query R(a, ?), predict the b that most often
satisfies R over all possible a in the knowledge
base. Thus, the higher Pmajority is, the simpler
the task. Predicting the functional predicates
is generally easier predicting the non-functional
predicates. The #Query column shows the number
of queries on which N-FOIL is able to match any
of its rules, and hence produce a candidate belief.
For most predicates, N-FOIL is only able to produce
results for at most a few hundred queries. In
comparison, PRA is able to produce results for 6,599
queries on average for each functional predicate, and
12,519 queries on average for each non-functional
predicate. Although the precision at 10 (p@10) of
N-FOIL is comparable to that of PRA, precision
at 10 and at 1000 (p@100 and p@1000) are much
lower6.
The #Path column shows the number of paths
learned by PRA, and the #Rule column shows the
number of rules learned by N-FOIL, with the num-
bers before brackets correspond to unspecialized
rules, and the numbers in brackets correspond to
5Certain workers label all the questions with the same
answer
6If a method makes k predictions, and k < n, then p@n is
the number correct out of the k predictions, divided by n
537
specialized rules. Generally, specialized rules have
much smaller recall than unspecialized rules. There-
fore, the PRA approach achieves high recall partially
by combining a large number of unspecialized paths,
which correspond to unspecialized rules. However,
learning more accurate specialized paths is part of
our future work.
A significant advantage of PRA over N-FOIL is
that it can be applied to non-functional predicates.
The last eight rows of Table 4 show PRA?s
performance on eight of these predicates. Compared
to the result on functional predicates, precisions
at 10 and at 100 of non-functional predicates
are slightly lower, but precisions at 1000 are
comparable. We note that for some predicates
precision at 1000 is better than at 100. After
some investigation we found that for many relations,
the top portion of the result list is more diverse:
i.e. showing products produced by different com-
panies, journalist working at different publications.
While the lower half of the result list is more
homogeneous: i.e. showing relations concentrated
on one or two companies/publications. On the
other hand, through the process of labeling the
Mechanical Turk workers seem to build up a prior
about which company/publication is likely to have
correct beliefs, and their judgments are positively
biased towards these companies/publications. These
two factors combined together result in positive bias
towards the lower portion of the result list. In future
work we hope to design a labeling strategy which
avoids this bias.
4 Conclusions and Future Work
We have shown that a soft inference procedure based
on a combination of constrained, weighted, random
walks through the knowledge base graph can be
used to reliably infer new beliefs for the knowledge
base. We applied this approach to a knowledge
base of approximately 500,000 beliefs extracted
imperfectly from the web by NELL. This new
system improves significantly over NELL?s earlier
Horn-clause learning and inference method: it
obtains nearly double the precision at rank 100. The
inference and learning are both very efficient?our
experiment shows that the inference time is as fast
as 10 milliseconds per query on average, and the
training for a predicate takes only a few seconds.
There are several prominent directions for future
work. First, inference starting from both the query
nodes and target nodes (Richards and Mooney,
1992) can be much more efficient in discovering
long paths than just inference from the query nodes.
Second, inference starting from the target nodes
of training queries is a potential way to discover
specialized paths (with grounded nodes). Third,
generalizing inference paths to inference trees or
graphs can produce more expressive random walk
inference models. Overall, we believe that random
walk is a promising way to scale up relational
learning to domains with very large data sets.
Acknowledgments
This work was supported by NIH under grant
R01GM081293, by NSF under grant IIS0811562,
by DARPA under awards FA8750-08-1-0009 and
AF8750-09-C-0179, and by a gift from Google.
We thank Geoffrey J. Gordon for the suggestion
of applying low variance sampling to random walk
inference. We also thank Bryan Kisiel for help with
the NELL system.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries - DL ?00, pages 85?94, New York, New York,
USA. ACM Press.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
Information Extraction from the Web. In IJCAI, pages
2670?2676.
Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe,
Soumen Chakrabarti, and S. Sudarshan. 2002.
Keyword searching and browsing in databases using
banks. ICDE, pages 431?440.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the eleventh annual conference on
Computational learning theory - COLT? 98, pages
92?100, New York, New York, USA. ACM Press.
MJ Cafarella, M Banko, and O Etzioni. 2006. Relational
Web Search. In WWW.
Jamie Callan and Mark Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
538
Mitchell. 2010. Toward an Architecture for
Never-Ending Language Learning. In AAAI.
William W. Cohen and David Page. 1995. Polyno-
mial learnability and inductive logic programming:
Methods and results. New Generation Comput.,
13(3&4):369?409.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Alpa Jain and Patrick Pantel. 2010. Factrank: Random
walks on a web of facts. In COLING, pages 501?509.
Ni Lao and William W. Cohen. 2010a. Fast query exe-
cution for retrieval models based on path-constrained
random walks. KDD.
Ni Lao and William W. Cohen. 2010b. Relational
retrieval using a combination of path-constrained
random walks. Machine Learning.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
EMNLP, pages 907?916.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In ACL.
J. Ross Quinlan and R. Mike Cameron-Jones. 1993.
FOIL: A Midterm Report. In ECML, pages 3?20.
Bradley L. Richards and Raymond J. Mooney. 1992.
Learning relations by pathfinding. In Proceedings
of the Tenth National Conference on Artificial Intel-
ligence (AAAI-92), pages 50?55, San Jose, CA, July.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling Textual Inference to the Web.
In EMNLP, pages 79?88.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic Taxonomy Induction from Heterogenous
Evidence. In ACL.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
2005. Probabilistic Robotics (Intelligent Robotics and
Autonomous Agents). The MIT Press.
Alexander Yates, Michele Banko, Matthew Broadhead,
Michael J. Cafarella, Oren Etzioni, and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. In HLT-NAACL (Demonstra-
tions), pages 25?26.
John M. Zelle, Cynthia A. Thompson, Mary Elaine
Califf, and Raymond J. Mooney. 1995. Inducing
logic programs without explicit negative examples.
In Proceedings of the Fifth International Workshop
on Inductive Logic Programming (ILP-95), pages
403?416, Leuven, Belgium.
539
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1017?1026, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Reading The Web with Learned Syntactic-Semantic Inference Rules
Ni Lao1?, Amarnag Subramanya2, Fernando Pereira2, William W. Cohen1
1Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
2Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA
nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu
Abstract
We study how to extend a large knowledge
base (Freebase) by reading relational informa-
tion from a large Web text corpus. Previous
studies on extracting relational knowledge
from text show the potential of syntactic
patterns for extraction, but they do not exploit
background knowledge of other relations
in the knowledge base. We describe a
distributed, Web-scale implementation of a
path-constrained random walk model that
learns syntactic-semantic inference rules for
binary relations from a graph representation
of the parsed text and the knowledge base.
Experiments show significant accuracy im-
provements in binary relation prediction over
methods that consider only text, or only the
existing knowledge base.
1 Introduction
Manually-created knowledge bases (KBs) often lack
basic information about some entities and their
relationships, either because the information was
missing in the initial sources used to create the
KB, or because human curators were not confident
about the status of some putative fact, and so they
excluded it from the KB. For instance, as we will
see in more detail later, many person entries in
Freebase (Bollacker et al 2008) lack nationality
information. To fill those KB gaps, we might use
general rules, ideally automatically learned, such as
?if person was born in town and town is in country
?This research was carried out during an internship at
Google Research
then the person is a national of the country.? Of
course, rules like this may be defeasible, in this case
for example because of naturalization or political
changes. Nevertheless, many such imperfect rules
can be learned and combined to yield useful KB
completions, as demonstrated in particular with the
Path-Ranking Algorithm (PRA) (Lao and Cohen,
2010; Lao et al 2011), which learns such rules on
heterogenous graphs for link prediction tasks.
Alternatively, we may attempt to fill KB gaps by
applying relation extraction rules to free text. For
instance, Snow et al(2005) and Suchanek et al
(2006) showed the value of syntactic patterns in
extracting specific relations. In those approaches,
KB tuples of the relation to be extracted serve as
positive training examples to the extraction rule
induction algorithm. However, the KB contains
much more knowledge about other relations that
could potentially be helpful in improving relation
extraction accuracy and coverage, but that is not
used in such purely text-based approaches.
In this work, we use PRA to learn weighted
rules (represented as graph path patterns) that
combine both semantic (KB) and syntactic infor-
mation encoded respectively as edges in a graph-
structured KB, and as syntactic dependency edges
in dependency-parsed Web text. Our approach can
easily incorporate existing knowledge in extraction
tasks, and its distributed implementation scales to
the whole of the Freebase KB and 60 million parsed
documents. To the best of our knowledge, this is the
first successful attempt to apply relational learning
methods to heterogeneous data with this scale.
1017
1.1 Terminology and Notation
In this study, we use a simplified KB consisting of a
set C of concepts and a set R of labels. Each label r
denotes some binary relation partially represented in
the KB. The concrete KB is a directed, edge-labeled
graph G = (C, T ) where T ? C ? R ? C is the
set of labeled edges (also known as triples) (c, r, c?).
Each triple represents an instance r(c, c?) of the
relation r ? R. The KB may be incomplete, that
is, r(c, c?) holds in the real world but (c, r, c?) 6? T .
Our method will attempt to learn rules to infer such
missing relation instances by combining the KB
with parsed text.
We denote by r?1 the inverse relation of r:
r(c, c?) ? r?1(c?, c). For instance Parent?1 is
equivalent to Children. It is convenient to take G
as containing triple (c?, r?1, c) whenever it contains
triple (c, r, c?).
A path type in G is a sequence pi = ?r1, . . . , rm?.
An instance of the path type is a sequence of nodes
c0, . . . , cm such that ri(ci?1, ci). For instance, ?the
persons who were born in the same town as the
query person?, and ?the nationalities of persons who
were born in the same town as the query person? can
be reached respectively through paths matching the
following types
pi1 :
?
BornIn,BornIn?1
?
pi2 :
?
BornIn,BornIn?1,Nationality
?
1.2 Learning Syntactic-Semantic Rules with
Path-Constrained Random Walks
Given a query concept s ? C and a relation
r ? R, PRA begins by enumerating a large set of
bounded-length path types. These path types are
treated as ranking ?experts,? each generating some
random instance of the path type starting from s, and
ranking end nodes t by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? by using logistic
regression to predict the probability that the relation
r(s, t) holds.
In this study, we test the hypothesis that PRA can
be used to find useful ?syntactic-semantic patterns?
? that is, patterns that exploit both semantic
and syntactic relationships, thereby using semantic
knowledge as background in interpreting syntactic
 
wrote
She
Mention
dobj
Charlotte
was
nsubj
nsubj
Jane Eyre
Charlotte
Bronte
Mention
Jane Eyre
Mention
Coreference Resolution
Entity 
Resolution
Freebase
News Corpus
Dependency Trees
Write
Patrick Bront?HasFather
?
Profession
Writer
Figure 1: Knowledge base and parsed text as a labeled
graph. For clarity, some word nodes are omitted.
relationships. As shown in Figure 1, we extend the
KB graph G with nodes and edges from text that
has been syntactically analyzed with a dependency
parser1 and where pronouns and other anaphoric
referring expressions have been clustered with their
antecedents. The text nodes are word/phrase
instances, and the edges are syntactic dependencies
labeled by the corresponding dependency type.
Mentions of entities in the text are linked to KB
concepts by mention edges created by an entity
resolution process.
Given for instance the query
Profession(CharlotteBronte, ?), PRA produces
a ranked list of answers that may have the relation
Profession with the query node CharlotteBronte.
The features used to score answers are the
random walk probabilities of reaching a certain
profession node from the query node by paths
with particular path types. PRA can learn path
types that combine background knowledge in
the database with syntactic patterns in the text
corpus. We now exemplify some path types
involving relations described in Table 3. Type
?
M, conj,M?1,Profession
?
is active (matches
paths) for professions of persons who are mentioned
in conjunction with the query person as in
?collaboration between McDougall and Simon
1Stanford dependencies (de Marneffe and Manning, 2008).
1018
Philips?. For a somewhat subtler example, type
?
M,TW,CW?1,Profession?1,Profession
?
is active
for persons who are mentioned by their titles as in
?President Barack Obama?. The type subsequence
?
Profession?1,Profession
?
ensures that only
profession concepts are activated. The features
generated from these path types combine syntactic
dependency relations (conj) and textual information
relations (TW and CW) with semantic relations in
the KB (Profession).
Experiments on three Freebase relations (profes-
sion, nationality and parents) show that exploiting
existing background knowledge as path features
can significantly improve the quality of extraction
compared with using either Freebase or the text
corpus alone.
1.3 Related Work
Information extraction from varied unstructured and
structured sources involves both complex relational
structure and uncertainty at all levels of the extrac-
tion process. Statistical Relational Learning (SRL)
seeks to combine statistical and relational learning
methods to address such tasks. However, most SRL
approaches (Friedman et al 1999; Richardson and
Domingos, 2006) suffer the complexity of inference
and learning when applied to large scale problems.
Recently, Lao and Cohen (2010) introduced Path
Ranking algorithm, which is applicable to larger
scale problems such as literature recommendation
(Lao and Cohen, 2010) and inference on a large
knowledge base (Lao et al 2011).
Much of the previous work on automatic relation
extraction was based on certain lexico-syntactic
patterns. Hearst (1992) first noticed that patterns
such as ?NP and other NP? and ?NP such as NP?
often imply hyponym relations (NP here refers to
a noun phrase). However, such approaches to
relation extraction are limited by the availability of
domain knowledge. Later systems for extracting
arbitrary relations from text mostly use shallow
surface text patterns (Etzioni et al 2004; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002).
The idea of using sequences of dependency edges
as features for relation extraction was explored by
Snow et al(2005) and Suchanek et al(2006). They
define features to be shortest paths on dependency
trees which connect pairs of NP candidates.
This study is most closely related to work of
Mintz et al(2009), who also study the problem of
extending Freebase with extraction from parsed text.
As in our work, they use a logistic regression model
with path features. However, their approach does not
exploit existing knowledge in the KB. Furthermore,
their path patterns are used as binary-values features.
We show experimentally that fractional-valued
features generated by random walks provide much
higher accuracy than binary-valued ones.
Culotta et al(2006)?s work is similar to our
approach in the sense of relation extraction by
discovering relational patterns. However while
they focus on identifying relation mentions in text
(microreading),this work attempts to infer new
tuples by gathering path evidence over the whole
corpus (macroreading). In addition, their work
involves a few thousand examples, while we aim for
Web-scale extraction.
Do and Roth (2010) use a KB (YAGO) to
aid the generation of features from free text.
However their method is designed specifically for
extracting hierarchical taxonomic structures, while
our algorithm can be used to discover relations for
general general graph-based KBs.
In this paper we extend the PRA algorithm along
two dimensions: combining syntactic and semantic
cues in text with existing knowledge in the KB;
and a distributed implementation of the learning and
inference algorithms that works at Web scale.
2 Path Ranking Algorithm
We briefly review the Path Ranking algorithm
(PRA), described in more detail by Lao and Cohen
(2010). Each path type pi = ?r1, r2, ..., r`? specifies
a real-valued feature. For a given query-answer node
pair (s, t), the value of the feature pi is P (s? t;pi),
the probability of reaching t from s by a random
walk that instantiates the type. More specifically,
suppose that the random walk has just reached vi by
traversing edges labeled r1, . . . , ri with s=v0. Then
vi+1 is drawn at random from all nodes reachable
from vi by edges labeled ri+1. A path type pi is
active for pair (s, t) if P (s? t;pi) > 0.
Let B = {?, pi1, ..., pin} be the set of all path
types of length no greater than ` that occur in
the graph together with the dummy type ?, which
1019
represents the bias feature. For convenience, we set
P (s ? t;?) = 1 for any nodes s, t. The score for
whether query node s is related to another node t by
relation r is given by
score(s, t) =
?
pi?B
P (s? t;pi)?pi ,
where ?pi is the weight of feature pi. The model
parameters to be learned are the vector ? =
??pi?pi?B . The procedures used to discover B and
estimate ? are described in the following. Finally,
note that we train a separate PRA model for each
relation r.
Path Discovery: Given a graph and a target
relation r, the total number of path types is an
exponential function of the maximum path length
` and considering all possible paths would be
computationally very expensive. As a result, B is
constructed using only path types that satisfy the
following two constraints:
1. the path type is active for more than K training
query nodes, and
2. the probability of reaching any correct target
node t is larger than a threshold ? on average
for the training query nodes s.
We will discuss how K, ? and the training queries
are chosen in Section 5. In addition to making the
training more efficient, these constraints are also
helpful in removing low quality path types.
Training Examples: For each relation r of inter-
est, we start with a set of node pairs Sr = {(si, ti)}.
From Sr, we create the training setDr = {(xi, yi)},
where xi = ?P (si ? ti;pi)?pi?B is the vector
of path feature values for the pair (si, ti), and yi
indicates whether r(si, ti) holds.
Following previous work (Lao and Cohen, 2010;
Mintz et al 2009), node pairs that are in r in
the KB are legitimate positive training examples2.
One can generate negative training examples by
considering all possible pairs of concepts whose
type is compatible with r (as given by the schema)
and are not present in the KB. However this
2In our experiments we subsample the positive examples.
See section 3.2 for more details.
procedure leads to a very large number of negative
examples (e.g., for the parents relation, any pair of
person concepts which are related by this relation
would be valid negative examples) which not only
makes training very expensive but also introduces
an incorrect bias in the training set. Following
Lao and Cohen (2010) we use a simple biased
sampling procedure to generate negative examples:
first, the path types discovered in the previous (path
discovery) step are used to construct an initial PRA
model (all feature weights are set to 1.0); then, for
each query node si, this model is used to retrieve
candidate answer nodes, which are then sorted in
descending order by their scores; finally, nodes at
the k(k + 1)/2-th positions are selected as negative
samples, where k = 0, 1, 2, ....
Logistic Regression Training: Given a training
set D, we estimate parameters ? by maximizing the
following objective
F(?) =
1
|D|
?
(x,y)?D
f(x, y;?)? ?1???1 ? ?2???22
where ?1 and ?2 control the strength of the L1-
regularization which helps with structure selection
and L22-regularization which prevents overfitting.
The log-likelihood f(x, y;?) of example (x, y) is
given by
f(x, y,?) = y ln p(x,?) + (1? y) ln(1? p(x,?))
p(x,?) =
exp(?Tx)
1 + exp(?Tx)
.
Inference: After a model is trained for a relation
r in the knowledge base, it can be used to produce
new instances of r. We first generate unlabeled
queries s which belong to the domain of r. Queries
which appear in the training set are excluded. For
each unlabeled query node s, we apply the trained
PRA model to generate a list of candidate t nodes
together with their scores. We then sort all the
predictions (s, t) by their scores in descending order,
and evaluate the top ones.
3 Extending PRA
As described in the previous section, the PRA model
is trained on positive and negative queries generated
from the KB. As Freebase contains millions of
1020
concepts and edges, training on all the generated
queries is computationally challenging. Further,
we extend the Freebase graph with parse paths of
mentions of concepts in Freebase in millions of Web
pages. Yet another issue is that the training queries
generated using Freebase are inherently biased
towards the distribution of concepts in Freebase
and may not reflect the distribution of mentions of
these concepts in text data. As one of the goals of
our approach is to learn relation instances that are
missing in Freebase, training on such a set biased
towards the distribution of concepts in Freebase may
not lead to good performance. In this section we
explain how we modified the PRA algorithm to
address those issues.
3.1 Scaling Up
Most relations in Freebase have a large set of
existing triples. For example, for the profession
relation, there are around 2 million persons in
Freebase, and about 0.3 million of them have known
professions. This results in more than 0.3 million
training queries (persons), each with one or more
positive answers (professions), and many negative
answers, which make training computationally
challenging. Generating all the paths for millions
of queries over a graph with millions of concepts
and edges further complicates the computational
issues. Incorporating the parse path features from
the text only exacerbates the matter. Finally once we
have trained a PRA model for a given relation, say
profession, we would like to infer the professions for
all the 1.7 million persons whose professions are not
known to Freebase (and possibly predict changes to
the profession information of the 0.3 million people
whose professions were given).
We use distributed computing to deal with the
large number of training and prediction queries
over a large graph. A key observation is that the
different stages of the PRA algorithm are based
on independent computations involving individual
queries. Therefore, we can use the MapReduce
framework to distribute the computation (Dean and
Ghemawat, 2008). For path discovery, we modify
Lao et als path finding (2011) approach to decouple
the queries: instead of using one depth-first search
that involves all the queries, we first find all paths
up to certain length for each query node in the
map stage, and then collect statistics for each path
from all the query nodes in the reduce stage. We
used a 500-machine, 8GB/machine cluster for these
computations.
Another challenge associated with applying PRA
to a graph constructed using a large amounts of
text is that we cannot load the entire graph on a
single machine. To circumvent this problem, we first
index all parsed sentences by the concepts that they
mention. Therefore, to perform a random walk for a
query concept s, we only load the sentences which
mention s.
3.2 Sampling Training Data
Using the r-edges in the KB as positive examples
distorts the training set. For example, for the
profession relation, there are 0.3 million persons
for whom Freebase has profession information, and
amongst these 0.24 million are either politicians
or actors. This may not reflect the distribution
of professions of persons mentioned in Web data.
Using all of these as training queries will most
certainly bias the trained model towards these
professions as PRA is trained discriminatively. In
other words, training directly with this data would
lead to a model that is more likely to predict
professions that are popular in Freebase. To avoid
this distortion, we use stratified sampling. For each
relation r and concept t ? C, we count the number
of r edges pointing to t
Nr,t = |{(s, r, t) ? T}| .
Given a training query (s, r, t) we sample it
according to
Pr,t = min
(
1,
?
m+Nr,t
Nr,t
)
We fix m = 100 in our experiments. If we take the
profession relation as an example, the above implies
that for popular professions, we only sample about
?
Nr,t out of the Nr,t possible queries that end in t,
whereas for the less popular professions we would
accept all the training queries.
3.3 Text Graph Construction
As we are processing Web text data (see following
section for more detail), the number of mentions
1021
of a concept follows a somewhat heavy-tailed
distribution: there are a small number of very
popular concepts (head) and a large number of not
so popular concepts (tail). For instance the concept
BarackObama is mentioned about 8.9 million times
in our text corpus. To prevent the text graph from
being dominated by the head concepts, for each
sentence that mentions concept c ? C, we accept
it as part of the text graph with probability:
Pc = min
(
1,
?
k + Sc
Sc
)
where Sc is the number of sentences in which c is
mentioned in the whole corpus. In our experiments
we use k = 105. This means that if Sc  k, then we
only sample about
?
Sc of the sentences that contain
a mention of the concept, while if Sc  k, then all
mentions of that concept will likely be included.
4 Datasets
We use Freebase as our knowledge base. Freebase
data is harvested from many sources, including
Wikipedia, AMG, and IMDB.3 As of this writing,
it contains more than 21 million concepts and 70
million labeled edges. For a large majority of con-
cepts that appear both in Freebase and Wikipedia,
Freebase maintains a link to the Wikipedia page of
that concept.
We also collect a large Web corpus and identify
60 million pages that mention concepts relevant
to this study. The free text on those pages
are POS-tagged and dependency parsed with an
accuracy comparable to that of the current Stanford
dependency parser (Klein and Manning, 2003). The
parser produces a dependency tree for each sentence
with each edge labeled with a standard dependency
tag (see Figure 1).
In each of the parsed documents, we use POS tags
and dependency edges to identify potential referring
noun phrases (NPs). We then use a within-document
coreference resolver comparable to that of Haghighi
and Klein (2009) to group referring NPs into
co-referring clusters. For each cluster that contains a
proper-name mention, we find the Freebase concept
or concepts, if any, with a name or alias that matches
3www.wikipedia.org, www.allmusic.com, www.
imdb.com.
Table 1: Size of training and test sets for each relation.
Task Training Set Test Set
Profession 22,829 15,219
Nationality 14,431 9,620
Parents 21,232 14,155
the mention. If a cluster has multiple possible
matching Freebase concepts, we choose a single
sense based on the following simple model. For
each Freebase concept c ? C, we computeN(c,m),
the number of times the concept c is referred by
mention m by using both the alias information
in Freebase and the anchors of the corresponding
Wikipedia page for that concept. Based on N(c,m)
we can calculate the empirical probability p(c|m) =
N(c,m)/
?
c? N(c
?,m). If u is a cluster with
mention set M(u) in the document, and C(m) the
set of concepts in KB with name or alias m, we
assign u to concept c? = argmax
c?C(m),m?M(u)
p(c|m),
provided that there exists at least one c ? C(m) and
m ? M(u) such that p(c|m) > 0. Note that M(c)
only contains the proper-name mentions in cluster c.
5 Results
We use three relations profession, nationality and
parents for our experiments. For each relation, we
select its current set of triples in Freebase, and apply
the stratified sampling (Section 3.2) to each of the
three triple sets. The resulting triple sets are then
randomly split into training (60% of the triples) and
test (the remaining triples). However, the parents
relation yields 350k triples after stratified sampling,
so to reduce experimental effort we further randomly
sub-sample 10% of that as input to the train-test
split. Table 1 shows the sizes of the training and
test sets for each relation.
To encourage PRA to find paths involving the
text corpus, we do not count relation M (which
connects concepts to their mentions) or M?1 when
calculating path lengths. We use L1/L22-regularized
logistic regression to learn feature weights. The
PRA hyperparameters (? and K as defined in
Section 2) and regularizer hyperparameters are
tuned by threefold cross validation (CV) on the
training set. We average the models across all
the folds and choose the model that gives the best
1022
Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and
KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and
text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results
shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions
significance test.
Task KB Text KB+Text KB+Text[b]
Profession 0.532 0.516 0.583 0.453
Nationality 0.734 0.729 0.812 0.693
Parents 0.329 0.332 0.392 0.319
performance on the training set for each relation.
We report results of two evaluations. First, we
evaluate the performance of the PRA algorithm
when trained on a subset of existing Freebase facts
and tested on the rest. Second, we had human
annotators verify facts proposed by PRA that are not
in Freebase.
5.1 Evaluation with Existing Knowledge
Previous work in relation extraction from parsed
text (Mintz et al 2009) has mostly used binary
features to indicate whether a pattern is present in
the sentences where two concepts are mentioned.
To investigate the benefit of having fractional valued
features generated by random walks (as in PRA), we
also evaluate a binarized PRA approach, for which
we use the same syntactic-semantic pattern features
as PRA does, but binarize the feature values from
PRA: if the original fractional feature value was
zero, the feature value is set to zero (equivalent to
not having the feature in that example), otherwise it
is set to 1.
Table 2 shows a comparison of the results
obtained using the PRA algorithm trained using
only Freebase (KB), using only the text corpus
graph (Text), trained with both Freebase and the
text corpus (KB+Text) and the binarized PRA
algorithm using both Freebase and the text corpus
(KB+Text[b]). We report Mean Reciprocal Rank
(MRR) where, given a set of queries Q,
MRR =
1
|Q|
?
q?Q
1
rank of q?s first correct answer
.
Comparing the results of first three columns we
see that combining Freebase and text achieves
significantly better results than using either Freebase
or text alone. Further comparing the results of last
two columns we also observe a significant drop in
MRR for the binarized version of PRA. This clearly
shows the importance of using the random walk
probabilities. It can also be seen that the MRR for
the parents relation is lower than those for other
relations. This is mainly because there are larger
number of potential answers for each query node of
Parent relation than for each query node of the other
two relations ? all persons in Freebase versus all
professions or nationalities. Finally, it is important
to point out that our evaluations are actually lower
bounds of actual performance, because, for instance,
a person might have a profession besides the ones in
Freebase and in such cases, this evaluation does not
give any credit for predicting those professions ?
they are treated as errors. We try to address this issue
with the manual evaluations in the next section.
Table 2 only reports results for the maximum path
length ` = 4 case. We found that shorter maximum
path lengths give worse results: for instance, with
` = 3 for the profession relation, MRR drops to
0.542, from 0.583 for ` = 4 when using both
Freebase and text. This difference is significant
at the 0.0001 level according to a difference of
proportions test. Further we find that using longer
path length takes much longer time to train and test,
but does not lead to significant improvements over
the ` = 4 case. For example, for profession, ` = 5
gives a MRR of 0.589.
Table 3 shows the top weighted features that
involve text edges for PRA models trained on both
Freebase and the text corpus. To make them
easier to understand, we group them based on their
functionality. For the profession and nationality
tasks, the conjunction dependency relation (in group
1,4) plays an important role: these features first find
persons mentioned in conjunction with the query
1023
Table 3: Top weighted path types involving text edges for each task grouped according to functionality. M relations
connect each concept in knowledge base to its mentions in the corpus. TW relations connect each token in a sentence to
the words in the text representation of this token. CW relations connect each concept in knowledge base to the words
in the text representation of this concept. We use lower case names to denote dependency edges, word capitalized
names to denote KB edges, and ??1 ? to denote the inverse of a relation.
Profession Top Weighted Features Comments
1
?
M, conj,M?1,Profession
?
Professions of persons mentioned in conjunction
with the query person: ?McDougall and Simon
Phillips collaborated ...?
?
M, conj?1,M?1,Profession
?
2
?
M,TW,CW?1,Profession?1,Profession
?
Active if a person is mentioned by his profession:
?The president said ...?
3
?
M,TW,TW?1,M?1,Children,Profession
?
First find persons with similar names or
mentioned in similar ways, then aggregate the
professions of their children/parents/advisors:
starting from the concept BarackObama, words
such as ?Obama?, ?leader?, ?president?, and
?he? are reachable through path ?M,TW?
?
M,TW,TW?1,M?1,Parents,Profession
?
?
M,TW,TW?1,M?1,Advisors,Profession
?
Nationality Top Weighted Features Comments
4
?
M, conj,TW,CW?1,Nationality
?
The nationalities of persons mentioned in
conjunction with the query person: ?McDougall
and Simon Phillips collaborated ...?
?
M, conj?1,TW,CW?1,Nationality
?
5
?
M, nc?1,TW,CW?1,Nationality
?
The nationalities of persons mentioned close to
the query person through other dependency
relations.
?
M, tmod?1,TW,CW?1,Nationality
?
?
M, nn,TW,CW?1,Nationality
?
6
?
M, poss, poss?1,M?1,PlaceOfBirth,ContainedBy
?
The birth/death places of the query person with
restrictions to different syntactic constructions.
?
M, title, title?1,M?1,PlaceOfDeath,ContainedBy
?
Parents Top Weighted Features Comments
7
?
M,TW,CW?1,Parents
?
The parents of persons with similar names or
mentioned in similar ways: starting from the
concept CharlotteBronte words such as
?Bronte?, ?Charlotte?, ?Patrick??, and ?she? are
reachable through path ?M,TW?.
8
?
M, nsubj, nsubj?1,TW,CW?1
?
Persons with similar names or mentioned in
similar ways to the query person with various
restrictions or expansions.
?
nsubj, nsubj?1
?
and
?
nc?1, nc
?
require the query to be subject and
noun compound respectively.
?
TW?1,TW
?
expands further by word similarities.
?
M, nsubj, nsubj?1,M?1,CW,CW?1
?
?
M, nc?1, nc,TW,CW?1
?
?
M,TW,CW?1
?
?
M,TW,TW?1,TW,CW?1
?
1024
person, and then find their professions or nation-
alities. The features in group 2 capture the fact
that sometimes people are mentioned by their pro-
fessions. The subpath
?
Profession?1,Profession
?
ensures that only profession related concepts are
activated. Features in group 3 first find persons
with similar names or mentioned in similar ways
to the query person, and then aggregate the
professions of their children, parents, or advisors.
Features in group 6 can be seen as special
versions of feature ?PlaceOfBirth,ContainedBy?
and ?PlaceOfDeath,ContainedBy?. The subpaths
?
M, poss, poss?1,M?1
?
and
?
M, title, title?1,M?1
?
return the random walks back to the query node only
if the mentions of the query node have poss (stands
for possessive modifier, e.g. ?Bill?s clothes?) or title
(stands for person?s title, e.g. ?President Obama?)
edges in text; otherwise these features are inactive.
Therefore, these features are active only for specific
subsets of queries. Features in group 8 generally find
persons with similar names or mentioned in similar
ways to the query person. However, they further
expand or restrict this person set in various ways.
Typically, each trained model includes hundreds
of paths with non-zero weights, so the bulk of
classifications are not based on a few high-precision-
recall patterns, but rather on the combination of
a large number of lower-precision high-recall or
high-precision lower-recall rules.
5.2 Manual Evaluation
We performed two sets of manual evaluations. In
each case, an annotator is presented with the triples
predicted by PRA, and asked if they are correct. The
annotator has access to the Freebase and Wikipedia
pages for the concepts (and is able to issue search
queries about the concepts).
In the first evaluation, we compared the perfor-
mance of two PRA models, one trained using the
stratified sampled queries and another trained using
a randomly sampled set of queries for the profession
relation. For each model, we randomly sample 100
predictions from the top 1000 predictions (sorted by
the scores returned by the model). We found that the
PRA model trained with stratified sampled queries
has 0.92 precision, while the other model has only
0.84 precision (significant at the 0.02 level). This
shows that stratified sampling leads to improved
Table 4: Human judgement for predicted new beliefs.
Task p@100 p@1k p@10k
Profession 0.97 0.92 0.84
Nationality 0.98 0.97 0.90
Parents 0.86 0.81 0.79
performance.
We also evaluated the new beliefs proposed by
the models trained for all the three relations using
stratified sampled queries. We estimated precision
for the top 100 predictions and randomly sampled
100 predictions each from the top 1,000 and 10,000
predictions. Here we use the PRA model trained
using both KB and text. The results of this
evaluation are shown in Table 4. It can be seen
that the PRA model is able to produce very high
precision predications even when one considers the
top 10,000 predictions.
Finally, note that our model is inductive. For
instance, for the profession relation, we are able to
predict professions for the around 2 million persons
in Freebase. The top 1000 profession facts extracted
by our system involve 970 distinct people, the top
10,000 facts involve 8,726 distinct people, and the
top 100,000 facts involve 79,885 people.
6 Conclusion
We have shown that path constrained random walk
models can effectively infer new beliefs from a
large scale parsed text corpus with background
knowledge. Evaluation by human annotators shows
that by combining syntactic patterns in parsed
text with semantic patterns in the background
knowledge, our model can propose new beliefs
with high accuracy. Thus, the proposed random
walk model can be an effective way to automate
knowledge acquisition from the web.
There are several interesting directions to con-
tinue this line of work. First, bidirectional search
from both query and target nodes can be an efficient
way to discover long paths. This would especially
useful for parsed text. Second, relation paths that
contain constant nodes (lexicalized features) and
conjunction of random walk features are potentially
very useful for extraction tasks.
1025
Acknowledgments
We thank Rahul Gupta, Michael Ringgaard, John
Blitzer and the anonymous reviewers for helpful
comments. The first author was supported by a
Google Research grant.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries, DL ?00, pages 85?94, New York, NY, USA.
ACM.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
296?303, New York City, USA, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Chris Manning.
2008. Stanford dependencies. http:
//www.tex.ac.uk/cgi-bin/texfaq2html?
label=citeURL.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113, January.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1099?1109, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th
international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi
Pfeffer. 1999. Learning Probabilistic Relational
Models. In IJCAI, volume 16, pages 1300?1309.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1152?1161, Singapore, August. Association for
Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proceedings
of COLING-92, pages 539?545. Association for
Computational Linguistics, August.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
423?430. Association for Computational Linguistics,
July.
Ni Lao and William Cohen. 2010. Relational retrieval
using a combination of path-constrained random
walks. Machine Learning, 81:53?67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large
scale knowledge base. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 529?539, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question answering
system. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 41?47, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304, Cambridge,
MA. NIPS Foundation, MIT Press.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?06, pages 712?717, New York, NY, USA.
ACM.
1026
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1302?1312, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multi-Domain Learning: When Do Domains Matter?
Mahesh Joshi
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
maheshj@cs.cmu.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, Maryland 21211
mdredze@cs.jhu.edu
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
wcohen@cs.cmu.edu
Carolyn P. Rose?
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cprose@cs.cmu.edu
Abstract
We present a systematic analysis of exist-
ing multi-domain learning approaches with re-
spect to two questions. First, many multi-
domain learning algorithms resemble ensem-
ble learning algorithms. (1) Are multi-domain
learning improvements the result of ensemble
learning effects? Second, these algorithms are
traditionally evaluated in a balanced class la-
bel setting, although in practice many multi-
domain settings have domain-specific class
label biases. When multi-domain learning
is applied to these settings, (2) are multi-
domain methods improving because they cap-
ture domain-specific class biases? An under-
standing of these two issues presents a clearer
idea about where the field has had success in
multi-domain learning, and it suggests some
important open questions for improving be-
yond the current state of the art.
1 Introduction
Research efforts in recent years have demonstrated
the importance of domains in statistical natural lan-
guage processing. A mismatch between training and
test domains can negatively impact system accuracy
as it violates a core assumption in many machine
learning algorithms: that data points are indepen-
dent and identically distributed (i.i.d.). As a result,
numerous domain adaptation methods (Chelba and
Acero, 2004; Daume? III and Marcu, 2006; Blitzer et
al., 2007) target settings with a training set from one
domain and a test set from another.
Often times the training set itself violates the i.i.d.
assumption and contains multiple domains. In this
case, training a single model obscures domain dis-
tinctions, and separating the dataset by domains re-
duces training data. Instead, multi-domain learn-
ing (MDL) can take advantage of these domain la-
bels to improve learning (Daume? III, 2007; Dredze
and Crammer, 2008; Arnold et al 2008; Finkel and
Manning, 2009; Zhang and Yeung, 2010; Saha et al
2011). One such example is sentiment classification
of product reviews. Training data is available from
many product categories and while all data should
be used to learn a model, there are important differ-
ences between the categories (Blitzer et al 2007)1.
While much prior research has shown improve-
ments using MDL, this paper explores what prop-
erties of an MDL setting matter. Are previous im-
provements from MDL algorithms discovering im-
portant distinctions between features in different do-
mains, as we would hope, or are other factors con-
tributing to learning success? The key question of
this paper is: when do domains matter?
Towards this goal we explore two issues. First,
we explore the question of whether domain distinc-
tions are used by existing MDL algorithms in mean-
ingful ways. While differences in feature behaviors
between domains will hurt performance (Blitzer et
al., 2008; Ben-David et al 2009), it is not clear
if the improvements in MDL algorithms can be at-
tributed to correcting these errors, or whether they
are benefiting from something else. In particular,
there are many similarities between MDL and en-
semble methods, with connections to instance bag-
1Blitzer et al(2007) do not consider the MDL setup, they
consider a single source domain, and a single target domain,
with little or no labeled data available for the target domain.
1302
ging, feature bagging and classifier combination. It
may be that gains in MDL are the usual ensemble
learning improvements.
Second, one simple way in which domains can
change is the distribution of the prior over the la-
bels. For example, reviews of some products may be
more positive on average than reviews of other prod-
uct types. Simply capturing this bias may account
for significant gains in accuracy, even though noth-
ing is learned about the behavior of domain-specific
features. Most prior work considers datasets with
balanced labels. However, in real world applica-
tions, where labels may be biased toward some val-
ues, gains from MDL could be attributed to simply
modeling domain-specific bias. A practical advan-
tage of such a result is ease of implementation and
the ability to scale to many domains.
Overall, irrespective of the answers to these ques-
tions, a better understanding of the performance of
existing MDL algorithms in different settings will
provide intuitions for improving the state of the art.
2 Multi-Domain Learning
In the multi-domain learning (MDL) setting, exam-
ples are accompanied by both a class label and a do-
main indicator. Examples are of the form (xi, y,di),
where xi ? RN , di is a domain indicator, xi is
drawn according to a fixed domain-specific distri-
bution Ddi , and yi is the label (e.g. yi ? {?1,+1}
for binary labels). Standard learning ignores di, but
MDL uses these to improve learning accuracy.
Why should we care about the domain label? Do-
main differences can introduce errors in a number
of ways (Ben-David et al 2007; Ben-David et al
2009). First, the domain-specific distributions Ddi
can differ such that they favor different features, i.e.
p(x) changes between domains. As a result, some
features may only appear in one domain. This aspect
of domain difference is typically the focus of un-
supervised domain adaptation (Blitzer et al 2006;
Blitzer et al 2007). Second, the features may be-
have differently with respect to the label in each do-
main, i.e. p(y|x) changes between domains. As a
result, a learning algorithm cannot generalize the be-
havior of features from one domain to another. The
key idea behind many MDL algorithms is to target
one or both of these properties of domain difference
to improve performance.
Prior approaches to MDL can be broadly catego-
rized into two classes. The first set of approaches
(Daume? III, 2007; Dredze et al 2008) introduce pa-
rameters to capture domain-specific behaviors while
preserving features that learn domain-general be-
haviors. A key of these methods is that they do not
explicitly model any relationship between the do-
mains. Daume? III (2007) proposes a very simple
?easy adapt? approach, which was originally pro-
posed in the context of adapting to a specific target
domain, but easily generalizes to MDL. Dredze et al
(2008) consider the problem of learning how to com-
bine different domain-specific classifiers such that
behaviors common to several domains can be cap-
tured by a shared classifier, while domain-specific
behavior is still captured by the individual classi-
fiers. We describe both of these approaches in ? 3.2.
The second set of approaches to MDL introduce
an explicit notion of relationship between domains.
For example, Cavallanti et al(2008) assume a fixed
task relationship matrix in the context of online
multi-task learning. The key assumption is that in-
stances from two different domains are half as much
related to each other as two instances from the same
domain. Saha et al(2011) improve upon the idea
of simply using a fixed task relationship matrix by
instead learning it adaptively. They derive an online
algorithm for updating the task interaction matrix.
Zhang and Yeung (2010) derive a convex formu-
lation for adaptively learning domain relationships.
We describe their approach in ? 3.2. Finally, Daume?
III (2009) proposes a joint task clustering and multi-
task/multi-domain learning setup, where instead of
just learning pairwise domain relationships, a hier-
archical structure among them is inferred. Hierar-
chical clustering of tasks is performed in a Bayesian
framework, by imposing a hierarchical prior on the
structure of the task relationships.
In all of these settings, the key idea is to learn
both domain-specific behaviors and behaviors that
generalize between (possibly related) domains.
3 Data
To support our analysis we develop several empir-
ical experiments. We first summarize the datasets
and methods that we use in our experiments, then
1303
proceed to our exploration of MDL.
3.1 Datasets
A variety of multi-domain datasets have been used
for demonstrating MDL improvements. In this pa-
per, we focus on two datasets representative of many
of the properties of MDL.
Amazon (AMAZON) Our first dataset is the Multi-
Domain Amazon data (version 2.0), first introduced
by Blitzer et al(2007). The task is binary sentiment
classification, in which Amazon product reviews are
labeled as positive or negative. Domains are defined
by product categories. We select the four domains
used in most studies: books, dvd, electronics
and kitchen appliances.
The original dataset contained 2,000 reviews for
each of the four domains, with 1,000 positive and
1,000 negative reviews per domain. Feature extrac-
tion follows Blitzer et al(2007): we use case insen-
sitive unigrams and bigrams, although we remove
rare features (those that appear less than five times
in the training set). The reduced feature set was se-
lected given the sensitivity to feature size of some of
the MDL methods.
ConVote (CONVOTE) Our second dataset is taken
from segments of speech from United States
Congress floor debates, first introduced by Thomas
et al(2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discus-
sion in the floor debate. We select this dataset be-
cause, unlike the AMAZON data, CONVOTE can be
divided into domains in several ways based on dif-
ferent metadata attributes available with the dataset.
We consider two types of domain divisions: the bill
identifier and the political party of the speaker. Di-
vision based on the bill creates domain differences
in that each bill has its own topic. Division based on
political party implies preference for different issues
and concerns, which manifest as different language.
We refer to these datasets as BILL and PARTY.
We use Version 1.1 of the CONVOTE dataset,
available at http://www.cs.cornell.edu/
home/llee/data/convote.html. More
specifically, we combine the training, development
and test folds from the data stage three/ ver-
sion, and sub-sample to generate different versions
of the dataset required for our experiments. For
BILL we randomly sample speech segments from
three different bills. The three bills and the number
of instances for each were chosen such that we have
sufficient data in each fold for every experiment.
For PARTY we randomly sample speech segments
from the two major political parties (Democrats and
Republicans). Feature processing was identical to
AMAZON, except that the threshold for feature re-
moval was two.
3.2 Learning Methods and Features
We consider three MDL algorithms, two are repre-
sentative of the first approach and one of the second
approach (learning domain similarities) (?2). We fa-
vored algorithms with available code or that were
straightforward to implement, so as to ensure repro-
ducibility of our results.
FEDA Frustratingly easy domain adaptation
(FEDA) (Daume? III, 2007; Daume? III et al 2010b;
Daume? III et al 2010a) is an example of a classifier
combination approach to MDL. The feature space
is a cross-product of the domain and input features,
augmented with the original input features (shared
features). Prediction is effectively a linear combina-
tion of a set of domain-specific weights and shared
weights. We combine FEDA with both the SVM
and logistic regression algorithms described below
to obtain FEDA-SVM and FEDA-LR.
MDR Multi-domain regularization (MDR) (Dredze
and Crammer, 2008; Dredze et al 2009) extends the
idea behind classifier combination by explicitly for-
mulating a classifier combination scheme based on
Confidence-Weighted learning (Dredze et al 2008).
Additionally, classifier updates (which happen in
an online framework) contain an explicit constraint
that the combined classifier should perform well on
the example. Dredze et al(2009) consider several
variants of MDR. We select the two best perform-
ing methods: MDR-L2, which uses the underlying
algorithm of Crammer et al(2008), and MDR-KL,
which uses the underlying algorithm of Dredze et al
(2008). We follow their approach to classifier train-
ing and parameter optimization.
MTRL The multi-task relationship learning
(MTRL) approach proposed by Zhang and Yeung
1304
(2010) achieves states of the art performance on
many MDL tasks. This method is representative
of methods that learn similarities between domains
and in turn regularize domain-specific parameters
accordingly. The key idea in their work is the use
of a matrix-normal distribution p(X|M ,?,?) as
a prior on the matrix W created by column-wise
stacking of the domain-specific classifier weight
vectors. ? represents the covariance matrix for the
variables along the columns of X . When used as
a prior over W it models the covariance between
the domain-specific classifiers (and therefore the
tasks). ? is learned jointly with the domain-specific
classifiers. This method has similar benefits to
FEDA in terms of classifier combination, but also
attempts to model domain relationships. We use
the implementation of MTRL made available by the
authors2. For parameter tuning, we perform a grid
search over the parameters ?1 and ?2, using the fol-
lowing values for each (a total of 36 combinations):
{0.00001, 0.0001, 0.001, 0.01, 0.1, 1}.
In addition to these multi-task learning methods,
we consider a common baseline: ignoring the do-
main distinctions and learning a single classifier
over all the data. This reflects single-domain learn-
ing, in which no domain knowledge is used and will
indicate baseline performance for all experiments.
While some earlier research has included a sepa-
rate one classifier per domain baseline, it almost al-
ways performs worse, since splitting the domains
provides much less data to each classifier (Dredze
et al 2009). So we omit this baseline for simplicity.
To obtain a single classifier we use two classifica-
tion algorithms: SVMs and logistic regression.
Support Vector Machines A single SVM run
over all the training data, ignoring domain labels.
We use the SVM implementation available in the LI-
BLINEAR package (Fan et al 2008). In particular,
we use the L2-regularized L2-loss SVM (option -s
1 in version 1.8 of LIBLINEAR, and also option -B
1 for including a standard bias feature). We tune
the SVM using five-fold stratified cross-validation
on the training set, using the following values for
the trade-off parameterC: {0.0001, 0.001, 0.01, 0.1,
0.2, 0.3, 0.5, 1}.
2http://www.cse.ust.hk/?zhangyu/codes/
MTRL.zip
Logistic Regression (LR) A single logistic re-
gression model run over all the training data, ignor-
ing domain labels. Again, we use the L2-regularized
LR implementation available in the LIBLINEAR
package (option -s 0, and also option -B 1). We
tune the LR model using the same strategy as the
one used for SVM above, including the values of the
trade-off parameter C.
For all experiments, we measure average accu-
racy overK-fold cross-validation, using 10 folds for
AMAZON, and 5 folds for both BILL and PARTY.
4 When Do Domains Matter?
We now empirically explore two questions regarding
the behavior of MDL.
4.1 Ensemble Learning
Question: Are MDL improvements the result of
ensemble learning effects?
Many of the MDL approaches bear a striking
resemblance to ensemble learning. Traditionally,
ensemble learning combines the output from sev-
eral different classifiers to obtain a single improved
model (Maclin and Opitz, 1999). It is well estab-
lished that ensemble learning, applied on top of a
diverse array of quality classifiers, can improve re-
sults for a variety of tasks. The key idea behind
ensemble learning, that of combining a diverse ar-
ray of models, has been applied to settings in which
data preprocessing is used to create many different
classifiers. Examples include instance bagging and
feature bagging (Dietterich, 2000).
The core idea of using diverse inputs in making
classification decisions is common in the MDL liter-
ature. In fact, the top performing and only success-
ful entry to the 2007 CoNLL shared task on domain
adaptation for dependency parsing was a straightfor-
ward implementation of ensemble learning by cre-
ating variants of parsers (Sagae and Tsujii, 2007).
Many MDL algorithms, among them Dredze and
Crammer (2008), Daume? III (2009), Zhang and Ye-
ung (2010) and Saha et al(2011), all include some
notion of learning domain-specific classifiers on the
training data, and combining them in the best way
possible. To be clear, we do not claim that these
approaches can be reduced to an existing ensem-
ble learning algorithm. There are crucial elements
1305
in each of these algorithms that separate them from
existing ensemble learning algorithms. One exam-
ple of such a distinction is the learning of domain
relationships by both Zhang and Yeung (2010) and
Saha et al(2011). However, we argue that their
core approach, that of combining parameters that are
trained on variants of the data (all data or individual
domains), is an ensemble learning idea.
Consider instance bagging, in which multiple
classifiers are each trained on random subsets of the
data. The resulting classifiers are then combined
to form a final model. In MDL, we can consider
each domain a subset of the data, albeit non-random
and non-overlapping. The final model combines the
domain-specific parameters and parameters trained
on other instances, which in the case of FEDA are the
shared parameters. In this light, these methods are a
complex form of instance bagging, and their devel-
opment could be justified from this perspective.
However, given this justification, are improve-
ments from MDL simply the result of standard en-
semble learning effects, or are these methods re-
ally learning something about domain behavior? If
knowledge of domain was withheld from the algo-
rithm, could we expect similar improvements? As
we will do in each empirical experiment, we propose
a contrarian hypothesis:
Hypothesis: Knowledge of domains is irrelevant
for MDL.
Empirical Evaluation We evaluate this hypothe-
sis as follows. We begin by constructing a true MDL
setting, in which we attempt to improve accuracy
through knowledge of the domains. We will apply
three MDL algorithms (FEDA, MDR, and MTRL) to
our three multi-domain datasets (AMAZON, BILL,
and PARTY) and compare them against a single clas-
sifier baseline. We will then withhold knowledge
of the true domains from these algorithms and in-
stead provide them with random ?pseudo-domains,?
and then evaluate the change in their behavior. The
question is whether we can obtain similar benefits
by ignoring domain labels and relying strictly on an
ensemble learning motivation (instance bagging).
For the ?True Domain? setting, we apply the
MDL algorithms as normal. For the ?Random Do-
main? setting, we randomly shuffle the domain la-
bels within a given class label within each fold, thus
maintaining the same number of examples for each
domain label, and also retaining the same class dis-
tribution within each randomized domain. The re-
sulting ?pseudo-domains? are then similar to ran-
dom subsets of the data used in ensemble learning.
Following the standard practice in previous work,
for this experiment we use a balanced number of
examples from each domain and a balanced num-
ber of positive and negative labels (no class bias).
For AMAZON (4 domains), we have 10 folds of 400
examples per fold, for BILL (3 domains) 5 folds of
60 examples per fold, and for PARTY (2 domains) 5
folds of 80 examples per fold. In the ?Random Do-
main? setting, since we are randomizing the domain
labels, we increase the number of trials. We repeat
each cross-validation experiment 5 times with differ-
ent randomization of the domain labels each time.
Results Results are shown in Table 1. The first
row shows absolute (average) accuracy for a single
classifier trained on all data, ignoring domain dis-
tinctions. The remaining cells indicate absolute im-
provements against the baseline.
First, we note for the well-studied AMAZON
dataset that our results with true domains are con-
sistent with the previous literature. FEDA is known
to not improve upon a single classifier baseline for
that dataset (Dredze et al 2009). Both MDR-L2 and
MDR-KL improve upon the single classifier baseline,
again as per Dredze et al(2009). And finally, MTRL
also improves upon the single classifier baseline. Al-
though the MTRL improvement is not as dramatic as
in the original paper3, the average accuracy that we
achieve for MTRL (84.2%) is better than the best av-
erage accuracy in the original paper (83.65%).
The main comparison to make in Table 1 is be-
tween having knowledge of true domains or not.
?Random Domain? in the table is the case where do-
main identifiers are randomly shuffled within a given
fold. Ignoring the significance test results for now,
overall the results indicate that knowing the true do-
mains is useful for MDL algorithms. Randomiz-
ing the domains does not work better than knowing
true domains in any case. However, in all except
one case, the improvements of MDL algorithms are
3This might be due to a different version of the dataset being
used in a cross-validation setup, rather than their train/test setup,
and also because of differences in baseline approaches.
1306
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
83.93% 83.78% 66.67% 68.00% 62.75% 64.00%
FEDA
True Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25
Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10
MDR-L2
True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00
Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05
MDR-KL
True Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75
Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 H
MTRL
True Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25
Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80
Table 1: A comparison between MDL methods with access to the ?True Domain? labels and methods that
use ?Random Domain? information, essentially ensemble learning. The first row has raw accuracy numbers,
whereas the remaining entries are absolute improvements over the baseline. N: Significantly better than the
corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly worse than
corresponding baseline, with p < 0.05, using a paired t-test.
significantly better only for the AMAZON dataset4.
And interestingly, exactly in the same case, ran-
domly shuffling the domains also gives significant
improvements compared to the baseline, showing
that there is an ensemble learning effect in operation
for MDR-L2 and MDR-KL on the AMAZON dataset.
For FEDA, randomizing the domains significantly
hurts its performance on the AMAZON data, as is
the case for MDR-KL on the PARTY data. Therefore,
while our contrarian hypothesis about irrelevance of
domains is not completely true, it is indeed the case
that some MDL methods benefit from the ensemble
learning effect.
A second observation to be made from these re-
sults is that, while all of empirical research on MDL
assumes the definition of domains as a given, the
question of how to split a dataset into domains given
various metadata attributes is still open. For exam-
ple, in our experiments, in general, using the po-
litical party as a domain distinction gives us more
improvements over the corresponding baseline ap-
proach5.
We provide a detailed comparison of using true
4Some numbers in Table 1 might appear to be significant,
but are not. That is because of high variance in the performance
of the methods across the different folds.
5The BILL and the PARTY datasets are not directly compa-
rable to each other, although the prediction task is the same.
vs. randomized domains in Table 6, after presenting
the second set of experimental results.
4.2 Domain-specific Class Bias
Question: Are MDL methods improving because
they capture domain-specific class biases?
In previous work, and the above section, experi-
ments have assumed a balanced dataset in terms of
class labels. It has been in these settings that MDL
methods improve. However, this is an unrealistic as-
sumption. Even in our datasets, the original versions
demonstrated class bias: Amazon product reviews
are generally positive, votes on bills are rarely tied,
and political parties vote in blocs. While it is com-
mon to evaluate learning methods on balanced data,
and then adjust for imbalanced real world datasets, it
is unclear what effect domain-specific class bias will
have on MDL methods. Domains can differ in their
proportion of examples of different classes. For ex-
ample, it is quite likely that less controversial bills in
the United States Congress will have more yes votes
than controversial bills. Similarly, if instead of the
category of a product, its brand is considered as a do-
main, it is likely that some brands receive a higher
proportion of positive reviews than others.
Improvements from MDL in such settings may
simply be capturing domain-specific class biases.
1307
domain class cb1 cb2 cb3 cb4
AMAZON
b
- 20 80 60 40
+ 80 20 40 60
d
- 40 20 80 60
+ 60 80 20 40
e
- 60 40 20 80
+ 40 60 80 20
k
- 80 60 40 20
+ 20 40 60 80
BILL
031
N 16 4 8 12
Y 4 16 12 8
088
N 12 16 4 8
Y 8 4 16 12
132
N 8 12 16 4
Y 12 8 4 16
PARTY
D
N 10 30 15 25
Y 30 10 25 15
R
N 30 10 25 15
Y 10 30 15 25
Table 2: The table shows the distribution of in-
stances across domains and class labels within one
fold of each of the datasets, for four different class
bias trials. These datasets with varying class bias
across domains were used for the experiments de-
scribed in ?4.2
Consider two domains, where each domain is biased
towards the opposite label. In this case, domain-
specific parameters may simply be capturing the bias
towards the class label, increasing the weight uni-
formly of features predictive of the dominant class.
Similarly, methods that learn domain similarity may
be learning class bias similarity.
Why does the effectiveness of these domain-
specific bias parameters matter? First, if capturing
domain-specific class bias is the source of improve-
ment, there are much simpler methods for learning
that can be just as effective. This would be espe-
cially important in settings where we have many do-
mains, and learning domain-specific parameters for
each feature becomes infeasible. Second, if class
bias accounted for most of the improvement in learn-
ing, it suggests that such settings could be amenable
to unsupervised adaptation of the bias parameters.
Hypothesis: MDL largely capitalizes on
domain-specific class bias.
Empirical Evaluation To evaluate our hypothe-
sis, for each of our three datasets we create 4 random
versions, each with some domain-specific class-bias.
A summary of the dataset partitions is shown in
Table 2. For example, for the AMAZON dataset,
we create 4 versions (cb1 . . . cb4), where each do-
main has 100 examples per fold and each domain
has a different balance between positive and nega-
tive classes. For each of these settings, we conduct
a 10-fold cross validation experiment, then average
the CV results for each of the 4 settings. The re-
sulting accuracy numbers therefore reflect an aver-
age across many types of bias, each evaluated many
times. We do a similar experiment for the BILL and
PARTY datasets, except we use 5-fold CV.
In addition to the multi-domain and baseline
methods, we add a new baseline: DOM-ID. In this
setting, we augment the baseline classifier (which
ignores domain labels) with a new feature that in-
dicates the domain label. While we already include
a general bias feature, as is common in classifica-
tion tasks, these new features will capture domain-
specific bias. This is the only change to the base-
line classifier, so improvements over the baseline are
indicative of the change in domain-bias that can be
captured using these simple features.
Results Results are shown in Table 3. The table
follows the same structure as Table 1, with the ad-
dition of the results for the DOM-ID approach. We
first examine the efficacy of MDL in this setting. An
observation that is hard to miss is that MDL results
in these experiments show significant improvements
in almost all cases, as compared to only a few cases
in Table 1, despite the fact that even the baseline ap-
proaches have a higher accuracy. This shows that
MDL results can be highly influenced by systematic
differences in class bias across domains. Note that
there is also a significant negative influence of class
bias on MTRL for the AMAZON data.
A comparison of the MDL results on true domains
to the DOM-ID baseline gives us an idea of how
much MDL benefits purely from class bias differ-
ences across domains. We see that in most cases,
about half of the improvement seen in MDL is ac-
counted for by a simple baseline of using the do-
main identifier as a feature, and all but one of the
improvements from DOM-ID are significant. This
1308
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
85.52% 85.46% 70.50% 70.67% 65.44% 65.81%
FEDA
True Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 N
Random Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73
MDR-L2
True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94
Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28
MDR-KL
True Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 N
Random Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44
MTRL
True Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 N
Random Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 N
DOM-ID
True Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 N
Random Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 N
Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we
evaluate the ensemble learning effect, we have a setting of using randomized domains. N: Significantly
better than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly
worse than corresponding baseline, with p < 0.05, using a paired t-test.
suggests that in a real-world scenario where differ-
ence in class bias across domains is quite likely, it is
useful to consider DOM-ID as a simple baseline that
gives good empirical performance. To our knowl-
edge, using this approach as a baseline is not stan-
dard practice in MDL literature.
Finally, we also include the ?Random Domain?
evaluation in the our class biased version of exper-
iments. Each ?Random Domain? result in Table 3
is an average over 20 cross-validation runs (5 ran-
domized trials for each of the four class biased tri-
als cb1 . . . cb4). This setup combines the effects
of ensemble learning and bias difference across do-
mains. As seen in the table, for MDL algorithms the
results are consistently better as compared to know-
ing the true domains for the AMAZON dataset. For
the other datasets, the performance after randomiz-
ing the domains is still significantly better than the
baseline. This evaluation on randomized domains
further strengthens the conclusion that differences in
bias across domains play an important role, even in
the case of noisy domains. Looking at the perfor-
mance of DOM-ID with randomized domains, we
see that in all cases the DOM-ID baseline performs
better with randomized domains. While the dif-
ference is significant mostly only on the AMAZON
domain class cb5 cb6 cb7 cb8
AMAZON
b
- 20 40 60 80
+ 80 60 40 20
d
- 20 40 60 80
+ 80 60 40 20
e
- 20 40 60 80
+ 80 60 40 20
k
- 20 40 60 80
+ 20 40 60 80
Table 4: The table shows the distribution of in-
stances across domains and class labels within one
fold of the AMAZON dataset, for four different class
bias trials. For the BILL and PARTY datasets, similar
folds with consistent bias were created (number of
examples used was different). These datasets with
consistent class bias across domains were used for
the experiments described in ?4.2.1
dataset (details in Table 6, columns under ?Varying
Class Bias,?) this trend is still counter-intuitive. We
suspect this might be because randomization creates
a noisy version of the domain labels, which helps
learners to avoid over-fitting that single feature.
1309
4.2.1 Consistent Class Bias
We also performed a set of experiments that ap-
ply MDL algorithms to a setting where the datasets
have different class biases (unlike the experiments
reported in Table 1, where the classes are balanced),
but, unlike the experiments reported in Table 3, the
class bias is the same within each of the domains.
We refer to this as the case of consistent class bias
across domains. The distribution of classes within
each domain within each fold is shown in Table 4.
The results for this set of experiments are reported
in Table 5. The structure of Table 5 is identical to
that of Table 1. Comparing these results to those
in Table 1, we can see that in most cases the im-
provements seen using MDL algorithms are lower
than those seen in Table 1. This is likely due to
the higher baseline performance in the consistent
class bias case. A notable difference is in the per-
formance of MTRL ? it is significantly worse for
the AMAZON dataset, and significantly better for the
PARTY dataset. For the AMAZON dataset, we be-
lieve that the domain distinctions are less meaning-
ful, and hence forcing MTRL to learn the relation-
ships results in lower performance. For the PARTY
dataset, in the case of a class-biased setup, know-
ing the party is highly predictive of the vote (in the
original CONVOTE dataset, Democrats mostly vote
?no? and Republicans mostly vote ?yes?), and this
is rightly exploited by MTRL.
4.2.2 True vs. Randomized Domains
In Table 6 we analyze the difference in perfor-
mance of MDL methods when using true vs. ran-
domized domain information. For the three sets of
results reported earlier, we evaluated whether using
true domains as compared to randomized domains
gives significantly better, significantly worse or
equal performance. Significance testing was done
using a paired t-test with ? = 0.05 as before. As the
table shows, for the first set of results where the class
labels were balanced (overall, as well as within each
domain), using true domains was significantly better
mostly only for the AMAZON dataset. FEDA-SVM
was the only approach that was consistently better
with true domains across all datasets. Note, how-
ever, that it was significantly better than the baseline
approach only for PARTY.
For the second set of results (Table 3) where the
class bias varied across the different domains, us-
ing true domains was either no different from using
randomized domains, or it was significantly worse.
In particular, it was consistently significantly worse
to use true domains on the AMAZON dataset. This
questions the utility of domains on the AMAZON
dataset in the context of MDL in a domain-specific
class bias scenario. Since randomizing the domains
works better for all of the MDL methods on AMA-
ZON, it suggests that an ensemble learning effect
is primarily responsible for the significant improve-
ments seen on the AMAZON data, when evaluated in
a domain-specific class bias setting.
Finally, for the case of consistent class bias across
domains, the trend is similar to the case of no class
bias ? using true domains is useful. This table
further supports the conclusion that domain-specific
class bias highly influences multi-domain learning.
5 Discussion and Open Questions
Our analysis of MDL algorithms revealed new
trends that suggest further avenues of exploration.
We suggest three open questions in response.
Question: When are MDL methods most effective?
Our empirical results suggest that MDL can be more
effective in settings with domain-specific class bi-
ases. However, we also saw differences in im-
provements for each method, and for different do-
mains. Differences emerge between the AMAZON
and CONVOTE datasets in terms of the ensemble
learning hypothesis. While there has been some the-
oretical analyses on the topic of MDL (Ben-David
et al 2007; Ben-David et al 2009; Mansour et
al., 2009; Daume? III et al 2010a), our results sug-
gest performing new analyses that relate ensemble
learning results with the MDL setting. These anal-
yses could provide insights into new algorithms that
can take advantage of the specific properties of each
multi-domain setting.
Question: What makes a good domain for MDL?
To the best of our knowledge, previous work has
assumed that domain identities are provided to the
learning algorithm. However, in reality, there may
be many ways to split a dataset into domains. For
example, consider the CONVOTE dataset, which we
split both by BILL and PARTY. The choice of splits
1310
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
86.06% 86.22% 76.42% 75.58% 69.31% 68.38%
FEDA
True Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25
Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04
MDR-L2
True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19
Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 H
MDR-KL
True Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 N
Random Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34
MTRL
True Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 N
Random Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 N
Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.
Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized
domains. N: Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a paired
t-test. H: Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.
MDL Method No Class Bias (Tab. 1) Varying Class Bias (Tab. 3) Consistent Class Bias (Tab. 5)
better worse equal better worse equal better worse equal
FEDA-SVM AM, BI, PA AM BI, PA AM, PA BI
FEDA-LR AM BI, PA AM BI, PA AM, BI PA
MDR-L2 AM BI, PA AM BI, PA AM, BI PA
MDR-KL PA AM, BI AM BI, PA AM, PA BI
MTRL AM BI, PA AM BI, PA AM, PA BI
DOM-ID-SVM ? ? ? AM BI, PA ? ? ?
DOM-ID-LR ? ? ? AM, BI PA ? ? ?
Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL method
using true domain information was significantly better, significantly worse, or not significantly different
(equal) as compared to using randomized domain information with the same MDL method.
impacted MDL. This poses new questions: what
makes a good domain? How should we choose to di-
vide data along possible metadata properties? If we
can gain improvements simply by randomly creat-
ing new domains (?Random Domain? setting in our
experiments) then there may be better ways to take
advantage of the provided metadata for MDL.
Question: Can we learn class-bias for
unsupervised domain adaptation?
Experiments with domain-specific class biases re-
vealed that a significant part of the improvements
could be achieved by adding domain-specific bias
features. Limiting the multi-domain improvements
to a small set of parameters raises an interesting
question: can these parameters be adapted to a new
domain without labeled data? Traditionally, domain
adaptation without target domain labeled data has
focused on learning the behavior of new features;
beliefs about existing feature behaviors could not be
corrected without new training data. However, by
collapsing the adaptation into a single bias parame-
ter, we may be able to learn how to adjust this pa-
rameter in a fully unsupervised way. This would
open the door to improvements in this challenging
setting for real world problems where class bias was
a significant factor.
Acknowledgments
Research presented here is supported by the Office
of Naval Research grant number N000141110221.
1311
References
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Transfer
Learning in Named Entity Recognition. In Proceed-
ings of ACL-08: HLT, pages 245?253.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In Proceedings of NIPS 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440?447.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
Bounds for Domain Adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS 2007).
Giovanni Cavallanti, Nicolo` Cesa-Bianchi, and Claudio
Gentile. 2008. Linear Algorithms for Online Multi-
task Classification. In Proceedings of COLT.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
Maximum Entropy Capitalizer: Little Data Can Help
a Lot. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 285?292.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2008. Exact convex confidence-weighted learning. In
Advances in Neural Information Processing Systems
(NIPS).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010a. A Co-regularization Based Semi-supervised
Domain Adaptation. In Neural Information Process-
ing Systems.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010b. Frustratingly Easy Semi-Supervised Domain
Adaptation. In Proceedings of the ACL 2010 Work-
shop on Domain Adaptation for Natural Language
Processing, pages 53?59.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Hal Daume? III. 2009. Bayesian multitask learning with
latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence.
Thomas G. Dietterich. 2000. An experimental compar-
ison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139?157.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1-2).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical Bayesian Domain Adaptation. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602?610.
Richard Maclin and David Opitz. 1999. Popular Ensem-
ble Methods: An Empirical Study. Journal of Artifi-
cial Intelligence Research, 11:169?198.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain Adaptation with Multiple
Sources. In Proceedings of NIPS 2008, pages 1041?
1048.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Conference on Natural Language
Learning (Shared Task).
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
1312
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152?1158,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency Parsing for Weibo:
An Efficient Probabilistic Logic Programming Approach
William Yang Wang, Lingpeng Kong, Kathryn Mazaitis, William W. Cohen
Language Technologies Institute & Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA.
{yww,lingpenk,krivard,wcohen}@cs.cmu.edu
Abstract
Dependency parsing is a core task in NLP,
and it is widely used by many applica-
tions such as information extraction, ques-
tion answering, and machine translation.
In the era of social media, a big chal-
lenge is that parsers trained on traditional
newswire corpora typically suffer from the
domain mismatch issue, and thus perform
poorly on social media data. We present a
new GFL/FUDG-annotated Chinese tree-
bank with more than 18K tokens from Sina
Weibo (the Chinese equivalent of Twit-
ter). We formulate the dependency pars-
ing problem as many small and paralleliz-
able arc prediction tasks: for each task,
we use a programmable probabilistic first-
order logic to infer the dependency arc of a
token in the sentence. In experiments, we
show that the proposed model outperforms
an off-the-shelf Stanford Chinese parser,
as well as a strong MaltParser baseline that
is trained on the same in-domain data.
1 Introduction
Weibo, in particular Sina Weibo
1
, has attracted
more than 30% of Internet users (Yang et al.,
2012), making it one of the most popular social
media services in the world. While Weibo posts
are abundantly available, NLP techniques for ana-
lyzing Weibo posts have not been well-studied in
the past.
Syntactic analysis of Weibo is made difficult
for three reasons: first, in the last few decades,
Computational Linguistics researchers have pri-
marily focused on building resources and tools us-
ing standard English newswire corpora
2
, and thus,
1
http://en.wikipedia.org/wiki/Sina Weibo
2
For example, Wall Street Journal articles are used for
building the Penn Treebank (Marcus et al., 1993).
there are fewer resources in other languages in
general. Second, microblog posts are typically
short, noisy (Gimpel et al., 2011), and can be
considered as a ?dialect?, which is very differ-
ent from news data. Due to the differences in
genre, part-of-speech taggers and parsers trained
on newswire corpora typically fail on social media
texts. Third, most existing parsers use language-
independent standard features (McDonald et al.,
2005), and these features may not be optimal for
Chinese (Martins, 2012). To most of the applica-
tion developers, the parser is more like a blackbox,
which is not directly programmable. Therefore,
it is non-trivial to adapt these generic parsers to
language-specific social media text.
In this paper, we present a new probabilistic de-
pendency parsing approach for Weibo, with the
following contributions:
? We present a freely available Chinese Weibo
dependency treebank
3
, manually annotated
with more than 18,000 tokens;
? We introduce a novel probabilistic logic
programming approach for dependency arc
prediction, making the parser directly pro-
grammable for theory engineering;
? We show that the proposed approach outper-
forms an off-the-shelf dependency parser, as
well as a strong baseline trained on the same
in-domain data.
In the next section, we describe existing work
on dependency parsing for Chinese. In Section 3,
we present the new Chinese Weibo Treebank to
the research community. In Section 4, we intro-
duce the proposed efficient probabilistic program-
ming approach for parsing Weibo. We show the
experimental results in Section 5, and conclude in
Section 6.
3
http://www.cs.cmu.edu/?yww/data/WeiboTreebank.zip
1152
2 Related Work
Chinese dependency parsing has attracted many
interests in the last fifteen years. Bikel and Chi-
ang (2000; 2002) are among the first to use Penn
Chinese Tree Bank for dependency parsing, where
they adapted Xia?s head rules (Xia, 1999). An im-
portant milestone for Chinese dependency pars-
ing is that, a few years later, the CoNLL shared
task launched a track for multilingual dependency
parsing, which also included Chinese (Buchholz
and Marsi, 2006; Nilsson et al., 2007). These
shared tasks soon popularized Chinese depen-
dency parsing by making datasets available, and
there has been growing amount of literature since
then (Zhang and Clark, 2008; Nivre et al., 2007;
Sagae and Tsujii, 2007; Che et al., 2010; Carreras,
2007; Duan et al., 2007).
Besides the CoNLL shared tasks, there are also
many interesting studies on Chinese dependency
parsing. For example, researchers have studied
case (Yu et al., 2008) and morphological (Li and
Zhou, 2012) structures for learning a Chinese de-
pendency parser. Another direction is to perform
joint learning and inference for POS tagging and
dependency parsing (Li et al., 2011; Hatori et al.,
2011; Li et al., 2011; Ma et al., 2012). In recent
years, there has been growing interests in depen-
dency arc prediction in Chinese (Che et al., 2014),
and researchers have also investigated character-
level Chinese dependency parsing (Zhang et al.,
2014). However, even though the above methods
all have merits, the results are reported only on
standard newswire based Chinese Treebank (e.g.
from People?s Daily (Liu et al., 2006)), and it is
unclear how they would perform on Weibo data.
To the best of our knowledge, together with the
recent study on parsing tweets (Kong et al., 2014),
we are among the first to study the problem of de-
pendency parsing for social media text.
3 The Chinese Weibo Treebank
We use the publicly available ?topia dataset (Ling
et al., 2013) for dependency annotation. An in-
teresting aspect of this Weibo dataset is that, be-
sides the Chinese posts, it also includes a copy of
the English translations. This allows us to observe
some interesting phenomena that mark the differ-
ences of the two languages. For example:
? Function words are more frequently used in
English than in Chinese. When examin-
Figure 1: An example of pro-drop phenomenon
from the Weibo data.
ing this English version of the Weibo cor-
pus for the total counts of the word ?the?,
there are 2,084 occurrences in 2,003 sen-
tences. Whereas in Chinese, there are only
52 occurrences of the word ?the? out of the
2,003 sentences.
? The other interesting thing is the position of
the head. In English, the head of the tree
occurs more frequent on the left-to-middle
of the sentence, while the distribution of the
head is more complicated in Chinese. This is
also verified from the parallel Weibo data.
? Another well-known issue in Chinese is that
Chinese is a pro-drop topical language. This
is extremely prominent in the short text,
which clearly creates a problem for parsing.
For example, in the Chinese Weibo data, we
have observed the sentence in Figure 1.
To facilitate the annotation process, we first
preprocess the Weibo posts using the Stanford
NLP pipeline, including a Chinese Word Seg-
menter (Tseng et al., 2005) and a Chinese Part-
of-Speech tagger (Toutanova and Manning, 2000).
Two native speakers of Chinese with strong lin-
guistic backgrounds have annotated the depen-
dency relations from 1,000 posts of the ?topia
dataset, using the FUDG (Schneider et al., 2013)
and GFL annotation tool (Mordowanec et al.,
2014). The annotators communicate regularly dur-
ing the annotation process, and a coding man-
ual that relies majorly on the Stanford Dependen-
cies (Chang et al., 2009) is designed. The anno-
tation process has two stages: in the first stage,
we rely on the word segmentation produced by
the segmenter, and produce a draft version of the
treebank; in the second stage, the annotators ac-
tively discuss the difficult cases to reach agree-
ments, manually correct the mis-segmented word
tokens, and revise the annotations of the tricky
cases. The final inter-annotator agreement rate on
a randomly-selected subset of 373 tokens in this
1153
treebank is 82.31%.
Fragmentary Unlabeled Dependency Grammar
(FUDG) is a newly proposed flexible framework
that offers a relative easy way to annotate the syn-
tactic structure of text. Beyond the traditional tree
view of dependency syntax in which the tokens
of a sentence form nodes in a tree, FUDG also
allows the annotation of additional lexical items
such as multiword expressions. It provides special
devices for coordination and coreference; and fa-
cilitates underspecified (partial) annotations where
producing a complete parse would be difficult.
Graph Fragment Language (GFL) is an implemen-
tation of unlabeled dependency annotations in the
FUDG framework, which fully supports Chinese,
English and other languages. The training set of
our Chinese Weibo Treebank
4
includes 14,774 to-
kens, while the development and test sets include
1,846 and 1,857 tokens respectively.
4 A Programmable Parser with
Personalized PageRank Inference
A key problem in multilingual dependency parsing
is that generic feature templates may not work well
for every language. For example, Martins (2012)
shows that for Chinese dependency parsing, when
adding the generic grandparents and siblings fea-
tures, the performance was worse than using the
standard bilexical, unilexical, and part-of-speech
features. Unfortunately, for many parsers such
as Stanford Chinese Parser (Levy and Manning,
2003) and MaltParser (Nivre et al., 2007), it is
very difficult for programmers to specify the fea-
ture templates and inference rules for dependency
arc prediction.
In this work, we present a Chinese dependency
parsing method for Weibo, based on efficient prob-
abilistic first-order logic programming (Wang et
al., 2013). The advantage of probabilistic pro-
gramming for parsing is that, software engineers
can simply conduct theory engineering, and op-
timize the performance of the parser for a spe-
cific genre of the target language. Recently, proba-
bilistic programming approaches (Goodman et al.,
2012; Wang et al., 2013; Lloyd et al., 2014) have
demonstrated its efficiency and effectiveness in
many areas such as information extraction (Wang
et al., 2014), entity linking, and text classifica-
tion (Wang et al., 2013).
4
The corpus is freely available for download at the URL
specified in Section 1.
Algorithm 1 A Dependency Arc Inference Algo-
rithm for Parsing Weibo
Given:
(1) a sentence with tokens T
i
, where i is the in-
dex, and L is the length;
(2) a databaseD of token relations from the cor-
pus;
(3) first-order logic inference rule set R.
for i = 1? L tokens do
S? ConstructSearchSpace(T
i
, R,D);
~
P
i
? InferParentUsingProPPR(T
i
,S);
end for
Greedy Global Inference
for i = 1? L tokens do
Y
i
= arg max
~
P
i
;
end for
4.1 Problem Formulation
We formulate the dependency parsing prob-
lem as many small dependency arc prediction
problems. For each token, we form the par-
ent inference problem of a token T
i
as solving a
query edge(T
i
, ?) using stochastic theorem prov-
ing on a search graph. Our approach relies on a
database D of inter-token relations. To construct
the database, we automatically extract the token
relations from the text data. For example, to de-
note the adjacency of two tokens T
1
and T
2
, we
store the entry adjacent(T
1
, T
2
) in D. One can
also store the part-of-speech tag of a token in the
form haspos(T
1
, DT ). There is no limitations
on the arity and the types of the predicates in the
database.
Given the database of token relations, one then
needs to construct the first-order logic inference
theory R for predicting dependency arcs. For ex-
ample, to construct simple bilexical and bi-POS
inference rules to model the dependency of an ad-
jacent head and a modifier, one can write first-
order clauses such as:
edge(V1,V2) :-
adjacent(V1,V2),hasword(V1,W1),
hasword(V2,W2),keyword(W1,W2) #adjWord.
edge(V1,V2) :-
adjacent(V1,V2),haspos(V1,W1),
haspos(V2,W2),keypos(W1,W2) #adjPos.
keyword(W1,W2) :- # kw(W1,W2).
keypos(W1,W2) :- # kp(W1,W2).
1154
Figure 2: After mapping the database D to theory R, here is an example of search space for dependency
arc inference. The query is edge(S
1
T
5
, X), and there exists one correct and multiple incorrect solutions
(highlighted in bold).
Here, we associate a feature vector ?
c
with each
clause, which is annotated using the # symbol af-
ter each clause in the theory set. Note that the last
two (keyword and keypos) clauses are feature tem-
plates that allow us to learn the specific bi-POS
tags and bilexical words from the data. In order
for one to solve the query edge(T
i
, ?), we first
need to map the entities from D to R to construct
the search space. The details for constructing and
searching in the graph can be found in previous
studies on probabilistic first-order logic (Wang et
al., 2013) and stochastic logic programs (Cussens,
2001). An example search space is illustrated in
Figure 2. Note that now the edges in the search
graph correspond to the feature vector ?
c
in R.
The overall dependency arc inference algorithm
can be found in Algorithm 1. For each of the par-
ent inference subtask, we use ProPPR (Wang et al.,
2013) to perform efficient personalized PageRank
inference. Note that to ensure the validity of the
dependency tree, we break the loops in the final
parse graph into a parse tree using the maximum
personalized PageRank score criteria. When mul-
tiple roots are predicted, we also select the most
likely root by comparing the personalized PageR-
ank solution scores.
To learn the more plausible theories, one needs
to upweight weights for relevant features, so
that they have higher transition probabilities on
the corresponding edges. To do this, we use
stochastic gradient descent to learn from training
queries, where the correct and incorrect solutions
are known. The details of the learning algorithm
are described in the last part of this section.
4.2 Personalized PageRank Inference
For the inference of the parent of each token, we
utilize ProPPR (Wang et al., 2013). ProPPR al-
lows a fast approximate proof procedure, in which
only a small subset of the full proof graph is
generated. In particular, if ? upper-bounds the
reset probability, and d upperbounds the degree
of nodes in the graph, then one can efficiently
find a subgraph with O(
1
?
) nodes which approx-
imates the weight for every node within an er-
ror of d (Wang et al., 2013), using a variant of
the PageRank-Nibble algorithm of Andersen et al
(2008).
4.3 Parameter Estimation
Our parameter learning algorithm is implemented
using a parallel stochastic gradient descent vari-
ant to optimize the log loss using the supervised
personalized PageRank algorithm (Backstrom and
1155
Method Dev. Test
Stanford Parser (Xinhua) 0.507 0.489
Stanford Parser (Chinese) 0.597 0.581
MaltParser (Full) 0.669 0.654
Our methods ? ProPPR
ReLU (Bi-POS) 0.506 0.517
ReLU (Bilexical) 0.635 0.616
ReLU (Full) 0.668 0.666
Truncated tanh (Bi-POS) 0.601 0.594
Truncated tanh (Bilexical) 0.650 0.634
Truncated tanh (Full) 0.667 0.675*
Table 1: Comparing our Weibo parser to other
baselines (UAS). The off-the-shelf Stanford parser
uses its attached Xinhua and Chinese factored
models, which are trained on external Chinese
treebank of newswire data. MaltParser was trained
on the same in-domain data as our proposed ap-
proach. * indicates p < .001 comparing to the
MaltParser.
Leskovec, 2011). The idea is that, given the
training queries, we perform a random walk with
restart process, and upweight the edges that are
more likely to end up with a known correct parent.
We learn the transition probability from two nodes
(u, v) in the search graph using: Pr
w
(v|u) =
1
Z
f(w,?
c
restart
), where we use two popular non-
linear parameter learning functions from the deep
learning community:
? Rectified Linear Unit (ReLU) (Nair and Hin-
ton, 2010): max(0, x);
? The Hyperbolic Function (Glorot and Ben-
gio, 2010): tanh(x).
as the f in this study. ReLU is a desirable
non-linear function, because it does not have the
vanishing gradient problem, and produces sparse
weights. For the weights learned from tanh(x),
we truncate the negative weights on the edges,
since the default weight on the feature edges is
w = 1.0 (existence), and w = 0.0 means that the
edge does not exist in the inference stage.
5 Experiments
In this experiment, we compare the proposed
parser with two well-known baselines. First,
we compare with an off-the-shelf Stanford Chi-
nese Parser (Levy and Manning, 2003). Second,
we compare with the MaltParser (Nivre et al.,
2007) that is trained on the same in-domain Weibo
dataset. The train, development, and test splits are
described in Section 3. We tune the regulariza-
tion hyperparameters of the models on the dev. set,
and report Unlabeled Attachment Score (UAS) re-
sults for both the dev. set and the hold-out test set.
We experiment with the bilexical and bi-POS first-
order logic theory separately, as well as a com-
bined full model with directional and distance fea-
tures.
The results are shown in Table 1. We see that
both of the two attached pre-trained models from
the Stanford parser do not perform very well on
this Weibo dataset, probably because of the mis-
matched training and test data. MaltParser is
widely considered as one of the most popular de-
pendency parsers, not only because of its speed,
but also the acclaimed accuracy. We see that when
using the full model, the UAS results between our
methods and MaltParser are very similar on the de-
velopment set, but both of our approaches outper-
form the Maltparser in the holdout test set. The
truncated tanh variant of ProPPR obtains the best
UAS score of 0.675.
6 Conclusion
In this paper, we present a novel Chinese de-
pendency treebank, annotated using Weibo data.
We introduce a probabilistic programming depen-
dency arc prediction approach, where theory en-
gineering is made easy. In experiments, we show
that our methods outperform an off-the-shelf Stan-
ford Chinese Parser, as well a strong MaltParser
that is trained on the same in-domain data. The
Chinese Weibo Treebank is made freely available
to the research community. In the future, we plan
to apply the proposed approaches to dependency
and semantic parsing of other languages.
Acknowledgements
We are grateful to anonymous reviewers for useful
comments. This research was supported in part
by DARPA grant FA8750-12-2-0342 funded un-
der the DEFT program, and a Google Research
Award. The authors are solely responsible for the
contents of the paper, and the opinions expressed
in this publication do not reflect those of the fund-
ing agencies.
1156
References
Reid Andersen, Fan R. K. Chung, and Kevin J. Lang.
2008. Local partitioning for directed graphs using
pagerank. Internet Mathematics, 5(1):3?22.
Lars Backstrom and Jure Leskovec. 2011. Supervised
random walks: predicting and recommending links
in social networks. In Proceedings of the fourth
ACM international conference on Web search and
data mining, pages 635?644. ACM.
Daniel M Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 12, pages 1?6. Associa-
tion for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL, pages 957?961.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13?16. Association for Computational Linguistics.
Wanxiang Che, Jiang Guo, and Ting Liu. 2014. Re-
liable dependency arc recognition. Expert Systems
with Applications, 41(4):1716?1722.
David Chiang and Daniel M. Bikel. 2002. Recover-
ing latent information in treebanks. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics - Volume 1, COLING ?02, pages
1?7, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
James Cussens. 2001. Parameter estimation in
stochastic logic programs. Machine Learning,
44(3):245?271.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In EMNLP-CoNLL, pages 940?
946.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42?47. Association for Computa-
tional Linguistics.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International Conference on Artificial
Intelligence and Statistics, pages 249?256.
Noah Goodman, Vikash Mansinghka, Daniel Roy,
Keith Bonawitz, and Daniel Tarlow. 2012. Church:
a language for generative models. arXiv preprint
arXiv:1206.3255.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In IJC-
NLP, pages 1216?1224.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), Doha, Qatar, October. ACL.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439?446. Association for Computational Lin-
guistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454. Association for
Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for chinese pos tagging and dependency pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1180?1191. Association for Computational Linguis-
tics.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
?13. Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Build-
ing a dependency treebank for improving chinese
parser. Journal of Chinese Language and Comput-
ing, 16(4):207?224.
1157
James Robert Lloyd, David Duvenaud, Roger Grosse,
Joshua B Tenenbaum, and Zoubin Ghahramani.
2014. Automatic construction and natural-language
description of nonparametric regression models.
arXiv preprint arXiv:1402.4304.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012.
Easy-first chinese pos tagging and dependency pars-
ing. In COLING, pages 1731?1746.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Andr?e Filipe Torres Martins. 2012. The Geometry of
Constrained Structured Prediction: Applications to
Inference and Learning of Natural Language Syn-
tax. Ph.D. thesis, Columbia University.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Michael T. Mordowanec, Nathan Schneider, Chris
Dyer, and Noah A. Smith. 2014. Simplified de-
pendency annotations with gfl-web. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations.
ACL.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807?814.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The conll 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL, pages 915?932. sn.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kenji Sagae and Jun?ichi Tsujii. 2007. Depen-
dency parsing and domain adaptation with lr models
and parser ensembles. In EMNLP-CoNLL, volume
2007, pages 1044?1050.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63?70. Association for
Computational Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2013. Programming with personalized
pagerank: a locally groundable first-order proba-
bilistic logic. In Proceedings of the 22nd ACM in-
ternational conference on Conference on informa-
tion & knowledge management, pages 2129?2138.
ACM.
William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom
Mitchell, and William W Cohen. 2014. Effi-
cient inference and learning in a large knowledge
base: Reasoning with extracted information using
a locally groundable first-order probabilistic logic.
arXiv preprint arXiv:1404.3301.
Fei Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proceedings of the 5th
Natural Language Processing Pacific Rim Sympo-
sium (NLPRS-99), pages 398?403.
Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.
Automatic detection of rumor on sina weibo. In Pro-
ceedings of the ACM SIGKDD Workshop on Mining
Data Semantics, page 13. ACM.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large scale
automatically constructed case structures. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 1049?
1056. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571. Association for Computa-
tional Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the 52th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2014), Baltimore, MD, USA, June. ACL.
1158
Proceedings of NAACL-HLT 2013, pages 685?690,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
What?s in a Domain? Multi-Domain Learning for Multi-Attribute Data
Mahesh Joshi? Mark Dredze? William W. Cohen? Carolyn P. Rose??
? School of Computer Science, Carnegie Mellon University
Pittsburgh, PA, 15213, USA
? Human Language Technology Center of Excellence, Johns Hopkins University
Baltimore, MD, 21211, USA
maheshj@cs.cmu.edu,mdredze@cs.jhu.edu
wcohen@cs.cmu.edu,cprose@cs.cmu.edu
Abstract
Multi-Domain learning assumes that a sin-
gle metadata attribute is used in order to di-
vide the data into so-called domains. How-
ever, real-world datasets often have multi-
ple metadata attributes that can divide the
data into domains. It is not always apparent
which single attribute will lead to the best do-
mains, and more than one attribute might im-
pact classification. We propose extensions to
two multi-domain learning techniques for our
multi-attribute setting, enabling them to si-
multaneously learn from several metadata at-
tributes. Experimentally, they outperform the
multi-domain learning baseline, even when it
selects the single ?best? attribute.
1 Introduction
Multi-Domain Learning (Evgeniou and Pontil,
2004; Daume? III, 2007; Dredze and Crammer, 2008;
Finkel and Manning, 2009; Zhang and Yeung, 2010;
Saha et al, 2011) algorithms learn when training in-
stances are spread across many domains, which im-
pact model parameters. These algorithms use exam-
ples from each domain to learn a general model that
is also sensitive to individual domain differences.
However, many data sets include a host of meta-
data attributes, many of which can potentially define
the domains to use. Consider the case of restaurant
reviews, which can be categorized into domains cor-
responding to the cuisine, location, price range, or
several other factors. For multi-domain learning, we
should use the metadata attribute most likely to char-
acterize a domain: a change in vocabulary (i.e. fea-
tures) that most impacts the classification decision
(Ben-David et al, 2009). This choice is not easy.
First, we may not know which metadata attribute is
most likely to fit this role. Perhaps the location most
impacts the review language, but it could easily be
the price of the meal. Second, multiple metadata
attributes could impact the classification decision,
and picking a single one might reduce classification
accuracy. Therefore, we seek multi-domain learn-
ing algorithms which can simultaneously learn from
many types of domains (metadata attributes).
We introduce the multi-attribute multi-domain
(MAMD) learning problem, in which each learning
instance is associated with multiple metadata at-
tributes, each of which may impact feature behavior.
We present extensions to two popular multi-domain
learning algorithms, FEDA (Daume? III, 2007) and
MDR (Dredze et al, 2009). Rather than selecting
a single domain division, our algorithms consider
all attributes as possible distinctions and discover
changes in features across attributes. We evaluate
our algorithms using two different data sets ? a data
set of restaurant reviews (Chahuneau et al, 2012),
and a dataset of transcribed speech segments from
floor debates in the United States Congress (Thomas
et al, 2006). We demonstrate that multi-attribute al-
gorithms improve over their multi-domain counter-
parts, which can learn distinctions from only a single
attribute.
2 MAMD Learning
In multi-domain learning, each instance x is drawn
from a domain d with distribution x ? Dd over a
vectors space RD and labeled with a domain spe-
cific function fd with label y ? {?1,+1} (for bi-
nary classification). In multi-attribute multi-domain
685
(MAMD) learning, we have M metadata attributes in
a data set, where the mth metadata attribute has Km
possible unique values which represent the domains
induced by that metadata attribute. Each instance xi
is drawn from a distribution xi ? Da specific to a
set of attribute values Ai associated with each in-
stance. Additionally, each unique set of attributes
indexes a function fA.1 Ai could contain a value for
each attribute, or no values for any attribute (which
would index a domain-agnostic ?background? distri-
bution and labeling function). Just as a domain can
change a feature?s probability and behavior, so can
each metadata attribute.
Examples of data for MAMD learning abound. The
commonly used Amazon product reviews data set
(Blitzer et al, 2007) only includes product types, but
the original reviews can be attributed with author,
product price, brand, and so on. Additional exam-
ples include congressional floor debate records (e.g.
political party, speaker, bill) (Joshi et al, 2012). In
this paper, we use restaurant reviews (Chahuneau et
al., 2012), which have upto 20 metadata attributes
that define domains, and congressional floor de-
bates, with two attributes that define domains.
It is difficult to apply multi-domain learning algo-
rithms when it is unclear which metadata attribute
to choose for defining the ?domains?. It is possible
that there is a single ?best? attribute to use for defin-
ing domains, one that when used in multi-domain
learning will yield the best classifier. To find this
attribute, one must rely on one?s intuition about the
problem,2 or perform an exhaustive empirical search
over all attributes using some validation set. Both
these strategies can be brittle, because as the nature
of data changes over time so may the ?best? do-
main distinction. Additionally, multi-domain learn-
ing was not designed to benefit from multiple helpful
attributes.
We note here that Eisenstein et al (2011), as well
as Wang et al (2012), worked with a ?multifaceted
topic model? using the framework of sparse addi-
tive generative models (SAGE). Both those models
capture interactions between topics and multiple as-
1Distributions and functions that share attributes could share
parameters.
2Intuition is often critical for learning and in some cases can
help, such as in the Amazon product reviews data set, where
product type clearly corresponds to domain. However, for other
data sets the choice may be less clear.
pects, and can be adapted to the case of MAMD. While
our problem formulation has significant conceptual
overlap with the SAGE?like multifaceted topic mod-
els framework, our proposed methods are motivated
from a fast online learning perspective.
A naive approach for MAMD would be to treat ev-
ery unique set of attributes as a domain, including
unique proper subsets of different attributes to ac-
count for the case of missing attributes in some in-
stances.3 However, introducing an exponential num-
ber of domains requires a similar increase in train-
ing data, clearly an infeasible requirement. Instead,
we develop multi-attribute extensions for two multi-
domain learning algorithms, such that the increase
in parameters is linear in the number of metadata at-
tributes, and no special handling is required for the
case where some metadata attributes might be miss-
ing from an instance.
Multi-Attribute FEDA The key idea behind
FEDA (Daume? III, 2007) is to encode each domain
using its own parameters, one per feature. FEDA
maps a feature vector x in RD to RD(K+1). This
provides a separate parameter sub-space for every
domain k ? 1 . . .K, and also maintains a domain-
agnostic shared sub-space. Essentially, each feature
is duplicated for every instance in the appropriate
sub-space of RD(K+1) that corresponds to the in-
stance?s domain. We extend this idea to the MAMD
setting by using one parameter per attribute value.
The original instance x ? RD is now mapped into
RD(1+
?
mKm); a separate parameter for each at-
tribute value and a shared set of parameters. In ef-
fect, for every metadata attribute a ? Ai, the original
features are copied into the appropriate sub-space.
This grows linearly with the number of metadata at-
tribute values, as opposed to exponentially in our
naive solution. While this is still substantial growth,
each instance retains the same feature sparsity as in
the original input space. In this new setup, FEDA al-
lows an instance to contribute towards learning the
shared parameters, and the attribute-specific param-
eters for all the attributes present on an instance. Just
like multi-domain FEDA, any supervised learning al-
gorithm can be applied to the transformed represen-
tation.
3While we used a similar setup for formulating our problem,
we did not rule out the potential for factoring the distributions.
686
Multi-Attribute MDR We make a similar change
to MDR (Dredze et al, 2009) to extend it for
the MAMD setting. In the original formulation,
Dredze et al used confidence-weighted (CW)
learning (Dredze et al, 2008) for learning shared
and domain-specific classifiers, which are combined
based on the confidence scores associated with the
feature weights. For training the MDR approaches in
a multi-domain learning setup, they found that com-
puting updates for the combined classifier and then
equally distributing them to the shared and domain-
specific classifiers was the best strategy, although it
approximated the true objective that they aimed to
optimize. In our multi-attribute setup confidence-
weighted (CW) classifiers are learned for each of the
?
mKm attribute values in addition to a shared CW
classifier. At classification time, a combined clas-
sifier is computed for every instance. However, in-
stead of combining the shared classifier and a single
domain-specific classifier, we combine the shared
CW classifier and |Ai| different attribute value-
specific CW classifiers associated with xi. The
combined classifier is found by minimizing the KL-
divergence of the combined classifier with respect to
each of the underlying classifiers.4
When learning the shared and domain-specific
classifiers, we follow the best result in Dredze et
al. and use the ?averaged update? strategy (?7.3 in
Dredze et al), where updates are computed for the
combined classifier, and are then distributed to the
shared and domain-specific classifiers. MDR-U will
indicate that the updates to the combined classifiers
are uniformly distributed to the underlying shared
and domain-specific classifiers.
Dredze et al also used another scheme called
?variance? to distribute the combined update to the
underlying classifiers (?4, last paragraph in Dredze
et al) Their idea was to give a lower portion
of the update to the underlying classifier that has
higher variance (or in their terminology, ?less con-
fidence?) since it contributed less to the combined
classifier. We refer to this as MDR-V. However, this
conflicts with the original CW intuition that features
with higher variance (lower confidence) should re-
ceive higher updates; since they are more in need
of change. Therefore, we implemented a modi-
fied ?variance? scheme, where the updates are dis-
4We also tried the l2 distance method of Dredze et al (2009)
but it gave consistently worse results.
tributed to the underlying classifiers such that higher
variance features receive the larger updates. We re-
fer to this as MDR-NV. We observed significant im-
provements with this modified scheme.
3 Experiments
To evaluate our multi-attribute algorithms we con-
sider two datasets. First, we use two subsets of the
restaurant reviews dataset (1,180,308 reviews) intro-
duced by Chahuneau et al (2012) with the goal of
labeling reviews as positive or negative. The first
subset (50K-RND) randomly selects 50,000 reviews
while the second (50K-BAL) is a class-balanced
sample. Following the approach of Blitzer et al
(2007), scores above and below 3-stars indicated
positive and negative reviews, while 3-star reviews
were discarded. Second, we use the transcribed seg-
ments of speech from the United States Congress
floor debates (Convote), introduced by Thomas
et al (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discussion
in the floor debate.
In the WordSalad datasets, each restaurant re-
view can have many metadata attributes, including a
unique identifier, name (which may not be unique),
address (we extract the zipcode), and type (Italian,
Chinese, etc.). We select the 20 most common meta-
data attributes (excluding latitude, longitude, and the
average rating). 5 In the Convote dataset, each
speech segment is associated with the political party
affiliation of the speaker (democrat, independent, or
republican) and the speaker identifier (we use bill
identifiers for creating folds in our 10-fold cross-
validation setup).
In addition to our new algorithms, we evalu-
ate several baselines. All methods use confidence-
weighted (CW) learning (Crammer et al, 2012).
BASE A single classifier trained on all the data,
and which ignores metadata attributes and uses uni-
gram features. For CW, we use the best-performing
setting from Dredze et al (2008) ? the ?variance?
algorithm, which computes approximate but closed?
form updates, which also lead to faster learning. Pa-
rameters are tuned over a validation set within each
training fold.
5Our method requires categorical metadata attributes, al-
though real-valued attributes can be discretized.
687
metadata 1-META FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D NONE (BASE) 92.29 (?0.14)
ALL (META) ? 92.69 (?0.10)
CATEGORY ? 92.48 (?0.11) 92.47 (?0.10) ?? 92.99 (?0.12) 91.16 (?0.16) ?? 93.24 (?0.13)
ZIPCODE 92.40 (?0.09) ? 92.73 (?0.09) ?? 92.99 (?0.12) 91.19 (?0.20) ?? 93.22 (?0.11)
NEIGHBORHOOD 92.42 (?0.11) ? 92.65 (?0.13) ?? 93.02 (?0.13) 91.17 (?0.21) ?? 93.21 (?0.12)
5
0
K
-
B
A
L NONE (BASE) 89.95 (?0.10)
ALL (META) ? 90.39 (?0.09)
CATEGORY 90.09 (?0.11) ? 90.50 (?0.11) ? 90.60 (?0.11) 87.89 (?0.13) ?? 91.33 (?0.08)
ZIPCODE 89.97 (?0.12) ? 90.42 (?0.13) ? 90.56 (?0.09) 87.78 (?0.16) ?? 91.30 (?0.10)
ID ? 90.42 (?0.11) ?? 90.64 (?0.11) ? 90.50 (?0.11) 87.78 (?0.25) ?? 91.27 (?0.09)
Table 1: Average accuracy (? standard error) for the best three metadata attributes, when using a single attribute at
a time. Results that are numerically the best within a row are in bold. Results significantly better than BASE are
marked with ?, and better than META are marked with ?. Significance is measured using a two-tailed paired t-test with
? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D MAMD ?? 93.07 (?0.19) ?? 93.12 (?0.11) 87.08 (?1.72) ?? 93.19 (?0.12)
1-ORCL ?? 93.06 (?0.11) ?? 93.17 (?0.11) 92.37 (?0.11) ?? 93.39 (?0.12)
1-TUNE ? 92.64 (?0.12) ? 92.81 (?0.16) 92.15 (?0.17) ?? 93.07 (?0.14)
1-MEAN ? 92.61 (?0.09) ? 92.59 (?0.10) 91.41 (?0.12) ? 92.58 (?0.10)
5
0
K
-
B
A
L MAMD ?? 91.42 (?0.09) ?? 91.06 (?0.04) 81.43 (?2.79) ?? 91.40 (?0.08)
1-ORCL ?? 90.89 (?0.10) ?? 90.87 (?0.11) 89.33 (?0.13) ?? 91.45 (?0.07)
1-TUNE ? 90.33 (?0.10) ?? 90.70 (?0.14) 89.13 (?0.16) ?? 91.26 (?0.08)
1-MEAN ? 90.30 (?0.06) 89.92 (?0.07) 88.25 (?0.07) 90.06 (?0.08)
Table 2: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all attributes, either
directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies described earlier.
Formatting and significance symbols are the same as in Table 1.
META Identical to BASE with a unique bias feature
added for each attribute value (Joshi et al, 2012).
1-META A special case of META where a unique
bias feature is added only for a single attribute.
To use multi-domain learning directly, we could
select a single attribute as the domain. We consider
several strategies for picking this attribute and eval-
uate both FEDA and MDR in this setting.
1-MEAN Choose an attribute randomly, equivalent
to the expected (mean) error over all attributes.
1-TUNE Select the best performing attribute on a
validation set.
1-ORCL Select the best performing attribute on
the test set. Though impossible in practice, this gives
the oracle upper bound on multi-domain learning.
All experiments use ten-fold cross-validation. We
report the mean accuracy, along with standard error.
4 Results
Table 1 shows the results of single-attribute multi-
domain learning methods for the WordSalad
datasets. The table shows the three best-performing
metadata attributes (as decided by the highest accu-
racy among all the methods across all 20 metadata
attributes). Clearly, several of the attributes can pro-
vide meaningful domains, which demonstrates that
methods that can select multiple attributes at once
are desirable. We also see that our modification to
MDR (MDR-NV) works the best.
Table 3 shows the results of single-attribute multi-
domain learning methods for the Convote dataset.
The first observation to be made on this dataset is
that neither the PARTY, nor the SPEAKER attribute
individually achieve significant improvement over
the META baseline, which uses both these attributes
as features. This is in contrast with the results on
the WordSalad dataset, where some attributes by
themselves showed an improvement over the META
baseline. Thus, this dataset represents a more chal-
lenging setup for our multi?attribute multi?domain
learning methods ? they need to exploit the two
weak attributes simultaneously.
We next demonstrate multi-attribute improve-
ments over the multi-domain baselines (Tables 2
and 4). For WordSalad datasets, our exten-
sions that can use all metadata attributes simul-
taneously are consistently better than both the
1-MEAN and the 1-TUNE strategies (except for
the case of the old variance scheme used by
(Dredze et al, 2009)). For the skewed subset
688
metadata 1-META FEDA MDR-U MDR-V MDR-NV
NONE (BASE) 67.08 (?1.74)
ALL (META) ? 82.60 (?1.95)
PARTY ? 78.81 (?1.47) ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
SPEAKER ? 77.49 (?1.75) ? 82.88 (?2.43) ? 78.32 (?1.91) 62.43 (?2.20) ? 72.26 (?1.37)
Table 3: Convote: Average accuracy (? standard error) when using a single attribute at a time. Results that are
numerically the best within a row are in bold. Results significantly better than BASE are marked with ?, and better
than META are marked with ?. Significance is measured using a two-tailed paired t-test with ? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
MAMD ?? 85.71 (?2.74) ? 84.12 (?2.56) 50.44 (?1.78) ?? 86.19 (?2.49)
1-ORCL ? 84.77 (?2.47) ? 83.88 (?2.27) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-TUNE ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-MEAN ? 83.53 (?2.40) ? 80.77 (?1.92) ? 71.91 (?1.82) ? 78.09 (?1.69)
Table 4: Convote: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all
attributes, either directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies
described earlier. Formatting and significance symbols are the same as in Table 3.
50K-RND, MAMD+FEDA is significantly better than
1-TUNE+FEDA; MAMD+MDR-U is significantly bet-
ter than 1-TUNE+MDR-U; MAMD+MDR-NV is not
significantly different from 1-TUNE+MDR-U. For
the balanced subset 50K-BAL, a similar pattern
holds, except that MAMD+MDR-NV is significantly
better than 1-TUNE+MDR-NV. Clearly, our multi-
attribute algorithms provide a benefit over existing
approaches. Even with oracle knowledge of the test
performance using multi-domain learning, we can
still obtain improvements (FEDA and MDR-U in the
50K-BAL set, and all the Convote results, except
MDR-V).
Although MAMD+MDR-NV is not significantly bet-
ter than 1-TUNE+MDR-NV on the 50K-RND set,
we found that in every single fold in our ten-
fold cross-validation experiments, the ?best? single
metadata attribute decided using a validation set did
not match the best-performing single metadata at-
tribute on the corresponding test set. This shows
the potential instability of choosing a single best at-
tribute. Also, note that MDR-NV is a variant that we
have proposed in the current work, and in fact for
the earlier variant of MDR (MDR-U), as well as for
FEDA, we do see significant improvements when us-
ing all metadata attributes. Furthermore, the compu-
tational cost of evaluating every metadata attribute
independently to tune the single best metadata at-
tribute can be high and often impractical. Our ap-
proach requires no such tuning. Finally, observe
that for FEDA, the 1-TUNE strategy is not signifi-
cantly different from 1-MEAN, which just randomly
picks a single best metadata attribute. For MDR-U,
1-TUNE is significantly better than 1-MEAN on the
balanced subset 50K-BAL, but not on the skewed
subset 50K-RND.
As mentioned earlier, the Convote dataset is a
challenging setting for our methods due to the fact
that no single attribute is strong enough to yield im-
provements over the META baseline. In this setting,
both MAMD+FEDA and MAMD+MDR-NV achieve a
significant improvement over the META baseline,
with MDR-NV being the best (though not signif-
icantly better than FEDA). Additionally, both of
them are significantly better than their correspond-
ing 1-TUNE strategies. This result further supports
our claim that using multiple attributes in combi-
nation for defining domains (even when any single
one of them is not particularly beneficial for multi?
domain learning) is important.
5 Conclusions
We propose multi-attribute multi-domain learning
methods that can utilize multiple metadata attributes
simultaneously for defining domains. Using these
methods, the definition of ?domains? does not have
to be restricted to a single metadata attribute. Our
methods achieve a better performance on two multi-
attribute datasets as compared to traditional multi-
domain learning methods that are tuned to use a sin-
gle ?best? attribute.
Acknowledgments
This research is supported by the Office of Naval
Research grant number N000141110221.
689
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440?447.
Association for Computational Linguistics.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge,
Lily Scherlis, and Noah A. Smith. 2012. Word
Salad: Relating Food Prices and Descriptions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning (EMNLP 2012).
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification for
text categorization. Journal of Machine Learning Re-
search (JMLR).
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263. Association for Computational Linguistics.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1?
2):123?149.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning (ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi?task learning. In Proceedings of
the 2004 ACM SIGKDD international conference on
Knowledge discovery and data mining - KDD ?04.
Jenny R Finkel and Christopher D Manning. 2009. Hier-
archical Bayesian Domain Adaptation. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 602?
610. Association for Computational Linguistics.
Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-
olyn P. Rose?. 2012. Multi-domain learning: When do
domains matter? In Proceedings of EMNLP-CoNLL
2012, pages 1302?1312.
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012. Historical Analysis of Legal
Opinions with a Sparse Mixed-Effects Latent Variable
Model. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012).
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
690
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35?40,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Natural Language Models for Predicting Programming Comments
Dana Movshovitz-Attias
Computer Science Department
Carnegie Mellon University
dma@cs.cmu.edu
William W. Cohen
Computer Science Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
Statistical language models have success-
fully been used to describe and analyze
natural language documents. Recent work
applying language models to program-
ming languages is focused on the task
of predicting code, while mainly ignoring
the prediction of programmer comments.
In this work, we predict comments from
JAVA source files of open source projects,
using topic models and n-grams, and we
analyze the performance of the models
given varying amounts of background data
on the project being predicted. We evalu-
ate models on their comment-completion
capability in a setting similar to code-
completion tools built into standard code
editors, and show that using a comment
completion tool can save up to 47% of the
comment typing.
1 Introduction and Related Work
Statistical language models have traditionally
been used to describe and analyze natural lan-
guage documents. Recently, software engineer-
ing researchers have adopted the use of language
models for modeling software code. Hindle et al
(2012) observe that, as code is created by humans
it is likely to be repetitive and predictable, similar
to natural language. NLP models have thus been
used for a variety of software development tasks
such as code token completion (Han et al, 2009;
Jacob and Tairas, 2010), analysis of names in code
(Lawrie et al, 2006; Binkley et al, 2011) and min-
ing software repositories (Gabel and Su, 2008).
An important part of software programming and
maintenance lies in documentation, which may
come in the form of tutorials describing the code,
or inline comments provided by the programmer.
The documentation provides a high level descrip-
tion of the task performed by the code, and may
include examples of use-cases for specific code
segments or identifiers such as classes, methods
and variables. Well documented code is easier to
read and maintain in the long-run but writing com-
ments is a laborious task that is often overlooked
or at least postponed by many programmers.
Code commenting not only provides a summa-
rization of the conceptual idea behind the code
(Sridhara et al, 2010), but can also be viewed as a
form of document expansion where the comment
contains significant terms relevant to the described
code. Accurately predicted comment words can
therefore be used for a variety of linguistic uses
including improved search over code bases using
natural language queries, code categorization, and
locating parts of the code that are relevant to a spe-
cific topic or idea (Tseng and Juang, 2003; Wan et
al., 2007; Kumar and Carterette, 2013; Shepherd
et al, 2007; Rastkar et al, 2011). A related and
well studied NLP task is that of predicting natural
language caption and commentary for images and
videos (Blei and Jordan, 2003; Feng and Lapata,
2010; Feng and Lapata, 2013; Wu and Li, 2011).
In this work, our goal is to apply statistical lan-
guage models for predicting class comments. We
show that n-gram models are extremely success-
ful in this task, and can lead to a saving of up
to 47% in comment typing. This is expected as
n-grams have been shown as a strong model for
language and speech prediction that is hard to im-
prove upon (Rosenfeld, 2000). In some cases how-
ever, for example in a document expansion task,
we wish to extract important terms relevant to the
code regardless of local syntactic dependencies.
We hence also evaluate the use of LDA (Blei et al,
2003) and link-LDA (Erosheva et al, 2004) topic
models, which are more relevant for the term ex-
traction scenario. We find that the topic model per-
formance can be improved by distinguishing code
and text tokens in the code.
35
2 Method
2.1 Models
We train n-gram models (n = 1, 2, 3) over source
code documents containing sequences of com-
bined code and text tokens from multiple training
datasets (described below). We use the Berkeley
Language Model package (Pauls and Klein, 2011)
with absolute discounting (Kneser-Ney smooth-
ing; (1995)) which includes a backoff strategy to
lower-order n-grams. Next, we use LDA topic
models (Blei et al, 2003) trained on the same data,
with 1, 5, 10 and 20 topics. The joint distribution
of a topic mixture ?, and a set of N topics z, for
a single source code document with N observed
word tokens, d = {wi}Ni=1, given the Dirichlet pa-
rameters ? and ?, is therefore
p(?, z, w|?, ?) = (1)
p(?|?)
?
w
p(z|?)p(w|z, ?)
Under the models described so far, there is no dis-
tinction between text and code tokens.
Finally, we consider documents as having a
mixed membership of two entity types, code and
text tokens, d = ({wcodei }Cni=1, {wtexti }Tni=1), where
the text words are tokens from comment and
string literals, and the code words include the pro-
gramming language syntax tokens (e.g., public,
private, for, etc? ) and all identifiers. In this
case, we train link-LDA models (Erosheva et al,
2004) with 1, 5, 10 and 20 topics. Under the link-
LDA model, the mixed-membership joint distribu-
tion of a topic mixture, words and topics is then
p(?, z, w|?, ?) = p(?|?)? (2)
?
wtext
p(ztext|?)p(wtext|ztext, ?)?
?
wcode
p(zcode|?)p(wcode|zcode, ?)
where ? is the joint topic distribution, w is the set
of observed document words, ztext is a topic asso-
ciated with a text word, and zcode a topic associ-
ated with a code word.
The LDA and link-LDA models use Gibbs sam-
pling (Griffiths and Steyvers, 2004) for topic infer-
ence, based on the implementation of Balasubra-
manyan and Cohen (2011) with single or multiple
entities per document, respectively.
2.2 Testing Methodology
Our goal is to predict the tokens of the JAVA class
comment (the one preceding the class definition)
in each of the test files. Each of the models de-
scribed above assigns a probability to the next
comment token. In the case of n-grams, the prob-
ability of a token word wi is given by considering
previous words p(wi|wi?1, . . . , w0). This proba-
bility is estimated given the previous n? 1 tokens
as p(wi|wi?1, . . . , wi?(n?1)).
For the topic models, we separate the docu-
ment tokens into the class definition and the com-
ment we wish to predict. The set of tokens of
the class comment wc, are all considered as text
tokens. The rest of the tokens in the document
wr, are considered to be the class definition, and
they may contain both code and text tokens (from
string literals and other comments in the source
file). We then compute the posterior probability
of document topics by solving the following infer-
ence problem conditioned on the wr tokens
p(?, zr|wr, ?, ?) = p(?, z
r, wr|?, ?)
p(wr|?, ?) (3)
This gives us an estimate of the document distri-
bution, ?, with which we infer the probability of
the comment tokens as
p(wc|?, ?) =
?
z
p(wc|z, ?)p(z|?) (4)
Following Blei et al (2003), for the case
of a single entity LDA, the inference problem
from equation (3) can be solved by considering
p(?, z, w|?, ?), as in equation (1), and by taking
the marginal distribution of the document tokens
as a continuous mixture distribution for the set
w = wr, by integrating over ? and summing over
the set of topics z
p(w|?, ?) =
?
p(?|?)? (5)
(?
w
?
z
p(z|?)p(w|z, ?)
)
d?
For the case of link-LDA where the document is
comprised of two entities, in our case code to-
kens and text tokens, we can consider the mixed-
membership joint distribution ?, as in equation (2),
and similarly the marginal distribution p(w|?, ?)
over both code and text tokens from wr. Since
comment words in wc are all considered as text
tokens they are sampled using text topics, namely
ztext, in equation (4).
36
3 Experimental Settings
3.1 Data and Training Methodology
We use source code from nine open source JAVA
projects: Ant, Cassandra, Log4j, Maven, Minor-
Third, Batik, Lucene, Xalan and Xerces. For each
project, we divide the source files into a training
and testing dataset. Then, for each project in turn,
we consider the following three main training sce-
narios, leading to using three training datasets.
To emulate a scenario in which we are predict-
ing comments in the middle of project develop-
ment, we can use data (documented code) from the
same project. In this case, we use the in-project
training dataset (IN). Alternatively, if we train a
comment prediction model at the beginning of the
development, we need to use source files from
other, possibly related projects. To analyze this
scenario, for each of the projects above we train
models using an out-of-project dataset (OUT) con-
taining data from the other eight projects.
Typically, source code files contain a greater
amount of code versus comment text. Since we are
interested in predicting comments, we consider a
third training data source which contains more En-
glish text as well as some code segments. We use
data from the popular Q&A website StackOver-
flow (SO) where users ask and answer technical
questions about software development, tools, al-
gorithms, etc?. We downloaded a dataset of all ac-
tions performed on the site since it was launched in
August 2008 until August 2012. The data includes
3,453,742 questions and 6,858,133 answers posted
by 1,295,620 users. We used only posts that are
tagged as JAVA related questions and answers.
All the models for each project are then tested
on the testing set of that project. We report results
averaged over all projects in Table 1.
Source files were tokenized using the Eclipse
JDT compiler tools, separating code tokens and
identifiers. Identifier names (of classes, methods
and variables), were further tokenized by camel
case notation (e.g., ?minMargin? was converted to
?min margin?). Non alpha-numeric tokens (e.g.,
dot, semicolon) were discarded from the code, as
well as numeric and single character literals. Text
from comments or any string literals within the
code were further tokenized with the Mallet sta-
tistical natural language processing package (Mc-
Callum, 2002). Posts from SO were parsed using
the Apache Tika toolkit1 and then tokenized with
the Mallet package. We considered as raw code
tokens anything labeled using a <code> markup
(as indicated by the SO users who wrote the post).
3.2 Evaluation
Since our models are trained using various data
sources the vocabularies used by each of them are
different, making the comment likelihood given by
each model incomparable due to different sets of
out-of-vocabulary tokens. We thus evaluate mod-
els using a character saving metric which aims at
quantifying the percentage of characters that can
be saved by using the model in a word-completion
settings, similar to standard code completion tools
built into code editors. For a comment word with
n characters, w = w1, . . . , wn, we predict the two
most likely words given each model filtered by the
first 0, . . . , n characters ofw. Let k be the minimal
ki for which w is in the top two predicted word to-
kens where tokens are filtered by the first ki char-
acters. Then, the number of saved characters for w
is n? k. In Table 1 we report the average percent-
age of saved characters per comment using each of
the above models. The final results are also aver-
aged over the nine input projects. As an example,
in the predicted comment shown in Table 2, taken
from the project Minor-Third, the token entity is
the most likely token according to the model SO
trigram, out of tokens starting with the prefix ?en?.
The saved characters in this case are ?tity?.
4 Results
Table 1 displays the average percentage of char-
acters saved per class comment using each of the
models. Models trained on in-project data (IN)
perform significantly better than those trained on
another data source, regardless of the model type,
with an average saving of 47.1% characters using
a trigram model. This is expected, as files from
the same project are likely to contain similar com-
ments, and identifier names that appear in the com-
ment of one class may appear in the code of an-
other class in the same project. Clearly, in-project
data should be used when available as it improves
comment prediction leading to an average increase
of between 6% for the worst model (26.6 for OUT
unigram versus 33.05 for IN) and 14% for the best
(32.96 for OUT trigram versus 47.1 for IN).
1http://tika.apache.org/
37
Model n-gram LDA Link-LDA
n / topics 1 2 3 20 10 5 1 20 10 5 1
IN 33.05 43.27 47.1 34.20 33.93 33.63 33.05 35.76 35.81 35.37 34.59
(3.62) (5.79) (6.87) (3.63) (3.67) (3.67) (3.62) (3.95) (4.12) (3.98) (3.92)
OUT 26.6 31.52 32.96 26.79 26.8 26.86 26.6 28.03 28 28 27.82
(3.37) (4.17) (4.33) (3.26) (3.36) (3.44) (3.37) (3.60) (3.56) (3.67) (3.62)
SO 27.8 33.29 34.56 27.25 27.22 27.34 27.8 28.08 28.12 27.94 27.9
(3.51) (4.40) (4.78) (3.67) (3.44) (3.55) (3.51) (3.48) (3.58) (3.56) (3.45)
Table 1: Average percentage of characters saved per comment using n-gram, LDA and link-LDA models
trained on three training sets: IN, OUT, and SO. The results are averaged over nine JAVA projects (with
standard deviations in parenthesis).
Model Predicted Comment
IN trigram ?Train a named-entity extractor?
IN link-LDA ?Train a named-entity extractor?
OUT trigram ?Train a named-entity extractor?
SO trigram ?Train a named-entity extractor?
Table 2: Sample comment from the Minor-Third
project predicted using IN, OUT and SO based
models. Saved characters are underlined.
Of the out-of-project data sources, models us-
ing a greater amount of text (SO) mostly out-
performed models based on more code (OUT).
This increase in performance, however, comes at
a cost of greater run-time due to the larger word
dictionary associated with the SO data. Note that
in the scope of this work we did not investigate the
contribution of each of the background projects
used in OUT, and how their relevance to the tar-
get prediction project effects their performance.
The trigram model shows the best performance
across all training data sources (47% for IN, 32%
for OUT and 34% for SO). Amongst the tested
topic models, link-LDA models which distinguish
code and text tokens perform consistently better
than simple LDA models in which all tokens are
considered as text. We did not however find a
correlation between the number of latent topics
learned by a topic model and its performance. In
fact, for each of the data sources, a different num-
ber of topics gave the optimal character saving re-
sults.
Note that in this work, all topic models are
based on unigram tokens, therefore their results
are most comparable with that of the unigram in
Dataset n-gram link-LDA
IN 2778.35 574.34
OUT 1865.67 670.34
SO 1898.43 638.55
Table 3: Average words per project for which each
tested model completes the word better than the
other. This indicates that each of the models is bet-
ter at predicting a different set of comment words.
Table 1, which does not benefit from the back-
off strategy used by the bigram and trigram mod-
els. By this comparison, the link-LDA topic model
proves more successful in the comment prediction
task than the simpler models which do not distin-
guish code and text tokens. Using n-grams without
backoff leads to results significantly worse than
any of the presented models (not shown).
Table 2 shows a sample comment segment for
which words were predicted using trigram models
from all training sources and an in-project link-
LDA. The comment is taken from the TrainEx-
tractor class in the Minor-Third project, a ma-
chine learning library for annotating and catego-
rizing text. Both IN models show a clear advan-
tage in completing the project-specific word Train,
compared to models based on out-of-project data
(OUT and SO). Interestingly, in this example the
trigram is better at completing the term named-
entity given the prefix named. However, the topic
model is better at completing the word extractor
which refers to the target class. This example indi-
cates that each model type may be more successful
in predicting different comment words, and that
combining multiple models may be advantageous.
38
This can also be seen by the analysis in Table 3
where we compare the average number of words
completed better by either the best n-gram or topic
model given each training dataset. Again, while
n-grams generally complete more words better, a
considerable portion of the words is better com-
pleted using a topic model, further motivating a
hybrid solution.
5 Conclusions
We analyze the use of language models for pre-
dicting class comments for source file documents
containing a mixture of code and text tokens. Our
experiments demonstrate the effectiveness of us-
ing language models for comment completion,
showing a saving of up to 47% of the comment
characters. When available, using in-project train-
ing data proves significantly more successful than
using out-of-project data. However, we find that
when using out-of-project data, a dataset based on
more words than code performs consistently bet-
ter. The results also show that different models
are better at predicting different comment words,
which motivates a hybrid solution combining the
advantages of multiple models.
Acknowledgments
This research was supported by the NSF under
grant CCF-1247088.
References
Ramnath Balasubramanyan and William W Cohen.
2011. Block-lda: Jointly modeling entity-annotated
text and entity-entity links. In Proceedings of the 7th
SIAM International Conference on Data Mining.
Dave Binkley, Matthew Hearn, and Dawn Lawrie.
2011. Improving identifier informativeness using
part of speech information. In Proc. of the Working
Conference on Mining Software Repositories. ACM.
David M Blei and Michael I Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval. ACM.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE transac-
tions on pattern analysis and machine intelligence.
Mark Gabel and Zhendong Su. 2008. Javert: fully au-
tomatic mining of general temporal properties from
dynamic traces. In Proceedings of the 16th ACM
SIGSOFT International Symposium on Foundations
of software engineering, pages 339?349. ACM.
Thomas L Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proc. of the National Academy of
Sciences of the United States of America.
Sangmok Han, David R Wallace, and Robert C Miller.
2009. Code completion from abbreviated input.
In Automated Software Engineering, 2009. ASE?09.
24th IEEE/ACM International Conference on, pages
332?343. IEEE.
Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. In Software Engineering (ICSE), 2012
34th International Conference on. IEEE.
Ferosh Jacob and Robert Tairas. 2010. Code template
inference using language models. In Proceedings
of the 48th Annual Southeast Regional Conference.
ACM.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., volume 1, pages 181?184. IEEE.
Naveen Kumar and Benjamin Carterette. 2013. Time
based feedback and query expansion for twitter
search. In Advances in Information Retrieval, pages
734?737. Springer.
Dawn Lawrie, Christopher Morrell, Henry Feild, and
David Binkley. 2006. Whats in a name? a study
of identifiers. In Program Comprehension, 2006.
ICPC 2006. 14th IEEE International Conference on,
pages 3?12. IEEE.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th annual meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, volume 1, pages 258?267.
Sarah Rastkar, Gail C Murphy, and Alexander WJ
Bradley. 2011. Generating natural language sum-
maries for crosscutting source code concerns. In
Software Maintenance (ICSM), 2011 27th IEEE In-
ternational Conference on, pages 103?112. IEEE.
39
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceedings of the IEEE, 88(8):1270?1278.
David Shepherd, Zachary P Fry, Emily Hill, Lori Pol-
lock, and K Vijay-Shanker. 2007. Using natu-
ral language program analysis to locate and under-
stand action-oriented concerns. In Proceedings of
the 6th international conference on Aspect-oriented
software development, pages 212?224. ACM.
Giriprasad Sridhara, Emily Hill, Divya Muppaneni,
Lori Pollock, and K Vijay-Shanker. 2010. To-
wards automatically generating summary comments
for java methods. In Proceedings of the IEEE/ACM
international conference on Automated software en-
gineering, pages 43?52. ACM.
Yuen-Hsien Tseng and Da-Wei Juang. 2003.
Document-self expansion for text categorization. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, pages 399?400. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proc. of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999.
Roung-Shiunn Wu and Po-Chun Li. 2011. Video
annotation using hierarchical dirichlet process mix-
ture model. Expert Systems with Applications,
38(4):3040?3048.
40
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12?19,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
What pushes their buttons? Predicting comment polarity from the content
of political blog posts
Ramnath Balasubramanyan
Language Technologies Institute
Carnegie Mellon University
rbalasub@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Doug Pierce and David P. Redlawsk
Political Science Department
Rutgers University
drpierce@eden.rutgers.edu, redlawsk@rutgers.edu
Abstract
Political blogs as a form of social media al-
low for an uniquely interactive form of politi-
cal discourse. This is especially evident in fo-
cused blogs with a strong ideological identity.
We investigate techniques to identify topics
within the context of the community, which
when discussed in a blog post evoke a dis-
cernible positive or negative collective opin-
ion from readers who respond to posts in com-
ments. This is done by using computational
methods to assign sentiment polarity to blog
comments and learning community specific
models that summarize issues tackled by blogs
and predict the polarity based on the topics
discussed in a blog post.
1 Introduction
Recent work in political psychology has made it
clear that political decision-making is strongly influ-
enced by emotion. For instance, (Lodge and Taber,
2000) propose a theory of ?motivated reasoning?, in
which political information is processed in a way
that is determined, in part, by a quickly-computed
emotional react to that information. Strong exper-
imental evidence for motivated reasoning (some-
times called ?hot cognition?) exists (Huang and
Price, 2001); (Redlawsk, 2002); (Redlawsk, 2006);
(Isbell et al, 2006). However, despite some recent
proposals (Kim et al, 2008) it is unclear how to
computationally model a person?s emotional reac-
tion to news, and how to collect the data necessary
to fit such a model. One problem is that emotional
reactions are different for different people - a fact ex-
ploited in the use of political ?code words? intended
to invoke a reaction in only a particular subset of the
electorate (a technique sometimes called ?dog whis-
tle politics?).
In this paper, we evaluate the use of machine
learning methods to predict how members of a spe-
cific political community will emotionally reaction
to different types of news. More specifically, we use
a dataset of widely read (?A-list?) political blogs,
and attempt to predict the aggregate sentiment in the
comment section of blogs, as a function of the tex-
tual content of the blog posting. In this paper, we
consider only predicting polarity (positive and neg-
ative feeling). In contrast to work done traditionally
in sentiment analysis which focuses on determining
the sentiment expressed in text, in this work, we fo-
cus on the task of predicting the sentiment that a
block of text will evoke in readers, expressed in the
comment section, as a response to the blog post.
This task is related to, but distinct from, several
other studies that have been made using comments
and discussions in political communities, or analy-
sis of sentiment in comments - (Yano et al, 2009),
(O?Connor et al, 2010), (Tumasjan et al, 2010).
Below we discuss the methods used to address the
various parts of this task. First, we evaluate two
methods to automatically determine the comment
polarity: SentiWordNet (Baccianella and Sebastiani,
2010) a general purpose resource that assigns sen-
timent scores to entries in WordNet, and an auto-
12
mated corpus-specific technique based on pointwise
mutual information. The quality of the polarity as-
sessments by these techniques are made by compar-
ing them to hand annotated assessments on a small
number of blog posts. Second, we consider two
methods for predicting comment polarity from post
content: support vector machine classification, and
sLDA, a topic-modeling-based approach. Finally,
we demonstrate that emotional reactions are indeed
community-specific, compare the accuracy of this
approach to the more traditional approach of pre-
dicting sentiment of a text from the text itself, and
present our conclusions.
2 Data
In this study, we use a collection of blog posts from
five blogs: Carpetbagger(CB)1, Daily Kos(DK)2,
Matthew Yglesias(MY)3, Red State(RS)4, and Right
Wing News(RWN)5, that focus on American politics
made available by (Yano et al, 2009). The posts
were collected during November 2007 to October
2008, which preceded the US presidential elections
held in November 2008. The blogs included in the
dataset vary in political idealogy with blogs like
Daily Kos that are Democrat-leaning and blogs like
Red State tending to be much more conservative.
Since we are interested in studying the responses
to blog posts, the corpus only contains posts where
there have been at least one comment in the six days
after the post was published. It is important to note
that only the text in the blog posts and comments are
used in this study. All non-textual information like
pictures, hyperlinks, videos etc. are discarded. In
terms of text processing, for each blog, a vocabulary
is created consisting of all terms that occur at least
5 times in the blog. Stopwords are eliminated us-
ing a standard stopword list. Each blog post is then
represented as a bag of words from the post. Table
2 shows statistics of the datasets. Each dataset is
studied separately for the most part in the rest of the
paper.
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com/
3http://yglesias.thinkprogress.org/
4http://www.redstate.com/
5http://rightwingnews.com/
3 Labelling comments with sentiment
polarity
The first step in understanding the nature of posts
that evoke emotional responses is to get a measure of
the polarity in the sentiment expressed in the com-
ments section of a blog post. The measure indicates
the ability of the issues in the blog post and its treat-
ment, to evoke strong emotions in readers.
3.1 SentiWordNet
In the first stage of the study, we use SentiWord-
Net (Baccianella and Sebastiani, 2010) which as-
sociates a large number of words in WordNet with
a positive, negative and objective score (summing
up to 1). Firstly, all the comments for a blog post
in the comment section are aggregated and for the
words in the comments that are found in SentiWord-
Net, the net positive and negative scores are com-
puted. Since SentiWordNet entries are associated
with word senses and because we don?t perform
word sense disambiguation, the SentiWordNet po-
larity of the most dominant word sense is used for
words in the comment section. The sentiment in the
comment section is deemed to be positive if the net
positive score exceeds the negative score and nega-
tive otherwise. Therefore, each blog post is now as-
sociated with a binary response variable indicating
the polarity of the sentiment expressed in the com-
ments.
3.2 Using pointwise mutual information
A second technique to determine the sentiment po-
larity of comments uses the principle of pointwise
mutual information (PMI)(Turney, 2002). We first
construct a seed list of positive and negative words
by choosing the 100 topmost positive and negative
words from SentiWordNet and manually eliminat-
ing words from this list that don?t pertain to senti-
ment in our context. (Appendix A has the list of
seed words used.) This seed list is used to construct
a larger set of positive and negative words by com-
puting the PMI of the words in the seed lists with
every other word in the vocabulary. It?s important
to note that this list is constructed for the specific
corpus that we work with. Because every blog is
processed separately, we construct a different senti-
ment word list for each blog based on the statistics
13
Blog Pol align-
ment
#posts Vocabulary size Avg
#words
per post
Avg #com-
ments per
post
Avg
#words per
comment
section
Carpetbagger
(CB)
liberal 1201 4998 170 31 1306
Daily Kos (DK) liberal 2597 6400 103 198 3883
Matthew Ygle-
sias (MY)
liberal 1813 4010 69 35 1420
Red State (RS) conservative 2357 8029 158 28 806
Right Wing Na-
tion (RWN)
conservative 1184 6205 185 33 1015
Table 1: Dataset statistics
of word occurences. Words in the vocabulary are
ranked by the difference in the average of the PMI
with positive and negative seed words. The top 1000
words in the resultant sorted list are treated as pos-
itive words and the bottom 1000 words as negative
words. The comment section of every post is tagged
with a positive or negative polarity as in the previous
section by computing the total positive and negative
word counts.
Using the same seed word list, the procedure is
performed separately for each blog resulting in sen-
timent polarity lists that are particular to the com-
munity and idealogy associated with each blog. It
should be noted that while this method provides bet-
ter estimates of comment sentiment polarity (as seen
in Section 4), it involves more manual work in con-
structing a seed set than the SentiWordNet method
which does not require any manual effort.
3.3 Human labels
As a third method that is accurate but expensive, we
manually labeled comments from approximately 30
blog posts from each blog, with a positive or neg-
ative label. The guideline in labeling was to deter-
mine if the sentiment in the comment section was
positive or negative to the subject of the post. The
chief intention of this exercise is to determine the
quality of the polarity assessments of the SentiWord-
Net and PMI methods. While it is possible to di-
rectly use the assessments and train a classifier, the
performance of the classifier will be limited by the
very small number of training examples (30 instead
of thousands of examples). The accuracy of the two
Blog SentiWordNet accuracy PMI accuracy
CB 0.56 0.78
DK 0.54 0.72
MY 0.61 0.83
RS 0.54 0.74
RWN 0.64 0.84
Table 2: Measuring accuracy of automatic comment po-
larity detection
automatic methods to determine comment polarity
is shown in Table 2
The better accuracy of the PMI method can be ex-
plained by the fact that SentiWordNet is a general
purpose list that is not customized for the domain
which tends to make it noisy for text in the politi-
cal domain. The PMI technique corresponds more
closely with the human labels but it requires a little
human effort in building the initial seed list of posi-
tive and negative words.
4 Predicting sentiment from blog content
We now address the problem of using machine
learning techniques to predict the polarity of the
comments based on the blog post contents.
4.1 SVM
Firstly, we use support vector machines (SVM) to
perform classification. We frame the classification
task as follows: The input features to the classifier
are the words in the blog post i.e each blog post is
treated as a bag of words and the output variable is
the binary comment polarity computed in the previ-
14
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.56 0.58 0.79 0.79
dk 0.61 0.64 0.75 0.77
my 0.67 0.59 0.87 0.87
rs 0.53 0.55 0.74 0.76
rwn 0.57 0.59 0.90 0.90
Table 3: Accuracy: Using blog posts to predict comment
sentiment polarity
ous section. For our experiments, we used the SVM-
Light package 6 with a simple linear kernel and eval-
uated the classifier using 10 fold cross validation.
Table 3 shows the accuracy of the classifier for the
different blogs and polarity measuring schemes. The
errors in classification can be attributed in part to
the inherent difficulty of the task due to the noise of
the polarity labeling schemes and in part due to the
difficulty in obtaining a signal to predict comment
polarity from the body of the post.
4.2 Supervised LDA
Next, we use Supervised LDA (sLDA) (Blei and
McAuliffe, 2008) to do the classification. sLDA is
a model that is an extension of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) that models each
document as having an output variable in addition to
the document contents. The output variable in the
classification case is modeled as an output of a lo-
gistic regression model that uses the posterior topic
distribution of the LDA model as features. In this
task, the output variable is +1 or -1 depending on
the polarity of the comment section. In the experi-
ments with sLDA, we set the number of topics as 15
after experimenting with a range of topics and use
10-fold cross validation. The number of topics is set
lower than it usually is with topic modeling, due to
the relatively short length and small number of doc-
uments.
The advantage of sLDA in this task is that we in-
duce topics from the bodies of the blog posts that
serve to characterize the different issues that each
blog addresses. In addition, the logistic regres-
sion parameters indicate how each topic influences
the output variable. Table 4 shows the top 1 or 2
6http://svmlight.joachims.org/
topics with the highest negative and positive logis-
tic regression coefficients for each blog. Inspect-
ing the top words of the topics confirms our no-
tions of the kinds of issues that appeal to the read-
ers of each of the blogs. For instance, in the top-
ics induced from Daily Kos, a very liberal leaning
blog, we see that the most negative topic (i.e. the
topic that contributes the most to potential nega-
tive comments) talks about the Bush adminstration
and Vice President Cheney, which was and remains
quite unpopular with people from the left. The other
negative topic concerns the war in Iraq which was
also very unpopular within people whose beliefs are
liberal-leaning. The most positive topic seemingly
focuses on campaign funding. Our conjecture for
the high comment polarity is the great success in the
then Democratic candidate Obama?s fund raising at-
tempts during the presidential campaign. In the sec-
ond blog, Right Wing News, which is a conservative
blog, we see a different picture. The most negative
topic deals with Islam and Muslim people which are
issues that have tended to evoke negative reactions
from certain sections of people with conservative
political beliefs. Global warming also evoked nega-
tive comments which is consistent with the conser-
vative viewpoint that there isn?t evidence to suggest
that greenhouse gases cause global warming. The
most positive topic seems to be about anti-abortion
issues which is an issue that frequently pops up in
conservative political discourse. Topics from the
other blogs also seem to be in line with the standard
positions taken by liberal and conservatives on lead-
ing issues in US politics like taxation, immigration,
public health and the presidential campaign which
was in full flow at the time the data was collected.
Table 3 shows the accuracy of sLDA in predict-
ing the comment polarity based on the blog posts.
It can be seen from the table that sLDA performs
marginally better than SVM when trained on blog
posts, even though documents are now represented
in the lower dimensional topic space in contrast to
the high dimensional word space that was used with
SVM. sLDA provides the additional advantage of
providing an overall summary of the corpus via the
topic tables it induces.
15
Blog Topic words Topic co-efficient
CB
* bush president news administration house white officials report fox government
office military department public cheney john journal week pentagon national
-0.79
* huckabee giuliani romney mccain republican presidential religious campaign gop
john party candidate mitt rudy mike conservative thompson support paul candidates
0.48
DK
* bush administration congress law government court house intelligence white ex-
ecutive committee time cheney federal course national act president congressional
information
-1.54
* iraq war bush troops news military american president iraqi starts maine cheers
days jeers mccain moreville rightnow day americans people
-0.60
* money health campaign foster energy district million people nrcc dccc care elec-
tion time bill change funds don global federal economy
0.62
MY
* iraq war american military iraqi government people troops bush security united
forces world country surge presence political force maliki afghanistan
-0.50
* people care health don public immigration college political education school is-
sue insurance social system policy real lot isn actually sense
1.05
RS
* economy market people financial economic markets money world rate rates fed-
eral mortgage government credit prices price term inflation reserve oil
-0.30
* tax government taxes money economic care people spending million jobs ameri-
can energy health increase pay economy private free federal business
0.61
RWN
* people muslim world country war american law muslims time police america
rights free peace death city islamic government freedom united
-0.68
* democrats warming global vote election obama energy democratic change votes
climate people john gore political gas don voters party bill
-0.39
* people life women woman time own little love person children world live read
believe god isn school feel mean
0.47
Table 4: Topics from sLDA and weights
SentiWordNet PMI
Blog SVM sLDA SVM sLDA
cb 0.66 0.56 0.79 0.79
dk 0.72 0.59 0.74 0.73
my 0.64 0.61 0.87 0.89
rs 0.65 0.57 0.75 0.80
rwn 0.65 0.60 0.90 0.90
Table 5: Accuracy: Using comments to predict comment
sentiment polarity
4.3 Using comments to predict comment
polarity
In the previous experiments we were using the bod-
ies of the blog posts to predict comment polarity.
There are multiple factors which make this a diffi-
cult task. One major factor is the difficulty of learn-
ing potentially noisy labels using automatic meth-
ods. More interestingly, we operate under the hy-
pothesis that there is signal about comment polarity
in the bodies of the blog posts. To test this hypoth-
esis, we train classifiers on the comment sections
themselves to predict comment polarity. This serves
to eliminate the effect of our hypothesis and focus
on the inherent difficulty in learning the noisy la-
bels. Table 5 shows the results of these experiments.
We see that once again, sLDA results are compara-
ble to the accuracies reported by SVM and that PMI
labels are less noisier than the labels obtained using
16
Evaluating Trained on DK Trained on RWN
DK 0.75/0.77 0.61/0.62
RWN 0.74/0.71 0.90/0.90
Table 6: Cross blog results: Accuracy using SVM/sLDA
SentiWordNet. More importantly, we note that the
accuracy in predicting the comment polarity while
higher than the accuracy in predicting the polarity
from blog posts, is not significantly higher which
strongly suggests that blog posts have quite a bit of
information regarding comment polarity.
4.4 Cross blog experiments
The effect of the nature of the blog on the classifier is
examined by training models on the blog posts from
a conservative blog (RWN) using PMI-determined
polarities as targets and by testing the model by run-
ning liberal blog data (from DK) through it. Simi-
larly, we test RWN blog entries by training it on a
classifier trained on DK posts. The results of the ex-
periments are in Table 6. For easy reference, the
table also includes the accuracies when blogs are
trained using posts from the same blog (obtained
from Table 3). We see that the accuracy in predict-
ing polarity degrades when blog posts are tested on
a classifier trained on posts from a blog of opposite
political affiliation. These results indicate that emo-
tion is tied to the blog and community that one is
involved in.
4.5 Conclusion
We addressed the task of predicting the emotional
response that is induced in political discourses. To
this end, we tackled the tasks of determining the sen-
timent polarity of comments in blogs and the task of
predicting the polarity based on the content of the
blog post. Our approach also characterized the is-
sues talked about in specific blog communities. Our
experiments show that the community specific PMI
method provides a more accurate picture of the sen-
timent in comments than the generic SentiWordNet
technique. We also see that the context of the com-
munity is key as seen in the poor performance of
models trained on blogs from one end of the politi-
cal spectrum in predicting the polarity of responses
to blog posts in communities on the other end of the
spectrum.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
David Blei and Jon McAuliffe, 2008. Supervised Topic
Models, pages 121?128. MIT Press, Cambridge, MA.
D. M Blei, A. Y Ng, and M. I Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
Li-Ning Huang and Vincent Price. 2001. Motivations,
goals, information search, and memory about political
candidates. Political Psychology, 22(4):pp. 665?692.
Linda M. Isbell, Victor C. Ottati, and Kathleen C. Bruns.
2006. Affect and politics: Effects on judgment, pro-
cessing, and information seeking. In David Redlawsk,
editor, Feeling Politics: Emotion in Political Infor-
mation Processing. Palgrave Macmillan, New York,
USA.
Sung-youn Kim, Charles S. Taber, and Milton Lodge.
2008. A Computational Model of the Citizen as Mo-
tivated Reasoner: Modeling the Dynamics of the 2000
Presidential Election. SSRN eLibrary.
Milton Lodge and Charles Taber, 2000. Three Steps
toward a Theory of Motivated Political Reasoning.
Cambridge University Press.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
David P. Redlawsk. 2002. Hot cognition or cool con-
sideration? testing the effects of motivated reasoning
on political decision making. The Journal of Politics,
64(04):1021?1044.
David Redlawsk. 2006. Motivated reasoning, affect, and
the role of memory in voter decision-making. In David
Redlawsk, editor, Feeling Politics: Emotion in Politi-
cal Information Processing. Palgrave Macmillan, New
York, USA.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elec-
tions with twitter: What 140 characters reveal about
political sentiment. In William W. Cohen and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.
Association for Computational Linguistics.
17
Tae. Yano, William. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of the North American Associ-
ation for Computational Linguistics Human Language
Technologies Conference.
Appendix A
18
Positive wonderfulness, admirableness, admirability, wonderful, admirable, top-flight, splendid, first-class, fantabu-
lous, excellent, good, balmy, mild, ennoble, dignified, amuse, agree, do good, benefit, vest, prefer, placate,
pacify, mollify, lenify, gentle, conciliate, assuage, appease, filigree, dazzle, admiringly, character, preem-
inence, note, eminence, distinction, radiance, amiability, bonheur, worship, adoration, divination, music,
euphony, judiciousness, essentialness, essentiality, gain, crispness, urbanity, courtesy, decency, modesty,
dedication, integrity, honourableness, honorableness, honor, goodness, good, morality, urbanity, tasteful-
ness, elegance, elegance, healthfulness, nutritiveness, nutritiousness, wholesomeness, fineness, choiceness,
loveliness, fairness, comeliness, beauteousness, picturesqueness, bluffness, good nature, character, props,
joke, jocularity, jest, worthy, salubrious, healthy, virtuous, esthetic, artistic, aesthetic, spiffing, superlative,
sterling, greatest, superb, brilliant, boss, banner, olympian, majestic, straightarrow, wide-eyed, round-eyed,
dewy-eyed, childlike, righteous, answerable, nice, decent, diffident, respected, reputable, self-respecting,
self-respectful, dignified, constructive, sweet, fabulous, fab, charming, admirable, idyllic, idealized, ide-
alised, ennobling, dignifying, nice, incumbent, clean, lucky, intellectual, formidable, awing, awful, awe-
some, awe-inspiring, amazing, important, joking, jocular, jocose, jesting, amicable, kind, genial, therapeu-
tic, sanative, remedial, healing, curative, gracious, gainly, goody-goody, good, superb, solid, good, inspired,
elysian, divine, worthy, quaint, discerning, golden, fortunate, blest, blessed, courteous, thorough, exhaus-
tive, better, benign, pretty, piquant, engaging, attractive, well, veracious, right, grace, goodwill, belong,
accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud,
glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately,
thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper-
ity, wellbeing, well-being, upbeat, wholeness, haleness, purity, pureness, innocence, antithesis, serendipity,
superordinate, superior, possible, pleaser, idolizer, idoliser, amoralist
Negative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de-
plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink,
reek, twinge, sting, prick, burn, sting, burn, bite, desensitize, desensitise, resent, begrudge, pity, compassion-
ate, abreact, agonize, agonise, muddy, settle, moan, groan, impugn, repudiate, deny, reject, disapprove, snub,
repel, rebuff, sting, stick, disapprove, refute, rebut, controvert, foul, curdle, smite, afflict, ease, comfort, ail,
inflame, woefully, sadly, lamentably, deplorably, hard, unluckily, unfortunately, regrettably, alas, worst,
throe, woe, suffering, inconvenience, incommodiousness, solacement, solace, dyspnoea, dyspnea, throe,
shrew, ruffian, rowdy, roughneck, hooligan, bully, plonk, sullenness, moroseness, glumness, moodiness,
malignity, malevolence, guilt, sorrow, ruefulness, rue, regret, dolour, dolor, dolefulness, gloating, gloat,
weakness, self-torture, self-torment, suffering, hurt, distress, torment, curse, straits, pass, head, excoriation,
canard, scurrility, billingsgate, scribble, scrawl, scratch, prejudice, preconception, bias, pill, onus, load, in-
cumbrance, encumbrance, burden, poignancy, pathos, penalty, badness, bad, fault, demerit, hardness, moldi-
ness, harshness, cruelty, cruelness, spitefulness, spite, nastiness, cattiness, bitchiness, malice, malevolency,
malevolence, heinousness, barbarousness, barbarity, atrocity, atrociousness, illegitimacy, unnaturalness, dis-
agreeableness, incongruousness, incongruity, ruggedness, hardness, unneighborliness, unfriendliness, dis-
agreeableness, sadness, lugubriousness, gloominess, shlock, schlock, dreck, mongrel, bastard, shenanigan,
roguishness, roguery, rascality, mischievousness, mischief-making, mischief, deviltry, devilry, devilment,
shitwork, overexertion, overacting, hamming, shlep, schlep, worst, upset, scrofulous, sick, ill, sheltered,
occult, trashy, rubbishy, undivided, worried, upset, disturbed, distressed, disquieted, troubled, unmanage-
able, uncontrollable, mussy, messy, unsympathetic, invalidating, disconfirming, wretched, woeful, miser-
able, execrable, deplorable, bush-league, bush, tinny, sleazy, punk, crummy, chintzy, cheesy, cheap, bum,
inferior, indifferent, lowly, humble, insufficient, deficient, insubordinate, cross-grained, contrarious, spas-
tic, spasmodic, convulsive, unaccepted, unacceptable, nonstandard, unsound, asocial, antisocial, feigned,
broken-down, vicious, reprehensible, deplorable, criminal, condemnable, notorious, infamous, ill-famed,
untreated, modified, limited, unmixed, unmingled, sheer, plain, cretinous, negative, imponderable, vexing,
maddening, infuriating, exasperating, ungrateful, sore, painful, afflictive, harsh, unpeaceable, unforbearing,
unpainted, underivative, scurrilous, opprobrious, abusive, verminous, outrageous, horrific, horrid, hideous,
creepy, pestilent, pernicious, deadly, baneful, paranormal, grotty, nasty, awful, transcendental, preternatural,
otherworldly, nonnatural, simulated, imitation, faux, false, fake, substitute, ersatz, strong, smart, wicked,
terrible, severe, unpitying, ruthless, remorseless, pitiless, unlikeable, unlikable, unmourned, unlamented,
rough, harsh, woeful, woebegone, lugubrious, heartsick, heartbroken, brokenhearted, bitter
Table 7: Seed words used in the PMI technique
19
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
Abstract
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
1 Introduction
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete ?prototype? records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
Table 1: A set of partially-complete prototype records,
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our system?s input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string ? not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
2
tain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our system?s ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al, 2007) in the
SEMEVAL ?Web People Search? shared task (Ar-
tiles et al, 2007).
2 Task Definition
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x?,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of ?prototypes? by x?; for entries
that are unspecified by the user, we write x?i,j = ?.
Prototypes are not assumed to provide complete in-
formation for any entity.
3 Model
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions ?
range over possible values at the entity level, not
over mentions in text. For example, ?2 might be
the distribution over possible last names and ?3 the
distribution over elected office titles. Note that ?2
would contain a low value for the last name Obama
? which indicates that few people have this last
name ? even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x?) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and x?i,j .
4 Inference
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
3
? ?2
x ? ?
w r ?r
c ?r
?c ?c
Figure 1: A plate diagram for the
text-and-tables graphical model.
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
? Generate the table entries. For each column j,
? Draw a concentration parameter ?j from a log-normal distribution,
log?j ? N (?, ?2).
? Draw a distribution over strings from a Dirichlet process ?j ?
DP(?j , G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
? For each row i, draw the entry xi,j ? ?j .
? Generate the text mentions.
? Draw a prior distribution over rows from a stick-breaking distribution,
?r ? Stick(?r).
? Draw a prior distribution over columns from a stick-breaking distribu-
tion, ?c ? Stick(?c).
? For each mention wm,
? Draw a row in the table rm ? ?r.
? For each word token wm,n (n ? {1, . . . , Nm}),
? Draw a column in the table cm,n ? ?c.
? Set the text wm,n = xrm,cm,n .
4.1 Gibbs sampling
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j . Using Bayes? rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions ?r, ?c, and ?j , using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x?). To do this, we must be
able to compute P (wm,n | rm = i, cm,n =
j, x?,w?(m,n), r?m, c?(m,n), ?j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w?(m,n), r?m,
and c?m,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column ?i, j?? that
is, if x?ij 6= ?. Then the probability is simply,
P (wm,n | rm = i, cm,n = j, x?, . . .) =
{
1, if x?ij = wm,n
0, if x?ij 6= wm,n.
(1)
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij , the probability P (wm,n | xij , rm = i, cm,n =
j) is still a delta function, so we have:
?
P (wm,n | xrm,cm,n)P (xrm,cm,n | . . .) dxrm,cm,n
= P (x = wm,n | w?(m,n), r?m, c?(m,n), x?, . . .)
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm?,n? then its value must be identical to that
4
string; otherwise it is unknown. More formally, for
any cell ?i, j?, if ?wm?,n? : rm? = i ? cm?,n? =
j ? ?m?, n?? 6= ?m,n?, then P (xi,j = wm?,n?) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x? or the observa-
tionsw?(m,n). We can define these known elements
as x?, where x?ij = ? if x?ij = ? ? @?m,n? : rm =
i ? cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
P (xij | x??(i,j), ?) =
{ N(x??(i,j)=xij)
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) > 0
?
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) = 0
(2)
In our implementation, we maintain the table x?,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where x?ij = ?.
4.1.2 Sampling columns
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes? rule
to obtain P (cm,n | wm,n, rm, . . .) ? P (cm,n |
c?(m,n), ?c)?P (wm,n | cm,n, rm, x?, . . .). The like-
lihood term P (wm,n | cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over ?c. Writing N(c?(m,n) = j) for the count of
occurrences of column j in the set c?(m,n), we ob-
tain
P (cm,n = j | c?(m,n), ?c) =
{ N(c?(m,n)=j)
N(c?(m,n))+?c
, if N(c?(m,n) = j) > 0
?c
N(c?(m,n))+?c
, if N(c?(m,n) = j) = 0
(3)
4.1.3 Sampling rows
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P (wm | rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
1This relies on the assumption that the values of {cm,n} are
mutually independent given c?m. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
P (wm | rm,w?m, r?m, . . .)
=
?
j
P (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x? nor the heldout column counts c?m
include counts from any of the cells in row m.
4.2 Column swaps
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling ? we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j ; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
5
in the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x?, c? indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
Paccept(x? x?) = min
(
1,
P (c?)P (x?)
P (c)P (x)
)
(5)
We first consider the ratio of the table probabili-
ties, P (x
?|?)
P (x|?) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P (xt,i | x?(t,i), ?i),
with x?(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
P (xt,i | x?(t,j), ?j)P (xt,j | x?(t,i), ?i)
P (xt,i | x?(t,i), ?i)P (xt,j | x?(t,j), ?j)
(6)
Next we consider the ratio of the column proba-
bilities, P (c
?)
P (c) . Again we can apply exchangeabil-
ity, P (c) = P ({cm : rm = t} | {cm? : rm? 6=
t})P ({cm? : rm? 6= t}). The second term P ({cm? :
rm? 6= t}) is unaffected by the move, and so is iden-
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
(
P (c = j | c?t, ?c)
P (c = i | c?t, ?c)
)N(r=t?c=i)?N(r=t?c=j)
(7)
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P (c = i | c?t, ?c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
4.3 Hyperparameters
The concentration parameters ?r and ?c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters ?j control the di-
versity of each column in the table: if ?j is low then
we expect a high degree of repetition, as with titles;
if ?j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log?j ? N (?, ?2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each ?j , using the joint like-
lihood,
P (?j , x?(j) | ?, ?2) ?
exp(?(log?j ? ?)2)?
kj
j ?(?j)
2?2?(nj + ?j)
,
where x?(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x? and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of ?j for
each column in the table, we update ? and ?2 to their
maximum-likelihood estimates.
5 Temporal Prominence
Andy Warhol predicted, ?in the future, everyone will
be world-famous for fifteen minutes.? A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
epoch t? 1:
6
P (r(t)m = i|r
(t)
1...m?1, r
(t?1), ?r) ?
{
N(r(t)1...m?1 = i) + N(r
(t?1) = i), if > 0;
?r, otherwise.
(8)
The count of row i in epoch t ? 1 is written
N(r(t?1) = i); the count in epoch t for mentions
1 to m ? 1 is written N(r(t)1...m?1 = i). As before,
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r(t)?m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t ? 1 but exists in some other
epoch; the probability mass ?r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r(t)m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
P (r(t+1)|r(t)?m, r
(t)
m = i, ?r) ?
?
???
???
1 if N(r(t+1) = i) = 0,
N(r(t+1)=i)
?r
if N(r(t)?m = i) = 0,
1 + N(r
(t+1)=i)
N(r(t)?m=i)
if N(r(t)?m = i) > 0.
(9)
This favors entities which are frequent in epoch
t+ 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, ?. The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter ?r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
6 Evaluation Setup
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a system?s response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
2Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al, 2011).
3Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al, 2004).
7
ilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
? larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x? obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
0.2 0.3 0.4 0.5 0.6 0.70
0.1
0.2
0.3
recall
pre
cis
ion
 
 
baseline
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
Table 2: A subset of the entity database discovered by
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al, 2008).
7 Results
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
8
Table 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler ? which treats
each mention independently ? is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al, 2010) may help.
8 Related Work
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al, 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al, 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated ?gazetteers? of names
and name parts as features to improve performance
(Borthwick et al, 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al (2004) use a set of manually-
crafted ?transformations? of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
9
Resolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al, 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al, 2000; Pasula et al, 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
9 Conclusion
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al2006).4 We en-
vision other potential applications of the HDP: for
example, learning ?topics? of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Google?s support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
4One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
10
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 64?69. Associa-
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 294?303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):16?23,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255?
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC?04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268?
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164?172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:1183?1210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
?10, pages 291?295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419?424.
11
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554?560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Man?s Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699?
704.
12
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 307?315,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Crowdsourced Comprehension:
Predicting Prerequisite Structure in Wikipedia
Partha Pratim Talukdar
Machine Learning Department
Carnegie Mellon University
ppt@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
The growth of open-access technical publica-
tions and other open-domain textual informa-
tion sources means that there is an increas-
ing amount of online technical material that
is in principle available to all, but in prac-
tice, incomprehensible to most. We propose
to address the task of helping readers com-
prehend complex technical material, by us-
ing statistical methods to model the ?prereq-
uisite structure? of a corpus ? i.e., the se-
mantic impact of documents on an individual
reader?s state of knowledge. Experimental re-
sults using Wikipedia as the corpus suggest
that this task can be approached by crowd-
sourcing the production of ground-truth labels
regarding prerequisite structure, and then gen-
eralizing these labels using a learned classifier
which combines signals of various sorts. The
features that we consider relate pairs of pages
by analyzing not only textual features of the
pages, but also how the containing corpora is
connected and created.
1 Introduction and Motivation
Nicholas Carr has argued in his recent popular book
?The Shallows? that existing Internet technologies
encourage ?shallow? processing of recent and pop-
ular information, at the expense of ?deeper?, con-
templative study of less immediately-accessible in-
formation (Carr, 2011) . While Carr?s hypothesis is
difficult to formalize rigorously, it seems intuitively
plausible. For instance, user-generated content from
Twitter and Facebook is mainly comprised of short,
shallow snippets of information. Most current re-
search in AI (and more broadly in computer science)
does not seem likely to reverse this trend: e.g., work
in crowdsourcing has concentrated on tasks that can
be easily decomposed into small pieces, and much
current NLP research aims at facilitating short-term
?shallow? goals, such as answering well-formulated
questions (e.g., (Kwok et al, 2001)) and extracting
concrete facts (e.g., (Etzioni et al, 2006; Yates et al,
2007; Carlson et al, 2010)). This raises the ques-
tion: what can AI do to facilitate deep, contempla-
tive study?
In this paper we address one aspect of this larger
goal. Specifically, we consider automation of a
novel task?using AI methods to facilitate the ?deep
comprehension? of complex technical material. We
conjecture that the primary reason that technical
documents are difficult to understand is lack of mod-
ularity: unlike a self-contained document written for
a general reader, technical documents require cer-
tain background knowledge to comprehend?while
that background knowledge may also be available in
other on-line documents, determining the proper se-
quence of documents that a particular reader should
study is difficult.
We thus formulate the problem of comprehending
technical material as a probabilistic planning prob-
lem, where reading a document is an operator that
will probabilistically change the state of knowledge
K(u, t) of a user u at time t, in a manner that de-
pends on u?s prior knowledge K(u, t ? 1). Solving
this task requires, among other things, understand-
ing the effect of reading individual documents d ?
specifically, the concepts that are explained by d,
and the concepts that are prerequisites for compre-
hending d. This paper addresses this problem. In
particular, we consider predicting whether one page
in Wikipedia is a prerequisite of another.
More generally, we define the ?prerequisite struc-
307
CRF
Statistical_model
Graphical_model
Discriminative_model Markov_random_field
Parameter_learning
Maximum_likelihood
Hidden_Markov_Model
Viterbi_algorithm
random_variable
probability_distribution
variable_mathematics
Conditional_independence
Gradient_descent
Baum_Welch_algorithmMarkov_chain
Inference dynamic_programming Expectional_maximization_algorithm
Figure 1: The prerequisite structure rooted at the page ?Conditional Random Fields?, omitting nodes that would
already be known a typical CS graduate student.
Variable (Mathematics) Random Variable Probability Distribution
Conditional Independence Statistical Model Graphical Model
Discriminative Model Markov Random Field
Gradient Descent Parameter Learning Maximum Likelihood
Inference Dynamic Programming Viterbi Algorithm
Markov Chain Expectation Maximization Algorithm Baum Welch Algorithm
Hidden Markov Model CRF
Figure 2: A plan for comprehending ?Conditional Random Fields? (to be read left-to-right, top-to-bottom). Horizontal
lines indicate breaks between independent sections of the subgraph.
ture? for a corpus as a graph, where nodes are con-
cepts to comprehend, and a directed edge d ? d?
corresponds to the assertion ?understanding d? is a
prerequisite to understanding d?. For Wikipedia, we
assume a one-to-one correspondence between doc-
ument titles and concepts explicated by (i.e., post-
conditions of) these documents. Figure 2 presents
a small example of a prerequisite structure, and in-
dicates how it might be used to construct a plan for
comprehending a specific concept.
Focusing on Wikipedia has several advantages.
First, it is densely linked, and hence a document d
will likely be linked directly to any prerequisite page
d?. (However, not all hyperlinks will indicate a pre-
requisite.) Second, Wikipedia?s standardized format
makes textual analysis easier. Finally, there is a great
deal of social information available about how docu-
ments are used by the Wikipedia community. These
properties make it easy for us to explore the infor-
mativeness of different types of information with re-
spect to predicting prerequisite structure.
Our overall plan for producing a prerequisite
structure for a corpus is first, to use crowdsourc-
ing approaches to obtain a subset of the prerequisite
structure; and second, to extrapolate this structure
to the entire corpus using machine learning. Below,
we first describe datasets that we have collected,
based on five technical concepts in Wikipedia from
five different fields. We then outline the specifics of
our procedure for annotating prerequisite structure,
using Amazon?s Mechanical Turk, and demonstrate
that meaningful signals about prerequisite structure
can be obtained using a classifier that exploits sev-
eral sources: graph analysis of Wikipedia?s link
graph; graph analysis of a bipartite graph relating
Wikipedia pages to Wikipedians that have edited
these pages; and textual analysis. We complete our
experimental analysis of the prerequisite-structure
prediction task by discussing and evaluating the de-
gree to which prerequisite-structure prediction is
domain-independent, and the degree to which differ-
ent subareas of Wikipedia (e.g., biology vs computer
science) require different predictors.
After discussing related work, we return in the
concluding remarks to the overarching goal of fa-
cilitating comprehension, and discuss the relation-
308
Target Concept #Nodes #Edges #Edits
Global Warming 19,170 501,608 1,490,967
Meiosis 19,811 444,100 880,684
Newton?s Laws of Motion 15,714 436,035 795,988
Parallel Postulate 14,966 363,462 858,785
Public-key cryptography 16,695 371,104 1,003,181
Table 1: Target concepts used in the experiments.
ship of the current study to these goals. Specifi-
cally we note that facilitating comprehension also
requires understanding a user?s goals, and her initial
state of knowledge, in addition to understanding the
prerequisite structure of the corpus. We also discuss
the relationship between planning and prerequisite-
structure prediction and suggest that use of appro-
priately robust planning methods may lead to good
comprehension plans, even with imperfectly pre-
dicted prerequisite structure.
2 Experiments
As discussed above, we focus in this paper
on predicting prerequisite structure in Wikipedia.
While most Wikipedia pages are accessible to a
general reader, there are many pages that de-
scribe technical concepts, such as ?conditional
random fields?, ?cloud radiative forcing?, and
?Corticotropin-releasing factor?. Most of these tech-
nical pages are not self-contained: for instance,
to read and comprehend the page on ?conditional
random fields?, one will have to first understand
?graphical model?, and so on, as suggested by Fig-
ure 1. In this section, we evaluate the following
questions:
? Can we train a statistical classifier for prereq-
uisite classification in a target domain, where
the classifier is trained on out of domain (i.e.,
non-target domain) data annotated using Ama-
zon Mechanical Turk service?
? What are the effects of different types of signals
on the performance of such a classifier?
? How does out of domain training compare to in
domain training?
2.1 Experimental Setup
For our experiments, we choose five targets from
differing areas for experimentation, listed in Table 1.
Several of the techniques we used are based on graph
analysis. The full graphs associated with Wikipedia
are unwieldy to use for experimentation because of
their size: therefore, for each target concept, we ex-
tracted a moderate-sized low-conductance subgraph
of Wikipedia?s link graph containing the target, us-
ing a variant of the PageRank-Nibble algorithm (An-
dersen et al, 2006).1. As parameters we used ? =
0.15 and  = 10?7, yielding graphs with approx-
imately 15-20,000 nodes and 350-500,000 edges
each. We also collected the edit history for each
page in every subgraph forming a second graph for
each sub-domain 2. On average, each page from
these subgraphs had been edited about 20 times, by
about 8 unique editors. Details are given in Table 1.
For classification, we used a Maximum Entropy
(MaxEnt) classifier. Given a pair of Wikipedia pages
x = (d, d
?
) connected by a directed edge (hyperlink)
from d to d
?
, the classifier will predict with probabil-
ity p(+1|x) whether the main concept in page d
?
is
a prerequisite for the main concept in page d. The
classifier has the form
p(y|x) =
exp(w ? ?(x, y))
?
y??Y exp(w ? ?(x, y
?))
, y ? Y = {?1,+1}
where ?(x, y) is a feature function which represents
the pair of pages x = (d, d
?
) in a high dimensional
space, and w is the parameter vector of the classifier
which is estimated from training data. We use the
Mallet package3 to train and evaluate classifiers. For
the experiments in this paper, we shall exploit the
following types of features:
WikiHyperlinks: Features include the random
walk with restart (RWR) score (Tong et al,
2006) of the target concept page d
?
starting
from the source page d. Additional features
include the PageRank score of the target and
source pages.
1Specifically, we used the ?ApproximatePageRank? method
from (Andersen et al, 2006) to find a set of nodes S containing
a low-conductance subgraph, but did not prune S to find the
lowest-conductance subgraph of it with a ?sweep?. The version
of Wikipedia?s link graph we used was DBPedia?s version 3.7
(Auer et al, 2007)
2Specifically, a bipartite graph connecting pages and editors.
We used a version of Wikipedia?s edit history extracted by other
researchers (Leskovec et al, 2010), discarding edits marked as
?minor? by the editor.
3Mallet package: http://mallet.cs.umass.edu/
309
Domain
Time (s) / Worker
# HITs ?
Evaluation / HIT
Meiosis 38 3 400 0.50
Public-key Cryp. 26 3 200 0.63
Parallel Postulate 41 3 200 0.55
Newton?s Laws 20 5 400 0.47
Global Warming 14 5 400 0.56
Average 27.8 - - 0.54
Table 2: Statistics about the Gold-standard data prepared
using Amazon Mechanical Turk. Also shown are the
averaged ? statistics-based inter-annotator agreement in
each domain. The last row corresponds to the ? value
averaged across all five domains.
WikiEdits: This includes one feature?the
analogous RWR score on the graph of edit in-
formation.
WikiPageContent: Features in this category
are derived from the contents of the two
Wikipedia pages d and d
?
. Examples include:
the category identity of the source page; the
category identity of the target page; whether
the titles of d
?
and d are mentioned in the first
sentence of d; the name of the first section in d
which contains a link to d
?
; whether there is any
overlap in categories between the two pages;
whether d is also linked from d
?
; and the log of
the number of times d? is linked form d. We use
the JWPL library (http://jwpl.googlecode.com)
for efficient and structured access to Wikipedia
pages from a recent dump obtained on Jan 4,
2012.
2.1.1 Gold-standard Annotation from
Mechanical Turk4
In order to evaluate different prerequisite classi-
fication systems and also to train the MaxEnt clas-
sifier, we collected gold prerequisite decisions us-
ing Amazon Mechanical Turk (AMT). Since prepar-
ing annotated gold data for entire graphs in Table 1
would be prohibitively expensive, we used the fol-
lowing strategy to sample a smaller subgraph from
the larger domain-specific subgraph, which in turn
will be used for training and evaluation purposes.
Preliminary investigation suggested that most of the
pages in the prerequisite structure rooted at a target
4Amazon Mechanical Turk: http://mturk.amazon.com
concept d are connected to d via many short hyper-
link paths. Hence, for each target domain, we first
selected the top 20 nodes with highest RWR scores,
relative to the target concept, in the subgraph for that
target concept (as listed in Table 1.) We then sam-
pled a total of 400 edges from these selected nodes,
with outgoing edges from a node sampled with a fre-
quency proportional to its RWR score. Thus, using
this strategy, we selected up to 400 pairs of pages
(d, d
?
), where each pair has a hyperlink from d to d?.
Classification of a pair of hyperlinked Wikipedia
pages (d, d
?
) into one of the four following classes
constituted a Human Intelligence Task (HIT): (1) d
?
is a prerequisite of d; (2) d is a prerequisite of d
?
; (3)
the two pages are unrelated; (4) Don?t know. Sub-
sequently, based on the feedback from the workers,
a fifth option was also added: the two concepts are
related, but they don?t have any prerequisite relation-
ship between them. Based on the available workers
and turnaround time, the number of assignments per
HIT (i.e., number of unique workers assigned to a
particular HIT) was either 3 or 5; and the number
of HITs used was either 200 or 400. Depending on
the hardness of domain and availability of workers
opting to work on a domain, reward per HIT assign-
ment was varied from $0.02 (for Global Warming
and Newton?s Laws) to $0.08 (for Public-key Cryp-
tography, Meiosis and Parallel Postulate). This data
collection stage spanning all five domains was com-
pleted in about a week at a total cost of $278. Statis-
tics about the data are presented in Table 25.
Starting with the AMT data collected as above,
we next created a binary-labeled training dataset,
where each instance corresponds to a pair of pages.
We ignored all ?Don?t Know? labels, treated option
(1) above as vote for the corresponding prerequisite
edge, and treated all other options as votes against.
We then assigned the final label for a node pair using
majority vote (breaking ties arbitrarily).
2.1.2 Consistency of labels
In contrast to standard setup of gold data prepara-
tion where a single annotator is guaranteed to pro-
vide feedback on every instance, the situation in
case of Mechanical Turk-based annotation is differ-
ent, as the workers are at liberty to choose the HITs
(or instances) they want to work on. This makes
5The dataset is available upon request from the authors.
310
40
50
60
70
80
Meiosis Public Key Para. Postulate Newton?s Law Global Warming Average
Performance Comparison for Prerequisite Prediction
A
c
c
u
r
a
c
y
Random Baseline MaxEnt Classifier
Figure 3: Comparison of performance between the Max-
Ent classifier (right bar in each group) against a random
baseline (left bar in each group) in all five domains. On
average, the MaxEnt classifier results in an 8.6% absolute
improvement in accuracy.
standard ? statistics-based inter-annotator computa-
tion (Fleiss, 1981) inapplicable in the current set-
ting. We circumvented this problem by first select-
ing all workers with at least 100 feedbacks, and then
computing pairwise ? statistics between all pairs of
these frequent workers. These ? statistics were aver-
aged across each domain, and also averaged across
all domains. The results, also shown in Table 2,
show moderate agreement (recall that ? = 0 indi-
cates no correlation). We are encouraged to observe
that moderate level of agreement is possible even in
this setting, where there is no control over worker
background and quality. We next explore whether
this level of agreement is sufficient to train statisti-
cal classifiers.
2.2 Prerequisite Classification
In this section, we explore whether it is possible to
train a MaxEnt classifier to determine prerequisite
structure in a target domain, with the training per-
formed in ?leave one domain out? manner, where
the training data originates from domains other than
the target domain. For example, for classifications in
the target domain, say ?Global Warming?, we train
the classifier with annotated data from the remaining
four domains (or whatever domains are available).
We note that training on ?out of domain?, if it is
successful, has several benefits. First, a successful
training strategy in this setup removes any need to
have labeled data in each target domain of interest,
which is particularly relevant as labeled data is ex-
pensive to prepare. Second, a classifier trained just
once can be repeatedly used across multiple domains
without requiring retraining.
Accuracies of MaxEnt classifiers trained using the
?leave one domain out? strategy are shown in Fig-
ure 3; we report the test accuracy on each target do-
main, as well as the average across domains. Perfor-
mance of a random classifier is presented as a base-
line. Classes in the train and test sets were balanced
by oversampling the minority class. From Figure 3,
we observe that it is indeed possible to train pre-
requisite classifiers in an out of domain setting, us-
ing data from the Amazon Mechanical Turk service;
on average, the classifier outperforms the random
baseline with 8.6% absolute improvement in classi-
fication accuracy. We also experimented with other
rule-based classifiers6, and in all cases, the trained
MaxEnt classifier outperformed these baselines. Al-
though more sophisticated training strategies and
more clever feature engineering would likely yield
further improvements, we find it encouraging that
even a relatively straightforward classification tech-
nology along with a basic set of features was able to
achieve significant improvement in performance on
the novel task of prerequisite prediction.
2.3 Feature Ablation Experiments
The MaxEnt classifier evaluated in the previous
section had access to all three types of features:
WikiEdits, WikiHyperLinks, and WikiPageContent,
as described in the beginning of this section. In or-
der to evaluate the contribution of each such sig-
nal, we created ablated versions of the full Max-
Ent classifier which uses only one of these three
subsets. We call these thee variants: MaxEnt-
WikiEdits, MaxEnt-WikiHyperLinks, and MaxEnt-
WikiPageContent, respectively. Average accuracies
across all five domains comparing these three vari-
ants, in comparison to the Random baseline and
the full classifier (MaxEnt-Full, as in previous sec-
tion) are presented in Table 3. From this, we ob-
serve that all three variants perform better than the
random baseline, with maximum gains achieved
by the MaxEnt-WikiPageContent classifier, which
uses page content-based features exclusively. We
6For example, classify d
?
as a prerequisite for d if d
?
is
linked from the first paragraph in d.
311
System Accuracy
Random 50.22
MaxEnt-WikiEdits 51.62
MaxEnt-WikiHyperlinks 52.70
MaxEnt-WikiPageContent 57.84
MaxEnt-Full 58.82
Table 3: Comparison of accuracies (averaged across all
five domains) of the full MaxEnt classifier with its ablated
versions which use a subset of the features, and also the
random baseline. The full classifier, which exploits all
three types of signals (viz., WikiEdits, WikiHyperlinks,
and WikiPageContent) achieves the highest performance.
Domain
Wiki- Wiki- WikiPage-
All
Edits HyperLinks Content
Meiosis 5.4 2.4 0.3 1
Public-key
-0.7 -1.8 15.1 17.1
Crypto.
Parallel
3.1 6.1 11.7 14.7
Postulate
Newton?s
-0.2 6.2 3.9 3.9
Laws
Global
-7.7 0.1 5.8 6.8
Warming
Table 4: Accuracy gains (absolute) relative to the Ran-
dom baseline achieved by the full MaxEnt classifier as
well as its ablated versions trained with three different
subsets of the full classifier. Positive gains are marked in
bold.
also note that the full classifier MaxEnt-Full, is
able to effectively combine three types of signals
improving performance even further. In Table 4,
we present a per-domain breakdown of the gains
achieved by these four classifiers over the random
baseline. From this, we observe that the MaxEnt-
WikiEdits classifier outperforms the random base-
line only in 2 out of 5 domains. This might be due
to the fact that the MaxEnt-WikiEdits uses uses only
one feature?the RWR score of the target page rela-
tive to the source page on the Wikipedia edits graph.
We hope that use of more discriminating features
should further help this classifier. From Table 4, we
also observe that MaxEnt-WikiHyperLinks is able to
outperform the random baseline in 4 out of 5 cases,
and the MaxEnt-WikiPageContent (as well as the
full classifier) outperforms the random baseline in
all 5 domains, sometimes with large gains (as in the
case of Public-key Cryptography domain).
40
50
60
70
80
Meiosis Public Key Para. Postulate Newton?s Laws Global Warming Average
Effect of Out of Domain vs In Domain Training
A
c
c
u
r
a
c
y
Out of Domain Training In Domain Training
Figure 4: Accuracy comparison of out of domain (left bar
in each group) and in domain training (right bar in each
group) for the five domains. From this we observe that
good generalization performance is possible even when
there is no in domain training data available.
2.4 Effect of Out of Domain Training
All the classifiers evaluated in previous sections
were trained in an out of domain setting, i.e., the
training data originated from domains outside the
domain in which the classifier is applied and eval-
uated. This has several benefits, as noted above. An
alternative and more standard way to train classi-
fiers is to have the training and evaluation data be
from the same domain (below, the in-domain set-
ting). While such a classifier will require labeled
training from each domain of interest, it is nonethe-
less of interest to compare in-domain and out-of-
domain learning. If there are substantive differences,
this could be used to improve prerequisite-structure
predictor in a subdomain (e.g., biology), or may
suggest alternative training methods (e.g., involving
transfer learning).
Motivated by this, for each domain, we com-
pare the performances of the out-of-domain and in-
domain classification performances. The results are
shown in Figure 4. On average, we observe that the
out-of-domain classifier is able to achieve 93% of
the performance of the in-domain classifier. We note
that this is encouraging for domain-independent
prerequisite-structure prediction, as this suggests
that for the prerequisite classification task, close to
optimal (i.e., in-domain performance) is possible
when the classifiers are trained in an out-of-domain
setting.
312
3 Related Work
We believe the task of prerequisite structure predic-
tion to be novel; however, it is clearly related to a
number of other well-studied research problems.
In light of our emphasis on Wikipedia, a con-
nection can be drawn between identifying prerequi-
sites and measuring the semantic relatedness of con-
cepts using Wikipedia?s link structure (Yeh et al,
2009). We consider here a related but narrower
question, namely whether an inter-page link will im-
prove comprehension for a specific reader.
In the area of intelligent tutoring and educational
data mining, recent research has looked at enriching
textbooks with authoritative web content (Agrawal
et al, 2010). Also, the problem of detecting pre-
requisite structure from differential student perfor-
mance on tests has been considered (e.g., (Pavlik
et al, 2008; Vuong et al, 2011)). Our proposal con-
siders discovering prerequisite structure from text,
rather than from exercises, and relies on different
signals.
Research in adaptive hypermedia (surveyed else-
where (Chen and Magoulas, 2005)) has goals similar
to ours. Most adaptive hypermedia systems operate
in narrow domains, which precludes use of some of
the crowd-based signals we consider here. In this lit-
erature, a distinction is often made between ?adapt-
ability? (the ability for a user to modify a presenta-
tion of hypermedia) and ?adaptivity? (the ability of
a system to adapt to a user?s needs.) In this frame-
work, our project focuses on adding ?adaptivity? to
existing corpora via a prerequisite structure, and our
principle contribution to this area is identifying tech-
niques that learn to combine textual features and so-
cial, crowd-based signals in order to usefully guide
comprehension.
Another related area is data-mining logs of Web
usage, as surveyed by Pierrakos et al(Pierrakos
et al, 2003). Our focus here is on facilitating a
particular type of Web usage, comprehension, rather
than more commonly-performed tasks like site nav-
igation and purchasing.
A number of ?open education? resources exist, in
which information can be organized into sharable
modules with known prerequisites between them
(e.g., Connexions (Baraniuk, 2008)). We focus here
on discovering prerequisite structure with machine-
learning methods rather than simply encoding it.
Similarly, a Wikimedia project7 has developed in-
frastructure allowing a user to manually assemble
Wikipedia pages into e-books. Our focus is on guid-
ing the process of finding and ordering the sections
of these books, not the infrastructure for generating
them. We also note that one widely-used way for
complex technical concepts to be broadly commu-
nicated is by writers or teams of writers, and pre-
vious researchers have investigated understanding
how collaborative writers work (Noe?l and Robert,
2004), and even developed tools for collaborative
writing (Zheng et al, 2006). Our work focuses on
tools to empower readers, rather than writers.
4 Conclusion
In this paper, we motivated the goal of ?crowdsourc-
ing? the task of helping readers comprehend com-
plex technical material, by using machine learning
to predict prerequisite structure from not only docu-
ment text, but also crowd-generated data such as hy-
perlinks and edit logs. While it is not immediately
obvious that this task is feasible, our experiments
suggest that relatively reliable features to predict
prerequisite structure exist, and can be successfully
combined using standard machine learning methods.
To achieve the broader goal of facilitating com-
prehension, predicting prerequisite structure is not
enough. Another important subproblem is using pre-
dicted prerequisites to build a feasible plan. As part
of ongoing work, we are exploring use of modern
optimization methods (such as Integer Linear Pro-
gramming) to compute ?reading plans? that mini-
mize a weighted linear combination of expected user
effort and probability of plan ?failure?8.
We also plan to explore another major subprob-
lem associated with facilitating comprehension?
personalizing a reading plan. Clearly, even if d? is
a prerequisite for d, a user interested in d need not
first read a page explaining d?, if she already under-
stands d?; instead, a reading plan based on prereq-
uisite structure should be adjusted based on what is
believed about the user?s prior knowledge state. In
7See http://en.labs.wikimedia.org/wiki/Wiki to print, the
?Wiki to Print? project.
8A plan ?failure? means that the plan not actually satisfy all
necessary prerequisites, leading to imperfect comprehension on
the part of the reader after she executes the plan.
313
the context of Wikipedia comprehension, one possi-
ble signal for predicting an individuals? prior knowl-
edge is the Wikipedia edit log: if we assume that
editors tend to edit things they know, the edit log
indicates which concepts tend to be jointly known,
and hence collaborative-filtering methods might be
able to more completely predict a user?s knowledge
given partial information about her knowledge?just
as collaborative-filtering is often used now to extrap-
olate user preference?s from knowledge of others?
joint preferences.
Besides contributing to the goal of facilitating
comprehension, we believe that the specific problem
of predicting prerequisite structure in Wikipedia is
a task of substantial independent interest. Prereq-
uisite structure can be thought of as a sort of ex-
planatory discourse structure, which is overlaid on
a hyperlink graph; hence, scaling up our methods
and applying them to all of Wikipedia would iden-
tify a canonical broad-coverage instance of such ex-
planatory discourse. This could be re-used for other
tasks much as lexical resources like WordNet are:
for instance, consider identifying explanatory dis-
course in an external technical text (e.g., a textbook)
by soft-matching it to the Wikipedia structure, us-
ing existing techniques to match the external text to
Wikipedia (Agrawal et al, 2010; Mihalcea and Cso-
mai, 2007; Milne and Witten, 2008).
Acknowledgments
This research has been supported in part by DARPA
(under contract number FA8750-09-C-0179), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors? and do not necessarily reflect those of the
sponsors. We are thankful to the anonymous review-
ers for their constructive comments
References
Agrawal, R., Gollapudi, S., Kenthapadi, K., Srivastava,
N., and Velu, R. (2010). Enriching textbooks through
data mining. In Proceedings of the First ACM Sympo-
sium on Computing for Development, page 19. ACM.
Andersen, R., Chung, F., and Lang, K. (2006). Local
graph partitioning using pagerank vectors. In Founda-
tions of Computer Science, 2006. FOCS?06. 47th An-
nual IEEE Symposium on, pages 475?486. IEEE.
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyga-
niak, R., and Ives, Z. (2007). Dbpedia: A nucleus
for a web of open data. In Aberer, K., Choi, K.-S.,
Noy, N., Allemang, D., Lee, K.-I., Nixon, L., Golbeck,
J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber,
G., and Cudr-Mauroux, P., editors, The Semantic Web,
volume 4825 of Lecture Notes in Computer Science,
pages 722?735. Springer Berlin / Heidelberg.
Baraniuk, R. (2008). Challenges and opportunities for
the open education movement: A Connexions case
study, pages 229?246. MIT Press, Cambridge, Mas-
sachusetts.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hr-
uschka Jr, E., and Mitchell, T. (2010). Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306?1313.
Carr, N. (2011). The shallows: What the Internet is doing
to our brains. WW Norton & Co Inc.
Chen, S. and Magoulas, G. (2005). Adaptable and adap-
tive hypermedia systems. IRM Press.
Etzioni, O., Banko, M., and Cafarella, M. (2006). Ma-
chine reading. In Proceedings of the National Confer-
ence on Artificial Intelligence.
Fleiss, J. (1981). The measurement of interrater agree-
ment. Statistical methods for rates and proportions,
2:212?236.
Kwok, C., Etzioni, O., and Weld, D. (2001). Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242?262.
Leskovec, J., Huttenlocher, D., and Kleinberg, J. (2010).
Governance in social media: a case study of the
wikipedia promotion process. In AAAI International
Conference on Weblogs and Social Media (ICWSM
?10). AAAI.
Mihalcea, R. and Csomai, A. (2007). Wikify!: linking
documents to encyclopedic knowledge. In CIKM, vol-
ume 7, pages 233?242.
Milne, D. and Witten, I. (2008). Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509?518. ACM.
Noe?l, S. and Robert, J. (2004). Empirical study on
collaborative writing: What do co-authors do, use,
and like? Computer Supported Cooperative Work
(CSCW), 13(1):63?89.
Pavlik, P., Cen, H., Wu, L., and Koedinger, K. (2008).
Using item-type performance to covariance to improve
the skill acquisition of an existing tutor. In Proc. of
the 1st International Conference on Educational Data
Mining.
Pierrakos, D., Paliouras, G., Papatheodorou, C., and Spy-
ropoulos, C. (2003). Web usage mining as a tool for
314
personalization: A survey. User Modeling and User-
Adapted Interaction, 13(4):311?372.
Tong, H., Faloutsos, C., and Pan, J.-Y. (2006). Fast ran-
dom walk with restart and its applications. In Proceed-
ings of the Sixth International Conference on Data
Mining, ICDM ?06.
Vuong, A., Nixon, T., and Towle, B. (2011). A method
for finding prerequisites within a curriculum. In Proc.
of the 4th International Conference on Educational
Data Mining.
Yates, A., Cafarella, M., Banko, M., Etzioni, O., Broad-
head, M., and Soderland, S. (2007). Textrunner: Open
information extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
Yeh, E., Ramage, D., Manning, C., Agirre, E., and Soroa,
A. (2009). Wikiwalk: random walks on wikipedia
for semantic relatedness. In Proceedings of the 2009
Workshop on Graph-based Methods for Natural Lan-
guage Processing, pages 41?49. Association for Com-
putational Linguistics.
Zheng, Q., Booth, K., and McGrenere, J. (2006). Co-
authoring with structured annotations. In Proceedings
of the SIGCHI conference on Human Factors in com-
puting systems, pages 131?140. ACM.
315
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 11?19,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Bootstrapping Biomedical Ontologies for Scientific Text using NELL
Dana Movshovitz-Attias
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
dma@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
wcohen@cs.cmu.edu
Abstract
We describe an open information extraction
system for biomedical text based on NELL
(the Never-Ending Language Learner) (Carl-
son et al, 2010), a system designed for ex-
traction from Web text. NELL uses a cou-
pled semi-supervised bootstrapping approach
to learn new facts from text, given an initial
ontology and a small number of ?seeds? for
each ontology category. In contrast to previ-
ous applications of NELL, in our task the ini-
tial ontology and seeds are automatically de-
rived from existing resources. We show that
NELL?s bootstrapping algorithm is suscepti-
ble to ambiguous seeds, which are frequent in
the biomedical domain. Using NELL to ex-
tract facts from biomedical text quickly leads
to semantic drift. To address this problem, we
introduce a method for assessing seed qual-
ity, based on a larger corpus of data derived
from the Web. In our method, seed quality
is assessed at each iteration of the bootstrap-
ping process. Experimental results show sig-
nificant improvements over NELL?s original
bootstrapping algorithm on two types of tasks:
learning terms from biomedical categories,
and named-entity recognition for biomedical
entities using a learned lexicon.
1 Introduction
NELL (the Never-Ending Language Learner) is a
semi-supervised learning system, designed for ex-
traction of information from the Web. The system
uses a coupled semi-supervised bootstrapping app-
roach to learn new facts from text, given an initial
ontology and a small number of ?seeds?, i.e., labeled
examples for each ontology category. The new facts
are stored in a growing structured knowledge base.
One of the concerns about gathering data from the
Web is that it comes from various un-authoritative
sources, and may not be reliable. This is especially
true when gathering scientific information. In con-
trast to Web data, scientific text is potentially more
reliable, as it is guided by the peer-review process.
Open access scientific archives make this informa-
tion available for all. In fact, the production rate of
publicly available scientific data far exceeds the abil-
ity of researchers to ?manually? process it, and there
is a growing need for the automation of this process.
The biomedical field presents a great potential for
text mining applications. An integral part of life sci-
ence research involves production and publication of
large collections of data by curators, and as part of
collaborative community effort. Prominent exam-
ples include: publication of genomic sequence data,
e.g., by the Human Genome Project; online col-
lections of three-dimensional coordinates of protein
structures; and databases holding data on genes. An
important resource, initiated as a means of enforc-
ing data standardization, are ontologies describing
biological, chemical and medical terms. These are
heavily used by the research community. With this
wealth of available data the biomedical field holds
many information extraction opportunities.
We describe an open information extraction sys-
tem adapting NELL to the biomedical domain. We
present an implementation of our approach, named
BioNELL, which uses three main sources of infor-
mation: (1) a public corpus of biomedical scientific
text, (2) commonly used biomedical ontologies, and
11
High PMI Seeds Random Seeds
SoxN achaete cycA cac section 33 28
Pax-6 Drosomycin Zfh-1 crybaby hv Bob
BX-C Ultrabithorax GATAe ael LRS dip
D-Fos sine oculis FMRFa chm sht 3520
Abd-A dCtBP Antp M-2 AGI tou
PKAc huckebein abd-A shanti disp zen
Hmgcr Goosecoid knirps Buffy Gap Scm
fkh decapentaplegic Sxl lac Mercurio REPO
abdA naked cuticle BR-C subcosta mef Ferritin
zfh-1 Kruppel hmgcr Slam dad dTCF
tkv gypsy insulator Dichaete Cbs Helicase mago
CrebA alpha-Adaptin Abd-B Sufu ora Pten
D-raf doublesex gusA pelo vu sb
MtnA FasII AbdA sombre domain II TrpRS
Dcr-2 GAGA factor dTCF TAS CCK ripcord
fushi
tarazu
kanamycin
resistance
Ecdysone
receptor
GABAA
receptor
diazepam
binding
inhibitor
yolk
protein
Tkv dCBP Debcl arm
Table 1: Two samples of fruit-fly genes, taken from the
complete fly gene dictionary. High PMI Seeds are the top
50 terms selected using PMI ranking, and Random Seeds
are a random draw of 50 terms from the dictionary. These
are used as seeds for the Fly Gene category (Section 4.2).
Notice that the random set contains many terms that are
often not used as genes including arm, 28, and dad. Us-
ing these as seeds can lead to semantic drift. In contrast,
high PMI seeds exhibit much less ambiguity.
(3) a corpus of Web documents.
NELL?s ontology, including categories and seeds,
has been manually designed during the system de-
velopment. Ontology design involves assembling a
set of interesting categories, organized in a meaning-
ful hierarchical structure, and providing represen-
tative seeds for each category. Redesigning a new
ontology for a technical domain is difficult without
non-trivial knowledge of the domain. We describe a
process of merging source ontologies into one struc-
ture of categories with seed examples.
However, as we will show, using NELL?s boot-
strapping algorithm to extract facts from a biomed-
ical corpus is susceptible to noisy and ambiguous
terms. Such ambiguities are common in biomedi-
cal terminology (see examples in Table 1), and some
ambiguous terms are heavily used in the literature.
For example, in the sentence ?We have cloned an
induced white mutation and characterized the in-
sertion sequence responsible for the mutant pheno-
type?, white is an ambiguous term referring to the
name of a gene. In NELL, ambiguity is limited us-
ing coupled semi-supervised learning (Carlson et al,
2009): if two categories in the ontology are declared
mutually exclusive, instances of one category are
used as negative examples for the other, and the two
categories cannot share any instances. To resolve
the ambiguity of white with mutual exclusion, we
would have to include a Color category in the ontol-
ogy, and declare it mutually exclusive with the Gene
category. Then, instances of Color will not be able
to refer to genes in the KB. It is hard to estimate what
additional categories should be added, and building
a ?complete? ontology tree is practically infeasible.
NELL also includes a polysemy resolution com-
ponent that acknowledges that one term, for exam-
ple white, may refer to two distinct concepts, say
a color and a gene, that map to different ontology
categories, such as Color and Fly Gene (Krishna-
murthy and Mitchell, 2011). By including a Color
category, this component can identify that white is
both a color and a gene. The polysemy resolver per-
forms word sense induction and synonym resolution
based on relations defined between categories in the
ontology, and labeled synonym examples. However,
at present, BioNELL?s ontology does not contain re-
lation definitions (it is based only on categories),
so we cannot include this component in our exper-
iments. Additionally, it is unclear how to avoid the
use of polysemous terms as category seeds, and no
method has been suggested for selecting seeds that
are representative of a single specific category.
To address the problem of ambiguity, we intro-
duce a method for assessing the desirability of noun
phrases to be used as seeds for a specific target cat-
egory. We propose ranking seeds using a Point-
wise Mutual Information (PMI) -based collocation
measure for a seed and a category name. Colloca-
tion is measured based on a large corpus of domain-
independent data derived from the Web, accounting
for uses of the seed in many different contexts.
NELL?s bootstrapping algorithm uses the mor-
phological and semantic features of seeds to pro-
pose new facts, which are added to the KB and used
as seeds in the next bootstrapping iteration to learn
more facts. This means that ambiguous terms may
be added at any learning iteration. Since white really
is a name of a gene, it is sometimes used in the same
semantic context as other genes, and may be added
to the KB despite not being used as an initial seed.
12
To resolve this problem, we propose measuring seed
quality in a Rank-and-Learn bootstrapping method-
ology: after every iteration, we rank all the instances
that have been added to the KB by their quality
as potential category seeds. Only high-ranking in-
stances are used as seeds in the next iteration. Low-
ranking instances are stored in the KB and ?remem-
bered? as true facts, but are not used for learning
new information. This is in contrast to NELL?s ap-
proach (and most other bootstrapping systems), in
which there is no distinction between acquired facts,
and facts that are used for learning.
2 Related Work
Biomedical Information Extraction systems have
traditionally targeted recognition of few distinct bi-
ological entities, focusing mainly on genes (e.g.,
(Chang et al, 2004)). Few systems have been devel-
oped for fact-extraction of many biomedical predi-
cates, and these are relatively small scale (Wattaru-
jeekrit et al, 2004), or they account for limited sub-
domains (Dolbey et al, 2006). We suggest a more
general approach, using bootstrapping to extend ex-
isting biomedical ontologies, including a wide range
of sub-domains and many categories. The current
implementation of BioNELL includes an ontology
with over 100 categories. To the best of our knowl-
edge, such large-scale biomedical bootstrapping has
not been done before.
Bootstrap Learning and Semantic Drift. Carl-
son et al (2010) use coupled semi-supervised boot-
strap learning in NELL to learn a large set of cate-
gory classifiers with high precision. One drawback
of using iterative bootstrapping is the sensitivity of
this method to the set of initial seeds (Pantel et al,
2009). An ambiguous set of seeds can lead to se-
mantic drift, i.e., accumulation of erroneous terms
and contexts when learning a semantic class. Strict
bootstrapping environments reduce this problem by
adding boundaries or limiting the learning process,
including learning mutual terms and contexts (Riloff
and Jones, 1999) and using mutual exclusion and
negative class examples (Curran et al, 2007).
McIntosh and Curran (2009) propose a metric
for measuring the semantic drift introduced by a
learned term, favoring terms different than the recent
m learned terms and similar to the first n, (shown
for n=20 and n=100), following the assumption that
semantic drift develops in late bootstrapping itera-
tions. As we will show, for biomedical categories,
semantic drift in NELL occurs within a handful of
iterations (< 5), however according to the authors,
using low values for n produces inadequate results.
In fact, selecting effective n and m parameters may
not only be a function of the data being used, but
also of the specific category, and it is unclear how to
automatically tune them.
Seed Set Refinement. Vyas et al (2009) suggest
a method for reducing ambiguity in seeds provided
by human experts, by selecting the tightest seed
clusters based on context similarity. The method is
described for an order of 10 seeds, however, in an
ontology containing hundreds of seeds per class, it is
unclear how to estimate the correct number of clus-
ters to choose from. Another approach, suggested
by Kozareva et al (2010), is using only constrained
contexts where both seed and class are present in a
sentence. Extending this idea, we consider a more
general collocation metric, looking at entire docu-
ments including both the seed and its category.
3 Implementation
3.1 NELL?s Bootstrapping System
We have implemented BioNELL based on the sys-
tem design of NELL. NELL?s bootstrapping algo-
rithm is initiated with an input ontology structure of
categories and seeds. Three sub-components oper-
ate to introduce new facts based on the semantic and
morphological attributes of known facts. At every
iteration, each component proposes candidate facts,
specifying the supporting evidence for each candi-
date, and the candidates with the most strongly sup-
ported evidence are added to the KB. The process
and sub-components are described in detail by Carl-
son et al (2010) and Wang and Cohen (2009).
3.2 Text Corpora
PubMed Corpus: We used a corpus of 200K full-
text biomedical articles taken from the PubMed
Central Open Access Subset (extracted in October
2010)1, which were processed using the OpenNLP
package2. This is the main BioNELL corpus and it
1http://www.ncbi.nlm.nih.gov/pmc/
2http://opennlp.sourceforge.net
13
is used to extract category instances in all the exper-
iments presented in this paper.
Web Corpus: BioNELL?s seed-quality colloca-
tion measure (Section 3.4) is based on a domain-
independent Web corpus, the English portion of the
ClueWeb09 data set (Callan and Hoy, 2009), which
includes 500 million web documents.
3.3 Ontology
BioNELL?s ontology is composed of six base on-
tologies, covering a wide range of biomedical sub-
domains: the Gene Ontology (GO) (Ashburner et
al., 2000), describing gene attributes; the NCBI Tax-
onomy for model organisms (Sayers et al, 2009);
Chemical Entities of Biological Interest (ChEBI)
(Degtyarenko et al, 2008), a dictionary focused on
small chemical compounds; the Sequence Ontol-
ogy (Eilbeck et al, 2005), describing biological se-
quences; the Cell Type Ontology (Bard et al, 2005);
and the Human Disease Ontology (Osborne et al,
2009). Each ontology provides a hierarchy of terms
but does not distinguish concepts from instances.
We used an automatic process for merging base
ontologies into one ontology tree. First, we group
the ontologies under one hierarchical structure, pro-
ducing a tree of over 1 million entities, including
856K terms and 154K synonyms. We then separate
these into potential categories and potential seeds.
Categories are nodes that are unambiguous (have a
single parent in the ontology tree), with at least 100
descendants. These descendants are the category?s
Potential seeds. This results in 4188 category nodes.
In the experiments of this paper we selected only
the top (most general) 20 categories in the tree of
each base ontology. We are left with 109 final cate-
gories, as some base ontologies had less than 20 cat-
egories under these restrictions. Leaf categories are
given seeds from their descendants in the full tree of
all terms and synonyms, giving a total of around 1
million potential seeds. Seed set refinement is de-
scribed below. The seeds of leaf categories are later
extended by the bootstrapping process.
3.4 BioNELL?s Bootstrapping System
3.4.1 PMI Collocation with the Category Name
We define a seed quality metric based on a large
corpus of Web data. Let s and c be a seed and a tar-
get category, respectively. For example, we can take
s = ?white?, the name of a gene of the fruit-fly, and c
= ?fly gene?. Now, let D be a document corpus (Sec-
tion 3.2 describes the Web corpus used for ranking),
and let Dc be a subset of the documents contain-
ing a mention of the category name. We measure
the collocation of the seed and the category by the
number of times s appears in Dc, |Occur(s,Dc)|.
The overall occurrence of s in the corpus is given
by |Occur(s,D)|. Following the formulation of
Church and Hanks (1990), we compute the PMI-
rank of s and c as
PMI(s, c) =
|Occur(s,Dc)|
|Occur(s,D)|
(1)
Since this measure is used to compare seeds of the
same category, we omit the log from the original for-
mulation. In our example, as white is a highly am-
biguous gene name, we find that it appears in many
documents that do not discuss the fruit fly, resulting
in a PMI rank close to 0.
The proposed ranking is sensitive to the descrip-
tive name given to categories. For a more robust
ranking, we use a combination of rankings of the
seed with several of its ancestors in the ontology hi-
erarchy. In (Movshovitz-Attias and Cohen, 2012)
we describe this hierarchical ranking in more detail
and additionally explore the use of the binomial log-
likelihood ratio test (BLRT) as an alternative collo-
cation measure for ranking.
We further note that some specialized biomedical
terms follow strict nomenclature rules making them
easily identifiable as category specific. These terms
may not be frequent in general Web context, lead-
ing to a low PMI rank under the proposed method.
Given such a set of high confidence seeds from a
reliable source, one can enforce their inclusion in
the learning process, and specialized seeds can addi-
tionally be identified by high-confidence patterns, if
such exist. However, the scope of this work involves
selecting seeds from an ambiguous source, biomed-
ical ontologies, thus we do not include an analysis
for these specialized cases.
3.4.2 Rank-and-Learn Bootstrapping
We incorporate PMI ranking into BioNELL using
a Rank-and-Learn bootstrapping methodology. Af-
ter every iteration, we rank all the instances that have
been added to the KB. Only high-ranking instances
14
Learning System Bootstrapping
Algorithm
Initial
Seeds
Corpus
BioNELL Rank-and-Learn
with PMI
PMI
top 50
PubMed
NELL NELL?s
algorithm
Random
50
PubMed
BioNELL+Random Rank-and-Learn
with PMI
Random
50
PubMed
Table 2: Learning systems used in our evaluation, all us-
ing the PubMed biomedical corpus and the biomedical
ontology described in Sections 3.2 and 3.3.
are added to the collection of seeds that are used in
the next learning iteration. Instances with low PMI
rank are stored in the KB and are not used for learn-
ing new information. We consider a high-ranking
instance to be one with PMI rank higher than 0.25.
4 Experimental Evaluation
4.1 Experimental Settings
4.1.1 Configurations of the Algorithm
In our experiments, we ran BioNELL and NELL
with the following system configurations, all using
the biomedical corpus and the ontology described in
Sections 3.2 and 3.3, and all running 50 iterations,
in order to evaluate the long term effects of ranking.
Section 4.2 includes a discussion on the learning rate
of the tested systems which motivates the reason for
evaluating performance at the 50th iteration.
To expand a category we used the following sys-
tems, also summarized in Table 2: (1) the BioNELL
system, using Rank-and-Learn bootstrapping (Sec-
tion 3.4.2) initialized with the top 50 seeds using
PMI ranking, (2) the NELL system, using NELL?s
original bootstrapping algorithm (Section 3.1) ini-
tialized with 50 random seeds from the category?s
potential seeds (NELL does not provide a seed se-
lection method), and (3) in order to distinguish
the contribution of Rank-and-Learn bootstrapping
over ranking the initial seeds, we tested a third
system, BioNELL+Random, using Rank-and-Learn
bootstrapping initialized with 50 random seeds.
4.1.2 Evaluation Methodology
Using BioNELL we can learn lexicons, collec-
tions of category terms accumulated after running
the system. One evaluation approach is to select
a set of learned instances and assess their correct-
ness (Carlson et al, 2010). This is relatively easy
for data extracted for general categories like City or
Sports Team. For example, it is easy to evaluate the
statement ?London is a City?. This task becomes
more difficult when assessing domain-specific facts
such as ?Beryllium is an S-block molecular entity?
(in fact, it is). We cannot, for example, use the help
of Mechanical Turk for this task. A possible alter-
native evaluation approach is asking an expert. On
top of being a costly and slow approach, the range
of topics covered by BioNELL is large and a single
expert is not likely be able to assess all of them.
We evaluated lexicons learned by BioNELL by
comparing them to available resources. Lexicons of
gene names for certain species are available, and the
Freebase database (Google, 2011), an open repos-
itory holding data for millions of entities, includes
some biomedical concepts. For most biomedical
categories, however, complete lexicons are scarce.
4.1.3 Data Sets
We compared learned lexicons to category dictio-
naries, lists of concept terms taken from the follow-
ing sources, which we consider as a Gold Standard.
We used three lexicons of biomedical categories
taken from Freebase: Disease (9420 terms), Chemi-
cal Compound (9225 terms), and Drug (3896 terms).
To evaluate gene names we used data from the
BioCreative Challenge (Hirschman et al, 2005),
an evaluation competition focused on annotations
of genes and gene products. The data includes
a dictionary of genes of the fruit-fly, Drosophila
Melanogaster, which specifies a set of gene iden-
tifiers and possible alternative forms of the gene
name, for a total of 7151 terms, which we consider
to be the complete fly gene dictionary.
We used additional BioCreative data for a named-
entity recognition task. This includes 108 scientific
abstracts, manually annotated by BioCreative with
gene IDs of fly genes discussed in the text. The ab-
stracts contain either the gene ID or any gene name.
4.2 Extending Lexicons of Biomedical
Categories
4.2.1 Recovering a Closed Category Lexicon
We used BioNELL to learn the lexicon of a
closed category, representing genes of the fruit-fly,
15
10 20 30 40 500
0.2
0.4
0.6
0.8
1
Iteration
Precis
ion
 
 
BioNELLNELLBioNELL+Random
(a) Precision
10 20 30 40 500
50
100
150
200
250
IterationC
umula
tive co
rrect l
exicon
 items
 
 
BioNELLNELLBioNELL+Random
(b) Cumulative correct items
10 20 30 40 500
100
200
300
400
500
IterationC
umula
tive in
correc
t lexic
on ite
ms
 
 BioNELLNELLBioNELL+Random
(c) Cumulative incorrect items
Figure 1: Performance per learning iteration for gene lexicons learned using BioNELL and NELL.
Learning System Precision Correct Total
BioNELL .83 109 132
NELL .29 186 651
BioNELL+Random .73 248 338
NELL by size 132 .72 93 130
Table 3: Precision, total number of instances (Total),
and correct instances (Correct) of gene lexicons learned
with BioNELL and NELL. BioNELL significantly im-
proves the precision of the learned lexicon compared with
NELL. When examining only the first 132 learned items,
BioNELL has both higher precision and more correct in-
stances than NELL (last row, NELL by size 132).
D. Melanogaster, a model organism used to study
genetics and developmental biology. Two samples
of genes from the full fly gene dictionary are shown
in Table 1: High PMI Seeds are the top 50 dictio-
nary terms selected using PMI ranking, and Random
Seeds are a random draw of 50 terms. Notice that the
random set contains many seeds that are not distinct
gene names including arm, 28, and dad. In con-
trast, high PMI seeds exhibit much less ambiguity.
We learned gene lexicons using the test systems de-
scribed in Section 4.1.1 with the high-PMI and ran-
dom seed sets shown in Table 1. We measured the
precision, total number of instances, and correct in-
stances of the learned lexicons against the full dic-
tionary of genes. Table 3 summarizes the results.
BioNELL, initialized with PMI-ranked seeds, sig-
nificantly improved the precision of the learned
lexicon over NELL (29% for NELL to 83% for
BioNELL). In fact, the two learning systems us-
ing Rank-and-Learn bootstrapping resulted in higher
precision lexicons (83%, 73%), suggesting that con-
strained bootstrapping using iterative seed ranking
successfully eliminates noisy and ambiguous seeds.
BioNELL?s bootstrapping methodology is highly
restrictive and it affects the size of the learned lexi-
con as well as its precision. Notice, however, that
while NELL?s final lexicon is 5 times larger than
BioNELL?s, the number of correctly learned items in
it are less than twice that of BioNELL. Additionally,
BioNELL+Random has learned a smaller dictionary
than NELL (338 and 651 terms, respectively) with a
greater number of correct instances (248 and 186).
We examined the performance of NELL after the
7th iteration, when it has learned a lexicon of 130
items, similar in size to BioNELL?s final lexicon (Ta-
ble 3, last row). After learning 130 items, BioNELL
achieved both higher precision (83% versus 72%)
and higher recall (109 versus 93 correct lexicon
instances) than NELL, indicating that BioNELL?s
learning method is overall more accurate.
After running for 50 iterations, all systems re-
cover only a small portion of the complete gene dic-
tionary (109-248 instances out of 7151), suggesting
either that, (1) more learning iterations are required,
(2) the biomedical corpus we use is too small and
does not contain (frequent) mentions of some gene
names from the dictionary, or (3) some other limita-
tions exist that prevent the learning algorithm from
finding additional class examples.
Lexicons learned using BioNELL show persis-
tently high precision throughout the 50 iterations,
even when initiated with random seeds (Figure 1A).
By the final iteration, all systems stop accumulating
further significant amounts of correct gene instances
(Figure 1B). Systems that use PMI-based Rank-
and-Learn bootstrapping also stop learning incorrect
16
Learning System Precision Correct Total
CC Drug Disease CC Drug Disease CC Drug Disease
BioNELL .66 .52 .43 63 508 276 96 972 624
NELL .15 .40 .37 74 522 288 449 1300 782
NELL by size .58 .47 .37 58 455 232 100 968 623
Table 4: Precision, total number of instances (Total), and correct instances (Correct) of learned lexicons of Chemical
Compound (CC), Drug, and Disease. BioNELL?s lexicons have higher precision on all categories compared with
NELL, while learning a similar number of correct instances. When restricting NELL to a total lexicon size similar to
BioNELL?s, BioNELL has both higher precision and more correct instances (last row, NELL by size).
instances (BioNELL and BioNELL+Random; Fig-
ure 1C). This is in contrast to NELL which continues
learning incorrect examples.
Interestingly, the highest number of correct gene
instances was learned using Rank-and-Learn boot-
strapping with random initial seeds (248 items;
BioNELL+Random). This may happen when the
random set includes genes that are infrequent in
the general Web corpus, despite being otherwise
category-specific in the biomedical context. As
such, these would result in low PMI rank (see note
in Section 3.4.1). However, random seed selection
does not offer any guarantees on the quality of the
seeds used, and therefore will result in unstable per-
formance. Note that BioNELL+Random was initi-
ated with the same random seeds as NELL, but due
to the more constrained Rank-and-Learn bootstrap-
ping it achieves both higher recall (248 versus 186
correct instances) and precision (73% versus 29%).
4.2.2 Extending Lexicons of Open Categories
We evaluated learned lexicons for three open cat-
egories, Chemical Compound (CC), Drug, and Dis-
ease, using dictionaries from Freebase. Since these
are open categories ? new drugs are being devel-
oped every year, new diseases are discovered, and
varied chemical compounds can be created ? the
Freebase dictionaries are not likely to contain com-
plete information on these categories. For our evalu-
ation, however, we considered them to be complete.
We used BioNELL and NELL to learn these cat-
egories, and for all of them BioNELL?s lexicons
achieved higher precision than NELL (Table 4). The
number of correct learned instances was similar in
both systems (63 and 74 for CC, 508 and 522 for
Drug, and 276 and 288 for Disease), however in
BioNELL, the additional bootstrapping restrictions
assist in rejecting incorrect instances, resulting in a
smaller, more accurate lexicon.
We examined NELL?s lexicons when they reached
a size similar to BioNELL?s final lexicons (at the 8th,
42nd and 39th iterations for CC, Drug, and Disease,
respectively). BioNELL?s lexicons have both higher
precision and higher recall (more correct learned in-
stances) than the comparable NELL lexicons (Ta-
ble 4, NELL by size, last row).
4.3 Named-Entity Recognition using a
Learned Lexicon
We examined the use of gene lexicons learned with
BioNELL and NELL for the task of recognizing
concepts in free text, using a simple strategy of
matching words in the text with terms from the lex-
icon. We use data from the BioCreative challenge
(Section 4.1.3), which includes text abstracts and the
IDs of genes that appear in each abstract. We show
that BioNELL?s lexicon achieves both higher preci-
sion and recall in this task than NELL?s.
We implemented an annotator for predicting what
genes are discussed in text, which uses a gene lexi-
con as input. Given sample text, if any of the terms
in the lexicon appear in the text, the corresponding
gene is predicted to be discussed in the text. Follow-
ing BioCreative?s annotation format, the annotator
emits as output the set of gene IDs of the genes pre-
dicted for the sample text.
We evaluated annotators that were given as in-
put: the complete fly-genes dictionary, a filtered
version of that dictionary, or lexicons learned us-
ing BioNELL and NELL. Using these annotators we
predicted gene mentions for all text abstracts in the
data. We report the average precision (over 108 text
17
Lexicon Precision Correct Total
BioNELL .90 18 20
NELL .02 5 268
BioNELL+Random .03 3 82
Complete Dictionary .09 153 1616
Filtered Dictionary .18 138 675
Table 5: Precision, total number of predicted genes (To-
tal), and correct predictions (Correct), in a named-entity
recognition task using a complete lexicon, a filtered lex-
icon, and lexicons learned with BioNELL and NELL.
BioNELL?s lexicon achieves the highest precision, and
makes more correct predictions than NELL.
abstracts) and number of total and correct predic-
tions of gene mentions, compared with the labeled
annotations for each text (Table 5).
Many gene names are shared among multiple
variants. For example, the name Antennapedia may
refer to several gene variations, e.g., Dgua\Antp or
Dmed\Antp. Thus, in our precision measurements,
we consider a prediction of a gene ID as ?true? if it
is labeled as such by BioCreative, or if it shares a
synonym name with another true labeled gene ID.
First, we used the complete fly gene dictionary
for the recognition task. Any dictionary gene that
is mentioned in the text was recovered, resulting
in high recall. However, the full dictionary con-
tains ambiguous gene names that contribute many
false predictions to the complete dictionary annota-
tor, leading to a low precision of 9%.
Some ambiguous terms can be detected using
simple rules, e.g., short abbreviations and numbers.
For example, section 9 is a gene named after the
functional unit to which it belongs, and abbreviated
by the symbol 9. Clearly, removing 9 from the full
lexicon should improve precision without great cost
to recall. We similarly filtered the full dictionary, re-
moving one- and two-letter abbreviations and terms
composed only of non-alphabetical characters, leav-
ing 6253 terms. Using the filtered dictionary, pre-
cision has doubled (18%) with minor compromise
to recall. Using complete or manually refined gene
dictionaries for named-entity recognition has been
shown before to produce similar high-recall and
low-precision results (Bunescu et al, 2005).
We evaluated annotators on gene lexicons learned
with BioNELL and NELL. BioNELL?s lexicon
achieved significantly higher precision (90%) than
other lexicons (2%-18%). It is evident that this lexi-
con contains few ambiguous terms as it leads to only
2 false predictions. Note also, that BioNELL?s lexi-
con has both higher precision and recall than NELL.
5 Conclusions
We have proposed a methodology for an open infor-
mation extraction system for biomedical scientific
text, using an automatically derived ontology of cat-
egories and seeds. Our implementation is based on
constrained bootstrapping in which seeds are ranked
at every iteration.
The benefits of iterative seed ranking have been
demonstrated, showing that our method leads to sig-
nificantly less ambiguous lexicons for all the eval-
uated biomedical concepts. BioNELL shows 51%
increase over NELL in the precision of a learned
lexicon of chemical compounds, and 45% increase
for a category of gene names. Importantly, when
BioNELL and NELL learn lexicons of similar size,
BioNELL?s lexicons have both higher precision and
recall. We have demonstrated the use of BioNELL?s
learned gene lexicon as a high precision annotator
in an entity recognition task (with 90% precision).
The results are promising, though it is currently dif-
ficult to provide a similar quantitative evaluation for
a wider range of concepts.
Many interesting improvements could be made
in the current system, mainly discovery of relations
between existing ontology categories. In addition,
we believe that Rank-and-Learn bootstrapping and
iterative seed ranking can be beneficial in general,
domain-independent settings, and we would like to
explore further use of this method.
Acknowledgments
This work was funded by grant 1R101GM081293
from NIH, IIS-0811562 from NSF and by a gift from
Google. The opinions expressed in this paper are
solely those of the authors.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, et al 2000. Gene ontology: tool for the
unification of biology. Nature genetics, 25(1):25.
18
J. Bard, S.Y. Rhee, and M. Ashburner. 2005. An ontol-
ogy for cell types. Genome Biology, 6(2):R21.
R. Bunescu, R. Ge, R.J. Kate, E.M. Marcotte, and R.J.
Mooney. 2005. Comparative experiments on learning
information extractors for proteins and their interac-
tions. Artificial Intelligence in Medicine, 33(2).
J. Callan and M. Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
A. Carlson, J. Betteridge, E.R. Hruschka Jr, T.M.
Mitchell, and SP Sao Carlos. 2009. Coupling semi-
supervised learning of categories and relations. Semi-
supervised Learning for Natural Language Process-
ing, page 1.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr, and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Twenty-Fourth Conference on Artificial
Intelligence (AAAI 2010), volume 2, pages 3?3.
J.T. Chang, H. Schu?tze, and R.B. Altman. 2004. Gap-
score: finding gene and protein names one word at a
time. Bioinformatics, 20(2):216.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
J.R. Curran, T. Murphy, and B. Scholz. 2007. Minimis-
ing semantic drift with mutual exclusion bootstrap-
ping. In Proceedings of the 10th Conference of the
Pacific Association for Computational Linguistics.
K. Degtyarenko, P. De Matos, M. Ennis, J. Hastings,
M. Zbinden, A. McNaught, R. Alca?ntara, M. Darsow,
M. Guedj, and M. Ashburner. 2008. Chebi: a database
and ontology for chemical entities of biological inter-
est. Nucleic acids research, 36(suppl 1):D344.
A. Dolbey, M. Ellsworth, and J. Scheffczyk. 2006.
Bioframenet: A domain-specific framenet extension
with links to biomedical ontologies. In Proceedings
of KR-MED, pages 87?94. Citeseer.
K. Eilbeck, S.E. Lewis, C.J. Mungall, M. Yandell,
L. Stein, R. Durbin, and M. Ashburner. 2005. The se-
quence ontology: a tool for the unification of genome
annotations. Genome biology, 6(5):R44.
Google. 2011. Freebase data dumps.
http://download.freebase.com/datadumps/.
L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia.
2005. Overview of biocreative: critical assessment of
information extraction for biology. BMC bioinformat-
ics, 6(Suppl 1):S1.
Z. Kozareva and E. Hovy. 2010. Not all seeds are equal:
measuring the quality of text mining seeds. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 618?626. Associa-
tion for Computational Linguistics.
J. Krishnamurthy and T.M. Mitchell. 2011. Which noun
phrases denote which concepts? In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Association for Computational Linguistics.
T. McIntosh and J.R. Curran. 2009. Reducing seman-
tic drift with bagging and distributional similarity. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 1-Volume 1, pages 396?404. As-
sociation for Computational Linguistics.
D. Movshovitz-Attias and W.W. Cohen. 2012. Boot-
strapping biomedical ontologies for scientific text us-
ing nell. Technical report, Carnegie Mellon Univer-
sity, CMU-ML-12-101.
J. Osborne, J. Flatow, M. Holko, S. Lin, W. Kibbe,
L. Zhu, M. Danila, G. Feng, and R. Chisholm. 2009.
Annotating the human genome with disease ontology.
BMC genomics, 10(Suppl 1):S6.
P. Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 938?947. As-
sociation for Computational Linguistics.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the National Conference on Artifi-
cial Intelligence (AAAI-99), pages 474?479.
E. W. Sayers, T. Barrett, D. A. Benson, S. H. Bryant,
K. Canese, V. Chetvernin, D. M. Church, M. DiCuc-
cio, R. Edgar, S. Federhen, M. Feolo, L. Y. Geer,
W. Helmberg, Y. Kapustin, D. Landsman, D. J.
Lipman, T. L. Madden, D. R. Maglott, V. Miller,
I. Mizrachi, J. Ostell, K. D. Pruitt, G. D. Schuler,
E. Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,
A. Souvorov, G. Starchenko, T. A. Tatusova, L. Wag-
ner, E. Yaschenko, and J. Ye. 2009. Database re-
sources of the National Center for Biotechnology In-
formation. Nucleic Acids Res., 37:5?15, Jan.
V. Vyas, P. Pantel, and E. Crestan. 2009. Helping edi-
tors choose better seed sets for entity set expansion. In
Proceeding of the 18th ACM conference on Informa-
tion and knowledge management. ACM.
R.C. Wang and W.W. Cohen. 2009. Character-level anal-
ysis of semi-structured documents for set expansion.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1503?1512. Association for Compu-
tational Linguistics.
T. Wattarujeekrit, P. Shah, and N. Collier. 2004. Pasbio:
predicate-argument structures for event extraction in
molecular biology. BMC bioinformatics, 5(1):155.
19
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 47?55,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Alignment-HMM-based Extraction of Abbreviations from Biomedical Text
Dana Movshovitz-Attias
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
dma@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
wcohen@cs.cmu.edu
Abstract
We present an algorithm for extracting abbre-
viation definitions from biomedical text. Our
approach is based on an alignment HMM,
matching abbreviations and their definitions.
We report 98% precision and 93% recall on
a standard data set, and 95% precision and
91% recall on an additional test set. Our re-
sults show an improvement over previously re-
ported methods and our model has several ad-
vantages. Our model: (1) is simpler and faster
than a comparable alignment-based abbrevia-
tion extractor; (2) is naturally generalizable to
specific types of abbreviations, e.g., abbrevia-
tions of chemical formulas; (3) is trained on a
set of unlabeled examples; and (4) associates a
probability with each predicted definition. Us-
ing the abbreviation alignment model we were
able to extract over 1.4 million abbreviations
from a corpus of 200K full-text PubMed pa-
pers, including 455,844 unique definitions.
1 Introduction
Abbreviations and acronyms are commonly used in
the biomedical literature for names of genes, dis-
eases and more (Ambrus, 1987). Abbreviation def-
initions are a source of ambiguity since they may
change depending on the context. The ability to rec-
ognize and extract abbreviations and map them to
a full definition can be useful for Information Ex-
traction tasks (Yu et al, 2007) and for the complete
understanding of scientific biomedical text.
Yu et al (2002) distinguish the two follow-
ing uses of abbreviations: (1) Common abbrevia-
tions are those that have become widely accepted as
synonyms, such as ?DNA, deoxyribonucleic acid?
or ?AIDS, acquired immunodeficiency syndrome?.
These represent common fundamental and impor-
tant terms and are often used, although not explic-
itly defined within the text (Fred and Cheng, 2003).
In contrast, (2) Dynamic abbreviations, are defined
by the author and used within a particular article.
Such definitions can often overlap, depending on
the context. For example, the term PBS most com-
monly abbreviates Phosphate Buffered Saline, but
in other contexts may refer to the following: Pain
Behavior Scale, Painful Bladder Syndrome, Paired
Domain-Binding Site, Particle Based Simulation,
Partitioned Bremer Support, Pharmaceutical Bene-
fits Scheme, and more. Some abbreviations fall be-
tween these two definitions in the sense that they are
normally defined in the text, however, they have be-
come widely used, and therefore they do not nor-
mally overlap with other abbreviations. An exam-
ple for this is the term ATP which, almost exclu-
sively, abbreviates adenosine triphosphate, and is
only rarely used in different contexts in biomedicine.
Gaudan et al (2005) define two similar con-
cepts, distinguishing Global and Local abbrevia-
tions. Global abbreviations are not defined within
the document, similar to common abbreviation. Lo-
cal abbreviations appear in the document alongside
the long form, similar to dynamic abbreviations.
The contextual ambiguity of dynamic, or local, ab-
breviations makes them an important target for ab-
breviation recognition tasks.
There is a great deal of variation in the way that
different authors produce abbreviations. Our defini-
tion of abbreviation is quite flexible and can best be
47
represented by the set of examples described in Ta-
ble 1. These include simple acronyms, in which the
first letter of every word from the long form is rep-
resented in the short form, as well as more complex
cases such as: inner letter matches, missing short
form characters, and specific substitutions (such as
of a chemical element and its symbol). We gener-
ally assume that the abbreviated form contains some
contraction of words or phrases from the full form.
This definition is consistent with the one defined by
many other extraction systems (see e.g., (Schwartz
and Hearst, 2002) and (Chang et al, 2002)).
We describe a method for extracting dynamic ab-
breviations, which are explicitly defined in biomed-
ical abstracts. For each of the input texts, the task
is to identify and extract ?short form, long form?
pairs of the abbreviations defined within the text. We
also provide a mapping, formed as an alignment, be-
tween the characters of the two forms, and the prob-
ability of this alignment according to our model.
Our approach is based on dividing the abbrevia-
tion recognition task into the following stages: (1)
Parsing the text and extracting candidate abbrevia-
tion pairs (long and short forms) based on textual
cues, such as parentheses; (2) Recovering a valid
alignment between the short and long form candi-
dates (valid alignments are defined in Section 3.2).
We perform a sequential alignment based on a pair-
HMM; (3) Extracting a final short and long form
from the alignment.
We will show that our approach is fast and accu-
rate: we report 98% precision and 93% recall on a
standard data set, and 95% precision and 91% recall
on a validation set. The alignment model: (1) is sim-
pler and faster than a comparable alignment-based
abbreviation extractor; (2) is naturally generalizable
to specific types of abbreviations; (3) is trained on a
set of unlabeled examples; and (4) associates a prob-
ability with each predicted definition.
2 Related Work
A wide variety of methods have been introduced
for recognizing abbreviations in biomedical context.
Many utilize one of the following techniques: rule-
based extraction, and extraction that relies on an
alignment of the abbreviation and full definition.
Abbreviation extraction methods have been used in
two main contexts: to create online collections of
abbreviations, normally extracted from PubMed ab-
stracts (Zhou et al, 2006; Gaudan et al, 2005; Adar,
2004), and as part of larger learning frameworks,
mainly for feature generation (Chowdhury et al,
2010; Huang et al, 2011).
Rule based extraction systems use a set of man-
ually crafted pattern-matching rules to recognize
and extract the pair of abbreviation and defini-
tion: Acrophile (Larkey et al, 2000) is an acronym
recognition system that exploits morphological rules
based on the case of the characters in the definitions.
Unlike many of the other available systems, it rec-
ognized acronyms that are defined without paren-
theses; The Alice system (Ao and Takagi, 2005) is
based on three extraction phases, each employing
an elaborate set of over 15 rules, patterns and stop
word lists. Liu and Friedman (2003) use a set of
statistical rules to resolve cases in which an abbre-
viation is defined more than once with several dif-
ferent definitions. While these methods normally
achieve high performance results, their main draw-
back is that they are difficult to implement and to
extend. Rule development is normally based on a
thorough investigation of the range of targeted ab-
breviations and the resulting heuristic patterns con-
tain subtleties that are hard to recreate or modify.
Several extraction methods have been developed
based on some variant of the Longest Common Sub-
sequence algorithm (LCS) (Schwartz and Hearst,
2002; Chang et al, 2002; Taghva and Gilbreth,
1999; Bowden et al, 1997). These systems search
for at least one possible alignment of an abbrevia-
tion and a full form definition.
The most widely used abbreviation extraction sys-
tem is that presented by Schwartz and Hearst (2002).
Their method scans the input text and extract pairs
of candidate abbreviations from text surrounding
parentheses. The algorithm scans the candidate defi-
nition from right to left, and searches for an implicit
alignment of the definition and abbreviation based
on few ad-hoc rules. This algorithm presents several
constraints on the type of recognized abbreviations,
the most restrictive being that every letter of the ab-
breviation must be matched during the process of
scanning the definition. Of the variety of available
extraction systems, this remains a popular choice
due to its simplicity and speed. However, as the au-
48
Short Long Type of Abbreviation
AMS Associated Medical Services Acronym using the first letter of each long-form word.
PS postsynaptic Inner letters are represented in the abbreviation.
NTx cross-linked N-telopeptides 1. Phonetic substitution (cross? x).
2. The short form is out-of-order.
3. Words from the long form are missing in the short form (linked).
EDI-2 Eating Disorders Inventory Characters from the short form are missing in the long form (-2).
NaB sodium butyrate Substitution of a chemical element by its symbol (sodium? Na).
MTIC 5-(3-N-methyltriazen-1-yl)-
imidazole-4-carboxamide
Chemical formula.
EBNA-1 Epstein-Barr virus (EBV) nuclear
antigen 1
Recursive definition, in which the long form contains another ab-
breviation definition.
3-D three-dimensional Substitution of a number name and symbol (three? 3).
A&E accident and emergency Substitution of a word and symbol (and? &).
anti-Tac antibody to the alpha subunit of the
IL-2 receptor
Synonym: the short form commonly represents the long form, al-
though it is not a direct abbreviation of it.
R.E.A.L. ?Revised European-American Clas-
sification of Lymphoid Neoplasms?
The long- and/or short-forms contain characters that are not di-
rectly related to the abbreviation (e.g., punctuation symbols).
Table 1: Examples of biomedical abbreviations.
thors report, this algorithm is less specific than other
approaches and consequently results in lower recall.
We will show that by performing an explicit align-
ment of the abbreviation using an alignment-HMM,
our model results in more accurate predictions, and
that the edit operations used in the alignment allow
for natural extensions of the abbreviations domain.
Another frequently used alignment based ap-
proach is that of Chang et al (2002), and it is closest
to our approach. After calculating an abbreviation
alignment, they convert the set of aligned terms into
a feature vector which is scored using a binary logis-
tic regression classifier. Using a correct threshold on
the alignment scores produces a high performance
abbreviation extractor. However this approach has
several drawbacks. The run-time of this algorithm
is fairly long (see Section 4.3), in part due to the
steps following the alignment recovery, i.e., calcu-
lating a feature vector, and generating an alignment
score. Additionally, choosing a score threshold may
depend on the genre of text, and different thresh-
olds lead to a variety of quality in the results. We
will show that presenting limitations on the range of
available alignments can produce correct alignments
more efficiently and quickly, maintaining high qual-
ity results, without the need for threshold selection.
Our alignment method distinguishes and penalizes
inner and leading gaps in the alignment, and it ap-
plies a set of constraints on the range of legal align-
ments. We will also show that relying solely on con-
strained alignments still allows for flexibility in the
definition of the range of desired abbreviations.
Ristad and Yianilos (1998) proposed a single state
alignment-HMM for learning string-edit distance
based on matched strings. In later work, Bilenko and
Mooney (2003) extend this model to include affine
gaps, by including in their model separate states
for Matches, Deletions and Insertions. McCallum
et al (2005) describe a discriminative string edit
CRF, following a similar approach to that of Bilenko
and Mooney. The CRF model includes two disjoint
sets of states, each representing either ?matching? or
?mismatching? string pairs. Each of the sets is sim-
ilar to the model described by Bilenko and Mooney.
All of these models require labeled training exam-
ples, and the CRF approach also requires negative
training examples, which train the ?mismatching?
states of the model. We describe an alignment HMM
that is suited for aligning abbreviation long and short
forms, and does not require any labeling of the input
text or training examples.
3 Method
In the following sections we describe a method for
extracting candidate abbreviation definitions from
text, and an alignment model with affine gaps for
49
Description Result
i. Input sentence: ?anti-sperm antibodies were studied by indirect mixed anti-globulin reaction test (MAR)?
ii. Candidate: ?MAR, by indirect mixed anti-globulin reaction test?
iii. Alignment:
HMM States
Short Form
Long Form
LG LG LG LG M M M M IG M M M IG
M A R
by indirect mixed anti - globulin reaction test
iv. Abbreviation: ?MAR, mixed anti-globulin reaction test?
Table 2: Example of the processing steps of a sample sentence. (i) Input sentence containing a single abbreviation.
(ii) Candidate ?short form, long form? pair extracted from the sentence (after truncating the long-form). (iii) The
most likely (Viterbi) alignment of the candidate pair, using our alignment model. Each state corresponds to a single
edit-operation, which consumed the corresponding short-form and long-form characters in the alignment. (iv) Final
abbreviation, extracted from the alignment by removing leading gaps.
matching the two forms of a candidate definition.
Finally we describe how to extract the final abbre-
viation prediction out of the alignment.
3.1 Extracting candidate abbreviations
The process described below scans the text for tex-
tual cues and extracts a list of candidate abbreviation
pairs, for every input document, in the form: ?short
form, long form?. The following text also describes
the restrictions and conditions of what we consider
to be valid candidate pairs. The assumptions made
in this work are generally less restrictive that those
introduced by previous extraction systems and they
lead to a larger pool of candidate definitions. We
will later show that false candidates normally pro-
duce invalid alignment of their short and long forms,
according to our alignment model, and so they are
removed and do not affect the final results.
The parsing process includes a search for both
single abbreviations, and abbreviation patterns. An
example of a sentence with a single abbreviation
can be seen in Table 2(i). We consider the fol-
lowing two cases of a single abbreviation defini-
tion: (1) ?long form (short form)?, and (2) ?short
form (long form)?. Note that in some cases, the
term within the parenthesis is parsed, e.g., in the
following text, ELISA is extracted from the paren-
thesis, by removing the text beyond the ?;? symbol:
?. . . human commercial enzyme-linked immunosor-
bent assay (ELISA; BioGen, Germany) . . . ?.
We also consider abbreviation patterns which
define multiple abbreviations simultaneously, as
demonstrated by these examples:
? ?anti-sperm (ASA), anti-phospholipid (APA),
and antizonal (AZA) antibodies? ? The main
noun (antibodies) follows the pattern.
? ?Epithelial-mesenchymal transition (EMT)
and interaction (EMI)? ? The main noun
(Epithelial-mesenchymal) is at the head of the
pattern.
Using textual cues (patterns and parentheses) we
extract candidate short and long forms. Whenever
possible, we consider the term within the parenthe-
sis as the short form, and the text to the left of the
parenthesis (until the beginning of the sentence) as
the candidate long form. We consider valid short
forms to be no longer than 3 words, having between
1 and 15 characters, and containing at least one let-
ter. In the case that the candidate short form was
found to be invalid by these definitions, we switch
the assignment of long and short forms. The long-
form string is truncated, following Park and Byrd
(2001), to a length of min(|A|+ 5, |A| ? 2), where
|A| is the length of the short form.
The length of the candidate long form is estimated
using the Park and Byrd formula, and it is therefore
normally the case that the resulting candidate long
form contains some leading characters that are not
part of the abbreviation definition. Next, we define
an alignment between short and long form strings
50
<?CRF-BP?, ?ligands for the corticotrophin-releasing factor binding protein?> 
       |  |   |  |   |  |C             | |R        | |F     | |-|B      | |P      | ligands|  |for|  |the|  |corticotrophin|-|releasing| |factor| | |binding| |protein| 
!"
#$"
%"
&$"
'"
Figure 1: Abbreviation alignment HMM model with
states: start (s), leading gaps (LG), match (M), inner gap
(IG) and end (e).
Edit
Operation
SF
Match
LF
Match
Valid
States
LF deletion  alpha-numeric
char
LG, IG
LF deletion  punct. symbol LG, M
LF deletion  word LG, IG
SF deletion digit or punct.  IG
Match char (partial) word M
Match char char M
Substitution ?&? ?and? M
Substitution ?1?-?9? ?one?-?nine? M
Substitution chem. symbol chemical name M
Table 3: Edit operations used in the alignment HMM
model including, long form (LF) and short form (SF)
deletions, matches and substitutions. We note the SF and
LF characters consumed by each edit operation, and the
HMM states in which it may be used.
which detects possible segments that are missing in
the alignment in either string (gaps).
3.2 Aligning candidate long and short forms
For each of the candidate pairs produced in the pre-
vious step, we find the best alignment (if any) be-
tween the short and the long form strings. We de-
scribe an alignment HMM that is suited for abbrevi-
ation alignments. The model is shown in Figure 1,
and Table 2 shows the parsing process of a sam-
ple sentence, including an alignment created for this
sample using the model.
3.3 Abbreviation Alignment with Affine
Leading and Inner Gaps
An alignment between a long and a short form of an
abbreviation can be modeled as a series of edit oper-
ations between the two strings, in which characters
from the short form may match a single or a series
of characters from the long form. In previous work,
Bilenko and Mooney (2003) describe a generative
model for string edit distance with affine gaps, and
an Expectation Maximization algorithm for learning
the model parameters using a labeled set of match-
ing strings. We propose a similar model for aligning
the short and long form of an abbreviation, using an
affine cost model for gaps
cost(g) = s+ e ? l (1)
where s is the cost of starting a gap, e is the cost of
extending a gap and l is the length of the gap. In our
method, we use extracted candidate pairs (candidate
short and long forms) as training examples.
As described above, candidate long forms are
formed by extracting text preceding parentheses and
truncating it to some length. This process may lead
to candidate long forms that contain leading charac-
ters that do not belong to the abbreviation, which
will result in leading gaps in the final alignment.
For example, the candidate long form presented in
Table 2(ii) contains the leading text ?by indirect ?.
While extra leading text is expected as an artifact of
our candidates extraction method, inner alignment
gaps are not expected to commonly appear in abbre-
viation alignments, and are usually an indication of a
bad alignment. The example presented in Table 2 is
of an abbreviation that does contain inner gaps (e.g.,
globulin) despite being a valid definition.
We distinguish leading and inner alignment gaps
using a model with five states: Leading Gap (LG),
Match (M), Inner Gap (IG), and two ?dummy? states
for the beginning and end of an alignment (Figure 1).
Since leading and inner gaps are represented by dif-
ferent states, their penalization is not coupled, i.e.,
they are associated with different s, e and l costs.
We use the EM algorithm to learn the model param-
eters, based on a set of unlabeled candidate pairs,
following the assumption that many false-candidates
will not produce a valid alignment, and will not af-
fect training. This is in contrast to previous string
edit distance models, which require labeled training
examples.
The main effort in developing a successful ab-
breviation alignment model involves generating a
meaningful set of edit operations. The edit opera-
tions used in our model,E = Ed?Em?Es, is shown
in Table 3 and includes: Ed, deletions of characters
or words from the long form, or of single characters
51
from the short form; Em, matches of a full of par-
tial word from the long form to a character in the
short form; and Es, word substitutions in which a
word from the long form is replaced by a symbol in
the short form. Note that: (1) while all types of dele-
tions from the long form are valid, deletions from the
short form are limited to digits and punctuation sym-
bols, and (2) deletion of non-alpha-numeric charac-
ters from the long form is not considered as opening
a gap but as a match, as it is common for non-alpha-
numeric characters to be missing in an abbreviation
(i.e., be ?matched? with the empty string, ).
Let x = x1 . . . xT be the short form candidate,
y = y1 . . . yV be the long form candidate, and
a = ?ap?np=1, ap = (ep, qp, ixp, jyp), be a pos-
sible alignment of the strings x and y. a repre-
sents as a sequence of HMM transitions, ap, where
ep ? E is an edit operation that consumes charac-
ters from x (deletion from the long form), y (dele-
tion from the short form), or both (match or substi-
tution), up to position ixp in x and jyp in y, and
is associated with a transition in the model to state
qp ? {LG,M, IG, e}. Let pi(q, q?) be the transition
probability between states q and q?, and let ?(q, e)
be the emission probability of the edit operation e at
state q. Given a candidate abbreviation pair ?x, y?,
and the model parameters pi and ? , the probability of
an alignment is given by
p(a|x, y, pi, ?) =
|a|?
p=1
pi(qp?1, qp) ? ?(qp, ep) (2)
where q0 is the start state. This probability can be
calculated efficiently using dynamic programming
with the forward-backward algorithm, and the most
likely alignment corresponds to the Viterbi distance
between x and y.
In our method, the model parameters, pi and ? ,
are estimated using the EM algorithm on an unla-
beled training set of candidate pairs that have been
extracted from the text, without any further process-
ing. At each EM iteration, we train on pairs that have
valid alignments (see below) with non-zero proba-
bility under the model parameters at that iteration.
3.3.1 Valid Alignments
Given the edit operations defined above, the only
valid way of matching a letter from the short form
to the long form is by matching that letter to the
beginning of a full or partial word, or by matching
that letter using a substitution operation. There is
no edit operation for deleting letters from the short
form (only digits and punctuation symbols can be
deleted). This means that for some candidate pairs
there are no valid alignments under this model, in
which case, no abbreviation will be predicted.
3.3.2 Extracting the Final Abbreviation
Given a valid alignment a between the candi-
date pair, x and y, we create a truncated alignment,
a?, by removing from a initial transitions in which
qp = LG. We consider a? valid if the number of
matches in a? = ?a?p?
n?
p=1 is greater than the number
of deletions,
n??
p=1
I(q?p = M) >
n??
p=1
I(q?p = IG) (3)
where I is an indicator function.
The final abbreviation prediction is given by the
portions of the x and y strings that are associated
with a?, named x? and y?, respectively. These may be
truncated compared to x and y, as leading alignment
gaps are removed. The final alignment probability is
given by p(a?|x?, y?, pi, ?).
3.4 Substitution Edit Operations
In contrast to rule-based extraction algorithms, in
our model, it is easy to introduce new types of edit
operations, and adjust the model to recognize a va-
riety of abbreviation types. As an example, we have
added a number of substitution operations (see Ta-
ble 3), including an operation for the commonly
used convention of replacing a chemical element
name (e.g., Sodium) with its symbol (Na). These
types of operations are not available using simpler
models, such as that presented by Schwartz and
Hearst (2002), making it impossible to recognize
some important biomedical entities, such as chem-
ical compounds (e.g., ?NaB, SodiumButyrate?).
In contrast, such additions are natural in our model.
4 Evaluation
4.1 Abbreviation Extraction Analysis
We evaluated the alignment abbreviation model over
two data sets (Table 4). The method was tuned using
52
Data Set Name Abstracts Abbreviations Testing Method
Development (D) Medstract 400 483 10-fold cross validation.
Validation (V) PubMed Sample 50 76 Training on set D and testing on set V.
Table 4: Evaluation Data Sets.
Model D (average %) V (%)
P R F1 P R F1
Alignment HMM 98 93 96 95 91 93
SH 96 88 91 97 83 89
Chang 0.88 99 46 62 97 47 64
Chang 0.14 94 89 91 95 91 93
Chang 0.03 92 91 91 88 93 90
Chang 0 49 92 64 53 93 67
Table 5: Results on validation (V) and development (D)
sets. Average results are shown for D set, which was
tested using 10-fold cross-validation (results rounded to
nearest percent, all standard deviations were < 0.1)
10 fold cross-validation over the publicly available
Medstract corpus (Pustejovsky et al, 2002) which
includes 400 Medline abstracts. The online version
of the corpus was missing the Gold Standard annota-
tions throughout the development of our algorithm,
nor was it possible to get them through communica-
tion with the authors. We therefore hand-annotated
the Medstract data, yielding 483 abbreviation defi-
nitions in the form of ?short form, long form? pairs.
In order to be consistent with previous evaluations
over Medstract, our annotations include only defini-
tions in which either the short or the long form ap-
pear in parenthesis, and it is assumed that there are
no trailing gaps in the term preceding the parenthe-
sis, although our model does detect such gaps.
We compare our results with two algorithms
available for download: the Schwartz and Hearst
(SH; (2002)) algorithm1, and the Chang et al (2002)
algorithm2 used at three score cutoffs reported in
their paper (0.88, 0.14, 0.03). We also use a fourth
score cutoff of 0 to account for any legal alignments
produced by the Chang model.
In Table 5 we report precision (P), recall (R) and
1Taken from http://biotext.berkeley.edu/software.html
2Taken from http://abbreviation.stanford.edu
F1 scores for all methods, calculated by
P =
correct predicted abbreviations
all predicted abbreviations
(4)
R =
correct predicted abbreviations
all correct abbreviations
(5)
On the development set, our alignment model
achieves 98% precision, 93% recall and 96% F1 (av-
erage values over cross-validation iterations, with
standard deviations all under 0.03).
To test the final model we used a validation
dataset consisting of 50 abstracts, randomly selected
out of a corpus of 200K full-text biomedical articles
taken from the PubMed Central Open Access Sub-
set (extracted in October 2010)3. These were hand-
annotated, yielding 76 abbreviation definitions.
On the validation set, we predicted 69 out of 76
abbreviations, with 4 false predictions, giving 95%
precision, 91% recall and 93% F1. Our alignment
model results in higher F1 score over all baselines
in both datasets (with Chang0.14 giving equal results
on the validation set). Our results are most compa-
rable with the Chang model at a score cutoff of 0.14,
though our model does not require selecting a score
cutoff, and as we will show, it is considerably faster.
Interestingly, our model results in lower recall than
precision on both data sets. This may be due to a
limited scope of edit operations.
In order to evaluate the usability of our method,
we used it to scan the 200K full-text documents of
the PubMed Central Open Access Subset corpus.
The process completed in under 3 hours, yielding
over 1.4 million abbreviations, including 455,844
unique definitions. A random sample of the ex-
tracted abbreviations suggests a low rate of false
positive predictions.
4.2 Error Analysis
Our model makes 4 incorrect predictions on the val-
idation set, 3 of which are partial matches to the
3http://www.ncbi.nlm.nih.gov/pmc/
53
Description D V
Letters in short form are missing (e.g., ?GlyRalpha2, glycine alpha2?) 5 3
Abbreviation missed due to extraction rules. 6 1
Abbreviation is a synonym (e.g., ?IRX-2, natural cytokine mixture?) 5 1
Abbreviation letters are out-of-order (e.g., ?VSV-G, G glycoprotein of vesicular stomatitis virus?) 4 1
Correct alignment was found but it is invalid due to many inner gaps (see Section 3.3.1). 5 0
Abbreviations of chemical formulas or compounds. 4 0
Table 6: Abbreviations missed in development (D) and validation (V) sets.
correct definitions, e.g., we predict the pair ?GlOx,
glutamate oxidase? instead of ?GlOx, L-glutamate
oxidase?. On the development set, 3 out of 5 incor-
rect predictions are partial matches.
Our model did not extract 7 of the abbreviations
from the validation set and 33 from the development
set. Many of these abbreviations (6 from the valida-
tion set and 29 from the development set) had one
of the properties described in Table 6. The remain-
ing 5 definitions have been missed due to miscel-
laneous issues. Note that while we added several
substitution operations for chemical formula recog-
nition, the elaborate set of operations required for
recovering the full range of chemical formulas was
not included in this work, leading to 4 chemical for-
mula abbreviations being missed.
4.3 Run-Time Analysis
We provide an estimated comparison of the run
time of our method and the baseline algorithms.
This analysis is especially interesting for cases in
which an abbreviation extraction model is included
within a larger learning framework (Chowdhury et
al., 2010; Huang et al, 2011), and may be used in
it in an online fashion. Run time was evaluated on
an Apple iMac with 4GB 1333 MHz RAM, and a
3.06 GHz Core i3, double-core processor, by run-
ning all models on a random set of 400 abstracts.
In order to evaluate the run time contribution of the
substitution operations introduced in our model we
ran it both with (88 docssec ) and without (98
docs
sec ) the
use of substitution operations. We find that using
substitutions did not have considerable effect on run
time, adding under 1 ms for processing each docu-
ment. We should note that the performance of the
substitution-less model on this test data was similar
to that of the original model, as substitutions were
relevant to only a smaller portion of the abbrevi-
ations. As expected, the SH algorithm is consid-
erably faster (6451 docssec ) than our model, as it is
based on only a number of simple rules. The Chang
model, however, is slower (4 docssec ) as it includes
processing steps following the discovery of an ab-
breviation alignment, which means that our model
provides comparable results to the Chang model and
runs an order-of-magnitude faster.
5 Conclusions and Discussion
We presented a method for extracting abbreviation
definitions with high precision and high recall (95%
precision, 91% recall and 93% F1 on a validation
set). Our model achieves higher F1 on both the de-
velopment and validation data sets, when compared
with two popular extraction methods.
Our approach is based on a sequential genera-
tive model, aligning the short and long form of an
abbreviation. Using the proposed method we ex-
tracted 1.4 million abbreviations from a corpus of
200K PubMed articles. This data can be valuable
for Information Extraction tasks and for the full un-
derstanding of biomedical scientific data.
The alignment abbreviation extractor can be eas-
ily extended by adding edit-operations over short
and long forms. This was demonstrated by including
substitutions of chemical elements and their sym-
bols, which facilitates recognition of chemical for-
mulas and compounds.
We have identified the main classes of abbrevia-
tion definitions missed by our approach. These in-
clude out-of-order matches, synonym-like abbrevia-
tions, and short forms with excess letters. It may be
possible to address some of these issues by includ-
ing ?global? information on abbreviations, such as
the occurrence of frequent definitions.
54
Acknowledgments
This work was funded by grant 1R101GM081293
from NIH, IIS-0811562 from NSF and by a gift from
Google. The opinions expressed in this paper are
solely those of the authors.
References
E. Adar. 2004. Sarad: A simple and robust abbreviation
dictionary. Bioinformatics, 20(4):527?533.
JL Ambrus. 1987. Acronyms and abbreviations. Journal
of medicine, 18(3-4):134.
H. Ao and T. Takagi. 2005. Alice: an algorithm to extract
abbreviations from medline. Journal of the American
Medical Informatics Association, 12(5):576?586.
M. Bilenko and R.J. Mooney. 2003. Adaptive duplicate
detection using learnable string similarity measures.
In Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 39?48. ACM.
P.R. Bowden, P. Halstead, and T.G. Rose. 1997. Dic-
tionaryless english plural noun singularisation using
a corpus-based list of irregular forms. LANGUAGE
AND COMPUTERS, 20:339?352.
J.T. Chang, H. Schu?tze, and R.B. Altman. 2002. Cre-
ating an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics As-
sociation, 9(6):612?620.
M. Chowdhury, M. Faisal, et al 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90. Association for Computa-
tional Linguistics.
H.L. Fred and T.O. Cheng. 2003. Acronymesis: the
exploding misuse of acronyms. Texas Heart Institute
Journal, 30(4):255.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in med-
line. Bioinformatics, 21(18):3658?3664.
M. Huang, J. Liu, and X. Zhu. 2011. Genetukit: a soft-
ware for document-level gene normalization. Bioin-
formatics, 27(7):1032?1033.
L.S. Larkey, P. Ogilvie, M.A. Price, and B. Tamilio.
2000. Acrophile: an automated acronym extractor and
server. In Proceedings of the fifth ACM conference on
Digital libraries, pages 205?214. ACM.
H. Liu, C. Friedman, et al 2003. Mining terminological
knowledge in large biomedical corpora. In Pac Symp
Biocomput, pages 415?426.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In Conference on Uncer-
tainty in AI (UAI).
Y. Park and R.J. Byrd. 2001. Hybrid text mining for find-
ing abbreviations and their definitions. In Proceedings
of the 2001 conference on empirical methods in natu-
ral language processing, pages 126?133.
J. Pustejovsky, J. Castano, R. Sauri, A. Rumshinsky,
J. Zhang, and W. Luo. 2002. Medstract: creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the ACL-02 workshop
on Natural language processing in the biomedical
domain-Volume 3, pages 85?92. Association for Com-
putational Linguistics.
E.S. Ristad and P.N. Yianilos. 1998. Learning string-edit
distance. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 20(5):522?532.
A.S. Schwartz and M.A. Hearst. 2002. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing 2003: Kauai, Hawaii, 3-7 January 2003, page 451.
World Scientific Pub Co Inc.
K. Taghva and J. Gilbreth. 1999. Recognizing acronyms
and their definitions. International Journal on Docu-
ment Analysis and Recognition, 1(4):191?198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical articles.
Journal of the American Medical Informatics Associa-
tion, 9(3):262?272.
H. Yu, W. Kim, V. Hatzivassiloglou, and W.J. Wilbur.
2007. Using medline as a knowledge source for dis-
ambiguating abbreviations and acronyms in full-text
biomedical journal articles. Journal of biomedical in-
formatics, 40(2):150?159.
W. Zhou, V.I. Torvik, and N.R. Smalheiser. 2006. Adam:
another database of abbreviations in medline. Bioin-
formatics, 22(22):2813.
55
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 155?162,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Evaluating Joint Modeling of Yeast Biology Literature and Protein-Protein
Interaction Networks
Ramnath Balasubramanyan and Kathryn Rivard and William W. Cohen
School of Computer Science
Carnegie Mellon University
rbalasub,krivard,wcohen@cs.cmu.edu
Jelena Jakovljevic and John Woolford
Deparment of Biological Sciences
Carnegie Mellon University
jelena,jw17@andrew.cmu.edu
Abstract
Block-LDA is a topic modeling approach to
perform data fusion between entity-annotated
text documents and graphs with entity-entity
links. We evaluate Block-LDA in the yeast bi-
ology domain by jointly modeling PubMed R?
articles and yeast protein-protein interaction
networks. The topic coherence of the emer-
gent topics and the ability of the model to re-
trieve relevant scientific articles and proteins
related to the topic are compared to that of a
text-only approach that does not make use of
the protein-protein interaction matrix. Eval-
uation of the results by biologists show that
the joint modeling results in better topic co-
herence and improves retrieval performance in
the task of identifying top related papers and
proteins.
1 Introduction
The prodigious rate at which scientific literature
is produced makes it virtually impossible for re-
searchers to manually read every article to identify
interesting and relevant papers. It is therefore crit-
ical to have automatic methods to analyze the liter-
ature to identify topical structure in it. The latent
structure that is identified can be used for different
applications such as enabling browsing, retrieval of
papers related to a particular sub-topic etc. Such ap-
plications assist in common scenarios such as help-
ing a researcher identify a set of articles to read (per-
haps a set of well-regarded surveys) to familiarize
herself with a new sub-field; helping a researcher to
stay abreast with the latest advances in his field by
identifying relevant articles etc.
In this paper, we focus on the task of organiz-
ing a large collection of literature about yeast biol-
ogy to enable topic oriented browsing and retrieval
from the literature. The analysis is performed using
topic modeling(Blei et al, 2003) which has, in the
last decade, emerged as a versatile tool to uncover
latent structure in document corpora by identifying
broad topics that are discussed in it. This approach
complements traditional information retrieval tasks
where the objective is to fulfill very specific infor-
mation needs.
In addition to literature, there often exist other
sources of domain information related to it. In the
case of yeast biology, an example of such a resource
is a database of known protein-protein interactions
(PPI) which have been identified using wetlab exper-
iments. We perform data fusion by combining text
information from articles and the database of yeast
protein-protein interactions, by using a latent vari-
able model ? Block-LDA (Balasubramanyan and
Cohen, 2011) that jointly models the literature and
PPI networks.
We evaluate the ability of the topic models to re-
turn meaningful topics by inspecting the top papers
and proteins that pertain to them. We compare the
performance of the joint model i.e. Block-LDA with
a model that only considers the text corpora by ask-
ing a yeast biologist to evaluate the coherence of
topics and the relevance of the retrieved articles and
proteins. This evaluation serves to test the utility of
Block-LDA on a real task as opposed to an internal
evaluation (such as by using perplexity metrics for
example). Our evaluaton shows that the joint model
outperforms the text-only approach both in topic co-
155
herence and in top paper and protein retrieval as
measured by precision@10 values.
The rest of the paper is organized as follows. Sec-
tion 2 describes the topic modeling approach used
in the paper. Section 3 describes the datasets used
followed by Section 4 which details the setup of the
experiments. The results of the evaluation are pre-
sented in Section 5 which is followed by the conclu-
sion.
2 Block-LDA
The Block-LDA model (plate diagram in Figure 1)
enables sharing of information between the compo-
nent on the left that models links between pairs of
entities represented as edges in a graph with latent
block structure, and the component on the right that
models text documents, through shared latent topics.
More specifically, the distribution over the entities of
the type that are linked is shared between the block
model and the text model.
The component on the right, which is an extension
of the LDA models documents as sets of ?bags of en-
tities?, each bag corresponding to a particular type
of entity. Every entity type has a topic wise multi-
nomial distribution over the set of entities that can
occur as an instance of the entity type. This model
is termed as Link-LDA(Nallapati et al, 2008) in the
literature.
The component on the left in the figure is a gen-
erative model for graphs representing entity-entity
links with an underlying block structure, derived
from the sparse block model introduced by Parkki-
nen et al (2009). Linked entities are generated from
topic specific entity distributions conditioned on the
topic pairs sampled for the edges. Topic pairs for
edges (links) are drawn from a multinomial defined
over the Cartesian product of the topic set with it-
self. Vertices in the graph representing entities there-
fore have mixed memberships in topics. In con-
trast to Mixed-membership Stochastic Blockmodel
(MMSB) introduced by Airoldi et al (2008), only
observed links are sampled, making this model suit-
able for sparse graphs.
LetK be the number of latent topics (clusters) we
wish to recover. Assuming documents consist of T
different types of entities (i.e. each document con-
tains T bags of entities), and that links in the graph
are between entities of type tl, the generative process
is as follows.
1. Generate topics: For each type t ? 1, . . . , T , and
topic z ? 1, . . . ,K, sample ?t,z ? Dirichlet(?), the
topic specific entity distribution.
2. Generate documents. For every document d ?
{1 . . . D}:
? Sample ?d ? Dirichlet(?D) where ?d is the
topic mixing distribution for the document.
? For each type t and its associated set of entity
mentions et,i, i ? {1, ? ? ? , Nd,t}:
? Sample a topic zt,i ?Multinomial(?d)
? Sample an entity et,i ?
Multinomial(?t,zt,i)
3. Generate the link matrix of entities of type tl:
? Sample piL ? Dirichlet(?L) where piL de-
scribes a distribution over the Cartesian prod-
uct of the set of topics with itself, for links in
the dataset.
? For every link ei1 ? ei2, i ? {1 ? ? ?NL}:
? Sample a topic pair ?zi1, zi2? ?
Multinomial(piL)
? Sample ei1 ?Multinomial(?tl,zi1)
? Sample ei2 ?Multinomial(?tl,zi2)
Note that unlike the MMSB model, this model
generates only realized links between entities.
Given the hyperparameters ?D, ?L and ?, the
joint distribution over the documents, links, their
topic distributions and topic assignments is given by
p(piL,?,?, z, e, ?z1, z2?, ?e1, e2?|?D, ?L, ?) ?
(1)
K?
z=1
T?
t=1
Dir(?t,z|?t)?
D?
d=1
Dir(?d|?D)
T?
t=1
Nd,t?
i=1
?
z(d)t,i
d ?
et,i
t,z(d)t,i
?
Dir(piL|?L)
NL?
i=1
pi?zi1,zi2?L ?
ei1
tl,z1
?ei2tl,z2
156
...
?d
?L
?D
pi L
N L
?
Dim: K x K
Dim: K
z i 1 z i 2
e i 2e i 1
Links
Docs
?t,z
T
K
D
z 1,i
e 1,i
z T ,i
e T ,i
Nd,TNd, 1
?L - Dirichlet prior for the topic pair distribution for links
?D - Dirichlet prior for document specific topic distributions
? - Dirichlet prior for topic multinomials
piL - multinomial distribution over topic pairs for links
?d - multinomial distribution over topics for document d
?t,z - multinomial over entities of type t for topic z
zt,i - topic chosen for the i-th entity of type t in a document
et,i - the i-th entity of type t occurring in a document
zi1 and zi2 - topics chosen for the two nodes participating in the i-th link
ei1 and ei2 - the two nodes participating in the i-th link
Figure 1: Block-LDA
A commonly required operation when using mod-
els like Block-LDA is to perform inference on the
model to query the topic distributions and the topic
assignments of documents and links. Due to the
intractability of exact inference in the Block-LDA
model, a collapsed Gibbs sampler is used to perform
approximate inference. It samples a latent topic for
an entity mention of type t in the text corpus con-
ditioned on the assignments to all other entity men-
tions using the following expression (after collaps-
ing ?D):
p(zt,i = z|et,i, z?i, e?i, ?D, ?) (2)
? (n?idz + ?D)
n?iztet,i + ?
?
e? n
?i
zte?
+ |Et|?
Similarly, we sample a topic pair for every link con-
ditional on topic pair assignments to all other links
after collapsing piL using the expression:
p(zi = ?z1, z2?|?ei1, ei2?, z?i, ?e1, e2??i, ?L, ?)(3)
?
(
nL?i?z1,z2? + ?L
)
?
(
n?iz1tlei1+?
)(
n?iz2tlei2+?
)
(?
e n
?i
z1tle
+|Etl |?
)(?
e n
?i
z2tle
+|Etl |?
)
Et refers to the set of all entities of type t. The n?s
are counts of observations in the training set.
? nzte - the number of times an entity e of type t
is observed under topic z
? nzd - the number of entities (of any type) with
topic z in document d
? nL?z1,z2? - count of links assigned to topic pair
?z1, z2?
The topic multinomial parameters and the topic
distributions of links and documents are easily re-
covered using their MAP estimates after inference
157
using the counts of observations.
?(e)t,z =
nzte + ?
?
e? nzte? + |Et|?
, (4)
?(z)d =
ndz + ?D
?
z? ndz? +K?D
and (5)
pi?z1,z2?L =
n?z1,z2? + ?L?
z?1,z
?
2
n?z?1,z?2? +K
2?L
(6)
A de-noised form of the entity-entity link matrix
can also be recovered from the estimated parame-
ters of the model. Let B be a matrix of dimensions
K ? |Etl | where row k = ?tl,k, k ? {1, ? ? ? ,K}.
Let Z be a matrix of dimensions K ?K s.t Zp,q =
?NL
i=1 I(zi1 = p, zi2 = q). The de-noised matrix M
of the strength of association between the entities in
Etl is given by M = B
TZB.
In the context of this paper, de-noising the
protein-protein interaction networks studied is an
important application. The joint model permits in-
formation from the large text corpus of yeast publi-
cations to be used to de-noise the PPI network and
to identify potential interactions that are missing in
the observed network. While this task is important
and interesting, it is outside the scope of this paper
and is a direction for future work.
3 Data
We use a collection of publications about yeast bi-
ology that is derived from the repository of sci-
entific publications at PubMed R?. PubMed R? is a
free, open-access on-line archive of over 18 mil-
lion biological abstracts and bibliographies, includ-
ing citation lists, for papers published since 1948.
The subset we work with consists of approximately
40,000 publications about the yeast organism that
have been curated in the Saccharomyces Genome
Database (SGD) (Dwight et al, 2004) with anno-
tations of proteins that are discussed in the publi-
cation. We further restrict the dataset to only those
documents that are annotated with at least one pro-
tein from the protein-protein interactions databases
described below. This results in a protein annotated
document collection of 15,776 publications. The
publications in this set were written by a total of
47,215 authors. We tokenize the titles and abstracts
based on white space, lowercase all tokens and elim-
inate stopwords. Low frequency (< 5 occurrences)
terms are also eliminated. The vocabulary that is ob-
tained consists of 45,648 words.
The Munich Institute for Protein Sequencing
(MIPS) database (Mewes et al, 2004) includes a
hand-crafted collection of protein interactions cover-
ing 8000 protein complex associations in yeast. We
use a subset of this collection containing 844 pro-
teins, for which all interactions were hand-curated.
Finally, we use another dataset of protein-protein
interactions in yeast that were observed as a result of
wetlab experiments by collaborators of the authors
of the paper. This dataset consists of 635 interac-
tions that deal primarily with ribosomal proteins and
assembly factors in yeast.
4 Setup
We conduct three different evaluations of the emer-
gent topics. Firstly, we obtain topics from only
the text corpus using a model that comprises of the
right half of Figure 1 which is equivalent to using
the Link-LDA model. For the second evaluation,
we use the Block-LDA model that is trained on the
text corpus and the MIPS protein-protein interac-
tion database. Finally, for the third evaluation, we
replace the MIPS database with the interaction ob-
tained from the wetlab experiments. In all the cases,
we set K, the number of topics to be 15. In each
variant, we represent documents as 3 sets of entities
i.e. the words in the abstracts of the article, the set
of proteins associated with the article as indicated in
the SGD database and finally the authors who wrote
the article. Each topic therefore consists of 3 differ-
ent multinomial distributions over the sets of the 3
kinds of entities described.
Topics that emerge from the different variants can
possibly be assigned different indices even when
they discuss the same semantic concept. To com-
pare topics across variants, we need a method to
determine which topic indices from the different
variants correspond to the same semantic concept.
To obtain the mapping between topics from each
variant, we utilize the Hungarian algorithm (Kuhn,
1955) to solve the assignment problem where the
cost of aligning topics together is determined using
the Jensen-Shannon divergence measure.
Once the topics are obtained, we firstly obtain the
proteins associated with the topic by retrieving the
158
Figure 2: Screenshot of the Article Relevance Annotation Tool
Variant Num. Coherent Topics
Only Text 12 / 15
Text + MIPS 13 / 15
Text + Wetlab 15 / 15
Table 1: Topic Coherence Evaluation
top proteins from the multinomial distribution cor-
responding to proteins. Then, the top articles cor-
responding to each topic is obtained using a ranked
list of documents with the highest mass of their topic
proportion distributions (?) residing in the topic be-
ing considered.
4.1 Manual Evaluation
To evaluate the topics, a yeast biologist who is an
expert in the field was asked to mark each topic with
a binary flag indicating if the top words of the dis-
tribution represented a coherent sub-topic in yeast
biology. This process was repeated for the 3 differ-
ent variants of the model. The variant used to obtain
results is concealed from the evaluator to remove the
possibility of bias. In the next step of the evaluation,
the top articles and proteins assigned to each topic
were presented in a ranked list and a similar judge-
ment was requested to indicate if the article/protein
was relevant to the topic in question. Similar to
the topic coherence judgements, the process was re-
peated for each variant of the model. Screenshots
of the tool used for obtaining the judgments can be
seen in Figure 2. It should be noted that since the
nature of the topics in the literature considered was
highly technical and specialized, it was impractical
to get judgements from multiple annotators.
159
Topic
Pr
ec
isi
on
 @
 10
0.2
0.4
0.6
0.8
1.0 l l l l
l
l l l l l l l
Variant
l With MIPS interactions
Only Text
With Wetlab interactions
(a) Article Retrieval
Topic
Pr
ec
isi
on
 @
 10
0.2
0.4
0.6
0.8
1.0 l
l
l
l l l l l l
Variant
l With MIPS interactions
Only Text
With Wetlab interactions
(b) Protein Retrieval
Figure 3: Retrieval Performance Evaluation (Horizontal lines indicate mean across all topics)
To evaluate the retrieval of the top articles and
proteins, we measure the quality of the results by
computing its precision@10 score.
5 Results
First we evaluate the coherence of the topics ob-
tained from the 3 variants described above. Table
1 shows that out of the 15 topics that were obtained,
12 topics were deemed coherent from the text-only
model and 13 and 15 topics were deemed coherent
from the Block-LDA models using the MIPS and
wetlab PPI datasets respectively.
Next, we study the precision@10 values for each
topic and variant for the article retrieval and protein
retrieval tasks, which is shown in Figure 3. The plots
also show horizontal lines representing the mean of
the precision@10 across all topics. It can be seen
from the plots that for both the article and protein
retrieval tasks, the joint models work better than the
text-only model on average. For the article retrieval
task, the model trained with the text + MIPS resulted
in the higher mean precision@10 whereas for the
protein retrieval task, the text + Wetlab PPI dataset
returned a higher mean precision@10 value. For
both the protein retrieval and paper retrieval tasks,
the improvements shown by the joint models using
either of the PPI datasets over the text-only model
(i.e. the Link LDA model) were statistically sig-
nificant at the 0.05 level using the paired Wilcoxon
sign test. The difference in performance between the
160
Topic: Protein Structure & Interactions
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* X-ray fiber diffraction of amyloid fibrils. * X-ray fiber diffraction of amyloid fibrils.
* Molecular surface area and hydrophobic effect. * Scalar couplings across hydrogen bonds.
* Counterdiffusion methods for macromolecular
crystallization.
* Dipolar couplings in macromolecular structure
determination.
* Navigating the ClpB channel to solution. * Structure of alpha-keratin.
* Two Rippled-Sheet Configurations of Polypep-
tide Chains, and a Note about the Pleated Sheets.
* Stable configurations of polypeptide chains.
* Molecular chaperones. Unfolding protein fold-
ing.
* The glucamylase and debrancher of S. diastati-
cus.
* The molten globule state as a clue for under-
standing the folding and cooperativity of globular-
protein structure.
* A study of 150 cases of pneumonia.
* Unfolding and hydrogen exchange of proteins:
the three-dimensional ising lattice as a model.
* Glycobiology.
* Packing of alpha-helices: geometrical con-
straints and contact areas.
* The conformation of thermolysin.
Topic: DNA Repair
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* Passing the baton in base excision repair. * Telomeres and telomerase.
* The bypass of DNA lesions by DNA and RNA
polymerases.
* Enzymatic photoreactivation: overview.
* The glucamylase and debrancher of S. diastati-
cus.
* High-efficiency transformation of plasmid DNA
into yeast.
* DNA replication fidelity. * The effect of ultraviolet light on recombination
in yeast.
* Base excision repair. * T-loops and the origin of telomeres.
* Nucleotide excision repair. * Directed mutation: between unicorns and goats.
* The replication of DNA in Escherichia Coli. * Functions of DNA polymerases.
* DNA topoisomerases: why so many? * Immortal strands? Give me a break.
Table 2: Sample of Improvements in Article Retrieval
two joint models that used the two different PPI net-
works were however insignificant which indicates
that there is no observable advantage in using one
PPI dataset over the other in conjunction with the
text corpus.
Table 2 shows examples of poor results of article
retrieval obtained using the publications-only model
and the improved set of results obtained using the
joint model.
5.1 Topics
Table 3 shows 3 sample topics that were retrieved
from each variant described earlier. The table shows
the top words and proteins associated with the top-
ics. The topic label on the left column was assigned
manually during the evaluation by the expert anno-
tator.
Conclusion
We evaluated topics obtained from the joint mod-
eling of yeast biology literature and protein-protein
interactions in yeast and compared them to top-
ics that were obtained from using only the litera-
ture. The topics were evaluated for coherence and
by measuring the mean precision@10 score of the
top articles and proteins that were retrieved for each
topic. Evaluation by a domain expert showed that
161
Topic Top Words & Proteins
Protein Structure & Inter-
actions
Words: protein structure binding residues domain structural beta complex
atp proteins alpha interactions folding structures form terminal peptide helix
model interaction bound domains molecular changes conformational
(Publications Only) Proteins: CYC1 SSA1 HSP82 SUP35 HSP104 HSC82 SSA2 YDJ1 URE2
KAR2 SSB1 SSA4 GCN4 SSA3 SSB2 PGK1 PDI1 SSC1 HSP60 STI1
SIS1 RNQ1 SEC61 SSE1 CCP1
DNA Repair Words:dna recombination repair replication strand single double cells mu-
tations stranded induced base uv mutants mutation homologous virus telom-
ere human type yeast activity telomerase mutant dna polymerase
(Using MIPS PPI) Proteins: RAD52 RAD51 RAD50 MRE11 RAD1 RAD54 SGS1 MSH2
RAD6 YKU70 REV3 POL30 RAD3 XRS2 RAD18 RAD2 POL3 RAD27
YKU80 RAD9 RFA1 TLC1 TEL1 EST2 HO
Vesicular Transport Words:membrane protein transport proteins atp golgi er atpase membranes
plasma membrane vesicles cells endoplasmic reticulum complex fusion
ca2 dependent translocation vacuolar intracellular yeast lipid channel hsp90
vesicle
(Using Wetlab PPI) Proteins: SSA1 HSP82 KAR2 PMA1 HSC82 SEC18 SSA2 YDJ1 SEC61
PEP4 HSP104 SEC23 VAM3 IRE1 SEC4 SSA4 SEC1 PMR1 PEP12
VMA3 VPH1 SSB1 VMA1 SAR1 HAC1
Table 3: Sample Topics
the joint modeling produced more coherent topics
and showed better precision@10 scores in the article
and protein retrieval tasks indicating that the model
enabled information sharing between the literature
and the PPI networks.
References
Edoardo M. Airoldi, David Blei, Stephen E. Fienberg,
and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. Journal of Machine Learning Research,
9:1981?2014, September.
Ramnath Balasubramanyan and William W. Cohen.
2011. Block-LDA: Jointly modeling entity-annotated
text and entity-entity links. In SDM, pages 450?461.
SIAM / Omnipress.
David. M Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Selina S. Dwight, Rama Balakrishnan, Karen R.
Christie, Maria C. Costanzo, Kara Dolinski, Sta-
cia R. Engel, Becket Feierbach, Dianna G. Fisk,
Jodi Hirschman, Eurie L. Hong, Laurie Issel-Tarver,
Robert S. Nash, Anand Sethuraman, Barry Starr,
Chandra L. Theesfeld, Rey Andrada, Gail Binkley,
Qing Dong, Christopher Lane, Mark Schroeder, Shuai
Weng, David Botstein, and Michael Cherry J. 2004.
Saccharomyces genome database: Underlying prin-
ciples and organisation. Briefings in bioinformatics,
5(1):9.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(1-2):83?97.
Hans-Werner Mewes, C. Amid, Roland Arnold, Dmitrij
Frishman, Ulrich Gldener, Gertrud Mannhaupt, Martin
Mnsterktter, Philipp Pagel, Normann Strack, Volker
Stmpflen, Jens Warfsmann, and Andreas Ruepp. 2004.
MIPS: Analysis and annotation of proteins from whole
genomes. Nucleic Acids Res, 32:41?44.
Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and
William W. Cohen. 2008. Joint latent topic models
for text and citations. In Proceeding of the 14th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 542?550, Las Vegas,
Nevada, USA. ACM.
Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, and
Samuel Kaski. 2009. A block model suitable for
sparse graphs. In Proceedings of the 7th International
Workshop on Mining and Learning with Graphs (MLG
2009), Leuven. Poster.
162
Proceedings of the TextGraphs-7 Workshop at ACL, pages 20?24,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Graph Based Similarity Measures for Synonym Extraction from Parsed Text
Einat Minkov
Dep. of Information Systems
University of Haifa
Haifa 31905, Israel
einatm@is.haifa.ac.il
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Abstract
We learn graph-based similarity measures for
the task of extracting word synonyms from a
corpus of parsed text. A constrained graph
walk variant that has been successfully ap-
plied in the past in similar settings is shown to
outperform a state-of-the-art syntactic vector-
based approach on this task. Further, we show
that learning specialized similarity measures
for different word types is advantageous.
1 Introduction
Many applications of natural language processing
require measures of lexico-semantic similarity. Ex-
amples include summarization (Barzilay and El-
hadad, 1999), question answering (Lin and Pantel,
2001), and textual entailment (Mirkin et al, 2006).
Graph-based methods have been successfully ap-
plied to evaluate word similarity using available on-
tologies, where the underlying graph included word
senses and semantic relationships between them
(Hughes and Ramage, 2007). Another line of re-
search aims at eliciting semantic similarity measures
directly from freely available corpora, based on the
distributional similarity assumption (Harria, 1968).
In this domain, vector-space methods give state-of-
the-art performance (Pado? and Lapata, 2007).
Previously, a graph based framework has been
proposed that models word semantic similarity from
parsed text (Minkov and Cohen, 2008). The un-
derlying graph in this case describes a text cor-
pus as connected dependency structures, accord-
ing to the schema shown in Figure 1. The toy
graph shown includes the dependency analysis of
two sentences: ?a major environmental disaster is
Figure 1: A joint graph of dependency structures
under way?, and ?combat the environmental catas-
trophe?. In the graph, word mentions (in circles)
and word types (in squares) are both represented
as nodes. Each word mention is linked to its
corresponding word type; for example, the nodes
?environmental3? and ?environmental204? represent
distinct word mentions and both nodes are linked
to the word type ?environmental?.1 For every edge
in the graph, there exists an edge in the oppo-
site direction (not shown in the figure). In this
graph, the terms disaster and catastrophe are re-
lated due to the connecting path disaster ?? disaster3
amod?inverse?? environmental3 ?? environmental ??
environmental204 amod?? catastrophe204 ?? catastrophe .
Given a query, which consists of a word of inter-
est (e.g., ?disaster?), various graph-based similarity
metrics can be used to assess inter-node relatedness,
so that a list of nodes ranked by their similarity to
the query is returned to the user. An advantage of
graph-based similarity approaches is that they pro-
duce similarity scores that reflect structural infor-
1We will sometimes refer to word types as terms.
20
mation in the graph (Liben-Nowell and Kleinberg,
2003). Semantically similar terms are expected to
share connectivity patterns with the query term in
the graph, and thus appear at the top of the list.
Notably, different edge types, as well as the paths
traversed, may have varying importance for differ-
ent types of similarity sought. For example, in the
parsed text domain, noun similarity and verb sim-
ilarity are associated with different syntactic phe-
nomena (Resnik and Diab, 2000). To this end, we
consider a path constrained graph walk (PCW) al-
gorithm, which allows one to learn meaningful paths
given a small number of labeled examples and incor-
porates this information in assessing node related-
ness in the graph (Minkov and Cohen, 2008). PCW
have been successfully applied to the extraction of
named entity coordinate terms, including city and
person names, from graphs representing newswire
text (Minkov and Cohen, 2008), where the special-
ized measures learned outperformed the state-of-
the-art dependency vectors method (Pado? and Lap-
ata, 2007) for small- and medium-sized corpora.
In this work, we apply the path constrained graph
walk method to the task of eliciting general word
relatedness from parsed text, conducting a set of ex-
periments on the task of synonym extraction. While
the tasks of named entity extraction and synonym
extraction from text have been treated separately in
the literature, this work shows that both tasks can be
addressed using the same general framework. Our
results are encouraging: the PCW model yields su-
perior results to the dependency vectors approach.
Further, we show that learning specialized similar-
ity measures per word type (nouns, verbs and adjec-
tives) is preferable to applying a uniform model for
all word types.
2 Path Constrained Graph Walks
PCW is a graph walk variant proposed recently that
is intended to bias the random walk process to fol-
low meaningful edge sequences (paths) (Minkov
and Cohen, 2008). In this approach, rather than as-
sume fixed (possibly, uniform) edge weight param-
eters ? for the various edge types in the graph, the
probability of following an edge of type ? from node
x is evaluated dynamically, based on the history of
the walk up to x.
The PCW algorithm includes two components.
First, it should provide estimates of edge weights
conditioned on the history of a walk, based on train-
ing examples. Second, the random walk algorithm
has to be modified to maintain historical information
about the walk compactly.
In learning, a dataset of N labelled example
queries is provided. The labeling schema is binary,
where a set of nodes considered as relevant answers
to an example query ei, denoted as Ri, is specified,
and graph nodes that are not explicitly included in
Ri are assumed irrelevant to ei. As a starting point,
an initial graph walk is applied to generate a ranked
list of graph nodes li for every example query ei. A
path-tree T is then constructed that includes all of
the acyclic paths up to length k leading to the top
M+ correct and M? incorrect nodes in each of the
retrieved lists li. Every path p is associated with
a maximum likelihood probability estimate Pr(p)
of reaching a correct node based on the number of
times the path was observed in the set of correct and
incorrect target nodes. These path probabilities are
propagated backwards in the path tree to reflect the
probability of reaching a correct node, given an out-
going edge type and partial history of the walk.
Given a new query, a constrained graph walk vari-
ant is applied that adheres both to the topology of the
graph G and the path tree T . In addition to tracking
the graph node that the random walker is at, PCW
maintains pointers to the nodes of the path tree that
represent the walk histories in reaching that graph
node. In order to reduce working memory require-
ments, one may prune paths that are associated with
low probability of reaching a correct node. This of-
ten leads to gains in accuracy.
3 Synonym Extraction
We learn general word semantic similarity measures
from a graph that represents a corpus of parsed text
(Figure 1). In particular, we will focus on evalu-
ating word synonymy, learning specialized models
for different word types. In the experiments, we
mainly compare PCW against the dependency vec-
tors model (DV), due to Pado? and Lapata (2007).
In the latter approach, a word wi is represented
as a vector of weighted scores, which reflect co-
occurrence frequency with words wj , as well as
21
properties of the dependency paths that connect the
word wi to word wj . In particular, higher weight
is assigned to connecting paths that include gram-
matically salient relations, based on the obliqueness
weighting hierarchy (Keenan and Comrie, 1977).
For example, co-occurrence of word wi with word
wj over a path that includes the salient subject rela-
tion receives higher credit than co-occurrences over
a non-salient relation such as preposition. In addi-
tion, Pado? and Lapata suggest to consider only a
subset of the paths observed that are linguistically
meaningful. While the two methods incorporate
similar intuitions, PCW learns meaningful paths that
connect the query and target terms from examples,
whereas DV involves manual choices that are task-
independent.
3.1 Dataset
To allow effective learning, we constructed a dataset
that represents strict word synonymy relations for
multiple word types. The dataset consists of 68 ex-
amples, where each example query consists of a sin-
gle term of interest, with its synonym defined as a
single correct answer. The dataset includes noun
synonym pairs (22 examples), adjectives (24) and
verbs (22). Example synonym pairs are shown in
Table 1. A corpus of parsed text was constructed
using the British National Corpus (Burnard, 1995).
The full BNC corpus is a 100-million word col-
lection of samples of written and spoken contem-
porary British English texts. We extracted rele-
vant sentences, which contained the synonymous
words, from the BNC corpus. (The number of ex-
tracted sentences was limited to 2,000 per word.)
For infrequent words, we extracted additional ex-
ample sentences from Associated Press (AP) arti-
cles included in the AQUAINT corpus (Bilotti et al,
2007). (Sentence count was complemented to 300
per word, where applicable.) The constructed cor-
pus, BNC+AP, includes 1.3 million words overall.
This corpus was parsed using the Stanford depen-
dency parser (de Marneffe et al, 2006).2. The parsed
corpus corresponds to a graph that includes about
0.5M nodes and 1.7M edges.
2http://nlp.stanford.edu/software/lex-parser.shtml
Nouns movie : film
murderer : assassin
Verbs answered : replied
enquire : investigate
Adjectives contemporary : modern
infrequent : rare
Table 1: Example word synonym pairs: the left words are
used as the query terms.
3.2 Experiments
Given a query like {term=?movie?}, we would like
to get synonymous words, such as film, to appear
at the top of the retrieved list. In our experimental
setting, we assume that the word type of the query
term is known. Rather than rank all words (terms) in
response to a query, we use available (noisy) part of
speech information to narrow down the search to the
terms of the same type as the query term, e.g. for the
query ?film? we retrieve nodes of type ? =noun.
We applied the PCW method to learn separate
models for noun, verb and adjective queries. The
path trees were constructed using the paths leading
to the node known to be a correct answer, as well
as to the otherwise irrelevant top-ranked 10 terms.
We required the paths considered by PCW to in-
clude exactly 6 segments (edges). Such paths rep-
resent distributional similarity phenomena, allowing
a direct comparison against the DV method. In con-
ducting the constrained walk, we applied a thresh-
old of 0.5 to truncate paths associated with lower
probability of reaching a relevant response, follow-
ing on previous work (Minkov and Cohen, 2008).
We implemented DV using code made available by
its authors,3 where we converted the syntactic pat-
terns specified to Stanford dependency parser con-
ventions. The parameters of the DV method were
set to medium context and oblique edge weighting
scheme, which were found to perform best (Pado?
and Lapata, 2007). In applying a vector-space based
method, a similarity score needs to be computed be-
tween every candidate from the corpus and the query
term to construct a ranked list. In practice, we used
the union of the top 300 words retrieved by PCW as
candidate terms for DV.
We evaluate the following variants of DV: hav-
3http://www.coli.uni-saarland.de/? pado/dv.html
22
Nouns Verbs Adjs All
CO-Lin 0.34 0.37 0.37 0.37
DV-Cos 0.24 0.36 0.26 0.29
DV-Lin 0.45 0.49 0.54 0.50
PCW 0.47 0.55 0.47 0.49
PCW-P 0.53 0.68 0.55 0.59
PCW-P-U 0.49 0.65 0.50 0.54
Table 2: 5-fold cross validation results: MAP
ing inter-word similarity computed using Lin?s mea-
sure (Lin, 1998) (DV-Lin), or using cosine similarity
(DV-Cos). In addition, we consider a non-syntactic
variant, where a word?s vector consists of its co-
occurrence counts with other terms (using a win-
dow of two words); that is, ignoring the dependency
structure (CO-Lin).
Finally, in addition to the PCW model described
above (PCW), we evaluate the PCW approach in set-
tings where random, noisy, edges have been elimi-
nated from the underlying graph. Specifically, de-
pendency links in the graph may be associated with
pointwise mutual information (PMI) scores of the
linked word mention pairs (Manning and Schu?tze,
1999); edges with low scores are assumed to rep-
resent word co-occurrences of low significance, and
so are removed. We empirically set the PMI score
threshold to 2.0, using cross validation (PCW-P).4
In addition to the specialized PCW models, we also
learned a uniform model over all word types in these
settings; that is, this model is trained using the union
of all training examples, being learned and tested us-
ing a mixture of queries of all types (PCW-P-U).
3.3 Results
Table 2 gives the results of 5-fold cross-validation
experiments in terms of mean average precision
(MAP). Since there is a single correct answer per
query, these results correspond to the mean recipro-
cal rank (MRR).5 As shown, the dependency vec-
tors model applied using Lin similarity (DV-Lin)
performs best among the vector-based models. The
improvement achieved due to edge weighting com-
4Eliminating low PMI co-occurrences has been shown to be
beneficial in modeling lexical selectional preferences recently,
using a similar threshold value (Thater et al, 2010).
5The query?s word inflections and words that are seman-
tically related but not synonymous were discarded from the
ranked list manually for evaluation purposes.
pared with the co-occurrence model (CO-Lin) is
large, demonstrating that syntactic structure is very
informative for modeling word semantics (Pado? and
Lapata, 2007). Interestingly, the impact of applying
the Lin similarity measure versus cosine (DV-Cos)
is even more profound. Unlike the cosine measure,
Lin?s metric was designed for the task of evaluating
word similarity from corpus statistics; it is based on
the mutual information measure, and allows one to
downweight random word co-occurrences.
Among the PCW variants, the specialized PCW
models achieve performance that is comparable to
the state-of-the-art DV measure (DV-Lin). Further,
removing noisy word co-occurrences from the graph
(PCW-P) leads to further improvements, yielding
the best results over all word types. Finally, the
graph walk model that was trained uniformly for all
word types (PCW-P-U) outperforms DV-Lin, show-
ing the advantage of learning meaningful paths. No-
tably, the uniformly trained model is inferior to
PCW trained separately per word type in the same
settings (PCW-P). This suggests that learning spe-
cialized word similarity metrics is beneficial.
4 Discussion
We applied a path constrained graph walk variant to
the task of extracting word synonyms from parsed
text. In the past, this graph walk method has been
shown to perform well on a related task, of extract-
ing named entity coordinate terms from text. While
the two tasks are typically treated distinctly, we have
shown that they can be addressed using the same
framework. Our results on a medium-sized cor-
pus were shown to exceed the performance of de-
pendency vectors, a syntactic state-of-the-art vector-
space method. Compared to DV, the graph walk ap-
proach considers higher-level information about the
connecting paths between word pairs, and are adap-
tive to the task at hand. In particular, we showed that
learning specialized graph walk models for different
word types is advantageous. The described frame-
work can be applied towards learning other flavors
of specialized word relatedness models (e.g., hyper-
nymy). Future research directions include learning
word similarity measures from graphs that integrate
corpus statistics with word ontologies, as well as im-
proved scalability (Lao and Cohen, 2010).
23
References
Regina Barzilay and Michael Elhadad. 1999. Text sum-
marizations with lexical chains, in Inderjeet Mani and
Mark Maybury, editors, Advances in Automatic Text
Summarization. MIT.
Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric
Nyberg. 2007. Structured retrieval for question an-
swering. In SIGIR.
Lou Burnard. 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service, Oxford, UK.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Zellig Harria. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In EMNLP.
Edward Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and universal grammar. Linguis-
tic Inquiry, 8.
Ni Lao and William W. Cohen. 2010. Fast query exe-
cution for retrieval models based on path constrained
random walks. In KDD.
Liben-Nowell and J. Kleinberg. 2003. The link predic-
tion problem for social networks. In CIKM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4).
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Chris Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
In EMNLP.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In ACL.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In Proceedings of the Annual Meeting of
the Cognitive Science Society.
Stefan Thater, Hagen F??urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In ACL.
24
