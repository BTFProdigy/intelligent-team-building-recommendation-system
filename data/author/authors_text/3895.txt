Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 176?183,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Parsing and Generation as Datalog Queries
Makoto Kanazawa
National Institute of Informatics
2?1?2 Hitotsubashi, Chiyoda-ku, Tokyo, 101?8430, Japan
kanazawa@nii.ac.jp
Abstract
We show that the problems of parsing and sur-
face realization for grammar formalisms with
?context-free? derivations, coupled with Mon-
tague semantics (under a certain restriction) can
be reduced in a uniform way to Datalog query
evaluation. As well as giving a polynomial-
time algorithm for computing all derivation trees
(in the form of a shared forest) from an in-
put string or input logical form, this reduction
has the following complexity-theoretic conse-
quences for all such formalisms: (i) the de-
cision problem of recognizing grammaticality
(surface realizability) of an input string (logical
form) is in LOGCFL; and (ii) the search prob-
lem of finding one logical form (surface string)
from an input string (logical form) is in func-
tional LOGCFL. Moreover, the generalized sup-
plementary magic-sets rewriting of the Datalog
program resulting from the reduction yields ef-
ficient Earley-style algorithms for both parsing
and generation.
1 Introduction
The representation of context-free grammars (aug-
mented with features) in terms of definite clause pro-
grams is well-known. In the case of a bare-bone
CFG, the corresponding program is in the function-
free subset of logic programming, known as Dat-
alog. For example, determining whether a string
John found a unicorn belongs to the language of the
CFG in Figure 1 is equivalent to deciding whether
the Datalog program in Figure 2 together with the
database in (1) can derive the query ???S(0, 4).?
(1) John(0, 1). found(1, 2). a(2, 3). unicorn(3, 4).
S ? NP VP
VP ? V NP
V ? V Conj V
NP ? Det N
NP ? John
V ? found
V ? caught
Conj ? and
Det ? a
N ? unicorn
Figure 1: A CFG.
S(i, j) :? NP(i, k),VP(k, j).
VP(i, j) :? V(i, k),NP(k, j).
V(i, j) :? V(i, k),Conj(k, l),V(l, j).
NP(i, j) :? Det(i, k),N(k, j).
NP(i, j) :? John(i, j).
V(i, j) :? found(i, j).
V(i, j) :? caught(i, j).
Conj(i, j) :? and(i, j).
Det(i, j) :? a(i, j).
N(i, j) :? unicorn(i, j).
Figure 2: The Datalog representation of a CFG.
By naive (or seminaive) bottom-up evaluation
(see, e.g., Ullman, 1988), the answer to such a query
can be computed in polynomial time in the size of
the database for any Datalog program. By recording
rule instances rather than derived facts, a packed rep-
resentation of the complete set of Datalog derivation
trees for a given query can also be obtained in poly-
nomial time by the same technique. Since a Data-
log derivation tree uniquely determines a grammar
derivation tree, this gives a reduction of context-free
recognition and parsing to query evaluation in Data-
log.
In this paper, we show that a similar reduction
to Datalog is possible for more powerful grammar
formalisms with ?context-free? derivations, such as
(multi-component) tree-adjoining grammars (Joshi
and Schabes, 1997; Weir, 1988), IO macro gram-
mars (Fisher, 1968), and (parallel) multiple context-
free grammars (Seki et al, 1991). For instance, the
TAG in Figure 3 is represented by the Datalog pro-
gram in Figure 4. Moreover, the method of reduc-
176
SA
?
ANA
a A
b A?NA c
d
Figure 3: A TAG with one initial tree (left) and one
auxiliary tree (right)
S(p1, p3) :? A(p1, p3, p2, p2).
A(p1, p8, p4, p5) :? A(p2, p7, p3, p6), a(p1, p2), b(p3, p4),
c(p5, p6), d(p7, p8).
A(p1, p2, p1, p2).
Figure 4: The Datalog representation of a TAG.
tion extends to the problem of tactical generation
(surface realization) for these grammar formalisms
coupled with Montague semantics (under a certain
restriction). Our method essentially relies on the en-
coding of different formalisms in terms of abstract
categorial grammars (de Groote, 2001).
The reduction to Datalog makes it possible to ap-
ply to parsing and generation sophisticated evalu-
ation techniques for Datalog queries; in particular,
an application of generalized supplementary magic-
sets rewriting (Beeri and Ramakrishnan, 1991) au-
tomatically yields Earley-style algorithms for both
parsing and generation. The reduction can also
be used to obtain a tight upper bound, namely
LOGCFL, on the computational complexity of the
problem of recognition, both for grammaticality of
input strings and for surface realizability of input
logical forms.
With regard to parsing and recognition of in-
put strings, polynomial-time algorithms and the
LOGCFL upper bound on the computational com-
plexity are already known for the grammar for-
malisms covered by our results (Engelfriet, 1986);
nevertheless, we believe that our reduction to Data-
log offers valuable insights. Concerning generation,
our results seem to be entirely new.1
2 Context-free grammars on ?-terms
Consider an augmentation of the grammar in Fig-
ure 1 with Montague semantics, where the left-hand
1We only consider exact generation, not taking into account
the problem of logical form equivalence, which will most likely
render the problem of generation computationally intractable
(Moore, 2002).
S(X1X2) ? NP(X1) VP(X2)
VP(?x.X2(?y.X1yx)) ? V(X1) NP(X2)
V(?yx.X2(X1yx)(X3yx)) ? V(X1) Conj(X2) V(X3)
NP(X1X2) ? Det(X1) N(X2)
NP(?u.u Johne) ? John
V(finde?e?t) ? found
V(catche?e?t) ? caught
Conj(?t?t?t) ? and
Det(?uv.?(e?t)?t(?y.?t?t?t(uy)(vy))) ? a
N(unicorne?t) ? unicorn
Figure 5: A context-free grammar with Montague
semantics.
S
NP
John
VP
V
found
NP
Det
a
N
unicorn
Figure 6: A derivation tree.
side of each rule is annotated with a ?-term that tells
how the meaning of the left-hand side is composed
from the meanings of the right-hand side nontermi-
nals, represented by upper-case variables X1, X2, . . .
(Figure 5).2
The meaning of a sentence is computed from its
derivation tree. For example, John found a unicorn
has the derivation tree in Figure 6, and the grammar
rules assign its root node the ?-term
(?u.u John)(?x.(?uv.?(?y.?(uy)(vy))) unicorn (?y.find y x)),
which ?-reduces to the ?-term
(2) ?(?y.?(unicorn y)(find y John))
encoding the first-order logic formula representing
the meaning of the sentence (i.e., its logical form).
Thus, computing the logical form(s) of a sentence
involves parsing and ?-term normalization. To find a
sentence expressing a given logical form, it suffices
2We follow standard notational conventions in typed ?-
calculus. Thus, an application M1M2M3 (written without paren-
theses) associates to the left, ?x.?y.M is abbreviated to ?xy.M,
and ?? ?? ? stands for ?? (?? ?). We refer the reader
to Hindley, 1997 or S?rensen and Urzyczyn, 2006 for standard
notions used in simply typed ?-calculus.
177
S(X1X2) :? NP(X1),VP(X2).
VP(?x.X2(?y.X1yx)) :? V(X1),NP(X2).
V(?yx.X2(X1yx)(X3yx)) :? V(X1),Conj(X2),V(X3).
NP(X1X2) :? Det(X1),N(X2).
NP(?u.u Johne).
V(finde?e?t).
V(catche?e?t).
Conj(?t?t?t).
Det(?uv.?(e?t)?t(?y.?t?t?t(uy)(vy))).
N(unicorne?t).
Figure 7: A CFLG.
to find a derivation tree whose root node is associ-
ated with a ?-term that ?-reduces to the given log-
ical form; the desired sentence can simply be read
off from the derivation tree. At the heart of both
tasks is the computation of the derivation tree(s) that
yield the input. In the case of generation, this may be
viewed as parsing the input ?-term with a ?context-
free? grammar that generates a set of ?-terms (in
normal form) (Figure 7), which is obtained from the
original CFG with Montague semantics by stripping
off terminal symbols. Determining whether a given
logical form is surface realizable with the original
grammar is equivalent to recognition with the result-
ing context-free ?-term grammar (CFLG).
In a CFLG such as in Figure 7, constants appear-
ing in the ?-terms have preassigned types indicated
by superscripts. There is a mapping ? from nonter-
minals to their types (? = {S 7? t,NP 7? (e? t)?
t,VP 7? e?t,V 7? e?e?t,Conj 7? t?t?t,Det 7?
(e?t)?(e?t)?t,N 7? e?t}). A rule that has A on
the left-hand side and B1, . . . , Bn as right-hand side
nonterminals has its left-hand side annotated with a
well-formed ?-term M that has type ?(A) under the
type environment X1 :?(B1), . . . , Xn :?(Bn) (in sym-
bols, X1 : ?(B1), . . . , Xn : ?(Bn) ` M : ?(A)).
What we have called a context-free ?-term gram-
mar is nothing but an alternative notation for an ab-
stract categorial grammar (de Groote, 2001) whose
abstract vocabulary is second-order, with the restric-
tion to linear ?-terms removed.3 In the linear case,
Salvati (2005) has shown the recognition/parsing
complexity to be PTIME, and exhibited an algorithm
similar to Earley parsing for TAGs. Second-order
3A ?-term is a ?I-term if each occurrence of ? binds at least
one occurrence of a variable. A ?I-term is linear if no subterm
contains more than one free occurrence of the same variable.
S(?y.X1(?z.z)y) :? A(X1).
A(?xy.ao?o(X1(?z.bo?o(x(co?oz)))(do?oy))) :? A(X1).
A(?xy.xy).
Figure 8: The CFLG encoding a TAG.
linear ACGs are known to be expressive enough to
encode well-known mildly context-sensitive gram-
mar formalisms in a straightforward way, includ-
ing TAGs and multiple context-free grammars (de
Groote, 2002; de Groote and Pogodalla, 2004).
For example, the linear CFLG in Figure 8 is an
encoding of the TAG in Figure 3, where?(S) = o?o
and ?(A) = (o? o)? o? o (see de Groote, 2002
for details of this encoding). In encoding a string-
generating grammar, a CFLG uses o as the type of
string position and o? o as the type of string. Each
terminal symbol is represented by a constant of type
o?o, and a string a1 . . . an is encoded by the ?-term
?z.ao?o1 (. . . (a
o?o
n z) . . . ), which has type o? o.
A string-generating grammar coupled with Mon-
tague semantics may be represented by a syn-
chronous CFLG, a pair of CFLGs with matching
rule sets (de Groote 2001). The transduction be-
tween strings and logical forms in either direction
consists of parsing the input ?-term with the source-
side grammar and normalizing the ?-term(s) con-
structed in accordance with the target-side grammar
from the derivation tree(s) output by parsing.
3 Reduction to Datalog
We show that under a weaker condition than linear-
ity, a CFLG can be represented by a Datalog pro-
gram, obtaining a tight upper bound (LOGCFL) on
the recognition complexity. Due to space limitation,
our presentation here is kept at an informal level;
formal definitions and rigorous proof of correctness
will appear elsewhere.
We use the grammar in Figure 7 as an example,
which is represented by the Datalog program in Fig-
ure 9. Note that all ?-terms in this grammar are al-
most linear in the sense that they are ?I-terms where
any variable occurring free more than once in any
subterm must have an atomic type. Our construction
is guaranteed to be correct only when this condition
is met.
Each Datalog rule is obtained from the corre-
sponding grammar rule in the following way. Let
178
S(p1) :? NP(p1, p2, p3),VP(p2, p3).
VP(p1, p4) :? V(p2, p4, p3),NP(p1, p2, p3).
V(p1, p4, p3) :?
V(p2, p4, p3),Conj(p1, p5, p2),V(p5, p4, p3).
NP(p1, p4, p5) :? Det(p1, p4, p5, p2, p3),N(p2, p3).
NP(p1, p1, p2) :? John(p2).
V(p1, p3, p2) :? find(p1, p3, p2).
V(p1, p3, p2) :? catch(p1, p3, p2).
Conj(p1, p3, p2) :? ?(p1, p3, p2).
Det(p1, p5, p4, p3, p4) :? ?(p1, p2, p4),?(p2, p5, p3).
N(p1, p2) :? unicorn(p1, p2).
Figure 9: The Datalog representation of a CFLG.
M be the ?-term annotating the left-hand side of the
grammar rule. We first obtain a principal (i.e., most
general) typing of M.4 In the case of the second rule,
this is
X1 : p3? p4? p2, X2 : (p3? p2)? p1 `
?x.X2(?y.X1yx) : p4? p1.
We then remove ? and parentheses from the types
in the principal typing and write the resulting se-
quences of atomic types in reverse.5 We obtain the
Datalog rule by replacing Xi and M in the grammar
rule with the sequence coming from the type paired
with Xi and M, respectively. Note that atomic types
in the principal typing become variables in the Data-
log rule. When there are constants in the ?-term M,
they are treated like free variables. In the case of the
second-to-last rule, the principal typing is
? : (p4? p2)? p1, ? : p3? p5? p2 `
?uv.?(?y.?(uy)(vy)) : (p4? p3)? (p4? p5)? p1.
If the same constant occurs more than once, distinct
occurrences are treated as distinct free variables.
The construction of the database representing the
input ?-term is similar, but slightly more complex.
A simple case is the ?-term (2), where each constant
occurs just once. We compute its principal typing,
treating constants as free variables.6
? : (4? 2)? 1, ? : 3? 5? 2,
unicorn : 4? 3, find : 4? 6? 5 , John : 6
` ?(?y.?(unicorn y)(find y John)) : 1.
4To be precise, we must first convert M to its ?-long form
relative to the type assigned to it by the grammar. For example,
X1X2 in the first rule is converted to X1(?x.X2x).
5The reason for reversing the sequences of atomic types is
to reconcile the ?-term encoding of strings with the convention
of listing string positions from left to right in databases like (1).
6We assume that the input ?-term is in ?-long normal form.
We then obtain the corresponding database (3) and
query (4) from the antecedent and succedent of this
judgment, respectively. Note that here we are using
1, 2, 3, . . . as atomic types, which become database
constants.
?(1, 2, 4). ?(2, 5, 3). unicorn(3, 4).
find(5, 6, 4). John(6).
(3)
??S(1).(4)
When the input ?-term contains more than one oc-
currence of the same constant, it is not always cor-
rect to simply treat them as distinct free variables,
unlike in the case of ?-terms annotating grammar
rules. Consider the ?-term (5) (John found and
caught a unicorn):
(5) ?(?y.?(unicorn y)(?(find y John)(catch y John))).
Here, the two occurrences of John must be treated
as the same variable. The principal typing is (6) and
the resulting database is (7).
? : (4? 2)? 1, ?1 : 3? 5? 2,
unicorn : 4? 3, ?2 : 6? 8? 5,
find : 4? 7? 6, John : 7, catch : 4? 7? 8
` ?(?y.?1(unicorn y)
(?2(find y John)(catch y John))) : 1.
(6)
?(1, 2, 4). ?(2, 5, 3). ?(5, 8, 6). unicron(3, 4).
find(6, 7, 4). John(7). catch(8, 7, 4).
(7)
It is not correct to identify the two occurrences of
? in this example. The rule is to identify distinct
occurrences of the same constant just in case they
occur in the same position within ?-equivalent sub-
terms of an atomic type. This is a necessary con-
dition for those occurrences to originate as one and
the same occurrence in the non-normal ?-term at the
root of the derivation tree. (As a preprocessing step,
it is also necessary to check that distinct occurrences
of a bound variable satisfy the same condition, so
that the given ?-term is ?-equal to some almost lin-
ear ?-term.7)
7Note that the way we obtain a database from an input
?-term generalizes the standard database representation of a
string: from the ?-term encoding ?z.ao?o1 (. . . (a
o?o
n z) . . . ) of a
string a1 . . . an, we obtain the database {a1(0, 1), . . . , an(n?1, n)}.
179
4 Correctness of the reduction
We sketch some key points in the proof of cor-
rectness of our reduction. The ?-term N obtained
from the input ?-term by replacing occurrences of
constants by free variables in the manner described
above is the normal form of some almost linear ?-
term N?. The leftmost reduction from an almost lin-
ear ?-term to its normal form must be non-deleting
and almost non-duplicating in the sense that when
a ?-redex (?x.P)Q is contracted, Q is not deleted,
and moreover it is not duplicated unless the type
of x is atomic. We can show that the Subject Ex-
pansion Theorem holds for such ?-reduction, so the
principal typing of N is also the principal typing of
N?. By a slight generalization of a result by Aoto
(1999), this typing ? ` N? : ? must be negatively
non-duplicated in the sense that each atomic type
has at most one negative occurrence in it. By Aoto
and Ono?s (1994) generalization of the Coherence
Theorem (see Mints, 2000), it follows that every ?-
term P such that ?? ` P : ? for some ?? ? ? must be
??-equal to N? (and consequently to N).
Given the one-one correspondence between the
grammar rules and the Datalog rules, a Data-
log derivation tree uniquely determines a grammar
derivation tree (see Figure 10 as an example). This
relation is not one-one, because a Datalog deriva-
tion tree contains database constants from the input
database. This extra information determines a typ-
ing of the ?-term P at the root of the grammar deriva-
tion tree (with occurrences of constants in the ?-term
corresponding to distinct facts in the database re-
garded as distinct free variables):
John : 6, find : 4? 6? 5, ? : (4? 2)? 1,
? : 3? 5? 2, unicorn : 4? 3 `
(?u.u John)
(?x.(?uv.?(?y.?(uy)(vy))) unicorn (?y.find y x)) : 1.
The antecedent of this typing must be a subset of the
antecedent of the principal typing of the ?-term N
from which the input database was obtained. By the
property mentioned at the end of the preceding para-
graph, it follows that the grammar derivation tree is
a derivation tree for the input ?-term.
Conversely, consider the ?-term P (with distinct
occurrences of constants regarded as distinct free
variables) at the root of a grammar derivation tree
for the input ?-term. We can show that there is a
substitution ? which maps the free variables of P
to the free variables of the ?-term N used to build
the input database such that ? sends the normal form
of P to N. Since P is an almost linear ?-term, the
leftmost reduction from P? to N is non-deleting and
almost non-duplicating. By the Subject Expansion
Theorem, the principal typing of N is also the prin-
cipal typing of P?, and this together with the gram-
mar derivation tree determines a Datalog derivation
tree.
5 Complexity-theoretic consequences
Let us call a rule A(M) :? B1(X1), . . . , Bn(Xn) in a
CFLG an ?-rule if n = 0 and M does not contain any
constants. We can eliminate ?-rules from an almost
linear CFLG by the same method that Kanazawa and
Yoshinaka (2005) used for linear grammars, noting
that for any ? and ?, there are only finitely many
almost linear ?-terms M such that ? ` M : ?. If a
grammar has no ?-rule, any derivation tree for the
input ?-term N that has a ?-term P at its root node
corresponds to a Datalog derivation tree whose num-
ber of leaves is equal to the number of occurrences
of constants in P, which cannot exceed the number
of occurrences of constants in N.
A Datalog program P is said to have the poly-
nomial fringe property relative to a class D of
databases if there is a polynomial p(n) such that for
every database D in D of n facts and every query q
such that P?D derives q, there is a derivation tree for
q whose fringe (i.e., sequence of leaves) is of length
at most p(n). For such P and D, it is known that
{ (D, q) | D ? D,P ? D derives q } is in the complex-
ity class LOGCFL (Ullman and Van Gelder, 1988;
Kanellakis, 1988).
We state without proof that the database-query
pair (D, q) representing an input ?-term N can be
computed in logspace. By padding D with extra use-
less facts so that the size of D becomes equal to the
number of occurrences of constants in N, we obtain
a logspace reduction from the set of ?-terms gener-
ated by an almost linear CFLG to a set of the form
{ (D, q) | D ? D,P ? D ` q }, where P has the poly-
nomial fringe property relative to D. This shows
that the problem of recognition for an almost linear
CFLG is in LOGCFL.
180
S(1)
NP(1, 1, 6)
John(6)
VP(1, 6)
V(5, 6, 4)
find(5, 6, 4)
NP(1, 5, 4)
Det(1, 5, 4, 3, 4)
?(1, 2, 4) ?(2, 5, 3)
N(3, 4)
unicorn(3, 4)
S((?u.u John)(?x.(?uv.?(?y.?(uy)(vy))) unicorn (?y.find y x)))
NP(?u.u John) VP(?x.(?uv.?(?y.?(uy)(vy))) unicorn (?y.find y x)))
V(find) NP((?uv.?(?y.?(uy)(vy))) unicorn)
Det(?uv.?(?y.?(uy)(vy))) N(unicorn)
Figure 10: A Datalog derivation tree (left) and the corresponding grammar derivation tree (right)
By the main result of Gottlob et al (2002), the re-
lated search problem of finding one derivation tree
for the input ?-term is in functional LOGCFL, i.e.,
the class of functions that can be computed by a
logspace-bounded Turing machine with a LOGCFL
oracle. In the case of a synchronous almost linear
CFLG, the derivation tree found from the source ?-
term can be used to compute a target ?-term. Thus,
to the extent that transduction back and forth be-
tween strings and logical forms can be expressed by
a synchronous almost linear CFLG, the search prob-
lem of finding one logical form of an input sentence
and that of finding one surface realization of an input
logical form are both in functional LOGCFL.8 As a
consequence, there are efficient parallel algorithms
for these problems.
6 Regular sets of trees as input
Almost linear CFLGs can represent a substan-
tial fragment of a Montague semantics for En-
glish and such ?linear? grammar formalisms as
(multi-component) tree-adjoining grammars (both
as string grammars and as tree grammars) and mul-
tiple context-free grammars. However, IO macro
grammars and parallel multiple context-free gram-
mars cannot be directly represented because repre-
senting string copying requires multiple occurrences
of a variable of type o ? o. This problem can be
solved by switching from strings to trees. We con-
vert the input string into the regular set of binary
trees whose yield equals the input string (using c
8If the target-side grammar is not linear, the normal form of
the target ?-term cannot be explicitly computed because its size
may be exponential in the size of the source ?-term. Neverthe-
less, a typing that serves to uniquely identify the target ?-term
can be computed from the derivation tree in logspace. Also, if
the target-side grammar is linear and string-generating, the tar-
get string can be explicitly computed from the derivation tree in
logspace (Salvati, 2007).
as the sole symbol of rank 2), and turn the gram-
mar into a tree grammar, replacing all instances of
string concatenation in the grammar with the tree
operation t1, t2 7? c(t1, t2). This way, a string gram-
mar is turned into a tree grammar that generates a
set of trees whose image under the yield function is
the language of the string grammar. (In the case of
an IO macro grammar, the result is an IO context-
free tree grammar (Engelfriet, 1977).) String copy-
ing becomes tree copying, and the resulting gram-
mar can be represented by an almost linear CFLG
and hence by a Datalog program. The regular set
of all binary trees that yield the input string is repre-
sented by a database that is constructed from a deter-
ministic bottom-up finite tree automaton recogniz-
ing it. Determinism is important for ensuring cor-
rectness of this reduction. Since the database can
be computed from the input string in logspace, the
complexity-theoretic consequences of the last sec-
tion carry over here.
7 Magic sets and Earley-style algorithms
Magic-sets rewriting of a Datalog program allows
bottom-up evaluation to avoid deriving useless facts
by mimicking top-down evaluation of the original
program. The result of the generalized supplemen-
tary magic-sets rewriting of Beeri and Ramakrish-
nan (1991) applied to the Datalog program repre-
senting a CFG essentially coincides with the deduc-
tion system (Shieber et al, 1995) or uninstantiated
parsing system (Sikkel, 1997) for Earley parsing.
By applying the same rewriting method to Datalog
programs representing almost linear CFLGs, we can
obtain efficient parsing and generation algorithms
for various grammar formalisms with context-free
derivations.
We illustrate this approach with the program
in Figure 4, following the presentation of Ullman
181
(1989a; 1989b). We assume the query to take the
form ??? S(0, x).?, so that the input database can be
processed incrementally. The program is first made
safe by eliminating the possibility of deriving non-
ground atoms:
S(p1, p3) :? A(p1, p3, p2, p2).
A(p1, p8, p4, p5) :? A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p5, p6), d(p7, p8).
A(p1, p8, p4, p5) :? a(p1, p2), b(p2, p4), c(p5, p6), d(p6, p8).
The subgoal rectification removes duplicate argu-
ments from subgoals, creating new predicates as
needed:
S(p1, p3) :? B(p1, p3, p2).
A(p1, p8, p4, p5) :? A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p5, p6), d(p7, p8).
A(p1, p8, p4, p5) :? a(p1, p2), b(p2, p4), c(p5, p6), d(p6, p8).
B(p1, p8, p4) :? A(p2, p7, p3, p6), a(p1, p2), b(p3, p4), c(p4, p6), d(p7, p8).
B(p1, p8, p4) :? a(p1, p2), b(p2, p4), c(p4, p6), d(p6, p8).
We then attach to predicates adornments indicating
the free/bound status of arguments in top-down eval-
uation, reordering subgoals so that as many argu-
ments as possible are marked as bound:
Sbf(p1, p3) :? Bbff(p1, p3, p2).
Bbff(p1, p8, p4) :? abf(p1, p2), Abfff(p2, p7, p3, p6), bbf(p3, p4), cbb(p4, p6),
dbf(p7, p8).
Bbff(p1, p8, p4) :? abf(p1, p2), bbf(p2, p4), cbf(p4, p6), dbf(p6, p8).
Abfff(p1, p8, p4, p5) :? abf(p1, p2), Abfff(p2, p7, p3, p6), bbf(p3, p4), cbb(p5, p6),
dbf(p7, p8).
Abfff(p1, p8, p4, p5) :? abf(p1, p2), bbf(p2, p4), cff(p5, p6), dbf(p6, p8).
The generalized supplementary magic-sets rewriting
finally gives the following rule set:
r1 : m B(p1) :? m S(p1).
r2 : S(p1, p3) :? m B(p1), B(p1, p3, p2).
r3 : sup2.1(p1, p2) :? m B(p1), a(p1, p2).
r4 : sup2.2(p1, p7, p3, p6) :? sup2.1(p1, p2), A(p2, p7, p3, p6).
r5 : sup2.3(p1, p7, p6, p4) :? sup2.2(p1, p7, p3, p6), b(p3, p4).
r6 : sup2.4(p1, p7, p4) :? sup2.3(p1, p7, p6, p4), c(p4, p6).
r7 : B(p1, p8, p4) :? sup2.4(p1, p7, p4), d(p7, p8).
r8 : sup3.1(p1, p2) :? m B(p1), a(p1, p2).
r9 : sup3.2(p1, p4) :? sup3.1(p1, p2), b(p2, p4).
r10 : sup3.3(p1, p4, p6) :? sup3.2(p1, p4), c(p4, p6).
r11 : B(p1, p8, p4) :? sup3.3(p1, p4, p6), d(p6, p8).
r12 : m A(p2) :? sup2.1(p1, p2).
r13 : m A(p2) :? sup4.1(p1, p2).
r14 : sup4.1(p1, p2) :? m A(p1), a(p1, p2).
r15 : sup4.2(p1, p7, p3, p6) :? sup4.1(p1, p2), A(p2, p7, p3, p6).
r16 : sup4.3(p1, p7, p6, p4) :? sup4.2(p1, p7, p3, p6), b(p3, p4).
r17 : sup4.4(p1, p7, p4, p5) :? sup4.3(p1, p7, p6, p4), c(p5, p6).
r18 : A(p1, p8, p4, p5) :? sup4.4(p1, p7, p4, p5), d(p7, p8).
r19 : sup5.1(p1, p2) :? m A(p1), a(p1, p2).
r20 : sup5.2(p1, p4) :? sup5.1(p1, p2), b(p2, p4).
r21 : sup5.3(p1, p4, p5, p6) :? sup5.2(p1, p4), c(p5, p6).
r22 : A(p1, p8, p4, p5) :? sup5.3(p1, p4, p5, p6), d(p6, p8).
The following version of chart parsing adds con-
trol structure to this deduction system:
1. (????) Initialize the chart to the empty set, the
agenda to the singleton {m S(0)}, and n to 0.
2. Repeat the following steps:
(a) Repeat the following steps until the
agenda is exhausted:
i. Remove a fact from the agenda, called
the trigger.
ii. Add the trigger to the chart.
iii. Generate all facts that are immediate
consequences of the trigger together
with all facts in the chart, and add to
the agenda those generated facts that
are neither already in the chart nor in
the agenda.
(b) (????) Remove the next fact from the in-
put database and add it to the agenda, in-
crementing n. If there is no more fact in
the input database, go to step 3.
3. If S(0, n) is in the chart, accept; otherwise re-
ject.
The following is the trace of the algorithm on in-
put string aabbccdd:
1. m S(0) ????
2. m B(0) r1, 1
3. a(0, 1) ????
4. sup2.1(0, 1) r3, 2, 3
5. sup3.1(0, 1) r8, 2, 3
6. m A(1) r12, 4
7. a(1, 2) ????
8. sup4.1(1, 2) r14, 6, 7
9. sup5.1(1, 2) r19, 6, 7
10. m A(2) r13, 8
11. b(2, 3) ????
12. sup5.2(1, 3) r20, 9, 11
13. b(3, 4) ????
14. c(4, 5) ????
15. sup5.3(1, 3, 4, 5) r21, 12, 14
16. c(6, 5) ????
17. sup5.3(1, 3, 5, 6) r21, 12, 16
18. d(6, 7) ????
19. A(1, 7, 3, 5) r22, 17, 18
20. sup2.2(0, 7, 3, 5) r4, 4, 19
21. sup2.3(0, 7, 5, 4) r5, 13, 20
22. sup2.4(0, 7, 4) r6, 14, 21
23. d(7, 8) ????
24. B(0, 8, 4) r7, 22, 23
25. S(0, 8) r2, 2, 24
Note that unlike existing Earley-style parsing al-
gorithms for TAGs, the present algorithm is an in-
stantiation of a general schema that applies to pars-
ing with more powerful grammar formalisms as well
as to generation with Montague semantics.
8 Conclusion
Our reduction to Datalog brings sophisticated tech-
niques for Datalog query evaluation to the problems
182
of parsing and generation, and establishes a tight
bound on the computational complexity of recogni-
tion for a wide range of grammars. In particular, it
shows that the use of higher-order ?-terms for se-
mantic representation need not be avoided for the
purpose of achieving computational tractability.
References
Aoto, Takahito. 1999. Uniqueness of normal proofs in
implicational intuitionistic logic. Journal of Logic,
Language and Information 8, 217?242.
Aoto, Takahito and Hiroakira Ono. 1994. Uniqueness of
normal proofs in {?,?}-fragment of NJ. Research Re-
port IS-RR-94-0024F. School of Information Science,
Japan Advanced Institute of Science and Technology.
Beeri, Catriel and Raghu Ramakrishnan. 1991. On the
power of magic. Journal of Logic Programming 10,
255?299.
Engelfriet, J. and E. M. Schmidt. 1977. IO and OI, part
I. The Journal of Computer and System Sciences 15,
328?353.
Engelfriet, Joost. 1986. The complexity of languages
generated by attribute grammars. SIAM Journal on
Computing 15, 70?86.
Fisher, Michael J. 1968. Grammars with Macro-Like
Productions. Ph.D. dissertation. Harvard University.
Gottlob, Georg, Nicola Lenoe, Francesco Scarcello.
2002. Computing LOGCFL certificates. Theoretical
Computer Science 270, 761?777.
de Groote, Philippe. 2001. Towards abstract catego-
rial grammars. In Association for Computational Lin-
guistics, 39th Annual Meeting and 10th Conference of
the European Chapter, Proceedings of the Conference,
pages 148?155.
de Groote, Philippe. 2002. Tree-adjoining gram-
mars as abstract categorial grammars. In Proceed-
ings of the Sixth International Workshop on Tree Ad-
joining Grammar and Related Frameworks (TAG+6),
pages 145?150. Universita? di Venezia.
de Groote, Philippe and Sylvain Pogodalla. 2004. On
the expressive power of abstract categorial grammars:
Representing context-free formalisms. Journal of
Logic, Language and Information 13, 421?438.
Hindley, J. Roger. 1997. Basic Simple Type Theory.
Cambridge: Cambridge University Press.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegoz Rozenberg and Arto
Salomaa, editors, Handbook of Formal Languages,
Vol. 3, pages 69?123. Berlin: Springer.
Kanazawa, Makoto and Ryo Yoshinaka. 2005. Lexi-
calization of second-order ACGs. NII Technical Re-
port. NII-2005-012E. National Institute of Informat-
ics, Tokyo.
Kanellakis, Paris C. 1988. Logic programming and
parallel complexity. In Jack Minker, editor, Foun-
dations of Deductive Databases and Logic Program-
ming, pages 547?585. Los Altos, CA: Morgan Kauf-
mann.
Mints, Grigori. 2000. A Short Introduction to Intuitionis-
tic Logic. New York: Kluwer Academic/Plenum Pub-
lishers.
Moore, Robert C. 2002. A complete, efficient sentence-
realization algorithm for unification grammar. In Pro-
ceedings, International Natural Language Generation
Conference, Harriman, New York, pages 41?48.
Salvati, Sylvain. 2005. Proble`mes de filtrage
et proble`mes d?analyse pour les grammaires
cate?gorielles abstraites. Doctoral dissertation,
l?Institut National Polytechnique de Lorraine.
Salvati, Sylvain. 2007. Encoding second order string
ACG with deterministic tree walking transducers. In
Shuly Wintner, editor, Proceedings of FG 2006: The
11th conference on Formal Grammar, pages 143?156.
FG Online Proceedings. Stanford, CA: CSLI Publica-
tions.
Seki, Hiroyuki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science 88, 191?229.
Shieber, Stuart M., Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementations of de-
ductive parsing. Journal of Logic Programming 24,
3?36.
Sikkel, Klaas. 1997. Parsing Schemata. Berlin:
Springer.
S?rensen, Morten Heine and Pawe? Urzyczyn. 2006.
Lectures on the Curry-Howard Isomorphism. Ams-
terdam: Elsevier.
Ullman, Jeffrey D. 1988. Principles of Database and
Knowledge-Base Systems. Volume I. Rockville, MD.:
Computer Science Press.
Ullman, Jeffrey D. 1989a. Bottom-up beats top-down
for Datalog. In Proceedings of the Eighth ACM
SIGACT-SIGMOD-SIGART Symposium on Principles
of Database Systems, Philadelphia, pages 140?149.
Ullman, Jeffrey D. 1989b. Principles of Database and
Knowledge-Base Systems. Volume II: The New Tech-
nologies. Rockville, MD.: Computer Science Press.
Ullman, Jeffrey D. and Allen Van Gelder. 1988. Par-
allel complexity of logical query programs. Algorith-
mica 3, 5?42.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. dissertation.
University of Pennsylvania.
183
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 666?674,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
MIX Is Not a Tree-Adjoining Language
Makoto Kanazawa
National Institute of Informatics
2?1?2 Hitotsubashi, Chiyoda-ku
Tokyo, 101?8430, Japan
kanazawa@nii.ac.jp
Sylvain Salvati
INRIA Bordeaux Sud-Ouest, LaBRI
351, Cours de la Libe?ration
F-33405 Talence Cedex, France
sylvain.salvati@labri.fr
Abstract
The language MIX consists of all strings over
the three-letter alphabet {a, b, c} that contain
an equal number of occurrences of each letter.
We prove Joshi?s (1985) conjecture that MIX
is not a tree-adjoining language.
1 Introduction
The language
MIX = {w ? {a, b, c}? | |w|a = |w|b = |w|c }
has attracted considerable attention in computational
linguistics.1 This language was used by Bach (1981)
in an exercise to show that the permutation closure
of a context-free language is not necessarily context-
free.2 MIX may be considered a prototypical exam-
ple of free word order language, but, as remarked by
Bach (1981), it seems that no human language ?has
such complete freedom for order?, because ?typi-
cally, certain constituents act as ?boundary domains?
for scrambling?. Joshi (1985) refers to MIX as rep-
resenting ?an extreme case of the degree of free
word order permitted in a language?, which is ?lin-
guistically not relevant?. Gazdar (1988) adopts a
similar position regarding the relation between MIX
1If w is a string and d is a symbol, we write |w|d to mean the
number of occurrences of d in w. We will use the notation |w| to
denote the length of w, i.e., the total number of occurrences of
symbols in w.
2According to Gazdar (1988), ?MIX was originally de-
scribed by Emmon Bach and was so-dubbed by students in
the 1983 Hampshire College Summer Studies in Mathematics?.
According to Bach (1988), the name MIX was ?the happy in-
vention of Bill Marsh?.
and natural languages, noting that ?it seems rather
unlikely that any natural language will turn out to
have a MIX-like characteristic?.
It therefore seems natural to assume that lan-
guages such as MIX should be excluded from any
class of formal languages that purports to be a tight
formal characterization of the possible natural lan-
guages. It was in this spirit that Joshi et al (1991)
suggested that MIX should not be in the class of so-
called mildly context-sensitive languages:
?[mildly context-sensitive grammars] cap-
ture only certain kinds of dependencies,
e.g., nested dependencies and certain lim-
ited kinds of cross-serial dependencies
(for example, in the subordinate clause
constructions in Dutch or some variations
of them, but perhaps not in the so-called
MIX (or Bach) language) . . . .?
Mild context-sensitivity is an informally defined no-
tion first introduced by Joshi (1985); it consists of
the three conditions of limited cross-serial depen-
dencies, constant growth, and polynomial parsing.
The first condition is only vaguely formulated, but
the other two conditions are clearly satisfied by tree-
adjoining grammars. The suggestion of Joshi et al
(1991) was that MIX should be regarded as a vio-
lation of the condition of limited cross-serial depen-
dencies.
Joshi (1985) conjectured rather strongly that MIX
is not a tree-adjoining language: ?TAGs cannot gen-
erate this language, although for TAGs the proof is
not in hand yet?. An even stronger conjecture was
made by Marsh (1985), namely, that MIX is not an
666
indexed language.3 (It is known that the indexed
languages properly include the tree-adjoining lan-
guages.) Joshi et al (1991), however, expressed a
more pessimistic view about the conjecture:
?It is not known whether TAG . . . can
generate MIX. This has turned out to be
a very difficult problem. In fact, it is
not even known whether an IG [(indexed
grammar)] can generate MIX.?
This open question has become all the more press-
ing after a recent result by Salvati (2011). This re-
sult says that MIX is in the class of multiple context-
free languages (Seki et al, 1991), or equivalently,
languages of linear context-free rewriting systems
(Vijay-Shanker et al, 1987; Weir, 1988), which has
been customarily regarded as a formal counterpart
of the informal notion of a mildly context-sensitive
language.4 It means that either we have to aban-
don the identification of multiple context-free lan-
guages with mildly context-sensitive languages, or
we should revise our conception of limited cross-
serial dependencies and stop regarding MIX-like
languages as violations of this condition. Surely, the
resolution of Joshi?s (1985) conjecture should cru-
cially affect the choice between these two alterna-
tives.
In this paper, we prove that MIX is not a tree-
adjoining language. Our proof is cast in terms of the
formalism of head grammar (Pollard, 1984; Roach,
1987), which is known to be equivalent to TAG
(Vijay-Shanker and Weir, 1994). The key to our
proof is the notion of an n-decomposition of a string
over {a, b, c}, which is similar to the notion of a
derivation in head grammars, but independent of any
particular grammar. The parameter n indicates how
unbalanced the occurrence counts of the three let-
ters can be at any point in a decomposition. We first
3The relation of MIX with indexed languages is also of in-
terest in combinatorial group theory. Gilman (2005) remarks
that ?it does not . . . seem to be known whether or not the
word problem of Z ? Z is indexed?, alluding to the language
O2 = {w ? {a, a?, b, b?}? | |w|a = |w|a?, |w|b = |w|b? }. Since O2 and
MIX are rationally equivalent, O2 is indexed if and only if MIX
is indexed (Salvati, 2011).
4Joshi et al (1991) presented linear context-free rewriting
systems as mildly context-sensitive grammars. Groenink (1997)
wrote ?The class of mildly context-sensitive languages seems to
be most adequately approached by LCFRS.?
show that if MIX is generated by some head gram-
mar, then there is an n such that every string in MIX
has an n-decomposition. We then prove that if every
string in MIX has an n-decomposition, then every
string in MIX must have a 2-decomposition. Finally,
we exhibit a particular string in MIX that has no 2-
decomposition. The length of this string is 87, and
the fact that it has no 2-decomposition was first ver-
ified by a computer program accompanying this pa-
per. We include here a rigorous, mathematical proof
of this fact not relying on the computer verification.
2 Head Grammars
A head grammar is a quadruple G = (N,?, P, S),
where N is a finite set of nonterminals, ? is a fi-
nite set of terminal symbols (alphabet), S is a distin-
guished element of N, and P is a finite set of rules.
Each nonterminal is interpreted as a binary predicate
on strings in ??. There are four types of rules:
A(x1x2y1, y2)? B(x1, x2),C(y1, y2)
A(x1, x2y1y2)? B(x1, x2),C(y1, y2)
A(x1y1, y2x2)? B(x1, x2),C(y1, y2)
A(w1,w2)?
Here, A, B,C ? N, x1, x2, y1, y2 are variables, and
w1,w2 ? ? ? {?}.5 Rules of the first three types are
binary rules and rules of the last type are terminat-
ing rules. This definition of a head grammar actu-
ally corresponds to a normal form for head gram-
mars that appears in section 3.3 of Vijay-Shanker
and Weir?s (1994) paper.6
The rules of head grammars are interpreted as im-
plications from right to left, where variables can be
instantiated to any terminal strings. Each binary
5We use ? to denote the empty string.
6This normal form is also mentioned in chapter 5, section 4
of Kracht?s (2003) book. The notation we use to express rules
of head grammars is borrowed from elementary formal sys-
tems (Smullyan, 1961; Arikawa et al, 1992), also known as
literal movement grammars (Groenink, 1997; Kracht, 2003),
which are logic programs over strings. In Vijay-Shanker and
Weir?s (1994) notation, the four rules are expressed as follows:
A? C2,2(B,C)
A? C1,2(B,C)
A? W(B,C)
A? C1,1(w1 ? w2)
667
rule involves an operation that combines two pairs
of strings to form a new pair. The operation in-
volved in the third rule is known as wrapping; the
operations involved in the first two rules we call left
concatenation and right concatenation, respectively.
If G = (N,?, P, S) is a head grammar, A ? N, and
w1,w2 ? ??, then we say that a fact A(w1,w2) is
derivable and write `G A(w1,w2), if A(w1,w2) can
be inferred using the rules in P. More formally, we
have `G A(w1,w2) if one of the following conditions
holds:
? A(w1,w2)? is a terminating rule in P.
? `G B(u1, u2), `G C(v1, v2), and there is a bi-
nary rule A(?1, ?2) ? B(x1, x2),C(y1, y2) in
P such that (w1,w2) is the result of substitut-
ing u1, u2, v1, v2 for x1, x2, y1, y2, respectively,
in (?1, ?2).
The language of G is
L(G) = {w1w2 | `G S(w1,w2) }.
Example 1. Let G = (N,?, P, S), where N =
{S, A, A?,C,D, E, F}, ? = {a, a?, #}, and P consists of
the following rules:
S(x1y1, y2x2)? D(x1, x2),C(y1, y2)
C(?, #)?
D(?, ?)?
D(x1y1, y2x2)? F(x1, x2),D(y1, y2)
F(x1y1, y2x2)? A(x1, x2), E(y1, y2)
A(a, a) ?
E(x1y1, y2x2)? D(x1, x2), A?(y1, y2)
A?(a?, a?)?
We have L(G) = {w#wR | w ? D
{a,a?} }, where D{a,a?}
is the Dyck language over {a, a?} and wR is the re-
versal of w. All binary rules of this grammar are
wrapping rules.
If `G A(w1,w2), a derivation tree for A(w1,w2) is
a finite binary tree whose nodes are labeled by facts
that are derived during the derivation of A(w1,w2).
A derivation tree for A(w1,w2) represents a ?proof?
of `G A(w1,w2), and is formally defined as follows:
? If A(w1,w2)? is a terminating rule, then a tree
with a single node labeled by A(w1,w2) is a
derivation tree for A(w1,w2).
S(aaa?a?aa?, #a?aa?a?aa)
D(aaa?a?aa?, a?aa?a?aa)
F(aaa?a?, a?a?aa)
A(a, a) E(aa?a?, a?a?a)
D(aa?, a?a)
F(aa?, a?a)
A(a, a) E(a?, a?)
D(?, ?) A?(a?, a?)
D(?, ?)
A?(a?, a?)
D(aa?, a?a)
F(aa?, a?a)
A(a, a) E(a?, a?)
D(?, ?) A?(a?, a?)
D(?, ?)
C(?, #)
Figure 1: An example of a derivation tree of a head gram-
mar.
? If `G A(w1,w2) is derived from `G B(u1, u2)
and `G C(v1, v2) by some binary rule, then a
binary tree whose root is labeled by A(w1,w2)
and whose immediate left (right) subtree is a
derivation tree for B(u1, u2) (for C(v1, v2), re-
spectively) is a derivation tree for A(w1,w2).
If w ? L(G), a derivation tree for w is a derivation
tree for some S(w1,w2) such that w1w2 = w.
Example 1 (continued). Figure 1 shows a derivation
tree for aaa?a?aa?#a?aa?a?aa.
The following lemma should be intuitively clear
from the definition of a derivation tree:
Lemma 1. Let G = (N,?, P, S) be a head grammar
and A be a nonterminal in N. Suppose that w ?
L(G) has a derivation tree in which a fact A(v1, v2)
appears as a label of a node. Then there are strings
z0, z1, z2 with the following properties:
(i) w = z0v1z1v2z2, and
(ii) `G A(u1, u2) implies z0u1z1u2z2 ? L(G).
Proof. We can prove by straightforward induction
on the height of derivation trees that whenever
A(v1, v2) appears on a node in a derivation tree for
B(w1,w2), then there exist z0, z1, z2, z3 that satisfy
one of the following conditions:
(a) w1 = z0v1z1v2z2, w2 = z3, and `G A(u1, u2)
implies `G B(z0u1z1u2z2, z3).
(b) w1 = z0, w2 = z1v1z2v2z3, and `G A(u1, u2)
implies `G B(z0, z1u1z2u2z3).
668
(c) w1 = z0v1z1, w2 = z2v2z3, and `G A(u1, u2)
implies `G B(z0u1z1, z2u2z3).
We omit the details. 
We call a nonterminal A of a head grammarG use-
less if A does not appear in any derivation trees for
strings in L(G). Clearly, useless nonterminals can be
eliminated from any head grammar without affecting
the language of the grammar.
3 Decompositions of Strings in MIX
Henceforth, ? = {a, b, c}. Let Z denote the set of in-
tegers. Define functions ?1, ?2 : ?? ? Z, ? : ?? ?
Z ? Z by
?1(w) = |w|a ? |w|c,
?2(w) = |w|b ? |w|c,
?(w) = (?1(w), ?2(w)).
Clearly, we have ?(a) = (1, 0), ?(b) = (0, 1), ?(c) =
(?1,?1), and
w ? MIX iff ?(w) = (0, 0).
Note that for all strings w1,w2 ? ??, ?(w1w2) =
?(w1)+?(w2). In other words, ? is a homomorphism
from the free monoid ?? to Z ? Z with addition as
the monoid operation and (0, 0) as identity.
Lemma 2. Suppose that G = (N,?, P, S) is a head
grammar without useless nonterminals such that
L(G) ? MIX. There exists a function ?G : N ? Z ?
Z such that `G A(u1, u2) implies ?(u1u2) = ?G(A).
Proof. Since G has no useless nonterminals, for
each nonterminal A of G, there is a derivation tree
for some string in L(G) in which A appears in a node
label. By Lemma 1, there are strings z0, z1, z2 such
that `G A(u1, u2) implies z0u1z1u2z2 ? L(G). Since
L(G) ? MIX, we have ?(z0u1z1u2z2) = (0, 0), and
hence
?(u1u2) = ??(z0z1z2). 
A decomposition of w ? ?? is a finite binary tree
satisfying the following conditions:
? the root is labeled by some (w1,w2) such that
w = w1w2,
? each internal node whose left and right children
are labeled by (u1, u2) and (v1, v2), respectively,
is labeled by one of (u1u2v1, v2), (u1, u2v1v2),
(u1v1, v2u2).
? each leaf node is labeled by some (s1, s2) such
that s1s2 ? {b, c}? ? {a, c}? ? {a, b}?.
Thus, the label of an internal node in a decomposi-
tion is obtained from the labels of its children by left
concatenation, right concatenation, or wrapping. It
is easy to see that ifG is a head grammar over the al-
phabet ?, any derivation for w ? L(G) induces a de-
composition ofw. (Just strip off nonterminals.) Note
that unlike with derivation trees, we have placed no
bound on the length of a string that may appear on
a leaf node of a decomposition. This will be conve-
nient in some of the proofs below.
When p and q are integers, we write [p, q] for the
set { r ? Z | p ? r ? q }. We call a decomposition of
w an n-decomposition if each of its nodes is labeled
by some (v1, v2) such that ?(v1v2) ? [?n, n]?[?n, n].
Lemma 3. If MIX = L(G) for some head grammar
G = (?,N, P, S), then there exists an n such that each
w ? MIX has an n-decomposition.
Proof. We may suppose without loss of generality
that G has no useless nonterminal. Since MIX =
L(G), there is a function ?G satisfying the condition
of Lemma 2. Since the set N of nonterminals of G
is finite, there is an n such that ?G(A) ? [?n, n] ?
[?n, n] for all A ? N. Then it is clear that a derivation
tree for w ? L(G) induces an n-decomposition of
w. 
If w = d1 . . . dm ? ?m, then for 0 ? i ? j ? m,
we write w[i, j] to refer to the substring di+1 . . . dj
of w. (As a special case, we have w[i, i] = ?.) The
following is a key lemma in our proof:
Lemma 4. If each w ? MIX has an n-
decomposition, then each w ? MIX has a 2-
decomposition.
Proof. Assume that each w ? MIX has an n-
decomposition. Define a homomorphism ?n : ?? ?
?
? by
?n(a) = an,
?n(b) = b
n
,
?n(c) = c
n
.
669
Clearly, ?n is an injection, and we have ?(?n(v)) =
n ? ?(v) for all v ? ??.
Let w ? MIX with |w| = m. Then w? = ?n(w) ?
MIX and |w?| = mn. By assumption, w? has an n-
decomposition D. We assign a 4-tuple (i, j, k, l) of
natural numbers to each node of D in such a way
that (w?[i, j],w?[k, l]) equals the label of the node.
This is done recursively in an obvious way, start-
ing from the root. If the root is labeled by (w1,w2),
then it is assigned (0, |w1|, |w1|, |w1w2|). If a node is
assigned a tuple (i, j, k, l) and has two children la-
beled by (u1, u2) and (v1, v2), respectively, then the
4-tuples assigned to the children are determined ac-
cording to how (u1, u2) and (v1, v2) are combined at
the parent node:
u1 u2 v1 v2
i j k l
i + |u1| i + |u1u2|
u1 u2 v1 v2
i j k l
k + |u2| k + |u2v1|
u1 v1 v2 u2
i j k l
i + |u1| k + |v2|
Now define a function f : [0,mn] ? { kn | 0 ?
k ? m } by
f (i) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
i if n divides i,
n ? bi/nc if n does not divide i and
w?[i ? 1, i] ? {a, b},
n ? di/ne if n does not divide i and
w?[i ? 1, i] = c.
Clearly, f is weakly increasing in the sense that i ? j
implies f (i) ? f ( j). LetD? be the result of replacing
the label of each node inD by
(w?[ f (i), f ( j)],w?[ f (k), f (l)]),
where (i, j, k, l) is the 4-tuple of natural numbers as-
signed to that node by the above procedure. It is easy
to see that D? is another decomposition of w?. Note
that since each of f (i), f ( j), f (k), f (l) is an integral
multiple of n, we always have
(w?[ f (i), f ( j)],w?[ f (k), f (l)]) = (?n(u), ?n(v))
for some substrings u, v of w. This implies that for
h = 1, 2,
?h(w?[ f (i), f ( j)]w?[ f (k), f (l)])
is an integral multiple of n.
Claim. D? is a 2n-decomposition.
We have to show that every node label (v1, v2) in D?
satisfies ?(v1v2) ? [?2n, 2n] ? [?2n, 2n]. For h =
1, 2, define ?h : [0,mn] ? [0,mn]? Z as follows:
?h(i, j) =
?
?
?
?
?
?
?
?h(w?[i, j]) if i ? j,
??h(w?[ j, i]) otherwise.
Then it is easy to see that for all i, j, i?, j? ? [0,mn],
?h(i
?
, j?) = ?h(i
?
, i) + ?h(i, j) + ?h( j, j
?).
Inspecting the definition of the function f , we can
check that
?h( f (i), i) ? [0, n ? 1]
always holds. Suppose that (i, j, k, l) is assigned
to a node in D. By assumption, we have
?h(w?[i, j]w?[k, l]) ? [?n, n], and
?h(w
?[ f (i), f ( j)]w?[ f (k), f (l)])
= ?h(w
?[ f (i), f ( j)]) + ?h(w
?[ f (k), f (l)])
= ?h( f (i), f ( j)) + ?h( f (k), f (l))
= ?h( f (i), i) + ?h(i, j) + ?h( j, f ( j))
+ ?h( f (k), k) + ?h(k, l) + ?h(l, f (l))
= ?h( f (i), i) + ?h(w?[i, j]) + ?h( j, f ( j))
+ ?h( f (k), k) + ?h(w
?[k, l]) + ?h(l, f (l))
= ?h(w
?[i, j]w?[k, l]) + ?h( f (i), i) + ?h( f (k), k)
+ ?h( j, f ( j)) + ?h(l, f (l))
? { p + q1 + q2 + r1 + r2 | p ? [?n, n],
q1, q2 ? [0, n ? 1], r1, r2 ? [?n + 1, 0] }
= [?3n + 2, 3n ? 2].
Since ?h(w?[ f (i), f ( j)]w?[ f (k), f (l)]) must be an in-
tegral multiple of n, it follows that
?h(w
?[ f (i), f ( j)]w?[ f (k), f (l)]) ? {?2n,?n, 0, n, 2n}.
This establishes the claim.
670
We have shown that each node ofD? is labeled by
a pair of strings of the form (?n(u), ?n(v)) such that
?(?n(u)?n(v)) ?
{?2n,?n, 0, n, 2n} ? {?2n,?n, 0, n, 2n}.
Now it is easy to see that inverting the homomor-
phism ?n at each node of D?
(?n(u), ?n(v)) 7? (u, v)
gives a 2-decomposition of w. 
4 A String in MIX That Has No
2-Decomposition
By Lemmas 3 and 4, in order to prove that there is no
head grammar for MIX, it suffices to exhibit a string
in MIX that has no 2-decomposition. The following
is such a string:
z = a5b14a19c29b15a5.
In this section, we prove that the string z has no 2-
decomposition.7
It helps to visualize strings in MIX as closed
curves in a plane. If w is a string in MIX, by plotting
the coordinates of ?(v) for each prefix v of w, we can
represent w by a closed curve C together with a map
t : [0, |w|] ? C. The representation of the string z is
given in Figure 2.
Let us call a string w ? {a, b, c}? such that ?(w) ?
[?2, 2] ? [?2, 2] long if w contains all three letters,
and short otherwise. (If ?(w) < [?2, 2] ? [?2, 2],
then w is neither short nor long.) It is easy to see
that a short string w always satisfies
|w|a ? 4, |w|b ? 4, |w|c ? 2.
The maximal length of a short string is 6. (For ex-
ample, a4c2 and b4c2 are short strings of length 6.)
We also call a pair of strings (v1, v2) long (or short)
if v1v2 is long (or short, respectively).
According to the definition of an n-
decomposition, a leaf node in a 2-decomposition
7This fact was first verified by the computer program ac-
companying this paper. The program, written in C, imple-
ments a generic, memoized top-down recognizer for the lan-
guage {w ? MIX | w has a 2-decomposition }, and does not rely
on any special properties of the string z.
0 5
19 38
67
82
87 a5
b14
a19
c29
b15
a5
Figure 2: Graphical representation of the string z =
a5b14a19c29b15a5. Note that every point (i, j) on the di-
agonal segment has i > 7 or j < ?2.
must be labeled by a short pair of strings. We call
a 2-decomposition normal if the label of every
internal node is long. Clearly, any 2-decomposition
can be turned into a normal 2-decomposition by
deleting all nodes that are descendants of nodes
with short labels.
One important property of the string z is the fol-
lowing:
Lemma 5. If z = x1vx2 and ?(v) ? [?2, 2]? [?2, 2],
then either v or x1x2 is short.
Proof. This is easy to see from the graphical rep-
resentation in Figure 2. If a substring v of z has
?(v) ? [?2, 2] ? [?2, 2], then the subcurve corre-
sponding to v must have initial and final coordi-
nates whose difference lies in [?2, 2] ? [?2, 2]. If
v contains all three letters, then it must contain as
a substring at least one of ba19c, ac29b, and cb15a.
The only way to satisfy both these conditions is to
have the subcurve corresponding to v start and end
very close to the origin, so that x1x2 is short. (Note
that the distance between the coordinate (5, 0) corre-
sponding to position 5 of z and the diagonal segment
corresponding to the substring c29 is large enough
that it is impossible for v to start at position 5 and
end in the middle of c29 without violating the condi-
tion ?(v) ? [?2, 2] ? [?2, 2].) 
Lemma 5 leads to the following observation. Let
us call a decomposition of a string concatenation-
free if each of its non-leaf labels is the wrapping of
the labels of the children.
671
Lemma 6. If z has a 2-decomposition, then z has a
normal, concatenation-free 2-decomposition.
Proof. Let D be a 2-decomposition of z. Without
loss of generality, we may assume that D is nor-
mal. Suppose that D contains a node ? whose la-
bel is the left or right concatenation of the labels
of its children, (u1, u2) and (v1, v2). We only con-
sider the case of left concatenation since the case
of right concatenation is entirely analogous; so we
suppose that the node ? is labeled by (u1u2v1, v2).
It follows that z = x1u1u2x2 for some x1, x2, and
by Lemma 5, either u1u2 or x1x2 is short. If u1u2
is short, then the left child of ? is a leaf because
D is normal. We can replace its label by (u1u2, ?);
the label (u1u2v1, v2) of ? will now be the wrapping
(as well as left concatenation) of the two child la-
bels, (u1u2, ?) and (v1, v2). If x1x2 is short, then we
can combine by wrapping a single node labeled by
(x1, x2) with the subtree ofD rooted at the left child
of ?, to obtain a new 2-decomposition of z. In ei-
ther case, the result is a normal 2-decomposition of
z with fewer instances of concatenation. Repeat-
ing this procedure, we eventually obtain a normal,
concatenation-free 2-decomposition of z. 
Another useful property of the string z is the fol-
lowing:
Lemma 7. Suppose that the following conditions
hold:
(i) z = x1u1v1yv2u2x2,
(ii) x1yx2 is a short string, and
(iii) both ?(u1u2) and ?(v1v2) are in [?2, 2] ?
[?2, 2].
Then either (u1, u2) or (v1, v2) is short.
Proof. Suppose (u1, u2) and (v1, v2) are both long.
Since (u1, u2) and (v1, v2) must both contain c, either
u1 ends in c and v1 starts in c, or else v2 ends in c
and u2 starts in c.
Case 1. u1 ends in c and v1 starts in c. Since
(v1, v2) must contain at least one occurrence of a,
the string v1yv2 must contain cb15a as a substring.
a5b14 a19 c29 b15 a5
v1yv2
Since x1yx2 is short, we have |y|b ? 4. It follows that
|v1v2|b ? 11. But v1yv2 is a substring of c28b15a5,
so |v1v2|a ? 5. This clearly contradicts ?(v1v2) ?
[?2, 2] ? [?2, 2].
Case 2. v2 ends in c and u2 starts in c. In this
case, cb15a5 is a suffix of u2x2. Since x1yx2 is short,
|x2|a ? 4. This means that cb15a is a substring of u2
and hence |u2|b = 15.
a5b14 a19 c29 b15 a5
u2 x2v1yv2u1
On the other hand, since (v1, v2) must contain at least
one occurrence of b, the string v1yv2 must contain
ba19c as a substring. This implies that |u1u2|a ? 10.
But since |u2|b = 15, we have |u1u2|b ? 15. This
clearly contradicts ?(u1u2) ? [?2, 2] ? [?2, 2]. 
We now assume that z has a normal,
concatenation-free 2-decomposition D and de-
rive a contradiction. We do this by following
a certain path in D. Starting from the root, we
descend in D, always choosing a non-leaf child, as
long as there is one. We show that this path will
never terminate.
The i-th node on the path will be denoted by
?i, counting the root as the 0-th node. The la-
bel of ?i will be denoted by (wi,1,wi,2). With each
i, we associate three strings xi,1, yi, xi,2 such that
xi,1wi,1yiwi,2xi,2 = z, analogously to Lemma 1. Since
?(wi,1wi,2) ? [?2, 2] ? [?2, 2] and ?(z) = (0, 0), we
will always have ?(xi,1yixi,2) ? [?2, 2] ? [?2, 2].
Initially, (w0,1,w0,2) is the label of the root ?0 and
x0,1 = y0 = x0,2 = ?. If ?i is not a leaf node, let
(ui,1, ui,2) and (vi,1, vi,2) be the labels of the left and
right children of ?i, respectively. If the left child
is not a leaf node, we let ?i+1 be the left child,
in which case we have (wi+1,1,wi+1,2) = (ui,1, ui,2),
xi+1,1 = xi,1, xi+1,2 = xi,2, and yi+1 = vi,1yvi,2. Oth-
erwise, ?i+1 will be the right child of ?i, and we
have (wi+1,1,wi+1,2) = (vi,1, vi,2), xi+1,1 = xi,1ui,1,
xi+1,2 = ui,2xi,2, and yi+1 = yi.
The path ?0, ?1, ?2, . . . is naturally divided into
two parts. The initial part of the path consists of
nodes where xi,1yixi,2 is short. Note that x0,1y0x0,2 =
? is short. As long as xi,1yixi,2 is short, (wi,1,wi,2)
must be long and ?i has two children labeled
by (ui,1, ui,2) and (vi,1, vi,2). By Lemma 7, either
(ui,1, ui,2) or (vi,1, vi,2) must be short. Since the length
672
of z is 87 and the length of a short string is at most 6,
exactly one of (ui,1, ui,2) and (vi,1, vi,2) must be long.
We must eventually enter the second part of
the path, where xi,1yixi,2 is no longer short. Let
?m be the first node belonging to this part of the
path. Note that at ?m, we have ?(xm,1ymxm,2) =
?(xm?1,1ym?1xm?1,2) + ?(v) for some short string v.
(Namely, v = um?1,1um?1,2 or v = vm?1,1vm?1,2.)
Lemma 8. If u and v are short strings and ?(uv) ?
[?2, 2]? [?2, 2], then |uv|d ? 4 for each d ? {a, b, c}.
Proof. Since u and v are short, we have |u|a ?
4, |u|b ? 4, |u|c ? 2 and |v|a ? 4, |v|b ? 4, |v|c ? 2. It
immediately follows that |uv|c ? 4. We distinguish
two cases.
Case 1. |uv|c ? 2. Since ?(uv) ? [?2, 2] ? [?2, 2],
we must have |uv|a ? 4 and |uv|b ? 4.
Case 2. |uv|c ? 3. Since |u|c ? 2 and |v|c ? 2,
we must have |u|c ? 1 and |v|c ? 1. Also, ?(uv) ?
[?2, 2] ? [?2, 2] implies that |uv|a ? 1 and |uv|b ? 1.
Since u and v are short, it follows that one of the
following two conditions must hold:
(i) |u|a ? 1, |u|b = 0 and |v|a = 0, |v|b ? 1.
(ii) |u|a = 0, |u|b ? 1 and |v|a ? 1, |v|b = 0.
In the former case, |uv|a = |u|a ? 4 and |uv|b = |v|b ?
4. In the latter case, |uv|a = |v|a ? 4 and |uv|b =
|u|b ? 4. 
By Lemma 8, the number of occurrences of each
letter in xm,1ymxm,2 is in [1, 4]. This can only be if
xm,1xm,2 = a
j
,
ym = c
kbl,
for some j, k, l ? [1, 4]. This means that the string z
must have been split into two strings (w0,1,w0,2) at
the root of D somewhere in the vicinity of position
67 (see Figure 2).
It immediately follows that for all i ? m, wi,1 is
a substring of a5b14a19c28 and wi,2 is a substring of
b14a5. We show by induction that for all i ? m, the
following condition holds:
(?) ba19c17 is a substring of wi,1.
The condition (?) clearly holds for i = m. Now as-
sume (?). Then (wi,1,wi,2) is long, and ?i has left and
right children, labeled by (ui,1, ui,2) and (vi,1, vi,2), re-
spectively, such that wi,1 = ui,1vi,1 and wi,2 = vi,2ui,2.
We consider two cases.
Case 1. ui,1 contains c. Then ba19c is a substring
of ui,1. Since ui,2 is a substring of b14a5, it cannot
contain any occurrences of c. Since ?1(ui,1ui,2) ?
[?2, 2], it follows that ui,1 must contain at least 17
occurrences of c; hence ba19c17 is a substring of ui,1.
Since (ui,1, ui,2) is long, (wi+1,1,wi+1,2) = (ui,1, ui,2).
Therefore, the condition (?) holds with i+ 1 in place
of i.
Case 2. ui,1 does not contain c. Then (ui,1, ui,2) is
short and (wi+1,1,wi+1,2) = (vi,1, vi,2). Note that vi,1
must contain at least 17 occurrences of c, but vi,2 is
a substring of b14a5 and hence cannot contain more
than 14 occurrences of b. Since ?2(vi,1vi,2) ? [?2, 2],
it follows that vi,1 must contain at least one occur-
rence of b. Therefore, ba19c17 must be a substring
of vi,1 = wi+1,1, which shows that (?) holds with i+1
in place of i.
We have proved that (?) holds for all i ? m. It fol-
lows that for all i, ?i has two children and hence ?i+1
is defined. This means that the path ?0, ?1, ?2, . . .
is infinite, contradicting the assumption that D is a
2-decomposition of z.
We have proved the following:
Lemma 9. There is a string in MIX that has no 2-
decomposition.
Theorem 10. There is no head grammar G such that
L(G) = MIX.
Proof. Immediate from Lemmas 3, 4, and 9. 
References
Setsuo Arikawa, Takeshi Shinohara, and Akihiro Ya-
mamoto. 1992. Learning elementary formal systems.
Theoretical Computer Science, 95(1):97?113.
Emmon Bach. 1981. Discontinuous constituents in gen-
eralized categorial grammars. In Victoria Burke and
James Pustejovsky, editors, Proceedings of the 11th
Annual Meeting of the North East Linguistic Society,
pages 1?12.
Emmon Bach. 1988. Categorial grammars as theories
of language. In Richard T. Oehrle, Emmon Bach, and
Deirdre Wheeler, editors, Categorial Grammars and
Natural Language Structures, pages 17?34. D. Reidel,
Dordrecht.
673
Gerald Gazdar. 1988. Applicability of indexed gram-
mars to natural languages. In U. Reyle and C. Rohrer,
editors,Natural Language Parsing and Linguistic The-
ories, pages 69?94. D. Reidel Publishing Company,
Dordrecht.
Robert Gilman. 2005. Formal languages and their ap-
plication to combinatorial group theory. In Alexan-
dre V. Borovik, editor, Groups, Languages, Algo-
rithms, number 378 in Contemporary Mathematics,
pages 1?36. American Mathematical Society, Provi-
dence, RI.
Annius V. Groenink. 1997. Mild context-sensitivity and
tuple-based generalizations of context-free grammar.
Linguistics and Philosophy, 20:607?636.
Aravind K. Joshi, Vijay K. Shanker, and David J. Weir.
1991. The converence of mildly context-sensitive
grammar formalisms. In Peter Sells, Stuart M.
Shieber, and ThomasWasow, editors, Foundational Is-
sues in Natural Language Processing, pages 31?81.
The MIT Press, Cambridge, MA.
Aravind K. Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide reason-
able structural descriptions? In David Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural
Language Parsing, pages 206?250. Cambridge Uni-
versity Press, Cambridge.
Markus Kracht. 2003. The Mathematics of Language,
volume 63 of Studies in Generative Grammar. Mou-
ton de Gruyter, Berlin.
William Marsh. 1985. Some conjectures on indexed
languages. Paper presented to the Association for
Symbolic Logic Meeting, Stanford University, July
15?19. Abstract appears in Journal of Symbolic
Logic 51(3):849 (1986).
Carl J. Pollard. 1984. Generalized Phrase Structure
Grammars, Head Grammars, and Natural Language.
Ph.D. thesis, Department of Linguistics, Stanford Uni-
versity.
Kelly Roach. 1987. Formal properties of head gram-
mars. In Alexis Manaster-Ramer, editor, Mathematics
of Language, pages 293?347. John Benjamins, Ams-
terdam.
Sylvain Salvati. 2011. MIX is a 2-MCFL and the word
problem in Z2 is captured by the IO and the OI hierar-
chies. Technical report, INRIA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context free gram-
mars. Theoretical Computer Science, 88(2):191?229.
Raymond M. Smullyan. 1961. Theory of Formal Sys-
tems. Princeton University Press, Princeton, NJ.
K. Vijay-Shanker and D. J. Weir. 1994. The equivalence
of four extensions of context-free grammars. Mathe-
matical Systems Theory, 27:511?546.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 104?111.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadephia, PA.
674
