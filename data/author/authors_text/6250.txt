Towards Terascale Knowledge Acquisition 
Patrick Pantel, Deepak Ravichandran and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{pantel,ravichan,hovy}@isi.edu 
 
Abstract 
Although vast amounts of textual data are freely 
available, many NLP algorithms exploit only a 
minute percentage of it. In this paper, we study the 
challenges of working at the terascale. We present 
an algorithm, designed for the terascale, for mining  
is-a relations that achieves similar performance to a 
state-of-the-art linguistically-rich method. We fo-
cus on the accuracy of these two systems as a func-
tion of processing time and corpus size. 
1 Introduction 
The Natural Language Processing (NLP) com-
munity has recently seen a growth in corpus-based 
methods. Algorithms light in linguistic theories but 
rich in available training data have been success-
fully applied to several applications such as ma-
chine translation (Och and Ney 2002), information 
extraction (Etzioni et al 2004), and question an-
swering (Brill et al 2001). 
In the last decade, we have seen an explosion in 
the amount of available digital text resources. It is 
estimated that the Internet contains hundreds of 
terabytes of text data, most of which is in an 
unstructured format. Yet, many NLP algorithms 
tap into only megabytes or gigabytes of this 
information. 
In this paper, we make a step towards acquiring 
semantic knowledge from terabytes of data. We 
present an algorithm for extracting is-a relations, 
designed for the terascale, and compare it to a state 
of the art method that employs deep analysis of 
text (Pantel and Ravichandran 2004). We show 
that by simply utilizing more data on this task, we 
can achieve similar performance to a linguistically-
rich approach. The current state of the art co-
occurrence model requires an estimated 10 years 
just to parse a 1TB corpus (see Table 1). Instead of 
using a syntactically motivated co-occurrence ap-
proach as above, our system uses lexico-syntactic 
rules. In particular, it finds lexico-POS patterns by 
making modifications to the basic edit distance 
algorithm. Once these patterns have been learnt, 
the algorithm for finding new is-a relations runs in 
O(n), where n is the number of sentences. 
In semantic hierarchies such as WordNet (Miller 
1990), an is-a relation between two words x and y 
represents a subordinate relationship (i.e. x is more 
specific than y). Many algorithms have recently 
been proposed to automatically mine is-a (hypo-
nym/hypernym) relations between words. Here, we 
focus on is-a relations that are characterized by the 
questions ?What/Who is X?? For example, Table 2 
shows a sample of 10 is-a relations discovered by 
the algorithms presented in this paper. In this table, 
we call azalea, tiramisu, and Winona Ryder in-
stances of the respective concepts flower, dessert 
and actress. These kinds of is-a relations would be 
useful for various purposes such as ontology con-
struction, semantic information retrieval, question 
answering, etc. 
The main contribution of this paper is a compari-
son of the quality of our pattern-based and co-
occurrence models as a function of processing time 
and corpus size. Also, the paper lays a foundation 
for terascale acquisition of knowledge. We will 
show that, for very small or very large corpora or 
for situations where recall is valued over precision, 
the pattern-based approach is best. 
2 Relevant Work 
Previous approaches to extracting is-a relations 
fall under two categories: pattern-based and co-
occurrence-based approaches. 
2.1 Pattern-based approaches 
Marti Hearst (1992) was the first to use a pat-
tern-based approach to extract hyponym relations 
from a raw corpus. She used an iterative process to 
semi-automatically learn patterns. However, a 
corpus of 20MB words yielded only 400 examples. 
Our pattern-based algorithm is very similar to the 
one used by Hearst. She uses seed examples to 
manually discover her patterns whearas we use a 
minimal edit distance algorithm to automatically 
discover the patterns. 
771
Riloff and Shepherd (1997) used a semi-
automatic method for discovering similar words 
using a few seed examples by using pattern-based 
techniques and human supervision. Berland and 
Charniak (1999) used similar pattern-based tech-
niques and other heuristics to extract meronymy 
(part-whole) relations. They reported an accuracy 
of about 55% precision on a corpus of 100,000 
words. Girju et al (2003) improved upon Berland 
and Charniak's work using a machine learning 
filter. Mann (2002) and Fleischman et al (2003) 
used part of speech patterns to extract a subset of 
hyponym relations involving proper nouns. 
Our pattern-based algorithm differs from these 
approaches in two ways. We learn lexico-POS 
patterns in an automatic way. Also, the patterns are 
learned with the specific goal of scaling to the 
terascale (see Table 2). 
2.2 Co-occurrence-based approaches 
The second class of algorithms uses co-
occurrence statistics (Hindle 1990, Lin 1998). 
These systems mostly employ clustering algo-
rithms to group words according to their meanings 
in text. Assuming the distributional hypothesis 
(Harris 1985), words that occur in similar gram-
matical contexts are similar in meaning. Curran 
and Moens (2002) experimented with corpus size 
and complexity of proximity features in building 
automatic thesauri. CBC (Clustering by Commit-
tee) proposed by Pantel and Lin (2002) achieves 
high recall and precision in generating similarity 
lists of words discriminated by their meaning and 
senses. However, such clustering algorithms fail to 
name their classes. 
Caraballo (1999) was the first to use clustering 
for labeling is-a relations using conjunction and 
apposition features to build noun clusters. Re-
cently, Pantel and Ravichandran (2004) extended 
this approach by making use of all syntactic de-
pendency features for each noun. 
3 Syntactical co-occurrence approach 
Much of the research discussed above takes a 
similar approach of searching text for simple sur-
face or lexico-syntactic patterns in a bottom-up 
approach. Our co-occurrence model (Pantel and 
Ravichandran 2004) makes use of semantic classes 
like those generated by CBC. Hyponyms are gen-
erated in a top-down approach by naming each 
group of words and assigning that name as a hypo-
nym of each word in the group (i.e., one hyponym 
per instance/group label pair). 
The input to the extraction algorithm is a list of 
semantic classes, in the form of clusters of words, 
which may be generated from any source. For ex-
ample, following are two semantic classes discov-
ered by CBC: 
(A) peach, pear, pineapple, apricot, 
mango, raspberry, lemon, cherry, 
strawberry, melon, blueberry, fig, apple, 
plum, nectarine, avocado, grapefruit, 
papaya, banana, cantaloupe, cranberry, 
blackberry, lime, orange, tangerine, ... 
(B) Phil Donahue, Pat Sajak, Arsenio 
Hall, Geraldo Rivera, Don Imus, Larry King, 
David Letterman, Conan O'Brien, Rosie 
O'Donnell, Jenny Jones, Sally Jessy Raph-
ael, Oprah Winfrey, Jerry Springer, Howard 
Stern, Jay Leno, Johnny Carson, ... 
The extraction algorithm first labels concepts 
(A) and (B) with fruit and host respectively. Then, 
is-a relationships are extracted, such as: apple is a 
fruit, pear is a fruit, and David Letterman is a host. 
An instance such as pear is assigned a hypernym 
fruit not because it necessarily occurs in any par-
ticular syntactic relationship with the word fruit, 
but because it belongs to the class of instances that 
does. The labeling of semantic classes is performed 
in three phases, as outlined below. 
3.1 Phase I 
In the first phase of the algorithm, feature vec-
tors are extracted for each word that occurs in a 
semantic class. Each feature corresponds to a 
grammatical context in which the word occurs. For 
example, ?catch __? is a verb-object context. If the 
word wave occurred in this context, then the con-
text is a feature of wave. 
We then construct a mutual information vector 
MI(e) = (mie1, mie2, ?, miem) for each word e, 
where mief is the pointwise mutual information 
between word e and context f, which is defined as: 
 
N
c
N
c
N
c
ef m
j
ej
n
i
if
ef
mi
??
== ?
=
11
log  
Table 2. Sample of 10 is-a relationships discovered by 
our co-occurrence and pattern-based systems.  
CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEM 
Word Hypernym Word Hypernym 
azalea flower American airline 
bipolar disorder disease Bobby Bonds coach 
Bordeaux wine radiation therapy cancer 
treatment 
Flintstones television show tiramisu dessert 
salmon fish Winona Ryder actress 
 
Table 1. Approximate processing time on a single 
Pentium-4 2.5 GHz machine. 
TOOL 15 GB ORPUS 1 TB CORPUS 
POS Tagger 2 days 125 days 
NP Chunker 3 days 214 days 
Dependency Parser 56 days 10.2 years 
Syntactic Parser 5.8 years 388.4 years 
 
772
where n is the number of elements to be clustered, 
cef is the frequency count of word e in grammatical 
context f, and N is the total frequency count of all 
features of all words. 
3.2 Phase II 
Following (Pantel and Lin 2002), a committee 
for each semantic class is constructed. A commit-
tee is a set of representative elements that unambi-
guously describe the members of a possible class. 
For example, in one of our experiments, the com-
mittees for semantic classes (A) and (B) from Sec-
tion 3 were: 
A) peach, pear, pineapple, apricot, mango, 
raspberry, lemon, blueberry 
B) Phil Donahue, Pat Sajak, Arsenio Hall, 
Geraldo Rivera, Don Imus, Larry King, 
David Letterman 
3.3 Phase III 
By averaging the feature vectors of the commit-
tee members of a particular semantic class, we 
obtain a grammatical template, or signature, for 
that class. For example, Figure 1 shows an excerpt 
of the grammatical signature for semantic class 
(B). The vector is obtained by averaging the fea-
ture vectors of the words in the committee of this 
class. The ?V:subj:N:joke? feature indicates a sub-
ject-verb relationship between the class and the 
verb joke while ?N:appo:N:host? indicates an ap-
position relationship between the class and the 
noun host. The two columns of numbers indicate 
the frequency and mutual information scores. 
To name a class, we search its signature for cer-
tain relationships known to identify class labels. 
These relationships, automatically learned in 
(Pantel and Ravichandran 2004), include apposi-
tions, nominal subjects, such as relationships, and 
like relationships. We sum up the mutual informa-
tion scores for each term that occurs in these rela-
tionships with a committee of a class. The highest 
scoring term is the name of the class. 
The syntactical co-occurrence approach has 
worst-case time complexity O(n2k), where n is the 
number of words in the corpus and k is the feature-
space (Pantel and Ravichandran 2004). Just to 
parse a 1 TB corpus, this approach requires ap-
proximately 10.2 years (see Table 2). 
4 Scalable pattern-based approach 
We propose an algorithm for learning highly 
scalable lexico-POS patterns. Given two sentences 
with their surface form and part of speech tags, the 
algorithm finds the optimal lexico-POS alignment. 
For example, consider the following 2 sentences: 
1) Platinum is a precious metal. 
2) Molybdenum is a metal. 
Applying a POS tagger (Brill 1995) gives the 
following output: 
 
Surface Platinum is a precious metal . 
POS NNP VBZ DT JJ NN . 
 
Surface Molybdenum is a metal . 
POS NNP VBZ DT NN . 
 
A very good pattern to generalize from the 
alignment of these two strings would be 
 
Surface  is a  metal . 
POS NNP     . 
 
We use the following notation to denote this 
alignment: ?_NNP is a (*s*) metal.?, where  
?_NNP represents the POS tag NNP?. 
To perform such alignments we introduce two 
wildcard operators, skip (*s*) and wildcard (*g*). 
The skip operator represents 0 or 1 instance of any 
word (similar to the \w* pattern in Perl), while the 
wildcard operator represents exactly 1 instance of 
any word (similar to the \w+ pattern in Perl). 
4.1 Algorithm 
We present an algorithm for learning patterns at 
multiple levels. Multilevel representation is de-
fined as the different levels of a sentence such as 
the lexical level and POS level. Consider two 
strings a(1, n) and b(1, m) of lengths n and m re-
spectively. Let a1(1, n) and a2(1, n) be the level 1 
(lexical level) and level 2 (POS level) representa-
tions for the string a(1, n). Similarly, let b1(1, m) 
and b2(1, m) be the level 1 and level 2 representa-
tions for the string b(1, m). The algorithm consists 
of two parts: calculation of the minimal edit dis-
tance and retrieval of an optimal pattern. The 
minimal edit distance algorithm calculates the 
number of edit operations (insertions, deletions and 
replacements) required to change one string to 
another string. The optimal pattern is retrieved by 
{Phil Donahue,Pat Sajak,Arsenio Hall} 
 N:gen:N  
  talk show 93 11.77 
  television show 24 11.30 
  TV show 25 10.45 
  show 255 9.98 
  audience 23 7.80 
  joke 5 7.37 
 V:subj:N  
  joke 39 7.11 
  tape 10 7.09 
  poke 15 6.87 
  host 40 6.47 
  co-host 4 6.14 
  banter 3 6.00 
  interview 20 5.89 
 N:appo:N  
  host 127 12.46 
  comedian 12 11.02 
  King 13 9.49 
  star 6 7.47 
Figure 1. Excerpt of the grammatical signature for the 
television host class. 
 
773
keeping track of the edit operations (which is the 
second part of the algorithm). 
Algorithm for calculating the minimal edit distance 
between two strings 
D[0,0]=0 
for i = 1 to n do  D[i,0] = D[i-1,0] + cost(insertion) 
for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) 
for i = 1 to n do 
 for j = 1 to m do 
  D[i,j] = min( D[i-1,j-1] + cost(substitution), 
        D[i-1,j] + cost(insertion), 
        D[i,j-1] + cost(deletion)) 
Print (D[n,m]) 
Algorithm for optimal pattern retrieval 
i = n, j = m; 
while i ? 0 and j ? 0 
 if D[i,j] = D[i-1,j] + cost(insertion) 
  print (*s*), i = i-1 
 else if D[i,j] = D[i,j-1] + cost(deletion) 
  print(*s*), j = j-1 
 else if a1i = b1j 
  print (a1i), i = i -1, j = j =1 
 else if a2i = b2j 
  print (a2i), i = i -1, j = j =1 
 else 
  print (*g*), i = i -1, j = j =1 
We experimentally set (by trial and error): 
cost(insertion)  = 3 
cost(deletion)  = 3 
cost(substitution) = 0 if a1i=b1j 
  = 1 if a1i?b1j, a2i=b2j 
  = 2 if a1i?b1j, a2i?b2j 
4.2 Implementation and filtering 
The above algorithm takes O(y2) time for every 
pair of strings of length at most y. Hence, if there 
are x strings in the collection, each string having at 
most length y, the algorithm has time complexity 
O(x2y2) to extract all the patterns in the collection. 
Applying the above algorithm on a corpus of 
3GB  with 50 is-a relationship seeds, we obtain a 
set of 600 lexico-POS. Following are two of them: 
1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ 
|NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?caldera or lava lake? 
2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP 
|NNP|VBN|NN#NN|VBG#NN|NN ,_, _DT 
Y_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP 
|NNP#NNP|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?leukemia, the cancer of ... 
Note that we store different POS variations of 
the anchors X and Y. As shown in example 1, the 
POS variations of the anchor X are (JJ NN, JJ NN 
NN, NN). The variations for anchor Y are (JJ JJ 
NN, JJ, etc.). The reason is quite straightforward: 
we need to determine the boundary of the anchors 
X and Y and a reasonable way to delimit them 
would be to use POS information.  All the patterns 
produced by the multi-level pattern learning algo-
rithm were generated from positive examples. 
From amongst these patterns, we need to find the 
most important ones. This is a critical step because 
frequently occurring patterns have low precision 
whereas rarely occurring patterns have high preci-
sion. From the Information Extraction point of 
view neither of these patterns is very useful. We 
need to find patterns with relatively high occur-
rence and high precision. We apply the log likeli-
hood principle (Dunning 1993) to compute this 
score. The top 15 patterns according to this metric 
are listed in Table 3 (we omit the POS variations 
for visibility). Some of these patterns are similar to 
the ones discovered by Hearst (1992) while other 
patterns are similar to the ones used by Fleischman 
et al (2003). 
4.3 Time complexity 
To extract hyponym relations, we use a fixed 
number of patterns across a corpus. Since we treat 
each sentences independently from others, the 
algorithm runs in linear time O(n) over the corpus 
size, where n is number of sentences in the corpus. 
5 Experimental Results 
In this section, we empirically compare the pat-
tern-based and co-occurrence-based models pre-
sented in Section 3 and Section 4. The focus is on 
the precision and recall of the systems as a func-
tion of the corpus size. 
5.1 Experimental Setup 
We use a 15GB newspaper corpus consisting of 
TREC9, TREC 2002, Yahoo! News ~0.5GB, AP 
newswire ~2GB, New York Times ~2GB, Reuters 
~0.8GB, Wall Street Journal ~1.2GB, and various 
online news website ~1.5GB. For our experiments, 
we extract from this corpus six data sets of differ-
ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB 
and 15GB. 
For the co-occurrence model, we used Minipar 
(Lin 1994), a broad coverage parser, to parse each 
data set. We collected the frequency counts of the 
grammatical relationships (contexts) output by 
Minipar and used them to compute the pointwise 
mutual information vectors described in Section 
3.1. For the pattern-based approach, we use Brill?s 
POS tagger (1995) to tag each data set. 
5.2 Precision 
We performed a manual evaluation to estimate 
the precision of both systems on each dataset. For 
each dataset, both systems extracted a set of is-a 
Table 3. Top 15 lexico-syntactic patterns discovered 
by our system. 
X, or Y X, _DT Y _(WDT|IN) Y like X and 
X, (a|an) Y X, _RB known as Y _NN, X and other Y 
X, Y X ( Y ) Y, including X, 
Y, or X Y such as X Y, such as X 
X is a Y X, _RB called Y Y, especially X 
 
774
relationships. Six sets were extracted for the pat-
tern-based approach and five sets for the co-
occurrence approach (the 15GB corpus was too 
large to process using the co-occurrence model ? 
see dependency parsing time estimates in Table 2). 
From each resulting set, we then randomly se-
lected 50 words along with their top 3 highest 
ranking is-a relationships. For example, Table 4 
shows three randomly selected names for the pat-
tern-based system on the 15GB dataset. For each 
word, we added to the list of hypernyms a human 
generated hypernym (obtained from an annotator 
looking at the word without any system or Word-
Net hyponym). We also appended the WordNet 
hypernyms for each word (only for the top 3 
senses). Each of the 11 random samples contained 
a maximum of 350 is-a relationships to manually 
evaluate (50 random words with top 3 system, top 
3 WordNet, and human generated relationship). 
We presented each of the 11 random samples to 
two human judges. The 50 randomly selected 
words, together with the system, human, and 
WordNet generated is-a relationships, were ran-
domly ordered. That way, there was no way for a 
judge to know the source of a relationship nor each 
system?s ranking of the relationships. For each 
relationship, we asked the judges to assign a score 
of correct, partially correct, or incorrect. We then 
computed the average precision of the system, 
human, and WordNet on each dataset. We also 
computed the percentage of times a correct rela-
tionship was found in the top 3 is-a relationships of 
a word and the mean reciprocal rank (MRR). For 
each word, a system receives an MRR score of 1 / 
M, where M is the rank of the first name judged 
correct. Table 5 shows the results comparing the 
two automatic systems. Table 6 shows similar 
results for a more lenient evaluation where both 
correct and partially correct are judged correct. 
For small datasets (below 150MB), the pattern-
based method achieves higher precision since the 
co-occurrence method requires a certain critical 
mass of statistics before it can extract useful class 
signatures (see Section 3). On the other hand, the 
pattern-based approach has relatively constant 
precision since most of the is-a relationships se-
lected by it are fired by a single pattern. Once the 
co-occurrence system reaches its critical mass (at 
150MB), it generates much more precise hypo-
nyms. The Kappa statistics for our experiments 
were all in the range 0.78 ? 0.85. 
Table 7 and Table 8 compare the precision of the 
pattern-based and co-occurrence-based methods 
with the human and WordNet hyponyms. The 
variation between the human and WordNet scores 
across both systems is mostly due to the relative 
cleanliness of the tokens in the co-occurrence-
based system (due to the parser used in the ap-
proach). WordNet consistently generated higher 
precision relationships although both algorithms 
approach WordNet quality on 6GB (the pattern-
based algorithm even surpasses WordNet precision 
on 15GB). Furthermore, WordNet only generated a 
hyponym 40% of the time. This is mostly due to 
the lack of proper noun coverage in WordNet. 
On the 6 GB corpus, the co-occurrence approach 
took approximately 47 single Pentium-4 2.5 GHz 
processor days to complete, whereas it took the 
pattern-based approach only four days to complete 
on 6 GB and 10 days on 15 GB. 
5.3 Recall 
The co-occurrence model has higher precision 
than the pattern-based algorithm on most datasets. 
Table 4. Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset). 
RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) 
Sanwa Bank bank none subsidiary / lender / bank 
MCI Worldcom Inc. telecommunications company none phone company / competitor / company 
cappuccino beverage none item / food / beverage 
 
Table 5. Average precision, top-3 precision, and MRR 
for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 
15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 
150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 
1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 
6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 
15GB 55.9% 54.0% 52.0% Too large to process 
 
Table 6. Lenient average precision, top-3 precision, 
and MRR for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 
15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 
150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 
1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 
6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 
15GB 67.8% 67.0% 65.0% Too large to process 
 
775
However, Figure 2 shows that the pattern-based 
approach extracts many more relationships. 
Semantic extraction tasks are notoriously diffi-
cult to evaluate for recall. To approximate recall, 
we defined a relative recall measure and conducted 
a question answering (QA) task of answering defi-
nition questions. 
5.3.1 Relative recall 
Although it is impossible to know the number of 
is-a relationships in any non-trivial corpus, it is 
possible to compute the recall of a system relative 
to another system?s recall. The recall of a system 
A, RA, is given by the following formula: 
 
C
C
R AA =  
where CA is the number of correct is-a relation-
ships extracted by A and C is the total number of 
correct is-a relationships in the corpus. We define 
relative recall of system A given system B, RA,B, as: 
 
B
A
B
A
BA C
C
R
R
R ==
,
 
Using the precision estimates, PA, from the pre-
vious section, we can estimate CA ? PA ? |A|, where 
A is the total number of is-a relationships discov-
ered by system A. Hence, 
 
BP
AP
R
B
A
BA
?
?
=
,
 
Figure 3 shows the relative recall of A = pattern-
based approach relative to B = co-occurrence 
model. Because of sparse data, the pattern-based 
approach has much higher precision and recall (six 
times) than the co-occurrence approach on the 
small 15MB dataset. In fact, only on the 150MB 
dataset did the co-occurrence system have higher 
recall. With datasets larger than 150MB, the co-
occurrence algorithm reduces its running time by 
filtering out grammatical relationships for words 
that occurred fewer than k = 40 times and hence 
recall is affected (in contrast, the pattern-based 
approach may generate a hyponym for a word that 
it only sees once). 
5.3.2 Definition questions 
Following Fleischman et al (2003), we select 
the 50 definition questions from the TREC2003 
(Voorhees 2003) question set. These questions are 
of the form ?Who is X?? and ?What is X?? For 
each question (e.g., ?Who is Niels Bohr??, ?What 
is feng shui??) we extract its respective instance 
(e.g., ?Neils Bohr? and ?feng shui?), look up their 
corresponding hyponyms from our is-a table, and 
present the corresponding hyponym as the answer. 
We compare the results of both our systems with 
WordNet. We extract at most the top 5 hyponyms 
provided by each system. We manually evaluate 
the three systems and assign 3 classes ?Correct 
(C)?, ?Partially Correct (P)? or ?Incorrect (I)? to 
each answer. 
This evaluation is different from the evaluation 
performed by the TREC organizers for definition 
questions. However, by being consistent across all 
Total Number of Is-A Relationships vs. Dataset
0
200000
400000
600000
800000
1000000
1200000
1400000
1.5MB 15MB 150MB 1.5GB 6GB 15GB
Datasets
To
ta
l I
s-
A 
Re
la
tio
n
s
hi
ps
s
Pattern-based System
Co-occurrence-based System
Figure 2. Number of is-a relationships extracted by 
the pattern-based and co-occurrence-based approaches. 
 
Table 7. Average precision of the pattern-based sys-
tem vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Pat. WNet Human Pat. WNet Human 
1.5MB
 
38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 
15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 
150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 
1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 
6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 
15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% 
 
Table 8. Average precision of the co-occurrence-
based system vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Co-occ WNet Human Co-occ WNet Human 
1.5MB
 
4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 
15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 
150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 
1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 
6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% 
 
Relative Recall (Pattern-based vs. Co-occurrence-based)
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1.5MB 15MB 150MB 1.5GB 6GB 15GB
(projected)
Datesets
Re
la
tiv
e 
Re
ca
ll
Figure 3. Relative recall of the pattern-based approach 
relative to the co-occurrence approach. 
 
 
776
systems during the process, these evaluations give 
an indication of the recall of the knowledge base. 
We measure the performance on the top 1 and the 
top 5 answers returned by each system. Table 9 
and Table 10 show the results. 
The corresponding scores for WordNet are 38% 
accuracy in both the top-1 and top-5 categories (for 
both strict and lenient). As seen in this experiment, 
the results for both the pattern-based and co-
occurrence-based systems report very poor per-
formance for data sets up to 150 MB. However, 
there is an increase in performance for both sys-
tems on the 1.5 GB and larger datasets. The per-
formance of the system in the top 5 category is 
much better than that of WordNet (38%). There is 
promise for increasing our system accuracy by re-
ranking the outputs of the top-5 hypernyms. 
6 Conclusions 
There is a long standing need for higher quality 
performance in NLP systems. It is possible that 
semantic resources richer than WordNet will en-
able them to break the current quality ceilings. 
Both statistical and symbolic NLP systems can 
make use of such semantic knowledge. With the 
increased size of the Web, more and more training 
data is becoming available, and as Banko and Brill 
(2001) showed, even rather simple learning algo-
rithms can perform well when given enough data. 
In this light, we see an interesting need to de-
velop fast, robust, and scalable methods to mine 
semantic information from the Web. This paper 
compares and contrasts two methods for extracting 
is-a relations from corpora. We presented a novel 
pattern-based algorithm, scalable to the terascale, 
which outperforms its more informed syntactical 
co-occurrence counterpart on very small and very 
large data. 
Albeit possible to successfully apply linguisti-
cally-light but data-rich approaches to some NLP 
applications, merely reporting these results often 
fails to yield insights into the underlying theories 
of language at play. Our biggest challenge as we 
venture to the terascale is to use our new found 
wealth not only to build better systems, but to im-
prove our understanding of language. 
References  
Banko, M. and Brill, E. 2001. Mitigating the paucity of data problem.  
In Proceedings of HLT-2001. San Diego, CA. 
Berland, M. and E. Charniak, 1999. Finding parts in very large 
corpora. In ACL-1999. pp. 57?64. College Park, MD. 
Brill, E., 1995. Transformation-based error-driven learning and 
natural language processing: A case study in part of speech 
tagging. Computational Linguistics, 21(4):543?566. 
Brill, E.; Lin, J.; Banko, M.; Dumais, S.; and Ng, A. 2001. Data-
intensive question answering. In Proceedings of the TREC-10 
Conference, pp 183?189. Gaithersburg, MD. 
Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled 
noun hierarchy from text. In Proceedings of ACL-99. pp 120?126, 
Baltimore, MD. 
Curran, J. and Moens, M. 2002. Scaling context space. In Proceedings 
of ACL-02. pp 231?238, Philadelphia, PA. 
Dunning, T. 1993. Accurate methods for the statistics of surprise and 
coincidence. Computational Linguistics 191 (1993), 61?74. 
Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; Popescu, A.M.; 
Shaked, T.; Soderland, S.; Weld, D. S.; and Yates, A. 2004. Web-
scale information extraction in Know-It All (Preliminary Results). 
To appear in the Conference on WWW. 
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies 
for online question answering: Answering questions before they are 
asked. In Proceedings of ACL-03. pp. 1?7. Sapporo, Japan. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic 
constraints for the automatic discovery of part-whole relations. In 
Proceedings of HLT/NAACL-03. pp. 80?87. Edmonton, Canada. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. pp. 
26?47. 
Hearst, M. 1992. Automatic acquisition of hyponyms from large text 
corpora. In COLING-92. pp. 539?545. Nantes, France. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268?275. Pittsburgh, PA. 
Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based 
parser. Proceedings of COLING-94. pp. 42?48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar words. In 
Proceedings of COLING/ACL-98. pp. 768?774. Montreal, Canada. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question 
Answering. SemaNet? 02: Building and Using Semantic Networks, 
Taipei, Taiwan. 
Miller, G. 1990. WordNet: An online lexical database. International 
Journal of Lexicography, 3(4). 
Och, F.J. and Ney, H. 2002. Discriminative training and maximum 
entropy models for statistical machine translation. In Proceedings 
of ACL. pp. 295?302. Philadelphia, PA. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of SIGKDD-02. pp. 613?619. Edmonton, Canada. 
Pantel, P. and Ravichandran, D. 2004. Automatically labeling seman-
tic classes. In Proceedings of HLT/NAACL-04. pp. 321?328. Bos-
ton, MA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-1997. 
Voorhees, E. 2003. Overview of the question answering track. In 
Proceedings of TREC-12 Conference. NIST, Gaithersburg, MD. 
Table 9. QA definitional evaluations for pattern-based 
system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 2.0% 2.0% 2.0% 2.0% 
1.5GB 16.0% 22.0% 20.0% 22.0% 
6GB 38.0% 52.0% 56.0% 62.0% 
15GB 38.0% 52.0% 70.0% 74.0% 
 
Table 10. QA definitional evaluations for co-
occurrence-based system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 0% 0% 0% 0% 
1.5GB 6.0% 8.0% 6.0% 8.0% 
6GB 36.0% 44.0% 60.0% 62.0% 
 
777
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 161?170, Prague, June 2007. c?2007 Association for Computational Linguistics
LEDIR: An Unsupervised Algorithm for Learning Directionality of Inference Rules 
Rahul Bhagat, Patrick Pantel, Eduard Hovy Information Sciences Institute University of Southern California Marina del Rey, CA {rahul,pantel,hovy}@isi.edu   Abstract Semantic inference is a core component of many natural language applications. In re-sponse, several researchers have developed algorithms for automatically learning infer-ence rules from textual corpora. However, these rules are often either imprecise or un-derspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identi-fies the directionality of correct ones. Based on an extension to Harris?s distribu-tional hypothesis, we use selectional pref-erences to gather evidence of inference di-rectionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 1 Introduction Paraphrases are textual expressions that convey the same meaning using different surface forms. Tex-tual entailment is a similar phenomenon, in which the presence of one expression licenses the validity of another. Paraphrases and inference rules are known to improve performance in various NLP applications like Question Answering (Harabagiu and Hickl 2006), summarization (Barzilay et al 1999) and Information Retrieval (Anick and Tipir-neni 1999).  Paraphrase and entailment involve inference rules that license a conclusion when a premise is given.  Deciding whether a proposed inference rule is fully valid is difficult, however, and most NL systems instead focus on plausible inference.  In this case, one statement has some likelihood of 
being identical in meaning to, or derivable from, the other.  In the rest of this paper we discuss plau-sible inference only.   Given the importance of inference, several re-searchers have developed inference rule collec-tions. While manually built resources like Word-Net (Fellbaum 1998) and Cyc (Lenat 1995) have been around for years, for coverage and domain adaptability reasons many recent approaches have focused on automatic acquisition of paraphrases (Barzilay and McKeown 2001) and inference rules (Lin and Pantel 2001; Szpektor et al 2004). The downside of these approaches is that they often result in incorrect inference rules or in inference rules that are underspecified in directionality (i.e. asymmetric but are wrongly considered symmet-ric). For example, consider an inference rule from DIRT (Lin and Pantel 2001): X eats Y ? X likes Y  (1)   All rules in DIRT are considered symmetric. Though here, one is most likely to infer that ?X eats Y? ? ?X likes Y?, because if someone eats something, he most probably likes it1, but if he likes something he might not necessarily be able to eat it. So for example, given the sentence ?I eat spicy food?, one is mostly likely to infer that ?I like spicy food?. On the other hand, given the sentence ?I like rollerblading?, one cannot infer that ?I eat rollerblading?. In this paper, we propose an algorithm called LEDIR (pronounced ?leader?) for LEarning Di-rectionality of Inference Rules. Our algorithm fil-ters incorrect inference rules and identifies the di-rectionality of the correct ones. Our algorithm                                                 1 There could be certain usages of ?X eats Y? where, one might not be able to infer ?X likes Y? (for example meta-phorical). But, in most cases, this inference holds. 
161
works with any resource that produces inference rules of the form shown in example (1). We use both the distributional hypothesis and selectional preferences as the basis for our algorithm. We pro-vide empirical evidence to validate the following main contribution:  Claim: Relational selectional preferences can be used to automatically determine the plausibility and directionality of an inference rule. 2 Related Work In this section, we describe applications that can benefit by using inference rules and their direc-tionality.  We then talk about some previous work in this area. 2.1 Applications Open domain question answering approaches often cast QA as the problem of finding some kind of semantic inference between a question and its an-swer(s) (Moldovan et al 2003; Echiabi and Marcu 2003). Harabagiu and Hickl (2006) recently dem-onstrated that textual entailment inference informa-tion, which in this system is a set of directional inference relations, improves the performance of a QA system significantly even without using any other form of semantic inference. This evidence supports the idea that learning the directionality of other sets of inference rules may improve QA per-formance.  In Multi-Document Summarization (MDS), paraphrasing is useful for determining sentences that have similar meanings (Barzilay et al 1999). Knowing the directionality between the inference rules here could allow the MDS system to choose either the more specific or general sentence de-pending on the purpose of the summary. In IR, paraphrases have been used for query ex-pansion, which is known to promote effective re-trieval (Anick and Tipirneni 1999). Knowing the directionality of rules here could help in making a query more general or specific depending on the user needs. 2.2 Learning Inference Rules Automatically learning paraphrases and inference rules from text is a topic that has received much attention lately. Barzilay and McKeown (2001) for paraphrases, DIRT (Lin and Pantel 2001) and TEASE (Szpektor et al 2004) for inference rules, 
are recent approaches that have achieved promis-ing results. While all these approaches produce collections of inference rules that have good recall, they suffer from the complementary problem of low precision. They also make no attempt to dis-tinguish between symmetric and asymmetric infer-ence rules. Given the potential positive impact shown in Section 2.1 of learning the directionality of inference rules, there is a need for methods, such as the one we present, to improve existing automatically created resources. 2.3 Learning Directionality There have been a few approaches at learning the directionality of restricted sets of semantic rela-tions, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmet-ric relations between verbs. They manually exam-ined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candi-date verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selec-tional preferences of a single verb, i.e. the semantic types of a verb?s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrences between verbs and nouns to learn the verb-verb inference rules. Like the previous two methods, their approach too deals only with verbs and is lim-ited to learning inference rules that are temporal in nature. Geffet and Dagan (2005) proposed an extension to the distributional hypothesis to discover entail-ment relation between words. They model the con-text of a word using its syntactic features and com-pare the contexts of two words for strict inclusion to infer lexical entailment. In principle, their work is the most similar to ours. Their method however 
162
is limited to lexical entailment and they show its effectiveness for nouns. Our method on the other hand deals with inference rules between binary relations and includes inference rules between ver-bal relations, non-verbal relations and multi-word relations. Our definition of context and the meth-odology for obtaining context similarity and over-lap is also much different from theirs. 3 Learning Directionality of Inference Rules The aim of this paper is to filter out incorrect infer-ence rules and to identify the directionality of the correct ones. Let pi ? pj be an inference rule where each p is a binary semantic relation between two entities x and y. Let <x, p, y> be an instance of relation p. Formal problem definition: Given the inference rule pi ? pj, we want to conclude which one of the following is more appropriate: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference Consider the example (1) from section 1. There, it is most plausible to conclude  ?X eats Y? ? ?X likes Y?.  Our algorithm LEDIR uses selectional prefer-ences along the lines of Resnik (1996) and Pantel et al (2007) to determine the plausibility and di-rectionality of inference rules. 3.1 Underlying Assumption Many approaches to modeling lexical semantics have relied on the distributional hypothesis (Harris 1954), which states that words that appear in the same contexts tend to have similar meanings. The idea is that context is a good indicator of a word meaning. Lin and Pantel (2001) proposed an exten-sion to the distributional hypothesis and applied it to paths in dependency trees, where if two paths tend to occur in similar contexts it is hypothesized that the meanings of the paths tend to be similar. In this paper, we assume and propose a further extension to the distributional hypothesis and call it the ?Directionality Hypothesis?. Directionality Hypothesis: If two binary semantic relations tend to occur in similar contexts and the first one occurs in significantly more contexts than 
the second, then the second most likely implies the first and not vice versa. The intuition here is that of generality. The more general a relation, more the types (and number) of contexts in which it is likely to appear. Consider the example (1) from section 1. The fact is that there are many more things that someone might like than those that someone might eat. Hence, by applying the directionality hypothesis, one can in-fer that ?X eats Y? ? ?X likes Y?. The key to applying the distributional hypothe-sis to the problem at hand is to model the contexts appropriately and to introduce a measure for calcu-lating context similarity. Concepts in semantic space, due to their abstractive power, are much richer for reasoning about inferences than simple surface words. Hence, we model the context of a relation p of the form <x, p, y> by using the seman-tic classes C(x) and C(y) of words that can be in-stantiated for x and y respectively. To measure context similarity of two relations, we calculate the overlap coefficient (Manning and Sch?tze, 1999) between their contexts. 3.2 Selectional Preferences The selectional preferences of a predicate is the set of semantic classes that its arguments can belong to (Wilks 1975). Resnik (1996) gave an informa-tion theoretical formulation of the idea. Pantel et al (2007) extended this idea to non-verbal rela-tions by defining the relational selectional prefer-ences (RSPs) of a binary relation p as the set of semantic classes C(x) and C(y) of words that can occur in positions x and y respectively. The set of semantic classes C(x) and C(y) can be obtained either from a manually created taxonomy like WordNet as proposed in the above previous approaches or by using automatically generated classes from the output of a word clustering algo-rithm as proposed in Pantel et al (2007). For ex-ample given a relation like ?X likes Y?, its RSPs from WordNet could be {individual, so-cial_group?} for X and {individual, food, activ-ity?} for Y. In this paper, we deployed both the Joint Rela-tional Model (JRM) and Independent Relational Model (IRM) proposed by Pantel et al (2007) to obtain the selectional preferences for a relation p.   
163
Model 1: Joint Relational Model (JRM) The JRM uses a large corpus to learn the selec-tional preferences of a binary semantic relation by considering its arguments jointly. Given a relation p and large corpus of English text, we first find all occurrences of relation p in the corpus. For every instance <x, p, y> in the cor-pus, we obtain the sets C(x) and C(y) of the seman-tic classes that x and y belong to. We then accumu-late the frequencies of the triples <c(x), p, c(y)> by assuming that every c(x) ? C(x) can co-occur with every  c(y) ? C(y) and vice versa. Every triple <c(x), p, c(y)> obtained in this manner is a candi-date selectional preference for p. Following Pantel et al (2007), we rank these candidates using Pointwise mutual information (Cover and Thomas 1991). The ranking function is defined as the strength of association between two semantic classes, cx and cy2, given the relation p: 
? 
pmi c
x
p; c
y
p
( )
= log
P c
x
,c
y
p
( )
P c
x
p
( )
P c
y
p
( )
                   (3.1) 
Let |cx, p, cy| denote the frequency of observing the instance <c(x), p, c(y)>. We estimate the prob-abilities of Equation 3.1 using maximum likeli-hood estimates over our corpus: 
? 
P c
x
p
( )
=
c
x
, p,?
?, p,?
P c
y
p
( )
=
?, p,c
y
?, p,?
P c
x
,c
y
p
( )
=
c
x
, p,c
y
?, p,?
                 (3.2) 
We estimate the above frequencies using: 
  
? 
c
x
, p,? =
w, p,?
C w
( )
w?c
x
?
?, p,c
y
=
?, p,w
C w
( )
w?c
y
?
c
x
, p,c
y
=
w
1
, p,w
2
C w
1
( )
? C w
2
( )
w
1
?c
x
,w
2
?c
y
?
       (3.3) 
where |x, p, y| denotes the frequency of observing the instance <x, p, y> and |C(w)| denotes the num-ber of classes to which word w belongs. |C(w)| dis-tributes w?s mass equally among all of its senses C(w). Model 2: Independent Relational Model (IRM) Due to sparse data, the JRM is likely to miss some pair(s) of valid relational selectional preferences. Hence we use the IRM, which models the argu-ments of a binary semantic relation independently.                                                 2 cx and cy are shorthand for c(x) and c(y) in our equations. 
Similar to JRM, we find all instances of the form <x, p, y> for a relation p. We then find the sets C(x) and C(y) of the semantic classes that x and y belong to and accumulate the frequencies of the triples <c(x), p, *> and <*, p, c(y)> where c(x) ? C(x) and  c(y) ? C(y). All the tuples <c(x), p, *> and <*, p, c(y)> are the independent candidate RSPs for a relation p and we rank them according to equation 3.3. Once we have the independently learnt RSPs, we need to convert them into a joint representation for use by the inference plausibility and direction-ality model. To do this, we obtain the Cartesian product between the sets <C(x), p, *>  and <*, p, C(y)> for a relation p. The Cartesian product be-tween two sets A and B is given by: 
?
A ? B = a,b
( )
:?a? A and ?b? B
{ }
        (3.4) Similarly we obtain: 
? 
C
x
, p,? ? ?, p,C
y
=
c
x
, p,c
y
: ? c
x
, p,? ? C
x
, p,? and
? ?, p,c
y
? ?, p,C
y
? 
? 
? 
? 
? 
? 
? 
? 
? 
? 
 (3.5) 
The Cartesian product in equation 3.5 gives the joint representation of the RSPs of the relation p learnt using IRM. In the joint representation, the IRM RSPs have the form <c(x), p, c(y)>  which is the same form as the JRM RSPs. 3.3 Inference plausibility and directionality model Our model for determining inference plausibility and directionality is based on the intuition that for an inference to hold between two semantic rela-tions there must exist sufficient overlap between their contexts and the directionality of the infer-ence depends on the quantitative comparison be-tween their contexts. Here we model the context of a relation by the selectional preferences of that relation. We deter-mine the plausibility of an inference based on the overlap coefficient (Manning and Sch?tze, 1999) between the selectional preferences of the two paths. We determine the directionality based on the difference in the number of selectional preferences of the relations when the inference seems plausi-ble.  Given a candidate inference rule pi ? pj, we first obtain the RSPs <C(x), pi, C(y)>  for pi and <C(x), pj, C(y)> for pj.  We then calculate the over-lap coefficient between their respective RSPs. Overlap coefficient is one of the many distribu-
164
tional similarity measures used to calculate the similarity between two vectors A and B: 
? 
sim A,B
( )
=
A? B
min A , B
( )
           (3.6) 
The overlap coefficient between the selectional preferences of pi and pj is calculated as: 
? 
sim p
i
, p
j
( )
=
C
x
, p
i
,C
y
? C
x
, p
j
,C
y
min C
x
, p
i
,C
y
,C
x
, p
j
,C
y
( )
          (3.7) 
If sim(pi,pj) is above a certain empirically de-termined threshold ? (?1), we conclude that the inference is plausible, i.e.: If  
? 
sim p
i
,p
j
( )
??  we conclude the inference is plausible else  we conclude the inference is not plausible For a plausible inference, we then compute the ratio between the number of selectional prefer-ences |C(x), pi, C(y)|  for pi and |C(x), pj, C(y)| for pj and compare it against an empirically determined threshold ? (?1) to determine the direction of in-ference. So the algorithm is: If   
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
? ?       we conclude pi ? pj 
else if  
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
?
1?     we conclude pi ? pj else                 we conclude pi ? pj 4 Experimental Setup In this section, we describe our experimental setup to validate our claim that LEDIR can be used to determine plausibility and directionality of an in-ference rule. Given an inference rule of the form pi ? pj, we want to use automatically learned relational selec-tional preferences to determine whether the infer-ence rule is valid and if it is valid then what its di-rectionality is.  4.1 Inference Rules LEDIR can work with any set of binary semantic inference rules. For the purpose of this paper, we chose the inference rules from the DIRT resource (Lin and Pantel 2001). DIRT consists of 12 million rules extracted from 1GB of newspaper text (AP Newswire, San Jose Mercury and Wall Street 
Journal). For example, ?X eats Y? ? ?X likes Y? is an inference rule from DIRT. 4.2 Semantic Classes Appropriate choice of semantic classes is crucial for learning relational selectional preferences. The ideal set should have semantic classes that have the right balance between abstraction and discrimina-tion, the two important characteristics that are of-ten conflicting. A very general class has limited discriminative power, while a very specific class has limited abstractive power. Finding the right balance here is a separate research problem of its own. Since the ideal set of universally acceptable se-mantic classes in unavailable, we decided to use the Pantel et al (2007) approach of using two sets of semantic classes. This approach gave us the ad-vantage of being able to experiment with sets of classes that vary a lot in the way they are generated but try to maintain the granularity by obtaining approximately the same number of classes. The first set of semantic classes was obtained by running the CBC clustering algorithm (Pantel and Lin, 2002) on TREC-9 and TREC-2002 newswire collections consisting of over 600 million words. This resulted in 1628 clusters, each representing a semantic class. The second set of semantic classes was obtained by using WordNet 2.1 (Fellbaum 1998). We ob-tained a cut in the WordNet noun hierarchy3 by manual inspection and used all the synsets below a cut point as the semantic class at that node. Our inspection showed that the synsets at depth four formed the most natural semantic classes4. A cut at depth four resulted in a set of 1287 semantic classes, a set that is much coarser grained than WordNet which has an average depth of 12. This seems to be a depth that gives a reasonable abstrac-tion while maintaining good discriminative power. It would however be interesting to experiment with more sophisticated algorithms for extracting se-mantic classes from WordNet and see their effect 
                                                3 Since we are dealing with only noun binary relations, we use only WordNet noun Hierarchy. 4 By natural, here, we simply mean that a manual inspection by the authors showed that, at depth four, the resulting clus-ters had struck a better granularity balance than other cutoff points. We acknowledge that this is a very coarse way of ex-tracting concepts from WordNet. 
165
on the relational selectional preferences, something we do not address this in this paper. 4.3 Implementation We implemented LEDIR with both the JRM and IRM models using inference rules from DIRT and semantic classes from both CBC and WordNet. We parsed the 1999 AP newswire collection consisting of 31 million words with Minipar (Lin 1993) and used this to obtain the probability statistics for the models (as described in section 3.2).  We performed both system-wide evaluations and intrinsic evaluations with different values of ? and ? parameters. Section 5 presents these results and our error analysis. 4.4 Gold Standard Construction In order to evaluate the performance of the differ-ent systems, we compare their outputs against a manually annotated gold standard. To create this gold standard, we randomly sampled 160 inference rules of the form pi ? pj from DIRT. We discarded three rules since they contained nominalizations5.  For every inference rule of the form pi ? pj, the annotation guideline asked annotators (in this pa-per we used two annotators) to choose the most appropriate of the four options: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference To help the annotators with their decisions, the annotators were provided with 10 randomly chosen instances for each inference rule. These instances, extracted from DIRT, provided the annotators with context where the inference could hold. So for ex-ample, for the inference rule ?X eats Y? ? ?X likes Y?, an example instance would be ?I eat spicy food? ? ?I like spicy food?. The annotation guide-line however gave the annotators the freedom to think of examples other than the ones provided to make their decisions. The annotators found that while some decisions were quite easy to make, the more complex ones                                                 5 For the purpose of simplicity, we in our experiments did not use DIRT rules containing nominalizations. The algo-rithm however can be applied without change to inference rules containing nominalization. In fact, in the resource that we plan to release soon, we have applied the algorithm without change to DIRT rules containing nominalizations. 
often involved the choice between bi-directionality and one of the directions. To minimize disagree-ments and to get a better understanding of the task, the annotators trained themselves by annotating several samples together. We divided the set of 157 inference rules, into a development set of 57 inference rules and a blind test set of 100 inference rules. Our two annotators annotated the development test set together to train themselves. The blind test set was then annotated individually to test whether the task is well de-fined. We used the kappa statistic (Siegel and Castellan Jr. 1988) to calculate the inter-annotator agreement, resulting in ?=0.63. The annotators then looked at the disagreements together to build the final gold standard. All this resulted in a final gold standard of 100 annotated DIRT rules. 4.5 Baselines To get an objective assessment of the quality of the results obtained by using our models, we compared the output of our systems against three baselines: B-random: Randomly assigns one of the four pos-sible tags to each candidate inference rule.  B-frequent: Assigns the most frequently occurring tag in the gold standard to each candidate infer-ence rule. B-DIRT: Assumes each inference rule is bidirec-tional and assigns the bidirectional tag to each candidate inference rule. 5 Experimental Results In this section, we provide empirical evidence to validate our claim that the plausibility and direc-tionality of an inference rule can be determined using LEDIR. 5.1 Evaluation Criterion We want to measure the effectiveness of LEDIR for the task of determining the validity and direc-tionality of a set of inference rules. We follow the standard approach of reporting system accuracy by comparing system outputs on a test set with a manually created gold standard. Using the gold standard described in Section 4.4, we measure the accuracy of our systems using the following for-mula: 
166
erencesinput
erencestaggedcorrectly
Accuracy
inf
inf
=
 
5.2 Result Summary We ran all our algorithms with different parameter combinations on the development set (the 57 DIRT rules described in Section 4.4). This resulted in a total of 420 experiments on the development set. Based on these experiments, we used the accuracy statistic to obtain the best parameter combination for each of our four systems. We then used these parameter values to obtain the corresponding per-centage accuracies on the test set for each of the four systems. Model ?  ? Accuracy (%) B-random - - 25 B-frequent - - 34 B-DIRT - - 25 CBC 0.15 2 38 JRM WN 0.55 2 38 CBC 0.15 3 48 IRM WN 0.45 2 43 Table 1: Summary of results on the test set Table 1 summarizes the results obtained on the test set for the three baselines and for each of the four systems using the best parameter combina-tions obtained as described above. The overall best performing system uses the IRM algorithm with RSPs form CBC. Its performance is found to be significantly better than all the three baselines us-ing the Student?s paired t-test (Manning and Sch?tze, 1999) at p<0.05. However, this system is not statistically significant when compared with the other LEDIR implementations (JRM and IRM with WordNet). 5.3 Performance and Error Analysis The best performing system selected using the de-velopment set is the IRM system using CBC with the parameters ?=0.15 and ?=3. In general, the results obtained on the test set show that the IRM tends to perform better than the JRM. This obser-vation points at the sparseness of data available for learning RSPs for the more restrictive JRM, the reason why we introduced the IRM in the first place. A much larger corpus would be needed to obtain good enough coverage for the JRM. 
GOLD STANDARD  ? ? ? NO ? 16 1 3 7 ? 0 3 1 3 ? 7 4 22 15 SYST
EM 
NO 2 3 4 9 Table 2: Confusion Matrix for the best performing system, IRM using CBC with ?=0.15 and ?=3. Table 2 shows the confusion matrix for the overall best performing system as selected using the development set (results are taken from the test set). The confusion matrix indicates that the system does a very good job of identifying the directional-ity of the correct inference rules, but gets a big per-formance hit from its inability to identify the incor-rect inference rules accurately. We will analyze this observation in more detail below. Figure 1 plots the variation in accuracy of IRM with different RSPs and different values of ? and ?. The figure shows a very interesting trend.  It is clear that for all values of ?, systems for IRM us-ing CBC tend to reach their peak in the range 0.15 ? ? ? 0.25, whereas the systems for IRM using WordNet (WN), tend to reach their peak in the range 0.4 ? ? ? 0.6. This variation indicates the kind of impact the selection of semantic classes could have on the overall performance of the sys-tem. This is not hard evidence, but it does suggest that finding the right set of semantic classes could be one big step towards improving system accu-racy. 
 Figure 1: Accuracy variation for IRM with differ-ent values of ? and ?. Two other factors that have a big impact on the performance of our systems are the values of the system parameters ? and ?, which decide the plau-
167
sibility and directionality of an inference rule, re-spectively. To better study their effect on the sys-tem performances, we studied the two parameters independently. 
 Figure 2: Accuracy variation in predicting correct versus incorrect inference rules for different values of ?. 
 Figure 3: Accuracy variation in predicting direc-tionality of correct inference rules for different values of ?. Figure 2 shows the variation in the accuracy for the task of predicting the correct and incorrect in-ference rules for the different systems when vary-ing the value of ?. To obtain this graph, we classi-fied the inference rules in the test set only as cor-rect and incorrect without further classification based on directionality. All of our four systems obtained accuracy scores in the range of 68-70% showing a good performance on the task of deter-mining plausibility. This however is only a small improvement over the baseline score of 66% ob-tained by assuming every inference to be plausible (as will be shown below, our system has most im-pact not on determining plausibility but on deter-
mining directionality). Manual inspection of some system errors showed that the most common errors were due to the well-known ?problem of an-tonymy? when applying the distributional hypothe-sis. In DIRT, one can learn rules like ?X loves Y? ? ?X hates Y?. Since the plausibility of inference rules is determined by applying the distributional hypothesis and the antonym paths tend to take the same set of classes for X and Y, our models find it difficult to filter out the incorrect inference rules which DIRT ends up learning for this very same reason. To improve our system, one avenue of re-search is to focus specifically on filtering incorrect inference rules involving antonyms (perhaps using methods similar to (Lin et al 2003)). Figure 3 shows the variation in the accuracy for the task of predicting the directionality of the cor-rect inference rules for the different systems when varying the value of ?.  To obtain this graph, we separated the correct inference rules form the in-correct ones and ran all the systems on only the correct ones, predicting only the directionality of each rule for different values of ?. Too low a value of ? means that the algorithms tend to predict most things as unidirectional and too high a value means that the algorithms tend to predict everything as bidirectional. It is clear from the figure that the performance of all the systems reach their peak performance in the range 2 ? ? ? 4, which agrees with our intuition of obtaining the best system ac-curacy in a medium range. It is also seen that the best accuracy for each of the models goes up as compared to the corresponding values obtained in the general framework. The best performing sys-tem, IRM using CBC RSPs, reaches a peak accu-racy of 63.64%, a much higher score than its accu-racy score of 48% under the general framework and also a significant improvement over the base-line score of 48.48% for this task. Paired t-test shows that the difference is statistically significant at p<0.05. The baseline score for this task is ob-tained by assigning the most frequently occurring direction to all the correct inference rules. This paints a very encouraging picture about the ability of the algorithm to identify the directionality much more accurately if it can be provided with a cleaner set of inference rules. 
168
6 Conclusion Semantic inferences are fundamental to under-standing natural language and are an integral part of many natural language applications such as question answering, summarization and textual entailment. Given the availability of large amounts of text and with the increase in computation power, learning them automatically from large text cor-pora has become increasingly feasible and popular. We introduced the Directionality Hypothesis, which states that if two paths share a significant number of relational selectional preferences (RSPs) and if the first has many more RSPs than the second, then the second path implies the first. Our experiments show empirical evidence that the Directionality Hypothesis with RSPs can indeed be used to filter incorrect inference rules and find the directionality of correct ones. We believe that this result is one step in the direction of solving the basic problem of semantic inference. Several questions must still be addressed. The models need to be improved in order to address the problem of incorrect inference rules. The distribu-tional hypothesis does not provide a framework to address the issue with antonymy relations like ?X loves Y? ? ?X hates Y? and hence other ideas need to be investigated. Ultimately, our goal is to improve the perform-ance of NLP applications with better inferencing capabilities. Several recent data points, such as  (Harabagiu and Hickl 2006), and others discussed in Section 2.1, give promise that refined inference rules for directionality may indeed improve ques-tion answering, textual entailment and multi-document summarization accuracies. It is our hope that methods such as the one proposed in this paper may one day be used to harness the richness of automatically created inference rule resources within large-scale NLP applications. References Anick, P.G. and Tipirneni, S. 1999. The Paraphrase Search Assistant: Terminology Feedback for Iterative Information Seeking. In Proceedings of SIGIR 1999. pp. 53-159. Berkeley, CA Barzilay, R. and McKeown, K.R. 2001.Extracting Para-phrases from a Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, France. 
Barzilay, R.; McKeown, K.R. and Elhadad, M. 1999. Information Fusion in the Context of Multi-Document Summarization. In Proceedings of ACL 1999. College Park, Maryland. Chklovski, T. and Pantel, P. 2004. VerbOCEAN: Min-ing the Web for Fine-Grained Semantic Verb Rela-tions. In Proceedings of EMNLP 2004. Barcellona Spain. Cover, T.M. and Thomas, J.A. 1991. Elements of Infor-mation Theory. John Wiley & Sons. Echihabi, A. and Marcu. D. 2003. A Noisy-Channel Approach to Question Answering. In Proceedings of ACL 2003. Sapporo, Japan. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press. Geffet, M.; Dagan, I. 2005. The Distributional Inclusion Hypothesis and Lexical Entailment. In Proceedings of ACL 2005. pp. 107-114. Ann Arbor, Michigan. Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual Entailment in Open-Domain Question An-swering. In Proceedings of ACL 2006.  pp. 905-912. Sydney, Australia. Harris, Z. 1954. Distributional structure. Word. 10(23): 146-162. Lenat, D. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33?38. Lin, D. 1993. Parsing Without OverGeneration. In Pro-ceedings of  ACL 1993. pp. 112-120. Columbus, OH. Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for Question Answering. Natural Language Engineering 7(4):343-360. Lin, D.; Zhao, S.; Qin, L. and Zhou, M. 2003. Identify-ing Synonyms among Distributionally Similar Words. In Proceedings of IJCAI 2003, pp. 1492-1493. Acapulco, Mexico. Manning, C.D. and Sch?tze, H. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA. Moldovan, D.; Clark, C.; Harabagiu, S. and Maiorano S.  2003. COGEX: A Logic Prover for Question An-swering. In Proceedings of HLT/NAACL 2003. Ed-monton, Canada. Pantel, P.; Bhagat, R.; Coppola, B.; Chklovski, T. and Hovy, E. 2007. ISP: Learning Inferential Selectional Preferences. In Proceedings of HLT/NAACL 2007. Rochester, NY. 
169
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD 2002. pp. 613-619. Edmonton, Canada. Resnik, P. 1996. Selectional Constraints: An Informa-tion-Theoretic Model and its Computational Realiza-tion. Cognition, 61:127?159. Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill. Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP 2004. pp. 41-48. Barce-lona, Spain. Torisawa, K. 2006. Acquiring Inference Rules with Temporal Constraints by Using Japanese Coordi-nated Sentences and Noun-Verb Co-occurances. In Proceedings of HLT/NAACL 2006. pp. 57-64. New York, New York. Wilks, Y. 1975. Preference Semantics.  In E.L. Keenan (ed.), Formal Semantics of Natural Language. Cam-bridge: Cambridge University Press. Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering Asymmetric Entailment Relations between Verbs using Selectional Preferences. In Pro-ceedings of COLING/ACL 2006. pp. 849-856. Syd-ney, Australia.   
170
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 792 ? 803, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Global Path-Based Refinement of Noisy Graphs  
Applied to Verb Semantics 
Timothy Chklovski and Patrick Pantel 
Information Sciences Institute,University of Southern California, 
4676 Admiralty Way,Marina del Rey, CA  90292 
{timc, pantel}@isi.edu 
Abstract. Recently, researchers have applied text- and web-mining algorithms 
to mine semantic resources. The result is often a noisy graph of relations be-
tween words. We propose a mathematically rigorous refinement framework, 
which uses path-based analysis, updating the likelihood of a relation between a 
pair of nodes using evidence provided by multiple indirect paths between the 
nodes. Evaluation on refining temporal verb relations in a semantic resource 
called VERBOCEAN showed a 16.1% error reduction after refinement. 
1   Introduction 
Increasingly, researchers are creating broad-coverage semantic resources by mining 
text corpora [1][5] and the Web [2][6]. These resources typically consist of a noisy 
collection of relations between words. The data is typically extracted on a per link 
basis (i.e., the relation between two nodes is determined without regard to other 
nodes). Yet, little work has taken a global view of the graph of relations, which may 
provide additional information to refine local decisions by identifying inconsistencies, 
updating confidences in specific edges (relations), and suggesting relations between 
additional pairs of nodes. 
For example, observing the temporal verb relations ?discover happens-before re-
fine? and ?refine happens-before exploit? provides evidence for the relation ?discover 
happens-before exploit,? because the happens-before relation is transitive. 
We conceptualize a semantic resource encoding relations between words as a graph 
where words are nodes and binary relations between words are edges. In this paper, we 
investigate the refinement of such graphs by updating the confidence in edges using a 
global analysis relying on link semantics. Our approach is based on the observation that 
some paths (chains of relations) between a pair of nodes xi and xj imply the presence or 
absence of a particular direct relation between xi and xj. Despite each individual path 
being noisy, multiple indirect paths can provide sufficient evidence for adding, remov-
ing, or altering a relation between two nodes. As illustrated by the earlier example, 
inferring a relation based on the presence of an indirect path relies on the semantics of 
the links that make up the path, like transitivity or equivalence classes. 
As an evaluation and a sample practical application, we apply our refinement 
framework to the task of refining the temporal precedence relations in VERBOCEAN,  
a broad-coverage noisy network of semantic relations between verbs extracted by 
mining the Web [2]. Examples of new edges discovered (added) by applying the 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 793 
framework include: ?ascertain happens-before evaluate?, ?approve happens-before 
back?, ?coat happens-before bake?, ?plan happens-before complete?, and ?interrogate 
happens-before extradite?. 
Examples of edges that are removed by applying our framework include: ?induce 
happens-before treat?, ?warm happens-before heat?, ?halve happens-before slice?, 
and ?fly happens-before operate?. 
Experiments show that our framework is particularly good at filtering out the in-
correct temporal relations in VERBOCEAN. Removing incorrect relations is particu-
larly important for inference systems. 
2   VerbOcean 
We apply our path-based refinement framework to VERBOCEAN [2], a web-extracted 
lexical semantics resource with potential applications to a variety of natural language 
tasks such as question answering, information retrieval, document summarization, and 
machine translation. VERBOCEAN is a graph of semantic relations between verbs, with 
3,477 verbs (nodes) and 22,306 relations (edges). Although the framework applies 
whenever some paths through the graph imply presence or absence of a relation, for 
the evaluation we focus on the temporal precedence relation in VERBOCEAN, and, in 
an ancillary role, on the similarity relation. Senses are not discriminated and an edge 
indicates that the relation is believed to hold between some senses of the verbs in this 
relation. 
The five semantic relations present in VERBOCEAN are presented in Table 1. Tem-
poral precedence (happens-before) is a transitive asymmetric temporal relation be-
tween verbs. Similarity is a relation that suggests two nodes are likely to be in the 
same equivalence class, although polysemy makes it only weakly transitive. 
Table 1. Types, examples and frequencies of 22,306 semantic relations in VERBOCEAN 
Semantic Relation Example Transitive Symmetric # in VERBOCEAN 
temporal precedence marry :: divorce Y N 4,205 
similarity produce :: create Y Y 11,515 
strength wound :: kill Y N 4,220 
antonymy open :: close N Y 1,973 
enablement fight :: win Y N 393 
In VERBOCEAN, asymmetric relations between two nodes are enforced to be unidi-
rectional (i.e., presence of an edge xi happens-before xj guarantees absence of an edge 
xj happens-before xi). Larger, inconsistent loops are possible, however, as extraction 
is strictly local. Taking advantage of the global picture to refine the edges of the graph 
can improve quality of the resource, helping performance of any algorithms or appli-
cations that rely on the resource. 
794 T. Chklovski and P. Pantel 
3   Global Refinement 
Our approach relies on a global view of the graph to refine a relation between a given 
pair of nodes xi and xj, based on multiple indirect paths between the two nodes.  The 
analysis processes triples <xi, r, xj> for the relation r to output r, its opposite (which 
we will denote q), or neither. The opposite of happens-before is the same relation in 
the reverse direction (happens-after). The refinement is based on evidence provided 
by indirect paths, over a probabilistic representation of the graph. 
Section 3.1 introduces the steps of the refinement, Section 3.2 details which paths 
are used as evidence, and Section 3.3 derives the statistical model used for combining 
evidence from multiple unreliable paths. 
3.1   Overview of the Refinement Algorithm 
We first introduce some notation. Let Ri,j denote the event that the relation r is present 
between nodes xi and xj in the original graph ? i.e., the graph indicates (perhaps spuri-
ously) the presence of the relation r between xi and xj. Let ri,j denote the relation r 
actually holding between xi and xj. Let ?i,j denote an acyclic path from xi to xj of (pos-
sibly distinct) relations {Ri,i+1 .. Rj-1,j}. For example, the path ?x1 similar x2 happens-
before x3? can be denoted ?1,3. If the edges of ?i,j indicate the relation r between the 
nodes xi and xj, we say that ?i,j indicates ri,j. 
Given a triple <xi, r, xj>, we identify the set ?r full of all paths ?i,j such that ?i,j indi-
cates ri,j and ?i,j?s sequence of relations {Ri,i+1 .. Rj-1,j} matches one of the allowed 
sequences.  That is, we only consider certain path types.  The restriction on types of 
paths considered is introduced because identifying and processing all possible paths 
indicating ri,j is too demanding computationally in a large non-sparse graph. The path 
types considered are detailed in Section 3.2. Note that the intermediate nodes of paths 
can range over the entire graph. 
For each ?i,j in the above set ?r full, we compute the estimated probability that ri,j 
holds given the observation of (relations that make up) ?i,j. Each edge in the input 
graph is treated as a probabilistic one, with probabilities P(ri,j) and P(ri,j|Ri,j) estimated 
from human judgments on a representative sample. Generally, longer paths and paths 
made up of less reliable edges will have lower probabilities. Section 3.3 presents the 
full model for estimating these probabilities. 
Next, we form the set ?r by selecting from ?r full only the paths which have no 
common intermediate nodes. This is done greedily, processing all paths in ?r full in 
order of decreasing score, placing each in ?r iff it does not share any intermediate 
nodes with any path already in ?r. This is done to avoid double-counting the available 
evidence in our framework, which operates assuming conditional independence of 
paths. 
Next, we compute P(ri,j | ?r), the probability of ri,j given the evidence provided by 
the paths in ?r.  The model for computing this is described in Section 3.3. Similarly, 
?q and P(qi,j | ?q) are computed for qi,j, the opposite of ri,j. Next, the evidence for r 
and q are reconciled by computing P(ri,j | ?r, ?q) and, similarly, P(qi,j | ?r, ?q). 
Finally, the more probable of the two relations ri,j and qi,j is output if its probability 
exceeds a threshold value Pmin (i.e., ri,j is output if P(ri,j | ?r, ?q) > P(qi,j | ?r, ?q) and 
P(ri,j | ?r, ?q) > Pmin. In Section 4.2, we experiment with varying values of Pmin. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 795 
3.2   Paths Considered 
The enabling observation behind our approach is that in a graph in which edges have 
certain properties such as transitivity, some paths ?i,j indicate the presence of a rela-
tion between the first node xi and the last node xj. In the paths we consider, we rely on 
two kinds of inferences: transitivity and equivalence. Also, we do not consider very 
long paths, as they tend to become unreliable due to accumulation of chance of false 
detection of each edge and sense drift in each intermediate node. The set of paths to 
consider was not rigorously motivated. Rather, we aimed to cover some common 
cases. Refining the sets of paths is a possible fruitful direction for future work. 
For the presence of happens-before, a transitive asymmetric relation, we consid-
ered all 11 path types of length 3 or less which imply happens-before between the end 
nodes based on transitivity and equivalence: 
 
?happens-before? ?similar, similar, happens-before? 
?happens-before, similar? ?happens-before, happens-before, similar? 
?similar, happens-before? ?similar, happens-before, happens-before? 
?happens-before, happens-before? ?happens-before, similar, happens-before? 
?happens-before, similar, similar? ?happens-before, happens-before, happens-before? 
?similar, happens-before, similar?  
3.3   Statistical Model for Combining Evidence 
This section presents a rigorous derivation of the probabilistic model for computing 
and combining probabilities with which indirect paths indicate a given edge. 
3.3.1   Estimating from a Single Path 
We first derive probability of r1,n given single path ? 1,n: 
 
( )nnrP ,1,1 |?
 
If n is 2, i.e. ?1,n has only one edge R1,2, we have simply the probability that the 
edge actually holds given its presence in the graph: 
 ( ) ( )2,12,12,12,1 || RrPrP =?  (1) 
Otherwise, ?1,n has intermediate nodes, in which case P(r1,n | ?1,n) can be estimated 
as follows: 
 
( ) ( ) ( ) ( )
( )( ) ( )( )nnnnnnnnn
nnnnnnnnnnnnnn
RRrrPrrRRrP
RRrrPrrRRrPRRrPrP
,12,1,12,1,12,1,12,1,1
,12,1,12,1,12,1,12,1,1,12,1,1,1,1
...,,|...,,...,,,...,,|
...,,|...,,...,,,...,,|...,,||
????
?????
??
+==?
 
Because r1,n is conditionally independent from Ri,i+1 given ri,i+1 or ?ri,i+1, we can 
simplify: 
 
( ) ( ) ( )
( )( ) ( )( )nnnnnnn
nnnnnnnnn
RRrrPrrrP
RRrrPrrrPrP
,12,1,12,1,12,1,1
,12,1,12,1,12,1,1,1,1
...,,|...,,...,,|
...,,|...,,...,,||
???
???
??
+=?
 
Assuming independence of a given relation ri,i+1 from all edges in ?1,n except for 
the edge Ri,i+1 yields: 
796 T. Chklovski and P. Pantel 
 
( ) ( ) ( )
( )( ) ( )( )?
?
?=
++?
?=
++?
??
+=
1..1 1,1,,12,1,1
1..1 1,1,,12,1,1,1,1
|1...,,|
|...,,||
ni iiiinnn
ni iiiinnnnn
RrPrrrP
RrPrrrPrP ?
 
Let Pmatch denote the probability that there is no significant shift in meaning at a 
given intermediate node.  Then, assume that path r1,2,?, rn-1,n indicates r1,n iff the 
meanings at n ? 2 intermediate nodes match: 
 ( ) 2
,12,1,1 ...,,| ?? = nmatchnnn PrrrP  
Also, when one or more of the relations ri,i+1 do not hold, nothing is generally im-
plied1 about r1,n, thus 
 ( )( ) ( )nnnn rPrrrP ,1,12,1,1 ...,,| =? ?  
Plugging these in, we have: 
 ( ) ( ) ( ) ( )( )??
?=
++
?
?=
++
?
?+=
1..1 1,1,
2
,11..1 1,1,
2
,1,1 |1|| ni iiiinmatchnni iiiinmatchnn RrPPrPRrPPrP ?  
which can be rewritten as: 
 ( ) ( ) ( )( ) ( )?
?=
++
?
?+=
1..1 1,1,
2
,1,1,1,1 |1| ni iiiinmatchnnnn RrPPrPrPrP ?  (2) 
where the prior P(r1,n) and the conditional P(ri,i+1 | Ri,i+1) can be estimated empirically 
by manually tagging the relations Ri,j in a graph as correct or incorrect: P(r1,n) is the 
probability that an edge will be labeled with relation r by a human judge, and 
P(ri,i+1 | Ri,i+1) is the precision with which the system could identify R. While Pmatch 
can be estimated empirically we have not done so. We experimentally set Pmatch = 0.9. 
3.3.2   Combining Estimates from Multiple Paths 
In this subsection we derive an estimate of the validity of inferring r1,n given the set 
?r of m paths ?1,n1, ?1,n2, ?, ?1,nm: 
 ( )mnnnnrP ,12,11,1,1 ,...,,| ???  (3) 
In the case of zero paths, we use simply P(r1,n)=P(r), the probability of observing r 
between a pair of nodes from a sample set with no additional evidence. The case of 
one path has been treated in the previous section. In the case of multiple paths, we 
derive the expression as follows (omitting for convenience subscripts on paths, and 
distinguishing them by their superscripts). We assume conditional independence of 
any two paths ?k and ?l given r or ?r. Using Bayes? rule yields2: 
 ( ) ( ) ( )( )
( ) ( )
( )mmk
k
m
m
m
n P
rPrP
P
rPrP
rP
??
?
??
????
,...,
|
,...,
|,...,
,...,| 1 ..11
1
1
,1
?
=
==
 (4) 
                                                          
1
  This is not the case for paths in which the value of one edge, given the other edges, is corre-
lated with the value of the end-to-end relation. The exception does not apply for happens-
before edges if there are other happens-before edges in the path, nor does it ever apply for 
any similar edges. 
2
  Here and afterward, the denominators must be non-zero; they are always so when we apply 
this model. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 797 
The above denominator can be rewritten as: 
 
( ) ( ) ( ) ( ) ( )
( ) ( ) ( ) ( )??
==
??+
=??+=
mk
k
mk
k
mmm
rPrPrPrP
rPrPrPrPP
..1..1
111
||
|,...,|,...,,...,
??
??????
 (5) 
Using Bayes? rule again, the expressions in the above products can be rewritten as 
follows: 
 ( ) ( ) ( )( )rP
PrP
rP
kk
k ??? || =  (6) 
 ( ) ( ) ( )( )
( )( ) ( )
( )rP
PrP
rP
PrP
rP
kkkk
k
?
?
=
?
?
=?
1
|1|| ?????  (7) 
Substituting into Eq. 5 the Eqs. 6 and 7 yields: 
( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )
( )( ) ( )
( )????
====
=???
?
???
?
?
?
?+???
?
???
?
=??+=
mk
kk
mk
kk
mk
k
mk
km
rP
PrP
rP
rP
PrP
rPrPrPrPrPP
..1..1..1..1
1
1
|11|||,..., ????????
 ( )( ) ( )( )( )
( )( )
( )( ) ??
?
?
?
??
?
?
?
?
?
+?
?
=
?
=
=
??
? 1..11..1
..1 1
|1|
m
mk
k
m
mk
k
mk
k
rP
rP
rP
rP
P
??
?
 
Using the above for the denominator of Eq. 4, using Eq. 6 in the numerator of Eq. 
4, and simplifying, we have: 
 
( ) ( ) ( )( )
( )
( )( )( )
( )( )
( )( )
( )( ) 1
..1
1
..1
1
..1
1
..11
1
|1|
|
,...,
|
,...,|
?
=
?
=
?
=
=
?
?
+
== ??
?
?
m
mk
k
m
mk
k
m
mk
k
m
mk
k
m
rP
rP
rP
rP
rP
rP
P
rPrP
rP ??
?
??
?
??
 
which can be rewritten as 
 
( ) ( )
( ) ( )( ) ( )( )??
?
=
?
=
=
????
?
???
?
?
+
=
mk
k
m
mk
k
mk
k
m
rP
rP
rP
rP
rP
rP
..1
1
..1
..11
|1
1
|
|
,...,|
??
?
??
 (8) 
where P(r | ?k) is as in Eq. 2 and P(r) can be estimated empirically. 
3.3.3   Estimating from Supporting and Opposing Paths 
Recall that q denotes the opposite of r. The previous section has shown how to com-
pute P(r | ?r) and, similarly, P(q | ?q). We now derive how to estimate r given both 
?r, ?q: 
 ( )qrrP ?? ,|  (9) 
We assume that r and q are disjoint, P(r,q)= P(r|q)= P(q|r)=0. We also assume that 
q is conditionally independent from ?r given ?r, i.e., 
798 T. Chklovski and P. Pantel 
 ( ) ( )rqPrqP r ?=?? |,|  and ( ) ( )qqr rqPrqP ??=??? ,|,,| , and similarly 
 ( ) ( )qrPqrP q ?=?? |,|  and ( ) ( )rqr qrPqrP ??=??? ,|,,|  
We proceed by deriving the following, each consequent relying on the previous re-
sult: 
LEMMA 1: P(q | ?r), in Eq. 10 
LEMMA 2: P(?q | ?r), in Eq. 12 
LEMMA 3: P(r | ?q, ?r) and P(q | ?r, ?q), in Eqs. 13 and 14 
THEOREM 1: P(r | ?r, ?q), in Eq. 18. 
LEMMA 1. From P(r | q) = 0, we observe: 
 
( ) ( ) ( ) ( ) ( ) ( ) ( )rqPrPrqPrPrqPrPqP ??=??+= |||
 
Solving for P(q | ?r), we obtain: 
 ( ) ( )( )rP
qP
rqP
?
=?|  (10) 
LEMMA 2. Using an approach similar to that of Lemma 1 and noting that P(q | r, ?r) = 
P(q | r) = 0 yields: 
 ( ) ( ) ( ) ( ) ( ) ( ) ( )rrrrrrr rqPrPrqPrPrqPrPqP ????+=????+??=? ,||0,||,|||  
Invoking the assumption P(q | ?r, ?r) = P(q | ?r), we can simplify: 
 ( ) ( ) ( )rqPrPqP
rr
???=? |||  
Substituting the result of Lemma 1 (Eq. 10) into the above yields: 
 ( ) ( ) ( )( )rP
qPrPqP rr
?
??
=? ||  (11) 
And thus 
 ( ) ( ) ( ) ( )( )rP
qPrPrPqP rr
?
????
=?? ||  (12) 
LEMMA 3. We derive P(r | ?q, ?r), using P(?q | r, ?r) = 1: 
 ( ) ( )( )
( ) ( )
( ) ( )
( )
( )
( )
( )r
r
r
r
rr
rr
r
r
r qP
rP
qP
qrP
PqP
PqrP
qP
qrP
qrP
??
?
=
??
??
=
???
???
=
??
??
=?? |
|
|
|,
|
|,
,
,,
,|  
Substituting the result of Lemma 2 (Eq. 12) into the above yields: 
 ( ) ( ) ( )( ) ( ) ( )qPrPrP
rPrPqrP
r
r
r ????
??
=?? |
|
,|  (13) 
Similarly, 
 
( ) ( ) ( )( ) ( ) ( )rPqPqP
qPqP
rqP
q
q
q ????
??
=?? |
|
,|
 (14) 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 799 
THEOREM 3 
 ( ) ( ) ( ) ( )( )( ) ( )( ) ( ) ( )( ) ( ) ( )( )qPqPrPrPqPrP
qPrPrP
rP
qr
qr
qr
???????
????
=?? ||11
||
,|  
P(r | ?r, ?q) can be derived using the above Lemmas, as follows: 
 ( ) ( ) ( ) ( ) ( )qrqrqrqrqr qrPqPqrPqPrP ??????+????=?? ,,|,|,,|,|,|  
The assumption P(r | q) = 0 implies P(r | q, ?r, , ?q) = 0. Also, since r is condi-
tionally independent of ?q given ?q, we have P(r |?q, ?r, ?q) = P(r | ?q, ?r).  Thus, 
we can simplify: 
 ( ) ( ) ( ) ( )( ) ( )rqrrqrqr qrPqPqrPqPrP ?????=?????=?? ,|,|1,|,|,|  (15) 
Similarly, 
 ( ) ( ) ( ) ( )( ) ( )qqrqqrqr rqPrPrqPrPqP ?????=?????=?? ,|,|1,|,|,|  (16) 
Substituting, Eq. 16 into Eq. 15 yields: 
 
( ) ( )( ) ( )( ) ( )
( ) ( )( ) ( ) ( ) ( )rqqrqr
rqqrqr
qrPrqPrPrqPqrP
qrPrqPrPrP
??????+?????=
????????=??
,|,|,|,|1,|
,|,|,|11,|
 
Solving for P(r | ?r, ?q), we get: 
 ( ) ( ) ( ) ( )( ) ( )qr
qrr
qr
rqPqrP
rqPqrPqrP
rP
?????
???????
=??
,|,|1
,|,|,|
,|  (17) 
Expanding and simplifying, we establish our Theorem 1: 
 ( ) ( ) ( ) ( )( )( ) ( )( ) ( ) ( )( ) ( ) ( )( )qPqPrPrPqPrP
qPrPrP
rP
qr
qr
qr
???????
????
=?? ||11
||
,|  (18) 
4   Experimental Results 
In this section, we evaluate our refinement framework on the temporal precedence 
relations discovered by VERBOCEAN, and present some observations on applying the 
refinement to other VERBOCEAN relations. 
4.1   Experimental Setup 
Following Chklovski and Pantel [2], we studied 29,165 pairs of verbs obtained from a 
paraphrasing algorithm called DIRT [4]. We applied VERBOCEAN to the 29,165 verb 
pairs, which tagged each pair with the semantic tag happens-before, happens-after 
and no temporal precedence3. 
                                                          
3
  VERBOCEAN actually produces additional relations such as similarity, antonymy, strength and 
enablement. For our purposes, we only consider the temporal relations. 
800 T. Chklovski and P. Pantel 
For our experiments, we randomly sampled 1000 of these verb pairs, and presented 
them to two human judges (without revealing the VERBOCEAN tag). The judges were 
asked to classify each pair among the following tags: 
Happens-before with entailment 
Happens-before without entailment 
Happens-after with entailment 
Happens-after without entailment 
Another semantic relation 
No semantic relation 
For the purposes of our evaluation, tags a and b align with VERBOCEAN?s happens-
before tag, tags c and d align with the happens-after tag, and tags e and f align with 
the no temporal relation tag4. The Kappa statistic [7] for the task was ? = 0.78. 
4.2   Refinement Results 
Table 2 shows the overall accuracy of VERBOCEAN tags on the 1000 verb pairs ran-
domly sampled from DIRT. Each row represents a different refinement. The number 
in parentheses is Pmin, the threshold value for the strength of the relation from Section 
3.1. As the threshold is increased, the refinement algorithm requires greater evidence 
(more supporting paths and absence of opposing evidence) to trigger a temporal rela-
tion between a pair of verbs. 
Table 2. Accuracy (95% confidence) of VERBOCEAN on a random sample of 1000 verb pairs 
tagged by two judges 
 Accuracy 
 
Judge1 Judge2 Total 
Unrefined
 
80.7% 74.8% 77.7% ? 2.0% 
Refined (0.5) 66.0% 63.7% 64.8% ? 2.6% 
Refined (0.66) 75.4% 71.7% 73.5% ? 2.4% 
Refined (0.9) 83.1% 77.2% 80.2% ? 2.1% 
Refined (0.95) 84.5% 78.0% 81.3% ? 1.9% 
Refined (Combo)* 86.8% 81.3% 84.0% ? 2.4% 
* Combo combines the no temporal relation from the 0.5 and the happens-before and 
happens-after from the and 0.95 refinements, where the reported accuracy is com-
puted on the subset of 716 verb pairs for which the algorithm is most confident. 
Table 3 shows the reassignments due to refinement. At the 0.5 level, the refinement 
left 76 of 81 relations unchanged, revising 3 to happens-after and 2 to no temporal 
relation. Similarly, only two of the original happens-after relations were changed 
with  refinement.  However,  of  the  849  originally  tagged  no temporal relation, the  
                                                          
4
  In future work, we plan to use the judges? classifications to evaluate the extraction of entail-
ment relations using VERBOCEAN. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 801 
Fig. 3. Refinement precision on all 1000 verb 
pairs vs. on the 819 verb pairs on which the 
annotators agree on tag 
Overall Precision vs. Precision on Agreed Pairs
60
65
70
75
80
85
90
95
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
Pr
e
c
is
io
n
 
(%
)
Overall Agreed Pairs
Table 3. Allocation change between semantic tags due to refinement 
 Happens-Before Happens-After No Temporal Relation 
Unrefined 81 70 849 
Refined (0.5) 190 180 630 
Refined (0.66) 118 124 758 
Refined (0.9) 53 66 881 
Refined (0.95) 40 46 914 
 
refinement moved 113 to happens-before 
and 109 to happens-after. The precision 
of the 0.5 refinement on the no temporal 
relation tag increased by 4%; however, 
the precision on the temporal relations 
decreased by 5.7%. At the 0.95 refine-
ment level, 54 of the 81 relations origi-
nally tagged happens-before and 45 of 
the 70 relations originally tagged hap-
pens-after were changed to no temporal 
relation. Only 34 of the 849 no temporal 
relations were changed. At this level, the 
precision of no temporal relation tag 
decreased by 0.8% and the temporal 
relations? precision increased by 4%. 
Hence, at the 0.5 level, pairs classified as no temporal relation were improved 
while at the 0.95 level, pairs classified as a temporal relation were improved. To lev-
erage benefits of the two, we applied both the 0.5 and 0.95 level refinements and kept 
happens-before and happens-after classifications from the 0.95 level, and kept the no 
temporal relation classification from the 0.5 level.5 284 verb pairs were left unclassi-
fied. On the 716 classified verb pairs, refinement improved accuracy by 6.3%.  
                                                          
5
  This combination is guaranteed to be free of conflicts in classification because it is impossi-
ble for a relation to be classified as temporal at the 0.95 threshold level while being classified 
as non-temporal at the 0.5 level. 
Fig. 1. Refinement precision on each semantic 
tag 
Precision of Semantic Tags
0
20
40
60
80
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
Pr
e
c
is
io
n
 (%
)
Happens-Before Happens-After
No Temporal Relation Overall
Fig. 2. Refinement recall on each semantic tag 
Recall of Semantic Tags
0
20
40
60
80
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
R
ec
a
ll 
(%
)
Happens-Before Happens-After
No Temporal Relation Overall
802 T. Chklovski and P. Pantel 
Figures 1 and 2 illustrate the refinement precision and recall for each semantic tag. 
Both annotators have agreed on 819 verb pairs, and we examined performance on 
these. Figure 3 shows a higher precision on these pairs as compared to the overall set, 
illustrating that what is easier for the annotators is easier for the system. 
4.3   Observations on Refining Other Relations 
We have briefly investigated refining other semantic relations in VERBOCEAN. The 
extent of the evaluation was limited by availability of human judgments. We ran-
domly sampled 100 pairs from DIRT and presented the classifications to three human 
judges for evaluation [2]. 
Of the 100 pairs, 66 were identified to have a relation. We applied our refinement 
algorithm to VERBOCEAN and inspected the output. On the 37 relations that 
VERBOCEAN got wrong, our system identified six of them. On the remaining 29 that 
VERBOCEAN got correct, only one was identified as incorrect (false positive). Hence, 
on the task of identifying incorrect relations in VERBOCEAN, our system has a preci-
sion of 85.7%, where precision is defined as the percentage of correctly identified 
erroneous relations. However, it only achieved a recall of 16.2%, where recall is the 
percentage of erroneous relations that our system identified. Table 4 presents the 
relations that were refined by our system. The first two columns show the verb pair 
while the next two columns show the original relation in VERBOCEAN. 
Table 4. Seven relations in VERBOCEAN refined by a small test run on other relations 
Verb 1 Verb 2 
VERBOCEAN 
Relation 
Refinement 
Relation 
Judge 1 Relation Judge 2 Relation Judge 3 Relation 
attach use 
happens-before 
similar 
similar none none none 
bounce get weaker than  stronger than none none none 
dispatch defeat opposite none none none happens-before 
doom complicate opposite similar* none stronger-than stronger-than 
flatten level stronger than no relation* similar similar similar 
outlaw codify similar opposite none none opposition 
privatize improve happens-before none happens-before happens-before happens-before 
* only revision of relation to its opposite or ?none? was attempted here 
4.4   Discussion 
Our evaluation focused on the presence or absence of relations after refinement, with-
out exploiting the fact that our framework also updates confidences in a given rela-
tion. The additional information about confidence can benefit probabilistic inference 
approaches (e.g., [3]). 
Possible extensions to the algorithm include a more elaborate inference from graph 
structure, for example treating the absence of certain paths as counter-evidence. Sup-
pose that relations A happens-before B and A similar A' were detected, but the rela-
tion A' happens-before B was not. Then, the absence of a path 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 803 
A similar A' happens-before B 
suggests the absence of A happens-before B. 
Other important avenues of future work include applying our framework to other 
relations (e.g., strength in VERBOCEAN) and to better characterize the refinement 
thresholds. 
5   Conclusions 
We presented a method for refining edges in graphs by leveraging the semantics of 
multiple noisy paths. We re-estimated the presence of an edge between a pair of nodes 
from the evidence provided by multiple indirect paths between the nodes. Our ap-
proach applies to a variety of relation types: transitive symmetric, transitive asymmet-
ric, and relations inducing equivalence classes. We applied our model to refining 
temporal verb relations in a semantic resource called VERBOCEAN. Experiments 
showed a 16.1% error reduction after refinement. On the 72% refinement decisions 
that it was most confident, the error reduction was 28.3%. 
The usefulness of a semantic resource is highly dependent on its quality, which is 
often poor in automatically mined resources. With graph refinement frameworks such 
as the one presented here, many of these resources may be improved automatically. 
References 
1. Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In ACL-1999. pp. 
57-64. College Park, MD. 
2. Chklovski, T., and Pantel, P. 2004. VERBOCEAN: Mining the Web for Fine-Grained 
Semantic Verb Relations. In Proceedings of 2004 Conference on Empirical Methods in 
Natural Language Processing (EMNLP 2004), Barcelona, Spain, July 25-26. 
3. Domingos, P. and Richardson, M. 2004. Markov Logic: A unifying framework for 
statistical relational learning. In Proceedings of ICML Workshop on Statistical Relational 
Learning and its Connections to Other Fields. Banff, Canada. 
4. Lin, D. and Pantel, P. 2001. Discovery of inference rules for question answering. Natural 
Language Engineering, 7(4):343-360. 
5. Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In 
Proceedings HLT/NAACL-04. pp. 321-328. Boston, MA. 
6. Shinzato, K. and Torisawa, K. 2004. Acquiring hyponymy relations from web documents. 
In Proceedings of HLT-NAACL-2004. pp. 73-80. Boston, MA. 
7. Siegel, S. and Castellan Jr., N. 1988. Nonparametric Statistics for the Behavioral Sciences. 
McGraw-Hill. 
59
60
61
62
63
64
65
66
Automatically Labeling Semantic Classes 
 
 
Patrick Pantel and Deepak Ravichandran 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{pantel,ravichan}@isi.edu 
 
 
Abstract 
Systems that automatically discover semantic 
classes have emerged in part to address the 
limitations of broad-coverage lexical re-
sources such as WordNet and Cyc. The cur-
rent state of the art discovers many semantic 
classes but fails to label their concepts. We 
propose an algorithm labeling semantic 
classes and for leveraging them to extract is-a 
relationships using a top-down approach. 
1 Introduction 
The natural language literature is rich in theories of se-
mantics (Barwise and Perry 1985; Schank and Abelson 
1977). However, WordNet (Miller 1990) and Cyc (Le-
nat 1995) aside, the community has had little success in 
actually building large semantic repositories. Such 
broad-coverage lexical resources are extremely useful in 
applications such as word sense disambiguation (Lea-
cock, Chodorow and Miller 1998) and question answer-
ing (Pasca and Harabagiu 2001). 
Current manually constructed ontologies such as 
WordNet and Cyc have important limitations. First, they 
often contain rare senses. For example, WordNet in-
cludes a rare sense of computer that means ?the person 
who computes?. Using WordNet to expand queries to an 
information retrieval system, the expansion of computer 
will include words like estimator and reckoner. Also, 
the words dog, computer and company all have a sense 
that is a hyponym of person. Such rare senses make it 
difficult for a coreference resolution system to use 
WordNet to enforce the constraint that personal pro-
nouns (e.g. he or she) must refer to a person. The second 
problem with these lexicons is that they miss many do-
main specific senses. For example, WordNet misses the 
user-interface-object sense of the word dialog (as often 
used in software manuals). WordNet alo contains a 
very poor coverage of proper nouns. 
There is a need for (semi-) automatic approaches to 
building and extending ontologies as well as for validat-
ing the structure and content of existing ones. With the 
advent of the Web, we have access to enormous 
amounts of text. The future of ontology growing lies in 
leveraging this data by harvesting it for concepts and 
semantic relationships. Moreover, once such knowledge 
is discovered, mechanisms must be in place to enrich 
current ontologies with this new knowledge. 
To address some of the coverage and specificity 
problems in WordNet and Cyc, Pantel and Lin (2002) 
proposed and algorithm, called CBC, for automatically 
extracting semantic classes. Their classes consist of 
clustered instances like the three shown below: 
(A) multiple sclerosis, diabetes, 
osteoporosis, cardiovascular disease, 
Parkinson's, rheumatoid arthritis, heart 
disease, asthma, cancer, hypertension, 
lupus, high blood pressure, arthritis, 
emphysema, epilepsy, cystic fibrosis, 
leukemia, hemophilia, Alzheimer, myeloma, 
glaucoma, schizophrenia, ... 
(B) Mike Richter, Tommy Salo, John 
Vanbiesbrouck, Curtis Joseph, Chris Osgood, 
Steve Shields, Tom Barrasso, Guy Hebert, 
Arturs Irbe, Byron Dafoe, Patrick Roy, Bill 
Ranford, Ed Belfour, Grant Fuhr, Dominik 
Hasek, Martin Brodeur, Mike Vernon, Ron 
Tugnutt, Sean Burke, Zach Thornton, Jocelyn 
Thibault, Kevin Hartman, Felix Potvin, ... 
(C) pink, red, turquoise, blue, purple, 
green, yellow, beige, orange, taupe, white, 
lavender, fuchsia, brown, gray, black, 
mauve, royal blue, violet, chartreuse, 
teal, gold, burgundy, lilac, crimson, 
garnet, coral, grey, silver, olive green, 
cobalt blue, scarlet, tan, amber, ... 
A limitation of these concepts is that CBC does not 
discover their actual names. That is, CBC discovers a 
semantic class of Canadian provinces such as Manitoba, 
Alberta, and Ontario, but stops short of labeling the 
concept as Canadian Provinces. Some applications such 
as question answering would benefit from class labels. 
For example, given the concept list (B) and a label 
goalie/goaltender, a QA system could look for answers 
to the question ?Which goaltender won the most Hart 
Trophys?? in the concept. 
In this paper, we propose an algorithm for automati-
cally inducing names for semantic classes and for find-
ing instance/concept (is-a) relationships. Using concept 
signatures (templates describing the prototypical syntac-
tic behavior of instances of a concept), we extract con-
cept names by searching for simple syntactic patterns 
such as ?concept apposition-of instance?. Searching 
concept signatures is more robust than searching the 
syntactic features of individual instances since many 
instances suffer from sparse features or multiple senses. 
Once labels are assigned to concepts, we can extract 
a hyponym relationship between each instance of a con-
cept and its label. For example, once our system labels 
list (C) as color, we may extract relationships such as: 
pink is a color, red is a color, turquoise is a color, etc. 
Our results show that of the 159,000 hyponyms we ex-
tract using this simple method, 68% are correct. Of the 
65,000 proper name hyponyms we discover, 81.5% are 
correct. 
The remainder of this paper is organized as follows. 
In the next section, we review previous algorithms for 
extracting semantic classes and hyponym relationships. 
Section 3 describes our algorithm for labeling concepts 
and for extracting hyponym relationships. Experimental 
results are presented in Section 4 and finally, we con-
clude with a discussion and future work. 
2 Previous Work 
There have been several approaches to automatically 
discovering lexico-semantic information from text 
(Hearst 1992; Riloff and Shepherd 1997; Riloff and 
Jones 1999; Berland and Charniak 1999; Pantel and Lin 
2002; Fleischman et al 2003; Girju et al 2003). One 
approach constructs automatic thesauri by computing 
the similarity between words based on their distribution 
in a corpus (Hindle 1990; Lin 1998). The output of 
these programs is a ranked list of similar words to each 
word. For example, Lin?s approach outputs the follow-
ing top-20 similar words of orange:  
(D) peach, grapefruit, yellow, lemon, pink, 
avocado, tangerine, banana, purple, Santa 
Ana, strawberry, tomato, red, pineapple, 
pear, Apricot, apple, green, citrus, mango 
A common problem of such lists is that they do not 
discriminate between the senses of polysemous words. 
For example, in (D), the color and fruit senses of orange 
are mixed up. 
Lin and Pantel (2001) proposed a clustering algo-
rithm, UNICON, which generates similar lists but 
discriminates between senses of words. Later, Pantel 
and Lin (2002) improved the precision and recall of 
UNICON clusters with CBC (Clustering by Commit-
tee). Using sets of representative elements called com-
mittees, CBC discovers cluster centroids that 
unambiguously describe the members of a possible 
class. The algorithm initially discovers committees that 
are well scattered in the similarity space. It then pro-
ceeds by assigning elements to their most similar com-
mittees. After assigning an element to a cluster, CBC 
removes their overlapping features from the element 
before assigning it to another cluster. This allows CBC 
to discover the less frequent senses of a word and to 
avoid discovering duplicate senses. 
CBC discovered both the color sense of orange, as 
shown in list (C) of Section 1, and the fruit sense shown 
below: 
(E) peach, pear, apricot, strawberry, ba-
nana, mango, melon, apple, pineapple, 
cherry, plum, lemon, grapefruit, orange, 
berry, raspberry, blueberry, kiwi, ... 
There have also been several approaches to discov-
ering hyponym (is-a) relationships from text. Hearst 
(1992) used seven lexico-syntactic patterns, for example 
?such NP as {NP,}*{(or|and)} NP? and ?NP {, NP}*{,} 
or other NP?. Berland and Charniak (1999) used similar 
pattern-based techniques and other heuristics to extract 
meronymy (part-whole) relations. They reported an 
accuracy of about 55% precision on a corpus of 100,000 
words. Girju, Badulescu and Moldovan (2003) 
improved upon this work by using a machine learning 
filter. Mann (2002) and Fleischman et al (2003) used 
part of speech patterns to extract a subset of hyponym 
relations involing proper nouns. 
3 Labeling Classes 
The research discussed above on discovering hyponym 
relationships all take a bottom up approach. That is, 
they use patterns to independently discover semantic 
relationships of words. However, for infrequent words, 
these patterns do not match or, worse yet, generate in-
correct relationships. 
Ours is a top down approach. We make use of co-
occurrence statistics of semantic classes discovered by 
algorithms like CBC to label their concepts. Hyponym 
relationships may then be extracted easily: one hypo-
nym per instance/concept label pair. For example, if we 
labeled concept (A) from Section 1 with disease, then 
we could extract is-a relationships such as: diabetes is a 
disease, cancer is a disease, and lupus is a disease. A 
concept instance such as lupus is assigned a hypernym 
disease not because it necessarily occurs in any particu-
lar syntactic relationship with disease, but because it 
belongs to the class of instances that does. 
The input to our labeling algorithm is a list of se-
mantic classes, in the form of clusters of words, which 
may be generated from any source. In our experiments, 
we used the clustering outputs of CBC (Pantel and Lin 
2002). The output of the system is a ranked list of con-
cept names for each semantic class. 
In the first phase of the algorithm, we extract feature 
vectors for each word that occurs in a semantic class. 
Phase II then uses these features to compute grammati-
cal signatures of concepts using the CBC algorithm. 
Finally, we use simple syntactic patterns to discover 
class names from each class? signature. Below, we de-
scribe these phases in detail. 
3.1 Phase I 
We represent each word (concept instance) by a feature 
vector. Each feature corresponds to a context in which 
the word occurs. For example, ?catch __? is a verb-
object context. If the word wave occurred in this con-
text, then the context is a feature of wave. 
We first construct a frequency count vector C(e) = 
(ce1, ce2, ?, cem), where m is the total number of features 
and cef is the frequency count of feature f occurring in 
word e. Here, cef is the number of times word e occurred 
in a grammatical context f. For example, if the word 
wave occurred 217 times as the object of the verb catch, 
then the feature vector for wave will have value 217 for 
its ?object-of catch? feature. In Section 4.1, we describe 
how we obtain these features. 
We then construct a mutual information vector 
MI(e) = (mie1, mie2, ?, miem) for each word e, where mief 
is the pointwise mutual information between word e and 
feature f, which is defined as: 
 
N
c
N
c
N
c
ef m
j
ej
n
i
if
ef
mi
?? == ?
=
11
log  (1) 
where n is the number of words and N = ? ?
= =
n
i
m
j
ijc
1 1
is the 
total frequency count of all features of all words. 
Mutual information is commonly used to measure 
the association strength between two words (Church and 
Hanks 1989). A well-known problem is that mutual 
information is biased towards infrequent ele-
ments/features. We therefore multiply mief with the fol-
lowing discounting factor: 
 
1,min
,min
1
11
11
+???
?
???
?
???
?
???
?
?+ ??
??
==
==
m
j
jf
n
i
ei
m
j
jf
n
i
ei
ef
ef
cc
cc
c
c  (2) 
3.2 Phase II 
Following (Pantel and Lin 2002), we construct a com-
mittee for each semantic class. A committee is a set of 
representative elements that unambiguously describe the 
members of a possible class. 
For each class c, we construct a matrix containing 
the similarity between each pair of words ei and ej in c 
using the cosine coefficient of their mutual information 
vectors (Salton and McGill 1983): 
 ( ) ??
?
?
?
=
f
fe
f
fe
f
fefe
ji
ji
ji
mimi
mimi
eesim
22
,  (3) 
For each word e, we then cluster its most similar in-
stances using group-average clustering (Han and Kam-
ber 2001) and we store as a candidate committee the 
highest scoring cluster c'  according to the following 
metric: 
 | c'| ? avgsim(c') (4) 
where |c'| is the number of elements in c' and avgsim(c') 
is the average pairwise similarity between words in c'. 
The assumption is that the best representative for a con-
cept is a large set of very similar instances. The commit-
tee for class c is then the highest scoring candidate 
committee containing only words from c. For example, 
below are the committee members discovered for the 
semantic classes (A), (B), and (C) from Section 1: 
1) cardiovascular disease, diabetes, 
multiple sclerosis, osteoporosis, 
Parkinson's, rheumatoid arthritis 
2) Curtis Joseph, John Vanbiesbrouck, Mike 
Richter, Tommy Salo 
3) blue, pink, red, yellow 
3.3 Phase III 
By averaging the feature vectors of the committee 
members of a particular semantic class, we obtain a 
grammatical template, or signature, for that class. For 
example, Figure 1 shows an excerpt of the grammatical 
signature for concept (B) in Section 1. The vector is 
obtained by averaging the feature vectors for the words 
Curtis Joseph, John Vanbiesbrouck, Mike Richter, and 
Tommy Salo (the committee of this concept). The  
?-V:subj:N:sprawl? feature indicates a subject-verb re-
lationship between the concept and the verb sprawl 
while ?N:appo:N:goaltender? indicates an apposition 
relationship between the concept and the noun goal-
tender. The (-) in a relationship means that the right 
hand side of the relationship is the head (e.g. sprawl is 
the head of the subject-verb relationship). The two col-
umns of numbers indicate the frequency and mutual 
information score for each feature respectively. 
In order to discover the characteristics of human 
naming conventions, we manually named 50 concepts 
discovered by CBC. For each concept, we extracted the 
relationships between the concept committee and the 
assigned label. We then added the mutual information 
scores for each extracted relationship among the 50 
concepts. The top-4 highest scoring relationships are: 
? Apposition (N:appo:N) 
e.g. ... Oracle, a company known 
for its progressive employment 
policies, ... 
? Nominal subject (-N:subj:N) 
e.g. ... Apple was a hot young com-
pany, with Steve Jobs in charge. 
? Such as (-N:such as:N) 
e.g. ... companies such as IBM must 
be weary ... 
? Like (-N:like:N) 
e.g. ... companies like Sun Micro-
systems do no shy away from such 
challenges, ... 
To name a class, we simply search for these syntac-
tic relationships in the signature of a concept. We sum 
up the mutual information scores for each term that oc-
curs in these relationships with a committee of a class. 
The highest scoring term is the name of the class. For 
example, the top-5 scoring terms that occurred in these 
relationships with the signature of the concept repre-
sented by the committee {Curtis Joseph, John 
Vanbiesbrouck, Mike Richter, Tommy Salo} are: 
1)      goalie 40.37 
2)      goaltender 33.64 
3)      goalkeeper 19.22 
4)      player 14.55 
5)      backup 9.40 
The numbers are the total mutual information scores 
of each name in the four syntactic relationships. 
4 Evaluation 
In this section, we present an evaluation of the class 
labeling algorithm and of the hyponym relationships 
discovered by our system. 
4.1 Experimental Setup 
We used Minipar (Lin 1994), a broad coverage parser, 
to parse 3GB of newspaper text from the Aquaint 
(TREC-9) collection. We collected the frequency counts 
of the grammatical relationships (contexts) output by 
Minipar and used them to compute the pointwise mutual 
information vectors described in Section 3.1. 
We used the 1432 noun clusters extracted by CBC1 
as the list of concepts to name. For each concept, we 
then used our algorithm described in Section 3 to extract 
the top-20 names for each concept. 
                                                          
1 Available at http://www.isi.edu/~pantel/demos.htm 
{Curtis Joseph, John Vanbiesbrouck, 
 Mike Richter, Tommy Salo} 
 -N:gen:N  
  pad 57 11.19 
  backup 29 9.95 
  crease 7 9.69 
  glove 52 9.57 
  stick 20 9.15 
  shutout 17 8.80 
 -N:conj:N  
  Hasek 15 12.36 
  Martin Brodeur 12 12.26 
  Belfour 13 12.22 
  Patrick Roy 10 11.90 
  Dominik Hasek 7 11.20 
  Roy 6 10.01 
 -V:subj:N  
  sprawl 11 6.69 
  misplay 6 6.55 
  smother 10 6.54 
  skate 28 6.43 
  turn back 10 6.28 
  stop 453 6.19 
 N:appo:N  
  goaltender 449 10.79 
  goalie 1641 10.76 
  netminder 57 10.39 
  goalkeeper 487 9.69 
 N:conj:N  
  Martin Brodeur 11 12.49 
  Dominik Hasek 11 12.33 
  Ed Belfour 10 12.04 
  Curtis Joseph 7 11.46 
  Tom Barrasso 5 10.85 
  Byron Dafoe 5 10.80 
  Chris Osgood 4 10.25 
Figure 1. Excerpt of the grammatical signature for the 
goalie/goaltender concept. 
4.2 Labeling Precision 
Out of the 1432 noun concepts, we were unable to name 
21 (1.5%) of them. This occurs when a concept?s com-
mittee members do not occur in any of the four syntactic 
relationships described in Section 0. We performed a 
manual evaluation of the remaining 1411 concepts. 
We randomly selected 125 concepts and their top-5 
highest ranking names according to our algorithm. Ta-
ble 1 shows the first 10 randomly selected concepts 
(each concept is represented by three of its committee 
members). 
For each concept, we added to the list of names a 
human generated name (obtained from an annotator 
looking at only the concept instances). We also ap-
pended concept names extracted from WordNet. For 
each concept that contains at least five instances in the 
WordNet hierarchy, we named the concept with the 
most frequent common ancestor of each pair of in-
stances. Up to five names were generated by WordNet 
for each concept. Because of the low coverage of proper 
nouns in WordNet, only 33 of the 125 concepts we 
evaluated had WordNet generated labels. 
We presented to three human judges the 125 ran-
domly selected concepts together with the system, hu-
man, and WordNet generated names randomly ordered. 
That way, there was no way for a judge to know the 
source of a label nor the system?s ranking of the labels. 
For each name, we asked the judges to assign a score of 
correct, partially correct, or incorrect. We then com-
puted the mean reciprocal rank (MRR) of the system, 
human, and WordNet labels. For each concept, a nam-
ing scheme receives a score of 1 / M where M is the 
rank of the first name judged correct. Table 2 shows the 
results. Table 3 shows similar results for a more lenient 
evaluation where M is the rank of the first name judged 
correct or partially correct. 
Our system achieved an overall MRR score of 
77.1%. We performed much better than the baseline 
WordNet (19.9%) because of the lack of coverage 
(mostly proper nouns) in the hierarchy. For the 33 con-
cepts that WordNet named, it achieved a score of 75.3% 
and a lenient score of 82.7%, which is high considering 
the simple algorithm we used to extract labels using 
WordNet. 
The Kappa statistic (Siegel and Castellan Jr. 1988) 
measures the agreements between a set of judges? as-
sessments correcting for chance agreements: 
 ( ) ( )( )EP
EPAPK ?
?=
1
 (5) 
where P(A) is the probability of agreement between the 
judges and P(E) is the probability that the judges agree 
Table 1. Labels assigned to 10 randomly selected concepts (each represented by three committee members.
CBC CONCEPT HUMAN LABEL WORDNET LABELS SYSTEM LABELS (RANKED) 
BMG, EMI, Sony record label none label / company / album / 
machine / studio 
Preakness Stakes, Preakness, Belmont 
Stakes 
horse race none race / event / run / victory / 
start 
Olympia Snowe, Susan Collins, James 
Jeffords 
US senator none republican / senator / chair-
man / supporter / conservative 
Eldoret, Kisumu, Mombasa African city none city / port / cut off / town / 
southeast 
Bronze Star, Silver Star, Purple Heart medal decoration / laurel 
wreath / medal / medal-
lion / palm 
distinction / set / honor / sym-
bol 
Mike Richter, Tommy Salo, John 
Vanbiesbrouck 
NHL goalie none goalie / goaltender / goal-
keeper / player / backup 
Dodoma, Mwanza, Mbeya African city none facilitator / town 
fresco, wall painting, Mural art painting / picture painting / world / piece / floor 
/ symbol 
Qinghua University, Fudan University, 
Beijing University 
university none university / institution / stock-
holder / college / school 
Federal Bureau of Investigation, Drug 
Enforcement Administration, FBI 
governmental depart-
ment 
law enforcement agency agency / police / investigation 
/ department / FBI 
 
by chance on an assessment. An experiment with K ? 
0.8 is generally viewed as reliable and 0.67 < K < 0.8 
allows tentative conclusions. The Kappa statistic for our 
experiment is K = 0.72. 
The human labeling is at a disadvantage since only 
one label was generated per concept. Therefore, the 
human scores either 1 or 0 for each concept. Our sys-
tem?s highest ranking name was correct 72% of the 
time. Table 4 shows the percentage of semantic classes 
with a correct label in the top 1-5 ranks returned by our 
system. 
Overall, 41.8% of the top-5 names extracted by our 
system were judged correct. The overall accuracy for 
the top-4, top-3, top-2, and top-1 names are 44.4%, 
48.8%, 58.5%, and 72% respectively. Hence, the name 
ranking of our algorithm is effective. 
4.3 Hyponym Precision 
The 1432 CBC concepts contain 18,000 unique words. 
For each concept to which a word belongs, we extracted 
up to 3 hyponyms, one for each of the top-3 labels for 
the concept. The result was 159,000 hyponym relation-
ships. 24 are shown in the Appendix. 
Two judges annotated two random samples of 100 
relationships: one from all 159,000 hyponyms and one 
from the subset of 65,000 proper nouns. For each in-
stance, the judges were asked to decide whether the 
hyponym relationship was correct, partially correct or 
incorrect. Table 5 shows the results. The strict measure 
counts a score of 1 for each correctly judged instance 
and 0 otherwise. The lenient measure also gives a score 
of 0.5 for each instance judged partially correct. 
Many of the CBC concepts contain noise. For ex-
ample, the wine cluster: 
Zinfandel, merlot, Pinot noir, Chardonnay, 
Cabernet Sauvignon, cabernet, riesling, 
Sauvignon blanc, Chenin blanc, sangiovese, 
syrah, Grape, Chianti ... 
contains some incorrect instances such as grape, appe-
lation, and milk chocolate. Each of these instances will 
generate incorrect hyponyms such as grape is wine and 
milk chocolate is wine. This hyponym extraction task 
would likely serve well for evaluating the accuracy of 
lists of semantic classes. 
Table 5 shows that the hyponyms involving proper 
nouns are much more reliable than common nouns. 
Since WordNet contains poor coverage of proper nouns, 
these relationships could be useful to enrich it. 
4.4 Recall 
Semantic extraction tasks are notoriously difficult to 
evaluate for recall. To approximate recall, we conducted 
two question answering (QA) tasks: answering 
definition questions and performing QA information 
retrieval. 
Table 2. MRR scores for the human evaluation of naming 125 
random concepts. 
JUDGE HUMAN 
LABELS 
WordNet 
Labels 
System 
Labels 
1 100% 18.1% 74.4% 
2 91.2% 20.0% 78.1% 
3 89.6% 21.6% 78.8% 
Combined 93.6% 19.9% 77.1% 
 
Table 3. Lenient MRR scores for the human evaluation of 
naming 125 random concepts. 
JUDGE HUMAN 
LABELS 
WordNet 
Labels 
System 
Labels 
1 100% 22.8% 85.0% 
2 96.0% 20.8% 86.5% 
3 92.0% 21.8% 85.2% 
Combined 96.0% 21.8% 85.6% 
 
Table 4. Percentage of concepts with a correct name in the 
top-5 ranks returned by our system. 
JUDGE TOP-1 TOP-2 TOP-3 TOP-4 TOP-5 
1 68.8% 75.2% 78.4% 83.2% 84.0% 
2 73.6% 80.0% 81.6% 83.2% 84.8% 
3 73.6% 80.0% 82.4% 84.0% 88.8% 
Combined 72.0% 78.4% 80.8% 83.5% 85.6% 
 
Table 5. Accuracy of 159,000 extracted hyponyms and a sub-
set of 65,000 proper noun hyponyms. 
JUDGE All Nouns Proper Nouns 
 Strict Lenient Strict Lenient 
1 62.0% 68.0% 79.0% 82.0% 
2 74.0% 76.5% 84.0% 85.5% 
Combined 68.0% 72.2% 81.5% 83.8% 
 
Definition Questions 
We chose the 50 definition questions that appeared in 
the QA track of TREC2003 (Voorhees, 2003). For ex-
ample: ?Who is Aaron Copland?? and ?What is the 
Kama Sutra?? For each question we looked for at most 
five corresponding concepts in our hyponym list. For 
example, for Aaron Copland, we found the following 
hypernyms: composer, music, and gift. We compared 
our system with the concepts in WordNet and Fleisch-
man et al?s instance/concept relations (Fleischman et al 
2003). Table 6 shows the percentage of correct answers 
in the top-1 and top-5 returned answers from each sys-
tem. All systems seem to have similar performance on 
the top-1 answers, but our system has many more an-
swers in the top-5. This shows that our system has com-
paratively higher recall for this task. 
Information (Passage) Retrieval 
Passage retrieval is used in QA to supply relevant in-
formation to an answer pinpointing module. The higher 
the performance of the passage retrieval module, the 
higher will be the performance of the answer pinpoint-
ing module. 
The passage retrieval module can make use of the 
hyponym relationships that are discovered by our sys-
tem. Given a question such as ?What color ??, the like-
lihood of a correct answer being present in a retrieved 
passage is greatly increased if we know the set of all 
possible colors and index them in the document collec-
tion appropriately. 
We used the hyponym relations learned by our sys-
tem to perform semantic indexing on a QA passage re-
trieval task. We selected the 179 questions from the QA 
track of TREC-2003 that had an explicit semantic an-
swer type (e.g. ?What band was Jerry Garcia with?? 
and ?What color is the top stripe on the U.S. flag??). 
For each expected semantic answer type corresponding 
to a given question (e.g. band and color), we indexed 
the entire TREC-2002 IR collection with our system?s 
hyponyms. 
We compared the passages returned by the passage 
retrieval module with and without the semantic index-
ing. We counted how many of the 179 questions had a 
correct answer returned in the top-1 and top-100 pas-
sages. Table 7 shows the results. 
Our system shows small gains in the performance of 
the IR output. In the top-1 category, the performance 
improved by 20%. This may lead to better answer selec-
tions. 
5 Conclusions and Future Work 
Current state of the art concept discovery algorithms 
generate lists of instances of semantic classes but stop 
short of labeling the classes with concept names. Class 
labels would serve useful in applications such as ques-
tion answering to map a question concept into a seman-
tic class and then search for answers within that class. 
We propose here an algorithm for automatically label-
ing concepts that searches for syntactic patterns within a 
grammatical template for a class. Of the 1432 noun con-
cepts discovered by CBC, our system labelled 98.5% of 
them with an MRR score of 77.1% in a human evalua-
tion. 
Hyponym relationships were then easily extracted, 
one for each instance/concept label pair. We extracted 
159,000 hyponyms and achieved a precision of 68%. On 
a subset of 65,000 proper names, our performance was 
81.5%. 
This work forms an important attempt to building 
large-scale semantic knowledge bases. Without being 
able to automatically name a cluster and extract hypo-
nym/hypernym relationships, the utility of automatically 
generated clusters or manually compiled lists of terms is 
limited. Of course, it is a serious open question how 
many names each cluster (concept) should have, and 
how good each name is. Our method begins to address 
this thorny issue by quantifying the name assigned to a 
class and by simultaneously assigning a number that can 
be interpreted to reflect the strength of membership of 
each element to the class. This is potentially a signifi-
cant step away from traditional all-or-nothing seman-
tic/ontology representations to a concept representation 
Table 6. Percentage of correct answers in the Top-1 and 
Top-5 returned answers on 50 definition questions. 
SYSTEM Top-1 Top-5 
 Strict Lenient Strict Lenient 
WordNet 38% 38% 38% 38% 
Fleischman 36% 40% 42% 44% 
Our System 36% 44% 60% 62% 
 
Table 7. Percentage of questions where the passage retrieval 
module returns a correct answer in the Top-1 and Top-100 
ranked passages (with and without semantic indexing). 
 CORRECT TOP-1 Correct Top-100 
With semantic 
indexing 
43 / 179 134 / 179 
Without semantic 
indexing 
36 / 179 131 / 179 
 
scheme that is more nuanced and admits multiple names 
and graded set memberships. 
Acknowledgements 
The authors wish to thank the reviewers for their helpful 
comments. This research was partly supported by NSF 
grant #EIA-0205111. 
References 
Barwise, J. and Perry, J. 1985. Semantic innocence and un-
compromising situations. In: Martinich, A. P. (ed.) The 
Philosophy of Language. New York: Oxford University 
Press. pp. 401?413. 
Berland, M. and E. Charniak, 1999. Finding parts in very large 
corpora. In ACL-1999. pp. 57?64. College Park, MD. 
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 76?83. Vancouver, Canada. 
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline 
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of ACL-03. pp. 
1?7. Sapporo, Japan. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning 
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT/NAACL-03. pp. 
80?87. Edmonton, Canada. 
Han, J. and Kamber, M. 2001. Data Mining ? Concepts and 
Techniques. Morgan Kaufmann. 
Hearst, M. 1992. Automatic acquisition of hyponyms from 
large text corpora. In COLING-92. pp. 539?545. Nantes, 
France. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268?275. Pitts-
burgh, PA. 
Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using 
corpus statistics and WordNet relations for sense identifica-
tion. Computational Linguistics, 24(1):147?165. 
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Lin, D. 1994. Principar - an efficient, broad-coverage, princi-
ple-based parser. Proceedings of COLING-94. pp. 42?48. 
Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar 
words. In Proceedings of COLING/ACL-98. pp. 768?774. 
Montreal, Canada. 
Lin, D. and Pantel, P. 2001. Induction of semantic classes 
from natural language text. In Proceedings of SIGKDD-01. 
pp. 317?322. San Francisco, CA. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. SemaNet? 02: Building and 
Using Semantic Networks, Taipei, Taiwan. 
Miller, G. 1990. WordNet: An online lexical database. Inter-
national Journal of Lexicography, 3(4). 
Pasca, M. and Harabagiu, S. 2001. The informative role of 
WordNet in Open-Domain Question Answering. In Pro-
ceedings of NAACL-01 Workshop on WordNet and Other 
Lexical Resources. pp. 138?143. Pittsburgh, PA. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from 
Text. In Proceedings of SIGKDD-02. pp. 613?619. Edmon-
ton, Canada. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-
1997. 
Riloff, E. and Jones, R. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of AAAI-99. pp. 474?479. Orlando, FL. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern 
Information Retrieval. McGraw Hill 
Schank, R. and Abelson, R. 1977. Scripts, Plans, Goals and 
Understanding: An Inquiry into Human Knowledge Struc-
tures. Lawrence Erlbaum Associates. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statis-
tics for the Behavioral Sciences. McGraw-Hill. 
Voorhees, E. 2003. Overview of the question answering track. 
To appear in Proceedings of TREC-12 Conference. NIST, 
Gaithersburg, MD. 
Appendix. Sample hyponyms discovered by our system.
INSTANCE CONCEPT INSTANCE CONCEPT 
actor hero price support benefit 
Ameritrade brokerage republican politician 
Arthur 
Rhodes 
pitcher Royal Air 
Force 
force 
bebop MUSIC Rwanda city 
Buccaneer team Santa Ana city 
Congressional  
Research 
Service 
agency shot-blocker player 
Cuba country slavery issue 
Dan Petrescu midfielder spa facility 
Hercules aircraft taxi vehicle 
Moscow city Terrence 
Malick 
director 
Nokia COMPANY verbena tree 
nominee candidate Wagner composer 
 
Proceedings of NAACL HLT 2007, pages 131?138,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Domain Restriction Hypothesis:
Relating Term Similarity and Semantic Consistency
Alfio Massimiliano Gliozzo
ITC-irst
Trento, Italy
gliozzo@itc.it
Marco Pennacchiotti
University of Rome Tor Vergata
Rome, Italy
pennacchiotti@info.uniroma2.it
Patrick Pantel
USC, Information Sciences Institute
Marina del Rey, CA
pantel@isi.edu
Abstract
In this paper, we empirically demonstrate
what we call the domain restriction hy-
pothesis, claiming that semantically re-
lated terms extracted from a corpus tend
to be semantically coherent. We apply
this hypothesis to define a post-processing
module for the output of Espresso, a state
of the art relation extraction system, show-
ing that irrelevant and erroneous relations
can be filtered out by our module, in-
creasing the precision of the final output.
Results are confirmed by both quantita-
tive and qualitative analyses, showing that
very high precision can be reached.
1 Introduction
Relation extraction is a fundamental step in
many natural language processing applications such
as learning ontologies from texts (Buitelaar et
al., 2005) and Question Answering (Pasca and
Harabagiu, 2001).
The most common approach for acquiring con-
cepts, instances and relations is to harvest semantic
knowledge from texts. These techniques have been
largely explored and today they achieve reasonable
accuracy. Harvested lexical resources, such as con-
cept lists (Pantel and Lin, 2002), facts (Etzioni et
al., 2002) and semantic relations (Pantel and Pen-
nacchiotti, 2006) could be then successfully used in
different frameworks and applications.
The state of the art technology for relation extrac-
tion primarily relies on pattern-based approaches
(Snow et al, 2006). These techniques are based on
the recognition of the typical patterns that express
a particular relation in text (e.g. ?X such as Y?
usually expresses an is-a relation). Yet, text-based
algorithms for relation extraction, in particular
pattern-based algorithms, still suffer from a number
of limitations due to complexities of natural lan-
guage, some of which we describe below.
Irrelevant relations. These are valid relations
that are not of interest in the domain at hand. For
example, in a political domain, ?Condoleezza Rice
is a football fan? is not as relevant as ?Condoleezza
Rice is the Secretary of State of the United States?.
Irrelevant relations are ubiquitous, and affect ontol-
ogy reliability, if used to populate it, as the relation
drives the wrong type of ontological knowledge.
Erroneous or false relations. These are particu-
larly harmful, since they directly affect algorithm
precision. A pattern-based relation extraction
algorithm is particularly likely to extract erroneous
relations if it uses generic patterns, which are
defined in (Pantel and Pennacchiotti, 2006) as
broad coverage, noisy patterns with high recall and
low precision (e.g. ?X of Y? for part-of relation).
Harvesting algorithms either ignore generic patterns
(Hearst, 1992) (affecting system recall) or use man-
ually supervised filtering approaches (Girju et al,
2006) or use completely unsupervised Web-filtering
methods (Pantel and Pennacchiotti, 2006). Yet,
these methods still do not sufficiently mitigate the
problem of erroneous relations.
Background knowledge. Another aspect that
makes relation harvesting difficult is related to the
131
nature of semantic relations: relations among enti-
ties are mostly paradigmatic (de Saussure, 1922),
and are usually established in absentia (i.e., they are
not made explicit in text). According to Eco?s posi-
tion (Eco, 1979), the background knowledge (e.g.
?persons are humans?) is often assumed by the
writer, and thus is not explicitly mentioned in text.
In some cases, such widely-known relations can be
captured by distributional similarity techniques but
not by pattern-based approaches.
Metaphorical language. Even when paradigmatic
relations are explicitly expressed in texts, it can
be very difficult to distinguish between facts and
metaphoric usage (e.g. the expression ?My mind is
a pearl? occurs 17 times on the Web, but it is clear
that mind is not a pearl, at least from an ontological
perspective).
The considerations above outline some of the dif-
ficulties of taking a purely lexico-syntactic approach
to relation extraction. Pragmatic issues (background
knowledge and metaphorical language) and onto-
logical issues (irrelevant relation) can not be solved
at the syntactic level. Also, erroneous relations can
always arise. These considerations lead us to the
intuition that extraction can benefit from imposing
some additional constraints.
In this paper, we integrate Espresso with a lex-
ical distribution technique modeling semantic co-
herence through semantic domains (Magnini et al,
2002). These are defined as common discourse top-
ics which demonstrate lexical coherence, such as
ECONOMICS or POLITICS. We explore whether se-
mantic domains can provide the needed additional
constraints to mitigate the acceptance of erroneous
relations. At the lexical level, semantic domains
identify clusters of (domain) paradigmatically re-
lated terms. We believe that the main advantage of
adopting semantic domains in relation extraction is
that relations are established mainly among terms in
the same Domain, while concepts belonging to dif-
ferent fields are mostly unrelated (Gliozzo, 2005),
as described in Section 2. For example, in a chem-
istry domain, an is-a will tend to relate only terms of
that domain (e.g., nitrogen is-a element), while out-
of-domain relations are likely to be erroneous e.g.,
driver is-a element.
By integrating pattern-based and distributional ap-
proaches we aim to capture the two characteristic
properties of semantic relations:
? Syntagmatic properties: if two terms X and
Y are in a given relation, they tend to co-
occur in texts, and are mostly connected by spe-
cific lexical-syntactic patterns (e.g., the patter
?X is a Y ? connects terms in is-a relations).
This aspect is captured using a pattern-based
approach.
? Domain properties: if a semantic relation
among two terms X and Y holds, both X
and Y should belong to the same semantic
domain (i.e. they are semantically coherent),
where semantic domains are sets of terms
characterized by very similar distributional
properties in a (possibly domain specific)
corpus.
In Section 2, we develop the concept of semantic do-
main and an automatic acquisition procedure based
on Latent Semantic Analysis (LSA) and we provide
empirical evidence of the connection between rela-
tion extraction and domain modelling. Section 3 de-
scribes the Espresso system. Section 4 concerns our
integration of semantic domains and Espresso. In
Section 5, we evaluate the impact of our LSA do-
main restriction module on improving a state of the
art relation extraction system. In Section 6 we draw
some interesting research directions opened by our
work.
2 Semantic Domains
Semantic domains are common areas of human
discussion, which demonstrate lexical coherence,
such as ECONOMICS, POLITICS, LAW, SCIENCE,
(Magnini et al, 2002). At the lexical level, se-
mantic domains identify clusters of (domain) related
lexical-concepts, i.e. sets of highly paradigmatically
related words also known as Semantic Fields.
In the literature, semantic domains have been
inferred from corpora by adopting term clustering
methodologies (Gliozzo, 2005), and have been used
for several NLP tasks, such as Text Categorization
and Ontology Learning (Gliozzo, 2006).
Semantic domains can be described by Domain
Models (DMs) (Gliozzo, 2005). A DM is a com-
132
putational model for semantic domains, that repre-
sents domain information at the term level, by defin-
ing a set of term clusters. Each cluster represents a
Semantic Domain, i.e. a set of terms that often co-
occur in texts having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of a Domain Model
DMs can be acquired from texts in a completely
unsupervised way by exploiting a lexical coherence
assumption. To this end, term clustering algorithms
can be used with each cluster representing a Se-
mantic Domain. The degree of association among
terms and clusters, estimated by the learning algo-
rithm, provides a domain relevance function. For
our experiments we adopted a clustering strategy
based on LSA (Deerwester et al, 1990), following
the methodology described in (Gliozzo, 2005). The
input of the LSA process is a term-by-document ma-
trix T reporting the term frequencies in the whole
corpus for each term. The matrix is decomposed by
means of a Singular Value Decomposition (SVD),
identifying the principal components of T. This op-
eration is done off-line, and can be efficiently per-
formed on large corpora. SVD decomposes T into
three matrixes T ' V?k?UT where ?k? is the di-
agonal k ? k matrix containing the highest k? ? k
eigenvalues of T on the diagonal, and all the re-
maining elements are 0. The parameter k? is the
dimensionality of the domain and can be fixed in
advance1. Under this setting we define the domain
matrix DLSA2 as
DLSA = INV
?
?k? (1)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
and ~w?i is the ith row of the matrix V
??k? .
1It is not clear how to choose the right dimensionality. In
our experiments we used 100 dimensions.
2Details of this operation can be found in (Gliozzo, 2005).
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevancies with respect to each domain. The DV
~t?i for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the corpus.
The domain similarity ?d(ti, tj) among terms is then
estimated by the cosine among their corresponding
DVs in the Domain Space, defined as follows:
?d(ti, tj) = ?
~ti, ~tj??
?~ti, ~ti??~tj , ~tj?
(2)
Figure 1: Probability of finding paradigmatic rela-
tions
The main advantage of adopting semantic do-
mains for relation extraction is that they allow us to
impose a domain restriction on the set of candidate
pairs of related terms. In fact, semantic relations can
be established mainly among terms in the same Se-
mantic Domain, while concepts belonging to differ-
ent fields are mostly unrelated.
To show the validity of the domain restriction we
conducted a preliminary experiment, contrasting the
probability for two words to be related in Word-
Net (Magnini and Cavaglia`, 2000) with their domain
similarity, measured in the Domain Space induced
from the British National Corpus. In particular, for
each couple of words, we estimated the domain sim-
ilarity, and we collected word pairs in sets charac-
terized by different ranges of similarity (e.g. all the
pairs between 0.8 and 0.9). Then we estimated the
133
probability of each couple of words in different sets
to be linked by a semantic relation in WordNet, such
as synonymy, hyperonymy, co-hyponymy and do-
main in WordNet Domains (Magnini et al, 2002).
Results in Figure 1 show a monotonic crescent rela-
tion between these two quantities. In particular the
probability for two words to be related tends to 0
when their similarity is negative (i.e., they are not
domain related), supporting the basic hypothesis of
this work. In Section 4 we will show that this prop-
erty can be used to improve the overall performances
of the relation extraction algorithm.
3 The pattern-based Espresso system
Espresso (Pantel and Pennacchiotti, 2006) is a
corpus-based general purpose, broad, and accurate
relation extraction algorithm requiring minimal su-
pervision, whose core is based on the framework
adopted in (Hearst, 1992). Espresso introduces two
main innovations that guarantee high performance:
(i) a principled measure for estimating the reliabil-
ity of relational patterns and instances; (ii) an algo-
rithm for exploiting generic patterns. Generic pat-
terns are broad coverage noisy patterns (high recall
and low precision), e.g. ?X of Y? for the part-of re-
lation. As underlined in the introduction, previous
algorithms either required significant manual work
to make use of generic patterns, or simply ignore
them. Espresso exploits an unsupervised Web-based
filtering method to detect generic patterns and to dis-
tinguish their correct and incorrect instances.
Given a specific relation (e.g. is-a) and a POS-
tagged corpus, Espresso takes as input few seed
instances (e.g. nitrogen is-a element) or seed surface
patterns (e.g. X/NN such/JJ as/IN Y/NN). It then
incrementally learns new patterns and instances
by iterating on the following three phases, until a
specific stop condition is met (i.e., new patterns are
below a pre-defined threshold of reliability).
Pattern Induction. Given an input set of seed
instances I , Espresso infers new patterns connecting
as many instances as possible in the given corpus.
To do so, Espresso uses a slight modification of the
state of the art algorithm described in (Ravichandran
and Hovy, 2002). For each instance in input, the
sentences containing it are first retrieved and then
generalized, by replacing term expressions with a
terminological label using regular expressions on
the POS-tags. This generalization allows to ease
the problem of data sparseness in small corpora.
Unfortunately, as patterns become more generic,
they are more prone to low precision.
Pattern Ranking and Selection. Espresso ranks
all extracted patterns using a reliability measure rpi
and discards all but the top-k P patterns, where k is
set to the number of patterns from the previous iter-
ation plus one. rpi captures the intuition that a reli-
able pattern is one that is both highly precise and one
that extracts many instances. rpi is formally defined
as the average strength of association between a pat-
tern p and each input instance i in I , weighted by the
reliability r? of the instance i (described later):
rpi(p) =
?
i?I
(
pmi(i,p)
maxpmi ? r?(i)
)
|I|
where pmi(i, p) is the pointwise mutual information
(pmi) between i and p (estimated with Maximum
Likelihood Estimation), and maxpmi is the maxi-
mum pmi between all patterns and all instances.
Instance Extraction, Ranking, Selection.
Espresso extracts from the corpus the set of in-
stances I matching the patterns in P . In this phase
generic patterns are detected, and their instances
are filtered, using a technique described in detail in
(Pantel and Pennacchiotti, 2006). Instances are then
ranked using a reliability measure r?, similar to that
adopted for patterns. A reliable instance should be
highly associated with as many reliable patterns as
possible:
r?(i) =
?
p?P
(
pmi(i,p)
maxpmi ? rpi(i)
)
|P |
Finally, the best scoring instances are selected for
the following iteration. If the number of extracted
instances is too low (as often happens in small
corpora) Espresso enters an expansion phase, in
which instances are expanded by using web based
and syntactic techniques.
134
The output Espresso is a list of instances
i = (X,Y ) ? I , ranked according to r?(i). This
score accounts for the syntagmatic similarity be-
tween X and Y , i.e., how strong is the co-occurrence
of X and Y in texts with a given pattern p.
A key role in the Espresso algorithm is played
by the reliability measures. The accuracy of the
whole extraction process is in fact highly sensitive
to the ranking of patterns and instances because, at
each iteration, only the best scoring entities are re-
tained. For instance, if an erroneous instance is se-
lected after the first iteration, it could in theory af-
fect the following pattern extraction phase and cause
drift in consequent iterations. This issue is criti-
cal for generic patterns (where precision is still a
problem, even with Web-based filtering), and could
sometimes also affect non-generic patterns.
It would be then useful to integrate Espresso with
a technique able to retain only very precise in-
stances, without compromising recall. As syntag-
matic strategies are already in place, another strategy
is needed. In the next Section, we show how this can
be achieved using instance domain information.
4 Integrating syntagmatic and domain
information
The strategy of integrating syntagmatic and do-
main information has demonstrated to be fruitful in
many NLP tasks, such as Word Sense Disambigua-
tion and open domain Ontology Learning (Gliozzo,
2006). According to the structural view (de Saus-
sure, 1922), both aspects contribute to determine
the linguistic value (i.e. the meaning) of words:
the meaning of lexical constituents is determined
by a complex network of semantic relations among
words. This suggests that relation extraction can
benefit from accounting for both syntagmatic and
domain aspects at the same time.
To demonstrate the validity of this claim we can
explore many different integration schemata. For ex-
ample we can restrict the search space (i.e. the set of
candidate instances) to the set of all those terms be-
longing to the same domain. Another possibility is
to exploit a similarity metric for domain relatedness
to re-rank the output instances I of Espresso, hoping
that the top ranked ones will mostly be those which
are correct. One advantage of this latter method-
ology is that it can be applied to the output of any
relation extraction system without any modification
to the system itself. In addition, this methodology
can be evaluated by adopting standard Information
Retrieval (IR) measures, such as mean average pre-
cision (see Section 5). Because of these advantages,
we decided to adopt the re-ranking procedure.
The procedure is defined as follows: each in-
stance extracted by Espresso is assigned a Domain
Similarity score ?d(X,Y ) estimated in the domain
space according to Equation 2; a higher score is
then assigned to the instances that tend to co-occur
in the same documents in the corpus. For exam-
ple, the candidate instances ethanol is-a nonaro-
matic alcohol has a higher score than ethanol is-a
something, as ethanol and alcohol are both from the
chemistry domain, while something is a generic term
and is thus not associated to any domain.
Instances are then re-ranked according to
?d(X,Y ), which is used as the new index of
reliability instead of the original reliability scores
of Espresso. In Subsection 5.2 we will show that
the re-ranking technique improves the original
reliability scores of Espresso.
5 Evaluation
In this Section we evaluate the benefits of applying
the domain information to relation extraction (ESP-
LSA), by measuring the improvements of Espresso
due to domain based re-ranking.
5.1 Experimental Settings
As a baseline system, we used the ESP- implemen-
tation of Espresso described in (Pantel and Pennac-
chiotti, 2006). ESP- is a fully functioning Espresso
system, without the generic pattern filtering module
(ESP+). We decided to use ESP- for two main rea-
sons. First, the manual evaluation process would
have been too time consuming, as ESP+ extracts
thousands of relations. Also, the small scale experi-
ment for EXP- allows us to better analyse and com-
pare the results.
To perform the re-ranking operation, we acquired
a Domain Model from the input corpus itself. To this
aim we performed a SVD of the term by document
matrix T describing the input corpus, indexing all
the candidate terms recognized by Espresso.
135
As an evaluation benchmark, we adopted the
same instance sets extracted by ESP- in the ex-
periment described in (Pantel and Pennacchiotti,
2006). We used an input corpus of 313,590 words,
a college chemistry textbook (Brown et al 2003),
pre-processed using the Alembic Workbench POS-
tagger (Day et al 1997). We considered the fol-
lowing relations: is-a, part-of, reaction (a relation
of chemical reaction among chemical entities) and
production (a process or chemical element/object
producing a result). ESP- extracted 200 is-a, 111
part-of, 40 reaction and 196 production instances.
5.2 Quantitative Analysis
The experimental evaluation compared the accuracy
of the ranked set of instances extracted by ESP- with
the re-ranking produced on these instances by ESP-
LSA. By analogy to IR, we are interested in ex-
tracting positive instances (i.e. semantically related
words). Accordingly, we utilize the standard defi-
nitions of precision and recall typically used in IR .
Table 2 reports the Mean Average Precision obtained
by both ESP- and ESP-LSA on the extracted rela-
tions, showing the substantial improvements on all
the relations due to domain based re-ranking.
ESP- ESP-LSA
is-a 0.54 0.75 (+0.21)
part-of 0.65 0.82 (+0.17)
react 0.75 0.82 (+0.07)
produce 0.55 0.62 (+0.07)
Table 2: Mean Average Precision reported by ESP-
and ESP-LSA
Figures 2, 3, 4 and 5 report the precision/recall
curves obtained for each relation, estimated by mea-
suring the precision / recall at each point of the
ranked list. Results show that precision is very high
especially for the top ranked relations extracted by
ESP-LSA. Precision reaches the upper bound for the
top ranked part of the part-of relation, while it is
close to 0.9 for the is-a relation. In all cases, the
precision reported by the ESP-LSA system surpass
those of the ESP- system at all recall points.
5.3 Qualitative Analysis
Table 3 shows the best scoring instances for ESP-
and ESP-LSA on the evaluated relations. Results
Figure 2: Syntagmatic vs. Domain ranking for the
is-a relation
Figure 3: Syntagmatic vs. Domain ranking for the
produce relation
show that ESP-LSA tends to assign a much lower
score to erroneous instances, as compared to the
original Espresso reliability ranking. For exam-
ple for the part-of relation, the ESP- ranks the er-
roneous instance geometry part-of ion in 23th po-
sition, while ESP-LSA re-ranks it in 92nd. In
this case, a lower score is assigned because ge-
ometry is not particularly tied to the domain of
chemistry. Also, ESP-LSA tends to penalize in-
stances derived from parsing/tokenization errors:
136
Figure 4: Syntagmatic vs. Domain ranking for the
part-of relation
Figure 5: Syntagmatic vs. Domain ranking for the
react relation
] binary hydrogen compounds hydrogen react ele-
ments is 16th for ESP-, while in the last tenth of
the ESP-LSA. In addition, out-of-domain relations
are successfully interpreted by ESP-LSA. For ex-
ample, the instance sentences part-of exceptions is
a possibly correct relation, but unrelated to the do-
main, as an exception in chemistry has nothing to
do with sentences. This instance lies at the bottom
of the ESP-LSA ranking, while is in the middle of
ESP- list. Also, low ranked and correct relations ex-
tracted by ESP- emerge with ESP-LSA. For exam-
ple, magnesium metal react elemental oxygen lies at
the end of ESP- rank, as there are not enough syntag-
matic evidence (co-occurrences) that let the instance
emerge. The domain analysis of ESP-LSA promotes
this instance to the 2nd rank position. However, in
few cases, the strategy adopted by ESP-LSA tends
to promote erroneous instances (e.g. high voltage
produce voltage). Yet, results show that these are
isolated cases.
6 Conclusion and future work
In this paper, we propose the domain restriction hy-
pothesis, claiming that semantically related terms
extracted from a corpus tend to be semantically co-
herent. Applying this hypothesis, we presented a
new method to improve the precision of pattern-
based relation extraction algorithms, where the inte-
gration of domain information allows the system to
filter out many irrelevant relations, erroneous can-
didate pairs and metaphorical language relational
expressions, while capturing the assumed knowl-
edge required to discover paradigmatic associations
among terms. Experimental evidences supports this
claim both qualitatively and quantitatively, opening
a promising research direction, that we plan to ex-
plore much more in depth. In the future, we plan
to compare LSA to other term similarity measures,
to train the LSA model on large open domain cor-
pora and to apply our technique to both generic and
specific corpora in different domains. We want also
to increase the level of integration of the LSA tech-
nique in the Espresso algorithm, by using LSA as an
alternative reliability measure at each iteration. We
will also explore the domain restriction property of
semantic domains to develop open domain ontology
learning systems, as proposed in (Gliozzo, 2006).
The domain restriction hypothesis has potential
to greatly impact many applications where match-
ing textual expressions is a primary component. It is
our hope that by combining existing ranking strate-
gies in applications such as information retrieval,
question answering, information extraction and doc-
ument classification, with knowledge of the coher-
ence of the underlying text, one will see significant
improvements in matching accuracy.
137
Relation ESP- ESP - LSA
X is-a Y Aluminum ; metal F ; electronegative atoms
nitride ion ; strong Br O ; electronegative atoms
heat flow ; calorimeter NaCN ; cyanide salt
complete ionic equation ; spectator NaCN ; cyanide salts
X part-of Y elements ; compound amino acid building blocks ; tripeptide
composition ; substance acid building blocks ; tripeptide
blocks ; tripeptide powdered zinc metal ; battery
elements ; sodium chloride building blocks ; tripeptide
X react Y hydrazine ; water magnesium metal ; elemental oxygen
magnesium metal ; hydrochloric acid nitrogen ; ammonia
magnesium ; oxygen sodium metal ; chloride
magnesium metal ; acid carbon dioxide ; methane
X produce Y bromine ; bromide high voltage ; voltage
oxygen ; oxide reactions ; reactions
common fuels ; dioxide dr jekyll ; hyde
kidneys ; stones yellow pigments ; green pigment
Table 3: Top scoring relations extracted by ESP- and ESP-LSA.
Acknowledgments
Thanks to Roberto Basili for his precious comments,
suggestions and support. Alfio Gliozzo was sup-
ported by the OntoText project, funded by the Au-
tonomous Province of Trento under the FUP-2004,
and the FIRB founded project N.RBIN045PXH.
References
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
F. de Saussure. 1922. Cours de linguistique ge?ne?rale.
Payot, Paris.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
U. Eco. 1979. Lector in fabula. Bompiani.
O. Etzioni, M.J. Cafarella, D. Downey, A.-M
A.M. Popescu, T. Shaked, S. Soderland, D.S.
Weld, and A. Yates. 2002. Unsupervised named-
entity extraction from the web: An experimental
study. Artificial Intelligence, 165(1):91?143.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proceedings of HLT/NAACL-
03, pages 80?87, Edmonton, Canada, July.
A. Gliozzo. 2005. Semantic Domains in Computational
Linguistics. Ph.D. thesis, University of Trento.
A. Gliozzo. 2006. The god model. In Proceedings of
EACL.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Nantes, France.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM Conference on
Knowledge Discovery and Data Mining, pages 613?
619.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL-COLING-06, pages 113?
120, Sydney, Australia.
M. Pasca and S. Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of NAACL-01 Workshop on WordNet and
Other Lexical Resources, pages 138?143, Pittsburgh,
PA.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41?47, Philadelphia, PA.
R. Snow, D. Jurafsky, and A.Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proceedings of the ACL/COLING-06, pages 801?808,
Sydney, Australia.
138
Proceedings of NAACL HLT 2007, pages 564?571,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ISP: Learning Inferential Selectional Preferences 
 
Patrick Pantel?, Rahul Bhagat?, Bonaventura Coppola?, 
Timothy Chklovski?, Eduard Hovy? 
?Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 
{pantel,rahul,timc,hovy}@isi.edu
?ITC-Irst and University of Trento 
Via Sommarive, 18 ? Povo 38050  
Trento, Italy 
coppolab@itc.it 
  
Abstract 
Semantic inference is a key component 
for advanced natural language under-
standing. However, existing collections of 
automatically acquired inference rules 
have shown disappointing results when 
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods 
for automatically learning admissible ar-
gument values to which an inference rule 
can be applied, which we call inferential 
selectional preferences, and methods for 
filtering out incorrect inferences. We 
evaluate ISP and present empirical evi-
dence of its effectiveness. 
1 Introduction 
Semantic inference is a key component for ad-
vanced natural language understanding. Several 
important applications are already relying heavily 
on inference, including question answering 
(Moldovan et al 2003; Harabagiu and Hickl 2006), 
information extraction (Romano et al 2006), and 
textual entailment (Szpektor et al 2004). 
In response, several researchers have created re-
sources for enabling semantic inference. Among 
manual resources used for this task are WordNet 
(Fellbaum 1998) and Cyc (Lenat 1995). Although 
important and useful, these resources primarily 
contain prescriptive inference rules such as ?X di-
vorces Y ? X married Y?. In practical NLP appli-
cations, however, plausible inference rules such as 
?X married Y? ? ?X dated Y? are very useful. This, 
along with the difficulty and labor-intensiveness of 
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule 
collections (Lin and Pantel 2001; Szpektor et al 
2004) and paraphrase collections (Barzilay and 
McKeown 2001). 
Using these resources in applications has been 
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether 
incorrect rules or because of blind application of 
plausible rules without considering the context of 
the relations or the senses of the words. For exam-
ple, consider the following sentence: 
Terry Nichols was charged by federal prosecutors for murder 
and conspiracy in the Oklahoma City bombing. 
and an inference rule such as: 
 X is charged by Y ? Y announced the arrest of X (1) 
Using this rule, we can infer that ?federal prosecu-
tors announced the arrest of Terry Nichols?. How-
ever, given the sentence: 
Fraud was suspected when accounts were charged by CCM 
telemarketers without obtaining consumer authorization. 
the plausible inference rule (1) would incorrectly 
infer that ?CCM telemarketers announced the ar-
rest of accounts?. 
This example depicts a major obstacle to the ef-
fective use of automatically learned inference 
rules. What is missing is knowledge about the ad-
missible argument values for which an inference 
rule holds, which we call Inferential Selectional 
Preferences. For example, inference rule (1) 
should only be applied if X is a Person and Y is a 
Law Enforcement Agent or a Law Enforcement 
Agency. This knowledge does not guarantee that 
the inference rule will hold, but, as we show in this 
paper, goes a long way toward filtering out errone-
ous applications of rules. 
In this paper, we propose ISP, a collection of 
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The 
564
presented algorithms apply to any collection of 
inference rules between binary semantic relations, 
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of 
text. Within ISP, we explore different probabilistic 
models of selectional preference to accept or reject 
specific inferences. We present empirical evidence 
to support the following main contribution: 
Claim: Inferential selectional preferences can be 
automatically learned and used for effectively fil-
tering out incorrect inferences. 
2 Previous Work 
Selectional preference (SP) as a foundation for 
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and 
Fodor 1963).  Overviews of NLP research on this 
theme are (Wilks and Fass 1992), which includes 
the influential theory of Preference Semantics by 
Wilks, and more recently (Light and Greiff 2002). 
Rather than venture into learning inferential 
SPs, much previous work has focused on learning 
SPs for simpler structures. Resnik (1996), the 
seminal paper on this topic, introduced a statistical 
model for learning SPs for predicates using an un-
supervised method. 
Learning SPs often relies on an underlying set of 
semantic classes, as in both Resnik?s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections 
of semantic classes include the hierarchies of 
WordNet (Fellbaum 1998), Levin verb classes 
(Levin 1993), and FrameNet (Baker et al 1998). 
Automatic derivation of semantic classes can take 
a variety of approaches, but often uses corpus 
methods and the Distributional Hypothesis (Harris 
1964) to automatically cluster similar entities into 
classes, e.g. CBC (Pantel and Lin 2002). In this 
paper, we experiment with two sets of semantic 
classes, one from WordNet and one from CBC. 
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay 
and McKeown 2001) and inference rules, e.g. 
TEASE1 (Szpektor et al 2004) and DIRT (Lin and 
Pantel 2001). While these systems differ in their 
approaches, neither provides for the extracted in-
                                                     
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably. 
ference rules to hold or fail based on SPs. Zanzotto 
et al (2006) recently explored a different interplay 
between SPs and inferences. Rather than examine 
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences.  For instance the 
preference of win for the subject player, a nomi-
nalization of play, is used to derive that ?win ? 
play?. Our work can be viewed as complementary 
to the work on extracting semantic inferences and 
paraphrases, since we seek to refine when a given 
inference applies, filtering out incorrect inferences. 
3 Selectional Preference Models 
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules. 
Let pi ? pj be an inference rule where p is a bi-
nary semantic relation between two entities x and 
y. Let ?x, p, y? be an instance of relation p. 
Formal task definition: Given an inference rule 
 pi ? pj and the instance ?x, pi, y?, our task is to 
determine if ?x, pj, y? is valid. 
Consider the example in Section 1 where we 
have the inference rule ?X is charged by Y? ? ?Y 
announced the arrest of X?. Our task is to auto-
matically determine that ?federal prosecutors an-
nounced the arrest of Terry Nichols? (i.e., 
?Terry Nichols, pj, federal prosecutors?) is valid 
but that ?CCM telemarketers announced the arrest 
of accounts? is invalid. 
Because the semantic relations p are binary, the 
selectional preferences on their two arguments may 
be either considered jointly or independently. For 
example, the relation p = ?X is charged by Y? 
could have joint SPs: 
 ?Person, Law Enforcement Agent? 
 ?Person, Law Enforcement Agency?  (2) 
 ?Bank Account, Organization? 
or independent SPs: 
 ?Person, *? 
 ?*, Organization? (3) 
 ?*, Law Enforcement Agent? 
This distinction between joint and independent 
selectional preferences constitutes the difference 
between the two models we present in this section. 
The remainder of this section describes the ISP 
approach. In Section 3.1, we describe methods for 
automatically determining the semantic contexts of 
each single relation?s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential 
565
selectional preference models. Finally, we propose 
inference filtering algorithms in Section 3.3. 
3.1 Relational Selectional Preferences 
Resnik (1996) defined the selectional preferences 
of a predicate as the semantic classes of the words 
that appear as its arguments. Similarly, we define 
the relational selectional preferences of a binary 
semantic relation pi as the semantic classes C(x) of 
the words that can be instantiated for x and as the 
semantic classes C(y) of the words that can be in-
stantiated for y. 
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in 
(Resnik 1996), such as WordNet, or from the 
classes extracted from a word clustering algorithm 
such as CBC (Pantel and Lin 2002). For example, 
given the relation ?X is charged by Y?, its rela-
tional selection preferences from WordNet could 
be {social_group, organism, state?} for X and 
{authority, state, section?} for Y. 
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically 
determining relational selectional preferences. 
Model 1: Joint Relational Model (JRM) 
Our joint model uses a corpus analysis to learn SPs 
for binary semantic relations by considering their 
arguments jointly, as in example (2). 
Given a large corpus of English text, we first 
find the occurrences of each semantic relation p. 
For each instance ?x, p, y?, we retrieve the sets C(x) 
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples ?c(x), p, c(y)?, where c(x) ? C(x) and  
c(y) ? C(y)2. 
Each triple ?c(x), p, c(y)? is a candidate selec-
tional preference for p. Candidates can be incorrect 
when: a) they were generated from the incorrect 
sense of a polysemous word; or b) p does not hold 
for the other words in the semantic class. 
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely 
associated given the relation p. Pointwise mutual 
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association 
strength between two events e1 and e2: 
                                                     
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).  
 ( )( ) ( )21
21
21
,
log);(
ePeP
eeP
eepmi =  (3.1) 
We define our ranking function as the strength 
of association between two semantic classes, cx and 
cy3, given the relation p: 
 ( ) ( )( ) ( )pcPpcP pccPpcpcpmi yx yxyx
,
log; =  (3.2) 
Let |cx, p, cy| denote the frequency of observing 
the instance ?c(x), p, c(y)?. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus: 
( ) ?? ?= ,, ,,ppcpcP xx
 ( ) ???= ,, ,, pcppcP yy  ( ) ??= ,, ,,, p cpcpccP yxyx   (3.3) 
Similarly to (Resnik 1996), we estimate the 
above frequencies using: 
( )??
?=?
xcw
x wC
pw
pc
,,
,,
 
( )??
?=?
ycw
y wC
wp
cp
,,
,,
 
( ) ( )??? ?= yx cwcwyx wCwC
wpw
cpc
21 , 21
21 ,,,,
 
where |x, p, y| denotes the frequency of observing 
the instance ?x, p, y? and |C(w)| denotes the number 
of classes to which word w belongs. |C(w)| distrib-
utes w?s mass equally to all of its senses cw. 
Model 2: Independent Relational Model (IRM) 
Because of sparse data, our joint model can miss 
some correct selectional preference pairs. For ex-
ample, given the relation  
 Y announced the arrest of X 
we may find occurrences from our corpus of the 
particular class ?Money Handler? for X and ?Law-
yer? for Y, however we may never see both of 
these classes co-occurring even though they would 
form a valid relational selectional preference. 
To alleviate this problem, we propose a second 
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3). 
Similarly to JRM, we extract each instance  
?x, p, y? of each semantic relation p and retrieve the 
set of semantic classes C(x) and C(y) that x and y 
belong to, accumulating the frequencies of the tri-
ples ?c(x), p, *? and ?*, p, c(y)?, where  
c(x) ? C(x) and c(y) ? C(y). 
All tuples ?c(x), p, *? and ?*, p, c(y)? are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the semantic class given 
the relation p, according to Equations 3.3. 
                                                     
3 cx and cy are shorthand for c(x) and c(y) in our equations. 
566
3.2 Inferential Selectional Preferences 
Whereas in Section 3.1 we learned selectional 
preferences for the arguments of a relation p, in 
this section we learn selectional preferences for the 
arguments of an inference rule pi ? pj. 
Model 1: Joint Inferential Model (JIM) 
Given an inference rule pi ? pj, our joint model 
defines the set of inferential SPs as the intersection 
of the relational SPs for pi and pj, as defined in the 
Joint Relational Model (JRM). For example, sup-
pose relation pi = ?X is charged by Y? gives the 
following SP scores under the JRM: 
 ?Person, pi, Law Enforcement Agent? = 1.45 
 ?Person, pi, Law Enforcement Agency? = 1.21  
 ?Bank Account, pi, Organization? = 0.97 
and that pj = ?Y announced the arrest of X? gives 
the following SP scores under the JRM: 
 ?Law Enforcement Agent, pj, Person? = 2.01 
 ?Reporter, pj, Person? = 1.98  
 ?Law Enforcement Agency, pj, Person? = 1.61 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, Person? 
 ?Law Enforcement Agency, Person? 
We rank the candidate inferential SPs according 
to three ways to combine their relational SP scores, 
using the minimum, maximum, and average of the 
SPs. For example, for ?Law Enforcement Agent, 
Person?, the respective scores would be 1.45, 2.01, 
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments, 
as discussed in Section 5. 
Model 2: Independent Inferential Model (IIM) 
Our independent model is the same as the joint 
model above except that it computes candidate in-
ferential SPs using the Independent Relational 
Model (IRM) instead of the JRM. Consider the 
same example relations pi and pj from the joint 
model and suppose that the IRM gives the follow-
ing relational SP scores for pi: 
 ?Law Enforcement Agent, pi, *? = 3.43 
 ?*, pi, Person? = 2.17  
 ?*, pi, Organization? = 1.24 
and the following relational SP scores for pj: 
 ?*, pj, Person? = 2.87 
 ?Law Enforcement Agent, pj, *? = 1.92  
 ?Reporter, pj, *? = 0.89 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, *? 
 ?*, Person?  
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM. 
3.3 Filtering Inferences 
Given an inference rule pi ? pj and the instance  
?x, pi, y?, the system?s task is to determine whether 
?x, pj, y? is valid. Let C(w) be the set of semantic 
classes c(w) to which word w belongs. Below we 
present three filtering algorithms which range from 
the least to the most permissive: 
? ISP.JIM, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, c(y)? was admitted by the 
Joint Inferential Model for some c(x) ? C(x) and 
c(y) ? C(y). 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SPs ?c(x), pj, *? AND ?*, pj, c(y)? were 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, *? OR ?*, pj, c(y)? was 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
Since both JIM and IIM use a ranking score in 
their inferential SPs, each filtering algorithm can 
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top ? percent highest ranking SPs. 
In our experiments, reported in Section 5, we 
tested each model using various values of ?. 
4 Experimental Methodology 
This section describes the methodology for testing 
our claim that inferential selectional preferences 
can be learned to filter incorrect inferences. 
Given a collection of inference rules of the form 
pi ? pj, our task is to determine whether a particu-
lar instance ?x, pj, y? holds given that ?x, pi, y? 
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used 
for forming selectional preferences, and evaluation 
criteria for measuring the filtering quality. 
                                                     
4 Recall that the inference rules we consider in this paper are 
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3. 
567
4.1 Inference Rules 
Our models for learning inferential selectional 
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In 
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001). 
DIRT consists of over 12 million rules which were 
extracted from a 1GB newspaper corpus (San Jose 
Mercury, Wall Street Journal and AP Newswire 
from the TREC-9 collection). For example, here 
are DIRT?s top 3 inference rules for ?X solves Y?: 
 ?Y is solved by X?, ?X resolves Y?, ?X finds a solution to Y? 
4.2 Semantic Classes 
The choice of semantic classes is of great impor-
tance for selectional preference. One important 
aspect is the granularity of the classes. Too general 
a class will provide no discriminatory power while 
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases. 
The absence of an attested high-quality set of 
semantic classes for this task makes discovering 
preferences difficult. Since many of the criteria for 
developing such a set are not even known, we de-
cided to experiment with two very different sets of 
semantic classes, in the hope that in addition to 
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about 
what makes good semantic classes in general. 
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to 
the TREC-9 and TREC-2002 (Aquaint) newswire 
collections consisting of over 600 million words. 
CBC generated 1628 noun concepts and these were 
used as our semantic classes for SPs. 
Secondly, we extracted semantic classes from 
WordNet 2.1 (Fellbaum 1998). In the absence of 
any externally motivated distinguishing features 
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch 
(1978)), we used the simple but effective method 
of manually truncating the noun synset hierarchy5 
and considering all synsets below each cut point as 
part of the semantic class at that node. To select 
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4 
                                                     
5 Only nouns are considered since DIRT semantic relations 
connect only nouns. 
to form the most natural semantic classes. Since 
the noun hierarchy in WordNet has an average 
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet 
itself. The cut produced 1287 semantic classes, a 
number similar to the classes in CBC. To properly 
test WordNet as a source of semantic classes for 
our selectional preferences, we would need to ex-
periment with different extraction algorithms. 
4.3 Evaluation Criteria 
The goal of the filtering task is to minimize false 
positives (incorrectly accepted inferences) and 
false negatives (incorrectly rejected inferences). A 
standard methodology for evaluating such tasks is 
to compare system filtering results with a gold 
standard using a confusion matrix. A confusion 
matrix captures the filtering performance on both 
correct and incorrect inferences: 
  
where A represents the number of correct instances 
correctly identified by the system, D represents the 
number of incorrect instances correctly identified 
by the system, B represents the number of false 
positives and C represents the number of false 
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices: 
? Sensitivity, defined as CA
A
+ , captures a filter?s 
probability of accepting correct inferences; 
? Specificity, defined as DB
D
+ , captures a filter?s 
probability of rejecting incorrect inferences; 
? Accuracy, defined as DCBA
DA
+++
+ , captures the 
probability of a filter being correct. 
5 Experimental Results 
In this section, we provide empirical evidence to 
support the main claim of this paper. 
Given a collection of DIRT inference rules of 
the form pi ? pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our 
ISP models for determining if ?x, pj, y? holds given 
that ?x, pi, y? holds. 
GOLD STANDARD   
1 0 
1 A B 
SY
ST
E
M
 
0 C D 
568
5.1 Experimental Setup 
Model Implementation 
For each filtering algorithm in Section 3.3, ISP.JIM, 
ISP.IIM.?, and ISP.IIM.?, we trained their probabil-
istic models using corpus statistics extracted from 
the 1999 AP newswire collection (part of the 
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in 
the text. This permits exact matches since DIRT 
inference rules are built from Minipar parse trees. 
For each system, we experimented with the dif-
ferent ways of combining relational SP scores: 
minimum, maximum, and average (see Section 
3.2). Also, we experimented with various values 
for the ? parameter described in Section 3.3. 
Gold Standard Construction 
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a 
representative set of inferences and manually anno-
tate them as correct or incorrect. 
We randomly selected 100 inference rules of the 
form pi ? pj from DIRT. For each pattern pi, we 
then extracted its instances from the Aquaint 1999 
AP newswire collection (approximately 22 million 
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For 
each instance of pi, applying DIRT?s inference rule 
would assert the instance ?x, pj, y?. Our evaluation 
tests how well our models can filter these so that 
only correct inferences are made. 
To form the gold standard, two human judges 
were asked to tag each instance ?x, pj, y? as correct 
or incorrect. For example, given a randomly se-
lected inference rule ?X is charged by Y ? Y an-
nounced the arrest of X? and the instance ?Terry 
Nichols was charged by federal prosecutors?, the 
judges must determine if the instance ?federal 
prosecutors, Y announced the arrest of X, Terry 
Nichols? is correct. The judges were asked to con-
sider the following two criteria for their decision: 
? ?x, pj, y? is a semantically meaningful instance; 
? The inference pi ? pj holds for this instance. 
Judges found that annotation decisions can range 
from trivial to difficult. The differences often were 
in the instances for which one of the judges fails to 
see the right context under which the inference 
could hold. To minimize disagreements, the judges 
went through an extensive round of training. 
To that end, the 1000 instances ?x, pj, y? were 
split into DEV and TEST sets, 500 in each. The 
two judges trained themselves by annotating DEV 
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and 
to verify whether the task is well-defined. The 
kappa statistic (Siegel and Castellan Jr. 1988) was 
? = 0.72. For the 70 disagreements between the 
judges, a third judge acted as an adjudicator. 
Baselines 
We compare our ISP algorithms to the following 
baselines: 
? B0: Rejects all inferences; 
? B1: Accepts all inferences; 
? Rand: Randomly accepts or rejects inferences. 
One alternative to our approach is admit instances 
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle 
yet critical issues with pattern canonicalization that 
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web 
corpora for this task. 
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on 
the TEST set ? the reported systems were selected based on the Accuracy criterion on the DEV set. 
PARAMETERS SELECTED FROM DEV SET 
SYSTEM 
RANKING STRATEGY ? (%) 
SENSITIVITY 
(95% CONF) 
SPECIFICITY 
(95% CONF) 
ACCURACY 
(95% CONF) 
B0 - - 0.00?0.00 1.00?0.00 0.50?0.04 
B1 - - 1.00?0.00 0.00?0.00 0.49?0.04 
Random - - 0.50?0.06 0.47?0.07 0.50?0.04 
ISP.JIM maximum 100 0.17?0.04 0.88?0.04 0.53?0.04 
ISP.IIM.? maximum 100 0.24?0.05 0.84?0.04 0.54?0.04 CBC 
ISP.IIM.? maximum 90 0.73?0.05 0.45?0.06 0.59?0.04? 
ISP.JIM minimum 40 0.20?0.06 0.75?0.06 0.47?0.04 
ISP.IIM.? minimum 10 0.33?0.07 0.77?0.06 0.55?0.04 WordNet 
ISP.IIM.? minimum 20 0.87?0.04 0.17?0.05 0.51?0.05 
? Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 
569
5.2 Filtering Quality 
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity, 
specificity and accuracy as described in Section 
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic 
class source, we selected the best parameter com-
binations according to the following criteria: 
? Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences. 
? 90%-Specificity: Several formal semantics and 
textual entailment researchers have commented 
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have 
asked for filtered versions that remove incorrect 
inferences even at the cost of removing correct 
inferences. In response, we show results for the 
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set. 
We evaluated the selected systems on the TEST 
set. Table 1 summarizes the quality of the systems 
selected according to the Accuracy criterion. The 
best performing system, ISP.IIM.?, performed  sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as 
shown in Table 16. This result is very promising 
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to 
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time. 
Performance and Error Analysis 
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set. 
The most accurate system was ISP.IIM.?, which is 
the most permissive of the algorithms. This sug-
                                                     
6 The reported sensitivity of ISP.Joint in Table 1 is below 
90%, however it achieved 90.7% on the DEV set. 
gests that a larger corpus for learning SPs may be 
needed to support stronger performance on the 
more restrictive methods. The system in Figure 
1b), selected for maximizing sensitivity while 
maintaining high specificity, was 70% correct in 
predicting correct inferences. 
Figure 2 illustrates the ROC curve for all our 
systems and parameter combinations on the TEST 
set. ROC curves plot the true positive rate against 
the false positive rate. The near-diagonal line plots 
the three baseline systems. 
Several trends can be observed from this figure. 
First, systems using the semantic classes from 
WordNet tend to perform less well than systems 
using CBC classes. As discussed in Section 4.2, we 
used a very simplistic extraction of semantic 
classes from WordNet. The results in Figure 2 
serve as a lower bound on what could be achieved 
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect 
but CBC got correct, it seemed that CBC had a 
much higher lexical coverage than WordNet. For 
example, several of the instances contained proper 
names as either the X or Y argument (WordNet has 
poor proper name coverage). When an argument is 
not covered by any class, the inference is rejected. 
Figure 2 also illustrates how our three different 
ISP algorithms behave. The strictest filters, ISP.JIM 
and ISP.IIM.?, have the poorest overall perform-
ance but, as expected, have a generally very low 
rate of false positives. ISP.IIM.?, which is a much 
more permissive filter because it does not require 
ROC on the TEST Set
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
S
en
si
tiv
ity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
Figure 2. ROC curves for our systems on TEST. 
GOLD STANDARD a)  
1 0 
1 184 139 
SY
ST
E
M
 
0 63 114 
GOLD STANDARD b)  
1 0 
1 42 28 
SY
ST
E
M
 
0 205 225 
Figure 1. Confusion matrices for a) ISP.IIM.? ? best 
Accuracy; and b) ISP.JIM ? best 90%-Specificity.
570
both arguments of a relation to match, has gener-
ally many more false positives but has an overall 
better performance. 
We did not include in Figure 2 an analysis of the 
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally 
produced nearly identical results. 
For the most accurate system, ISP.IIM.?, we ex-
plored the impact of the cutoff threshold ? on the 
sensitivity, specificity, and accuracy, as shown in 
Figure 3. Rather than step the values by 10% as we 
did on the DEV set, here we stepped the threshold 
value by 2% on the TEST set. The more permis-
sive values of ? increase sensitivity at the expense 
of specificity. Interestingly, the overall accuracy 
remained fairly constant across the entire range of 
?, staying within 0.05 of the maximum of 0.62 
achieved at ?=30%. 
Finally, we manually inspected several incorrect 
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect 
?antonymy? inference rules generated by DIRT, 
such as ?X is rejected in Y???X is accepted in Y?. 
This recognized problem in DIRT occurs because 
of the distributional hypothesis assumption used to 
form the inference rules. Our ISP algorithms suffer 
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for 
X (and Y). For these cases, ISP algorithms learn 
many selectional preferences that accept the same 
types of entities as those that made DIRT learn the 
inference rule in the first place, hence ISP will not 
filter out many incorrect inferences. 
6 Conclusion 
We presented algorithms for learning what we call 
inferential selectional preferences, and presented 
evidence that learning selectional preferences can 
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic 
classes used as SP?s. This work constitutes a step 
towards better understanding of the interaction of 
selectional preferences and inferences, bridging 
these two aspects of semantics. 
References 
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a 
Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, 
France. 
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley 
FrameNet Project. In Proceedings of COLING/ACL 1998.  pp. 86-
90. Montreal, Canada. 
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. 
John Wiley & Sons. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT 
Press. 
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual 
Entailment in Open-Domain Question Answering. In Proceedings 
of ACL 2006.  pp. 905-912. Sydney, Australia. 
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. 
Language, vol 39. pp.170?210.  
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary 
Investigation. University of Chicago Press, Chicago, IL. 
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction 
and Use of Selectional Preferences. Cognitive Science,26:269?281. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of  
ACL-93. pp. 112-120. Columbus, OH. 
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for 
Question Answering. Natural Language Engineering 7(4):343-360. 
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. 
COGEX: A Logic Prover for Question Answering. In Proceedings 
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic 
Model and its Computational Realization. Cognition, 61:127?159. 
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. 
Investigating a Generic Paraphrase-Based Approach for Relation 
Extraction. In EACL-2006. pp. 409-416. Trento, Italy. 
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd 
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.  
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling 
web-based acquisition of entailment relations. In Proceedings of 
EMNLP 2004. pp. 41-48. Barcelona,Spain. 
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. 
Computing and Mathematics with Applications, 23(2). A shorter 
version in the second edition of the Encyclopedia of Artificial 
Intelligence, (ed.) S. Shapiro. 
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering 
Asymmetric Entailment Relations between Verbs using Selectional 
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia. 
Figure 3. ISP.IIM.? (Best System)?s performance 
variation over different values for the ? threshold. 
ISP.IIM.OR (Best System)'s Performance vs. Tau-Thresholds
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
Sensitivity Specificity Accuracy
571
Proceedings of the 43rd Annual Meeting of the ACL, pages 125?132,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Inducing Ontological Co-occurrence Vectors 
 
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
pantel@isi.edu 
 
 
Abstract 
In this paper, we present an unsupervised 
methodology for propagating lexical co-
occurrence vectors into an ontology such 
as WordNet. We evaluate the framework 
on the task of automatically attaching new 
concepts into the ontology. Experimental 
results show 73.9% attachment accuracy 
in the first position and 81.3% accuracy in 
the top-5 positions. This framework could 
potentially serve as a foundation for on-
tologizing lexical-semantic resources and 
assist the development of other large-
scale and internally consistent collections 
of semantic information. 
1 Introduction 
Despite considerable effort, there is still today no 
commonly accepted semantic corpus, semantic 
framework, notation, or even agreement on pre-
cisely which aspects of semantics are most useful 
(if at all). We believe that one important reason 
for this rather startling fact is the absence of truly 
wide-coverage semantic resources. 
Recognizing this, some recent work on wide 
coverage term banks, like WordNet (Miller 1990) 
and CYC (Lenat 1995), and annotated corpora, 
like FrameNet (Baker et al 1998), Propbank 
(Kingsbury et al 2002) and Nombank (Meyers et 
al. 2004), seeks to address the problem.  But man-
ual efforts such as these suffer from two draw-
backs: they are difficult to tailor to new domains, 
and they have internal inconsistencies that can 
make automating the acquisition process difficult.   
In this work, we introduce a general frame-
work for inducing co-occurrence feature vectors 
for nodes in a WordNet-like ontology. We be-
lieve that this framework will be useful for a va-
riety of applications, including adding additional 
semantic information to existing semantic term 
banks by disambiguating lexical-semantic re-
sources. 
Ontologizing semantic resources 
Recently, researchers have applied text- and 
web-mining algorithms for automatically creating 
lexical semantic resources like similarity lists 
(Lin 1998), semantic lexicons (Riloff and Shep-
herd 1997), hyponymy lists (Shinzato and Tori-
sawa 2004; Pantel and Ravichandran 2004), part-
whole lists (Girgu et al 2003), and verb relation 
graphs (Chklovski and Pantel 2004). However, 
none of these resources have been directly linked 
into an ontological framework. For example, in 
VERBOCEAN (Chklovski and Pantel 2004), we 
find the verb relation ?to surpass is-stronger-than 
to hit?, but it is not specified that it is the achiev-
ing sense of hit where this relation applies. 
We term ontologizing a lexical-semantic re-
source as the task of sense disambiguating the re-
source. This problem is different but not 
orthogonal to word-sense disambiguation. If we 
could disambiguate large collections of text with 
high accuracy, then current methods for building 
lexical-semantic resources could easily be applied 
to ontologize them by treating each word?s senses 
as separate words. Our method does not require 
the disambiguation of text. Instead, it relies on the 
principle of distributional similarity and that 
polysemous words that are similar in one sense 
are dissimilar in their other senses. 
125
Given the enriched ontologies produced by 
our method, we believe that ontologizing lexical-
semantic resources will be feasible. For example, 
consider the example verb relation ?to surpass is-
stronger-than to hit? from above. To disambigu-
ate the verb hit, we can look at all other verbs that 
to surpass is stronger than (for example, in 
VERBOCEAN, ?to surpass is-stronger-than to 
overtake? and ?to surpass is-stronger-than to 
equal?). Now, we can simply compare the lexical 
co-occurrence vectors of overtake and equal with 
the ontological feature vectors of the senses of hit 
(which are induced by our framework). The sense 
whose feature vector is most similar is selected. 
It remains to be seen in future work how well 
this approach performs on ontologizing various 
semantic resources. In this paper, we focus on the 
general framework for inducing the ontological 
co-occurrence vectors and we apply it to the task 
of linking new terms into the ontology. 
2 Relevant work 
Our framework aims at enriching WordNet-like 
ontologies with syntactic features derived from a 
non-annotated corpus. Others have also made 
significant additions to WordNet. For example, in 
eXtended WordNet (Harabagiu et al 1999), the 
rich glosses in WordNet are enriched by disam-
biguating the nouns, verbs, adverbs, and adjec-
tives with synsets. Another work has enriched 
WordNet synsets with topically related words ex-
tracted from the Web (Agirre et al 2001). While 
this method takes advantage of the redundancy of 
the web, our source of information is a local 
document collection, which opens the possibility 
for domain specific applications. 
Distributional approaches to building semantic 
repositories have shown remarkable power. The 
underlying assumption, called the Distributional 
Hypothesis (Harris 1985), links the semantics of 
words to their lexical and syntactic behavior. The 
hypothesis states that words that occur in the 
same contexts tend to have similar meaning. Re-
searchers have mostly looked at representing 
words by their surrounding words (Lund and Bur-
gess 1996) and by their syntactical contexts 
(Hindle 1990; Lin 1998). However, these repre-
sentations do not distinguish between the differ-
ent senses of words. Our framework utilizes these 
principles and representations to induce disam-
biguated feature vectors. We describe these rep-
resentations further in Section 3. 
In supervised word sense disambiguation, 
senses are commonly represented by their sur-
rounding words in a sense-tagged corpus (Gale et 
al. 1991). If we had a large collection of sense-
tagged text, then we could extract disambiguated 
feature vectors by collecting co-occurrence fea-
tures for each word sense. However, since there is 
little sense-tagged text available, the feature vec-
tors for a random WordNet concept would be 
very sparse. In our framework, feature vectors are 
induced from much larger untagged corpora (cur-
rently 3GB of newspaper text). 
Another approach to building semantic reposi-
tories is to collect and merge existing ontologies.  
Attempts to automate the merging process have 
not been particularly successful (Knight and Luk 
1994; Hovy 1998; Noy and Musen 1999).  The 
principal problems of partial and unbalanced cov-
erage and of inconsistencies between ontologies 
continue to hamper these approaches. 
3 Resources 
The framework we present in Section 4 propa-
gates any type of lexical feature up an ontology. 
In previous work, lexicals have often been repre-
sented by proximity and syntactic features. Con-
sider the following sentence: 
The tsunami left a trail of horror. 
In a proximity approach, a word is represented 
by a window of words surrounding it. For the 
above sentence, a window of size 1 would yield 
two features (-1:the and +1:left) for the word tsu-
nami. In a syntactic approach, more linguistically 
rich features are extracted by using each gram-
matical relation in which a word is involved (e.g. 
the features for tsunami are determiner:the and 
subject-of:leave). 
For the purposes of this work, we consider the 
propagation of syntactic features. We used Mini-
par (Lin 1994), a broad coverage parser, to ana-
lyze text. We collected the statistics on the 
grammatical relations (contexts) output by Mini-
par and used these as the feature vectors. Follow-
ing Lin (1998), we measure each feature f for a 
word e not by its frequency but by its pointwise 
mutual information, mief: 
126
 ( )( ) ( )fPeP
fePmief ?=
,log  
4 Inducing ontological features 
The resource described in the previous section 
yields lexical feature vectors for each word in a 
corpus. We term these vectors lexical because 
they are collected by looking only at the lexicals 
in the text (i.e. no sense information is used). We 
use the term ontological feature vector to refer to 
a feature vector whose features are for a particu-
lar sense of the word. 
In this section, we describe our framework for 
inducing ontological feature vectors for each 
node of an ontology. Our approach employs two 
phases. A divide-and-conquer algorithm first 
propagates syntactic features to each node in the 
ontology. A final sweep over the ontology, which 
we call the Coup phase, disambiguates the feature 
vectors of lexicals (leaf nodes) in the ontology. 
4.1 Divide-and-conquer phase 
In the first phase of the algorithm, we propagate 
features up the ontology in a bottom-up approach. 
Figure 1 gives an overview of this phase. 
The termination condition of the recursion is 
met when the algorithm processes a leaf node. 
The feature vector that is assigned to this node is 
an exact copy of the lexical feature vector for that 
leaf (obtained from a large corpus as described in 
Section 3). For example, for the two leaf nodes 
labeled chair in Figure 2, we assign to both the 
same ambiguous lexical feature vector, an excerpt 
of which is shown in Figure 3. 
When the recursion meets a non-leaf node, 
like chairwoman in Figure 2, the algorithm first 
recursively applies itself to each of the node?s 
children. Then, the algorithm selects those fea-
tures common to its children to propagate up to 
its own ontological feature vector. The assump-
tion here is that features of other senses of 
polysemous words will not be propagated since 
they will not be common across the children. Be-
low, we describe the two methods we used to 
propagate features: Shared and Committee. 
Shared propagation algorithm 
The first technique for propagating features to a 
concept node n from its children C is the simplest 
and scored best in our evaluation (see Section 
5.2). The goal is that the feature vector for n 
Input: A node n and a corpus C. 
Step 1: Termination Condition: 
  If n is a leaf node then assign to n its lexical 
feature vector as described in Section 3. 
Step 2: Recursion Step: 
  For each child c of n, reecurse on c and C. 
  Assign a feature vector to n by propagating 
features from its children. 
Output: A feature vector assigned to each node of the 
tree rooted by n. 
Figure 1. Divide-and-conquer phase. 
chair stool armchairchaise-longue
taboret musicstool
step
stool
cutty
stool
desk
chair
chair
seating
furniture
furniture
furniture bedmirror table
concept
leaf node
Legend:
chair chairman president chair-woman
vice
chairman
vice
chairman
chair-
woman
leader
Decom-
posable
object
Figure 2. Subtrees of WordNet illustrating two senses 
of chair. 
"chair" 
 conjunction: 
   sofa 77 11.8 
   professor 11 6.0 
   dining room 2 5.6 
   cushion 1 4.5 
   council member 1 4.4 
   President 9 2.9 
   foreign minister 1 2.8 
 nominal subject 
   Ottoman 8 12.1 
   director 22 9.1 
   speaker 8 8.6 
   Joyner 2 8.22 
   recliner 2 7.7 
   candidate 1 3.5  
Figure 3. Excerpt of a lexical feature vector for the 
word chair. Grammatical relations are in italics (con-
junction and nominal-subject). The first column of 
numbers are frequency counts and the other are mutual 
information scores. In bold are the features that inter-
sect with the induced ontological feature vector for the 
parent concept of chair?s chairwoman sense. 
127
represents the general grammatical behavior that 
its children will have. For example, for the con-
cept node furniture in Figure 2, we would like to 
assign features like object-of:clean since 
mosttypes of furniture can be cleaned. However, 
even though you can eat on a table, we do not 
want the feature on:eat for the furniture concept 
since we do not eat on mirrors or beds. 
In the Shared propagation algorithm, we 
propagate only those features that are shared by at 
least t children. In our experiments, we experi-
mentally set t = min(3, |C|). 
The frequency of a propagated feature is ob-
tained by taking a weighted sum of the frequency 
of the feature across its children. Let fi be the fre-
quency of the feature for child i, let ci be the total 
frequency of child i, and let N be the total fre-
quency of all children. Then, the frequency f of 
the propagated feature is given by: 
 ? ?=
i
i
i N
c
ff  (1) 
Committee propagation algorithm 
The second propagation algorithm finds a set of 
representative children from which to propagate 
features. Pantel and Lin (2002) describe an algo-
rithm, called Clustering By Committee (CBC), 
which discovers clusters of words according to 
their meanings in test. The key to CBC is finding 
for each class a set of representative elements, 
called a committee, which most unambiguously 
describe the members of the class. For example, 
for the color concept, CBC discovers the follow-
ing committee members: 
purple, pink, yellow, mauve, turquoise, 
beige, fuchsia 
Words like orange and violet are avoided be-
cause they are polysemous. For a given concept c, 
we build a committee by clustering its children 
according to their similarity and then keep the 
largest and most interconnected cluster (see 
Pantel and Lin (2002) for details). 
The propagated features are then those that are 
shared by at least two committee members. The 
frequency of a propagated feature is obtained us-
ing Eq. 1 where the children i are chosen only 
among the committee members. 
Generating committees using CBC works best 
for classes with many members. In its original 
application (Pantel and Lin 2002), CBC discov-
ered a flat list of coarse concepts. In the finer 
grained concept hierarchy of WordNet, there are 
many fewer children for each concept so we ex-
pect to have more difficulty finding committees. 
4.2 Coup phase 
At the end of the Divide-and-conquer phase, the 
non-leaf nodes of the ontology contain disam-
biguated features1. By design of the propagation 
algorithm, each concept node feature is shared by 
at least two of its children. We assume that two 
polysemous words, w1 and w2, that are similar in 
one sense will be dissimilar in its other senses. 
Under the distributional hypothesis, similar words 
occur in the same grammatical contexts and dis-
similar words occur in different grammatical con-
texts. We expect then that most features that are 
shared between w1 and w2 will be the grammati-
cal contexts of their similar sense. Hence, mostly 
disambiguated features are propagated up the on-
tology in the Divide-and-conquer phase. 
However, the feature vectors for the leaf 
nodes remain ambiguous (e.g. the feature vectors 
for both leaf nodes labeled chair in Figure 2 are 
identical). In this phase of the algorithm, leaf 
node feature vectors are disambiguated by look-
ing at the parents of their other senses. 
Leaf nodes that are unambiguous in the ontol-
ogy will have unambiguous feature vectors. For 
ambiguous leaf nodes (i.e. leaf nodes that have 
more than one concept parent), we apply the al-
gorithm described in Figure 4. Given a polyse-
mous leaf node n, we remove from its ambiguous 
                                                     
1 By disambiguated features, we mean that the features 
are co-occurrences with a particular sense of a word; the 
features themselves are not sense-tagged. 
Input: A node n and the enriched ontology O output 
from the algorithm in Figure 1. 
Step 1: If n is not a leaf node then return. 
Step 2: Remove from n?s feature vector all features 
that intersect with the feature vector of any of 
n?s other senses? parent concepts, but are not 
in n?s parent concept feature vector. 
Output: A disambiguated feature vector for each leaf 
node  n. 
Figure 4. Coup phase. 
128
feature vector those features that intersect with 
the ontological feature vector of any of its other 
senses? parent concept but that are not in its own 
parent?s ontological feature vector. For example, 
consider the furniture sense of the leaf node chair 
in Figure 2. After the Divide-and-conquer phase, 
the node chair is assigned the ambiguous lexical 
feature vector shown in Figure 3. Suppose that 
chair only has one other sense in WordNet, 
which is the chairwoman sense illustrated in Fig-
ure 2. The features in bold in Figure 3 represent 
those features of chair that intersect with the on-
tological feature vector of chairwoman. In the 
Coup phase of our system, we remove these bold 
features from the furniture sense leaf node chair. 
What remains are features like ?chair and sofa?, 
?chair and cushion?, ?Ottoman is a chair?, and 
?recliner is a chair?. Similarly, for the chair-
woman sense of chair, we remove those features 
that intersect with the ontological feature vector 
of the chair concept (the parent of the other chair 
leaf node). 
As shown in the beginning of this section, 
concept node feature vectors are mostly unambi-
guous after the Divide-and-conquer phase. How-
ever, the Divide-and-conquer phase may be 
repeated after the Coup phase using a different 
termination condition. Instead of assigning to leaf 
nodes ambiguous lexical feature vectors, we use 
the leaf node feature vectors from the Coup 
phase. In our experiments, we did not see any 
significant performance difference by skipping 
this extra Divide-and-conquer step. 
5 Experimental results 
In this section, we provide a quantitative and 
qualitative evaluation of our framework. 
5.1 Experimental Setup 
We used Minipar (Lin 1994), a broad coverage 
parser, to parse two 3GB corpora (TREC-9 and 
TREC-2002). We collected the frequency counts 
of the grammatical relations (contexts) output by 
Minipar and used these to construct the lexical 
feature vectors as described in Section 3. 
WordNet 2.0 served as our testing ontology. 
Using the algorithm presented in Section 4, we 
induced ontological feature vectors for the noun 
nodes in WordNet using the lexical co-occurrence 
features from the TREC-2002 corpus. Due to 
memory limitations, we were only able to propa-
gate features to one quarter of the ontology. We 
experimented with both the Shared and Commit-
tee propagation models described in Section 4.1. 
5.2 Quantitative evaluation 
To evaluate the resulting ontological feature vec-
tors, we considered the task of attaching new 
nodes into the ontology. To automatically evalu-
ate this, we randomly extracted a set of 1000 
noun leaf nodes from the ontology and accumu-
lated lexical feature vectors for them using the 
TREC-9 corpus (a separate corpus than the one 
used to propagate features, but of the same 
genre). We experimented with two test sets: 
? Full: The 424 of the 1000 random nodes that 
existed in the TREC-9 corpus 
? Subset: Subset of Full where only nodes that do 
not have concept siblings are kept (380 nodes). 
For each random node, we computed the simi-
larity of the node with each concept node in the 
ontology by computing the cosine of the angle 
(Salton and McGill 1983) between the lexical 
feature vector of the random node ei and the onto-
logical feature vector of the concept nodes ej: 
 ( ) ??
?
?
?
=
f
fe
f
fe
f
fefe
ji
ji
ji
mimi
mimi
eesim
22
,
 
We only kept those similar nodes that had a 
similarity above a threshold ?. We experimentally 
set ? = 0.1. 
Top-K accuracy 
We collected the top-K most similar concept 
nodes (attachment points) for each node in the 
test sets and computed the accuracy of finding a 
correct attachment point in the top-K list. Table 1 
shows the result. 
We expected the algorithm to perform better 
on the Subset data set since only concepts that 
have exclusively lexical children must be consid-
ered for attachment. In the Full data set, the algo-
rithm must consider each concept in the ontology 
as a potential attachment point. However, consid-
ering the top-5 best attachments, the algorithm 
performed equally well on both data sets.  
The Shared propagation algorithm performed 
consistently slightly better than the Committee 
method. As described in Section 4.1, building a 
129
committee performs best for concepts with many 
children. Since many nodes in WordNet have few 
direct children, the Shared propagation method is 
more appropriate. One possible extension of the 
Committee propagation algorithm is to find com-
mittee members from the full list of descendants 
of a node rather than only its immediate children. 
Precision and Recall 
We computed the precision and recall of our sys-
tem on varying numbers of returned attachments. 
Figure 5 and Figure 6 show the attachment preci-
sion and recall of our system when the maximum 
number of returned attachments ranges between 1 
and 5. In Figure 5, we see that the Shared propa-
gation method has better precision than the 
Committee method. Both methods perform simi-
larly on recall. The recall of the system increases 
most dramatically when returning two attach-
ments without too much of a hit on precision. The 
low recall when returning only one attachment is 
due to both system errors and also to the fact that 
many nodes in the hierarchy are polysemous. In 
the next section, we discuss further experiments 
on polysemous nodes. Figure 6 illustrates the 
large difference on both precision and recall 
when using the simpler Subset data set. All 95% 
confidence bounds in Figure 5 and Figure 6 range 
between ?2.8% and ?5.3%. 
Polysemous nodes 
84 of the nodes in the Full data set are polyse-
mous (they are attached to more than one concept 
node in the ontology). On average, these nodes 
have 2.6 senses for a total of 219 senses. Figure 7 
compares the precision and recall of the system 
on all nodes in the Full data set vs. the 84 
polysemous nodes. The 95% confidence intervals 
range between ?3.8% and ?5.0% for the Full data 
set and between ?1.2% and ?9.4% for the 
polysemous nodes. The precision on the polyse-
mous nodes is consistently better since these have 
more possible correct attachments. 
Clearly, when the system returns at most one 
or two attachments, the recall on the polysemous 
nodes is lower than on the Full set. However, it is 
interesting to note that recall on the polysemous 
nodes equals the recall on the Full set after K=3. 
Table 1. Correct attachment point in the top-K attachments (with 95% conf.) 
K Shared (Full) Committee (Full) Shared (Subset) Committee (Subset) 
1 73.9% ? 4.5% 72.0% ? 4.9% 77.4% ? 3.6% 76.1% ? 5.1% 
2 78.7% ? 4.1% 76.6% ? 4.2% 80.7% ? 4.0% 79.1% ? 4.5% 
3 79.9% ? 4.0% 78.2% ? 4.2% 81.2% ? 3.9% 80.5% ? 4.8% 
4 80.6% ? 4.1% 79.0% ? 4.0% 81.5% ? 4.1% 80.8% ? 5.0% 
5 81.3% ? 3.8% 79.5% ? 3.9% 81.7% ? 4.1% 81.3% ? 4.9% 
 
Figure 5. Attachment precision and recall for the 
Shared and Committee propagation methods when 
returning at most K attachments (on the Full set). 
Precision and Recall (Shared and Committee) vs. 
Number of Returned Attachments
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
K
Precision (Shared) Recall (Shared)
Precision (Committee) Recall (Committee)
Precision and Recall (Full and Subset) vs. 
Number of Returned Attachments
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
K
Precision (Full) Recall (Full)
Precision (Subset) Recall (Subset)
Figure 6. Attachment precision and recall for the 
Full and Subset data sets when returning at most K 
attachments (using the Shared propagation method). 
130
5.3 Qualitative evaluation 
Inspection of errors revealed that the system often 
makes plausible attachments. Table 2 shows 
some example errors generated by our system. 
For the word arsenic, the system attached it to the 
concept trioxide, which is the parent of the cor-
rect attachment. 
The system results may be useful to help vali-
date the ontology. For example, for the word law, 
the system attached it to the regulation (as an or-
ganic process) and ordinance (legislative act) 
concepts. According to WordNet, law has seven 
possible attachment points, none of which are a 
legislative act. Hence, the system has found that 
in the TREC-9 corpus, the word law has a sense 
of legislative act. Similarly, the system discov-
ered the symptom sense of vomiting. 
The system discovered a potential anomaly in 
WordNet with the word slob. The system classi-
fied slob as follows: 
fool ? simpleton ? someone 
whereas WordNet classifies it as: 
vulgarian ? unpleasant person ? unwel-
come person ? someone 
The ontology could use this output to verify if 
fool should link in the unpleasant person subtree. 
Capitalization is not very trustworthy in large 
collections of text. One of our design decisions 
was to ignore the case of words in our corpus, 
which in turn caused some errors since WordNet 
is case sensitive. For example, the lexical node 
Munch (Norwegian artist) was attached to the 
munch concept (food) by error because our sys-
tem accumulated all features of the word Munch 
in text regardless of its capitalization. 
6 Discussion 
One question that remains unanswered is how 
clean an ontology must be in order for our meth-
odology to work. Since the structure of the ontol-
ogy guides the propagation of features, a very 
noisy ontology will result in noisy feature vec-
tors. However, the framework is tolerant to some 
amount of noise and can in fact be used to correct 
some errors (as shown in Section 5.3). 
We showed in Section 1 how our framework 
can be used to disambiguate lexical-semantic re-
sources like hyponym lists, verb relations, and 
unknown words or terms. Other avenues of future 
work include: 
Adapting/extending existing ontologies 
It takes a large amount of time to build resources 
like WordNet. However, adapting existing re-
sources to a new corpus might be possible using 
our framework. Once we have enriched the on-
tology with features from a corpus, we can rear-
range the ontological structure according to the 
inter-conceptual similarity of nodes. For example, 
consider the word computer in WordNet, which 
has two senses: a) a machine; and b) a person 
who calculates. In a computer science corpus, 
sense b) occurs very infrequently and possibly a 
new sense of computer (e.g. a processing chip) 
occurs. A system could potentially remove sense 
b) since the similarity of the other children of b) 
and computer is very low. It could also uncover 
the new processing chip sense by finding a high 
similarity between computer and the processing 
chip concept. 
Validating ontologies 
This is a holy grail problem in the knowledge 
representation community. As a small step, our 
framework can be used to flag potential anoma-
lies to the knowledge engineer. 
What makes a chair different from a recliner? 
Given an enriched ontology, we can remove from 
the feature vectors of chair and recliner those 
features that occur in their parent furniture con-
cept. The features that remain describe their dif-
ferent syntactic behaviors in text. 
Figure 7. Attachment precision and recall on the 
Full set vs. the polysemous nodes in the Full set 
when the system returns at most K attachments. 
Precision and Recall
(All vs. Polysemous Nodes)
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
K
Precision (All) Recall (All)
Precision (Polysemous) Recall (Polysemous)
131
7 Conclusions 
We presented a framework for inducing ontologi-
cal feature vectors from lexical co-occurrence 
vectors. Our method does not require the disam-
biguation of text. Instead, it relies on the principle 
of distributional similarity and the fact that 
polysemous words that are similar in one sense 
tend to be dissimilar in their other senses. On the 
task of attaching new words to WordNet using 
our framework, our experiments showed that the 
first attachment has 73.9% accuracy and that a 
correct attachment is in the top-5 attachments 
with 81.3% accuracy. 
We believe this work to be useful for a variety 
of applications. Not only can sense selection tasks 
such as word sense disambiguation, parsing, and 
semantic analysis benefit from our framework, 
but more inference-oriented tasks such as ques-
tion answering and text summarization as well.  
We hope that this work will assist with the devel-
opment of other large-scale and internally consis-
tent collections of semantic information. 
References 
Agirre, E.; Ansa, O.; Martinez, D.; and Hovy, E. 2001. Enriching 
WordNet concepts with topic signatures. In Proceedings of 
the NAACL workshop on WordNet and Other Lexical Re-
sources: Applications, Extensions and Customizations. Pitts-
burgh, PA. 
Baker, C.; Fillmore, C.; and Lowe, J. 1998. The Berkeley Fra-
meNet project. In Proceedings of COLING-ACL. Montreal, 
Canada. 
Chklovski, T., and Pantel, P. VERBOCEAN: Mining the Web for 
Fine-Grained Semantic Verb Relations. In Proceedings of 
EMNLP-2004. pp. 33-40. Barcelona, Spain. 
Gale, W.; Church, K.; and Yarowsky, D. 1992. A method for 
disambiguating word senses in a large corpus. Computers and 
Humanities, 26:415-439. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning se-
mantic constraints for the automatic discovery of part-whole 
relations. In Proceedings of HLT/NAACL-03. pp. 80-87. Ed-
monton, Canada. 
Harabagiu, S.; Miller, G.; and Moldovan, D. 1999. WordNet 2 - 
A Morphologically and Semantically Enhanced Resource. In 
Proceedings of SIGLEX-99. pp.1-8. University of Maryland. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University 
Press. pp. 26-47. 
Hovy, E. 1998. Combining and standardizing large-scale, practi-
cal ontologies for machine translation and other uses. In Pro-
ceedings LREC-98. pp. 535-542.  Granada, Spain. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268-275. Pitts-
burgh, PA. 
Kingsbury, P; Palmer, M.; and Marcus, M. 2002. Adding seman-
tic annotation to the Penn TreeBank. In Proceedings of HLT-
2002. San Diego, California. 
Knight, K. and Luk, S. K. 1994. Building a large-scale knowl-
edge base for machine translation.  In Proceedings of AAAI-
1994.  Seattle, WA. 
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33-38. 
Lin, D. 1998. Automatic retrieval and clustering of similar 
words. In Proceedings of COLING/ACL-98. pp. 768-774. 
Montreal, Canada. 
Lin, D. 1994. Principar - an efficient, broad-coverage, principle-
based parser. Proceedings of COLING-94. pp. 42-48. Kyoto, 
Japan. 
Lund, K. and Burgess, C. 1996. Producing high-dimensional 
semantic spaces from lexical co-occurrence. Behavior Re-
search Methods, Instruments, and Computers, 28:203-208. 
Meyers, A.; Reeves, R.; Macleod, C.; Szekely, R.; Zielinska, V.; 
Young, B.; and Grishman, R. Annotating noun argument 
structure for NomBank. In Proceedings of LREC-2004. Lis-
bon, Portugal. 
Miller, G. 1990. WordNet: An online lexical database. Interna-
tional Journal of Lexicography, 3(4). 
Noy, N. F. and Musen, M. A. 1999. An algorithm for merging 
and aligning ontologies: Automation and tool support.  In 
Proceedings of the Workshop on Ontology Management 
(AAAI-99). Orlando, FL. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. 
In Proceedings of SIGKDD-02. pp. 613-619. Edmonton, Can-
ada. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-1997. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern In-
formation Retrieval. McGraw Hill. 
Shinzato, K. and Torisawa, K. 2004. Acquiring hyponymy rela-
tions from web documents. In Proceedings of HLT-NAACL-
2004. pp. 73-80. Boston, MA. 
 
Table 2. Example attachment errors by our system. 
Node System Attachment 
Correct  
Attachment 
arsenic* trioxide arsenic OR element 
law regulation law OR police OR ? 
Munch? munch Munch 
slob fool slob 
vomiting fever emesis 
* the system?s attachment was a parent of the correct attachment. 
? error due to case mix-up (our algorithm does not differentiate 
between case). 
132
Proceedings of the 43rd Annual Meeting of the ACL, pages 622?629,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Randomized Algorithms and NLP: Using Locality Sensitive Hash Function
for High Speed Noun Clustering
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292.
{ravichan, pantel, hovy}@ISI.EDU
Abstract
In this paper, we explore the power of
randomized algorithm to address the chal-
lenge of working with very large amounts
of data. We apply these algorithms to gen-
erate noun similarity lists from 70 million
pages. We reduce the running time from
quadratic to practically linear in the num-
ber of elements to be computed.
1 Introduction
In the last decade, the field of Natural Language Pro-
cessing (NLP), has seen a surge in the use of cor-
pus motivated techniques. Several NLP systems are
modeled based on empirical data and have had vary-
ing degrees of success. Of late, however, corpus-
based techniques seem to have reached a plateau
in performance. Three possible areas for future re-
search investigation to overcoming this plateau in-
clude:
1. Working with large amounts of data (Banko and
Brill, 2001)
2. Improving semi-supervised and unsupervised al-
gorithms.
3. Using more sophisticated feature functions.
The above listing may not be exhaustive, but it is
probably not a bad bet to work in one of the above
directions. In this paper, we investigate the first two
avenues. Handling terabytes of data requires more
efficient algorithms than are currently used in NLP.
We propose a web scalable solution to clustering
nouns, which employs randomized algorithms. In
doing so, we are going to explore the literature and
techniques of randomized algorithms. All cluster-
ing algorithms make use of some distance similar-
ity (e.g., cosine similarity) to measure pair wise dis-
tance between sets of vectors. Assume that we are
given n points to cluster with a maximum of k fea-
tures. Calculating the full similarity matrix would
take time complexity n2k. With large amounts of
data, say n in the order of millions or even billions,
having an n2k algorithm would be very infeasible.
To be scalable, we ideally want our algorithm to be
proportional to nk.
Fortunately, we can borrow some ideas from the
Math and Theoretical Computer Science community
to tackle this problem. The crux of our solution lies
in defining Locality Sensitive Hash (LSH) functions.
LSH functions involve the creation of short signa-
tures (fingerprints) for each vector in space such that
those vectors that are closer to each other are more
likely to have similar fingerprints. LSH functions
are generally based on randomized algorithms and
are probabilistic. We present LSH algorithms that
can help reduce the time complexity of calculating
our distance similarity atrix to nk.
Rabin (1981) proposed the use of hash func-
tions from random irreducible polynomials to cre-
ate short fingerprint representations for very large
strings. These hash function had the nice property
that the fingerprint of two identical strings had the
same fingerprints, while dissimilar strings had dif-
ferent fingerprints with a very small probability of
collision. Broder (1997) first introduced LSH. He
proposed the use of Min-wise independent functions
to create fingerprints that preserved the Jaccard sim-
622
ilarity between every pair of vectors. These tech-
niques are used today, for example, to eliminate du-
plicate web pages. Charikar (2002) proposed the
use of random hyperplanes to generate an LSH func-
tion that preserves the cosine similarity between ev-
ery pair of vectors. Interestingly, cosine similarity is
widely used in NLP for various applications such as
clustering.
In this paper, we perform high speed similarity
list creation for nouns collected from a huge web
corpus. We linearize this step by using the LSH
proposed by Charikar (2002). This reduction in
complexity of similarity computation makes it pos-
sible to address vastly larger datasets, at the cost,
as shown in Section 5, of only little reduction in
accuracy. In our experiments, we generate a simi-
larity list for each noun extracted from 70 million
page web corpus. Although the NLP community
has begun experimenting with the web, we know
of no work in published literature that has applied
complex language analysis beyond IR and simple
surface-level pattern matching.
2 Theory
The core theory behind the implementation of fast
cosine similarity calculation can be divided into two
parts: 1. Developing LSH functions to create sig-
natures; 2. Using fast search algorithm to find near-
est neighbors. We describe these two components in
greater detail in the next subsections.
2.1 LSH Function Preserving Cosine Similarity
We first begin with the formal definition of cosine
similarity.
Definition: Let u and v be two vectors in a k
dimensional hyperplane. Cosine similarity is de-
fined as the cosine of the angle between them:
cos(?(u, v)). We can calculate cos(?(u, v)) by the
following formula:
cos(?(u, v)) =
|u.v|
|u||v|
(1)
Here ?(u, v) is the angle between the vectors u
and v measured in radians. |u.v| is the scalar (dot)
product of u and v, and |u| and |v| represent the
length of vectors u and v respectively.
The LSH function for cosine similarity as pro-
posed by Charikar (2002) is given by the following
theorem:
Theorem: Suppose we are given a collection of
vectors in a k dimensional vector space (as written as
Rk). Choose a family of hash functions as follows:
Generate a spherically symmetric random vector r
of unit length from this k dimensional space. We
define a hash function, hr, as:
hr(u) =
{
1 : r.u ? 0
0 : r.u < 0
(2)
Then for vectors u and v,
Pr[hr(u) = hr(v)] = 1?
?(u, v)
pi
(3)
Proof of the above theorem is given by Goemans
and Williamson (1995). We rewrite the proof here
for clarity. The above theorem states that the prob-
ability that a random hyperplane separates two vec-
tors is directly proportional to the angle between the
two vectors (i,e., ?(u, v)). By symmetry, we have
Pr[hr(u) 6= hr(v)] = 2Pr[u.r ? 0, v.r < 0]. This
corresponds to the intersection of two half spaces,
the dihedral angle between which is ?. Thus, we
have Pr[u.r ? 0, v.r < 0] = ?(u, v)/2pi. Proceed-
ing we have Pr[hr(u) 6= hr(v)] = ?(u, v)/pi and
Pr[hr(u) = hr(v)] = 1 ? ?(u, v)/pi. This com-
pletes the proof.
Hence from equation 3 we have,
cos(?(u, v)) = cos((1? Pr[hr(u) = hr(v)])pi)
(4)
This equation gives us an alternate method for
finding cosine similarity. Note that the above equa-
tion is probabilistic in nature. Hence, we generate a
large (d) number of random vectors to achieve the
process. Having calculated hr(u) with d random
vectors for each of the vectors u, we apply equation
4 to find the cosine distance between two vectors.
As we generate more number of random vectors, we
can estimate the cosine similarity between two vec-
tors more accurately. However, in practice, the num-
ber (d) of random vectors required is highly domain
dependent, i.e., it depends on the value of the total
number of vectors (n), features (k) and the way the
vectors are distributed. Using d random vectors, we
623
can represent each vector by a bit stream of length
d.
Carefully looking at equation 4, we can ob-
serve that Pr[hr(u) = hr(v)] = 1 ?
(hamming distance)/d1 . Thus, the above theo-
rem, converts the problem of finding cosine distance
between two vectors to the problem of finding ham-
ming distance between their bit streams (as given by
equation 4). Finding hamming distance between two
bit streams is faster and highly memory efficient.
Also worth noting is that this step could be consid-
ered as dimensionality reduction wherein we reduce
a vector in k dimensions to that of d bits while still
preserving the cosine distance between them.
2.2 Fast Search Algorithm
To calculate the fast hamming distance, we use the
search algorithm PLEB (Point Location in Equal
Balls) first proposed by Indyk and Motwani (1998).
This algorithm was further improved by Charikar
(2002). This algorithm involves random permuta-
tions of the bit streams and their sorting to find the
vector with the closest hamming distance. The algo-
rithm given in Charikar (2002) is described to find
the nearest neighbor for a given vector. We mod-
ify it so that we are able to find the top B closest
neighbor for each vector. We omit the math of this
algorithm but we sketch its procedural details in the
next section. Interested readers are further encour-
aged to read Theorem 2 from Charikar (2002) and
Section 3 from Indyk and Motwani (1998).
3 Algorithmic Implementation
In the previous section, we introduced the theory for
calculation of fast cosine similarity. We implement
it as follows:
1. Initially we are given n vectors in a huge k di-
mensional space. Our goal is to find all pairs of
vectors whose cosine similarity is greater than
a particular threshold.
2. Choose d number of (d << k) unit random
vectors {r0, r1, ......, rd} each of k dimensions.
A k dimensional unit random vector, in gen-
eral, is generated by independently sampling a
1Hamming distance is the number of bits which differ be-
tween two binary strings.
Gaussian function with mean 0 and variance 1,
k number of times. Each of the k samples is
used to assign one dimension to the random
vector. We generate a random number from
a Gaussian distribution by using Box-Muller
transformation (Box and Muller, 1958).
3. For every vector u, we determine its signature
by using the function hr(u) (as given by equa-
tion 4). We can represent the signature of vec-
tor u as: u? = {hr1(u), hr2(u), ......., hrd(u)}.
Each vector is thus represented by a set of a bit
streams of length d. Steps 2 and 3 takes O(nk)
time (We can assume d to be a constant since
d << k).
4. The previous step gives n vectors, each of them
represented by d bits. For calculation of fast
hamming distance, we take the original bit in-
dex of all vectors and randomly permute them
(see Appendix A for more details on random
permutation functions). A random permutation
can be considered as random jumbling of the
bits of each vector2. A random permutation
function can be approximated by the following
function:
pi(x) = (ax + b)mod p (5)
where, p is prime and 0 < a < p , 0 ? b < p,
and a and b are chosen at random.
We apply q different random permutation for
every vector (by choosing random values for a
and b, q number of times). Thus for every vec-
tor we have q different bit permutations for the
original bit stream.
5. For each permutation function pi, we lexico-
graphically sort the list of n vectors (whose bit
streams are permuted by the function pi) to ob-
tain a sorted list. This step takes O(nlogn)
time. (We can assume q to be a constant).
6. For each sorted list (performed after applying
the random permutation function pi), we calcu-
late the hamming distance of every vector with
2The jumbling is performed by a mapping of the bit index
as directed by the random permutation function. For a given
permutation, we reorder the bit indexes of all vectors in similar
fashion. This process could be considered as column reording
of bit vectors.
624
B of its closest neighbors in the sorted list. If
the hamming distance is below a certain prede-
termined threshold, we output the pair of vec-
tors with their cosine similarity (as calculated
by equation 4). Thus, B is the beam parameter
of the search. This step takes O(n), since we
can assume B, q, d to be a constant.
Why does the fast hamming distance algorithm
work? The intuition is that the number of bit
streams, d, for each vector is generally smaller than
the number of vectors n (ie. d << n). Thus, sort-
ing the vectors lexicographically after jumbling the
bits will likely bring vectors with lower hamming
distance closer to each other in the sorted lists.
Overall, the algorithm takes O(nk+nlogn) time.
However, for noun clustering, we generally have the
number of nouns, n, smaller than the number of fea-
tures, k. (i.e., n < k). This implies logn << k and
nlogn << nk. Hence the time complexity of our
algorithm is O(nk + nlogn) ? O(nk). This is a
huge saving from the original O(n2k) algorithm. In
the next section, we proceed to apply this technique
for generating noun similarity lists.
4 Building Noun Similarity Lists
A lot of work has been done in the NLP community
on clustering words according to their meaning in
text (Hindle, 1990; Lin, 1998). The basic intuition
is that words that are similar to each other tend to
occur in similar contexts, thus linking the semantics
of words with their lexical usage in text. One may
ask why is clustering of words necessary in the first
place? There may be several reasons for clustering,
but generally it boils down to one basic reason: if the
words that occur rarely in a corpus are found to be
distributionally similar to more frequently occurring
words, then one may be able to make better infer-
ences on rare words.
However, to unleash the real power of clustering
one has to work with large amounts of text. The
NLP community has started working on noun clus-
tering on a few gigabytes of newspaper text. But
with the rapidly growing amount of raw text avail-
able on the web, one could improve clustering per-
formance by carefully harnessing its power. A core
component of most clustering algorithms used in the
NLP community is the creation of a similarity ma-
trix. These algorithms are of complexity O(n2k),
where n is the number of unique nouns and k is the
feature set length. These algorithms are thus not
readily scalable, and limit the size of corpus man-
ageable in practice to a few gigabytes. Clustering al-
gorithms for words generally use the cosine distance
for their similarity calculation (Salton and McGill,
1983). Hence instead of using the usual naive cosine
distance calculation between every pair of words we
can use the algorithm described in Section 3 to make
noun clustering web scalable.
To test our algorithm we conduct similarity based
experiments on 2 different types of corpus: 1. Web
Corpus (70 million web pages, 138GB), 2. Newspa-
per Corpus (6 GB newspaper corpus)
4.1 Web Corpus
We set up a spider to download roughly 70 million
web pages from the Internet. Initially, we use the
links from Open Directory project3 as seed links for
our spider. Each webpage is stripped of HTML tags,
tokenized, and sentence segmented. Each docu-
ment is language identified by the software TextCat4
which implements the paper by Cavnar and Trenkle
(1994). We retain only English documents. The web
contains a lot of duplicate or near-duplicate docu-
ments. Eliminating them is critical for obtaining bet-
ter representation statistics from our collection. The
problem of identifying near duplicate documents in
linear time is not trivial. We eliminate duplicate and
near duplicate documents by using the algorithm de-
scribed by Kolcz et al (2004). This process of dupli-
cate elimination is carried out in linear time and in-
volves the creation of signatures for each document.
Signatures are designed so that duplicate and near
duplicate documents have the same signature. This
algorithm is remarkably fast and has high accuracy.
This entire process of removing non English docu-
ments and duplicate (and near-duplicate) documents
reduces our document set from 70 million web pages
to roughly 31 million web pages. This represents
roughly 138GB of uncompressed text.
We identify all the nouns in the corpus by us-
ing a noun phrase identifier. For each noun phrase,
we identify the context words surrounding it. Our
context window length is restricted to two words to
3http://www.dmoz.org/
4http://odur.let.rug.nl/?vannoord/TextCat/
625
Table 1: Corpus description
Corpus Newspaper Web
Corpus Size 6GB 138GB
Unique Nouns 65,547 655,495
Feature size 940,154 1,306,482
the left and right of each noun. We use the context
words as features of the noun vector.
4.2 Newspaper Corpus
We parse a 6 GB newspaper (TREC9 and
TREC2002 collection) corpus using the dependency
parser Minipar (Lin, 1994). We identify all nouns.
For each noun we take the grammatical context of
the noun as identified by Minipar5. We do not use
grammatical features in the web corpus since pars-
ing is generally not easily web scalable. This kind of
feature set does not seem to affect our results. Cur-
ran and Moens (2002) also report comparable results
for Minipar features and simple word based proxim-
ity features. Table 1 gives the characteristics of both
corpora. Since we use grammatical context, the fea-
ture set is considerably larger than the simple word
based proximity feature set for the newspaper cor-
pus.
4.3 Calculating Feature Vectors
Having collected all nouns and their features, we
now proceed to construct feature vectors (and
values) for nouns from both corpora using mu-
tual information (Church and Hanks, 1989). We
first construct a frequency count vector C(e) =
(ce1, ce2, ..., cek), where k is the total number of
features and cef is the frequency count of feature
f occurring in word e. Here, cef is the number
of times word e occurred in context f . We then
construct a mutual information vector MI(e) =
(mie1,mie2, ...,miek) for each word e, where mief
is the pointwise mutual information between word e
and feature f , which is defined as:
mief = log
cef
N
?n
i=1
cif
N ?
?k
j=1
cej
N
(6)
where n is the number of words and N =
5We perform this operation so that we can compare the per-
formance of our system to that of Pantel and Lin (2002).
?n
i=1
?m
j=1 cij is the total frequency count of all
features of all words.
Having thus obtained the feature representation of
each noun we can apply the algorithm described in
Section 3 to discover similarity lists. We report re-
sults in the next section for both corpora.
5 Evaluation
Evaluating clustering systems is generally consid-
ered to be quite difficult. However, we are mainly
concerned with evaluating the quality and speed of
our high speed randomized algorithm. The web cor-
pus is used to show that our framework is web-
scalable, while the newspaper corpus is used to com-
pare the output of our system with the similarity lists
output by an existing system, which are calculated
using the traditional formula as given in equation
1. For this base comparison system we use the one
built by Pantel and Lin (2002). We perform 3 kinds
of evaluation: 1. Performance of Locality Sensitive
Hash Function; 2. Performance of fast Hamming
distance search algorithm; 3. Quality of final simi-
larity lists.
5.1 Evaluation of Locality sensitive Hash
function
To perform this evaluation, we randomly choose 100
nouns (vectors) from the web collection. For each
noun, we calculate the cosine distance using the
traditional slow method (as given by equation 1),
with all other nouns in the collection. This process
creates similarity lists for each of the 100 vectors.
These similarity lists are cut off at a threshold of
0.15. These lists are considered to be the gold stan-
dard test set for our evaluation.
For the above 100 chosen vectors, we also calcu-
late the cosine similarity using the randomized ap-
proach as given by equation 4 and calculate the mean
squared error with the gold standard test set using
the following formula:
errorav =
?
?
i
(CSreal,i ? CScalc,i)
2/total
(7)
where CSreal,i and CScalc,i are the cosine simi-
larity scores calculated using the traditional (equa-
tion 1) and randomized (equation 4) technique re-
626
Table 2: Error in cosine similarity
Number of ran-
dom vectors d
Average error in
cosine similarity
Time (in hours)
1 1.0000 0.4
10 0.4432 0.5
100 0.1516 3
1000 0.0493 24
3000 0.0273 72
10000 0.0156 241
spectively. i is the index over all pairs of elements
that have CSreal,i >= 0.15
We calculate the error (errorav) for various val-
ues of d, the total number of unit random vectors r
used in the process. The results are reported in Table
26. As we generate more random vectors, the error
rate decreases. For example, generating 10 random
vectors gives us a cosine error of 0.4432 (which is a
large number since cosine similarity ranges from 0
to 1.) However, generation of more random vectors
leads to reduction in error rate as seen by the val-
ues for 1000 (0.0493) and 10000 (0.0156). But as
we generate more random vectors the time taken by
the algorithm also increases. We choose d = 3000
random vectors as our optimal (time-accuracy) cut
off. It is also very interesting to note that by using
only 3000 bits for each of the 655,495 nouns, we
are able to measure cosine similarity between every
pair of them to within an average error margin of
0.027. This algorithm is also highly memory effi-
cient since we can represent every vector by only a
few thousand bits. Also the randomization process
makes the the algorithm easily parallelizable since
each processor can independently contribute a few
bits for every vector.
5.2 Evaluation of Fast Hamming Distance
Search Algorithm
We initially obtain a list of bit streams for all the
vectors (nouns) from our web corpus using the ran-
domized algorithm described in Section 3 (Steps 1
to 3). The next step involves the calculation of ham-
ming distance. To evaluate the quality of this search
algorithm we again randomly choose 100 vectors
(nouns) from our collection. For each of these 100
vectors we manually calculate the hamming distance
6The time is calculated for running the algorithm on a single
Pentium IV processor with 4GB of memory
with all other vectors in the collection. We only re-
tain those pairs of vectors whose cosine distance (as
manually calculated) is above 0.15. This similarity
list is used as the gold standard test set for evaluating
our fast hamming search.
We then apply the fast hamming distance search
algorithm as described in Section 3. In particular, it
involves steps 3 to 6 of the algorithm. We evaluate
the hamming distance with respect to two criteria: 1.
Number of bit index random permutations functions
q; 2. Beam search parameter B.
For each vector in the test collection, we take the
top N elements from the gold standard similarity list
and calculate how many of these elements are actu-
ally discovered by the fast hamming distance algo-
rithm. We report the results in Table 3 and Table 4
with beam parameters of (B = 25) and (B = 100)
respectively. For each beam, we experiment with
various values for q, the number of random permu-
tation function used. In general, by increasing the
value for beam B and number of random permu-
tation q , the accuracy of the search algorithm in-
creases. For example in Table 4 by using a beam
B = 100 and using 1000 random bit permutations,
we are able to discover 72.8% of the elements of the
Top 100 list. However, increasing the values of q and
B also increases search time. With a beam (B) of
100 and the number of random permutations equal
to 100 (i.e., q = 1000) it takes 570 hours of process-
ing time on a single Pentium IV machine, whereas
with B = 25 and q = 1000, reduces processing time
by more than 50% to 240 hours.
We could not calculate the total time taken to
build noun similarity list using the traditional tech-
nique on the entire corpus. However, we estimate
that its time taken would be at least 50,000 hours
(and perhaps even more) with a few of Terabytes of
disk space needed. This is a very rough estimate.
The experiment was infeasible. This estimate as-
sumes the widely used reverse indexing technique,
where in one compares only those vector pairs that
have at least one feature in common.
5.3 Quality of Final Similarity Lists
For evaluating the quality of our final similarity lists,
we use the system developed by Pantel and Lin
(2002) as gold standard on a much smaller data set.
We use the same 6GB corpus that was used for train-
627
Table 3: Hamming search accuracy (Beam B = 25)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 6.1% 4.9% 4.2% 3.1% 2.4% 1.9%
50 6.1% 5.1% 4.3% 3.2% 2.5% 1.9%
100 11.3% 9.7% 8.2% 6.2% 5.7% 5.1%
500 44.3% 33.5% 30.4% 25.8% 23.0% 20.4%
1000 58.7% 50.6% 48.8% 45.0% 41.0% 37.2%
Table 4: Hamming search accuracy (Beam B = 100)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 9.2% 9.5% 7.9% 6.4% 5.8% 4.7%
50 15.4% 17.7% 14.6% 12.0% 10.9% 9.0%
100 27.8% 27.2% 23.5% 19.4% 17.9% 16.3%
500 73.1% 67.0% 60.7% 55.2% 53.0% 50.5%
1000 87.6% 84.4% 82.1% 78.9% 75.8% 72.8%
ing by Pantel and Lin (2002) so that the results are
comparable. We randomly choose 100 nouns and
calculate the top N elements closest to each noun in
the similarity lists using the randomized algorithm
described in Section 3. We then compare this output
to the one provided by the system of Pantel and Lin
(2002). For every noun in the top N list generated
by our system we calculate the percentage overlap
with the gold standard list. Results are reported in
Table 5. The results shows that we are able to re-
trieve roughly 70% of the gold standard similarity
list. In Table 6, we list the top 10 most similar words
for some nouns, as examples, from the web corpus.
6 Conclusion
NLP researchers have just begun leveraging the vast
amount of knowledge available on the web. By
searching IR engines for simple surface patterns,
many applications ranging from word sense disam-
biguation, question answering, and mining seman-
tic resources have already benefited. However, most
language analysis tools are too infeasible to run on
the scale of the web. A case in point is generat-
ing noun similarity lists using co-occurrence statis-
tics, which has quadratic running time on the input
size. In this paper, we solve this problem by pre-
senting a randomized algorithm that linearizes this
task and limits memory requirements. Experiments
show that our method generates cosine similarities
between pairs of nouns within a score of 0.03.
In many applications, researchers have shown that
more data equals better performance (Banko and
Brill, 2001; Curran and Moens, 2002). Moreover,
at the web-scale, we are no longer limited to a snap-
shot in time, which allows broader knowledge to be
learned and processed. Randomized algorithms pro-
vide the necessary speed and memory requirements
to tap into terascale text sources. We hope that ran-
domized algorithms will make other NLP tools fea-
sible at the terascale and we believe that many al-
gorithms will benefit from the vast coverage of our
newly created noun similarity list.
Acknowledgement
We wish to thank USC Center for High Performance
Computing and Communications (HPCC) for help-
ing us use their cluster computers.
References
Banko, M. and Brill, E. 2001. Mitigating the paucity of dat-
aproblem. In Proceedings of HLT. 2001. San Diego, CA.
Box, G. E. P. and M. E. Muller 1958. Ann. Math. Stat. 29,
610?611.
Broder, Andrei 1997. On the Resemblance and Containment of
Documents. Proceedings of the Compression and Complex-
ity of Sequences.
Cavnar, W. B. and J. M. Trenkle 1994. N-Gram-Based Text
Categorization. In Proceedings of Third Annual Symposium
on Document Analysis and Information Retrieval, Las Ve-
gas, NV, UNLV Publications/Reprographics, 161?175.
628
Table 5: Final Quality of Similarity Lists
Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
Accuracy 70.7% 71.9% 72.2% 71.7% 71.2% 71.1%
Table 6: Sample Top 10 Similarity Lists
JUST DO IT computer science TSUNAMI Louis Vuitton PILATES
HAVE A NICE DAY mechanical engineering tidal wave PRADA Tai Chi
FAIR AND BALANCED electrical engineering LANDSLIDE Fendi Cardio
POWER TO THE PEOPLE chemical engineering EARTHQUAKE Kate Spade SHIATSU
NEVER AGAIN Civil Engineering volcanic eruption VUITTON Calisthenics
NO BLOOD FOR OIL ECONOMICS HAILSTORM BURBERRY Ayurveda
KINGDOM OF HEAVEN ENGINEERING Typhoon GUCCI Acupressure
If Texas Wasn?t Biology Mudslide Chanel Qigong
BODY OF CHRIST environmental science windstorm Dior FELDENKRAIS
WE CAN PHYSICS HURRICANE Ferragamo THERAPEUTIC TOUCH
Weld with your mouse information science DISASTER Ralph Lauren Reflexology
Charikar, Moses 2002. Similarity Estimation Techniques from
Rounding Algorithms In Proceedings of the 34th Annual
ACM Symposium on Theory of Computing.
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 76?83. Vancouver, Canada.
Curran, J. and Moens, M. 2002. Scaling context space. In
Proceedings of ACL-02 pp 231?238, Philadelphia, PA.
Goemans, M. X. and D. P. Williamson 1995. Improved Ap-
proximation Algorithms for Maximum Cut and Satisfiability
Problems Using Semidefinite Programming. JACM 42(6):
1115?1145.
Hindle, D. 1990. Noun classification from predicate-argument
structures. In Proceedings of ACL-90. pp. 268?275. Pitts-
burgh, PA.
Lin, D. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of COLING/ACL-98. pp. 768?774.
Montreal, Canada.
Indyk, P., Motwani, R. 1998. Approximate nearest neighbors:
towards removing the curse of dimensionality Proceedings
of 30th STOC, 604?613.
A. Kolcz, A. Chowdhury, J. Alspector 2004. Improved ro-
bustness of signature-based near-replica detection via lexi-
con randomization. Proceedings of ACM-SIGKDD (2004).
Lin, D. 1994 Principar - an efficient, broad-coverage,
principle-based parser. Proceedings of COLING-94, pp. 42?
48. Kyoto, Japan.
Pantel, Patrick and Dekang Lin 2002. Discovering Word
Senses from Text. In Proceedings of SIGKDD-02, pp. 613?
619. Edmonton, Canada
Rabin, M. O. 1981. Fingerprinting by random polynomials.
Center for research in Computing technology , Harvard Uni-
versity, Report TR-15-81.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
Appendix A. Random Permutation
Functions
We define [n] = {0, 1, 2, ..., n? 1}.
[n] can thus be considered as a set of integers from
0 to n? 1.
Let pi : [n] ? [n] be a permutation function chosen
at random from the set of all such permutation func-
tions.
Consider pi : [4] ? [4].
A permutation function pi is a one to one mapping
from the set of [4] to the set of [4].
Thus, one possible mapping is:
pi : {0, 1, 2, 3} ? {3, 2, 1, 0}
Here it means: pi(0) = 3, pi(1) = 2, pi(2) = 1,
pi(3) = 0
Another possible mapping would be:
pi : {0, 1, 2, 3} ? {3, 0, 1, 2}
Here it means: pi(0) = 3, pi(1) = 0, pi(2) = 1,
pi(3) = 2
Thus for the set [4] there would be 4! = 4?3?2 =
24 possibilities. In general, for a set [n] there would
be n! unique permutation functions. Choosing a ran-
dom permutation function amounts to choosing one
of n! such functions at random.
629
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 113?120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Espresso: Leveraging Generic Patterns for  
Automatically Harvesting Semantic Relations 
 
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
pantel@isi.edu 
Marco Pennacchiotti 
ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
  
Abstract 
In this paper, we present Espresso, a 
weakly-supervised, general-purpose, 
and accurate algorithm for harvesting 
semantic relations. The main contribu-
tions are: i) a method for exploiting ge-
neric patterns by filtering incorrect 
instances using the Web; and ii) a prin-
cipled measure of pattern and instance 
reliability enabling the filtering algo-
rithm. We present an empirical com-
parison of Espresso with various state of 
the art systems, on different size and 
genre corpora, on extracting various 
general and specific relations. Experi-
mental results show that our exploita-
tion of generic patterns substantially 
increases system recall with small effect 
on overall precision. 
1 Introduction 
Recent attention to knowledge-rich problems 
such as question answering (Pasca and Harabagiu 
2001) and textual entailment (Geffet and Dagan 
2005) has encouraged natural language process-
ing researchers to develop algorithms for auto-
matically harvesting shallow semantic resources. 
With seemingly endless amounts of textual data 
at our disposal, we have a tremendous opportu-
nity to automatically grow semantic term banks 
and ontological resources. 
To date, researchers have harvested, with 
varying success, several resources, including 
concept lists (Lin and Pantel 2002), topic signa-
tures (Lin and Hovy 2000), facts (Etzioni et al 
2005), and word similarity lists (Hindle 1990). 
Many recent efforts have also focused on extract-
ing semantic relations between entities, such as 
entailments (Szpektor et al 2004), is-a (Ravi-
chandran and Hovy 2002), part-of (Girju et al 
2006), and other relations. 
The following desiderata outline the properties 
of an ideal relation harvesting algorithm: 
? Performance: it must generate both high preci-
sion and high recall relation instances; 
? Minimal supervision: it must require little or no 
human annotation; 
? Breadth: it must be applicable to varying cor-
pus sizes and domains; and 
? Generality: it must be applicable to a wide va-
riety of relations (i.e., not just is-a or part-of). 
To our knowledge, no previous harvesting algo-
rithm addresses all these properties concurrently. 
In this paper, we present Espresso, a general-
purpose, broad, and accurate corpus harvesting 
algorithm requiring minimal supervision. The 
main algorithmic contribution is a novel method 
for exploiting generic patterns, which are broad 
coverage noisy patterns ? i.e., patterns with high 
recall and low precision. Insofar, difficulties in 
using these patterns have been a major impedi-
ment for minimally supervised algorithms result-
ing in either very low precision or recall. We 
propose a method to automatically detect generic 
patterns and to separate their correct and incor-
rect instances. The key intuition behind the algo-
rithm is that given a set of reliable (high 
precision) patterns on a corpus, correct instances 
of a generic pattern will fire more with reliable 
patterns on a very large corpus, like the Web, 
than incorrect ones. Below is a summary of the 
main contributions of this paper: 
? Algorithm for exploiting generic patterns: 
Unlike previous algorithms that require signifi-
cant manual work to make use of generic pat-
terns, we propose an unsupervised Web-
filtering method for using generic patterns; and 
? Principled reliability measure: We propose a 
new measure of pattern and instance reliability 
which enables the use of generic patterns. 
113
Espresso addresses the desiderata as follows: 
? Performance: Espresso generates balanced 
precision and recall relation instances by ex-
ploiting generic patterns; 
? Minimal supervision: Espresso requires as in-
put only a small number of seed instances; 
? Breadth: Espresso works on both small and 
large corpora ? it uses Web and syntactic ex-
pansions to compensate for lacks of redun-
dancy in small corpora; 
? Generality: Espresso is amenable to a wide 
variety of binary relations, from classical is-a 
and part-of to specific ones such as reaction 
and succession. 
Previous work like (Girju et al 2006) that has 
made use of generic patterns through filtering has 
shown both high precision and high recall, at the 
expensive cost of much manual semantic annota-
tion. Minimally supervised algorithms, like 
(Hearst 1992; Pantel et al 2004), typically ignore 
generic patterns since system precision dramati-
cally decreases from the introduced noise and 
bootstrapping quickly spins out of control. 
2 Relevant Work 
To date, most research on relation harvesting has 
focused on is-a and part-of. Approaches fall into 
two categories: pattern- and clustering-based. 
Most common are pattern-based approaches. 
Hearst (1992) pioneered using patterns to extract 
hyponym (is-a) relations. Manually building 
three lexico-syntactic patterns, Hearst sketched a 
bootstrapping algorithm to learn more patterns 
from instances, which has served as the model 
for most subsequent pattern-based algorithms. 
Berland and Charniak (1999) proposed a sys-
tem for part-of relation extraction, based on the 
(Hearst 1992) approach. Seed instances are used 
to infer linguistic patterns that are used to extract 
new instances. While this study introduces statis-
tical measures to evaluate instance quality, it re-
mains vulnerable to data sparseness and has the 
limitation of considering only one-word terms. 
Improving upon (Berland and Charniak 1999), 
Girju et al (2006) employ machine learning al-
gorithms and WordNet (Fellbaum 1998) to dis-
ambiguate part-of generic patterns like ?X?s Y? 
and ?X of Y?. This study is the first extensive at-
tempt to make use of generic patterns. In order to 
discard incorrect instances, they learn WordNet-
based selectional restrictions, like ?X(scene#4)?s 
Y(movie#1)?. While making huge grounds on 
improving precision/recall, heavy supervision is 
required through manual semantic annotations. 
Ravichandran and Hovy (2002) focus on scal-
ing relation extraction to the Web. A simple and 
effective algorithm is proposed to infer surface 
patterns from a small set of instance seeds by 
extracting substrings relating seeds in corpus sen-
tences. The approach gives good results on spe-
cific relations such as birthdates, however it has 
low precision on generic ones like is-a and part-
of. Pantel et al (2004) proposed a similar, highly 
scalable approach, based on an edit-distance 
technique, to learn lexico-POS patterns, showing 
both good performance and efficiency. Espresso 
uses a similar approach to infer patterns, but we 
make use of generic patterns and apply refining 
techniques to deal with wide variety of relations. 
Other pattern-based algorithms include (Riloff 
and Shepherd 1997), who used a semi-automatic 
method for discovering similar words using a 
few seed examples, KnowItAll (Etzioni et al 
2005) that performs large-scale extraction of 
facts from the Web, Mann (2002) who used part 
of speech patterns to extract a subset of is-a rela-
tions involving proper nouns, and (Downey et al 
2005) who formalized the problem of relation 
extraction in a coherent and effective combinato-
rial model that is shown to outperform previous 
probabilistic frameworks. 
Clustering approaches have so far been ap-
plied only to is-a extraction. These methods use 
clustering algorithms to group words according 
to their meanings in text, label the clusters using 
its members? lexical or syntactic dependencies, 
and then extract an is-a relation between each 
cluster member and the cluster label. Caraballo 
(1999) proposed the first attempt, which used 
conjunction and apposition features to build noun 
clusters. Recently, Pantel and Ravichandran 
(2004) extended this approach by making use of 
all syntactic dependency features for each noun. 
The advantage of clustering approaches is that 
they permit algorithms to identify is-a relations 
that do not explicitly appear in text, however 
they generally fail to produce coherent clusters 
from fewer than 100 million words; hence they 
are unreliable for small corpora. 
3 The Espresso Algorithm 
Espresso is based on the framework adopted in 
(Hearst 1992). It is a minimally supervised boot-
strapping algorithm that takes as input a few seed 
instances of a particular relation and iteratively 
learns surface patterns to extract more instances. 
The key to Espresso lies in its use of generic pat-
ters, i.e., those broad coverage noisy patterns that 
114
extract both many correct and incorrect relation 
instances. For example, for part-of relations, the 
pattern ?X of Y? extracts many correct relation 
instances like ?wheel of the car? but also many 
incorrect ones like ?house of representatives?. 
The key assumption behind Espresso is that in 
very large corpora, like the Web, correct in-
stances generated by a generic pattern will be 
instantiated by some reliable patterns, where 
reliable patterns are patterns that have high preci-
sion but often very low recall (e.g., ?X consists of 
Y? for part-of relations). In this section, we de-
scribe the overall architecture of Espresso, pro-
pose a principled measure of reliability, and give 
an algorithm for exploiting generic patterns. 
3.1 System Architecture 
Espresso iterates between the following three 
phases: pattern induction, pattern rank-
ing/selection, and instance extraction. 
The algorithm begins with seed instances of a 
particular binary relation (e.g., is-a) and then it-
erates through the phases until it extracts ?1 pat-
terns or the average pattern score decreases by 
more than ?2 from the previous iteration. In our 
experiments, we set ?1 = 5 and ?2 = 50%. 
For our tokenization, in order to harvest multi-
word terms as relation instances, we adopt a 
slightly modified version of the term definition 
given in (Justeson 1995), as it is one of the most 
commonly used in the NLP literature: 
 ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun 
Pattern Induction 
In the pattern induction phase, Espresso infers a 
set of surface patterns P that connects as many of 
the seed instances as possible in a given corpus. 
Any pattern learning algorithm would do. We 
chose the state of the art algorithm described in 
(Ravichandran and Hovy 2002) with the follow-
ing slight modification. For each input instance 
{x, y}, we first retrieve all sentences containing 
the two terms x and y. The sentences are then 
generalized into a set of new sentences Sx,y by 
replacing all terminological expressions by a 
terminological label, TR. For example: 
 ?Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN 
  and/CC x is/VBZ a/DT y? 
is generalized as: 
 ?Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y? 
Term generalization is useful for small corpora to 
ease data sparseness. Generalized patterns are 
naturally less precise, but this is ameliorated by 
our filtering step described in Section 3.3. 
As in the original algorithm, all substrings 
linking terms x and y are then extracted from Sx,y, 
and overall frequencies are computed to form P. 
Pattern Ranking/Selection 
In (Ravichandran and Hovy 2002), a frequency 
threshold on the patterns in P is set to select the 
final patterns. However, low frequency patterns 
may in fact be very good. In this paper, instead of 
frequency, we propose a novel measure of pat-
tern reliability, r?, which is described in detail in 
Section 3.2. 
Espresso ranks all patterns in P according to 
reliability r? and discards all but the top-k, where 
k is set to the number of patterns from the previ-
ous iteration plus one. In general, we expect that 
the set of patterns is formed by those of the pre-
vious iteration plus a new one. Yet, new statisti-
cal evidence can lead the algorithm to discard a 
pattern that was previously discovered. 
Instance Extraction 
In this phase, Espresso retrieves from the corpus 
the set of instances I that match any of the pat-
terns in P. In Section 3.2, we propose a princi-
pled measure of instance reliability, r?, for 
ranking instances. Next, Espresso filters incor-
rect instances using the algorithm proposed in 
Section 3.3 and then selects the highest scoring m 
instances, according to r?, as input for the subse-
quent iteration. We experimentally set m=200. 
In small corpora, the number of extracted in-
stances can be too low to guarantee sufficient 
statistical evidence for the pattern discovery 
phase of the next iteration. In such cases, the sys-
tem enters an expansion phase, where instances 
are expanded as follows: 
Web expansion: New instances of the patterns 
in P are retrieved from the Web, using the 
Google search engine. Specifically, for each in-
stance {x, y}? I, the system creates a set of que-
ries, using each pattern in P instantiated with y. 
For example, given the instance ?Italy, country? 
and the pattern ?Y such as X?, the resulting 
Google query will be ?country such as *?. New 
instances are then created from the retrieved Web 
results (e.g. ?Canada, country?) and added to I. 
The noise generated from this expansion is at-
tenuated by the filtering algorithm described in 
Section 3.3. 
Syntactic expansion: New instances are cre-
ated from each instance {x, y}? I by extracting 
sub-terminological expressions from x corre-
sponding to the syntactic head of terms. For ex-
115
ample, the relation ?new record of a criminal 
conviction part-of FBI report? expands to: ?new 
record part-of FBI report?, and ?record part-of 
FBI report?. 
3.2 Pattern and Instance Reliability 
Intuitively, a reliable pattern is one that is both 
highly precise and one that extracts many in-
stances. The recall of a pattern p can be approxi-
mated by the fraction of input instances that are 
extracted by p. Since it is non-trivial to estimate 
automatically the precision of a pattern, we are 
wary of keeping patterns that generate many in-
stances (i.e., patterns that generate high recall but 
potentially disastrous precision). Hence, we de-
sire patterns that are highly associated with the 
input instances. Pointwise mutual information 
(Cover and Thomas 1991) is a commonly used 
metric for measuring this strength of association 
between two events x and y: 
 
( ) ( )( ) ( )yPxP
yxP
yxpmi
,
log, =
 
We define the reliability of a pattern p, r?(p), 
as its average strength of association across each 
input instance i in I, weighted by the reliability of 
each instance i: 
 ( )
( )
I
ir
pipmi
pr
Ii pmi
?
? ?
?
?
?
???
? ?
=
?
?
max
),(
 
where r?(i) is the reliability of instance i (defined 
below) and maxpmi is the maximum pointwise 
mutual information between all patterns and all 
instances. r?(p) ranges from [0,1]. The reliability 
of the manually supplied seed instances are r?(i) 
= 1. The pointwise mutual information between 
instance i = {x, y} and pattern p is estimated us-
ing the following formula: 
 ( )
,**,,*,
,,
log,
pyx
ypx
pipmi =  
where |x, p, y| is the frequency of pattern p in-
stantiated with terms x and y and where the aster-
isk (*) represents a wildcard. A well-known 
problem is that pointwise mutual information is 
biased towards infrequent events. We thus multi-
ply pmi(i, p) with the discounting factor sug-
gested in (Pantel and Ravichandran 2004). 
Estimating the reliability of an instance is 
similar to estimating the reliability of a pattern. 
Intuitively, a reliable instance is one that is 
highly associated with as many reliable patterns 
as possible (i.e., we have more confidence in an 
instance when multiple reliable patterns instanti-
ate it.) Hence, analogous to our pattern reliability 
measure, we define the reliability of an instance 
i, r?(i), as: 
 ( )
( )
P
pr
pipmi
ir Pp pmi
?
??
?
=
?
?
max
),(
 
where r?(p) is the reliability of pattern p (defined 
earlier) and maxpmi is as before. Note that r?(i) 
and r?(p) are recursively defined, where r?(i) = 1 
for the manually supplied seed instances. 
3.3 Exploiting Generic Patterns 
Generic patterns are high recall / low precision 
patterns (e.g, the pattern ?X of Y? can ambigu-
ously refer to a part-of, is-a and possession rela-
tions). Using them blindly increases system 
recall while dramatically reducing precision. 
Minimally supervised algorithms have typically 
ignored them for this reason. Only heavily super-
vised approaches, like (Girju et al 2006) have 
successfully exploited them. 
Espresso?s recall can be significantly in-
creased by automatically separating correct in-
stances extracted by generic patterns from 
incorrect ones. The challenge is to harness the 
expressive power of the generic patterns while 
remaining minimally supervised. 
The intuition behind our method is that in a 
very large corpus, like the Web, correct instances 
of a generic pattern will be instantiated by many 
of Espresso?s reliable patterns accepted in P. Re-
call that, by definition, Espresso?s reliable pat-
terns extract instances with high precision (yet 
often low recall). In a very large corpus, like the 
Web, we assume that a correct instance will oc-
cur in at least one of Espresso?s reliable pattern 
even though the patterns? recall is low. Intui-
tively, our confidence in a correct instance in-
creases when, i) the instance is associated with 
many reliable patterns; and ii) its association 
with the reliable patterns is high. At a given Es-
presso iteration, where PR represents the set of 
previously selected reliable patterns, this intui-
tion is captured by the following measure of con-
fidence in an instance i = {x, y}: 
 ( ) ( ) ( )?
?
?=
RPp
p T
pr
iSiS ?   
where T is the sum of the reliability scores r?(p) 
for each pattern p ? PR, and 
 ( ) ( )
,**,,*,
,,
log,
pyx
ypx
pipmiiS p ?==
  
116
where pointwise mutual information between 
instance i and pattern p is estimated with Google 
as follows: 
 ( )
pyx
ypx
iS p ???
,,   
An instance i is rejected if S(i) is smaller than 
some threshold ?. 
Although this filtering may also be applied to 
reliable patterns, we found this to be detrimental 
in our experiments since most instances gener-
ated by reliable patterns are correct. In Espresso, 
we classify a pattern as generic when it generates 
more than 10 times the instances of previously 
accepted reliable patterns. 
4 Experimental Results 
In this section, we present an empirical compari-
son of Espresso with three state of the art sys-
tems on the task of extracting various semantic 
relations. 
4.1 Experimental Setup 
We perform our experiments using the following 
two datasets: 
? TREC: This dataset consists of a sample of 
articles from the Aquaint (TREC-9) newswire 
text collection. The sample consists of 
5,951,432 words extracted from the following 
data files: AP890101 ? AP890131, AP890201 
? AP890228, and AP890310 ? AP890319. 
? CHEM: This small dataset of 313,590 words 
consists of a college level textbook of introduc-
tory chemistry (Brown et al 2003). 
Each corpus is pre-processed using the Alembic 
Workbench POS-tagger (Day et al 1997). 
Below we describe the systems used in our 
empirical evaluation of Espresso. 
? RH02: The algorithm by Ravichandran and 
Hovy (2002) described in Section 2. 
? GI03: The algorithm by Girju et al (2006) de-
scribed in Section 2. 
? PR04: The algorithm by Pantel and Ravi-
chandran (2004) described in Section 2. 
? ESP-: The Espresso algorithm using the pat-
tern and instance reliability measures, but 
without using generic patterns. 
? ESP+: The full Espresso algorithm described 
in this paper exploiting generic patterns. 
For ESP+, we experimentally set ? from Section 
3.3 to ? = 0.4 for TREC and ? = 0.3 for CHEM 
by manually inspecting a small set of instances. 
Espresso is designed to extract various seman-
tic relations exemplified by a given small set of 
seed instances. We consider the standard is-a and 
part-of relations as well as the following more 
specific relations: 
? succession: This relation indicates that a person 
succeeds another in a position or title. For ex-
ample, George Bush succeeded Bill Clinton 
and Pope Benedict XVI succeeded Pope John 
Paul II. We evaluate this relation on the 
TREC-9 corpus. 
? reaction: This relation occurs between chemi-
cal elements/molecules that can be combined 
in a chemical reaction. For example, hydrogen 
gas reacts-with oxygen gas and zinc reacts-with 
hydrochloric acid. We evaluate this relation on 
the CHEM corpus. 
? production: This relation occurs when a proc-
ess or element/object produces a result1. For 
example, ammonia produces nitric oxide. We 
evaluate this relation on the CHEM corpus. 
For each semantic relation, we manually ex-
tracted a small set of seed examples. The seeds 
were used for both Espresso as well as RH02. 
Table 1 lists a sample of the seeds as well as 
sample outputs from Espresso. 
4.2 Precision and Recall 
We implemented the systems outlined in Section 
4.1, except for GI03, and applied them to the 
                                                     
1 Production is an ambiguous relation; it is intended to be 
a causation relation in the context of chemical reactions. 
Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number 
in the parentheses for each relation denotes the total number of seeds used as input for the system. 
 Is-a (12) Part-Of (12) Succession (12) Reaction (13) Production (14) 
Seeds 
wheat :: crop 
George Wendt :: star 
nitrogen :: element 
diborane :: substance 
leader :: panel 
city :: region 
ion :: matter 
oxygen :: water 
Khrushchev :: Stalin 
Carla Hills :: Yeutter 
Bush :: Reagan 
Julio Barbosa :: Mendes 
magnesium :: oxygen 
hydrazine :: water 
aluminum metal :: oxygen 
lithium metal :: fluorine gas 
bright flame :: flares 
hydrogen :: metal hydrides 
ammonia :: nitric oxide 
copper :: brown gas 
Es-
presso 
Picasso :: artist 
tax :: charge 
protein :: biopolymer 
HCl :: strong acid 
trees :: land 
material :: FBI report 
oxygen :: air 
atom :: molecule 
Ford :: Nixon 
Setrakian :: John Griesemer 
Camero Cardiel :: Camacho
Susan Weiss :: editor 
hydrogen :: oxygen 
Ni :: HCl 
carbon dioxide :: methane 
boron :: fluorine 
electron :: ions 
glycerin :: nitroglycerin 
kidneys :: kidney stones 
ions :: charge 
 
117
Table 8. System performance: CHEM/production.
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 197 57.5% 0.80 
ESP- 196 72.5% 1.00 
ESP+ 1676 55.8% 6.58 
 
TREC and CHEM datasets. For each output set, 
per relation, we evaluate the precision of the sys-
tem by extracting a random sample of instances 
(50 for the TREC corpus and 20 for the CHEM 
corpus) and evaluating their quality manually 
using two human judges (a total of 680 instances 
were annotated per judge). For each instance, 
judges may assign a score of 1 for correct, 0 for 
incorrect, and ? for partially correct. Example 
instances that were judged partially correct in-
clude ?analyst is-a manager? and ?pilot is-a 
teacher?. The kappa statistic (Siegel and Castel-
lan Jr. 1988) on this task was ? = 0.692. The pre-
cision for a given set of instances is the sum of 
the judges? scores divided by the total instances. 
Although knowing the total number of correct 
instances of a particular relation in any non-
trivial corpus is impossible, it is possible to com-
pute the recall of a system relative to another sys-
tem?s recall. Following (Pantel et al 2004), we 
define the relative recall of system A given sys-
tem B, RA|B, as: 
 
BP
AP
C
C
R
R
R
B
A
B
A
C
C
C
C
B
A
BA
B
A
?
?====|  
where RA is the recall of A, CA is the number of 
correct instances extracted by A, C is the (un-
known) total number of correct instances in the 
corpus, PA is A?s precision in our experiments, 
                                                     
2 The kappa statistic jumps to ? = 0.79 if we treat partially 
correct classifications as correct. 
and |A| is the total number of instances discov-
ered by A. 
Tables 2 ? 8 report the total number of in-
stances, precision, and relative recall of each sys-
tem on the TREC-9 and CHEM corpora 3 4 . The 
relative recall is always given in relation to the 
ESP- system. For example, in Table 2, RH02 has 
a relative recall of 5.31 with ESP-, which means 
that the RH02 system outputs 5.31 times more 
correct relations than ESP- (at a cost of much 
lower precision). Similarly, PR04 has a relative 
recall of 0.23 with ESP-, which means that PR04 
outputs 4.35 fewer correct relations than ESP- 
(also with a smaller precision). We did not in-
clude the results from GI03 in the tables since the 
system is only applicable to part-of relations and 
we did not reproduce it. However, the authors 
evaluated their system on a sample of the TREC-
9 dataset and reported 83% precision and 72% 
recall (this algorithm is heavily supervised.) 
                                                     
* Because of the small evaluation sets, we estimate the 
95% confidence intervals using bootstrap resampling to be 
in the order of ? 10-15% (absolute numbers). 
? Relative recall is given in relation to ESP-. 
Table 2. System performance: TREC/is-a. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 57,525 28.0% 5.31 
PR04 1,504 47.0% 0.23 
ESP- 4,154 73.0% 1.00 
ESP+ 69,156 36.2% 8.26 
Table 4. System performance: TREC/part-of. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 12,828 35.0% 42.52 
ESP- 132 80.0% 1.00 
ESP+ 87,203 69.9% 577.22 
Table 3. System performance: CHEM/is-a. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 2556 25.0% 3.76 
PR04 108 40.0% 0.25 
ESP- 200 85.0% 1.00 
ESP+ 1490 76.0% 6.66 
Table 5. System performance: CHEM/part-of. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 11,582 33.8% 58.78 
ESP- 111 60.0% 1.00 
ESP+ 5973 50.7% 45.47 
Table 7. System performance: CHEM/reaction. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 6,083 30% 53.67 
ESP- 40 85% 1.00 
ESP+ 3102 91.4% 89.39 
Table 6. System performance: TREC/succession.
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 49,798 2.0% 36.96 
ESP- 55 49.0% 1.00 
ESP+ 55 49.0% 1.00 
118
In all tables, RH02 extracts many more rela-
tions than ESP-, but with a much lower precision, 
because it uses generic patterns without filtering. 
The high precision of ESP- is due to the effective 
reliability measures presented in Section 3.2. 
4.3 Effect of Generic Patterns 
Experimental results, for all relations and the two 
different corpus sizes, show that ESP- greatly 
outperforms the other methods on precision. 
However, without the use of generic patterns, the 
ESP- system shows lower recall in all but the 
production relation. 
As hypothesized, exploiting generic patterns 
using the algorithm from Section 3.3 substan-
tially improves recall without much deterioration 
in precision. ESP+ shows one to two orders of 
magnitude improvement on recall while losing 
on average below 10% precision. The succession 
relation in Table 6 was the only relation where 
Espresso found no generic pattern. For other re-
lations, Espresso found from one to five generic 
patterns. Table 4 shows the power of generic pat-
terns where system recall increases by 577 times 
with only a 10% drop in precision. In Table 7, we 
see a case where the combination of filtering 
with a large increase in retrieved instances re-
sulted in both higher precision and recall. 
In order to better analyze our use of generic 
patterns, we performed the following experiment. 
For each relation, we randomly sampled 100 in-
stances for each generic pattern and built a gold 
standard for them (by manually tagging each in-
stance as correct or incorrect). We then sorted the 
100 instances according to the scoring formula 
S(i) derived in Section 3.3 and computed the av-
erage precision, recall, and F-score of each top-K 
ranked instances for each pattern5. Due to lack of 
space, we only present the graphs for four of the 
22 generic patterns: ?X is a Y? for the is-a rela-
tion of Table 2, ?X in the Y? for the part-of rela-
tion of Table 4, ?X in Y? for the part-of relation 
of Table 5, and ?X and Y? for the reaction rela-
tion of Table 7. Figure 1 illustrates the results. 
In each figure, notice that recall climbs at a 
much faster rate than precision decreases. This 
indicates that the scoring function of Section 3.3 
effectively separates correct and incorrect in-
stances. In Figure 1a), there is a big initial drop 
in precision that accounts for the poor precision 
reported in Table 1. 
Recall that the cutoff points on S(i) were set to 
? = 0.4 for TREC and ? = 0.3 for CHEM. The 
figures show that this cutoff is far from the 
maximum F-score. An interesting avenue of fu-
ture work would be to automatically determine 
the proper threshold for each individual generic 
pattern instead of setting a uniform threshold. 
                                                     
5 We can directly compute recall here since we built a 
gold standard for each set of 100 samples. 
Figure 1. Precision, recall and F-score curves of the Top-K% ranking instances of patterns ?X is a Y? 
(TREC/is-a), ?X in Y? (TREC/part-of), ?X in the Y? (CHEM/part-of), and ?X and Y? (CHEM/reaction). 
a) TREC/is-a: "X is a Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
d) CHEM/reaction: "X and Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
c) CHEM/part-of: "X in Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
b) TREC/part-of: "X in the Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
119
5 Conclusions 
We proposed a weakly-supervised, general-
purpose, and accurate algorithm, called Espresso, 
for harvesting binary semantic relations from raw 
text. The main contributions are: i) a method for 
exploiting generic patterns by filtering incorrect 
instances using the Web; and ii) a principled 
measure of pattern and instance reliability ena-
bling the filtering algorithm. 
We have empirically compared Espresso?s 
precision and recall with other systems on both a 
small domain-specific textbook and on a larger 
corpus of general news, and have extracted sev-
eral standard and specific semantic relations: is-
a, part-of, succession, reaction, and production. 
Espresso achieves higher and more balanced per-
formance than other state of the art systems. By 
exploiting generic patterns, system recall sub-
stantially increases with little effect on precision. 
There are many avenues of future work both in 
improving system performance and making use 
of the relations in applications like question an-
swering. For the former, we plan to investigate 
the use of WordNet to automatically learn selec-
tional constraints on generic patterns, as pro-
posed by (Girju et al 2006). We expect here that 
negative instances will play a key role in deter-
mining the selectional restrictions. 
Espresso is the first system, to our knowledge, 
to emphasize concurrently performance, minimal 
supervision, breadth, and generality. It remains 
to be seen whether one could enrich existing on-
tologies with relations harvested by Espresso, 
and it is our hope that these relations will benefit 
NLP applications. 
References 
Berland, M. and E. Charniak, 1999. Finding parts in very 
large corpora. In Proceedings of ACL-1999. pp. 57-64. 
College Park, MD. 
Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 
2003. Chemistry: The Central Science, Ninth Edition. 
Prentice Hall. 
Caraballo, S. 1999. Automatic acquisition of a hypernym-
labeled noun hierarchy from text. In Proceedings of 
ACL-99. pp 120-126, Baltimore, MD. 
Cover, T.M. and Thomas, J.A. 1991. Elements of 
Information Theory. John Wiley & Sons. 
Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; 
Robinson, P.; and Vilain, M. 1997. Mixed-initiative 
development of language processing systems. In 
Proceedings of ANLP-97. Washington D.C. 
Downey, D.; Etzioni, O.; and Soderland, S. 2005. A 
Probabilistic model of redundancy in information 
extraction. In Proceedings of IJCAI-05. pp. 1034-1041. 
Edinburgh, Scotland. 
Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; 
Shaked, T.; Soderland, S.; Weld, D.S.; and Yates, A. 
2005. Unsupervised named-entity extraction from the 
Web: An experimental study. Artificial Intelligence, 
165(1): 91-134. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press. 
Geffet, M. and Dagan, I. 2005. The Distributional Inclusion 
Hypotheses and Lexical Entailment. In Proceedings of 
ACL-2005. Ann Arbor, MI. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2006. 
Automatic Discovery of Part-Whole Relations. 
Computational Linguistics, 32(1): 83-135. 
Hearst, M. 1992. Automatic acquisition of hyponyms from 
large text corpora. In Proceedings of COLING-92. pp. 
539-545. Nantes, France. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 268?
275. Pittsburgh, PA. 
Justeson J.S. and Katz S.M. 1995. Technical Terminology: 
some linguistic properties and algorithms for 
identification in text. In Proceedings of ICCL-95. 
pp.539-545. Nantes, France. 
Lin, C.-Y. and Hovy, E.H.. 2000. The Automated 
acquisition of topic signatures for text summarization. In 
Proceedings of COLING-00. pp. 495-501. Saarbr?cken, 
Germany. 
Lin, D. and Pantel, P. 2002. Concept discovery from text. In 
Proceedings of COLING-02. pp. 577-583. Taipei, 
Taiwan. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. In Proceedings of SemaNet? 02: 
Building and Using Semantic Networks, Taipei, Taiwan. 
Pantel, P. and Ravichandran, D. 2004. Automatically 
labeling semantic classes. In Proceedings of 
HLT/NAACL-04. pp. 321-328. Boston, MA. 
Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards 
terascale knowledge acquisition. In Proceedings of 
COLING-04. pp. 771-777. Geneva, Switzerland. 
Pasca, M. and Harabagiu, S. 2001. The informative role of 
WordNet in Open-Domain Question Answering. In 
Proceedings of NAACL-01 Workshop on WordNet and 
Other Lexical Resources. pp. 138-143. Pittsburgh, PA. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach 
for building semantic lexicons. In Proceedings of 
EMNLP-97. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. 
Scaling web-based acquisition of entailment relations. In 
Proceedings of EMNLP-04. Barcelona, Spain. 
120
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 793?800,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ontologizing Semantic Relations 
 
Marco Pennacchiotti 
ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA90292 
pantel@isi.edu 
  
Abstract 
Many algorithms have been developed 
to harvest lexical semantic resources, 
however few have linked the mined 
knowledge into formal knowledge re-
positories. In this paper, we propose two 
algorithms for automatically ontologiz-
ing (attaching) semantic relations into 
WordNet. We present an empirical 
evaluation on the task of attaching part-
of and causation relations, showing an 
improvement on F-score over a baseline 
model. 
1 Introduction 
NLP researchers have developed many algo-
rithms for mining knowledge from text and the 
Web, including facts (Etzioni et al 2005), se-
mantic lexicons (Riloff and Shepherd 1997), 
concept lists (Lin and Pantel 2002), and word 
similarity lists (Hindle 1990). Many recent ef-
forts have also focused on extracting binary se-
mantic relations between entities, such as 
entailments (Szpektor et al 2004), is-a (Ravi-
chandran and Hovy 2002), part-of (Girju et al 
2003), and other relations. 
The output of most of these systems is flat lists 
of lexical semantic knowledge such as ?Italy is-a 
country? and ?orange similar-to blue?. However, 
using this knowledge beyond simple keyword 
matching, for example in inferences, requires it 
to be linked into formal semantic repositories 
such as ontologies or term banks like WordNet 
(Fellbaum 1998). 
Pantel (2005) defined the task of ontologizing 
a lexical semantic resource as linking its terms to 
the concepts in a WordNet-like hierarchy. For 
example, ?orange similar-to blue? ontologizes in 
WordNet to ?orange#2 similar-to blue#1? and 
?orange#2 similar-to blue#2?. In his framework, 
Pantel proposed a method of inducing ontologi-
cal co-occurrence vectors 1  which are subse-
quently used to ontologize unknown terms into 
WordNet with 74% accuracy. 
In this paper, we take the next step and explore 
two algorithms for ontologizing binary semantic 
relations into WordNet and we present empirical 
results on the task of attaching part-of and causa-
tion relations. Formally, given an instance  
(x, r, y) of a binary relation r between terms x 
and y, the ontologizing task is to identify the 
WordNet senses of x and y where r holds. For 
example, the instance (proton, PART-OF, element) 
ontologizes into WordNet as (proton#1, PART-OF, 
element#2). 
The first algorithm that we explore, called the 
anchoring approach, was suggested as a promis-
ing avenue of future work in (Pantel 2005). This 
bottom up algorithm is based on the intuition that 
x can be disambiguated by retrieving the set of 
terms that occur in the same relation r with y and 
then finding the senses of x that are most similar 
to this set. The assumption is that terms occur-
ring in the same relation will tend to have similar 
meaning. In this paper, we propose a measure of 
similarity to capture this intuition. 
In contrast to anchoring, our second algorithm, 
called the clustering approach, takes a top-down 
view. Given a relation r, suppose that we are 
given every conceptual instance of r, i.e., in-
stances of r in the upper ontology like (parti-
cles#1, PART-OF, substances#1). An instance  
(x, r, y) can then be ontologized easily by finding 
the senses of x and y that are subsumed by ances-
tors linked by a conceptual instance of r. For ex-
ample, the instance (proton, PART-OF, element) 
ontologizes to (proton#1, PART-OF, element#2) 
since proton#1 is subsumed by particles and 
element#2 is subsumed by substances. The prob-
lem then is to automatically infer the set of con-
                                                     
1 The ontological co-occurrence vector of a concept con-
sists of all lexical co-occurrences with the concept in a 
corpus. 
793
ceptual instances. In this paper, we develop a 
clustering algorithm for generalizing a set of re-
lation instances to conceptual instances by look-
ing up the WordNet hypernymy hierarchy for 
common ancestors, as specific as possible, that 
subsume as many instances as possible. An in-
stance is then attached to its senses that are sub-
sumed by the highest scoring conceptual 
instances. 
2 Relevant Work 
Several researchers have worked on ontologizing 
semantic resources. Most recently, Pantel (2005) 
developed a method to propagate lexical co-
occurrence vectors to WordNet synsets, forming 
ontological co-occurrence vectors. Adopting an 
extension of the distributional hypothesis (Harris 
1985), the co-occurrence vectors are used to 
compute the similarity between synset/synset and 
between lexical term/synset. An unknown term is 
then attached to the WordNet synset whose co-
occurrence vector is most similar to the term?s 
co-occurrence vector. Though the author sug-
gests a method for attaching more complex lexi-
cal structures like binary semantic relations, the 
paper focused only on attaching terms. 
Basili (2000) proposed an unsupervised 
method to infer semantic classes (WordNet syn-
sets) for terms in domain-specific verb relations. 
These relations, such as (x, EXPAND, y) are first 
automatically learnt from a corpus. The semantic 
classes of x and y are then inferred using concep-
tual density (Agirre and Rigau 1996), a Word-
Net-based measure applied to all instantiation of 
x and y in the corpus. Semantic classes represent 
possible common generalizations of the verb ar-
guments. At the end of the process, a set of syn-
tactic-semantic patterns are available for each 
verb, such as: 
(social_group#1, expand, act#2) 
(instrumentality#2, expand, act#2) 
The method is successful on specific relations 
with few instances (such as domain verb rela-
tions) while its value on generic and frequent 
relations, such as part-of, was untested. 
Girju et al (2003) presented a highly super-
vised machine learning algorithm to infer seman-
tic constraints on part-of relations, such as 
(object#1, PART-OF, social_event#1). These con-
straints are then used as selectional restrictions in 
harvesting part-of instances from ambiguous 
lexical patterns, like ?X of Y?. The approach 
shows high performance in terms of precision 
and recall, but, as the authors acknowledge, it 
requires large human effort during the training 
phase. 
Others have also made significant additions to 
WordNet. For example, in eXtended WordNet 
(Harabagiu et al 1999), the glosses in WordNet 
are enriched by disambiguating the nouns, verbs, 
adverbs, and adjectives with synsets. Another 
work has enriched WordNet synsets with topi-
cally related words extracted from the Web 
(Agirre et al 2001). Finally, the general task of 
word sense disambiguation (Gale et al 1991) is 
relevant since there the task is to ontologize each 
term in a passage into a WordNet-like sense in-
ventory. If we had a large collection of sense-
tagged text, then our mining algorithms could 
directly discover WordNet attachment points at 
harvest time. However, since there is little high 
precision sense-tagged corpora, methods are re-
quired to ontologize semantic resources without 
fully disambiguating text. 
3 Ontologizing Semantic Relations 
Given an instance (x, r, y) of a binary relation r 
between terms x and y, the ontologizing task is to 
identify the senses of x and y where r holds. In 
this paper, we focus on WordNet 2.0 senses, 
though any similar term bank would apply. 
Let Sx and Sy be the sets of all WordNet senses 
of x and y. A sense pair, sxy, is defined as any 
pair of senses of x and y: sxy={sx, sy} where sx?Sx 
and sy?Sy. The set of all sense pairs Sxy consists 
of all permutations between senses in Sx and Sy. 
In order to attach a relation instance (x, r, y) 
into WordNet, one must: 
? Disambiguate x and y, that is, find the subsets 
S'x?Sx and S'y?Sy for which the relation r holds; 
and 
? Instantiate the relation in WordNet, using the 
synsets corresponding to all correct permuta-
tions between the senses in S'x and S'y. We de-
note this set of attachment points as S'xy. 
If Sx or Sy is empty, no attachments are produced. 
For example, the instance (study, PART-OF, re-
port) is ontologized into WordNet through the 
senses S'x={survey#1, study#2} and 
S?y={report#1}. The final attachment points S'xy 
are: 
(survey#1, PART-OF, report#1) 
(study#1, PART-OF, report#1) 
Unlike common algorithms for word sense 
disambiguation, here it is important to take into 
consideration the semantic dependency between 
the two terms x and y. For example, an entity that 
is part-of a study has to be some kind of informa-
794
tion. This knowledge about mutual selectional 
preference (the preferred semantic class that fills 
a certain relation role, as x or y) can be exploited 
to ontologize the instance. 
In the following sections, we propose two al-
gorithms for ontologizing binary semantic rela-
tions. 
3.1 Method 1: Anchor Approach 
Given an instance (x, r, y), this approach fixes the 
term y, called the anchor, and then disambiguates 
x by looking at all other terms that occur in the 
relation r with y. Based on the principle of distri-
butional similarity (Harris 1985), the algorithm 
assumes that the words that occur in the same 
relation r with y will be more similar to the cor-
rect sense(s) of x than the incorrect ones. After 
disambiguating x, the process is then inverted 
with x as the anchor to disambiguate y. 
In the first step, y is fixed and the algorithm 
retrieves the set of all other terms X' that occur in 
an instance (x', r, y), x' ? X'2. For example, given 
the instance (reflections, PART-OF, book), and a 
resource containing the following relations: 
 (false allegations, PART-OF, book) 
 (stories, PART-OF, book) 
 (expert analysis, PART-OF, book) 
 (conclusions, PART-OF, book) 
the resulting set X' would be: {allegations, sto-
ries, analysis, conclusions}. 
All possible permutations, Sxx', between the 
senses of x and the senses of each term in X', 
called Sx', are computed. For each sense pair  
{sx, sx'} ? Sxx', a similarity score r(sx, sx') is calcu-
lated using WordNet: 
 )(
1),(
1
),( '
'
' x
xx
xx sfssd
ssr ?+=  
where the distance d(sx, sx') is the length of the 
shortest path connecting the two synsets in the 
hypernymy hierarchy of WordNet, and f(sx') is 
the number of times sense sx' occurs in any of the 
instances of X'. Note that if no connection be-
tween two synsets exists, then r(sx, sx') = 0. 
The overall sense score for each sense sx of x 
is calculated as: 
 ?
?
=
''
),()( '
xx Ss
xxx ssrsr  
Finally, the algorithm inverts the process by 
setting x as the anchor and computes r(sy) for 
                                                     
2 For semantic relations between complex terms, like (ex-
pert analysis, PART-OF, book), only the head noun of terms 
are recorded, like ?analysis?. As a future work, we plan to 
use the whole term if it is present in WordNet. 
each sense of y. All possible permutations of 
senses are computed and scored by averaging 
r(sx) and r(sy). Permutations scoring higher than a 
threshold ?1 are selected as the attachment points 
in WordNet. We experimentally set ?1 = 0.02. 
3.2 Method 2: Clustering Approach 
The main idea of the clustering approach is to 
leverage the lexical behaviors of the two terms in 
an instance as a whole. The assumption is that 
the general meaning of the relation is derived 
from the combination of the two terms. 
The algorithm is divided in two main phases. 
In the first phase, semantic clusters are built us-
ing the WordNet senses of all instances. A se-
mantic cluster is defined by the set of instances 
that have a common semantic generalization. We 
denote the conceptual instance of the semantic 
cluster as the pair of WordNet synsets that repre-
sents this generalization. For example the follow-
ing two part-of instances: 
 (second section, PART-OF, Los Angeles-area news) 
 (Sandag study, PART-OF, report) 
are in a common cluster represented by the fol-
lowing conceptual instance: 
 [writing#2, PART-OF, message#2] 
since writing#2 is a hypernym of both section 
and study, and message#2 is a hypernym of news 
and report3. 
In the second phase, the algorithm attaches an 
instance into WordNet by using WordNet dis-
tance metrics and frequency scores to select the 
best cluster for each instance. A good cluster is 
one that: 
? achieves a good trade-off between generality 
and specificity; and 
? disambiguates among the senses of x and y us-
ing the other instances? senses as support. 
For example, given the instance (second section, 
PART-OF, Los Angeles-area news) and the follow-
ing conceptual instances: 
 [writing#2, PART-OF, message#2] 
 [object#1, PART-OF, message#2] 
 [writing#2, PART-OF, communication#2]  
 [social_group#1, PART-OF, broadcast#2]   
 [organization#, PART-OF, message#2] 
the first conceptual instance should be scored 
highest since it is both not too generic nor too 
specific and is supported by the instance (Sandag 
study, PART-OF, report), i.e., the conceptual in-
stance subsumes both instances. The second and 
                                                     
3 Again, here, we use the syntactic head of each term for 
generalization since we assume that it drives the meaning 
of the term itself. 
795
the third conceptual instances should be scored 
lower since they are too generic, while the last 
two should be scored lower since the sense for 
section and news are not supported by other in-
stances. The system then outputs, for each in-
stance, the set of sense pairs that are subsumed 
by the highest scoring conceptual instance. In the 
previous example: 
(section#1, PART-OF, news#1) 
(section#1, PART-OF, news#2) 
(section#1, PART-OF, news#3) 
are selected, as they are subsumed by [writing#2, 
PART-OF, message#2]. These sense pairs are then 
retained as attachment points into WordNet. 
Below, we describe each phase in more detail. 
Phase 1: Cluster Building 
Given an instance (x, r, y), all sense pair permu-
tations sxy={sx, sy} are retrieved from WordNet. 
A set of candidate conceptual instances, Cxy, is 
formed for each instance from the permutation of 
each WordNet ancestor of sx and sy, following the 
hypernymy link, up to degree ?2. 
Each candidate conceptual instance,  
c={cx, cy}, is scored by its degree of generaliza-
tion as follows: 
 
)1()1(
1
)( +?+= yx nn
cr  
where ni is the number of hypernymy links 
needed to go from si to ci, for i ? {x, y}. r(c) 
ranges from [0, 1] and is highest when little gen-
eralization is needed. 
For example, the instance (Sandag study, 
PART-OF, report) produces 70 sense pairs since 
study has 10 senses and report has 7 senses. As-
suming ?2=1, the instance sense (survey#1, PART-
OF, report#1) has the following set of candidate 
conceptual instances: 
 
Cxy nx ny r(c)
(survey#1, PART-OF,report#1) 0 0 1
(survey#1, PART-OF,document#1) 0 1 0.5
(examination#1, PART-OF,report#1) 1 0 0.5
(examination#1, PART-OF,document#1) 1 1 0.25
 
Finally, each candidate conceptual instance c 
forms a cluster of all instances (x, r, y) that have 
some sense pair sx and sy as hyponyms of c. Note 
also that candidate conceptual instances may be 
subsumed by other candidate conceptual in-
stances. Let Gc refer to the set of all candidate 
conceptual instances subsumed by candidate 
conceptual instance c. 
Intuitively, better candidate conceptual in-
stances are those that subsume both many in-
stances and other candidate conceptual instances, 
but at the same time that have the least distance 
from subsumed instances. We capture this intui-
tion with the following score of c: 
 cc
c
Gg GI
G
gr
cscore c loglog
)(
)( ??=
?
?  
where Ic is the set of instances subsumed by c. 
We experimented with different variations of this 
score and found that it is important to put more 
weight on the distance between subsumed con-
ceptual instances than the actual number of sub-
sumed instances. Without the log terms, the 
highest scoring conceptual instances are too ge-
neric (i.e., they are too high up in the ontology). 
Phase 2: Attachment Points Selection 
In this phase, we utilize the conceptual instances 
of the previous phase to attach each instance  
(x, r, y) into WordNet. 
At the end of Phase 1, an instance can be clus-
tered in different conceptual instances. In order 
to select an attachment, the algorithm selects the 
sense pair of x and y that is subsumed by the 
highest scoring candidate conceptual instance. It 
and all other sense pairs that are subsumed by 
this conceptual instance are then retained as the 
final attachment points. 
As a side effect, a final set of conceptual in-
stances is obtained by deleting from each candi-
date those instances that are subsumed by a 
higher scoring conceptual instance. Remaining 
conceptual instances are then re-scored using 
score(c). The final set of conceptual instances 
thus contains unambiguous sense pairs. 
4 Experimental Results 
In this section we provide an empirical evalua-
tion of our two algorithms. 
4.1 Experimental Setup 
Researchers have developed many algorithms for 
harvesting semantic relations from corpora and 
the Web. For the purposes of this paper, we may 
choose any one of them and manually validate its 
mined relations. We choose Espresso4, a general-
purpose, broad, and accurate corpus harvesting 
algorithm requiring minimal supervision. Adopt-
                                                     
4 Reference suppressed ? the paper introducing Espresso 
has also been submitted to COLING/ACL 2006. 
796
ing a bootstrapping approach, Espresso takes as 
input a few seed instances of a particular relation 
and iteratively learns surface patterns to extract 
more instances. 
Test Sets 
We experiment with two relations: part-of and 
causation. The causation relation occurs when an 
entity produces an effect or is responsible for 
events or results, for example (virus, CAUSE, in-
fluenza) and (burning fuel, CAUSE, pollution). We 
manually built five seed relation instances for 
both relations and apply Espresso to a dataset 
consisting of a sample of articles from the 
Aquaint (TREC-9) newswire text collection. The 
sample consists of 55.7 million words extracted 
from the Los Angeles Times data files. Espresso 
extracted 1,468 part-of instances and 1,129 cau-
sation instances. We manually validated the out-
put and randomly selected 200 correct relation 
instances of each relation for ontologizing into 
WordNet 2.0. 
Gold Standard 
We manually built a gold standard of all correct 
attachments of the test sets in WordNet. For each 
relation instance (x, r, y), two human annotators 
selected from all sense permutations of x and y 
the correct attachment points in WordNet. For 
example, for (synthetic material, PART-OF, filter), 
the judges selected the following attachment 
points: (synthetic material#1, PART-OF, filter#1) 
and (synthetic material#1, PART-OF, filter#2). The 
kappa statistic (Siegel and Castellan Jr. 1988) on 
the two relations together was ? = 0.73. 
Systems 
The following three systems are evaluated: 
? BL: the baseline system that attaches each rela-
tion instance to the first (most common) 
WordNet sense of both terms; 
? AN: the anchor approach described in Section 
3.1. 
? CL: the clustering approach described in Sec-
tion 3.2. 
4.2 Precision, Recall and F-score 
For both the part-of and causation relations, we 
apply the three systems described above and 
compare their attachment performance using pre-
cision, recall, and F-score. Using the manually 
built gold standard, the precision of a system on a 
given relation instance is measured as the per-
centage of correct attachments and recall is 
measured as the percentage of correct attach-
ments retrieved by the system. Overall system 
precision and recall are then computed by aver-
aging the precision and recall of each relation 
instance. 
Table 1 and Table 2 report the results on the 
part-of and causation relations. We experimen-
tally set the CL generalization parameter ?2 to 5 
and the ?1 parameter for AN to 0.02. 
4.3 Discussion 
For both relations, CL and AN outperform the 
baseline in overall F-score. For part-of, Table 1 
shows that CL outperforms BL by 13.6% in F-
score and AN by 9.4%. For causation, Table 2 
shows that AN outperforms BL by 4.4% on F-
score and CL by 0.6%. 
The good results of the CL method on the 
part-of relation suggest that instances of this rela-
tion are particularly amenable to be clustered. 
The generality of the part-of relation in fact al-
lows the creation of fairly natural clusters, corre-
sponding to different sub-types of part-of, as 
those proposed in (Winston 1983). The causation 
relation, however, being more difficult to define 
at a semantic level (Girju 2003), is less easy to 
cluster and thus to disambiguate. 
Both CL and AN have better recall than BL, 
but precision results vary with CL beating BL 
only on the part-of relation. Overall, the system 
performances suggest that ontologizing semantic 
relations into WordNet is in general not easy. 
The better results of CL and AN with respect 
to BL suggest that the use of comparative seman-
tic analysis among corpus instances is a good 
way to carry out disambiguation. Yet, the BL 
SYSTEM PRECISION RECALL F-SCORE 
BL 45.0% 25.0% 32.1% 
AN 41.7% 32.4% 36.5% 
CL 40.0% 32.6% 35.9% 
Table 2. System precision, recall and F-score on 
the causation relation. 
 
SYSTEM PRECISION RECALL F-SCORE 
BL 54.0% 31.3% 39.6% 
AN 40.7% 47.3% 43.8% 
CL 57.4% 49.6% 53.2% 
Table 1. System precision, recall and F-score on 
the part-of relation. 
 
797
method shows surprisingly good results. This 
indicates that also a simple method based on 
word sense usage in language can be valuable. 
An interesting avenue of future work is to better 
combine these two different views in a single 
system. 
The low recall results for CL are mostly at-
tributed to the fact that in Phase 2 only the best 
scoring cluster is retained for each instance. This 
means that instances with multiple senses that do 
not have a common generalization are not cap-
tured. For example the part-of instance (wings, 
PART-OF, chicken) should cluster both in 
[body_part#1, PART-OF, animal#1] and 
[body_part#1, PART-OF, food#2], but only the 
best scoring one is retained. 
5 Conceptual Instances: Other Uses 
Our clustering approach from Section 3.2 is en-
abled by learning conceptual instances ? relations 
between mid-level ontological concepts. Beyond 
the ontologizing task, conceptual instances may 
be useful for several other tasks. In this section, 
we discuss some of these opportunities and pre-
sent small qualitative evaluations. 
Conceptual instances represent common se-
mantic generalizations of a particular relation. 
For example, below are two possible conceptual 
instances for the part-of relation: 
 [person#1, PART-OF, organization#1] 
 [act#1, PART-OF, plan#1] 
The first conceptual instance in the example sub-
sumes all the part-of instances in which one or 
more persons are part of an organization, such as: 
 (president Brown, PART-OF, executive council) 
 (representatives, PART-OF, organization) 
 (students, PART-OF, orchestra) 
 (players, PART-OF, Metro League) 
Below, we present three possible ways of ex-
ploiting these conceptual instances. 
Support to Relation Extraction Tools 
Conceptual instances may be used to support re-
lation extraction algorithms such as Espresso. 
Most minimally supervised harvesting algo-
rithm do not exploit generic patterns, i.e. those 
patterns with high recall but low precision, since 
they cannot separate correct and incorrect rela-
tion instances. For example, the pattern ?X of Y? 
extracts many correct relation instances like 
?wheel of the car? but also many incorrect ones 
like ?house of representatives?. 
Girju et al (2003) described a highly super-
vised algorithm for learning semantic constraints 
on generic patterns, leading to a very significant 
increase in system recall without deteriorating 
precision. Conceptual instances can be used to 
automatically learn such semantic constraints by 
acting as a filter for generic patterns, retaining 
only those instances that are subsumed by high 
scoring conceptual instances. Effectively, con-
ceptual instances are used as selectional restric-
tions for the relation. For example, our system 
discards the following incorrect instances: 
 (week, CAUSE, coalition) 
 (demeanor, CAUSE, vacuum) 
as they are both part of the very low scoring con-
ceptual instance [abstraction#6, CAUSE, state#1]. 
Ontology Learning from Text 
Each conceptual instance can be viewed as a 
formal specification of the relation at hand. For 
example, Winston (1983) manually identified six 
sub-types of the part-of relation: member-
collection, component-integral object, portion-
mass, stuff-object, feature-activity and place-
area. Such classifications are useful in applica-
tions and tasks where a semantically rich organi-
zation of knowledge is required. Conceptual 
instances can be viewed as an automatic deriva-
tion of such a classification based on corpus us-
age. Moreover, conceptual instances can be used 
to improve the ontology learning process itself. 
For example, our clustering approach can be 
seen as an inductive step producing conceptual 
instances that are then used in a deductive step to 
learn new instances. An algorithm could iterate 
between the induction/deduction cycle until no 
new relation instances and conceptual instances 
can be inferred. 
Word Sense Disambiguation 
Word Sense Disambiguation (WSD) systems can 
exploit the selectional restrictions identified by 
conceptual instances to disambiguate ambiguous 
terms occurring in particular contexts. For exam-
ple, given the sentence: 
?the board is composed by members of different countries? 
and a harvesting algorithm that extracts the part-
of relation (members, PART-OF, board), the sys-
tem could infer the correct senses for board and 
members by looking at their closest conceptual 
instance. In our system, we would infer the at-
tachment (member#1, PART-OF, board#1) since it 
is part of the highest scoring conceptual instance 
[person#1, PART-OF, organization#1]. 
798
5.1 Qualitative Evaluation 
Table 3 and Table 4 list samples of the highest 
ranking conceptual instances obtained by our 
system for the part-of and causation relations. 
Below we provide a small evaluation to verify: 
? the correctness of the conceptual instances. 
Incorrect conceptual instances such as [attrib-
ute#2, CAUSE, state#4], discovered by our sys-
tem, can impede WSD and extraction tools 
where precise selectional restrictions are 
needed; and 
? the accuracy of the conceptual instances. 
Sometimes, an instance is incorrectly attached 
to a correct conceptual instance. For example, 
the instance (air mass, PART-OF, cold front) is 
incorrectly clustered in [group#1, PART-OF, 
multitude#3] since mass and front both have a 
sense that is descendant of group#1 and multi-
tude#3. However, these are not the correct 
senses of mass and front for which the part-of 
relation holds. 
For evaluating correctness, we manually ver-
ify how many correct conceptual instances are 
produced by Phase 2 of the clustering approach 
described in Section 3.2. The claim is that a cor-
rect conceptual instance is one for which the re-
lation holds for all possible subsumed senses. For 
example, the conceptual instance [group#1, 
PART-OF, multitude#3] is correct, as the relation 
holds for every semantic subsumption of the two 
senses. An example of an incorrect conceptual 
instance is [state#4, CAUSE, abstraction#6] since 
it subsumes the incorrect instance (audience, 
CAUSE, new context). A manual evaluation of the 
highest scoring 200 conceptual instances, gener-
ated on our test sets described in Section 4.1, 
showed 82% correctness for the part-of relation 
and 86% for causation. 
For estimating the overall clustering accuracy, 
we evaluated the number of correctly clustered 
instances in each conceptual instance. For exam-
ple, the instance (business people, PART-OF, 
committee) is correctly clustered in [multitude#3, 
PART-OF, group#1] and the instance (law, PART-
OF, constitutional pitfalls) is incorrectly clustered 
in [group#1, PART-OF, artifact#1]. We estimated 
the overall accuracy by manually judging the 
instances attached to 10 randomly sampled con-
ceptual instances. The accuracy for part-of is 
84% and for causation it is 76.6%. 
6 Conclusions 
In this paper, we proposed two algorithms for 
automatically ontologizing binary semantic rela-
tions into WordNet: an anchoring approach and 
a clustering approach. Experiments on the part-
of and causation relations showed promising re-
sults. Both algorithms outperformed the baseline 
on F-score. Our best results were on the part-of 
relation where the clustering approach achieved 
13.6% higher F-score than the baseline. 
The induction of conceptual instances has 
opened the way for many avenues of future 
work. We intend to pursue the ideas presented in 
Section 5 for using conceptual instances to:  
i) support knowledge acquisition tools by learn-
ing semantic constraints on extracting patterns; 
ii) support ontology learning from text; and iii) 
improve word sense disambiguation through se-
lectional restrictions. Also, we will try different 
similarity score functions for both the clustering 
and the anchor approaches, as those surveyed in 
Corley and Mihalcea (2005). 
CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES 
[multitude#3, PART-OF, group#1] 2.04 10 
(ordinary people, PART-OF, Democratic Revolutionary Party) 
(unlicensed people, PART-OF, underground economy) 
(young people, PART-OF, commission) 
(air mass, PART-OF, cold front) 
[person#1, PART-OF, organization#1] 1.71 43 
(foreign ministers, PART-OF, council) 
(students, PART-OF, orchestra) 
(socialists, PART-OF, Iraqi National Joint Action Committee) 
(players, PART-OF, Metro League) 
[act#2, PART-OF, plan#1] 1.60 16 
(major concessions, PART-OF, new plan) 
(attacks, PART-OF, coordinated terrorist plan) 
(visit, PART-OF, exchange program) 
(survey, PART-OF, project) 
[communication#2, PART-OF, book#1] 1.14 10 
(hints, PART-OF, booklet) 
(soup recipes, PART-OF, book) 
(information, PART-OF, instruction manual) 
(extensive expert analysis, PART-OF, book) 
[compound#2, PART-OF, waste#1] 0.57 3 
(salts, PART-OF, powdery white waste) 
(lime, PART-OF, powdery white waste) 
(resin, PART-OF, waste) 
Table 3. Sample of the highest scoring conceptual instances learned for the part-of relation. For each 
conceptual instance, we report the score(c), the number of instances, and some example instances. 
799
The algorithms described in this paper may be 
applied to ontologize many lexical resources of 
semantic relations, no matter the harvesting algo-
rithm used to mine them. In doing so, we have 
the potential to quickly enrich our ontologies, 
like WordNet, thus reducing the knowledge ac-
quisition bottleneck. It is our hope that we will be 
able to leverage these enriched resources, albeit 
with some noisy additions, to improve perform-
ance on knowledge-rich problems such as ques-
tion answering and textual entailment. 
References 
Agirre, E. and Rigau, G. 1996. Word sense 
disambiguation using conceptual density. In 
Proceedings of COLING-96. pp. 16-22. Copenhagen, 
Danmark. 
Agirre, E.; Ansa, O.; Martinez, D.; and Hovy, E. 2001. 
Enriching WordNet concepts with topic signatures. In 
Proceedings of NAACL Workshop on WordNet and 
Other Lexical Resources: Applications, Extensions 
and Customizations. Pittsburgh, PA. 
Basili, R.; Pazienza, M.T.; and Vindigni, M. 2000. 
Corpus-driven learning of event recognition rules. In 
Proceedings of Workshop on Machine Learning and 
Information Extraction (ECAI-00). 
Corley, C. and Mihalcea, R. 2005. Measuring the 
Semantic Similarity of Texts. In Proceedings of the 
ACL Workshop on Empirical Modelling of Semantic 
Equivalence and Entailment. Ann Arbor, MI. 
Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-
M.; Shaked, T.; Soderland, S.; Weld, D.S.; and Yates, 
A. 2005. Unsupervised named-entity extraction from 
the Web: An experimental study. Artificial 
Intelligence, 165(1): 91-134. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press. 
Gale, W.; Church, K.; and Yarowsky, D. 1992. A 
method for disambiguating word senses in a large 
corpus. Computers and Humanities, 26:415-439. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. 
Learning semantic constraints for the automatic 
discovery of part-whole relations. In Proceedings of 
HLT/NAACL-03. pp. 80-87. Edmonton, Canada. 
Girju, R. 2003. Automatic Detection of Causal Relations 
for Question Answering. In Proceedings of ACL 
Workshop on Multilingual Summarization and 
Question Answering. Sapporo, Japan. 
Harabagiu, S.; Miller, G.; and Moldovan, D. 1999. 
WordNet 2 - A Morphologically and Semantically 
Enhanced Resource. In Proceedings of SIGLEX-99. 
pp.1-8. University of Maryland. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. 
(ed.) The Philosophy of Linguistics. New York: 
Oxford University Press. pp. 26?47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Lin, D. and Pantel, P. 2002. Concept discovery from text. 
In Proceedings of COLING-02. pp. 577-583. Taipei, 
Taiwan. 
Pantel, P. 2005. Inducing Ontological Co-occurrence 
Vectors. In Proceedings of ACL-05. pp. 125-132. Ann 
Arbor, MI. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, 
PA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based 
approach for building semantic lexicons. In 
Proceedings of EMNLP-97. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. 
Scaling web-based acquisition of entailment relations. 
In Proceedings of EMNLP-04. Barcelona, Spain. 
Winston, M.; Chaffin, R.; and Hermann, D. 1987. A 
taxonomy of part-whole relations. Cognitive Science, 
11:417?444. 
CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES 
[change#3, CAUSE, state#4] 1.49 17 
(separation, CAUSE, anxiety) 
(demotion, CAUSE, roster vacancy) 
(budget cuts, CAUSE, enrollment declines) 
(reduced flow, CAUSE, vacuum) 
[act#2, CAUSE, state#3] 0.81 20 
(oil drilling, CAUSE, air pollution) 
(workplace exposure, CAUSE, genetic injury) 
(industrial emissions, CAUSE, air pollution) 
(long recovery, CAUSE, great stress) 
[person#1, CAUSE, act#2] 0.64 12 
(homeowners, CAUSE, water waste) 
(needlelike puncture, CAUSE, physician) 
(group member, CAUSE, controversy) 
(children, CAUSE, property damage) 
[organism#1, CAUSE, disease#1] 0.03 4 
(parasites, CAUSE, pneumonia) 
(virus, CAUSE, influenza) 
(chemical agents, CAUSE, pneumonia) 
(genetic mutation, CAUSE, Dwarfism) 
Table 4. Sample of the highest scoring conceptual instances learned for the causation relation. For 
each conceptual instance, we report score(c) , the number of instances, and some example instances. 
 
800
VERBOCEAN: Mining the Web for Fine-Grained Semantic Verb Relations 
Timothy Chklovski and Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{timc, pantel}@isi.edu 
 
Abstract 
Broad-coverage repositories of semantic relations 
between verbs could benefit many NLP tasks. We 
present a semi-automatic method for extracting 
fine-grained semantic relations between verbs. We 
detect similarity, strength, antonymy, enablement, 
and temporal happens-before relations between 
pairs of strongly associated verbs using lexico-
syntactic patterns over the Web. On a set of 29,165 
strongly associated verb pairs, our extraction algo-
rithm yielded 65.5% accuracy. Analysis of  
error types shows that on the relation strength we 
achieved 75% accuracy. We provide the  
resource, called VERBOCEAN, for download at 
http://semantics.isi.edu/ocean/. 
1 Introduction 
Many NLP tasks, such as question answering, 
summarization, and machine translation could 
benefit from broad-coverage semantic resources 
such as WordNet (Miller 1990) and EVCA (Eng-
lish Verb Classes and Alternations) (Levin 1993). 
These extremely useful resources have very high 
precision entries but have important limitations 
when used in real-world NLP tasks due to their 
limited coverage and prescriptive nature (i.e. they 
do not include semantic relations that are plausible 
but not guaranteed). For example, it may be valu-
able to know that if someone has bought an item, 
they may sell it at a later time. WordNet does not 
include the relation ?X buys Y? happens-before ?X 
sells Y? since it is possible to sell something with-
out having bought it (e.g. having manufactured or 
stolen it). 
Verbs are the primary vehicle for describing 
events and expressing relations between entities. 
Hence, verb semantics could help in many natural 
language processing (NLP) tasks that deal with 
events or relations between entities. For tasks 
which require canonicalization of natural language 
statements or derivation of plausible inferences 
from such statements, a particularly valuable re-
source is one which (i) relates verbs to one another 
and (ii) provides broad coverage of the verbs in the 
target language. 
In this paper, we present an algorithm that semi-
automatically discovers fine-grained verb seman-
tics by querying the Web using simple lexico-
syntactic patterns. The verb relations we discover 
are similarity, strength, antonymy, enablement, and 
temporal relations. Identifying these relations over 
29,165 verb pairs results in a broad-coverage re-
source we call VERBOCEAN. Our approach extends 
previously formulated ones that use surface pat-
terns as indicators of semantic relations between 
nouns (Hearst 1992; Etzioni 2003; Ravichandran 
and Hovy 2002). We extend these approaches in 
two ways: (i) our patterns indicate verb conjuga-
tion to increase their expressiveness and specificity 
and (ii) we use a measure similar to mutual infor-
mation to account for both the frequency of the 
verbs whose semantic relations are being discov-
ered as well as for the frequency of the pattern. 
2 Relevant Work 
In this section, we describe application domains 
that can benefit from a resource of verb semantics. 
We then introduce some existing resources and 
describe previous attempts at mining semantics 
from text. 
2.1 Applications 
Question answering is often approached by can-
onicalizing the question text and the answer text 
into logical forms. This approach is taken, inter 
alia, by a top-performing system (Moldovan et al 
2002). In discussing future work on the system?s 
logical form matching component, Rus (2002 p. 
143) points to incorporating entailment and causa-
tion verb relations to improve the matcher?s per-
formance. In other work, Webber et al (2002) 
have argued that successful question answering 
depends on lexical reasoning, and that lexical rea-
soning in turn requires fine-grained verb semantics 
in addition to troponymy (is-a relations between 
verbs) and antonymy. 
In multi-document summarization, knowing verb 
similarities is useful for sentence compression and 
for determining sentences that have the same 
meaning (Lin 1997). Knowing that a particular 
action happens before another or is enabled by 
another is also useful to determine the order of the 
events (Barzilay et al 2002). For example, to order 
summary sentences properly, it may be useful to 
know that selling something can be preceded by 
either buying, manufacturing, or stealing it. Fur-
thermore, knowing that a particular verb has a 
meaning stronger than another (e.g. rape vs. abuse 
and renovate vs. upgrade) can help a system pick 
the most general sentence. 
In lexical selection of verbs in machine transla-
tion and in work on document classification, prac-
titioners have argued for approaches that depend 
on wide-coverage resources indicating verb simi-
larity and membership of a verb in a certain class. 
In work on translating verbs with many counter-
parts in the target language, Palmer and Wu (1995) 
discuss inherent limitations of approaches which 
do not examine a verb?s class membership, and put 
forth an approach based on verb similarity. In 
document classification, Klavans and Kan (1998) 
demonstrate that document type is correlated with 
the presence of many verbs of a certain EVCA 
class (Levin 1993). In discussing future work, Kla-
vans and Kan point to extending coverage of the 
manually constructed EVCA resource as a way of 
improving the performance of the system. A wide-
coverage repository of verb relations including 
verbs linked by the similarity relation will provide 
a way to automatically extend the existing verb 
classes to cover more of the English lexicon. 
2.2 Existing resources 
Some existing broad-coverage resources on 
verbs have focused on organizing verbs into 
classes or annotating their frames or thematic roles. 
EVCA (English Verb Classes and Alternations) 
(Levin 1993) organizes verbs by similarity and 
participation / nonparticipation in alternation pat-
terns. It contains 3200 verbs classified into 191 
classes. Additional manually constructed resources 
include PropBank (Kingsbury et al 2002), Frame-
Net (Baker et al 1998), VerbNet (Kipper et al 
2000), and the resource on verb selectional restric-
tions developed by Gomez (2001). 
Our approach differs from the above in its focus. 
We relate verbs to each other rather than organize 
them into classes or identify their frames or the-
matic roles. WordNet does provide relations be-
tween verbs, but at a coarser level. We provide 
finer-grained relations such as strength, enable-
ment and temporal information. Also, in contrast 
with WordNet, we cover more than the prescriptive 
cases. 
2.3 Mining semantics from text 
Previous web mining work has rarely addressed 
extracting many different semantic relations from 
Web-sized corpus. Most work on extracting se-
mantic information from large corpora has largely 
focused on the extraction of is-a relations between 
nouns. Hearst (1992) was the first followed by 
recent larger-scale and more fully automated ef-
forts (Pantel and Ravichandran 2004; Etzioni et al 
2004; Ravichandran and Hovy 2002). Recently, 
Moldovan et al (2004) present a learning algo-
rithm to detect 35 fine-grained noun phrase rela-
tions. 
Turney (2001) studied word relatedness and 
synonym extraction, while Lin et al (2003) present 
an algorithm that queries the Web using lexical 
patterns for distinguishing noun synonymy and 
antonymy. Our approach addresses verbs and pro-
vides for a richer and finer-grained set of seman-
tics.  Reliability of estimating bigram counts on the 
web via search engines has been investigated by 
Keller and Lapata (2003). 
Semantic networks have also been extracted 
from dictionaries and other machine-readable re-
sources. MindNet (Richardson et al 1998) extracts 
a collection of triples of the type ?ducks have 
wings? and ?duck capable-of flying?. This re-
source, however, does not relate verbs to each 
other or provide verb semantics. 
3 Semantic relations among verbs 
In this section, we introduce and motivate the 
specific relations that we extract. Whilst the natural 
language literature is rich in theories of semantics 
(Barwise and Perry 1985; Schank and Abelson 
1977), large-coverage manually created semantic 
resources typically only organize verbs into a flat 
or shallow hierarchy of classes (such as those de-
scribed in Section 2.2). WordNet identifies synon-
ymy, antonymy, troponymy, and cause. As sum-
marized in Figure 1, Fellbaum (1998) discusses a 
finer-grained analysis of entailment, while the 
WordNet database does not distinguish between, 
e.g., backward presupposition (forget :: know, 
where know must have happened before forget) 
from proper temporal inclusion (walk :: step). In 
formulating our set of relations, we have relied on 
the finer-grained analysis, explicitly breaking out 
the temporal precedence between entities. 
In selecting the relations to identify, we aimed at 
both covering the relations described in WordNet 
and covering the relations present in our collection 
Figure 1. Fellbaum?s (1998) entailment hierarchy.
+Temporal Inclusion
Entailment
-Temporal Inclusion
+Troponymy
 (coextensiveness)
 march-walk
-Troponymy
 (proper inclusion)
 walk-step
Backward
Presupposition
forget-know
Cause
show-see
of strongly associated verb pairs. We relied on the 
strongly associated verb pairs, described in Section 
4.4, for computational efficiency. The relations we 
identify were experimentally found to cover 99 out 
of 100 randomly selected verb pairs. 
Our algorithm identifies six semantic relations 
between verbs. These are summarized in Table 1 
along with their closest corresponding WordNet 
category and the symmetry of the relation (whether 
V1 rel V2 is equivalent to V2 rel V1). 
Similarity. As Fellbaum (1998) and the tradition 
of organizing verbs into similarity classes indicate, 
verbs do not neatly fit into a unified is-a (tro-
ponymy) hierarchy. Rather, verbs are often similar 
or related. Similarity between action verbs, for 
example, can arise when they differ in connota-
tions about manner or degree of action. Examples 
extracted by our system include maximize :: en-
hance, produce :: create, reduce :: restrict. 
Strength. When two verbs are similar, one may 
denote a more intense, thorough, comprehensive or 
absolute action. In the case of change-of-state 
verbs, one may denote a more complete change. 
We identify this as the strength relation. Sample 
verb pairs extracted by our system, in the order 
weak to strong, are: taint :: poison, permit :: au-
thorize, surprise :: startle, startle :: shock. Some 
instances of strength sometimes map to WordNet?s 
troponymy relation. 
Strength, a subclass of similarity, has not been 
identified in broad-coverage networks of verbs, but 
may be of particular use in natural language gen-
eration and summarization applications. 
Antonymy. Also known as semantic opposition, 
antonymy between verbs has several distinct sub-
types. As discussed by Fellbaum (1998), it can 
arise from switching thematic roles associated with 
the verb (as in buy :: sell, lend :: borrow). There is 
also antonymy between stative verbs (live :: die, 
differ :: equal) and antonymy between sibling 
verbs which share a parent (walk :: run) or an en-
tailed verb (fail :: succeed both entail try). 
Antonymy also systematically interacts with the 
happens-before relation in the case of restitutive 
opposition (Cruse 1986). This subtype is exempli-
fied by damage :: repair, wrap :: unwrap. In terms 
of the relations we recognize, it can be stated that 
restitutive-opposition(V1, V2) = happens-
before(V1, V2), and antonym(V1, V2). Examples of 
antonymy extracted by our system include: assem-
ble :: dismantle; ban :: allow; regard :: condemn, 
roast :: fry. 
Enablement. This relation holds between two 
verbs V1 and V2 when the pair can be glossed as V1 
is accomplished by V2. Enablement is classified as 
a type of causal relation by Barker and Szpakowicz 
(1995). Examples of enablement extracted by our 
system include: assess :: review and accomplish :: 
complete. 
Happens-before. This relation indicates that the 
two verbs refer to two temporally disjoint intervals 
or instances. WordNet?s cause relation, between a 
causative and a resultative verb (as in buy :: own), 
would be tagged as instances of happens-before by 
our system. Examples of the happens-before rela-
tion identified by our system include marry :: di-
vorce, detain :: prosecute, enroll :: graduate, 
schedule :: reschedule, tie :: untie. 
4 Approach 
We discover the semantic relations described 
above by querying the Web with Google for 
lexico-syntactic patterns indicative of each rela-
tion. Our approach has two stages. First, we iden-
tify pairs of highly associated verbs co-occurring 
on the Web with sufficient frequency using previ-
ous work by Lin and Pantel (2001), as described in 
Section 4.4. Next, for each verb pair, we tested 
lexico-syntactic patterns, calculating a score for 
each possible semantic relation as described in 
Section 4.2. Finally, as described in Section 4.3, 
we compare the strengths of the individual seman-
tic relations and, preferring the most specific and 
then strongest relations, output a consistent set as 
the final output.  As a guide to consistency, we use 
a simple theory of semantics indicating which se-
mantic relations are subtypes of other ones, and 
which are compatible and which are mutually ex-
clusive. 
4.1 Lexico-syntactic patterns 
The lexico-syntactic patterns were manually se-
lected by examining pairs of verbs in known se-
mantic relations. They were refined to decrease 
capturing wrong parts of speech or incorrect se-
mantic relations. We used 50 verb pairs and the 
overall process took about 25 hours. 
We use a total of 35 patterns, which are listed in 
Table 2 along with the estimated frequency of hits. 
Table 1. Semantic relations identified in VERBOCEAN.  Sib-
lings in the WordNet column refers to terms with the same 
troponymic parent, e.g. swim and fly. 
SEMANTIC 
RELATION 
EXAMPLE 
Alignment with 
WordNet 
Symmetric
similarity transform :: integrate synonyms or siblings Y 
strength wound :: kill synonyms or siblings N 
antonymy open :: close antonymy Y 
enablement fight :: win cause N 
happens-
before 
buy :: sell; 
marry :: divorce 
cause 
entailment, no 
temporal inclusion 
N 
 
Note that our patterns specify the tense of the verbs 
they accept. When instantiating these patterns, we 
conjugate as needed. For example, ?both Xed and 
Yed? instantiates on sing and dance as ?both sung 
and danced?. 
4.2 Testing for a semantic relation 
In this section, we describe how the presence of 
a semantic relation is detected. We test the rela-
tions with patterns exemplified in Table 2. We 
adopt an approach inspired by mutual information 
to measure the strength of association, denoted 
Sp(V1, V2),  between three entities: a verb pair V1 
and V2 and a lexico-syntactic pattern p: 
 
)()()(
),,(),(
21
21
21 VPVPpP
VpVPVVS p ??=
 
The probabilities in the denominator are difficult 
to calculate directly from search engine results. For 
a given lexico-syntactic pattern, we need to esti-
mate the frequency of the pattern instantiated with 
appropriately conjugated verbs. For verbs, we need 
to estimate the frequency of the verbs, but avoid 
counting other parts-of-speech (e.g. chair as a 
noun or painted as an adjective). Another issue is 
that some relations are symmetric (similarity and 
antonymy), while others are not (strength, enable-
ment, happens-before). For symmetric relations 
only, the verbs can fill the lexico-syntactic pattern 
in either order. To address these issues, we esti-
mate Sp(V1,V2) using: 
 
N
CVtohits
N
CVtohits
N
phits
N
VpVhits
VVS
vvest
P ????
? )"(")"(")(
),,(
),(
21
21
21
 
for asymmetric relations and 
N
CVtohits
N
CVtohits
N
phits
N
VpVhits
N
VpVhits
VVS
vvest
P ????
+
? )"(")"(")(*2
),,(),,(
),(
21
1221
21
 
for symmetric relations. 
Here, hits(S) denotes the number of documents 
containing the string S, as returned by Google. N is 
the number of words indexed by the search engine 
(N ? 7.2 ? 1011), Cv is a correction factor to obtain 
the frequency of the verb V in all tenses from the 
frequency of the pattern ?to V?. Based on several 
verbs, we have estimated Cv = 8.5. Because pattern 
counts, when instantiated with verbs, could not be 
estimated directly, we have computed the frequen-
cies of the patterns in a part-of-speech tagged 
500M word corpus and used it to estimate the ex-
pected number of hits hitsest(p) for each pattern.  
We estimated the N with a similar method. 
We say that the semantic relation Sp indicated by 
lexico-syntactic pattern p is present between V1 
and V2 if 
 Sp(V1,V2) > C1 
As a result of tuning the system on a tuning set of 
50 verb pairs, C1 = 8.5. 
Additional test for asymmetric relations.  For 
the asymmetric relations, we require not only that 
),( 21 VVSP exceed a certain threshold, but that 
there be strong asymmetry of the relation: 
 
2
12
21
12
21
),,(
),,(
),(
),(
C
VpVhits
VpVhits
VVS
VVS
p
p >=  
From the tuning set, C2 = 5. 
4.3 Pruning identified semantic relations 
Given a pair of semantic relations from the set 
we identify, one of three cases can arise: (i) one 
Table 2. Semantic relations and the 35 surface patterns used 
to identify them. Total number of patterns for that relation is 
shown in parentheses. In patterns, ?*? matches any single 
word.  Punctuation does not count as words by the search 
engine used (Google). 
SEMANTIC 
RELATION 
 Surface 
Patterns 
Hitsest for 
patterns
narrow  
similarity (2)* 
X ie Y 
Xed ie Yed 219,480
broad  
similarity (2)* 
Xed and Yed 
to X and Y 154,518,326
strength (8) 
X even Y 
Xed even Yed 
X and even Y 
Xed and even Yed 
Y or at least X 
Yed or at least Xed 
not only Xed but Yed 
not just Xed but Yed  
1,016,905
enablement (4) 
Xed * by Ying the 
Xed * by Ying or 
to X * by Ying the 
to X * by Ying or 
2,348,392
antonymy (7) 
either X or Y 
either Xs or Ys 
either Xed or Yed 
either Xing or Ying 
whether to X or Y 
Xed * but Yed 
to X * but Y 
18,040,916
happens-before 
(12) 
to X and then Y 
to X * and then Y 
Xed and then Yed 
Xed * and then Yed 
to X and later Y 
Xed and later Yed 
to X and subsequently Y 
Xed and subsequently Yed 
to X and eventually Y 
Xed and eventually Yed 
8,288,871
*narrow- and broad- similarity overlap in their coverage 
and are treated as a single category, similarity, when 
postprocessed. Narrow similarity tests for rare patterns 
and hitsest for it had to be approximated rather than 
estimated from the smaller corpus. 
relation is more specific (strength is more specific 
than similarity, enablement is more specific than 
happens-before), (ii) the relations are compatible 
(antonymy and happens-before), where presence of 
one does not imply or rule out presence of the 
other, and (iii) the relations are incompatible (simi-
larity and antonymy). 
It is not uncommon for our algorithm to identify 
presence of several relations, with different 
strengths. To produce the most likely output, we 
use semantics of compatibility of the relations to 
output the most likely one(s).  The rules are as 
follows: 
If the frequency was too low (less than 10 on the 
pattern ?X * Y? OR ?Y * X? OR ?X * * Y? OR ?Y 
* * X?), output that the statements are unrelated 
and stop. 
If happens-before is detected, output presence of 
happens-before (additional relation may still be 
output, if detected). 
If happens-before is not detected, ignore detec-
tion of enablement (because enablement is more 
specific than happens-before, but is sometimes 
falsely detected in the absence of happens-before). 
If strength is detected, score of similarity is ig-
nored (because strength is more specific than simi-
larity). 
Of the relations strength, similarity, opposition 
and enablement which were detected (and not ig-
nored), output the one with highest Sp. 
If nothing has been output to this point, output 
unrelated. 
4.4 Extracting highly associated verb pairs 
To exhaustively test the more than 64 million 
unordered verb pairs for WordNet?s more than 
11,000 verbs would be computationally intractable. 
Instead, we use a set of highly associated verb 
pairs output by a paraphrasing algorithm called 
DIRT (Lin and Pantel 2001).  Since we are able to 
test up to 4000 verb pairs per day on a single ma-
chine (we issue at most 40 queries per test and 
each query takes approximately 0.5 seconds), we 
are able to test several dozen associated verbs for 
each verb in WordNet in a matter of weeks. 
Lin and Pantel (2001) describe an algorithm 
called DIRT (Discovery of Inference Rules from 
Text) that automatically learns paraphrase expres-
sions from text.  It is a generalization of previous 
algorithms that use the distributional hypothesis 
(Harris 1985) for finding similar words.  Instead of 
applying the hypothesis to words, Lin and Pantel 
applied it to paths in dependency trees.  Essen-
tially, if two paths tend to link the same sets of 
words, they hypothesized that the meanings of the 
corresponding paths are similar. It is from paths of 
the form subject-verb-object that we extract our set 
of associated verb pairs. Hence, this paper is con-
cerned only with relations between transitive 
verbs. 
A path, extracted from a parse tree, is an expres-
sion that represents a binary relation between two 
nouns. A set of paraphrases was generated for each 
pair of associated paths.  For example, using a 
1.5GB newspaper corpus, here are the 20 most 
associated paths to ?X solves Y? generated by 
DIRT: 
Y is solved by X, X resolves Y, X finds 
a solution to Y, X tries to solve Y, X 
deals with Y, Y is resolved by X, X ad-
dresses Y, X seeks a solution to Y, X 
does something about Y, X solution to 
Y, Y is resolved in X, Y is solved 
through X, X rectifies Y, X copes with 
Y, X overcomes Y, X eases Y, X tackles 
Y, X alleviates Y, X corrects Y, X is a 
solution to Y, X makes Y worse, X irons 
out Y 
This list of associated paths looks tantalizingly 
close to the kind of axioms that would prove useful 
in an inference system. However, DIRT only out-
puts pairs of paths that have some semantic rela-
tion. We used these as our set to extract finer-
grained relations. 
5 Experimental results 
In this section, we empirically evaluate the accu-
racy of VERBOCEAN1. 
5.1 Experimental setup 
We studied 29,165 pairs of verbs. Applying 
DIRT to a 1.5GB newspaper corpus2, we extracted 
4000 paths that consisted of single verbs in the 
relation subject-verb-object (i.e. paths of the form 
?X verb Y?) whose verbs occurred in at least 150 
documents on the Web. For example, from the 20 
most associated paths to ?X solves Y? shown in 
Section 4.4, the following verb pairs were ex-
tracted: 
solves :: resolves 
solves :: addresses 
solves :: rectifies 
solves :: overcomes 
solves :: eases 
solves :: tackles 
solves :: corrects 
5.2 Accuracy 
We classified each verb pair according to the 
semantic relations described in Section 2. If the 
system does not identify any semantic relation for 
a verb pair, then the system tags the pair as having 
                                                     
1 VERBOCEAN is available for download at 
http://semantics.isi.edu/ocean/. 
2 The 1.5GB corpus consists of San Jose Mercury, 
Wall Street Journal and AP Newswire articles from the 
TREC-9 collection. 
no relation. To evaluate the accuracy of the sys-
tem, we randomly sampled 100 of these verb pairs, 
and presented the classifications to two human 
judges. The adjudicators were asked to judge 
whether or not the system classification was ac-
ceptable (i.e. whether or not the relations output by 
the system were correct). Since the semantic rela-
tions are not disjoint (e.g. mop is both stronger 
than and similar to sweep), multiple relations may 
be appropriately acceptable for a given verb pair. 
The judges were also asked to identify their pre-
ferred semantic relations (i.e. those relations which 
seem most plausible). Table 3 shows five randomly 
selected pairs along with the judges? responses. 
The Appendix shows sample relationships discov-
ered by the system. 
Table 4 shows the accuracy of the system. The 
baseline system consists of labeling each pair with 
the most common semantic relation, similarity, 
which occurs 33 times. The Tags Correct column 
represents the percentage of verb pairs whose sys-
tem output relations were deemed correct. The 
Preferred Tags Correct column gives the percent-
age of verb pairs whose system output relations 
matched exactly the human?s preferred relations. 
The Kappa statistic (Siegel and Castellan 1988) for 
the task of judging system tags as correct and in-
correct is ? = 0.78 whereas the task of identifying 
the preferred semantic relation has ? = 0.72. For 
the latter task, the two judges agreed on 73 of the 
100 semantic relations. 73% gives an idea of an 
upper bound for humans on this task. On these 73 
relations, the system achieved a higher accuracy of 
70.0%. The system is allowed to output the hap-
pens-before relation in combination with other 
relations. On the 17 happens-before relations out-
put by the system, 67.6% were judged correct. 
Ignoring the happens-before relations, we achieved 
a Tags Correct precision of 68%. 
Table 5 shows the accuracy of the system on 
each of the relations. The stronger-than relation is 
a subset of the similarity relation. Considering a 
coarser extraction where stronger-than relations 
are merged with similarity, the task of judging 
system tags and the task of identifying the pre-
ferred semantic relation both jump to 68.2% accu-
racy. Also, the overall accuracy of the system 
climbs to 68.5%. 
As described in Section 2, WordNet contains 
verb semantic relations. A significant percentage 
of our discovered relations are not covered by 
WordNet?s coarser classifications. Of the 40 verb 
pairs whose system relation was tagged as correct 
by both judges in our accuracy experiments and 
whose tag was not ?no relation?, only 22.5% of 
them existed in a WordNet relation. 
5.3 Discussion 
The experience of extracting these semantic rela-
tions has clarified certain important challenges. 
While relying on a search engine allows us to 
query a corpus of nearly a trillion words, some 
issues arise: (i) the number of instances has to be 
approximated by the number of hits (documents); 
(ii) the number of hits for the same query may 
fluctuate over time; and (iii) some needed counts 
are not directly available. We addressed the latter 
issue by approximating these counts using a 
smaller corpus. 
Table 3. Five randomly selected pairs along with the system tag (in bold) and the judges? responses. 
 CORRECT PREFERRED SEMANTIC RELATION 
PAIRS WITH SYSTEM TAG (IN BOLD) JUDGE 1 JUDGE 2 JUDGE 1 JUDGE 2 
X absolve Y is similar to X vindicate Y Yes Yes is similar to is similar to 
X bottom Y has no relation with X abate Y Yes Yes has no relation with has no relation with 
X outrage Y happens-after / is stronger than X shock Y Yes Yes happens-before / is 
stronger than 
happens-before/ is stronger 
than 
X pool Y has no relation with X increase Y Yes No has no relation with can result in 
X insure Y is similar to X expedite Y No No has no relation with has no relation with 
 
Table 4. Accuracy of system-discovered relations. 
 ACCURACY 
 Tags 
Correct 
Preferred Tags 
Correct 
Baseline 
Correct 
Judge 1 66% 54% 24% 
Judge 2 65% 52% 20% 
Average 65.5% 53% 22% 
 
Table 5. Accuracy of each semantic relation. 
SEMANTIC 
RELATION 
SYSTEM 
TAGS 
Tags 
Correct 
Preferred Tags 
Correct 
similarity 41 63.4% 40.2% 
strength 14 75.0% 75.0% 
antonymy 8 50.0% 43.8% 
enablement 2 100% 100% 
no relation 35 72.9% 72.9% 
happens before 17 67.6% 55.9% 
 
We do not detect entailment with lexico-
syntactic patterns.  In fact, we propose that whether 
the entailment relation holds between V1 and V2 
depends on the absence of another verb V1' in the 
same relationship with V2. For example, given the 
relation marry happens-before divorce, we can 
conclude that divorce entails marry. But, given the 
relation buy happens-before sell, we cannot con-
clude entailment since manufacture can also hap-
pen before sell. This also applies to the enablement 
and strength relations. 
Corpus-based methods, including ours, hold the 
promise of wide coverage but are weak on dis-
criminating senses. While we hope that applica-
tions will benefit from this resource as is, an inter-
esting next step would be to augment it with sense 
information. 
6 Future work 
There are several ways to improve the accuracy 
of the current algorithm and to detect relations 
between low frequency verb pairs. One avenue 
would be to automatically learn or manually craft 
more patterns and to extend the pattern vocabulary 
(when developing the system, we have noticed that 
different registers and verb types require different 
patterns).  Another possibility would be to use 
more relaxed patterns when the part of speech 
confusion is not likely (e.g. ?eat? is a common 
verb which does not have a noun sense, and pat-
terns need not protect against noun senses when 
testing such verbs). 
Our approach can potentially be extended to 
multiword paths.  DIRT actually provides two 
orders of magnitude more relations than the 29,165 
single verb relations (subject-verb-object) we ex-
tracted. On the same 1GB corpus described in Sec-
tion 5.1, DIRT extracted over 200K paths and 6M 
unique paraphrases. These provide an opportunity 
to create a much larger corpus of semantic rela-
tions, or to construct smaller, in-depth resources 
for selected subdomains.  For example, we could 
extract that take a trip to is similar to travel to, and 
that board a plane happens before deplane. 
If the entire database is viewed as a graph, we 
currently leverage and enforce only local consis-
tency.  It would be useful to enforce global consis-
tency, e.g. V1 stronger-than V2, and V2 stronger-
than V3 indicates that V1 stronger-than V3, which 
may be leveraged to identify additional relations or 
inconsistent relations (e.g. V3 stronger-than V1). 
Finally, as discussed in Section 5.3, entailment 
relations may be derivable by processing the com-
plete graph of the identified semantic relation. 
7 Conclusions 
We have demonstrated that certain fine-grained 
semantic relations between verbs are present on the 
Web, and are extractable with a simple pattern-
based approach. In addition to discovering rela-
tions identified in WordNet, such as opposition and 
enablement, we obtain strong results on strength 
relations (for which no wide-coverage resource is 
available). On a set of 29,165 associated verb 
pairs, experimental results show an accuracy of 
65.5% in assigning similarity, strength, antonymy, 
enablement, and happens-before. 
Further work may refine extraction methods and 
further process the mined semantics to derive other 
relations such as entailment. 
We hope to open the way to inferring implied, 
but not stated assertions and to benefit applications 
such as question answering, information retrieval, 
and summarization. 
Acknowledgments 
The authors wish to thank the reviewers for their 
helpful comments and Google Inc. for supporting 
high volume querying of their index. This research 
was partly supported by NSF grant #EIA-0205111. 
References  
Baker, C.; Fillmore, C.; and Lowe, J. 1998. The 
Berkeley FrameNet project. In Proceedings of 
COLING-ACL. Montreal, Canada. 
Barker, K.; and Szpakowicz, S. 1995. Interactive 
Semantic Analysis of Clause-Level 
Relationships. In Proceedings of PACLING `95. 
Brisbane.  
Barwise, J. and Perry, J. 1985. Semantic innocence 
and uncompromising situations. In: Martinich, 
A. P. (ed.) The Philosophy of Language. New 
York: Oxford University Press. pp. 401?413. 
Barzilay, R.; Elhadad, N.; and McKeown, K. 2002. 
Inferring strategies for sentence ordering in 
multidocument summarization. JAIR, 17:35?55. 
Cruse, D. 1992 Antonymy Revisited: Some 
Thoughts on the Relationship between Words 
and Concepts, in A. Lehrer and E.V. Kittay 
(eds.), Frames, Fields, and Contrasts, Hillsdale, 
NJ, Lawrence Erlbaum associates, pp. 289-306. 
Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; 
Popescu, A.; Shaked, T.; Soderland, S.; Weld, 
D.; and Yates, A. 2004. Web-scale information 
extraction in KnowItAll. To appear in WWW-
2004. 
Fellbaum, C. 1998. Semantic network of English 
verbs. In Fellbaum, (ed). WordNet: An 
Electronic Lexical Database, MIT Press. 
Gomez, F. 2001. An Algorithm for Aspects of 
Semantic Interpretation Using an Enhanced 
WordNet. In NAACL-2001, CMU, Pittsburgh.  
Harris, Z. 1985. Distributional Structure. In: Katz, 
J. J. (ed.), The Philosophy of Linguistics. New 
York: Oxford University Press. pp. 26?47. 
Hearst, M. 1992. Automatic acquisition of 
hyponyms from large text corpora. In COLING-
92. pp. 539?545. Nantes, France. 
Keller, F.and Lapata, M. 2003. Using the Web to 
Obtain Frequencies for Unseen Bigrams. 
Computational Linguistics 29:3.  
Kingsbury, P; Palmer, M.; and Marcus, M. 2002. 
Adding semantic annotation to the Penn 
TreeBank. In Proceedings of HLT-2002. San 
Diego, California. 
Kipper, K.; Dang, H.; and Palmer, M. 2000. Class-
based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000. Austin, TX. 
Klavans, J. and Kan, M. 1998. Document classi-
fication: Role of verb in document analysis. In 
Proceedings COLING-ACL '98. Montreal, 
Canada. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
University of Chicago Press, Chicago, IL. 
Lin, C-Y. 1997. Robust Automated Topic 
Identification. Ph.D. Thesis. University of 
Southern California. 
Lin, D. and Pantel, P. 2001. Discovery of inference 
rules for question answering. Natural Language 
Engineering, 7(4):343?360. 
Lin, D.; Zhao, S.; Qin, L.; and Zhou, M. 2003. 
Identifying synonyms among distributionally 
similar words. In Proceedings of IJCAI-03. 
pp.1492?1493. Acapulco, Mexico. 
Miller, G. 1990. WordNet: An online lexical 
database. International Journal of Lexicography, 
3(4). 
Moldovan, D.; Badulescu, A.; Tatu, M.; Antohe, 
D.; and Girju, R. 2004. Models for the semantic 
classification of noun phrases. To appear in 
Proceedings of HLT/NAACL-2004 Workshop on 
Computational Lexical Semantics. 
Moldovan, D.; Harabagiu, S.; Girju, R.; 
Morarescu, P.; Lacatusu, F.; Novischi, A.; 
Badulescu, A.; and Bolohan, O. 2002. LCC tools 
for question answering. In Notebook of the 
Eleventh Text REtrieval Conference (TREC-
2002). pp. 144?154. 
Palmer, M., Wu, Z. 1995. Verb Semantics for 
English-Chinese Translation Machine 
Translation, 9(4). 
Pantel, P. and Ravichandran, D. 2004. 
Automatically labeling semantic classes. To 
appear in Proceedings of HLT/NAACL-2004. 
Boston, MA. 
Ravichandran, D. and Hovy, E., 2002.  Learning 
surface text patterns for a question answering 
system.  In Proceedings of ACL-02. 
Philadelphia, PA. 
Richardson, S.; Dolan, W.; and Vanderwende, L. 
1998. MindNet: acquiring and structuring 
semantic information from text. In Proceedings 
of COLING '98. 
Rus, V. 2002. Logic Forms for WordNet Glosses. 
Ph.D. Thesis. Southern Methodist University. 
Schank, R. and Abelson, R. 1977. Scripts, Plans, 
Goals and Understanding: An Inquiry into 
Human Knowledge Structures. Lawrence 
Erlbaum Associates. 
Siegel, S. and Castellan Jr., N. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill. 
Turney, P. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. In Proceedings 
ECML-2001. Freiburg, Germany. 
Webber, B.; Gardent, C.; and Bos, J. 2002. 
Position statement: Inference in question 
answering. In Proceedings of LREC-2002. Las 
Palmas, Spain. 
Appendix. Sample relations extracted by our system. 
SEMANTIC 
RELATION EXAMPLES 
SEMANTIC 
RELATION EXAMPLES 
SEMANTIC 
RELATION EXAMPLES 
similarity 
maximize :: enhance 
produce :: create 
reduce :: restrict 
enablement 
assess :: review 
accomplish :: complete 
double-click :: click 
happens 
before 
detain :: prosecute 
enroll :: graduate 
schedule :: reschedule 
strength 
permit :: authorize 
surprise :: startle 
startle :: shock 
antonymy 
assemble :: dismantle 
regard :: condemn 
roast :: fry 
 
 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423?430,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Assessing Review Helpfulness 
 
Soo-Min Kim?, Patrick Pantel?, Tim Chklovski?, Marco Pennacchiotti? 
?Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{skim,pantel,timc}@isi.edu 
?ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
 
 
 
Abstract 
User-supplied reviews are widely and 
increasingly used to enhance e-
commerce and other websites. Because 
reviews can be numerous and varying in 
quality, it is important to assess how 
helpful each review is. While review 
helpfulness is currently assessed manu-
ally, in this paper we consider the task 
of automatically assessing it. Experi-
ments using SVM regression on a vari-
ety of features over Amazon.com 
product reviews show promising results, 
with rank correlations of up to 0.66. We 
found that the most useful features in-
clude the length of the review, its uni-
grams, and its product rating. 
1 Introduction 
Unbiased user-supplied reviews are solicited 
ubiquitously by online retailers like Ama-
zon.com, Overstock.com, Apple.com and Epin-
ions.com, movie sites like imdb.com, traveling 
sites like citysearch.com, open source software 
distributors like cpanratings.perl.org, and count-
less others. Because reviews can be numerous 
and varying in quality, it is important to rank 
them to enhance customer experience. 
In contrast with ranking search results, assess-
ing relevance when ranking reviews is of little 
importance because reviews are directly associ-
ated with the relevant product or service. Instead, 
a key challenge when ranking reviews is to de-
termine which reviews the customers will find 
helpful. 
Most websites currently rank reviews by their 
recency or product rating (e.g., number of stars 
in Amazon.com reviews). Recently, more sophis-
ticated ranking schemes measure reviews by their 
helpfulness, which is typically estimated by hav-
ing users manually assess it. For example, on 
Amazon.com, an interface allows customers to 
vote whether a particular review is helpful or not. 
Unfortunately, newly written reviews and re-
views with few votes cannot be ranked as several 
assessments are required in order to properly es-
timate helpfulness. For example, for all MP3 
player products on Amazon.com, 38% of the 
20,919 reviews received three or fewer helpful-
ness votes. Another problem is that low-traffic 
items may never gather enough votes. Among the 
MP3 player reviews that were authored at least 
three months ago on Amazon.com, still only 31% 
had three or fewer helpfulness votes. 
It would be useful to assess review helpfulness 
automatically, as soon as the review is written. 
This would accelerate determining a review?s 
ranking and allow a website to provide rapid 
feedback to review authors. 
In this paper, we investigate the task of auto-
matically predicting review helpfulness using a 
machine learning approach. Our main contribu-
tions are: 
? A system for automatically ranking reviews 
according to helpfulness; using state of the art 
SVM regression, we empirically evaluate our 
system on a real world dataset collected from 
Amazon.com on the task of reconstructing the 
helpfulness ranking; and 
? An analysis of different classes of features 
most important to capture review helpful-
ness; including structural (e.g., html tags, 
punctuation, review length), lexical (e.g., n-
grams), syntactic (e.g., percentage of verbs and 
nouns), semantic (e.g., product feature men-
tions), and meta-data (e.g., star rating). 
2 Relevant Work 
The task of automatically assessing product re-
view helpfulness is related to these broader areas 
423
of research: automatic analysis of product re-
views, opinion and sentiment analysis, and text 
classification. 
In the thriving area of research on automatic 
analysis and processing of product reviews (Hu 
and Liu 2004; Turney 2002; Pang and Lee 2005), 
little attention has been paid to the important task 
studied here ? assessing review helpfulness. Pang 
and Lee (2005) have studied prediction of prod-
uct ratings, which may be particularly relevant 
due to the correlation we find between product 
rating and the helpfulness of the review (dis-
cussed in Section 5). However, a user?s overall 
rating for the product is often already available. 
Helpfulness, on the other hand, is valuable to 
assess because it is not explicitly known in cur-
rent approaches until many users vote on the 
helpfulness of a review.  
In opinion and sentiment analysis, the focus is 
on distinguishing between statements of fact vs. 
opinion, and on detecting the polarity of senti-
ments being expressed. Many researchers have 
worked in various facets of opinion analysis. 
Pang et al (2002) and Turney (2002) classified 
sentiment polarity of reviews at the document 
level.  Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features.  
Riloff and Wiebe (2003) extracted subjective 
expressions from sentences using a bootstrapping 
pattern learning process. Yu and Hatzivassi-
loglou (2003) identified the polarity of opinion 
sentences using semantically oriented words. 
These techniques were applied and examined in 
different domains, such as customer reviews (Hu 
and Liu 2004) and news articles (TREC novelty 
track 2003 and 2004).  
In text classification, systems typically use 
bag-of-words models, although there is some 
evidence of benefits when introducing relevant 
semantic knowledge (Gabrilovich and Mark-
ovitch, 2005). In this paper, we explore the use of 
some semantic features for review helpfulness 
ranking. Another potential relevant classification 
task is academic and commercial efforts on de-
tecting email spam messages1, which aim to cap-
ture a much broader notion of helpfulness. For an 
SVM-based approach, see (Drucker et al 1999).  
Finally, a related area is work on automatic es-
say scoring, which seeks to rate the quality of an 
essay (Attali and Burstein 2006; Burstein et al 
2004). The task is important for reducing the 
human effort required in scoring large numbers 
                                                     
1 See http://www.ceas.cc/, http://spamconference.org/  
of student essays regularly written for standard 
tests such as the GRE. The exact scoring ap-
proaches developed in commercial systems are 
often not disclosed. However, more recent work 
on one of the major systems, e-rater 2.0, has fo-
cused on systematizing and simplifying the set of 
features used (Attali and Burstein 2006). Our 
choice of features to test was partially influenced 
by the features discussed by Attali and Burstein. 
At the same time, due to differences in the tasks, 
we did not use features aimed at assessing essay 
structure such as discourse structure analysis fea-
tures. Our observations suggest that even helpful 
reviews vary widely in their discourse structure. 
We present the features which we have used be-
low, in Section 3.2. 
3 Modeling Review Helpfulness 
In this section, we formally define the learning 
task and we investigate several features for as-
sessing review helpfulness. 
3.1 Task Definition 
Formally, given a set of reviews R for a particu-
lar product, our task is to rank the reviews ac-
cording to their helpfulness. We define a review 
helpfulness function, h, as: 
 ( ) ( )( ) ( )rratingrrating
rrating
Rrh
?+
+
+=?  (1) 
where rating+(r) is the number of people that will 
find a review helpful and rating-(r) is the number 
of people that will find the review unhelpful. For 
evaluation, we resort to estimates of h from man-
ual review assessments on websites like Ama-
zon.com, as described in Section 4. 
3.2 Features 
One aim of this paper is to investigate how well 
different classes of features capture the helpful-
ness of a review. We experimented with various 
features organized in five classes: Structural, 
Lexical, Syntactic, Semantic, and Meta-data. Be-
low we describe each feature class in turn. 
Structural Features 
Structural features are observations of the docu-
ment structure and formatting. Properties such as 
review length and average sentence length are 
hypothesized to relate structural complexity to 
helpfulness. Also, HTML formatting tags could 
help in making a review more readable, and con-
sequently more helpful. We experimented with 
the following features: 
424
? Length (LEN): The total number of tokens in a 
syntactic analysis2 of the review. 
? Sentential (SEN): Observations of the sen-
tences, including the number of sentences, the 
average sentence length, the percentage of 
question sentences, and the number of excla-
mation marks. 
? HTML (HTM): Two features for the number of 
bold tags <b> and line breaks <br>. 
Lexical Features 
Lexical features capture the words observed in 
the reviews. We experimented with two sets of 
features: 
? Unigram (UGR): The tf-idf statistic of each 
word occurring in a review. 
? Bigram (BGR): The tf-idf statistic of each bi-
gram occurring in a review. 
For both unigrams and bigrams, we used lemma-
tized words from a syntactic analysis of the re-
views and computed the tf-idf statistic (Salton 
and McGill 1983) using the following formula: 
 ( )
N
idftf
idftf
log?=  
where N is the number of tokens in the review. 
Syntactic Features 
Syntactic features aim to capture the linguistic 
properties of the review. We grouped them into 
the following feature set: 
? Syntax (SYN): Includes the percentage of 
parsed tokens that are open-class (i.e., nouns, 
verbs, adjectives and adverbs), the percentage 
of tokens that are nouns, the percentage of to-
kens that are verbs, the percentage of tokens 
that are verbs conjugated in the first person, 
and the percentage of tokens that are adjectives 
or adverbs. 
Semantic Features 
Most online reviews are fairly short; their spar-
sity suggests that bigram features will not per-
form well (which is supported by our 
experiments described in Section 5.3). Although 
semantic features have rarely been effective in 
many text classification problems (Moschitti and 
Basili 2004), there is reason here to hypothesize 
that a specialized vocabulary of important words 
might help with the sparsity. We hypothesized 
                                                     
2  Reviews are analyzed using the Minipar dependency 
parser (Lin 1994). 
that good reviews will often contain: i) refer-
ences to the features of a product (e.g., the LCD 
and resolution of a digital camera), and ii) men-
tions of sentiment words (i.e., words that express 
an opinion such as ?great screen?). Below we 
describe two families of features that capture 
these semantic observations within the reviews: 
? Product-Feature (PRF): The features of prod-
ucts that occur in the review, e.g., capacity of 
MP3 players and zoom of a digital camera. 
This feature counts the number of lexical 
matches that occur in the review for each prod-
uct feature. There is no trivial way of obtaining 
a list of all the features of a product. In Section 
5.1 we describe a method for automatically ex-
tracting product features from Pro/Con listings 
from Epinions.com. Our assumption is that 
pro/cons are the features that are important for 
customers (and hence should be part of a help-
ful review). 
? General-Inquirer (GIW): Positive and negative 
sentiment words describing products or prod-
uct features (e.g., ?amazing sound quality? and 
?weak zoom?). The intuition is that reviews 
that analyze product features are more helpful 
than those that do not. We try to capture this 
analysis by extracting sentiment words using 
the publicly available list of positive and nega-
tive sentiment words from the General Inquirer 
Dictionaries3. 
Meta-Data Features 
Unlike the previous four feature classes, meta-
data features capture observations which are in-
dependent of the text (i.e., unrelated with linguis-
tic features). We consider the following feature: 
? Stars (STR): Most websites require reviewers 
to include an overall rating for the products 
that they review (e.g., star ratings in Ama-
zon.com). This feature set includes the rating 
score (STR1) as well as the absolute value of 
the difference between the rating score and the 
average rating score given by all reviewers 
(STR2). 
We differentiate meta-data features from seman-
tic features since they require external knowl-
edge that may not be available from certain 
review sites. Nowadays, however, most sites that 
collect user reviews also collect some form of 
product rating (e.g., Amazon.com, Over-
stock.com, and Apple.com). 
                                                     
3 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
425
4 Ranking System 
In this paper, we estimate the helpfulness func-
tion in Equation 1 using user ratings extracted 
from Amazon.com, where rating+(r) is the num-
ber of unique users that rated the review r as 
helpful and rating-(r) is the number of unique 
users that rated r as unhelpful. 
Reviews from Amazon.com form a gold stan-
dard labeled dataset of {review, h(review)} pairs 
that can be used to train a supervised machine 
learning algorithm. In this paper, we applied an 
SVM (Vapnik 1995) package on the features ex-
tracted from reviews to learn the function h. 
Two natural options for learning helpfulness 
according to Equation 1 are SVM Regression and 
SVM Ranking (Joachims 2002). Though learning 
to rank according to helpfulness requires only 
SVM Ranking, the helpfulness function provides 
non-uniform differences between ranks in the 
training set. Also, in practice, many products 
have only one review, which can serve as train-
ing data for SVM Regression but not SVM Rank-
ing. Furthermore, in large sites such as 
Amazon.com, when new reviews are written it is 
inefficient to re-rank all previously ranked re-
views. We therefore choose SVM Regression in 
this paper. We describe the exact implementation 
in Section 5.1. 
After the SVM is trained, for a given product 
and its set of reviews R, we rank the reviews of R 
in decreasing order of h(r), r ? R. 
Table 1 shows four sample reviews for the 
iPod Photo 20GB product from Amazon.com, 
their total number of helpful and unhelpful votes, 
as well as their rank according to the helpfulness 
score h from both the gold standard from Ama-
zon.com and using the SVM prediction of our 
best performing system described in Section 5.2. 
5 Experimental Results 
We empirically evaluate our review model and 
ranking system, described in Section 3 and Sec-
tion 4, by comparing the performance of various 
feature combinations on products mined from 
Amazon.com. Below, we describe our experi-
mental setup, present our results, and analyze 
system performance. 
5.1 Experimental Setup 
We describe below the datasets that we extracted 
from Amazon.com, the implementation of our 
SVM system, and the method we used for ex-
tracting features of reviews. 
Extraction and Preprocessing of Datasets 
We focused our experiments on two products 
from Amazon.com: MP3 Players and Digital 
Cameras. 
Using Amazon Web Services API, we col-
lected reviews associated with all products in the 
MP3 Players and Digital Cameras categories. 
For MP3 Players, we collected 821 products and 
33,016 reviews; for Digital Cameras, we col-
lected 1,104 products and 26,189 reviews. 
In most retailer websites like Amazon.com, 
duplicate reviews, which are quite frequent, skew 
statistics and can greatly affect a learning algo-
rithm. Looking for exact string matches between 
reviews is not a sufficient filter since authors of 
duplicated reviews often make small changes to 
the reviews to avoid detection. We built a simple 
filter that compares the distribution of word bi-
grams across each pair of reviews. A pair is 
deemed a duplicate if more than 80% of their 
bigrams match. 
Also, whole products can be duplicated. For 
different product versions, such as iPods that can 
come in black or white models, reviews on Ama-
zon.com are duplicated between them. We filter 
Table 1. Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-
zon.com along with their ratings as well as their helpfulness ranks (from both the gold 
standard from Amazon.com and the SVM prediction of our best performing system de-
scribed in Section 5.2). 
RANK(h) 
REVIEW TITLE 
HELPFUL 
VOTES 
UNHELPFUL 
VOTES GOLD 
STANDARD 
SVM 
PREDICTION 
?iPod Moves to All-color Line-up? 215 11 7 1 
?iPod: It's NOT Music to My Ears? 11 13 25 30 
?The best thing I ever bought? 22 32 26 27 
?VERY disappointing? 1 18 40 40 
 
426
out complete products where each of its reviews 
is detected as a duplicate of another product (i.e., 
only one iPod version is retained). 
The filtering of duplicate products and dupli-
cate reviews discarded 85 products and 12,097 
reviews for MP3 Players and 38 products and 
3,692 reviews for Digital Cameras. 
In order to have accurate estimates for the 
helpfulness function in Equation 1, we filtered 
out any review that did not receive at least five 
user ratings (i.e., reviews where less than five 
users voted it as helpful or unhelpful are filtered 
out). This filtering was performed before dupli-
cate detection and discarded 45.7% of the MP3 
Players reviews and 32.7% of the Digital Cam-
eras reviews. 
Table 2 describes statistics for the final data-
sets after the filtering steps. 10% of products for 
both datasets were withheld as development cor-
pora and the remaining 90% were randomly 
sorted into 10 sets for 10-fold cross validation. 
SVM Regression 
For our regression model, we deployed the state 
of the art SVM regression tool SVMlight 
(Joachims 1999). We tested on the development 
sets various kernels including linear, polynomial 
(degrees 2, 3, and 4), and radial basis function 
(RBF). The best performing kernel was RBF and 
we report only these results in this paper (per-
formance was measured using Spearman?s corre-
lation coefficient, described in Section 5.2). 
We tuned the RBF kernel parameters C (the 
penalty parameter) and ? (the kernel width hy-
perparameter) performing full grid search over 
the 110 combinations of exponentially spaced 
parameter pairs (C,?) following (Hsu et al 2003). 
Feature Extraction 
To extract the features described in Section 3.2, 
we preprocessed each review using the Minipar 
dependency parser (Lin 1994). We used the 
parser tokenization, sentence breaker, and syn-
tactic categorizations to generate the Length, 
Sentential, Unigram, Bigram, and Syntax feature 
sets. 
In order to count the occurrences of product 
features for the Product-Feature set, we devel-
oped an automatic way of mining references to 
product features from Epinions.com. On this 
website, user-generated product reviews include 
explicit lists of pros and cons, describing the best 
and worst aspects of a product. For example, for 
MP3 players, we found the pro ?belt clip? and 
the con ?Useless FM tuner?. Our assumption is 
that the pro/con lists tend to contain references to 
the product features that are important to cus-
tomers, and hence their occurrence in a review 
may correlate with review helpfulness. We fil-
tered out all single-word entries which were in-
frequently seen (e.g., hold, ever). After splitting 
and filtering the pro/con lists, we were left with a 
total of 9,110 unique features for MP3 Players 
and 13,991 unique features for Digital Cameras. 
The Stars feature set was created directly from 
the star ratings given by each author of an Ama-
zon.com review. 
For each feature measurement f, we applied 
the following standard transformation: 
 ( )1ln +f  
and then scaled each feature between [0, 1] as 
suggested in (Hsu et al 2003). 
We experimented with various combinations 
of feature sets. Our results tables use the abbre-
viations presented in Section 3.2. For brevity, we 
report the combinations which contributed to our 
best performing system and those that help assess 
the power of the different feature classes in cap-
turing helpfulness. 
5.2 Ranking Performance 
Evaluating the quality of a particular ranking is 
difficult since certain ranking intervals can be 
more important than others (e.g., top-10 versus 
bottom-10) We adopt the Spearman correlation 
coefficient ? (Spearman 1904) since it is the 
most commonly used measure of correlation be-
tween two sets of ranked data points4. 
For each fold in our 10-fold cross-validation 
experiments, we trained our SVM system using 9 
folds. For the remaining test fold, we ranked each 
product?s reviews according to the SVM predic-
tion (described in Section 4) and computed the ? 
                                                     
4 We used the version of Spearman?s correlation coeffi-
cient that allows for ties in rankings. See Siegel and Cas-
tellan (1988) for more on alternate rank statistics such as 
Kendall?s tau. 
Table 2. Overview of filtered datasets extracted 
from Amazon.com. 
 MP3 PLAYERS 
DIGITAL 
CAMERAS 
Total Products 736 1066 
Total Reviews 11,374 14,467 
Average Reviews/Product 15.4 13.6 
Min/MaxReviews/Product 1 / 375 1 / 168 
 
427
correlation between the ranking and the gold 
standard ranking from the test fold5. 
Although our task definition is to learn review 
rankings according to helpfulness, as an interme-
diate step the SVM system learns to predict the 
absolute helpfulness score for each review. To 
test the correlation of this score against the gold 
standard, we computed the standard Pearson cor-
relation coefficient. 
Results show that the highest performing fea-
ture combination consisted of the Length, the 
Unigram, and the Stars feature sets. Table 3 re-
ports the evaluation results for every combination 
of these features with 95% confidence bounds. 
Of the three features alone, neither was statisti-
cally more significant than the others. Examining 
each pair combination, only the combination of 
length with stars outperformed the others. Sur-
prisingly, adding unigram features to this combi-
nation had little effect for the MP3 Players. 
Given our list of features defined in Section 
3.2, helpfulness of reviews is best captured with 
a combination of the Length and Stars features. 
Training an RBF-kernel SVM regression model 
does not necessarily make clear the exact rela-
tionship between input and output variables. To 
investigate this relationship between length and 
helpfulness, we inspected their Pearson correla-
tion coefficient, which was 0.45. Users indeed 
tend to find short reviews less helpful than longer 
ones: out of the 5,247 reviews for MP3 Players 
that contained more than 1000 characters, the 
average gold standard helpfulness score was 
82%; the 204 reviews with fewer than 100 char-
acters had on average a score of 23%. The ex-
plicit product rating, such as Stars is also an 
                                                     
5 Recall that the gold standard is extracted directly from 
user helpfulness votes on Amazon.com (see Section 4). 
indicator of review helpfulness, with a Pearson 
correlation coefficient of 0.48. 
The low Pearson correlations of Table 3 com-
pared to the Spearman correlations suggest that 
we can learn the ranking without perfectly learn-
ing the function itself. To investigate this, we 
tested the ability of SVM regression to recover 
the target helpfulness score, given the score itself 
as the only feature. The Spearman correlation for 
this test was a perfect 1.0. Interestingly, the Pear-
son correlation was only 0.798, suggesting that 
the RBF kernel does learn the helpfulness rank-
ing without learning the function exactly. 
5.3 Results Analysis 
Table 3 shows only the feature combinations of 
our highest performing system. In Table 4, we 
report several other feature combinations to show 
why we selected certain features and what was 
the effect of our five feature classes presented in 
Section 3.2. 
In the first block of six feature combinations in 
Table 4, we show that the unigram features out-
perform the bigram features, which seem to be 
suffering from the data sparsity of the short re-
views. Also, unigram features seem to subsume 
the information carried in our semantic features 
Product-Feature (PRF) and General-Inquirer 
(GIW). Although both PRF and GIW perform 
well as standalone features, when combined with 
unigrams there is little performance difference 
(for MP3 Players we see a small but insignificant 
decrease in performance whereas for Digital 
Cameras we see a small but insignificant im-
provement). Recall that PRF and GIW are simply 
subsets of review words that are found to be 
product features or sentiment words. The learn-
ing algorithm seems to discover on its own which 
Table 3. Evaluation of the feature combinations that make up our best performing system 
(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-
ing to helpfulness. 
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
LEN 0.575 ? 0.037 0.391 ? 0.038 0.521 ? 0.029 0.357 ? 0.029 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
UGR+STR1 0.644 ? 0.033 0.436 ? 0.038 0.490 ? 0.032 0.324 ? 0.032 
LEN+UGR 0.582 ? 0.036 0.401 ? 0.038 0.553 ? 0.028 0.394 ? 0.029 
LEN+STR1 0.652 ? 0.033 0.470 ? 0.038 0.577 ? 0.029 0.423 ? 0.031 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN=Length; UGR=Unigram; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
428
words are most important in a review and does 
not use additional knowledge about the meaning 
of the words (at least not the semantics contained 
in PRF and GIW). 
We tested two different versions of the Stars 
feature: i) the number of star ratings, STR1; and 
ii) the difference between the star rating and the 
average rating of the review, STR2. The second 
block of feature combinations in Table 4 shows 
that neither is significantly better than the other 
so we chose STR1 for our best performing sys-
tem. 
Our experiments also revealed that our struc-
tural features Sentential and HTML, as well as 
our syntactic features, Syntax, did not show any 
significant improvement in system performance. 
In the last block of feature combinations in Table 
4, we report the performance of our best per-
forming features (Length, Unigram, and Stars) 
along with these other features. Though none of 
the features cause a performance deterioration, 
neither of them significantly improves perform-
ance. 
5.4 Discussion 
In this section, we discuss the broader implica-
tions and potential impacts of our work, and pos-
sible connections with other research directions. 
The usefulness of the Stars feature for deter-
mining review helpfulness suggests the need for 
developing automatic methods for assessing pro-
duct ratings, e.g., (Pang and Lee 2005).  
Our findings focus on predictors of helpful-
ness of reviews of tangible consumer products 
(consumer electronics). Helpfulness is also solic-
ited and tracked for reviews of many other types 
of entities: restaurants (citysearch.com), films 
(imdb.com), reviews of open-source software 
modules (cpanratings.perl.org), and countless 
others. Our findings of the importance of Length, 
Unigrams, and Stars may provide the basis of 
comparison for assessing helpfulness of reviews 
of other entity types. 
Our work represents an initial step in assessing 
helpfulness. In the future, we plan to investigate 
other possible indicators of helpfulness such as a 
reviewer?s reputation, the use of comparatives 
(e.g., more and better than), and references to 
other products. 
Taken further, this work may have interesting 
connections to work on personalization, social 
networks, and recommender systems, for in-
stance by identifying the reviews that a particular 
user would find helpful.  
Our work on helpfulness of reviews also has 
potential applications to work on automatic gen-
Table 4. Performance evaluation of various feature combinations for ranking reviews of MP3 Players 
and Digital Cameras on Amazon.com according to helpfulness. The first six lines suggest that uni-
grams subsume the semantic features; the next two support the use of the raw counts of product ratings 
(stars) rather than the distance of this count from the average rating; the final six investigate the impor-
tance of auxiliary feature sets.  
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
BGR 0.499 ? 0.040 0.293 ? 0.038 0.434 ? 0.032 0.242 ? 0.029 
PRF 0.591? 0.037 0.400 ? 0.039 0.527 ? 0.030 0.316 ? 0.028 
GIW 0.571 ? 0.036 0.381 ? 0.038 0.524 ? 0.030 0.333 ? 0.028 
UGR+PRF 0.570 ? 0.037 0.375 ? 0.038 0.546 ? 0.029 0.348 ? 0.028 
UGR+GIW 0.554 ? 0.037 0.358 ? 0.038 0.568 ? 0.031 0.324 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
STR2 0.556 ? 0.032 0.303 ? 0.038 0.504 ? 0.027 0.229 ? 0.027 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SEN 0.653 ? 0.033 0.470 ? 0.038 0.599 ? 0.028 0.448 ? 0.030 
LEN+UGR+STR1+HTM 0.640 ? 0.035 0.459 ? 0.039 0.594 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SYN 0.645 ? 0.034 0.469 ? 0.039 0.595 ? 0.028 0.447 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN 0.631 ? 0.035 0.453 ? 0.039 0.600 ? 0.028 0.452 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601 ? 0.035 0.396 ? 0.038 0.604 ? 0.027 0.460 ? 0.030 
LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram; 
SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
429
eration of review information, by providing a 
way to assess helpfulness of automatically gener-
ated reviews. Work on generation of reviews in-
cludes review summarization and extraction of 
useful reviews from blogs and other mixed texts. 
6 Conclusions 
Ranking reviews according to user helpfulness is 
an important problem for many online sites such 
as Amazon.com and Ebay.com. To date, most 
websites measure helpfulness by having users 
manually assess how helpful each review is to 
them. In this paper, we proposed an algorithm for 
automatically assessing helpfulness and ranking 
reviews according to it. Exploiting the multitude 
of user-rated reviews on Amazon.com, we 
trained an SVM regression system to learn a 
helpfulness function and then applied it to rank 
unlabeled reviews. Our best system achieved 
Spearman correlation coefficient scores of 0.656 
and 0.604 against a gold standard for MP3 play-
ers and digital cameras. 
We also performed a detailed analysis of dif-
ferent features to study the importance of several 
feature classes in capturing helpfulness. We 
found that the most useful features were the 
length of the review, its unigrams, and its product 
rating. Semantic features like mentions of prod-
uct features and sentiment words seemed to be 
subsumed by the simple unigram features. Struc-
tural features (other than length) and syntactic 
features had no significant impact. 
It is our hope through this work to shed some 
light onto what people find helpful in user-
supplied reviews and, by automatically ranking 
them, to ultimately enhance user experience. 
References 
Attali, Y. and Burstein, J. 2006. Automated Essay 
Scoring With e-rater? V.2. Journal of Technology, 
Learning, and Assessment, 4(3). 
Burstein, J., Chodorow, M., and Leacock, C. 2004. 
Automated essay evaluation: the criterion online 
writing service. AI Magazine. 25(3), pp 27?36.  
Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector 
machines for spam categorization. IEEE Trans. 
Neural Netw., 10, 1048?1054. 
Gabrilovich, E. and Markovitch, S. 2005. 
Feature Generation for Text Categorization Using 
World Knowledge. In Proceedings of IJCAI-2005. 
Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A 
practical guide to SVM classification. Technical 
report, Department of Computer Science and 
Information Technology, National Taiwan University. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. KDD?04. pp.168 ? 177 
Kim, S. and Hovy, E. 2004. Determining the Sentiment 
of Opinions. Proceedings of COLING-04. 
Joachims, T. 1999. Making Large-Scale SVM Learning 
Practical. In B. Sch?lkopf, C. Burges, and A. Smola 
(eds), Advances in Kernel Methods: Support Vector 
Learning. MIT Press. Cambridge, MA. 
Joachims, T. 2002. Optimizing Search Engines Using 
Clickthrough Data. In Proceedings of ACM KDD-02.  
Moschitti, A. and Basili R. 2004. Complex Linguistic 
Features for Text Classification: A Comprehensive 
Study. In Proceedings of ECIR 2004. Sunderland, 
U.K. 
Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques.  Proceedings of EMNLP 2002. 
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class 
relationships for sentiment categorization with respect 
to rating scales. In Proceedings of the ACL, 2005. 
Riloff , E. and J. Wiebe. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of 
EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning 
Subjective Nouns Using Extraction Pattern 
Bootstrapping. Proceedings of CoNLL-03 
Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003. 
A Hybrid Text Classification Approach for Analysis 
of Student Essays. In Proc. of the HLT-NAACL, 2003. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Spearman C. 1904. The Proof and Measurement of 
Association Between Two Things. American Journal 
of Psychology, 15:72?101. 
Turney, P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 40th 
Annual Meeting of the ACL, Philadelphia, 417?424. 
Vapnik, V.N. 1995. The Nature of Statistical Learning 
Theory. Springer.  
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of the 37th Annual Meeting of the 
Association for Computational Linguistics(ACL-99), 
246?253. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proceedings of EMNLP 2003.
  
430
A Bootstrapping Algorithm for  
Automatically Harvesting Semantic Relations 
Marco Pennacchiotti 
Department of Computer Science 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
pantel@isi.edu 
Abstract 
In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a 
web-based knowledge expansion technique, for extracting binary semantic relations. Given a 
small set of seed instances for a particular relation, the system learns lexical patterns, applies 
them to extract new instances, and then uses the Web to filter and expand the instances. 
Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of 
semantic relations when compared with two state of the art systems. 
1. Introduction 
Recent attention to knowledge-rich problems such as question answering [18] and textual 
entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop 
algorithms for automatically harvesting shallow semantic resources. With seemingly endless 
amounts of textual data at our disposal, we have a tremendous opportunity to automatically 
grow semantic term banks and ontological resources. Methods must be accurate, adaptable 
and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), 
and independent or weakly dependent on human supervision. 
In this paper we present Espresso, a novel bootstrapping algorithm for automatically 
harvesting semantic relations, aiming at effectively supporting NLP applications, 
emphasizing two major points that have been partially neglected by previous systems: 
generality and weak supervision. 
From the one side, Espresso is intended as a general-purpose system able to extract a wide 
variety of binary semantic relations, from the classical is-a and part-of relations, to more 
specific and domain oriented ones like chemical reactants in a chemistry domain and position 
succession in political texts. The system architecture is designed with generality in mind, 
avoiding any relation-specific inference technique. Indeed, for each semantic relation, the 
system builds specific lexical patterns inferred from textual corpora. 
From the other side, Espresso requires only weak human supervision. In order to start the 
extraction process, a user provides only a small set of seed instances of a target relation (e.g. 
Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed 
instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger 
set is required. To guarantee weakest supervision, Espresso combines its bootstrapping 
approach with a web-based knowledge expansion technique and linguistic analysis, 
exploiting the seeds as much as possible. 
2. Relevant Work 
To date, most research on lexical relation harvesting has focused on is-a and part-of relations. 
Approaches fall into two main categories: pattern- and clustering-based. 
Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract 
hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a 
bootstrapping algorithm to learn more patterns from instances, which has served as the model 
for most subsequent pattern-based algorithms. 
Berland and Charniak [1] propose a system for part-of relation extraction, based on the 
Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used 
to extract new instances, ranked according to various statistical measures. While this study 
introduces statistical measures to evaluate instance reliability, it remains vulnerable to data 
sparseness and has the limitation of taking into consideration only one-word terms. 
Improving upon Berland and Charniak [1], Girju et al [11] employ machine learning 
algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP?s part-
NP]. This study is the first extensive attempt to solve the problem of generic relational 
patterns, that is, those expressive patterns that have high recall while suffering low precision, 
as they subsume a large set of instances. In order to discard incorrect instances, Girju et al 
learn WordNet-based selectional restrictions, like [whole-NP(scene#4)?s part-NP(movie#1)]. 
While making huge grounds on improving precision/recall, the system requires heavy 
supervision through manual semantic annotations. 
Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to 
terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from 
a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. 
The frequencies of the substrings in the corpus are then used to retain the best patterns. The 
approach gives good results on specific relations such as birthdates, however it has low 
precision on generic ones like is-a and part-of. Pantel et al [17] proposed a similar, highly 
scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing 
both good performances and efficiency. Espresso uses a similar approach to infer patterns, 
but we then apply refining techniques to deal with various types of relations. 
Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic 
method for discovering similar words using a few seed examples by using pattern-based 
techniques and human supervision, KnowItAll [7] that performs large-scale extraction of 
facts from the Web, Mann [15] and Fleischman et al [9] who used part of speech patterns to 
extract a subset of is-a relations involving proper nouns, and Downey et al [6] who 
formalized the problem of relation extraction in a coherent and effective combinatorial model 
that is shown to outperform previous probabilistic frameworks. 
Clustering approaches to relation extraction are less common and have insofar been applied 
only to is-a extraction. These methods employ clustering algorithms to group words 
according to their meanings in text, label the clusters using its members? lexical or syntactic 
dependencies, and then extract an is-a relation between each cluster member and the cluster 
label. Caraballo [3] proposed the first attempt, which used conjunction and apposition 
features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this 
approach by making use of all syntactic dependency features for each noun. The advantage of 
clustering approaches is that they permit algorithms to identify is-a relations that do not 
explicitly appear in text, however they generally fail to produce coherent clusters from fewer 
than 100 million words; hence they are unreliable for small corpora. 
3. The Espresso Algorithm 
The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a 
specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of 
seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the 
relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm 
begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that 
captures as many of the seed instances as possible in C. In the second phase, we define a 
reliability measure to select the best set of patterns P'?P. In phase three, the patterns in P' are 
used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and 
then selects the best instances I' as input seeds for the next iteration. The algorithm terminates 
when a predefined stopping condition is met (for our preliminary experiments, the stopping 
condition is set according to the size of the corpus). For each induced pattern p and instance i, 
the information theoretic scores, r?(p) and r?(i) respectively, aim to express their reliability. 
Below, Sections 3.2?3.5 describe in detail these different phases of Espresso. 
3.1. Term definition 
Before one can extract relation instances from a corpus, it is necessary to define a 
tokenization procedure for extracting terms. Terms are commonly defined as surface 
representations of stable and key domain concepts [19]. Defining regular expressions over 
POS-tagged corpora is the most commonly used technique to both define and extract terms. 
We adopt a slightly modified version of the term definition given in [13], as it is one of the 
most commonly used in the literature: 
 ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun 
We operationally extend the definition of Adj to include present and past participles as most 
noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, 
unlike many approaches for automatic relation extraction, we allow complex multi-word 
terms as anchor points. Hence, we can capture relations between complex terms, such as 
?record of a criminal conviction? part-of ?FBI report?. 
3.2. Phase 1: Pattern discovery 
The pattern discovery phase takes as input a set of instances I' and produces as output a set of 
lexical patterns P. For the first iteration I' = Is, the set of initial seeds. In order to induce P, we 
apply a slight modification to the approach presented in [20]. For each input instance i = {x, 
y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then 
generalized into a set of new sentences SGx,y by replacing all terminological expressions by a 
terminological label (TR). For example: 
 ?Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y? 
is generalized as: 
 ?Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y? 
All substrings linking terms x and y are then extracted from the set SGx,y, and overall 
frequencies are computed. The most frequent substrings then represent the set of new patterns 
P, where the frequency cutoff is experimentally set. Term generalization is particularly useful 
for small corpora, where generalization is vital to ease the data sparseness. However, the 
generalized patterns are naturally less precise. Hence, when dealing with bigger corpora, the 
system allows the use of Sx,y?SGx,y in order to extract substrings. For our experiments, we 
used the set SGx,y . 
3.3. Phase 2: Pattern filtering 
In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a 
reliable pattern is one that is both highly precise and one that extracts many instances. The 
recall of a pattern p can be approximated by the fraction of input instances in I' that are 
extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are 
weary of keeping patterns that generate many instances (i.e., patterns that generate high recall 
but potentially disastrous precision). We thus prefer patterns that are highly associated with 
the input patterns I'. Pointwise mutual information [4] is a commonly used metric for 
measuring the strength of association between two events x and y: 
 ( ) ( )( ) ( )yPxP
yxP
yxpmi
,
log, =  
We define the reliability of a pattern p, r?(p), as its average strength of association across 
each input instance i in I', weighted by the reliability of each instance i: 
 ( )
( )
I
ir
pipmi
pr
Ii pmi
?
???
?
???
? ?
=
?
??
?
?
max
),(
 
where r?(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum 
pointwise mutual information between all patterns and all instances. r?(p) ranges from [0,1]. 
The reliability of the manually supplied seed instances are r?(i) = 1. The pointwise mutual in-
formation between instance i = {x, y} and pattern p is estimated using the following formula: 
 ( )
,**,,*,
,,
log,
pyx
ypx
pipmi =  
where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the 
asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual 
information is biased towards infrequent events. To address this, we multiply pmi(i, p) with 
the discounting factor suggested in [16]. 
The set of highest n scoring patterns P', according to r?(p), are then selected and retained for 
the next phase, where n is the number of patterns of the previous iteration incremented by 1. 
In general, we expect that the set of patterns is formed by those of the previous iteration plus 
a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was 
previously discovered. 
Moreover, to further discourage too generic patterns that might have low precision, a 
threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than 
t instances are then discarded, no matter what their score is. In this paper, we experimentally 
set t to a value dependent on the size of the corpus. In future work, this parameter can be 
learned using a development corpus. 
Our reliability measure ensures that overly generic patterns, which may potentially have very 
low precision, are discarded. However, we are currently exploring a web-expansion algorithm 
that could both help detect generic patterns and also filter out their incorrect instances. We 
estimate the precision of the instance set generated by a new pattern p by looking at the 
number of these instances that are instantiated on the Web by previously accepted patterns. 
Generic patterns will generate instances with higher Web counts than incorrect patterns. 
Then, the Web counts can also be used to filter out incorrect instances from the generic 
patterns? instantiations. More details are discussed in Section 4.3. 
3.4. Phase 3: Instance discovery 
In this phase, Espresso retrieves from the corpus the set of instances I that match any of the 
lexical patterns in P'. 
In small corpora, the number of extracted instances can be too low to guarantee sufficient 
statistical evidence for the pattern discovery phase of the next iteration. In such cases, the 
system enters a web expansion phase, in which new instances for the given patterns are 
retrieved from the Web, using the Google search engine. Specifically, for each instance i? I, 
the system creates a set of queries, using each pattern in P' with its y term instantiated with i?s 
y term. For example, given the instance ?Italy ; country? and the pattern [Y such as X] , the 
resulting Google query will be ?country such as *?. New instances are then created from the 
retrieved Web results (e.g. ?Canada ; country?) and added to I. We are currently exploring 
filtering mechanisms to avoid retrieving too much noise. 
Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of 
new instances is created for each instance i? I by extracting sub-terminological expressions 
from x corresponding to the syntactic head of terms. For example, expanding the relation 
?new record of a criminal conviction? part-of ?FBI report?, the following new instances are 
obtained: ?new record? part-of ?FBI report?, and ?record? part-of ?FBI report?. 
3.5. Phase 4: Instance filtering 
Estimating the reliability of an instance is similar to estimating the reliability of a pattern. 
Intuitively, a reliable instance is one that is highly associated with as many reliable patterns 
as possible (i.e., we have more confidence in an instance when multiple reliable patterns 
instantiate it.) Hence, analogous to our pattern reliability measure in Section 3.3, we define 
the reliability of an instance i, r?(i), as: 
 ( )
( )
P
pr
pipmi
ir Pp pmi?
?
=
?
??
?
?
max
),(
 
where r?(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum 
pointwise mutual information between all patterns and all instances, as in Section 3.3. 
Espresso finally selects the highest scoring m instances, I', and retains them as input for the 
subsequent iteration. In this paper, we experimentally set m = 200. 
4. Experimental Results 
4.1. Experimental Setup 
In this section, we present a preliminary comparison of Espresso with two state of the art 
systems on the task of extracting various semantic relations. 
4.1.1. Datasets 
We perform our experiments using the following two datasets: 
? TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) 
newswire text collection. The sample consists of 5,951,432 words extracted from the 
following data files: AP890101 ? AP890131, AP890201 ? AP890228, and AP890310 
? AP890319. 
? CHEM: This small dataset of 313,590 words consists of a college level textbook of 
introductory chemistry [2]. 
We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 
4.1.2. Systems 
We compare the results of Espresso with the following two state of the art extraction 
systems: 
? RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction 
patterns from a set of seed instances of a particular relation (see Section 2.) 
? PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first 
automatically induces concepts (clusters) from a raw corpus, names the concepts, and 
then extracts an is-a relation between each cluster member and its cluster label. For 
each cluster member, the system may generate multiple possible is-a relations, but in 
this evaluation we only keep the highest scoring one. To apply this algorithm, both 
datasets were first analyzed using the Minipar parser [14]. 
? ESP: This is the algorithm described in this paper (details in Section 3). 
4.1.3. Semantic Relations 
Espresso is designed to extract various semantic relations exemplified by a given small set of 
seed instances. For our preliminary evaluation, we consider the standard is-a and part-of 
relations as well as three novel relations: 
? succession: This relation indicates that one proper noun succeeds another in a position 
or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI 
succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus. 
? reaction: This relation occurs between chemical elements/molecules that can be 
combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas 
and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM 
corpus. 
? production: This relation occurs when a process or element/object produces a result. 
For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM 
corpus. 
For each semantic relation, we manually extracted a set of seed examples. The seeds were 
used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample 
outputs from Espresso. 
4.2. Precision and Recall 
We implemented each of the three systems outlined in Section 4.1.2 and applied them to the 
TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the 
system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 
                                                 
1 PR04 does not require any seeds. 
CHEM corpus) and evaluating their quality manually using one human judge2. For each 
instance, the judge may assign a score of 1 for correct, 0 for incorrect, and ? for partially 
correct. Example instances that were judged partially correct include ?analyst is-a manager? 
and ?pilot is-a teacher?. The precision for a given set of relation instances is the sum of the 
judge?s scores divided by the number of instances. 
Although knowing the total number of instances of a particular relation in any non-trivial 
corpus is impossible, it is possible to compute the recall of a system relative to another 
system?s recall. The recall of a system A, RA, is given by the following formula: 
 C
C
R AA =
 
where CA is the number of correct instances of a particular relation extracted by A and C is 
the total number of correct instances in the corpus. Following [17], we define the relative 
recall of system A given system B, RA|B, as: 
 
BP
AP
C
C
R
R
R
B
A
B
A
B
A
BA ?
?===|  
Using the precision estimates, PA, from our precision experiments, we can estimate CA ? PA ? 
|A|, where A is the total number of instances of a particular relation discovered by system A. 
                                                 
2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and 
agreement scores. 
Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The 
number in the parentheses for each relation denotes the total number of seeds. 
  SEEDS ESP 
Is-a (12) 
wheat :: crop 
George Wendt :: star 
Miami :: city 
shark :: predator 
Picasso :: artist 
tax :: charge 
drug dealers :: felons 
Italy :: country 
Part-Of (12) 
leader :: panel 
city :: region 
plastic :: explosive 
United States :: alliance 
shield :: nuclear missile 
biblical quotations :: book 
trees :: land 
material :: FBI report 
T
R
E
C
9 
Succession (12) 
Khrushchev :: Stalin 
Carla Hills :: Yeutter 
George Bush :: Ronald Reagan 
Julio Barbosa de Aquino :: Mendes 
Ford :: Nixon 
Setrakian :: John Griesemer 
Camero Cardiel :: Camacho 
Susan Weiss :: editor 
Is-a (12) 
NaCl :: ionic compounds 
diborane :: substance 
nitrogen :: element 
gold :: precious metal 
Na :: element 
protein :: biopolymer 
HCl :: strong acid 
electromagnetic radiation :: energy 
Part-Of (12) 
ion :: matter 
oxygen :: water 
light particle :: gas 
element :: substance 
oxygen :: air 
powdered zinc metal :: battery 
atom :: molecule 
ethylene glycol :: automotive antifreeze 
Reaction (13) 
magnesium :: oxygen 
hydrazine :: water 
aluminum metal :: oxygen 
lithium metal :: fluorine gas 
hydrogen :: oxygen 
Ni :: HCl 
carbon dioxide :: methane 
boron :: fluorine 
C
H
E
M 
Production (14) 
bright flame :: flares 
hydrogen :: solid metal hydrides 
ammonia :: nitric oxide 
copper :: brown gas 
electron :: ions 
glycerin :: nitroglycerin 
kidneys :: kidney stones 
ions :: charge 
 
Table 8. System performance on the production
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 197 57.5% 0.80 
ESP 196 72.5% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Tables 2 ? 8 reports the total number of 
instances, precision, and relative recall of 
each system on the TREC-9 and CHEM 
corpora. The relative recall is always given in 
relation to the Espresso system. For example, 
in Table 2, RH02 has a relative recall of 5.31 
with Espresso, which means that the RH02 
system output 5.31 times more correct 
relations than Espresso (at a cost of much 
lower precision). Similarly, PR04 has a relative recall of 0.23 with Espresso, which means 
that PR04 outputs 4.35 fewer correct relations than Espresso (also with a smaller precision). 
4.3. Discussion 
Experimental results, for all relations and the two different corpus sizes, show that Espresso 
greatly outperforms the other two methods on precision. However, Espresso fails to match 
the recall level of RH02 in all but the experiment on the production relation. Indeed, the 
filtering of unreliable patterns and instances during the bootstrapping algorithm not only 
discards the patterns that are unrelated to the actual relation, but also patterns that are too 
generic and ambiguous ? hence resulting in a loss of recall. 
As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise 
in the system (e.g, the pattern [X of Y] can ambiguously refer to a part-of, is-a or possession 
Table 2. System performance on the is-a
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 57,525 28.0% 5.31 
PR04 1,504 47.0% 0.23 
ESP 4,154 73.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 3. System performance on the is-a
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 2556 25.0% 3.76 
PR04 108 40.0% 0.25 
ESP 200 85.0% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 4. System performance on the part-of
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 12,828 35.0% 42.52 
ESP 132 80.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 5. System performance on the part-of
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 11,582 33.8% 58.78 
ESP 111 60.0% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 6. System performance on the succession
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 49,798 2.0% 36.96 
ESP 55 49.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 7. System performance on the reaction
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 6,083 30% 53.67 
ESP 40 85% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
relation). However, generic patterns, while having low precision, yield a high recall, as also 
reported by [11]. We ran an experiment on the reaction relation, retaining the generic patterns 
produced during Espresso?s selection process. As expected, we obtained 1923 instances 
instead of the 40 reported in Table 7, but precision dropped from 85% to 30%. 
The challenge, then, is to harness the expressive power of the generic patterns whilst 
maintaining the precision of Espresso. We propose the following solution that helps both in 
distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances 
produced by generic patterns. Unlike Girju et al [11] that propose a highly supervised 
machine learning approach based on selectional restriction, ours is an unsupervised method 
based on statistical evidence obtained from the Web. At a given iteration in Espresso, the 
intuition behind our solution is that the Web is large enough that correct instances will be 
instantiated by many of the currently accepted patterns P. Hence, we can distinguish between 
generic patterns and incorrect patterns by inspecting the relative frequency distribution of 
their instances using the patterns in P. More formally, given an instance i produced by a 
generic or incorrect pattern, we count how many times i instantiates on the Web with every 
pattern in P, using Google. The instance i is then considered correct if its web count surpasses 
a given threshold. The pattern in question is accepted as a generic pattern if a sufficient 
number of its instances are considered correct, otherwise it is rejected as an incorrect pattern. 
Although our results in Section 4.2 do not include this algorithm, we performed a small 
experiment by adding an a-posteriori generic pattern recovery phase to Espresso. We tested 
the 7,634 instances extracted by the generic pattern [X of Y] on the CHEM corpus for the 
part-of relation. We randomly sample 200 of these instances and then queried Google for 
these instances using the pattern [X consists of Y]. Manual evaluation of the 25 instances that 
occurred at least once on Google showed 50% precision. Adding these instances to the results 
from Table 5 decreases the system precision from 60% to 51%, but dramatically increases 
Espresso?s recall by a factor of 8.16. Furthermore, it is important to note that there are several 
other generic patterns, like [X?s Y], from which we expect a similar precision of 50% with a 
continual increase of recall. This is a very exciting avenue of further investigation. 
5. Conclusions 
We proposed a weakly supervised bootstrapping algorithm, called Espresso, for 
automatically extracting a wide variety of binary semantic relations from raw text. Given a 
small set of seed instances for a particular relation, the system learns reliable lexical patterns, 
applies them to extract new instances ranked by an information theoretic definition of 
reliability, and then uses the Web to filter and expand the instances. 
There are many avenues of future work. Preliminary results show that Espresso generates 
highly precise relations, but at the expense of lower recall. As mentioned above in Section 
4.3, we are working on improving system recall with a web-based method to identify generic 
patterns and filter their instances. Early results appear very promising. We also plan to 
investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here 
that negative instances will play a key role in determining the selectional restriction on 
generic patterns. 
Espresso is the first system, to our knowledge, to emphasize both minimal supervision and 
generality, both in identification of a wide variety of relations and in extensibility to various 
corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations 
harvested by Espresso, and if these relations can benefit NLP applications such as QA. 
Acknowledgements 
The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for 
evaluating the outputs of the systems. 
References 
[1] Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In Proceedings of ACL-1999. pp. 
57-64. College Park, MD. 
[2] Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 2003. Chemistry: The Central Science, Ninth 
Edition. Prentice Hall. 
[3] Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled noun hierarchy from text. In Proceedings 
of ACL-99. pp 120-126, Baltimore, MD. 
[4] Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. John Wiley & Sons. 
[5] Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; Robinson, P.; and Vilain, M. 1997. Mixed-initiative 
development of language processing systems. In Proceedings of ANLP-1997. Washington D.C. 
[6] Downey, D.; Etzioni, O.; and Soderland, S. 2005. A Probabilistic model of redundancy in information 
extraction. In Proceedings of IJCAI-2005. pp. 1034-1041. Edinburgh, Scotland. 
[7] Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; Shaked, T.; Soderland, S.; Weld, D.S.; and 
Yates, A. 2005. Unsupervised named-entity extraction from the Web: An experimental study. Artificial 
Intelligence, 165(1): 91-134. 
[8] Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press. 
[9] Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies for online question answering: 
Answering questions before they are asked. In Proceedings of ACL-03. pp. 1-7. Sapporo, Japan. 
[10] Geffet, M. and Dagan, I. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. In 
Proceedings of ACL-2005. Ann Arbor, MI. 
[11] Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic constraints for the automatic 
discovery of part-whole relations. In Proceedings of HLT/NAACL-03. pp. 80-87. Edmonton, Canada. 
[12] Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92. pp. 539-545. 
Nantes, France. 
[13] Justeson J.S. and Katz S.M. 1995. Technical Terminology: some linguistic properties and algorithms for 
identification in text. In Proceedings of ICCL-1995. pp.539-545. Nantes, France. 
[14] Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING-
94. pp. 42-48. Kyoto, Japan. 
[15] Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. In Proceedings of 
SemaNet? 02: Building and Using Semantic Networks, Taipei, Taiwan. 
[16] Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In Proceedings of 
HLT/NAACL-04. pp. 321-328. Boston, MA. 
[17] Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards terascale knowledge acquisition. In Proceedings of 
COLING-04. pp. 771-777. Geneva, Switzerland. 
[18] Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering. 
In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138-143. Pittsburgh, 
PA. 
[19] Pazienza M.T. 2000. A domain-specific terminology-extraction system. In Terminology, 5:2. 
[20] Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA. 
[21] Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings 
of EMNLP-1997. 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 238?247,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Entity Extraction via Ensemble Semantics
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA, 94089
pennac@yahoo-inc.com
Patrick Pantel
Yahoo! Labs
Sunnyvale, CA, 94089
ppantel@yahoo-inc.com
Abstract
Combining information extraction sys-
tems yields significantly higher quality re-
sources than each system in isolation. In
this paper, we generalize such a mixing of
sources and features in a framework called
Ensemble Semantics. We show very large
gains in entity extraction by combining
state-of-the-art distributional and pattern-
based systems with a large set of fea-
tures from a webcrawl, query logs, and
Wikipedia. Experimental results on a web-
scale extraction of actors, athletes and mu-
sicians show significantly higher mean av-
erage precision scores (29% gain) com-
pared with the current state of the art.
1 Introduction
Mounting evidence shows that combining infor-
mation sources and information extraction algo-
rithms leads to improvements in several tasks
such as fact extraction (Pas?ca et al, 2006), open-
domain IE (Talukdar et al, 2008), and entailment
rule acquisition (Mirkin et al, 2006). In this paper,
we show large gains in entity extraction by com-
bining state-of-the-art distributional and pattern-
based systems with a large set of features from
a 600 million document webcrawl, one year of
query logs, and a snapshot of Wikipedia. Further,
we generalize such a mixing of sources and fea-
tures in a framework called Ensemble Semantics.
Distributional and pattern-based extraction al-
gorithms capture aspects of paradigmatic and syn-
tagmatic dimensions of semantics, respectively,
and are believed to be quite complementary. Pas?ca
et al (2006) showed that filtering facts, extracted
by a pattern-based system, according to their ar-
guments? distributional similarity with seed facts
yielded large precision gains. Mirkin et al (2006)
showed similar gains on the task of acquiring lex-
ical entailment rules by exploring a supervised
combination of distributional and pattern-based al-
gorithms using an ML-based SVM classifier.
This paper builds on the above work, by study-
ing the impact of various sources of features exter-
nal to distributional and pattern-based algorithms,
on the task of entity extraction. Mirkin et al?s re-
sults are corroborated on this task and large and
significant gains over this baseline are obtained
by incorporating 402 features from a webcrawl,
query logs and Wikipedia. We extracted candidate
entities for the classes Actors, Athletes and Mu-
sicians from a webcrawl using a variant of Pas?ca
et al?s (2006) pattern-based engine and Pantel et
al.?s (2009) distributional extraction system. A
gradient boosted decision tree is used to learn a re-
gression function over the feature space for rank-
ing the candidate entities. Experimental results
show 29% gains (19% nominal) in mean average
precision over Mirkin et al?s method and 34%
gains (22% nominal) in mean average precision
over an unsupervised baseline similar to Pas?ca et
al.?s method. Below we summarize the contribu-
tions of this paper:
? We explore the hypothesis that although dis-
tributional and pattern-based algorithms are
complementary, they do not exhaust the se-
mantic space; other sources of evidence can
be leveraged to better combine them;
? We model the mixing of knowledge sources
and features in a novel and general informa-
tion extraction framework called Ensemble
Semantics; and
? Experiments over an entity extraction task
show that our model achieves large and sig-
nificant gains over state-of-the-art extractors.
A detailed analysis of feature correlations
and interactions shows that query log and we-
bcrawl features yield the highest gains, but
easily accessible Wikipedia features also im-
prove over current state-of-the-art systems.
238
Figure 1: The Ensemble Semantics framework for information extraction.
The remainder of this paper is organized as fol-
lows. In the next section, we present our Ensemble
Semantics framework and outline how various in-
formation extraction systems can be cast into the
framework. Section 3 then presents our entity ex-
traction system as an instance of Ensemble Se-
mantics, comparing and contrasting it with previ-
ous information extraction systems. Our experi-
mental methodology and analysis is described in
Section 4 and shows empirical evidence that our
extractor significantly outperforms prior art. Fi-
nally, Section 5 concludes with a discussion and
future work.
2 Ensemble Semantics
Ensemble Semantics (ES) is a general framework
for modeling information extraction algorithms
that combine multiple sources of information and
multiple extractors. The ES framework allows to:
? Represent multiple sources of knowledge and
multiple extractors of that knowledge;
? Represent multiple sources of features;
? Integrate both rule-based and ML-based
knowledge ranking algorithms; and
? Model previous information extraction sys-
tems (i.e., backwards compatibility).
2.1 ES Framework
ES can be instantiated to extract various types of
knowledge such as entities, facts, and lexical en-
tailment rules. It can also be used to better under-
stand the commonalities and differences between
existing information extraction systems.
After presenting the framework in the next sec-
tion, Section 2.2 shows how previous information
extraction algorithms can be cast into ES. In Sec-
tion 3 we describe our novel entity extraction al-
gorithm based on ES.
The ES framework is illustrated in Figure 1. It
decomposes the process of information extraction
into the following components:
Sources (S): textual repositories of information,
either structured (e.g., a database such as DBpe-
dia), semi-structured (e.g., Wikipedia Infoboxes or
HTML tables) or unstructured (e.g., news articles
or a webcrawl).
Knowledge Extractors (KE): algorithms re-
sponsible for extracting candidate instances such
as entities or facts. Examples include fact extrac-
tion systems such as (Cafarella et al, 2005) and
entity extraction systems such as (Pas?ca, 2007).
Feature Generators (FG): methods that extract
evidence (features) of knowledge in order to de-
cide which candidate instances extracted from
KEs are correct. Examples include capitalization
features for named entity extractors, and the dis-
tributional similarity matrix used in (Pas?ca et al,
2006) for filtering facts.
Aggregator (A). A module collecting and as-
sembling the instances coming from the different
extractors. This module keeps the footprint of
each instance, i.e. the number and the type of the
KEs that extracted the instance. This information
can be used by the Ranker module to build a rank-
ing strategy, as described below.
Ranker (R): a module for ranking the knowl-
edge instances returned from KEs using the fea-
tures generated by FGs. Ranking algorithms may
be rule-based (e.g., the one using a threshold on
distributional similarity in (Pas?ca et al, 2006)) or
ML-based (e.g., the SVM model in (Mirkin et al,
2006) for combining pattern-based and distribu-
tional features).
239
The Ranker is composed of two sub-modules:
the Modeler and the Decoder. The Modeler is re-
sponsible for creating the model which ranks the
candidate instances. The Decoder collects the can-
didate instances from the Aggregator, and applies
the model to produce the final ranking.
In rule-based systems, the Modeler corresponds
to a set of manually crafted or automatically in-
duced rules operating on the features (e.g. a com-
bination of thresholds). In ML-based systems, it is
an actual machine learning algorithm, that takes as
input a set of labeled training instances, and builds
the model according to their features. Training in-
stances can be obtained as a subset of those col-
lected by the Aggregator, or from some exter-
nal resource. In many cases, training instances
are manually labeled by human experts, through
a long and costly editorial process.
Information sources (S) serve as inputs to the
system. Some sources will serve as sources for
knowledge extractors to generate candidate in-
stances, some will serve as sources for feature gen-
erators to generate features or evidence of knowl-
edge, and some will serve as both.
2.2 Related Work
To date, most information extraction systems rely
on a model composed of a single source S, a single
extractor KE and a single feature generator FG.
For example, many classic relation extraction sys-
tems (Hearst, 1992; Riloff and Jones, 1999; Pan-
tel and Pennacchiotti, 2006; Pas?ca et al, 2006)
are based on a single pattern-based extractor KE,
which is seeded with a set of patterns or instances
for a given relation (e.g. the pattern ?X starred in
Y? for the act-in relation). The extractor then itera-
tively extracts new instances until a stop condition
is met. The resulting extractor scores are proposed
by FG as a feature. The Ranker simply consists
of a sorting function on the feature from FG.
Systems such as the above that do not consist
of multiple sources, knowledge extractors or fea-
ture generators are not considered Ensemble Se-
mantics models, even though they can be cast into
the framework. Recently, some researchers have
explored more complex systems, having multiple
sources, extractors and feature generators. Below
we show examples and describe how they map as
Ensemble Semantics systems. We use this charac-
terization to clearly outline how our proposed en-
tity extraction system, proposed in Section 3, dif-
fers from previous work.
Talukdar et al (2008) present a weakly-
supervised system for extracting large sets of
class-instance pairs using two knowledge extrac-
tors: a pattern-based extractor supported by distri-
butional evidence, which harvests candidate pairs
from a Web corpus; and a table extractor that har-
vests candidates from Web tables. The Ranker
uses graph random walks to combine the informa-
tion of the two extractors and output the final list.
The authors show large improvements in coverage
with little precision loss.
Mirkin et al (2006) introduce a machine learn-
ing system for extracting lists of lexical entail-
ments (e.g. ?government?? ?organization?). They
rely on two knowledge extractors, operating on a
same large textual source: a pattern-based extrac-
tor, leveraging the Hearst (1992) is-a patterns; and
a distributional extractor applied to a set of entail-
ment seeds. Candidate instances are passed to an
SVM Ranker, which uses features stemming from
the two extractors, to decide which instances are
output in the final list. The authors report a +9%
increase in F-measure over a rule-based system
that takes the union of the instances extracted by
the two modules.
Other examples include the system for
taxonomic-relation extraction by Cimiano et
al. (2005), using a pool of feature genera-
tors based on pattern-based, distributional
and WordNet techniques; and Pas?ca and Van
Durme?s (2008) system that uses a Web corpus
and query logs to extract semantic classes and
their attributes.
Similarly to these methods, our proposed entity
extractor (Section 3) utilizes multiple sources and
extractors. A key difference of our method lies in
the Feature Generator module. We propose sev-
eral generators resulting in 402 features extracted
from Web pages, query logs and Wikipedia arti-
cles. The use of these features results in dramatic
performance improvements, reported in Section 4.
3 ES for Entity Extraction
Entity extraction is a fundamental task in NLP
responsible for extracting instances of semantic
classes (e.g., ?Brad Pitt? and ?Tom Hanks? are in-
stances of the class Actors). It forms a build-
ing block for various NLP tasks such as on-
tology learning (Cimiano and Staab, 2004) and
co-reference resolution (Mc Carthy and Lehn-
240
Family Type Features
Web (w) Frequency (wF ) term frequency; document frequency; term frequency as noun phrase
Pattern (wP ) confidence score returned by KE
pat
; pmi with the 100 most reliable patterns
used by KE
pat
Distributional (wD) distributional similarity with the centroid in KE
dis
; distributional similarities
with each seed in S
Termness (wT ) ratio between term frequency as noun phrase and term frequency; pmi between
internal tokens of the instance; capitalization ratio
Query log (q) Frequency (qF ) number of queries matching the instance; number of queries containing the in-
stance
Co-occurrence (qC) query log pmi with any seed in S
Pattern (qP ) pmi with a set of trigger words T (i.e., the 10 words in the query logs with
highest pmi with S)
Distributional (qD) distributional similarity with S (vector coordinates consist of the instance?s pmi
with the words in T )
Termness (qT ) ratio between the two frequency features F
Web table (t) Frequency (tF ) table frequency
Co-occurrence (tC) table pmi with S; table pmi with any seed in S
Wikipedia (k) Frequency (kF ) term frequency
Co-occurrence (kC) pmi with any seed in S
Distributional (kD) distributional similarity with S
Table 1: Feature space describing each candidate instance (S indicates the set of seeds for a given class).
ert, 2005). Search engines such as Yahoo, Live,
and Google collect large sets of entities (Pas?ca,
2007; Chaudhuri et al, 2009) to better interpret
queries (Tan and Peng, 2006), to improve query
suggestions (Cao et al, 2008) and to understand
query intents (Hu et al, 2009). Entity extraction
differs from the similar task of named entity ex-
traction, in that classes are more fine-grained and
possibly overlapping.
Below, we propose a new method for entity ex-
traction built on the ES framework (Section 3.1).
Then, we comment on related work in entity ex-
traction (Section 3.2).
3.1 ES Entity Extraction Model
In this section, we propose a novel entity ex-
traction model following the Ensemble Semantics
framework presented in Section 2. The sources of
our systems can come from any textual corpus. In
our experiments (described in Section 4.1), we ex-
tracted entities from a large crawl of the Web, and
generated features from this crawl as well as query
logs and Wikipedia.
3.1.1 Knowledge extractors
Our system relies on two knowledge extractors:
one pattern-based and the other distributional.
Pattern-based extractor (KE
pat
). We reimple-
mented Pas?ca et al?s (2006) state-of-the-art web-
scale fact extractor, which, given seed instances of
a binary relation, finds instances of that relation.
We extract entities of a class, such as Actors, by
instantiating typical relations involving that class
such as act-in(Actor, Movie). We instantiate such
relations instead of the classical is-a patterns since
these have been shown to bring in too many false
positives, see (Pantel and Pennacchiotti, 2006) for
a discussion of such generic patterns. The extrac-
tor?s confidence score for each instance is used by
the Ranker to score the entities being extracted.
Section 4.1 lists the system parameters we used in
our experiments.
Distributional extractor (KE
dis
). We use Pan-
tel et al?s (2009) distributional entity extractor.
For each noun in our source corpus, we build a
context vector consisting of the noun chunks pre-
ceding and following the target noun, scored us-
ing pointwise mutual information (pmi). Given
a small set of seed entities S of a class, the ex-
tractor computes the centroid of the seeds? context
vectors as a geometric mean, and then returns all
nouns whose similarity with the centroid exceeds a
threshold ? (using the cosine measure between the
context vectors). Full algorithmic details are pre-
sented in (Pantel et al, 2009). Section 4.1 lists the
threshold and text preprocessing algorithms used
in our experiments.
The Aggregator simply takes a union of the en-
tities discovered by the two extractors.
3.1.2 Feature generators
Our model includes four feature generators,
which compute a total of 402 features (full set
described in Table 1). Each generator extracts
from a specific source a feature family, as follows:
? Web (w): a body of 600 million documents
241
crawled from the Web at Yahoo! in 2008;
? Query logs (q): one year of web search
queries issued to the Yahoo! search engine;
? Web tables: all HTML inner tables extracted
from the above Web source; and
? Wikipedia: an official Wikipedia dump from
February 2008, consisting of about 2 million
articles.
Feature families are further subclassified into
five types: frequency (F) (frequency-based fea-
tures); co-occurrence (C) (features capturing first
order co-occurrences between an instance and
class seeds); distributional (D) (features based on
the distributional similarity between an instance
and class seeds); pattern (P) (features indicat-
ing class-specific lexical pattern matches); and
termness (T) (features used to distinguish well-
formed terms such as ?Brad Pitt? from ill-formed
ones such as ?with Brad Pitt?). The seeds S used
in many of the feature families are the same seeds
used by the KE
pat
extractor, described in Sec-
tion 3.1.1.
The different seed families are designed to cap-
ture different semantic aspects: paradigmatic (D),
syntagmatic (C and P), popularity (F), and term
cohesiveness (T).
3.1.3 ML-based Ranker
Our Modeler adopts a supervised ML regression
model. Specifically, we use a Gradient Boosted
Decision Tree regression model - GBDT (Fried-
man, 2001), which consists of an ensemble of de-
cision trees, fitted in a forward step-wise manner
to current residuals. Friedman (2001) shows that
by drastically easing the problem of overfitting on
training data (which is common in boosting al-
gorithms), GDBT competes with state-of-the-art
machine learning algorithms such as SVM (Fried-
man, 2006) with much smaller resulting models
and faster decoding time. The model is trained
on a manually annotated random sample of enti-
ties taken from the Aggregator, using the features
generated by the four generators presented in Sec-
tion 3.1.2. The Decoder then ranks each entity ac-
cording to the trained model.
3.2 Related Work
Entity extraction systems follow two main ap-
proaches: pattern-based and distributional. The
pattern-based approach leverages lexico-syntactic
patterns to extract instances of a given class. Most
commonly used are is-a pattern families such as
those first proposed by Hearst (1992) (e.g., ?Y such
as X? for matching ?actors such as Brad Pitt?).
Minimal supervision is used in the form of small
sets of manually provided seed patterns or seed in-
stances. This approach is very common in both
the NLP and Semantic Web communities (Cimi-
ano and Staab, 2004; Cafarella et al, 2005; Pantel
and Pennacchiotti, 2006; Pas?ca et al, 2006).
The distributional approach uses contextual ev-
idence to model the instances of a given class,
following the distributional hypothesis (Harris,
1964). Weakly supervised, these methods take a
small set of seed instances (or the class label) and
extract new instances from noun phrases that are
most similar to the seeds (i.e., that share similar
contexts). Following Lin (1998), example sys-
tems include Fleischman and Hovy (2002), Cimi-
ano and Volker (2005), Tanev and Magnini (2006),
and Pantel et al (2009).
4 Experimental Evaluation
This section reports our experiments, showing the
effectiveness of our entity extraction system and
the importance of our different feature families.
4.1 Experimental Setup
Evaluated classes. We evaluate our system over
three classes: Actors (movie, tv and stage ac-
tors); Athletes (professional and amateur); Musi-
cians (singers, musicians, composers, bands, and
orchestras)
System setup. We instantiated our knowledge
extractors, KE
pat
and KE
dis
from Section 3.1.1,
over our Web crawl of 600 million documents (see
Section 3.1.2). The documents were preprocessed
using Brill?s POS-tagger (Brill, 1995) and the Ab-
ney?s chunker (Abney, 1991). ForKE
dis
, context
vectors are extracted for noun phrases recognized
as NP-chunks with removed modifiers. The vec-
tor space includes the 250M most frequent noun
chunks in the corpus. KE
dis
returns as instances
all noun phrases having a similarity with the seeds?
centroid above ? = 0.005
1
. The sets of seeds S
for KE
dis
include 10, 24 and 10 manually chosen
instances for respectively the Actors, Athletes and
Musicians classes
2
. The sets of seedsP forKE
pat
1
Experimentally set on an independent development set.
2
The higher number of seeds for Athletes is chosen to
cover different sports.
242
Dataset Actors Athletes Musicians
KE
pat
58,005 40,816 125,657
KE
dis
72,659 24,380 24,593
KE
pat
?KE
dis
113,245 61,709 142,694
KE
pat
?KE
dis
17,419 3,487 7,556
R 500 500 500
P=80 P=258 P=134
N=420 N=242 N=366
Table 2: Number of extracted instances and the
sample sizes (P and N indicate positive and neg-
ative annotations).
include 11, 8 and 9 pairs respectively for the Ac-
tors (relation acts-in), Athletes (relation plays-for)
and Musicians (relation part-of-band) classes. Ta-
ble 6 lists all seeds for both KE
dis
and KE
pat
.
The GBDT ranker uses an ensemble of 300 trees.
3
Goldset Preparation. The number of instances
extracted by KE
pat
and KE
dis
for each class
is reported in Table 2. For each class, we ex-
tract a random sample R of 500 instances from
KE
pat
?KE
dis
. A pool of 10 paid expert editors
annotated the instances of each class inR as posi-
tive or negative. Inter-annotator overlap was 0.88.
Uncertain instances were manually adjudicated by
a separate paid expert editor, yielding a gold stan-
dard dataset for each class.
Evaluation Metrics. Entity extraction perfor-
mance is evaluated using the average precision
(AP) statistic, a standard information retrieval
measure for evaluating ranking algorithms, de-
fined as:
AP (L) =
?
|L|
i=1
P (i) ? corr(i)
?
|L|
i=1
corr(i)
(1)
where L is a ranked list produced by an extractor,
P (i) is the precision of L at rank i, and corr(i) is 1
if the instance at rank i is correct, and 0 otherwise.
AP is computed overR for each class.
We also evaluate the coverage, i.e. the percent-
age of instances extracted by a system wrt those
extracted by all systems.
In order to accurately compute statistical signif-
icance, our experiments are performed using 10-
fold cross validation.
Baselines and comparisons. We compare our
proposed ES entity extractor, using different fea-
ture configurations, with state-of-the-art systems
(referred to as baselines B* below):
3
GBDT model parameters were experimentally set on an
independent development set as follows: trees=300, shrink-
age=0.01, max nodes per tree=12, sample rate=0.5.
System Actors Athletes Musicians
AP Cov AP Cov AP Cov
B1 0.729 51.2% 0.616 66.1% 0.570 88.1%
B2 0.618 64.1% 0.687 39.5% 0.681 17.2%
B3 0.676 100% 0.664 100% 0.576 100%
B4 0.715 100% 0.697 100% 0.579 100%
ES-all 0.860? 100% 0.915? 100% 0.788? 100%
Table 3: Average precision (AP) and coverage
(Cov) results for our proposed system ES-all and
the baselines. ? indicates AP statistical signifi-
cance at the 0.95 level wrt all baselines.
ES-all. Our ES system, usingKE
pat
andKE
dis
,
the full set of feature families described in
Section 3.1.2, and the GBDT ranker.
B1. KE
pat
alone, a state-of-the-art pattern-
based extractor reimplementing (Pas?ca et al,
2006), where the Ranker assigns scores to in-
stances using the confidence score returned
by KE
pat
.
B2. KE
dis
alone, a state-of-the-art distributional
system implementing (Pantel et al, 2009),
where the Ranker assigns scores to instances
using the similarity score returned by KE
dis
alone.
B3. A rule-based ES system, combining B1 and
B2. This system uses bothKE
pat
andKE
dis
as extractors, and a Ranker that assigns
scores to instances according to the sum of
their normalized confidence scores.
B4. A state-of-the-art machine learning system
based on (Mirkin et al, 2006). This ES
system uses KE
pat
and KE
dis
as extractors.
The Ranker is a GBDT regression model,
using the full sets of features derived from
the two extractors, i.e., wP and wD (see
Table 1). GBDT parameters are set as for our
proposed ES-all system.
4.2 Experimental Results
Table 3 summarizes the average-precision (AP)
and coverage results for our ES-all system and the
baselines. Figure 2 reports the precision at each
rank for the Athletes class (the other two classes
follow similar trends). Table 6 lists the top-10 en-
tities discovered for each class on one test fold.
ES-all outperforms all baselines in AP (all results
are statistically significant at the 0.95 level), offer-
ing at the same time full coverage
4
.
4
Recall that coverage is reported relative to all instances
retrieved by extractors KE
pat
and KE
dis
.
243
Figure 2: Precision at rank for the different sys-
tems on the Athletes class.
Our simple rule-based combination baseline,
B3, leads to a large increase in coverage wrt the in-
dividual extractors alone (B1 and B2) without sig-
nificant impact on precision. The supervised ML-
based combination baseline (B4) consistently im-
proves AP across classes wrt the rule-based com-
bination (B3), but without statistical significance.
These results corroborate those found in (Mirkin et
al., 2006), where this ML-based combination was
reported to be significantly better than a rule-based
one on the task of lexical entailment acquisition.
The large set of features adopted in ES-all ac-
counts for a dramatic improvement in AP, indicat-
ing that existing state-of-the-art systems for entity
extraction (reflected by our baselines strategies)
are not making use of enough semantic cues. The
adoption of evidence other than distributional and
pattern-based, such as features coming from web
documents, HTML tables and query logs, is here
demonstrated to be highly valuable.
The above empirical claim can be grounded and
corroborated by a deeper semantic analysis. From
a semantic perspective, the above results translate
in the observation that distributional and pattern-
based evidence do not completely capture all se-
mantic aspects of entities. Other evidence, such as
popularity, term cohesiveness and co-occurrences
capture other aspects. For instance, in one of our
Actors folds, the B3 system ranks the incorrect in-
stance ?Tom Sellek? (a misspelling of ?Tom Sel-
leck?) in 9
th
position (out of 142), while ES-all
lowers it to the 33
rd
position, by relying on table-
based features (intuitively, tables contain much
fewer misspelling than running text). Other than
misspellings, ES-all fixes errors that are either typ-
ical of distributional approaches, such as the in-
clusion of instances of other classes (e.g. the
movie ?Someone Like You? often appears in con-
texts similar to those of actors); errors typical
of pattern-based approaches, such as incorrect in-
System AP MAP
Actors Athletes Musicians
B3 0.676 0.664 0.576 0.639
B4 0.715 0.697 0.579 0.664
B4+w 0.813
?
0.908
?
0.724
?
0.815
?
B4+q 0.815
?
0.905
?
0.743
?
0.821
?
B4+t 0.784
?
0.825
?
0.727
?
0.779
?
B4+k 0.776
?
0.825
?
0.624 0.741
?
B4+w+q 0.835
?
0.915
?
0.758
?
0.836
?
B4+w+t 0.840
?
0.906
?
0.774
?
0.840
?
B4+w+k 0.814
?
0.903
?
0.725
?
0.814
?
B4+q+t 0.847
?
0.910
?
0.774
?
0.844
?
B4+q+k 0.832
?
0.906
?
0.748
?
0.829
?
B4+t+k 0.817
?
0.861
?
0.743
?
0.807
?
B4+w+q+t 0.846
?
0.917
?
0.782
?
0.849
?
B4+w+q+k 0.841
?
0.916
?
0.756
?
0.838
?
B4+w+t+k 0.835
?
0.906
?
0.783
?
0.841
?
Es-all 0.860
?
0.915
?
0.788
?
0.854
?
Table 4: Overall AP results of the different feature
configurations, compared to two baselines. ? in-
dicates statistical significance at the 0.95 level wrt
B3. ? indicates statistical significance at 0.95 level
wrt both B3 and B4.
stances highly-associated with an ambiguous pat-
tern (e.g., the pattern ?X of the rock band Y? for
finding Musicians matched an incorrect instance
?song submission?); or errors typical of both, such
as the inclusion of common nouns (e.g. ?country
music hall?) or too generic last names (e.g. ?John-
son?). ES-all successfully recovers all these error
by using termness, co-occurrence and frequency
features.
We also compare ES-all with a state-of-the-art
random walk system (RW) presented by Talukdar
et al (2008) (see Section 2.2 for a description).
As we could not reimplement the system, we re-
port the following indirect comparison. RW was
evaluated on five entity classes, one of which, NFL
players, overlaps with our Athletes class. On this
class, they report 0.95 precision on the top-100
ranked entities. Unfortunately, they do not report
coverage or recall statistics, making the interpre-
tation of this analysis difficult. In an attempt to
compare RW with ES-all, we evaluated the preci-
sion of our top-100 Athletes, obtaining 0.99. Us-
ing a random sample of our extracted Athletes, we
approximate the precision of the top-22,000 Ath-
letes to be 0.97? 0.01 (at the 0.95 level).
4.3 Feature Analysis
Feature family analysis: Table 4 reports the av-
erage precision (AP) for our system using different
feature family combinations (see Table 1). Col-
umn 1 reports the family combinations; columns
244
2-4 report AP for each class; and column 5 reports
the mean-average-precision (MAP) across classes.
In all configurations, except the k family alone,
and along all classes, our system significantly out-
performs (at the 0.95 level) the baselines.
Rows 3-6 report the performance of each fea-
ture family alone. w and t are consistently better
than q and k, across all classes. k is shown to be
the least useful family. This is mostly due to data
sparseness, e.g., in our experiments almost 40%
of the test instances in the Actors sample do not
have any occurrence in Wikipedia. However, with-
out access to richer resources such as a webcrawl
or query logs, the features from k do indeed pro-
vide large gains over current baselines (on average
+10.2% and +7.7% over B3 and B4).
Rows 7-12 report results for combinations of
two feature families. All combinations (except
those with k) appear valuable, substantially in-
creasing the single-family results in rows 3-6, in-
dicating that combining different feature families
(as suggested by the ES paradigm) is helpful. Sec-
ond, it indicates that q, w and t convey comple-
mentary information, thus boosting the regression
model when combined together. It is interesting to
notice that q+t tends to be the best combination,
surprising given that t alone did not show high per-
formance (row 5). One would expect the combina-
tion q+w to outperform q+t, but the good perfor-
mance of q+t is mainly due to the fact that these
two families are more complementary than q+w.
To verify this intuition, we compute the Spearman
correlation coefficient r among the rankings pro-
duced by the different combinations. As expected,
q and w have a higher correlation (r = 0.82) than
q and t (r = 0.67) and w and t (r = 0.66), suggest-
ing that q and w tend to subsume each other (i.e.
no added information for the regression model).
Rows 13-15 report results for combinations of
three feature families. As expected, the best
combination is q+w+t with an average precision
nearly identical to the full ES-all system. If one
has access to Web or query log sources, then the
value of the Wikipedia features tends to be sub-
sumed by our web and query log features.
Feature by feature analysis: The feature fam-
ilies analyzed in the previous section consist of
402 features. For each trained GBDT model,
one can inspect the resulting most important fea-
tures (Friedman, 2001). Consistently, the two
most important features for ES-all are, as ex-
System AP MAP
Actors Athletes Musicians
B4 0.715 0.697 0.579 0.664
B4+w 0.813 0.908 0.724 0.815
B4+wF 0.798 0.865 0.679 0.781
B4+wT 0.806 0.891 0.717 0.805
B4+t 0.784 0.825 0.727 0.779
B4+tF 0.760 0.802 0.701 0.781
B4+tC 0.771 0.815 0.718 0.805
B4+q 0.815 0.905 0.743 0.821
B4+qF 0.786 0.890 0.693 0.790
B4+qC 0.715 0.738 0.581 0.678
B4+qD 0.735 0.709 0.644 0.696
B4+qP 0.779 0.796 0.648 0.741
B4+qT 0.780 0.868 0.725 0.791
B4+qF+qW+qT 0.816 0.906 0.743 0.822
ES-all 0.860 0.915 0.788 0.854
Table 5: Ablation study of the web (w), query-
log (q) and table (t) features (bold letters indicate
whole feature families).
pected, the confidence scores of KE
pat
and
KE
dis
. This suggests that syntagmatic and
paradigmatic information are most important in
defining the semantics of entities. Also very im-
portant, in third position, is a feature from qT ,
namely the ratio between the number of queries
matching the instance and the number of queries
containing it as a substring. This feature is a strong
indicator of termness.
Webcrawl term frequencies and document fre-
quencies (from the wF set) are also important.
Very frequent and infrequent instances were found
to be often incorrect (e.g., respectively ?song? and
?Brad Pitttt?). Table PMI (a feature in the qC fam-
ily) also ranked high in importance: instances that
co-occurr very frequently in the same column/row
with seeds S are often found to be correct (e.g.,
a column containing the seeds ?Brad Pitt? and
?Tom Hanks? will likely contains other actors).
Other termness (T ), frequency-based (F ) and co-
occurrence (C) features also play some role in the
model.
Variable importance is only an intrinsic indi-
cator of feature relevance. In order to better as-
sess the actual impact of the single features on
AP, we ran our system on each feature type: re-
sults for the web (w), query log (q) and table (t)
families are reported in Table 5. For reason of
space constraints, we here only focus on some
high level observations. The set of web termness
features (wT ) and frequency features (wF ) are
alone able to provide a large improvement over B4
(row 1), while their combination (row 2) does not
improve much over the features taken individually.
245
Seed instances forKE
dis
Actors Athletes Musicians
Jodie Foster Bob Gibson Jared Allen Randy Moss Rise Against the Machine
Humphrey Bogart Don Drysdale Andres Romero Peyton Manning Pink Floyd
Anthony Hopkins Albert Pujols Kenny Perry Jerry Rice Spice Girls
Katharine Hepburn Yogi Berra Martin Kaymer Robert Karlsson Pussycat Dolls
Christopher Walken Dejan Bodiroga Alexander Ovechkin Gheorghe Hagi The Beatles
Gene Hackman Allen Iverson Shea Weber Marco Van Basten Iron Maiden
Diane Keaton Yao Ming Patrick Roy Zinedine Zidane John Lennon
Edward Norton Tim Duncan Alexei Kovalev Roberto Baggio Frank Sinatra
Robert Duvall Led Zeppelin
Hilary Swank Freddie Mercury
Seed instances forKE
pat
Actors Athletes Musicians
Dennis Hopper - The Good Life Dallas cowboys - Julius Crosslin Kevin Brown - Corndaddy
Tom Hanks - The Terminal New york Giants - Plaxico Burress Barry Gibb - The Bee Gees
Julia Roberts - Mona Lisa Smile Philadelphia Eagles - Danny Amendola Patty Smyth - Scandal
Kevin Bacon - Footloose Washington Redskins - Rock Cartwright Dave Matthews - Dave Mathews Band
Keanu Reeves - The Lake House New England Patriots - Laurence Maroney Gwen Stefani - No Doubt
Marlon Brando - Don Jaun Demarco Buffalo Bills - Xavier Omon George Michael - Wham
Morgan Freeman - The Shawshank Redemption Miami Dolphins - Ernest Wilford Mark Knopfler - Dire Straits
Nicole Kidman - Eyes Wide Shut New York Jets - Chansi Stuckey Brian Jones - The Rolling Stones
Al Pacino - The Godfather Pete Shelley - Buzzcocks
Johnny Depp - Chocolat
Halle Berry - Monster?s Ball
10-best ranked instances in one test fold
Actors Athletes Musicians
Gordon Tootoosis Ron Randell Rumeal Robinson Todd Warriner Colin Newman Wu-tang Clan
Rosalind Chao Alimi Ballard Jeff Mcinnis Hong-chih Kuo Ghost Circus Tristan Prettyman
John Hawkes Fernando Lamas Ahmad Nivins Leon Clarke Ray Dorset Top Cats
Jeffrey Dean Morgan Bruno Cremer Carlos Marchena Josh Dollard Plastic Tree *Roseanne
George Macready Muhammad Bakri Chad Kreuter Robbie Alomar *Doomwatch John Moen
Table 6: Listing of all seeds used for KE
dis
and KE
pat
, as well as the top-10 entities discovered by
ES-all on one of our test folds.
This suggests that wT and wF capture very simi-
lar information: they are indeed highly correlated
(r = 0.80). Rows 5-7 refer to web table features:
the features tC outperform and subsume the fre-
quency features tF (r = 0.92). For query log
features (rows 8-14), only qF , qP and qT signif-
icantly increase performance. Distributional and
co-occurrence features (qD and qC) have very low
effect, as they are mostly subsumed by the others.
The combination of qF , qP and qT (row 14) per-
forms as well as the whole q (row 8).
Experiment conclusions: From our experi-
ments, we can draw the following conclusions:
1. Wikipedia features taken alone outperform
the baselines, however, web and query log
features, if available, subsume Wikipedia fea-
tures;
2. q, t and w are all important, and should be
used in combination, as they drive mostly in-
dependent information;
3. the syntagmatic and paradigmatic informa-
tion conveyed by the two extractors are most
relevant, and can be significantly boosted by
adding frequency- and termness-based fea-
tures from other sources.
5 Conclusions and Future Work
In this paper, we presented a general informa-
tion extraction framework, called Ensemble Se-
mantics, for combining multiple sources of knowl-
edge, and we instantiated the framework to build
a novel ML-based entity extraction system. The
system significantly outperforms state-of-the-art
ones by up to 22% in mean average precision.
We provided an in-depth analysis of the impact of
our proposed 402 features, their feature families
(Web documents, HTML tables, query logs, and
Wikipedia), and feature types.
There is ample directions for future work. On
entity extraction, exploring more knowledge ex-
tractors from different sources (such as the HTML
tables and query log sources used for our features)
is promising. Other feature types may potentially
capture other aspects of the semantics of entities,
such as WordNet and search engine click logs. For
the ranking system, semi- or weakly-supervised
algorithms may provide competing performance
to our model with reduced manual labor. Finally,
there are many opportunities for applying the gen-
eral Ensemble Semantics framework to other in-
formation extraction tasks such as fact extraction
and event extraction.
246
References
Steven Abney. 1991. Learning taxonomic rela-
tions from heterogeneous sources of evidence. In
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4).
Michael J. Cafarella, Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. KnowItNow: Fast,
scalable information extraction from the web. In
Proceedings of EMNLP-2005.
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen
Liao, Enhong Chen, and Hang Li. 2008. Context-
aware query suggestion by mining click-through and
session data. In Proceedings of KDD-08, pages
875?883.
Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms
for entities. In Proceedings of WWW-09, pages 151?
160.
Philipp Cimiano and Steffen Staab. 2004. Learning by
googling. SIGKDD Explorations, 6(2):24?34.
Philipp Cimiano and Johanna Volker. 2005. To-
wards large-scale, open-domain and ontology-based
named entity classification. In Proceedings of
RANLP-2005, pages 166?172.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-
Thieme, and Steffen Staab. 2005. Learning taxo-
nomic relations from heterogeneous sources of ev-
idence. In Ontology Learning from Text: Meth-
ods, Evaluation and Applications, pages 59?73. IOS
Press.
Michael Fleischman and Eduard Hovy. 2002. Classi-
fication of named entities. In Proceedings of COL-
ING 2002.
Jerome H. Friedman. 2001. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29(5):1189?1232.
Jerome H. Friedman. 2006. Recent advances in pre-
dictive (machine) learning. Journal of Classifica-
tion, 23(2):175?197.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING-92, pages 539?545.
Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun,
and Zheng Chen. 2009. Understanding user?s query
intent with Wikipedia. In Proceedings of WWW-09,
pages 471?480.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL-
98.
Joseph F. Mc Carthy and Wendy G Lehnert. 2005. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI-1995, pages 1050?1055.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
Proceedings of ACL/COLING-06, pages 579?586.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proceedings of ACL-08, pages 19?27.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the world wide web of facts - step one:
The one-million fact extraction challenge. In Pro-
ceedings of AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Pro-
ceedings of CIKM-07, pages 683?690, New York,
NY, USA.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: A Bootstrapping Algorithm for Automat-
ically Harvesting Semantic Relations. In Proceed-
ings of ACL-2006, Sydney, Australia.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09, Singapore.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In Proceedings of AAAI-99, pages
474?479.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP-08, pages 582?
590.
Bin Tan and Fuchun Peng. 2006. Unsupervised
query segmentation using generative language mod-
els and wikipedia. In Proceedings of WWW-06,
pages 1400?1405.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of EACL-2006.
247
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938?947,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
 
Web-Scale Distributional Similarity and Entity Set Expansion 
Patrick Pantel?, Eric Crestan?, Arkady Borkovsky?, Ana-Maria Popescu?, Vishnu Vyas? 
?Yahoo! Labs 
Sunnyvale, CA 94089 
{ppantel,ecrestan}@yahoo-inc.com 
{amp,vishnu}@yahoo-inc.com 
?Yandex Labs 
Burlingame, CA 94010 
arkady@yandex-team.ru 
  
 
Abstract 
Computing the pairwise semantic similarity 
between all words on the Web is a compu-
tationally challenging task. Parallelization 
and optimizations are necessary. We pro-
pose a highly scalable implementation 
based on distributional similarity, imple-
mented in the MapReduce framework and 
deployed over a 200 billion word crawl of 
the Web. The pairwise similarity between 
500 million terms is computed in 50 hours 
using 200 quad-core nodes. We apply the 
learned similarity matrix to the task of au-
tomatic set expansion and present a large 
empirical study to quantify the effect on 
expansion performance of corpus size, cor-
pus quality, seed composition and seed 
size. We make public an experimental 
testbed for set expansion analysis that in-
cludes a large collection of diverse entity 
sets extracted from Wikipedia. 
1 Introduction 
Computing the semantic similarity between terms 
has many applications in NLP including word clas-
sification (Turney and Littman 2003), word sense 
disambiguation (Yuret and Yatbaz 2009), context-
spelling correction (Jones and Martin 1997), fact 
extraction (Pa?ca et al 2006), semantic role labe-
ling (Erk 2007), and applications in IR such as 
query expansion (Cao et al 2008) and textual ad-
vertising (Chang et al 2009). 
For commercial engines such as Yahoo! and 
Google, creating lists of named entities found on 
the Web is critical for query analysis, document 
categorization, and ad matching. Computing term 
similarity is typically done by comparing co-
occurrence vectors between all pairs of terms 
(Sarmento et al 2007). Scaling this task to the 
Web requires parallelization and optimizations. 
In this paper, we propose a large-scale term si-
milarity algorithm, based on distributional similari-
ty, implemented in the MapReduce framework and 
deployed over a 200 billion word crawl of the 
Web. The resulting similarity matrix between 500 
million terms is applied to the task of expanding 
lists of named entities (automatic set expansion). 
We provide a detailed empirical analysis of the 
discovered named entities and quantify the effect 
on expansion accuracy of corpus size, corpus 
quality, seed composition, and seed set size. 
2 Related Work 
Below we review relevant work in optimizing si-
milarity computations and automatic set expansion. 
2.1 Computing Term Similarities 
The distributional hypothesis (Harris 1954), which 
links the meaning of words to their contexts, has 
inspired many algorithms for computing term simi-
larities (Lund and Burgess 1996; Lin 1998; Lee 
1999; Erk and Pad? 2008; Agirre et al 2009). 
Brute force similarity computation compares all 
the contexts for each pair of terms, with complexi-
ty O(n2m) where n is the number of terms and m is 
the number of possible contexts. More efficient 
strategies are of three kinds: 
938
Smoothing: Techniques such as Latent Semantic 
Analysis reduce the context space by applying 
truncated Singular Value Decomposition (SVD) 
(Deerwester et al 1990). Computing the matrix 
decomposition however does not scale well to 
web-size term-context matrices. Other currently 
unscalable smoothing techniques include Probabil-
istic Latent Semantic Analysis (Hofmann 1999), 
Iterative Scaling (Ando 2000), and Latent Dirichlet 
Allocation (Blei et al 2003). 
Randomized Algorithms: Randomized tech-
niques for approximating various similarity meas-
ures have been successfully applied to term simi-
larity (Ravichandran et al 2005; Gorman and Cur-
ran 2006). Common techniques include Random 
Indexing based on Sparse Distributed Memory 
(Kanerva 1993) and Locality Sensitive Hashing 
(Broder 1997). 
Optimizations and Distributed Processing: 
Bayardo et al (2007) present a sparse matrix opti-
mization strategy capable of efficiently computing 
the similarity between terms which?s similarity 
exceeds a given threshold. Rychl? and Kilgarriff 
(2007), Elsayed et al (2008) and Agirre et al 
(2009) use reverse indexing and the MapReduce 
framework to distribute the similarity computa-
tions across several machines. Our proposed ap-
proach combines these two strategies and efficient-
ly computes the exact similarity (cosine, Jaccard, 
Dice, and Overlap) between all pairs. 
2.2 Entity extraction and classification 
Building entity lexicons is a task of great interest 
for which structured, semi-structured and unstruc-
tured data have all been explored (GoogleSets; 
Sarmento et al 2007; Wang and Cohen 2007; Bu-
nescu and Mooney 2004; Etzioni et al 2005; Pa?ca 
et al 2006). Our own work focuses on set expan-
sion from unstructured Web text. Apart from the 
choice of a data source, state-of-the-art entity ex-
traction methods differ in their use of numerous, 
few or no labeled examples, the open or targeted 
nature of the extraction as well as the types of fea-
tures employed. Supervised approaches (McCal-
lum and Li 2003, Bunescu and Mooney 2004) rely 
on large sets of labeled examples, perform targeted 
extraction and employ a variety of sentence- and 
corpus-level features. While very precise, these 
methods are typically used for coarse grained enti-
ty classes (People, Organizations, Companies) for 
which large training data sets are available. Unsu-
pervised approaches rely on no labeled data and 
use either bootstrapped class-specific extraction 
patterns (Etzioni et al 2005) to find new elements 
of a given class (for targeted extraction) or corpus-
based term similarity (Pantel and Lin 2002) to find 
term clusters (in an open extraction framework). 
Finally, semi-supervised methods have shown 
great promise for identifying and labeling entities 
(Riloff and Shepherd 1997; Riloff and Jones 1999; 
Banko et al 2007; Downey et al 2007; Pa?ca et al 
2006; Pa?ca 2007a; Pa?ca 2007b; Pa?ca and Durme 
2008). Starting with a set of seed entities, semi-
supervised extraction methods use either class-
specific patterns to populate an entity class or dis-
tributional similarity to find terms similar to the 
seed set (Pa?ca?s work also examines the advan-
tages of combining these approaches). Semi-
supervised methods (including ours) are useful for 
extending finer grain entity classes, for which large 
unlabeled data sets are available. 
2.3 Impact of corpus on system performance 
Previous work has examined the effect of using 
large, sometimes Web-size corpora, on system per-
formance in the case of familiar NLP tasks. Banko 
and Brill (2001) show that Web-scale data helps 
with confusion set disambiguation while Lapata 
and Keller (2005) find that the Web is a good 
source of n-gram counts for unsupervised models. 
Atterer and Schutze (2006) examine the influence 
of corpus size on combining a supervised approach 
with an unsupervised one for relative clause and 
PP-attachment. Etzioni et al (2005) and Pantel et 
al. (2004) show the advantages of using large 
quantities of generic Web text over smaller corpora 
for extracting relations and named entities. Overall, 
corpus size and quality are both found to be impor-
tant for extraction. Our paper adds to this body of 
work by focusing on the task of similarity-based 
set expansion and providing a large empirical 
study quantify the relative corpus effects. 
2.4 Impact of seeds on extraction performance 
Previous extraction systems report on the size and 
quality of the training data or, if semi-supervised, 
the size and quality of entity or pattern seed sets. 
Narrowing the focus to closely related work, Pa?ca 
(2007a; 2007b) and Pa?ca and Durme (2008) show 
the impact of varying the number of instances rep-
resentative of a given class and the size of the 
attribute seed set on the precision of class attribute 
extraction. An example observation is that good 
939
quality class attributes can still be extracted using 
20 or even 10 instances to represent an entity class. 
Among others, Etzioni et al (2005) shows that a 
small pattern set can help bootstrap useful entity 
seed sets and reports on the impact of seed set 
noise on final performance. Unlike previous work, 
empirically quantifying the influence of seed set 
size and quality on extraction performance of ran-
dom entity types is a key objective of this paper. 
3 Large-Scale Similarity Model 
Term semantic models normally invoke the distri-
butional hypothesis (Harris 1985), which links the 
meaning of terms to their contexts. Models are 
built by recording the surrounding contexts for 
each term in a large collection of unstructured text 
and storing them in a term-context matrix. Me-
thods differ in their definition of a context (e.g., 
text window or syntactic relations), or by a means 
to weigh contexts (e.g., frequency, tf-idf, pointwise 
mutual information), or ultimately in measuring 
the similarity between two context vectors (e.g., 
using Euclidean distance, Cosine, Dice). 
In this paper, we adopt the following methodol-
ogy for computing term similarity. Our various 
web crawls, described in Section 6.1, are POS-
tagged using Brill?s tagger (1995) and chunked 
using a variant of the Abney chunker (Abney 
1991). Terms are NP chunks with some modifiers 
removed; their contexts (i.e., features) are defined 
as their rightmost and leftmost stemmed chunks. 
We weigh each context f using pointwise mutual 
information (Church and Hanks 1989). Let PMI(w) 
denote a pointwise mutual information vector, con-
structed for each term as follows: PMI(w) = (pmiw1, 
pmiw2, ?, pmiwm), where pmiwf is the pointwise 
mutual information between term w and feature f: 
 
???
?=
==
m
j
wj
n
i
if
wf
wf
cc
Nc
pmi
11
log
 
where cwf is the frequency of feature f occurring for 
term w, n is the number of unique terms and N is 
the total number of features for all terms. 
Term similarities are computed by comparing 
these pmi context vectors using measures such as 
cosine, Jaccard, and Dice. 
3.1 Large-Scale Implementation  
Computing the similarity between terms on a large 
Web crawl is a non-trivial problem, with a worst 
case cubic running time ? O(n2m) where n is the 
number of terms and m is the dimensionality of the 
feature space. Section 2.1 introduces several opti-
mization techniques; below we propose an algo-
rithm for large-scale term similarity computation 
which calculates exact scores for all pairs of terms, 
generalizes to several different metrics, and is scal-
able to a large crawl of the Web. 
Our optimization strategy follows a generalized 
sparse-matrix multiplication approach (Sarawagi 
and Kirpal 2004), which is based on the well-
known observation that a scalar product of two 
vectors depends only on the coordinates for which 
both vectors have non-zero values. Further, we 
observe that most commonly used similarity scores 
for feature vectors x
r
 and y
r
, such as cosine and 
Dice, can be decomposed into three values: one 
depending only on features of x
r
, another depend-
ing only on features of y
r
, and the third depending 
on the features shared both by x
r
 and y
r
. More for-
mally, commonly used similarity scores ( )yxF rr,  
can be expressed as: 
 ( ) ( ) ( ) ( )??
???
?= ? yfxfyxffyxF
i
ii
rrrr
3210 ,,,,
 
Table 1 defines f0, f1, f2, and f3 for some common 
similarity functions. For each of these scores, f2 = 
f3. In our work, we compute all of these scores, but 
report our results using only the cosine function. 
Let A and B be two matrices of PMI feature vec-
tors. Our task is to compute the similarity between 
all vectors in A and all vectors in B. In computing 
the similarity between all pairs of terms, A = B. 
Figure 1 outlines our algorithm for computing 
the similarity between all elements of A and B. Ef-
ficient computation of the similarity matrix can be 
achieved by leveraging the fact that ( )yxF rr,  is de-
termined solely by the features shared by x
r
 and y
r
 
(i.e., f1(0,x) = f1(x,0) = 0 for any x) and that most of 
Table 1. Definitions for f0, f1, f2, and f3 for commonly used 
similarity scores. 
METRIC ( )zyxf ,,0  ( )yxf ,1  ( ) ( )xfxf rr 32 =  
Overlap x  1 0  
Jaccard* xzy
x
?+
 ( )yx ,min  ?
i
ix
Dice* 
zy
x
+
2
 
yx ?  ?
i
ix
2
Cosine zy
x
?
 
yx ?  ?
i
ix
2
*weighted generalization  
 
940
the feature vectors are very sparse (i.e., most poss-
ible contexts never occur for a given term). In this 
case, calculating f1(x, y) is only required when both 
feature vectors have a shared non-zero feature, sig-
nificantly reducing the cost of computation. De-
termining which vectors share a non-zero feature 
can easily be achieved by first building an inverted 
index for the features. The computational cost of 
this algorithm is ? 2iN , where Ni is the number of 
vectors that have a non-zero ith coordinate. Its 
worst case time complexity is O(ncv) where n is 
the number of terms to be compared, c is the max-
imum number of non-zero coordinates of any vec-
tor, and v is the number of vectors that have a non-
zero ith coordinate where i is the coordinate which 
is non-zero for the most vectors. In other words, 
the algorithm is efficient only when the density of 
the coordinates is low. On our datasets, we ob-
served near linear running time in the corpus size. 
Bayardo et al (2007) described a strategy that 
potentially reduces the cost even further by omit-
ting the coordinates with the highest number of 
non-zero value. However, their algorithm gives a 
significant advantage only when we are interested 
in finding solely the similarity between highly sim-
ilar terms. In our experiments, we compute the ex-
act similarity between all pairs of terms. 
Distributed Implementation 
The pseudo-code in Figure 1 assumes that A can fit 
into memory, which for large A may be impossible. 
Also, as each element of B is processed indepen-
dently, running parallel processes for non-
intersecting subsets of B makes the processing 
faster. In this section, we outline our MapReduce 
implementation of Figure 1 deployed using Ha-
doop1, the open-source software package imple-
menting the MapReduce framework and distri-
buted file system. Hadoop has been shown to scale 
to several thousands of machines, allowing users to 
write simple ?map? and ?reduce? code, and to 
seamlessly manage the sophisticated parallel ex-
ecution of the code. A good primer on MapReduce 
programming is in (Dean and Ghemawat 2008). 
Our implementation employs the MapReduce 
model by using the Map step to start M?N Map 
tasks in parallel, each caching 1/Mth part of A as 
an inverted index and streaming 1/Nth part of B 
through it. The actual inputs are read by the tasks 
                                                 
1 Hadoop, http://lucene.apache.org/hadoop/ 
directly from HDFS (Hadoop Distributed File Sys-
tem). Each part of A is processed N times, and each 
part of B is processed M times. M is determined by 
the amount of memory dedicated for the inverted 
index, and N should be determined by trading off 
the fact that as N increases, more parallelism can 
be obtained at the increased cost of building the 
same inverse index N times. 
The similarity algorithm from Figure 1 is run in 
each task of the Map step of a MapReduce job. 
The Reduce step is used to group the output by bi. 
4 Application to Set Expansion 
Creating lists of named entities is a critical prob-
lem at commercial engines such as Yahoo! and 
Google. The types of entities to be expanded are 
often not known a priori, leaving supervised clas-
sifiers undesirable. Additionally, list creators typi-
cally need the ability to expand sets of varying 
granularity. Semi-supervised approaches are pre-
dominantly adopted since they allow targeted ex-
pansions while requiring only small sets of seed 
entities. State-of-the-art techniques first compute 
term-term similarities for all available terms and 
then select candidates for set expansion from 
amongst the terms most similar to the seeds (Sar-
mento et al 2007). 
Input: Two matrices A and B of feature vectors. 
## Build an inverted index for A (optimiza- 
## tion for data sparseness) 
AA = an empty hash-table 
for i in (1..n): 
   F2[i] = f2(A[i]) ## cache values of f2(x) 
   for k in non-zero features of A[i]: 
      if k not in AA: AA[k] = empty-set 
      ## append <vector-id, feature-value> 
      ## pairs to the set of non-zero 
      ## values for feature k 
      AA[k].append( (i,A[i,k]) ) 
## Process the elements of B 
for b in B: 
   F1 = {} ## the set of Ai that have non-
zero similarity with b 
   for k in non-zero features of b: 
      for i in AA[k]: 
         if i not in sim: sim[i] = 0 
         F1[i] += f1( AA[k][i], b[k]) 
   F3 = f3(b) 
   for i in sim: 
      print i, b, f0( F1[i], F2[i], F3) 
Output: A matrix containing the similarity between 
all elements in A and in B. 
Figure 1. Similarity computation algorithm. 
941
Formally, we define our expansion task as: 
Task Definition: Given a set of seed entities S = 
{s1, s2, ?, sk} of a class C = {s1, s2, ?, sk, ?,, sn} and 
an unlabeled textual corpus T, find all members of 
the class C. 
For example, consider the class of Bottled Water 
Brands. Given the set of seeds S = {Volvic, San 
Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our 
task is to find all other members of this class, such 
as {Agua Vida, Apenta, Culligan, Dasani, Ethos 
Water, Iceland Pure Spring Water, Imsdal, ?} 
4.1 Set Expansion Algorithm 
Our goal is not to propose a new set expansion al-
gorithm, but instead to test the effect of using our 
Web-scale term similarity matrix (enabled by the 
algorithm proposed in Section 3) on a state-of-the-
art distributional set expansion algorithm, namely 
(Sarmento et al 2007). 
We consider S as a set of prototypical examples 
of the underlying entity set. A representation for 
the meaning of S is computed by building a feature 
vector consisting of a weighted average of the fea-
tures of its seed elements s1, s2, ?, sk, a centroid. For 
example, given the seed elements {Volvic, San Pel-
legrino, Gerolsteiner Brunnen, Bling H2O}, the 
resulting centroid consists of (details of the feature 
extraction protocol are in Section 6.1): 
brand, mineral water, monitor, 
lake, water, take over, ? 
Centroids are represented in the same space as 
terms allowing us to compute the similarity be-
tween centroids and all terms in our corpus. A 
scored and ranked set for expansion is ultimately 
generated by sorting all terms according to their 
similarity to the seed set centroid, and applying a 
cutoff on either the similarity score or on the total 
number of retrieved terms. In our reported experi-
ments, we expanded over 22,000 seed sets using 
our Web similarity model from Section 3. 
5 Evaluation Methodology 
In this section, we describe our methodology for 
evaluating Web-scale set expansion. 
5.1 Gold Standard Entity Sets 
Estimating the quality of a set expansion algorithm 
requires a random sample from the universe of all 
entity sets that may ever be expanded, where a set 
represents some concept such as Stage Actors. An 
approximation of this universe can be extracted 
from the ?List of? pages in Wikipedia2. 
Upon inspection of a random sample of the ?List 
of? pages, we found that several lists were compo-
sitions or joins of concepts, for example ?List of 
World War II aces from Denmark? and ?List of 
people who claimed to be God?. We addressed this 
issue by constructing a quasi-random sample as 
follows. We randomly sorted the list of every noun 
occurring in Wikipedia2. Then, for each noun we 
verified whether or not it existed in a Wikipedia 
list, and if so we extracted this list. If a noun be-
longed to multiple lists, the authors chose the list 
that seemed most appropriate. Although this does 
not generate a perfect random sample, diversity is 
ensured by the random selection of nouns and rele-
vancy is ensured by the author adjudication. 
The final gold standard consists of 50 sets, in-
cluding: classical pianists, Spanish provinces, 
Texas counties, male tennis players, first ladies, 
cocktails, bottled water brands, and Archbishops of 
Canterbury. For each set, we then manually 
scraped every instance from Wikipedia keeping 
track also of the listed variants names. 
The gold standard is available for download at: 
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=sse-
gold/wikipedia.20071218.goldsets.tgz 
The 50 sets consist on average of 208 instances 
(with a minimum of 11 and a maximum of 1,116) 
for a total of 10,377 instances. 
5.2 Trials 
In order to analyze the corpus and seed effects on 
performance, we created 30 copies of each of the 
50 sets and randomly sorted each copy. Then, for 
each of the 1500 copies, we created a trial for each 
of the following 23 seed sizes: 1, 2, 5, 10, 20, 30, 
40, ?, 200. Each trial of seed size s was created by 
taking the first s entries in each of the 1500 random 
copies. For sets that contained fewer than 200 
items, we only generated trials for seed sizes 
                                                 
2 In this paper, extractions from Wikipedia are taken 
from a snapshot of the resource in December 2008. 
942
smaller than the set size. The resulting trial dataset 
consists of 20,220 trials3. 
5.3 Judgments 
Set expansion systems consist of an expansion al-
gorithm (such as the one described in Section 4.1) 
as well as a corpus (such as Wikipedia, a news 
corpus, or a web crawl). For a given system, each 
of the 20,220 trials described in the previous sec-
tion are expanded. In our work, we limited the total 
number of system expansions, per trial, to 1000. 
Before judgment of an expanded set, we first 
collapse each instance that is a variant of another 
(determined using the variants in our gold stan-
dard) into one single instance (keeping the highest 
system score)4. Then, each expanded instance is 
judged as correct or incorrect automatically 
against the gold standard described in Section 5.1. 
5.4 Analysis Metrics 
Our experiments in Section 6 consist of precision 
vs. recall or precision vs. rank curves, where: 
a) precision is defined as the percentage of correct 
instances in the expansion of a seed set; and 
b) recall is defined as the percentage of non-seed 
gold standard instances retrieved by the system. 
Since the gold standard sets vary significantly in 
size, we also provide the R-precision metric to 
normalize for set size: 
c) R-precision is defined as the average precision 
of all trials where precision is taken at rank R = 
{size of trial?s associated gold standard set}, 
thereby normalizing for set size. 
                                                 
3 Available for download at http://www.patrickpantel.com/cgi-
bin/Web/Tools/getfile.pl?type=data&id=sse-gold/wikipedia.20071218.trials.tgz. 
4 Note also that we do not allow seed instances nor their 
variants to appear in an expansion set. 
For the above metrics, 95% confidence bounds are 
computed using the randomly generated samples 
described in Section 5.2. 
6 Experimental Results 
Our goal is to study the performance gains on set 
expansion using our Web-scale term similarity al-
gorithm from Section 3. We present a large empir-
ical study quantifying the importance of corpus 
and seeds on expansion accuracy. 
6.1 Experimental Setup 
We extracted statistics to build our model from 
Section 3 using four different corpora, outlined in 
Table 2. The Wikipedia corpus consists of a snap-
shot of the English articles in December 20085. 
The Web100 corpus consists of an extraction from 
a large crawl of the Web, from Yahoo!, of over 
600 million English webpages. For each crawled 
document, we removed paragraphs containing 
fewer than 50 tokens (as a rough approximation of 
the narrative part of a webpage) and then removed 
all duplicate sentences. The resulting corpus con-
sists of over 200 billion words. The Web020 cor-
pus is a random sample of 1/5th of the sentences in 
Web100 whereas Web004 is a random sample of 
1/25th of Web100. 
For each corpus, we tagged and chunked each 
sentence as described in Section 3. We then com-
puted the similarity between all noun phrase 
chunks using the model of Section 3.1. 
6.2 Quantitative Analysis 
Our proposed optimization for term similarity 
computation produces exact scores (unlike rando-
mized techniques) for all pairs of terms on a large 
Web crawl. For our largest corpus, Web100, we 
computed the pairwise similarity between over 500 
million words in 50 hours using 200 four-core ma-
chines. Web004 is of similar scale to the largest 
reported randomized technique (Ravichandran et 
al. 2005). On this scale, we compute the exact si-
milarity matrix in a little over two hours whereas 
Ravichandran et al (2005) compute an approxima-
tion in 570 hours. On average they only find 73% 
                                                 
5 To avoid biasing our Wikipedia corpus with the test 
sets, Wikipedia ?List of? pages were omitted from our 
statistics as were any page linked to gold standard list 
members from ?List of? pages. 
Table 2. Corpora used to build our expansion models.
CORPORA 
UNIQUE 
SENTENCES 
(MILLIONS) 
TOKENS 
(MILLIONS) 
UNIQUE 
WORDS 
(MILLIONS) 
Web100 5,201 217,940 542 
Web020? 1040 43,588 108 
Web004? 208 8,717 22 
Wikipedia6 30 721 34 
?Estimated from Web100 statistics. 
 
943
of the top-1000 similar terms of a random term 
whereas we find all of them. 
For set expansion, experiments have been run on 
corpora as large as Web004 and Wikipedia (Sar-
mento et al 2007), a corpora 300 times smaller 
than our Web crawl. Below, we compare the ex-
pansion accuracy of Sarmento et al (2007) on Wi-
kipedia and our Web crawls. 
Figure 2 illustrates the precision and recall tra-
deoff for our four corpora, with 95% confidence 
intervals computed over all 20,220 trials described 
in Section 4.2. Table 3 lists the resulting R-
precision along with the system precisions at ranks 
25, 50, and 100 (see Figure 2 for detailed precision 
analysis). Why are the precision scores so low? 
Compared with previous work that manually select 
entity types for expansion, such as countries and 
companies, our work is the first to evaluate over a 
large set of randomly selected entity types. On just 
the countries class, our R-Precision was 0.816 us-
ing Web100. 
The following sections analyze the effects of 
various expansion variables: corpus size, corpus 
quality, seed size, and seed quality. 
6.2.1 Corpus Size and Corpus Quality Effect 
Not surprisingly, corpus size and quality have a 
significant impact on expansion performance. Fig-
ure 2 and Table 3 quantify this expectation. On our 
Web crawl corpora, we observe that the full 200+ 
billion token crawl (Web100) has an average R-
precision 13% higher than 1/5th of the crawl 
(Web020) and 53% higher than 1/25th of the crawl. 
Figure 2 also illustrates that throughout the full 
precision/recall curve, Web100 significantly out-
performs Web020, which in turn significantly out-
performs Web004. 
The higher text quality Wikipedia corpus, which 
consists of roughly 60 times fewer tokens than 
Web020, performs nearly as well as Web020 (see 
Figure 2). We omitted statistics from Wikipedia 
?List of? pages in order to not bias our evaluation 
to the test set described in Section 5.1. Inspection 
of the precision vs. rank graph (omitted for lack of 
space) revealed that from rank 1 thru 550, Wikipe-
dia had the same precision as Web020. From rank 
550 to 1000, however, Wikipedia?s precision 
dropped off significantly compared with Web020, 
accounting for the fact that the Web corpus con-
tains a higher recall of gold standard instances. The 
R-precision reported in Table 3 shows that this 
precision drop-off results in a significantly lower 
R-precision for Wikipedia compared with Web020. 
6.2.2  The Effect of Seed Selection 
Intuitively, some seeds are better than others. We 
study the impact of seed selection effect by in-
specting the system performance for several ran-
domly selected seed sets of fixed size and we find 
that seed set composition greatly affects perfor-
mance. Figure 3 illustrates the precision vs. recall 
tradeoff on our best performing corpus Web100 for 
30 random seed sets of size 10 for each of our 50 
gold standard sets (i.e., 1500 trials were tested.) 
Each of the trials performed better than the average 
system performance (the double-lined curve lowest 
in Figure 3). Distinguishing between the various 
data series is not important, however important to 
notice is the very large gap between the preci-
sion/recall curves of the best and worst performing 
random seed sets. On average, the best performing 
seed sets had 42% higher precision and 39% higher 
recall than the worst performing seed set. Similar 
Table 3. Corpora analysis: R-precision and Precision at var-
ious ranks. 95% confidence bounds are all below 0.005?. 
CORPORA R-PREC PREC@25 PREC@50 PREC@100 
Web100 0.404 0.407 0.347 0.278 
Web020 0.356 0.377 0.319 0.250 
Web004 0.264 0.353 0.298 0.239 
Wikipedia 0.315 0.372 0.314 0.253 
?95% confidence bounds are computed over all trials described in Section 5.2. 
Figure 2. Corpus size and quality improve performance. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0 0.1 0.2 0.3 0.4 0.5 0.6
Re
ca
ll
Precision
Corpora?Analysis
(Precision?vs.?Recall)
Web100
Web020
Web004
Wikipedia
CORPORA R-PREC PREC@25 PREC@50 PREC@100 
Web100 0.404 0.407 0.347 0.278 
Web020 0.356 0.377 0.319 0.250 
Web004 0.264 0.353 0.298 0.239 
Wikipedia 0.315 0.372 0.314 0.253 
?95% confidence bounds are computed over all trials described in Section 5.2. 
944
curves were observed for inspected seed sets of 
size 5, 20, 30, and 40. 
Although outside of the scope of this paper, we 
are currently investigating ways to automatically 
detect which seed elements are better than others in 
order to reduce the impact of seed selection effect. 
6.2.3 The Effect of Seed Size 
Here we aim to confirm, with a large empirical 
study, the anecdotal claims in (Pa?ca and Durme 
2008) that few seeds are necessary. We found that 
a) very small seed sets of size 1 or 2 are not suffi-
cient for representing the intended entity set; b) 5-
20 seeds yield on average best performance; and c) 
surprisingly, increasing the seed set size beyond 
20 or 30 on average does not find any new correct 
instances. 
We inspected the effect of seed size on R-
precision over the four corpora. Each seed size 
curve is computed by averaging the system per-
formance over the 30 random trials of all 50 sets. 
For each corpus, R-precision increased sharply 
from seed size 1 to 10 and the curve flattened out 
for seed sizes larger than 20 (figure omitted for 
lack of space). Error analysis on the Web100 cor-
pus shows that once our model has seen 10-20 
seeds, the distributional similarity model seems to 
have enough statistics to discover as many new 
correct instances as it could ever find. Some enti-
ties could never be found by the distributional si-
milarity model since they either do not occur or 
infrequently occur in the corpus or they occur in 
contexts that vary a great deal from other set ele-
ments. Figure 4 illustrates this behavior by plotting 
for each seed set size the rate of increase in discov-
ery of new correct instances (i.e., not found in 
smaller seed set sizes). 
We see that most gold standard instances are 
discovered with the first 5-10 seeds. After the 30th 
seed is introduced, no new correct instances are 
found. An important finding is that the error rate 
does not increase with increased seed set size (see 
Figure 5). This study shows that only few seeds 
(10-20) yield best performance and that adding 
more seeds beyond this does not on average affect 
performance in a positive or negative way. 
Figure 3. Seed set composition greatly affects system performance (with 30 different seed samples of size 10). 
Figure 4. Few new instances are discovered with more 
than 5-20 seeds on Web100 (with 95% confidence). 
Figure 5. Percentage of errors does not increase as 
seed size increases on Web100 (with 95% confidence).
0
0.5
1
1.5
2
2.5
3
0 20 40 60 80 100 120 140 160 180 200
Ra
te
?o
f?N
ew
?C
or
re
ct
?
Seed?Size
Rate?of?New?Correct?Expansions
vs.?Seed?Size
0
0.2
0.4
0.6
0.8
1
0 20 40 60 80 100 120 140 160 180 200
%
?o
f?E
rr
or
Seed?Size
Seed?Size?vs.?%?of?Errors
0
0.2
0.4
0.6
0.8
0 0.2 0.4 0.6 0.8 1
Re
ca
ll
Precision
Web100:?Seed?Selection?Effect
Precision?vs.?Recall
Web100 s010
s010.t01 s010.t02
s010.t03 s010.t04
s010.t05 s010.t06
s010.t07 s010.t08
s010.t09 s010.t10
s010.t11 s010.t12
s010.t13 s010.t14
s010.t15 s010.t16
s010.t17 s010.t18
s010.t19 s010.t20
s010.t21 s010.t22
s010.t23 s010.t24
s010.t25 s010.t26
s010.t27 s010.t28
s010.t29 s010.t30
945
7 Conclusion  
We proposed a highly scalable term similarity al-
gorithm, implemented in the MapReduce frame-
work, and deployed over a 200 billion word crawl 
of the Web. The pairwise similarity between 500 
million terms was computed in 50 hours using 200 
quad-core nodes. We evaluated the impact of the 
large similarity matrix on a set expansion task and 
found that the Web similarity matrix gave a large  
performance boost over a state-of-the-art expan-
sion algorithm using Wikipedia. Finally, we re-
lease to the community a testbed for experimental-
ly analyzing automatic set expansion, which in-
cludes a large collection of nearly random entity 
sets extracted from Wikipedia and over 22,000 
randomly sampled seed expansion trials.  
References 
Abney, S. Parsing by Chunks. In: Robert Berwick, Ste-
ven Abney and Carol Tenny (eds.), Principle-Based 
Parsing. Kluwer Academic Publishers, Dordrecht. 
1991. 
Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pasca, 
M.; and Soroa, A.. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proceedings of NAACL HLT 09. 
Ando, R. K. 2000. Latent semantic space: Iterative scal-
ing improves precision of interdocument similarity 
measurement. In Proceedings of SIGIR-00. pp. 216?
223. 
Atterer, M. and Schutze, H., 2006. The Effect of Corpus 
Size when Combining Supervised and Unsupervised 
Training for Disambiguation. In Proceedings of ACL-
06. 
Banko, M. and Brill, E. 2001. Mitigating the paucity of 
data problem. In Proceedings of HLT-2001. San Di-
ego, CA. 
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.; 
Etzioni, O. 2007. Open Information Extraction from 
the Web. In Proceedings of IJCAI. 
Bayardo, R. J.; Ma, Y.; Srikant, R. 2007. Scaling Up 
All-Pairs Similarity Search. In Proceedings of WWW-
07. pp. 131-140. Banff, Canada. 
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent 
Dirichlet Allocation. Journal of Machine Learning 
Research, 3:993?1022. 
Brill, E. 1995. Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
Broder, A. 1997. On the resemblance and containment 
of documents. In Compression and Complexity of 
Sequences. pp. 21-29. 
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In 
Proceedings of ACL-04, pp. 438-445. 
Cao, H.; Jiang, D.; Pei, J.; He, Q.; Liao, Z.; Chen, E.; 
and Li, H. 2008. Context-aware query suggestion by 
mining click-through and session data. In Proceed-
ings of KDD-08. pp. 875?883. 
Chang, W.; Pantel, P.; Popescu, A.-M.; and Gabrilovich, 
E. 2009. Towards intent-driven bidterm suggestion. 
In Proceedings of WWW-09 (Short Paper), Madrid, 
Spain. 
Church, K. and Hanks, P. 1989. Word association 
norms, mutual information, and lexicography. In 
Proceedings of ACL89. pp. 76?83. 
Dean, J. and Ghemawat, S. 2008. MapReduce: Simpli-
fied Data Processing on Large Clusters. Communica-
tions of the ACM, 51(1):107-113. 
Deerwester, S. C.; Dumais, S. T.; Landauer, T. K.; Fur-
nas, G. W.; and Harshman, R. A. 1990. Indexing by 
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41(6):391?407. 
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating 
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07.  
Elsayed, T.; Lin, J.; Oard, D. 2008. Pairwise Document 
Similarity in Large Collections with MapReduce. In 
Proceedings of ACL-08: HLT, Short Papers (Com-
panion Volume). pp. 265?268. Columbus, OH. 
Erk, K. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL-07. pp. 
216?223. Prague, Czech Republic. 
Erk, K. and Pad?, S. 2008. A structured vector space 
model for word meaning in context. In Proceedings 
of EMNLP-08. Honolulu, HI. 
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; 
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. 
Unsupervised named-entity extraction from the Web: 
An Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Gorman, J. and Curran, J. R. 2006. Scaling distribution-
al similarity to large corpora. In Proceedings of ACL-
06. pp. 361-368. 
946
Harris, Z. 1985. Distributional Structure. In: Katz, J. J. 
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Hofmann, T. 1999. Probabilistic Latent Semantic Index-
ing. In Proceedings of SIGIR-99. pp. 50?57, Berke-
ley, California. 
Kanerva, P. 1993. Sparse distributed memory and re-
lated models. pp. 50-76. 
Lapata, M. and Keller, F., 2005. Web-based Models for 
Natural Language Processing, In ACM Transactions 
on Speech and Language Processing (TSLP), 2(1). 
Lee, Lillian. 1999. Measures of Distributional Similarity. 
In Proceedings of ACL-93. pp. 25-32. College Park, 
MD. 
Lin, D. 1998. Automatic retrieval and clustering of 
similar words. In Proceedings of COLING/ACL-98. 
pp. 768?774. Montreal, Canada. 
Lund, K., and Burgess, C. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments, 
and Computers, 28(2):203?208. 
McCallum, A. and Li, W. Early Results for Named 
Entity Recognition with Conditional Random Fields, 
Feature Induction and Enhanced Lexicons. In Pro-
ceedings of CoNLL-03. 
McQueen, J. 1967. Some methods for classification and 
analysis of multivariate observations. In Proceedings 
of 5th Berkeley Symposium on Mathematics, Statistics 
and Probability, 1:281?298. 
Pa?ca, M. 2007a. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM-07. pp. 683-690. 
Pa?ca, M. 2007b. Organizing and Searching the World 
Wide Web of Facts ? Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07. 
Pa?ca, M. and Durme, B.J. 2008. Weakly-supervised 
Acquisition of Open-Domain Classes and Class 
Attributes from Web Documents and Query Logs. In 
Proceedings of ACL-08. 
Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 
2006. Names and Similarities on the Web: Fast Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006. pp. 113-120. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses 
from Text. In Proceedings of KDD-02. pp. 613-619. 
Edmonton, Canada. 
Pantel, P.; Ravichandran, D; Hovy, E.H. 2004. Towards 
terascale knowledge acquisition. In proceedings of 
COLING-04. pp 771-777. 
Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Ran-
domized algorithms and NLP: Using locality sensi-
tive hash function for high speed noun clustering. In 
Proceedings of ACL-05. pp. 622-629. 
Riloff, E. and Jones, R. 1999 Learning Dictionaries for 
Information Extraction by Multi-Level Boostrapping. 
In Proceedings of AAAI/IAAAI-99. 
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of EMNLP-97. 
Rychl?, P. and Kilgarriff, A. 2007. An efficient algo-
rithm for building a distributional thesaurus (and oth-
er Sketch Engine developments). In Proceedings of 
ACL-07, demo sessions. Prague, Czech Republic. 
Sarawagi, S. and Kirpal, A. 2004. Efficient set joins on 
similarity predicates. In Proceedings of SIGMOD '04. 
pp. 74 ?754. New York, NY. 
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 
2007. ?More like these?: growing entity classes from 
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal. 
Turney, P. D., and Littman, M. L. 2003. Measuring 
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4). 
Wang, R.C. and Cohen, W.W. 2008. Iterative Set Ex-
pansion of Named Entities using the Web. In Pro-
ceedings of ICDM 2008. Pisa, Italy. 
Wang. R.C. and Cohen, W.W. 2007 Language-
Independent Set Expansion of Named Entities Using 
the Web. In Proceedings of ICDM-07. 
Yuret, D., and Yatbaz, M. A. 2009. The noisy channel 
model for unsupervised word sense disambiguation. 
Computational Linguistics. Under review. 
 
947
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 681?688
Manchester, August 2008
A Joint Information Model for n-best Ranking 
Patrick Pantel 
Yahoo! Inc. 
Santa Clara, CA 95054 
me@patrickpantel.com 
Vishnu Vyas 
USC Information Sciences Institute 
Marina del Rey, CA 
vishnu@isi.edu 
 
Abstract 
In this paper, we present a method for 
modeling joint information when gene-
rating n-best lists. We apply the method 
to a novel task of characterizing the simi-
larity of a group of terms where only a 
small set of many possible semantic 
properties may be displayed to a user. 
We demonstrate that considering the re-
sults jointly, by accounting for the infor-
mation overlap between results, generates 
better n-best lists than considering them 
independently. We propose an informa-
tion theoretic objective function for mod-
eling the joint information in an n-best 
list and show empirical evidence that 
humans prefer the result sets produced by 
our joint model. Our results show with 
95% confidence that the n-best lists gen-
erated by our joint ranking model are 
significantly different from a baseline in-
dependent model 50.0% ? 3.1% of the 
time, out of which they are preferred 
76.6% ? 5.2% of the time. 
1 Introduction 
Ranking result sets is a pervasive problem in the 
NLP and IR communities, exemplified by key-
word search engines such as Google (Brin and 
Page 1998), machine translation systems (Zhang 
et al 2006), and recommender systems (Sharda-
nand and Maes 1995; Resnick and Varian 1997). 
Consider the lexical semantics task of explain-
ing why a set of terms are similar: given a set of 
terms and a large set of possible explanations for 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
their similarity, one must choose only the best n 
explanations to display to a user. There are many 
ways to explain why terms are similar2; one way 
is to list the semantic properties that are shared 
by the terms. For example, consider the follow-
ing set of terms corresponding to fruit names: 
 {apple, ume, pawpaw, quince} 
Example semantic properties that could be 
used to explain their similarity include: they are 
products, they can be eaten, they are solid (but 
not they are companies, for example). The list of 
such semantic properties can be very large and 
some are much more informative than others. For 
example, the property can-be-eaten is much 
more informative of the similarity of {apple, ume, 
pawpaw, quince} than the property is-solid. Us-
ing a simple measure of association between 
properties and queries, explained in detail later in 
this paper, one can rank each property and obtain 
the following three highest scoring properties for 
explaining the similarity of these terms: 
{they are products, they can be 
imported, they can be exported} 
Even though can be imported and can be ex-
ported are highly ranked explanations, taken 
jointly, once we know one the other does not of-
fer much more information since most things that 
can be imported can also be exported. In other 
words, there is a large overlap in information 
between the two properties. A more informative 
set of explanations could be obtained by replac-
ing one of these two properties with a property 
that scored lower but had less information over-
lap with the others, for example: 
                                                 
2 In (Vyas and Pantel 2008), we explore the task of 
explaining the similarity between terms in detail. In 
this paper, we focus on the task of choosing the best 
set of explanations given a set of candidates. 
681
{they are products, they can be 
imported, they can be eaten} 
Even though, taken alone, the property can be 
eaten may not be as informative as can be ex-
ported, it does indeed add more information to 
the explanation set when considered jointly with 
the other explanations. 
In this paper, we propose an information theo-
retic objective function for modeling the joint 
information in an n-best list. Derived using con-
ditional self-information, we measure the amount 
of information that each property contributes to a 
query. Intuitively, when adding a new property to 
a result set, we should prefer a property that con-
tributes the maximum amount of information to 
the existing set. In our experiments, we show 
empirical evidence that humans prefer our joint 
model?s result sets on the task of explaining why 
a set of terms are similar. 
The remainder of this paper is organized as 
follows. In the next section, we review related 
literature and position our contribution within 
that landscape. Section 3 presents the task of ex-
plaining the similarity of a set of terms and de-
scribes a method for generating candidate expla-
nations from which we will apply our ranking 
model.  In Section 4, we formally define our 
ranking task and present our Joint Information 
Ranking model. Experimental results are pre-
sented in Section 5 and finally, we conclude with 
a discussion and future work. 
2 Related Work 
There are a vast number of applications of 
ranking and its importance to the commercial 
success at companies such as Google and Yahoo 
have fueled a great deal of research in recent 
years. In this paper, we investigate one particular 
aspect of ranking, the importance of considering 
the results in an n-best list jointly because of the 
information overlap issues described in the 
introduction, and one particular application, 
namely explaining why a set of terms are similar. 
Considering results jointly is not a new idea 
and is very similar to the concept of diversity-
based ranking introduced in the IR community 
by Carbonell and Goldstein (1998). In short, se-
lecting an n-best list is a balancing act between 
maximizing the relevance of the list and the in-
formation novelty of its results. One commonly 
used approach is to define a measure of novel-
ty/semantic similarity between documents and to 
apply heuristics to reduce the relevance score of 
a result item (a hit) by a function of the similarity 
of this item to other results in the list (Carbonell 
and Goldstein 1998; Zhu et al 2007). Another 
common approach is to cluster result documents 
according to their semantic similarity and present 
clusters to users instead of individual documents 
(Hearst and Pedersen 1996; Leuski 2001; Liu and 
Croft 2004). In this paper, we argue that the bal-
ance between relevance and novelty can be cap-
tured by a formal model that maximizes the joint 
information content of a result set. Instead of 
ranking documents in an IR setting, we focus in 
this paper on a new task of selecting the best se-
mantic properties that describe the similarity of a 
set of query terms. 
By no means an exhaustive list, the most 
commonly cited ranking and scoring algorithms 
are HITS (Kleinberg 1998) and PageRank (Page 
et al 1998), which rank hyperlinked documents 
using the concepts of hubs and authorities. The 
most well-known keyword scoring methods 
within the IR community are the tf-idf (Salton 
and McGill 1983) and pointwise mutual informa-
tion (Church and Hanks 1989) measures, which 
put more importance on matching keywords that 
occur frequently in a document relative to the 
total number of documents that contain the key-
word (by normalizing term frequencies with in-
verse document frequencies). Various methods 
including tf-idf have been comparatively eva-
luated by Salton and Buckley (1987). Creating n-
best lists using the above algorithms produce 
result sets where each result is considered inde-
pendently. In this paper, we investigate the utility 
of considering the result sets jointly and compare 
our joint method to a pointwise mutual informa-
tion model. 
Within the NLP community, n-best list rank-
ing has been looked at carefully in parsing, ex-
tractive summarization (Barzilay et al 1999; 
Hovy and Lin 1998), and machine translation 
(Zhang et al 2006), to name a few. The problem 
of learning to rank a set of objects by combining 
a given collection of ranking functions using 
boosting techniques is investigated in (Freund et 
al. 2003). This rank boosting technique has been 
used in re-ranking parsers (Collins and Koo 
2000; Charniak and Johnson 2005). Such re-
ranking approaches usually improve the likelih-
ood of candidate results using extraneous fea-
tures and, for example in parsing, the properties 
of the trees. In this paper, we focus on a differ-
ence task: the lexical semantics task of selecting 
the best semantic properties that help explain 
why a set of query terms are similar. Unlike in 
parsing and machine translation, we are not ulti-
682
mately looking for the best single result, but in-
stead the n-best. 
Looking at commercial applications, there are 
many examples showcasing the importance of 
ranking, for example Internet search engines like 
Google and Yahoo (Brin and Page 1998). Anoth-
er application is online recommendation systems 
where suggestions must be ranked before being 
presented to a user (Shardanand and Maes 1995). 
Also, in online social networks such as Facebook 
and LinkedIn, new connections or communities 
are suggested to users by leveraging their social 
connections (Spretus, et al 2005). 
3 Explaining Similarity 
Several applications, such as IR engines, return 
the n-best ranked results to a query. Although we 
expect our joint information model, presented in 
Section 4.2, to generalize to many ranking tasks, 
our focus in this paper is on the task of choosing 
the n-best explanations that describe the similari-
ty of a set of terms. That is, given a set of terms, 
one must choose the best set of characterizations 
of why the terms are similar, chosen from a large 
set of possible explanations. 
Analyzing the different ways in which one can 
explain/characterize the similarity between terms 
is beyond the scope of this paper3. The types of 
explanations that we consider in this paper are 
semantic properties that are shared by the terms. 
For example, consider the query terms {apple, 
ume, pawpaw, quince} presented in Section 1. 
An example set of properties that explains the 
similarity of these words might include {they are 
products, they can be imported, they can be ex-
ported, they are tasty, they grow}. 
The range of possible semantic properties is 
large. For the above example, we may have of-
fered many other properties like {they are enti-
ties, they can be eaten, they have skin, they are 
words, they can be roasted, they can be shipped, 
etc.} Choosing a high quality concise set of 
properties is the goal of this paper. 
Our hypothesis is that considering items in a 
result set jointly for ranking produces better re-
sult sets than considering them independently. 
An important question then is: what is a utility 
function for measuring a better result? We pro-
pose that a result set is considered better than 
another if a person could more easily reconstruct 
the original query from it. Or, in other words, a 
result set is considered better than another if it 
                                                 
3 This topic is the focus of (Vyas and Pantel 2008).  
reduces more the uncertainty of what the original 
query was. Here, reducing the uncertainty means 
making it easier for a human to understand the 
original question (i.e., a good explanation should 
clarify the query).  
Formally, we define our ranking task as: 
Task Definition: Given a query Q = {q1, q2, ?, 
qm} and a set of candidate properties R = {r1, 
r2, ?, rk}, where q is a term and r is a property, 
find the set of properties R' = {r1, r2, ?, rn} that 
most reduces the uncertainty of Q, where n << k. 
Recall from Section 1 the example Q = {apple, 
ume, pawpaw, quince}. The set of properties: 
{they are products, they can be 
imported, they can be eaten} 
is preferred over the set  
{they are products, they can be 
imported, they can be exported} 
since it reduces more the uncertainty of what the 
original query is. That is, if we hid the query 
{apple, ume, pawpaw, quince} from a person, 
the first set of properties would help more that 
person guess the query elements than the second 
properties. 
In Section 4, we describe two models for mea-
suring this uncertainty reduction and in Section 
5.1, we describe an evaluation methodology for 
quantifying this reduction in uncertainty using 
human judgments. 
3.1 Source of Properties 
What is the source of the semantic properties to 
be used as explanations? Following Lin (1998), 
we use syntactic dependencies between words to 
model their semantic properties. The assumption 
here is that some grammatical relations, such as 
subject and object can often yield semantic 
properties of terms. For example, given enough 
corpus occurrences of a phrase like ?students eat 
many apples?, then we can infer the properties 
can-be-eaten for apples and can-eat for students. 
Unfortunately, many grammatical relations do 
not specify semantic properties, such as most 
conjunction relations for example. In this paper, 
we use a combination of corpus statistics and 
manual filters of grammatical relations (such as 
omitting conjunction relations) to uncover 
candidate semantic properties, as described in the 
next section. With this method, we unfortunately 
uncover some non-semantic properties and fail to 
uncover some correct semantic properties. 
683
Improving the candidate lists of semantic 
properties is grounds for further investigation. 
3.2 Extracting Properties 
Given a set of similar terms, we look at the 
overlapping syntactic dependencies between the 
words in the set to form candidate semantic 
properties. Example properties extracted by our 
system (described below) for a random sample of 
two instances from a cluster of food, {apple, 
beef}, include4: 
shredded, sliced, lean, sour, de-
licious, cooked, import, export, 
eat, cook, dice, taste, market, 
consume, slice, ... 
We obtain candidate properties by parsing a 
large textual corpus with the Minipar parser (Lin 
1993)5. For each word in the corpus, we extract 
all of its dependency links, forming a feature 
vector of syntactic dependencies. For example, 
below is a sample of the feature vector for the 
word apple: 
adj-mod:gala, adj-mod:shredded,  
object-of:caramelize, object-of:eat, 
object-of:import, ... 
Intersecting apple?s feature vector with beef?s, 
we are left with the following candidate 
properties: 
adj-mod:shredded, object-of:eat,  
object-of:import, ... 
In this paper, we omit the relation name of the 
syntactic dependencies, and instead write: 
 shredded, eat, import, ... 
This list of syntactic dependencies forms the 
candidate properties for our ranking task defined 
in Section 3.  
In Section 4, we use corpus statistics over 
these syntactic dependencies to find the most 
informative properties that explain the similarity 
of a set of terms. Some syntactic dependencies 
are not reliably descriptive of the similarity of 
words such as conjunctions and determiners. We 
omit these dependency links from our model. 
4 Ranking Models 
In this section, we present our ranking models for 
choosing the n-best results to a query according 
to our task definition from Section 3. The models 
                                                 
4 We omit the syntactic relations for readability. 
5 Section 5.1 describes the specific corpus and method 
that was used to obtain our reported results. 
are expected to generalize to many ranking tasks, 
however in this paper we focus solely on the 
problem of choosing the best semantic properties 
that describe the similarity of a set of terms. 
In the next section, we outline our baseline in-
dependent model, which is based on a commonly 
used ranking metric in lexical semantics for se-
lecting the most informative properties of a term. 
Then in Section 4.2, we propose our new model 
for considering the properties jointly. 
4.1 EIIR: Expected Independent Informa-
tion Ranking Model (Baseline Model) 
Recall the task definition from Section 3. Finding 
a property r that most reduces the uncertainty in 
a query set Q can be modeled by measuring the 
strength of association between r and Q. 
Following Pantel and Lin (2002), we use 
pointwise mutual information (pmi) to measure 
the association strength between two events q 
and r, where q is a term in Q and r is syntactic 
dependency, as follows (Church and Hanks 
1989): 
 ( ) ( )( ) ( )
N
fqc
N
rwc
N
rqc
FfWw
rqpmi ???
=
??
,,
,
log,  (4.1) 
where c(q,r) is the frequency of r in the feature 
vector of q (as defined in Section 3.2), W is the 
set of all words in our corpus, F is the set of all 
syntactic dependencies in our corpus, and  
N = ( )? ?
? ?Ww Ff
fwc , is the total frequency count of 
all features of all words. 
We estimate the association strength between 
a property r and a set of terms Q by taking the 
expected pmi between r and each term in Q as: 
 ( ) ( ) ( )?
?
=
Qq
rqpmiqPrQpmi ,,  (4.2) 
where P(q) is the probability of q in the corpus. 
Finally, the EIIR model chooses an n-best list 
by selecting the n properties from R that have 
highest pmi(Q, r). 
4.2 JIR: Joint Information Ranking Model 
The hypothesis of this paper is that considering 
items in an n-best result set jointly for ranking 
produces better result sets than considering them 
independently, an example of which is shown in 
Section 1. 
Recall our task definition from Section 3: to 
select an n-best list R' from R such that it most 
reduces the uncertainty of Q. Recall that for ex-
plaining the similarity of terms, Q is the set of 
684
query words to be explained and R is the set of 
all properties shared by words in Q. The above 
task of finding R' can be captured by the follow-
ing objective function: 
 ( )RQIR
RR
?=?
??
minarg  (4.3) 
where I(Q|R') is the amount of information in Q 
given R':6 
 ( ) ( ) ( )?
?
??=?
Qq
RqIqPRQI  (4.4) 
where P(q) is the probability of term q in our 
corpus (defined in the Section 4.1) and I(q|R') is 
the amount of information in q given R', which is 
defined as the conditional self-information 
between q and R' (Merhav and Feder 1998): 
 
( ) ( )
( )
( )
( )Rc
Rqc
rrrqP
rrrqIRqI
n
n
?
??=
?=
=?
*,
,
log
,...,,log
,...,,
21
21
 (4.5) 
where c(q,R') is the frequency of all properties in 
R' occurring with word q and * represents all 
possible terms in the corpus7. We have: 
 ( ) ( )?
??
=?
Rr
rqcRqc ,,  and ( ) ( )??
?? ??
=?
Rr Qq
rqcRc ,*,  
where c(q,r) is defined as in Section 4.1 and Q' is 
the set of all words that have all the properties in 
R'. Computing c(*,R') efficiently can be done 
using a reverse index from properties to terms. 
The Joint Information Ranking model (JIR) is 
the objective function in Eq. 4.3. We find a sub-
optimal solution to Eq. 4.3 using a greedy algo-
rithm by starting with an empty set R' and itera-
tively adding one property r at a time into R' such 
that: 
 ( ) ( )?
????
???=
QqRRr
rRqIqPr minarg  (4.6) 
The intuition behind this algorithm is as fol-
lows: when choosing a property r to add to a par-
tial result set, we should choose the r that contri-
butes the maximum amount of information to the 
existing set (where all properties are considered 
jointly). 
                                                 
6 Note that finding the set R' that minimizes the 
amount of information in Q given R' equates to find-
ing the R' that reduces most the uncertainty in Q. 
7 Note that each property in R' is shared by q because 
of the way the candidate properties in R were con-
structed (see Section 3.2). 
A brute force optimal solution to Eq. 4.3 in-
volves computing I(Q|R') for all subsets R' of size 
n of R. In future work, we will investigate heuris-
tic search algorithms for finding better solutions 
to Eq. 4.3, but our experimental results discussed 
in Section 5 show that our greedy solution to Eq. 
4.3 already yields significantly better n-best lists 
than the baseline EIIR model. 
5 Experimental Results 
In this section, we show empirical evidence that 
considering items in an n-best result set jointly 
for ranking produces better result sets than con-
sidering them independently. We validate this 
claim by testing whether or not human judges 
prefer the set of explanations generated by our 
joint model (JIR) over the independent model 
(EIIR). 
5.1 Experimental Setup 
We trained the probabilities described in Section 
4 using corpus statistics extracted from the 
TREC-9 and TREC-2002 Aquaint collections 
consisting of approximately 600 million words. 
We used the Minipar parser (Lin 1993) to ana-
lyze each sentence and we collected the frequen-
cy counts of the grammatical contexts output by 
Minipar and used them to compute the probabili-
ty and pointwise mutual information values from 
Sections 4.1 and 4.2. Given any set of words Q 
from the corpus, our joint and independent mod-
els generate a ranked list of n-best explanations 
(i.e., properties) for the similarity of the words. 
Recall the example set Q = {apple, beef} from 
Section 3.2. Following Section 3.2, all grammat-
ical contexts output by Minipar that both words 
share form a candidate explanation set R for their 
similarity. For {apple, beef}, our systems found 
312 candidate explanations. Applying the inde-
pendent ranking model, EIIR, we obtain the fol-
lowing top-5 best explanations, R': 
product, import of, export, ban 
on, industry 
Using the joint model, JIR, we obtain: 
export, product, eat, ban on, 
from menu 
5.2 Comparing Ranking Models 
In order to obtain a representative set of similar 
terms as queries to our systems, we randomly 
chose 100 concepts from the CBC collection 
(Pantel and Lin 2002) consisting of 1628 clusters 
of nouns. For each of these concepts, we ran-
domly chose a set of cluster instances (nouns), 
685
where the size of each set was randomly chosen 
to consist of two or three noun (chosen to reduce 
the runtime of our algorithm). For example, three 
of our randomly sampled concepts were Music, 
Flowers, and Alcohol and below are the random 
instances selected from these concepts: 
? {concerto, quartet, Fifth Symphony} 
? {daffodil, lily} 
? {gin, alcohol, rum} 
Each of these three samples forms a query. 
Applying both our EIIR and JIR models, we gen-
erated the top-5 explanations for each of the 100 
samples. For example, below are the explana-
tions returned for {daffodil, lily}:  
? EIIR: bulb, bouquet of, yellow, pink, hybr-
id 
? JIR: flowering, bulb, bouquet of, hybrid, 
yellow 
Two judges then independently annotated 500 
test cases using the following scheme. For each 
of the 100 samples, a judge is presented with the 
sample along with the top-1 explanation of both 
systems, randomly ordered for each sample such 
that the judge can never know which system 
generated which explanation. The judge then 
must make one of the following three choices: 
? Explanation 1: The judge prefers the first 
explanation to the second. 
? Explanation 2: The judge prefers the 
second explanation to the first. 
? Equal: The judge cannot determine that 
one explanation is better than the other. 
The judge is then presented with the top-2 ex-
planations from each system, then the top-3, top-
4, and finally the top-5 explanations, making the 
above annotation decision each time. Once the 
judge has seen the top-5 explanations for the 
sample, the judge moves on to the next sample 
and repeats this process until all 100 samples are 
annotated. Allowing the judges to see the top-1, 
top-2, up to top-5 explanations allows us to later 
inspect how our ranking algorithms perform on 
different sizes of explanation sets. 
The above annotation task was performed in-
dependently by two judges and the resulting 
agreement between the judges, using the Kappa 
statistic (Siegel and Castellan Jr. 1988), was ? = 
0.60. Table 1 lists the full confusion matrix on 
the annotation task. On just the annotations of the 
top-5 explanations, the agreement was ? = 0.73. 
Table 2 lists the Kappas for the different sizes of 
explanation sets. It is more difficult for judges to 
determine the quality of smaller explanation sets. 
For the above top-5 explanations for the query 
{daffodil, lily}, both judges preferred the JIR 
properties since flowering was deemed more in-
formative than pink given that we also know the 
property yellow. 
5.2.1 Evaluation Results 
Table 3 shows sample n-best lists generated by 
our system and Table 4 presents the results of the 
experiment described in the previous section. 
Table 4 lists the preferences of the judges for the 
n-best lists generated by the independent and 
joint models, in terms of the percentage of sam-
ples preferred by each judge on each model. We 
report our results on both all 500 annotations and 
on the 100 annotations for the explanation sets of 
size n = 5. Instead of using an adjudicator for 
resolving the two judges? disagreements, we 
weighted each judge?s decision by 0.5. We used 
bootstrap resampling to obtain the 95% confi-
dence intervals. 
The judges significantly preferred the joint 
model over the independent model. Looking at 
all annotated explanation sets (varying n from 1 
to 5), the n-best lists from JIR were preferred 
39.7% of the time. On the 50.0% ? 3.1% test 
cases where one list was preferred over another, 
the JIR lists were preferred overall 76.6% ? 5.2% 
of the time, with 95% confidence. Caution 
should be taken when interpreting the results for 
n < 3 since the annotator agreement for these was 
very low. However, as shown in Figure 1, human 
preference for the JIR model was higher at n ? 3. 
Table 2. Inter-annotator agreement statistics over 
varying explanation set sizes n. 
n AGREEMENT (%) KAPPA (?) 
1 75.0 0.47 
2 70.0 0.50 
3 77.0 0.62 
4 78.0 0.63 
5 84.0 0.73 
 
Table 1. Confusion matrix between the two judges on 
the annotation task over all explanation set sizes 
(n = 1 ? 5). 
 JIR EIIR EQUAL 
JIR 153 2 48 
EIIR 11 33 19 
EQUAL 29 7 198 
 
686
5.2.2 Discussion and Error Analysis 
Figure 1 illustrates the annotated preferences 
over varying sizes of explanation sets, for n ? 
[1 .. 5]. Except in the case where only one expla-
nation is returned, we see consistent preferences 
between the judges. Manual inspection of the 
size 1 explanation sets showed that often one 
property is not enough to understand the similari-
ty of the query words. For example, consider the 
following two explanation sets: {sell} and 
{drink}. If you did not know the original query Q, 
one list would not be much better than the other 
in determining what the query was. But, by add-
ing one more property, we get: {sell, drink} and 
{drink, spike with}. The second explanation list 
reduces much more the uncertainty that the query 
consists of alcoholic beverages, as you probably 
guessed (the first list also reduces the uncertainty, 
but not as much as the second). The above ex-
ample is taken from our random sample list for 
the query words {gin, alcohol, rum} ? the expla-
nation {drink, spike with} was generated using 
the JIR model. 
We manually inspected some of the sample 
queries where both judges preferred the EIIR n-
best list. One such sample query was: {Jerry 
Falwell, Jim Bakker, Pat Robertson}. The n-best 
lists returned by the JIR and EIIR models respec-
tively were {televangelist, evangelist, Rev., tele-
vision, founder} and {evangelist, television, Rev., 
founder, religious}. Both judges preferred the 
EIIR list because of the overlap in information 
between televangelist and evangelist. The prob-
lem here in JIR was that the word televangelist 
was very rare in the corpus and thus few terms 
had both the feature televangelist and evangelist. 
We would expect in a larger corpus to see a larg-
er overlap with the two features, in which case 
evangelist would not be chosen by the JIR model. 
As discussed in Section 2, considering results 
jointly is not a new idea and is very similar to the 
concept of diversity-based ranking introduced in 
the IR community by Carbonell and Goldstein 
(1998). Their proposed technique, called maxim-
al marginal relevance (MMR), forms the basis of 
most schemes used today and works as follows. 
Initially, each result item is scored independently 
of the others. Then, the n-best list is selected by 
iteratively choosing the highest scoring result 
and then discounting each remaining candidate?s 
score by some function of the similarity (or in-
formation gain) between that candidate and the 
currently selected members of the n-best list. In 
practice, these heuristic-based algorithms are fast 
to compute and are used heavily by commercial 
IR engines. The purpose of this paper is to inves-
tigate a principled definition of diversity using 
the concept of maximal joint information. The 
objective function proposed in Eq. 4.3 provides a 
basis for understanding diversity through the lens 
of information theory. Although this paper fo-
Table 3. Five example n-best lists, drawn from our random sample described in Section 5.1, using the joint JIR
model and the independent EIIR model (for n=5). 
Query (Q) JIR n-best (R') EIIR n-best (R') 
{gin, alcohol, rum} drink, spike with, sell, use, consume sell, drink, use, consume, buy 
{Temple University, Michigan State} 
political science at, professor at, 
director at, student at, attend 
professor at, professor, director at, 
student at, student 
{concerto, quartet, Fifth Symphony} Beethoven, his, play, write, performance his, play, write, performance, perform 
{ranch house, loft} 
offer, brick, sprawling, rambling, 
turn-of-the-century 
his, live, her, buy, small 
{dysentery, tuberculosis} morbidity, die of, case, patient, suffer from die of, case, patient, case of, have 
 
Table 4. Percentage of test cases where the judges 
preferred JIR vs. EIIR vs. they had no preference, 
computed over all explanation set sizes (n = 1 ? 5) 
vs. only the explanation sets of size n = 5. 
SYSTEM ALL (95% CONF?) N=5 (95% CONF?) 
JIR 39.7% ? 3.0% 43.7% ? 6.9% 
EIIR 10.4% ? 1.3% 10.1% ? 4.2% 
Equal 50.0% ? 3.1% 45.2% ? 6.9% 
?95% confidence intervals estimated using bootstrap resampling. 
Figure 1. Percentage of human preference for each 
model with varying sizes of explanation sets (n). 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5
Pr
ef
er
en
ce
Number?of?explanations? (n)
Model?Preference?vs.?Number?of?Explanations
JIR?Preferred EIIR?Baseline?Preferred Equal?(No?Preference)
687
cuses on the task of explaining the similarity of 
terms, we plan in future work to apply our me-
thod to an IR task in order to compare and con-
trast our method with MMR. 
6 Conclusion 
This paper investigates the problem of n-best 
ranking on the lexical semantics task of explain-
ing/characterizing the similarity of a group of 
terms where only a small set of many possible 
semantic properties may be displayed to a user. 
We propose that considering the results jointly, 
by accounting for the information overlap be-
tween results, helps generate better n-best lists. 
We presented an information theoretic objective 
function, called Joint Information Ranking, for 
modeling the joint information in an n-best list. 
On our lexical semantics task, empirical evidence 
shows that humans significantly prefer JIR n-best 
lists over a baseline model that considers the ex-
planations independently. Our results show that 
the n-best lists generated by the joint model are 
judged to be significantly different from those 
generated by the independent model 50.0% ? 
3.1% of the time, out of which they are preferred 
76.6% ? 5.2% of the time, with 95% confidence. 
In future work, we plan to investigate other 
joint models using latent semantic analysis tech-
niques, and to investigate heuristic algorithms to 
both optimize search efficiency and to better ap-
proximate our JIR objective function. Although 
applied only to the task of characterizing the si-
milarity of terms, it is our hope that the JIR mod-
el will generalize well to many ranking tasks, 
from keyword search ranking, to recommenda-
tion systems, to advertisement placements. 
References 
Barzilay, R.; McKeown, K.; and Elhadad, M. 1999. Information 
Fusion in the Context of Multi-Document Summarization. In 
Proceedings of ACL-1999. pp. 550-557. College Park, MD. 
Brin, S. and Page, L. 1998. The Anatomy of a Large-Scale Hyper-
textual Web Search Engine. Computer Networks and ISDN Sys-
tems, 30:107-117. 
Carbonell, J. G. and Goldstein, J. 1998. The Use of MMR, Diversi-
ty-Based Reranking for Reordering Documents and Producing 
Summaries. In Proceedings of SIGIR-1998. pp. 335-336. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best parsing 
and MaxEnt disciriminative reranking. In Proceedings of ACL-
2005. pp. 173-180. Ann Arbor, MI. 
Church, K. and Hanks, P. 1989. Word association norms, mutual 
information, and lexicography. In Proceedings of ACL-89. pp. 
76-83. Vancouver, Canada. 
Collins, M. and Koo, T. 2000. Discriminative Reranking for Natu-
ral Laguage Parsing. In Proceedings ICML-2000. pp. 175-182. 
Palo Alto, CA 
Freund, Y.; Iyer, R.; Schapier, E.R and Singer, Y. 2003. An effi-
cient boosting algorithm for combining preferences. The Journal 
of Machine Learning Research, 4:933-969. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. 
pp. 26-47. 
Hearst, M. A. and Pedersen, J. O. 1996. Reexamining the cluster 
hypothesis: Scatter/gather on retrieval results. In Proceedings of 
SIGIR-1996. pp. 76-84. Zurich, Switzerland. 
Hovy, E.H. and Lin, C.-Y. 1998. Automated Text Summarization in 
SUMMARIST. In M. Maybury and I. Mani (eds), Advances in 
Automatic Text Summarization. Cambridge, MIT Press. 
Kleinberg, J. 1998. Authoritative sources in a hyperlinked environ-
ment. In Proceedings of the Ninth Annual ACM-SIAM Sympo-
sium on Discrete Algorithms. Pp. 668-677. New York, NY. 
Leuski, A. 2001. Evaluating document clustering for interactive 
information retrieval. In Proceedings of CIKM-2001. pp. 33-40. 
Atlanta, GA. 
Lin, D. 1998. Automatic retrieval and clustering of similar words. 
In Proceedings of COLING/ACL-98. pp. 768-774. Montreal, 
Canada. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of 
ACL-93. pp. 112-120. Columbus, OH. 
Liu, X. and Croft, W. B. 2004. Cluster-based retrieval using lan-
guage models. In Proceedings of SIGIR-2004. pp. 186-193. 
Sheffield, UK. 
Merhav, N. and Feder, M. 1998. Universal Prediction. IEEE Trans-
actions on Information Theory, 44(6):2124-2147. 
Page, L.; Brin, S.; Motwani R.; Winograd, T. 1998. The PageRank 
Citation Ranking: Bringing Order to the Web. Stanford Digital 
Library Technologies Project. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. 
In Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnick, P. and Varian, H. R. 1997. Recommender Systems. Com-
munications of the ACM, 40(3):56-58. 
Salton, G. and Buckley, C. 1987. Term Weighting Approaches in 
Automatic Text Retrieval. Technical Report:TR81-887, Ithaca, 
NY. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern Infor-
mation Retrieval. McGraw Hill. 
Shardanand, U. and Maes, P. 1995. Social Information Filtering: 
Algorithms for Automating ?Word of Mouth?. In Proceedings 
of ACM CHI-1995. pp. 210-217. New York. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Spretus, E.; Sahami, M.; and Buyukkokten, O. 2005. Evaluating 
Similarity Measures: A Large-Scale Study in the Orkut Social 
Network. In Proceedings of SIGKDD-2005. pp. 678-684. Chi-
cago, IL. 
Vyas, V. and Pantel, P. 2008. Explaining Similarity of Terms. In 
Proceedings of COLING-2008. Manchester, England. 
Zhu, X.; Goldberg, A.; Van Gael, J.; and Andrzejewski, D. 2007. 
Improving Diversity in Ranking using Absorbing Random 
Walks. In Proceedings of NAACL HLT 2007. pp. 97-104. 
Rochester, NY. 
Zhang, Y,; Callan, J.; and Minka, T. 2002. Novelty and redundancy 
detection in adaptive filtering. In Proceedings of SIGIR-2002. 
pp. 81-88. Tampere, Finland. 
Zhang, Y.; Hildebrand, A. S.; and Vogel, S. 2006. Distributed Lan-
guage Modeling for N-best List Re-ranking. In Proceedings of 
EMNLP-2006. Pp. 216-223. Sydney, Australia. 
688
Coling 2008: Companion volume ? Posters and Demonstrations, pages 131?134
Manchester, August 2008
Explaining Similarity of Terms
Vishnu Vyas
USC Information Sciences Institute
Marina del Rey, CA
vishnu@isi.edu
Patrick Pantel
Yahoo! Inc.
Santa Clara, CA 95054
me@patrickpantel.com
Abstract
Computing the similarity between entities
is a core component of many NLP tasks
such as measuring the semantic similarity
of terms for generating a distributional the-
saurus. In this paper, we study the problem
of explaining post-hoc why a set of terms
are similar. Given a set of terms, our task is
to generate a small set of explanations that
best characterizes the similarity of those
terms. Our contributions include: 1) an
information-theoretic objective function
for quantifying the utility of an explana-
tion set; 2) a survey of psycholinguistics
and philosophy for evidence of different
sources of explanations such as descriptive
properties and prototypes; 3) computa-
tional baseline models for automatically
generating various types of explanations;
and 4) a qualitative evaluation of our
explanation generation engine.
1 Introduction
Computing similarity is at the core of many
computer science tasks. Many have developed
algorithms for computing the semantic similarity
of words (Lee, 1999), of expressions to gener-
ate paraphrases (Lin and Pantel, 2001) and of
documents (Salton and McGill, 1983). However,
little investigation has been spent on automatically
explaining why a particular set of elements are
similar to one another.
Explaining similarity is an important part of
various natural language applications such as
question answering and building lexical ontolo-
gies such as WordNet (Fellbaum, 1998). Several
questions must be addressed before one can begin
to explore this topic. First, what constitutes a good
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
explanation and what are the sources of these
explanations? Second, how can we automatically
generate these different types of explanations?
Third, how do we empirically evaluate the quality
of an explanation? In this paper, we propose a first
analysis of these questions.
2 Related Work
The task of generating explanations has been stud-
ied in relation to Question Answering (Hirschman
and Gaizauskas, 2001) and Knowledge Represen-
tation and Reasoning (Cohen et al, 1998). Within
Question Answering, explanations have mostly
been viewed from a deductive framework and
have focused on proof trees and inference traces
as sources of explanations (Moldovan and Rus,
2001). Summarization and text generation from
proof trees have also been explored as explanations
in QA systems (Barker et al, 2004). Lester (1997)
proposed explanation design packages, a hybrid
representation for discourse knowledge that
generates multi-sentential explanations.
Detailed psycholinguistic studies into how
people explain things suggests that people explain
similarity using ?feature complexes? (Fillenbaum,
1969), a bundle of features semantically related
with a term. This suggests considering explana-
tions of similarity as the shared features among
a set of terms. Another competing idea from
linguistic philosophy is the Prototype theory by
Rosch (1975). It is argued that objects within
a semantic category are represented by another,
more commonly used or much simpler member
of the same semantic category, called a prototype.
And, within this view, explanations for similarity
are prototypes from the same semantic category
as the given terms. Deese (1966) investigated
similarity in terms of stimulus-response word
association experiments providing empirical
evidence to consider other semantically similar
words as explanations.
131
3 An Information Theoretic Framework for
Explaining Similarity
In this section, we present an information-theoretic
framework that defines a good explanation using
the intuition that they are highly informative and
reduce the uncertainty in the set of query terms.
For example, consider the set of query terms
{Maybach, Maserati, Renault}. One possible
explanation of their similarity which is very infor-
mative is they are all like a Ford (i.e., a prototype
explanation). Other possible explanations include
they can be driven using a steering wheel and
they have wheels (i.e., descriptive properties as
explanations). Each of these explanations reduces
the uncertainty regarding the semantics of the
original set of terms. In information theory, the
concept of reduction in uncertainty is related to
information gain, and good explanation sets can
be quantified in terms of information gain.
Formally, given a set of query termsQ, and a set
of explanations E, we define the best explanation
set as one which provides maximum information
to the set Q, or in other words,
E = argmax
E
?
??(?)
I(Q;E
?
) (1)
where ? is the set of all explanations (discussed
in detail in Section 4) and ?(X) represents the
power set of X . The problem of choosing the
best explanation set for a given query set is now
reduced to a problem of optimization under I .
3.1 The Information Function
The information function I in Eq. (1) is a set
function which defines the amount of information
contributed by the set of explanations E
?
to the
set of query words Q. There are many possible
information functions, but we would like all of
them to have some common properties.
Consistency
The information function should be consis-
tent. For two sets, E and E
?
, if E ? E
?
then
I(Q;E
?
) ? I(Q;E). In other words, given two
explanation sets E and E
?
, with E
?
containing
extra explanations, not in E, the information
function should assign larger values to E
?
with
respect to Q than it assigns to E.
Explanation Set Cardinality
Another important requirement regarding I , is
the size of the explanation sets. Any consistent
information function would assign larger values
to larger sets of explanations. This leads to a
problem where the optimal solution is always the
set of all explanations. We overcome this by fixing
an upper bound for the size of explanation sets
that are generated by the function I .
Redundancy and Joint Information
Many explanations in an explanation set might
overlap semantically and the information function
has to account for such overlaps. However,
information functions which take such semantic
overlap into account are computationally hard
to optimize. One approach to this problem is to
find approximate solutions using heuristic search
techniques, however, relaxing this constraint lets
us use common association measures such as
mutual information (Cover and Thomas, 1991) as
information functions.
3.2 Marginal Formulation of the Information
Function
Another equivalent formulation of Eq. (1) is to
use marginal information gains. This formulation
also gives a simple greedy algorithm to the op-
timization problem when the size of explanation
set is fixed. Let us define the marginal gain in
information to the set Q, when the explanation e
is added to the set of explanations E as:
IG
Q;E
(e) = I(Q;E ? {e})? I(Q;E)
Then, the best set of explanations of size k can be
recursively defined as
E
0
= {}
E
n
= E
n?1
? {e}
such that
e = argmax
e
?
??
IG
Q;E
n?1
(e
?
)
and
|E
n
| ? k
If our marginal information gain is independent
of the set of explanations to which it is added,
we can rank explanations by their marginal
information gains as added to the empty set. Then,
choosing the top k explanations gives us the k-best
explanation set for the query.
4 Sources for Similarity Explanations
In Section 3, we presented a framework for
quantifying a good explanation set. In this section
we present two sources of explanations, using
descriptive properties and using prototypes.
4.1 Explanations from Descriptive Properties
The concept of essence as discussed by early
empiricists was the first study of using descriptive
properties to explain the similarity of a set of
terms. Descriptive properties are the shared
essential attributes of a set of similar terms and
one way of explaining the similarity of a set of
terms is to generate descriptive properties.
Within our framework in Section 3, let the query
set Q be a set of similar words, and let ?, the
set of all explanations be the set of all properties
132
that are shared by all the words within the query
set. Using mutual information as our measure of
association between properties and terms we can
rewrite our information function I as:
I(Q;E) =
?
q?Q
p(q)
?
e?E
p(e | q) log
p(e | q)
p(e)
The marginal information gain for a single
explanation e is:
IG
Q;E
(e) =
?
q?Q
p(q) ? p(e | q) log
p(e | q)
p(e)
Since the information gain is independent of
the explanation set E, we can find the best set of
size k by greedily choosing explanations until our
explanation set reaches the desired size.
4.2 Explanations from Prototypes
As discussed in Section 2 given a set of query
terms, people can represent their meaning using
other common members from the same semantic
category, called prototypes. Within the framework
of Section 3, let Q be our set of query terms.
To generate the set of all explanations ?, we use
clusters in the CBC resource (Pantel and Lin,
2002) as an approximation to semantic categories
and we collect all possible words that belong to
that cluster which then becomes our candidate set.
Let C
q
denote the cluster to which the query
term q belongs to. Also let the set C(Q) be the
set of all clusters to which the query terms of Q
belong to. Then
? = {w|C
w
? C(Q)}
Now our information function can be written as:
I(Q;E) =
?
q?Q
p(C
q
)
?
e?E
p(e | C
q
) log
p(e | C
q
)
p(e)
The marginal formulation of the above function is:
I
Q;E
(e) =
?
q?Q
p(C
q
) ? p(e | C
q
) log
p(e | C
q
)
p(e)
We can find the optimal set of explanations of size
k using a greedy algorithm as in Section 4.1.
5 Experimental Results
5.1 Experimental Setup
For each source of explanation discussed in
Section 4, we estimated the model probabilities
using corpus statistics extracted from the 1999
AP newswire collection (part of the TREC-2002
Aquaint collection).
In order to obtain a representative set of similar
terms as queries to our systems, we randomly
chose 100 concepts from the CBC collection (Pan-
tel and Lin, 2002) consisting of 1628 clusters of
nouns. For each of these concepts, we randomly
chose a set of cluster instances (nouns), where the
size of each set was randomly chosen to consist of
two to five nouns.
Each of these samples forms a query. For
each explanation source described in Section 4,
we generated explanation sets for the random
samples and in the next section we show a random
selection of these system outputs.
5.2 Examples of Explanations using Descrip-
tive Properties
For the algorithm discussed in Section 4.1, we
derived our descriptive properties using the output
of the dependency analysis generated by the
Minipar (Lin, 1994) dependency parser. We use
syntactic dependencies between words to model
their semantic properties. The assumption here is
that some grammatical relations, such as subject
and object can yield semantic properties of terms.
For example, from a phrase like ?students eat
many apples?, we can infer the properties can-be-
eaten for apples and can-eat for students. In this
paper, we use a combination of corpus statistics
and manual filters for grammatical relations to
uncover candidate semantic properties.
Table 1: Explanations generated using descriptive
properties.
Query Sets Explanations
Palestinian-Israeli,
India-Pakistan
talks(NN), conflict(NN),
dialogue(NN),
relation(NN), peace(NN).
TV, television-station cable(NN), watch(obj),
see(ON), channel(NN),
local(ADJ-MOD)
Britney
Spears, Janet Jackson
like(OBJ),
concert(NN), video(NN),
fan(NN), album(GEN)
Crisis,
Uncertainty, Difficulty
face(OBJ), resolve(OBJ),
overcome(OBJ),
financial(ADJ-MOD),
political(ADJ-MOD)
Intuitively, one would prefer adjectival modi-
fiers and verbal propositions as good descriptive
properties for explanations, and from the exam-
ples, we can see our algorithm generates such
descriptive properties because of the high infor-
mation contribution of such properties to the query
set. However, our algorithm does not try to reduce
the redundancy within the sets of explanations. We
can see redundant explanations for examples in Ta-
ble 1. The reason is that each explanation added to
the set is independent of the ones already present
in the set. In Pantel and Vyas (2008) we propose a
joint information model to overcome this problem.
5.3 Explanations using Prototypes
The algorithm discussed in Section 4.2 uses words
that share the semantic category with words within
the query set as the set of candidate explanations.
133
We can approximate the notion of semantic
categories using clusters of semantically similar
words. For this we used the CBC collection (Pan-
tel and Lin 2002) of nouns. Using these clusters as
semantic categories, the candidate set of all expla-
nations is the set of all the words that belong to the
same cluster. Table 2 shows some system outputs.
Table 2: Explanations generated using prototypes.
Query Sets Explanations
TV, television
station
station, network, radio, channel,
television
Budweiser, Coors
Light
Anheuser-Busch, Heineken, Coors,
San Miguel, Lion Nathan
atom, elec-
tron,photon
particle, molecule, proton, Ion,
isotope
Temple Univer-
sity,Michigan State
University
University of Texas, University of
Massachusetts, University of North
Carolina,University of Virginia,
University of Minnesota
6 Conclusions and Future Work
Computing the similarity between entities forms
the basis of many computer science algorithms.
However, we have little understanding of what
constitutes the underlying similarity. In this
paper, we investigated the problem of explaining
why a set of terms are similar. We proposed
an information-theoretic objective function for
quantifying the utility of an explanation set, by
capturing the intuition that the best explanation
will be the one that is highly informative to the
original query terms. We also explored various
sources of explanations such as descriptive prop-
erties and prototypes. We then proposed baseline
algorithms to automatically generate these types
of explanations and we presented a qualitative
evaluation of the baselines.
However, many other explanation sources were
not addressed. Hypernyms and other hierarchical
relations among words also form good explanation
sources; for example the similarity of the terms
{Ford, Toyota} can be explained using the term
car, a hypernym. Also our current explanation
types would fail for query sets consisting of related
terms such as {bus, road}. More appropriate for
these queries would be identifying the relation
linking the terms or giving analogies such as
{boat, water}. We are working on algorithms
to generate these explanation types within our
information-theoretic framework. We are also
investigating application-level quantitative evalu-
ation methodologies. Candidate applications in-
clude providing answer support by explaining the
answers generated by a QA system and explaining
why a document was returned in an IR system.
References
Barker, K., Chaw, S., Fan, J., Porter, B., Tecuci, D., Yeh,
P. Z., Chaudhri, V., Israel, D., Mishra, S., Romero, P.,
and Clark, P. 2004. A Question-Answering System
for AP Chemistry: Assessing KR&R Technologies. In
Proceedings of the Ninth International Conference on the
Principles of Knowledge Representation and Reasoning
(KR 2004). Whistler, 488-497
Cohen, P., Schrag, R., Jones, E., Pease, A., Lin, A., Start,
B., Gunning, D., and Burke, M. 1998. The DARPA High
Performance Knowledge Bases Project. AI Magazine
19(4): 5-49.
Cover, T. M. and Thomas, J. A. 1991. Elements of
Information Theory. Wiley Interscience, New York.
Deese, J. 1966. The Structure of Associations in Language
and Thought. John Hopkins Press, Oxford, England.
Fellbaum, C. 1998. WordNet: An electronic lexical database.
MIT Press.
Fillenbaum, S. 1969. Words as feature complexes : false
recognition of antonyms and synonyms Journal of
Exp.Psychology, 1969.
L Hirschman, R Gaizauskas - Natural Language Engineering
2001. Natural language question answering: the view
from here. Natural Language Engineering.
Lee, Lillian. 1999. Measures of Distributional Similarity. In
Proceedings of ACL-93. pp. 25-32. College Park, MD
Lester, J. C., and Porter, B. W. 1997. Developing and
Empirically Evaluating Robust Explanation Generators:
The KNIGHT Experiments Computational Linguistics,
v.23(1) p.65-101.
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules
for Question Answering. Natural Language Engineering
7(4):343-360.
Lin, D. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING-94.
pp. 4248. Kyoto, Japan.
Moldovan, D. I., and Rus, V. 2001. Logic Form Trans-
formation of WordNet and its Applicability to Question
Answering Meeting of the Association for Computational
Linguistics, p. 394-401.
Pantel, P. and Lin, D. 2002. Discovering Word Senses
from Text. In Proceedings of SIGKDD-02. pp. 613619.
Edmonton, Canada.
Pantel, P. and Vyas, V. 2008 A Joint Information Model
for n-best Ranking In Procesedings of COLING-2008.
Manchester, UK.
Rosch, E. 1975. Cognitive representations of semantic
categories. Journal of Exp.Psychology: General, 104,
192-233.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
134
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 290?298,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semi-Automatic Entity Set Refinement 
 
 
Vishnu Vyas and Patrick Pantel 
Yahoo! Labs 
Santa Clara, CA 95054 
{vishnu,ppantel}@yahoo-inc.com 
   
 
 
Abstract 
State of the art set expansion algorithms pro-
duce varying quality expansions for different 
entity types. Even for the highest quality ex-
pansions, errors still occur and manual re-
finements are necessary for most practical 
uses. In this paper, we propose algorithms to 
aide this refinement process, greatly reducing 
the amount of manual labor required. The me-
thods rely on the fact that most expansion er-
rors are systematic, often stemming from the 
fact that some seed elements are ambiguous. 
Using our methods, empirical evidence shows 
that average R-precision over random entity 
sets improves by 26% to 51% when given 
from 5 to 10 manually tagged errors. Both 
proposed refinement models have linear time 
complexity in set size allowing for practical 
online use in set expansion systems. 
1 Introduction 
Sets of named entities are extremely useful in a 
variety of natural language and information re-
trieval tasks. For example, companies such as Ya-
hoo! and Google maintain sets of named entities 
such as cities, products and celebrities to improve 
search engine relevance. 
Manually creating and maintaining large sets of 
named entities is expensive and laborious. In re-
sponse, many automatic and semi-automatic me-
thods of creating sets of named entities have been 
proposed, some are supervised (Zhou and Su, 
2001), unsupervised (Pantel and Lin 2002, Nadeau 
et al 2006), and others semi-supervised (Kozareva 
et al 2008). Semi-supervised approaches are often 
used in practice since they allow for targeting spe-
cific entity classes such as European Cities and 
French Impressionist Painters. Methods differ in 
complexity from simple ones using lexico-
syntactic patterns (Hearst 1992) to more compli-
cated techniques based on distributional similarity 
(Pa?ca 2007a). 
Even for state of the art methods, expansion er-
rors inevitably occur and manual refinements are 
necessary for most practical uses requiring high 
precision (such as for query interpretation at com-
mercial search engines). Looking at expansions 
from state of the art systems such as GoogleSets1 , 
we found systematic errors such as those resulting 
from ambiguous seed instances. For example, con-
sider the following seed instances for the target set 
Roman Gods: 
Minerva, Neptune, Baccus, Juno, 
Apollo 
GoogleSet?s expansion as well others employing 
distributional expansion  techniques consists of a 
mishmash of Roman Gods and celestial bodies, 
originating most likely from the fact that Neptune 
is both a Roman God and a Planet. Below is an 
excerpt of the GoogleSet expansion: 
Mars, Venus, *Moon, Mercury, 
*asteroid, Jupiter, *Earth, 
*comet, *Sonne, *Sun, ? 
The inherent semantic similarity between the errors 
can be leveraged to quickly clean up the expan-
sion. For example, given a manually tagged error 
?asteroid?, a distributional similarity thesaurus 
                                                 
1 http://labs.google.com/sets 
290
such as (Lin 1998)2 can identify comet as similar to 
asteroid and therefore potentially also as an error. 
This method has its limitations since a manually 
tagged error such as Earth would correctly remove 
Moon and Sun, but it would also incorrectly re-
move Mars, Venus and Jupiter since they are also 
similar to Earth3. 
In this paper, we propose two algorithms to im-
prove the precision of automatically expanded enti-
ty sets by using minimal human negative 
judgments. The algorithms leverage the fact that 
set expansion errors are systematically caused by 
ambiguous seed instances which attract incorrect 
instances of an unintended entity type. We use dis-
tributional similarity and sense feature modeling to 
identify such unintended entity types in order to 
quickly clean up errors with minimal manual labor. 
We show empirical evidence that average R-
precision over random entity sets improves by 26% 
to 51% when given from 5 to 10 manually tagged 
errors. Both proposed refinement models have li-
near time complexity in set size allowing for prac-
tical online use in set expansion systems. 
The remainder of this paper is organized as fol-
lows. In the next section we review related work 
and position our contribution within its landscape. 
Section 3 presents our task of dynamically model-
ing the similarity of a set of words and describes 
algorithms for refining sets of named entities. The 
datasets and our evaluation methodology used to 
perform our experiments are presented in Section 4 
and in Section 5 we describe experimental results. 
Finally, we conclude with some discussion and 
future work. 
2 Related Work 
There is a large body of work for automatically 
building sets of named entities using various tech-
niques including supervised, unsupervised and 
semi-supervised methods. Supervised techniques 
use large amounts of training data to detect and 
classify entities into coarse grained classes such as 
People, Organizations, and Places (Bunescu and 
Mooney 2004; Etzioni et al 2005). On the other 
hand, unsupervised methods require no training 
                                                 
2 See http://demo.patrickpantel.com/ for a demonstration of 
the distributional thesaurus.  
3 In practice, this problem is rare since most terms that are 
similar in one of their senses tend not to be similar in their 
other senses. 
data and rely on approaches such as clustering, 
targeted patterns and co-occurrences to extract sets 
of entities (Pantel and Lin 2002; Downey et al 
2007). 
Semi-supervised approaches are often used in 
practice since they allow for targeting specific enti-
ty classes. These methods rely on a small set of 
seed examples to extract sets of entities. They ei-
ther are based on distributional approaches or em-
ploy lexico-syntactic patterns to expand a small set 
of seeds to a larger set of candidate expansions. 
Some methods such as (Riloff and Shepherd 1997; 
Riloff and Jones 1999; Banko et al 2007;Pa?ca 
2007a)  use lexico-syntactic patterns to expand a 
set of seeds from web text and query logs. Others 
such as (Pa?ca et al 2006; Pa?ca 2007b; Pa?ca and 
Durme 2008) use distributional approaches. Wang 
and Cohen (2007) use structural cues in semi-
structured text to expand sets of seed elements. In 
all methods however, expansion errors inevitably 
occur. This paper focuses on the task of post 
processing any such system?s expansion output 
using minimal human judgments in order to re-
move expansion errors. 
Using user feedback to improve a system?s per-
formance is a common theme within many infor-
mation retrieval and machine learning tasks. One 
form of user feedback is active learning (Cohn et 
al. 1994), where one or more classifiers are used to 
focus human annotation efforts on the most benefi-
cial test cases. Active learning has been successful-
ly applied to various natural language tasks such as 
parsing (Tang et al 2001), POS tagging (Dagan 
and Engelson 1995) and providing large amounts 
of annotations for common natural language 
processing tasks such as word sense disambigua-
tion (Banko and Brill 2001). Relevance feedback is 
another popular feedback paradigm commonly 
used in information retrieval (Harman 1992), 
where user feedback (either explicit or implicit) is 
used to refine the search results of an IR system. 
Relevance feedback has been successfully applied 
to many IR applications including content-based 
image retrieval (Zhouand Huang 2003) and web 
search (Vishwa et al 2005). Within NLP applica-
tions relevance feedback has also been used to 
generate sense tagged examples for WSD tasks 
(Stevenson et al 2008), and Question Answering 
(Negri 2004). Our methods use relevance feedback 
in the form of negative examples to refine the re-
sults of a set expansion system. 
291
3 Dynamic Similarity Modeling 
The set expansion algorithms discussed in Section 
2 often produce high quality entity sets, however 
inevitably errors are introduced. Applications re-
quiring high precision sets must invest significant-
ly in editorial efforts to clean up the sets. Although 
companies like Yahoo! and Google can afford to 
routinely support such manual labor, there is a 
large opportunity to reduce the refinement cost 
(i.e., number of required human judgments).  
Recall the set expansion example of Roman 
Gods from Section 1. Key to our approach is the 
hypothesis that most expansion errors result from 
some systematic cause. Manual inspection of ex-
pansions from GoogleSets and distributional set 
expansion techniques revealed that most errors are 
due to the inherent ambiguity of seed terms (such 
as Neptune in our example) and data sparseness 
(such as Sonne in our example, a very rare term). 
The former kind of error is systematic and can be 
leveraged by an automatic method by assuming 
that any entity semantically similar to an identified 
error will also be erroneous. 
In this section, we propose two methods for le-
veraging this hypothesis. In the first method, de-
scribed in Section 3.1, we use a simple 
distributional thesaurus and remove all entities 
which are distributionally similar to manually iden-
tified errors. In the second method, described in 
Section 3.2, we model the semantics of the seeds 
using distributional features and then dynamically 
change the feature space according to the manually 
identified errors and rerank the entities in the set. 
Both methods rely on the following two observa-
tions: 
a) Many expansion errors are systematically 
caused by ambiguous seed examples which 
draw in several incorrect entities of its unin-
tended senses (such as seed Neptune in our 
Roman Gods example which drew in celestial 
bodies such as Earth and Sun); 
b) Entities which are similar in one sense are 
usually not similar in their other senses. For 
example, Apple and Sun are similar in their 
Company sense but their other senses (Fruit 
and Celestial Body) are not similar. Our exam-
ple in Section 1 illustrates a rare counterexam-
ple where Neptune and Mercury are similar in 
both their Planets and Roman Gods senses. 
Task Outline: Our task is to remove errors from 
entity sets by using a minimal amount of manual 
judgments. Incorporating feedback into this 
process can be done in multiple ways. The most 
flexible system would allow a judge to iteratively 
remove as many errors as desired and then have 
the system automatically remove other errors in 
each iteration. Because it is intractable to test arbi-
trary numbers of manually identified errors in each 
iteration, we constrain the judge to identify at most 
one error in each iteration. 
Although this paper focuses solely on removing 
errors in an entity set, it is also possible to improve 
expanded sets by using feedback to add new ele-
ments to the sets. We consider this task out of 
scope for this paper.  
3.1 Similarity Method (SIM) 
Our first method directly models observation a) in 
the previous section. Following Lin (1998), we 
model the similarity between entities using the dis-
tributional hypothesis, which states that similar 
terms tend to occur in similar contexts (Harris 
1985). A semantic model can be obtained by re-
cording the surrounding contexts for each term in a 
large collection of unstructured text. Methods dif-
fer in their definition of a context (e.g., text win-
dow or syntactic relations), or a means to weigh 
contexts (e.g., frequency, tf-idf, pointwise mutual 
information), or ultimately in measuring the simi-
larity between two context vectors (e.g., using Euc-
lidean distance, Cosine, Dice). In this paper, we 
use a text window of size 1, we weigh our contexts 
using pointwise mutual information, and we use 
the cosine score to compute the similarity between 
context vectors (i.e., terms). Section 5.1 describes 
our source corpus and extraction details. Compu-
ting the full similarity matrix for many terms over 
a very large corpus is computationally intensive. 
Our specific implementation follows the one pre-
sented in (Bayardo et al 2007). 
The similarity matrix computed above is then 
directly used to refine entity sets. Given a manual-
ly identified error at each iteration, we automatical-
ly remove each entity in the set that is found to be 
semantically similar to the error. The similarity 
threshold was determined by manual inspection 
and is reported in Section 5.1. 
Due to observation b) in the previous section, 
we expect that this method will perform poorly on 
292
entity sets such as the one presented in our exam-
ple of Section 1 where the manual removal of 
Earth would likely remove correct entities such as 
Mars, Venus and Jupiter. The method presented in 
the next section attempts to alleviate this problem. 
3.2 Feature Modification Method (FMM) 
Under the distributional hypothesis, the semantics 
of a term are captured by the contexts in which it 
occurs. The Feature Modification Method (FMM), 
in short, attempts to automatically discover the 
incorrect contexts of the unintended senses of seed 
elements and then filters out expanded terms 
whose contexts do not overlap with the other con-
texts of the seed elements. 
Consider the set of seed terms S and an errone-
ous expanded instance e. In the SIM method of 
Section 3.1 all set elements that have a feature vec-
tor (i.e., context vector) similar to e are removed. 
The Feature Modification Method (FMM) instead 
tries to identify the subset of features of the error e 
which represent the unintended sense of the seed 
terms S. For example, let S = {Minerva, Neptune, 
Baccus, Juno, Apollo}. Looking at the contexts of 
these words in a large corpus, we construct a cen-
troid context vector for S by taking a weighted av-
erage of the contexts of the seeds in S. In 
Wikipedia articles we see contexts (i.e., features) 
such as4: 
attack, kill, *planet, destroy, 
Goddess, *observe, statue, *launch, 
Rome, *orbit, ? 
Given an erroneous expansion such as e = Earth, 
we postulate that removing the intersecting fea-
tures from Earth?s feature vector and the above 
feature vector will remove the unintended Planet 
sense of the seed set caused by the seed element 
Neptune. The intersecting features that are re-
moved are bolded in the above feature vector for S. 
The similarity between this modified feature vector 
for S and all entities in the expansion set can be 
recomputed as described in Section 3.1. Entities 
with a low similarity score are removed from the 
expanded set since they are assumed to be part of 
the unintended semantic class (Planet in this ex-
ample). 
Unlike the SIM method from Section 3.1, this 
method is more stable with respect to observation 
                                                 
4 The full feature vector for these and all other terms in Wiki-
pedia can be found at http://demo.patrickpantel.com/.. 
b) in Section 3. We showed that SIM would incor-
rectly remove expansions such as Mars, Venus and 
Jupiter given the erroneous expansion Earth. The 
FMM method would instead remove the Planet 
features from the seed feature vectors and the re-
maining features would still overlap with Mars, 
Venus and Jupiter?s Roman God sense. 
Efficiency: FMM requires online similarity com-
putations between centroid vectors and all ele-
ments of the expanded set. For large corpora such 
as Wikipedia articles or the Web, feature vectors 
are large and storing them in memory and perform-
ing similarity computations repeatedly for each 
editorial judgment is computationally intensive. 
For example, the size of the feature vector for a 
single word extracted from Wikipedia can be in the 
order of a few gigabytes. Storing the feature vec-
tors for all candidate expansions and the seed set is 
inefficient and too slow for an interactive system. 
The next section proposes a solution that makes 
this computation very fast, requires little memory, 
and produces near perfect approximations of the 
similarity scores. 
3.3 Approximating Cosine Similarity 
There are engineering optimizations that are avail-
able that allow us to perform a near perfect approx-
imation of the similarity computation from the 
previous section. The proposed method requires us 
to only store the shared features between the cen-
troid and the words rather than the complete fea-
ture vectors, thus reducing our space requirements 
dramatically. Also, FMM requires us to repeatedly 
calculate the cosine similarity between a modified 
centroid feature vector and each candidate expan-
sion at each iteration. Without the full context vec-
tors of all candidate expansions, computing the 
exact cosine similarity is impossible. Given, how-
ever, the original cosine scores between the seed 
elements and the candidate expansions before the 
first refinement iteration as well as the shared fea-
tures, we can approximate with very high accuracy 
the updated cosine score between the modified 
centroid and each candidate expansion. Our me-
thod relies on the fact that features (i.e., contexts) 
are only ever removed from the original centroid ? 
no new features are ever added. 
Let ? be the original centroid representing the 
seed instances. Given an expansion error e, FMM 
creates a modified centroid by removing all fea-
293
tures intersecting between e and ?. Let ?' be this 
modified centroid. FMM requires us to compute 
the similarity between ?' and all candidate expan-
sions x as: 
cos x, ? ? ( )= xi ? ? i?
x ? ? ?  
where  i iterates over the feature space. 
In our efficient setting, the only element that we 
do not have for calculating the exact cosine simi-
larity is the norm of x, x . Given that we have the 
original cosine similarity score, cos(x, ?) and that 
we have the shared features between the original 
centroid ? and the candidate expansion x we can 
calculate x  as: 
x = xi?i?? ? cos x,?( )  
Combining the two equations, have: 
cos x, ? ? ( )= cos x,?( )? xi ? ? i?
xi?i? ?
?
? ?  
In the above equation, the modified cosine score 
can be considered as an update to the original co-
sine score, where the update depends only on the 
shared features and the original centroid. The 
above update equation can be used to recalculate 
the similarity scores without resorting to an expen-
sive computation involving complete feature vec-
tors. 
Storing the original centroid is expensive and 
can be approximated instead from only the shared 
features between the centroid and all instances in 
the expanded set. We empirically tested this ap-
proximation by comparing the cosine scores be-
tween the candidate expansions and both the true 
centroid and the approximated centroid. The aver-
age error in cosine score was 9.5E-04 ? 7.83E-05 
(95% confidence interval). 
4 Datasets and Baseline Algorithm 
We evaluate our algorithms against manually 
scraped gold standard sets, which were extracted 
from Wikipedia to represent a random collection of 
concepts. Section 4.1 discusses the gold standard 
sets and the criteria behind their selection. To 
present a statistically significant view of our results 
we generated a set of trials from gold standard sets 
to use as seeds for our seed set expansion algo-
rithm. Also, in section 4.2 we discuss how we can 
simulate editorial feedback using our gold standard 
sets. 
4.1 Gold Standard Entity Sets 
The gold standard sets form an essential part of our 
evaluation. These sets were chosen to represent a 
single concept such as Countries and Archbishops 
of Canterbury. These sets were selected from the 
List of pages from Wikipedia5. We randomly 
sorted the list of every noun occurring in Wikipe-
dia. Then, for each noun we verified whether or 
not it existed in a Wikipedia list, and if so we ex-
tracted this list ? up to a maximum of 50 lists. If a 
noun belonged to multiple lists, the authors chose 
the list that seemed most appropriate. Although 
this does not generate a perfect random sample, 
diversity is ensured by the random selection of 
nouns and relevancy is ensured by the author adju-
dication. 
Lists were then scraped from the Wikipedia 
website and they went through a manual cleanup 
process which included merging variants. . The 50 
sets contain on average 208 elements (with a min-
imum of 11 and a maximum of 1116 elements) for 
a total of 10,377 elements. The final gold standard 
lists contain 50 sets including classical pianists, 
Spanish provinces, Texas counties, male tennis 
players, first ladies, cocktails, bottled water 
brands, and Archbishops of Canterbury6. 
4.2 Generation of Experimental Trials 
To provide a statistically significant view of the 
performance of our algorithm, we created more 
than 1000 trials as follows. For each of the gold 
standard seed sets, we created 30 random sortings. 
These 30 random sortings were then used to gener-
ate trial seed sets with a maximum size of 20 
seeds. 
4.3 Simulating User Feedback and Baseline 
Algorithm 
User feedback forms an integral part of our algo-
rithm. We used the gold standard sets to judge the 
                                                 
5 In this paper, extractions from Wikipedia are taken from a 
snapshot of the resource in December 2007. 
6 The gold standard is available for download at 
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl? 
type=data&id=sse-gold/wikipedia.20071218.goldsets.tgz 
294
candidate expansions. The judged expansions were 
used to simulate user feedback by marking those 
candidate expansions that were incorrect. The first 
candidate expansion that was marked incorrect in 
each editorial iteration was used as the editor?s 
negative example and was given to the system as 
an error. 
In the next section, we report R-precision gains 
at each iteration in the editorial process for our two 
methods described in Section 3. Our baseline me-
thod simply measures the gains obtained by re-
moving the first incorrect entry in a candidate 
expansion set at each iteration. This simulates the 
process of manually cleaning a set by removing 
one error at a time. 
5 Experimental Results 
5.1 Experimental Setup 
Wikipedia5 served as the source corpus for our al-
gorithms described in Sections 3.1 and 3.2. All 
articles were POS-tagged using (Brill 1995) and 
later chunked using a variant of (Abney 1991). 
Corpus statistics from this processed text were col-
lected to build the similarity matrix for the SIM 
method (Section 3.1) and to extract the features 
required for the FMM method (Section 3.2). In 
both cases corpus statistics were extracted over the 
semi-syntactic contexts (chunks) to approximate  
term meanings. The minimum similarity thresholds 
were experimentally set to 0.15 and 0.11 for the 
SIM and FMM algorithms respectively. 
Each experimental trial described in Section 
4.2, which consists of a set of seed instances of one 
of our 50 random semantic classes, was expanded 
using a variant of the distributional set expansion 
algorithm from Sarmento et al (2007). The expan-
sions were judged against the gold standard and 
each candidate expansion was marked as either 
correct or incorrect. This set of expanded and 
judged candidate files were used as inputs to the 
algorithms described in Sections 3.1 and 3.2. 
Choosing the first candidate expansion that was 
judged as incorrect simulated our user feedback. 
This process was repeated for each iteration of the 
algorithm and results are reported for 10 iterations. 
The outputs of our algorithms were again 
judged against the gold standard lists and the per-
formance was measured in terms of precision gains 
over the baseline at various ranks. Precision gain 
for an algorithm over a baseline is the percentage 
increase in precision for the same values of para-
meters of the algorithm over the baseline. Also, as 
the size of our gold standard lists vary, we report 
another commonly used statistic, R-precision. R-
precision for any set is the precision at the size of 
the gold standard set. For example, if a gold stan-
dard set contains 20 elements, then R-precision for 
any set expansion is measured as the precision at 
rank 20. The average R-precision over each set is 
then reported. 
5.2 Quantitative Analysis 
Table 1 lists the performance of our baseline algo-
rithm (Section 4.3) and our proposed methods SIM 
and FMM (Sections 3.1 and 3.2) in terms of their 
R-precision with 95% confidence bounds over 10 
iterations of each algorithm. 
The FMM of Section 3.2 is the best performing 
method in terms of R-precision reaching a maxi-
mum value of 0.322 after the 10th iteration. For 
small numbers of iterations, however, the SIM me-
thod outperforms FMM since it is bolder in its re-
finements by removing all elements similar to the 
tagged error. Inspection of FMM results showed 
that bad instances get ranked lower in early itera-
tions but it is only after 4 or 5 iterations that they 
get pushed passed the similarity threshold (ac-
counting for the low marginal increase in precision 
gain for FMM in the first 4 to 5 iterations). 
FMM outperforms the SIM method by an aver-
age of 4% increase in performance (13% im-
provement after 10 iterations). However both the 
FMM and the SIM method are able to outperform 
Table 1. R-precision of the three methods with 95% confi-
dence bounds. 
ITERATION BASELINE SIM FMM 
1 0.219?0.012 0.234?0.013 0.220?0.015 
2 0.223?0.013 0.242?0.014 0.227?0.017 
3 0.227?0.013 0.251?0.015 0.235?0.019 
4 0.232?0.013 0.26?0.016 0.252?0.021 
5 0.235?0.014 0.266?0.017 0.267?0.022 
6 0.236?0.014 0.269?0.017 0.282?0.023 
7 0.238?0.014 0.273?0.018 0.294?0.023 
8 0.24?0.014 0.28?0.018 0.303?0.024 
9 0.242?0.014 0.285?0.018 0.315?0.025 
10 0.243?0.014 0.286?0.018 0.322?0.025 
295
the baseline method. Using the FMM method one 
would achieve an average of 17% improvement in 
R-precision over manually cleaning up the set 
(32.5% improvement after 10 iterations). Using the 
SIM method one would achieve an average of 13% 
improvement in R-precision over manually clean-
ing up the set (17.7% improvement after 10 itera-
tions). 
5.3 Intrinsic Analysis of the SIM Algorithm 
Figure 1 shows the precision gain of the similarity 
matrix based algorithm over the baseline algo-
rithm. The results are shown for precision at ranks 
1, 2, 5, 10, 25, 50 and 100, as well as for R-
precision. The results are also shown for the first 
10 iterations of the algorithm.  
SIM outperforms the baseline algorithm for all 
ranks and increases in gain throughout the 10 itera-
tions. As the number of iterations increases the 
change in precision gain levels off. This behavior 
can be attributed to the fact that we start removing 
errors from top to bottom and in each iteration the 
rank of the error candidate provided to the algo-
rithm is lower than in the previous iteration. This 
results in errors which are not similar to any other 
candidate expansions. These are random errors and 
the discriminative capacity of this method reduces 
severely. 
Figure 1 also shows that the precision gain of 
the similarity matrix algorithm over the baseline 
algorithm is higher at ranks 1, 2 and 5.  Also, the 
performance increase drops at ranks 50 and 100. 
This is because low ranks contain candidate expan-
sions that are random errors introduced due to data 
sparsity. Such unsystematic errors are not detecta-
ble by the SIM method. 
5.4 Intrinsic Analysis of the FMM Algorithm 
The feature modification method of Section 3.2 
shows similar behavior to the SIM method, how-
ever as Figure 2 shows, it outperforms SIM me-
thod in terms of precision gain for all values of 
ranks tested. This is because the FMM method is 
able to achieve fine-grained control over what it 
removes and what it doesn?t, as described in Sec-
tion 5.2. 
Another interesting aspect of FMM is illu-
strated in the R-precision curve. There is a sudden 
jump in precision gain after the fifth iteration of 
the algorithm. In the first iterations only few errors 
are pushed beneath the similarity threshold as cen-
troid features intersecting with tagged errors are 
slowly removed. As the feature vector for the cen-
troid gets smaller and smaller, remaining features 
look more and more unambiguous to the target 
entity type and erroneous example have less 
chance of overlapping with the centroid causing 
them to be pushed pass the conservative similarity 
threshold. Different conservative thresholds 
yielded similar curves. High thresholds yield bad 
performance since all but the only very prototypi-
cal set instances are removed as errors. 
The R-precision measure indirectly models re-
call as a function of the target coverage of each set. 
We also directly measured recall at various ranks 
 
Figure 1. Precision gain over baseline algorithm for SIM 
method. 
Figure 2. Precision gain over baseline algorithm for FMM 
method.
296
and FMM outperformed SIM at all ranks and itera-
tions. 
5.5 Discussion 
In this paper we proposed two techniques which 
use user feedback to remove systematic errors in 
set expansion systems caused by ambiguous seed 
instances. Inspection of expansion errors yielded 
other types of errors. 
First, model errors are introduced in candidate 
expansion sets by noise from various pre-
processing steps involved in generating the expan-
sions. Such errors cause incorrect contexts (or fea-
tures) to be extracted for seed instances and 
ultimately can cause erroneous expansions to be 
produced. These errors do not seem to be systemat-
ic and are hence not discoverable by our proposed 
method. 
Other errors are due to data sparsity. As the fea-
ture space can be very large, the difference in simi-
larity between a correct candidate expansion and 
an incorrect expansion can be very small for sparse 
entities. Previous approaches have suggested re-
moving candidate expansions for which too few 
statistics can be extracted, however at the great 
cost of recall (and lower R-precision). 
6 Conclusion 
In this paper we presented two algorithms for im-
proving the precision of automatically expanded 
entity sets by using minimal human negative 
judgments. We showed that systematic errors 
which arise from the semantic ambiguity inherent 
in seed instances can be leveraged to automatically 
refine entity sets. We proposed two techniques: 
SIM which boldly removes instances that are dis-
tributionally similar to errors, and FMM which 
more conservatively removes features from the 
seed set representing its unintended (ambiguous) 
concept in order to rank lower potential errors. 
We showed empirical evidence that average R-
precision over random entity sets improves by 26% 
to 51% when given from 5 to 10 manually tagged 
errors. These results were reported by testing the 
refinement algorithms on a set of 50 randomly 
chosen entity sets expanded using a state of the art 
expansion algorithm. Given very small amounts of 
manual judgments, the SIM method outperformed 
FMM (up to 4 manual judgments). FMM outper-
formed the SIM method given more than 6 manual 
judgments. Both proposed refinement models have 
linear time complexity in set size allowing for 
practical online use in set expansion systems. 
This paper only addresses techniques for re-
moving erroneous entities from expanded entity 
sets. A complimentary way to improve perfor-
mance would be to investigate the addition of rele-
vant candidate expansions that are not already in 
the initial expansion. We are currently investigat-
ing extensions to FMM that can efficiently add 
new candidate expansions to the set by computing 
the similarity between modified centroids and all 
terms occurring in a large body of text. 
We are also investigating ways to use the find-
ings of this work to a priori remove ambiguous 
seed instances (or their ambiguous contexts) before 
running the initial expansion algorithm. It is our 
hope that most of the errors identified in this work 
could be automatically discovered without any 
manual judgments. 
References 
Abney, S. Parsing by Chunks. 1991. In: Robert Ber-
wick, Steven Abney and Carol Tenny (eds.), Prin-
ciple-Based Parsing. Kluwer Academic Publishers, 
Dordrecht. 
Banko, M. and Brill, E. 2001. Scaling to very large cor-
pora for natural language disambiguation. In Pro-
ceedings of ACL-2001.pp 26-33. Morristown, NJ. 
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, 
M.; Etzioni, O. 2007. Open Information Extraction 
from the Web. In Proceedings of IJCAI-07. 
Bayardo, R. J; Yiming Ma,; Ramakrishnan Srikant.; 
Scaling Up All-Pairs Similarity Search. In Proc. of 
the 16th Int'l Conf. on World Wide Web. pp 131-140 
2007. 
Brill, E. 1995. Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In 
Proceedings of ACL-04.pp. 438-445. 
Cohn, D. A., Atlas, L., and Ladner, R. E. 1994. Improv-
ing Generalization with Active Learning. Machine 
Learning, 15(2):201-221. Springer, Netherlands. 
Dagan, I. and Engelson, S. P. 1995. Selective Sampling 
in Natural Language Learning. In Proceedings of 
IJCAI-95 Workshop on New Approaches to Learning 
for Natural Language Processing. Montreal, Canada. 
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating 
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07. 
297
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; 
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. 
Unsupervised named-entity extraction from the Web: 
An Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Harris, Z. 1985. Distributional Structure. In: Katz, J. J. 
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47. 
Harman, D. 1992. Relevance feedback revisited. In 
Proceeedings of SIGIR-92. Copenhagen, Denmark. 
Hearst, M. A. 1992.Automatic acquisition of hyponyms 
from large text corpora.In Proceedings of COLING-
92. Nantes, France. 
Kozareva, Z., Riloff, E. and Hovy, E. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs.In Proceedings of ACL-08.pp 1048-
1056. Columbus, OH 
Lin, D. 1998.Automatic retrieval and clustering of simi-
lar words.In Proceedings of COLING/ACL-98.pp. 
768?774. Montreal, Canada. 
Nadeau, D., Turney, P. D. and Matwin., S. 2006. Unsu-
pervised Named-Entity Recognition: Generating Ga-
zetteers and Resolving Ambiguity. In Advances in 
Artifical Intelligence.pp 266-277. Springer Berlin, 
Heidelberg. 
Negri, M. 2004. Sense-based blind relevance feedback 
for question answering. In Proceedings of SIGIR-04 
Workshop on Information Retrieval For Question 
Answering (IR4QA). Sheffield, UK, 
Pantel, P. and Lin, D. 2002. Discovering Word Senses 
from Text. In Proceedings of KDD-02.pp. 613-619. 
Edmonton, Canada. 
Pa?ca, M. 2007a.Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM-07.pp. 683-690. 
Pasca, M. 2007b. Organizing and Searching the World 
Wide Web of Facts - Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07. pp. 
101-110. 
Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 
2006. Names and Similarities on the Web: Fact Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006.pp. 113-120. 
Pa?ca, M. and Durme, B.J. 2008. Weakly-supervised 
Acquisition of Open-Domain Classes and Class 
Attributes from Web Documents and Query Logs. In 
Proceedings of ACL-08. 
Riloff, E. and Jones, R. 1999 Learning Dictionaries for 
Information Extraction by Multi-Level Boostrap-
ping.In Proceedings of AAAI/IAAAI-99. 
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons.In Proceedings 
of EMNLP-97. 
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 
2007. ?More like these?: growing entity classes from 
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal. 
Stevenson, M., Guo, Y. and  Gaizauskas, R. 2008. Ac-
quiring Sense Tagged Examples using Relevance 
Feedback. In Proceedings ofCOLING-08. Manches-
ter UK. 
Tang, M., Luo, X., and Roukos, S. 2001. Active learn-
ing for statistical natural language parsing.In Pro-
ceedings of ACL-2001.pp 120 -127. Philadelphia, 
PA. 
Vishwa. V, Wood, K., Milic-Frayling, N. and Cox, I. J. 
2005. Comparing Relevance Feedback Algorithms 
for Web Search. In Proceedings of WWW 2005. Chi-
ba, Japan. 
Wang. R.C. and Cohen, W.W. 2007.Language-
Independent Set Expansion of Named Entities Using 
the Web.In Proceedings of ICDM-07. 
Zhou, X. S. and Huang, S. T. 2003. Relevance Feedback 
in Image Retrieval: A Comprehensive Review - 
Xiang Sean Zhou, Thomas S. Huang Multimedia 
Systems. pp 8:536-544. 
Zhou, G. and Su, J. 2001. Named entity recognition 
using an HMM-based chunk tagger. In Proceedings 
of ACL-2001.pp. 473-480. Morristown, NJ. 
 
 
298
  
Concept Discovery from Text 
 
Dekang Lin and Patrick Pantel 
Department of Computing Science 
University of Alberta 
Edmonton, Alberta, Canada, T6G 2E8 
{lindek,ppantel}@cs.ualberta.ca 
 
 
Abstract 
Broad-coverage lexical resources such as 
WordNet are extremely useful. However, 
they often include many rare senses while 
missing domain-specific senses. We present 
a clustering algorithm called CBC (Cluster-
ing By Committee) that automatically 
discovers concepts from text. It initially 
discovers a set of tight clusters called 
committees that are well scattered in the 
similarity space. The centroid of the 
members of a committee is used as the 
feature vector of the cluster. We proceed by 
assigning elements to their most similar 
cluster. Evaluating cluster quality has 
always been a difficult task. We present a 
new evaluation methodology that is based 
on the editing distance between output 
clusters and classes extracted from WordNet 
(the answer key). Our experiments show that 
CBC outperforms several well-known 
clustering algorithms in cluster quality. 
1 Introduction 
Broad-coverage lexical resources such as 
WordNet are extremely useful in applications 
such as Word Sense Disambiguation (Leacock, 
Chodorow, Miller 1998) and Question-
Answering (Pasca and Harabagiu 2001). 
However, they often include many rare senses 
while missing domain-specific senses. For 
example, in WordNet, the words dog, computer 
and company all have a sense that is a hyponym 
of person. Such rare senses make it difficult for 
a coreference resolution system to use WordNet 
to enforce the constraint that personal pronouns 
(e.g. he or she) must refer to a person. On the 
other hand, WordNet misses the user-interface-
object sense of the word dialog (as often used in 
software manuals). One way to deal with these 
problems is to use a clustering algorithm to 
automatically induce semantic classes (Lin and 
Pantel 2001). 
Many clustering algorithms represent a cluster 
by the centroid of all of its members (e.g., K-
means) (McQueen 1967) or by a representative 
element (e.g., K-medoids) (Kaufmann and 
Rousseeuw 1987). When averaging over all 
elements in a cluster, the centroid of a cluster 
may be unduly influenced by elements that only 
marginally belong to the cluster or by elements 
that also belong to other clusters. For example, 
when clustering words, we can use the contexts 
of the words as features and group together the 
words that tend to appear in similar contexts. For 
instance, U.S. state names can be clustered this 
way because they tend to appear in the following 
contexts: 
(List A) ___ appellate court campaign in ___ 
 ___ capital governor of ___ 
 ___ driver's license illegal in ___ 
 ___ outlaws sth. primary in ___ 
 ___'s sales tax senator for ___ 
If we create a centroid of all the state names, the 
centroid will also contain features such as: 
(List B) ___'s airport archbishop of ___ 
 ___'s business district fly to ___ 
 ___'s mayor mayor of ___ 
 ___'s subway outskirts of ___ 
because some of the state names (like New York 
and Washington) are also names of cities. 
Using a single representative from a cluster 
may be problematic too because each individual 
element has its own idiosyncrasies that may not 
be shared by other members of the cluster. 
In this paper, we propose a clustering algo-
rithm, CBC (Clustering By Committee), in 
which the centroid of a cluster is constructed by 
averaging the feature vectors of a subset of the 
cluster members. The subset is viewed as a 
committee that determines which other elements 
belong to the cluster. By carefully choosing 
committee members, the features of the centroid 
tend to be the more typical features of the target 
  
class. For example, our system chose the 
following committee members to compute the 
centroid of the state cluster: Illinois, Michigan, 
Minnesota, Iowa, Wisconsin, Indiana, Nebraska 
and Vermont. As a result, the centroid contains 
only features like those in List A. 
Evaluating clustering results is a very difficult 
task. We introduce a new evaluation methodol-
ogy that is based on the editing distance between 
output clusters and classes extracted from 
WordNet (the answer key). 
2 Previous Work 
Clustering algorithms are generally categorized 
as hierarchical and partitional. In hierarchical 
agglomerative algorithms, clusters are 
constructed by iteratively merging the most 
similar clusters. These algorithms differ in how 
they compute cluster similarity. In single-link 
clustering, the similarity between two clusters is 
the similarity between their most similar 
members while complete-link clustering uses the 
similarity between their least similar members. 
Average-link clustering computes this similarity 
as the average similarity between all pairs of 
elements across clusters. The complexity of 
these algorithms is O(n2logn), where n is the 
number of elements to be clustered (Jain, Murty, 
Flynn 1999).  
Chameleon is a hierarchical algorithm that 
employs dynamic modeling to improve 
clustering quality (Karypis, Han, Kumar 1999). 
When merging two clusters, one might consider 
the sum of the similarities between pairs of 
elements across the clusters (e.g. average-link 
clustering). A drawback of this approach is that 
the existence of a single pair of very similar 
elements might unduly cause the merger of two 
clusters. An alternative considers the number of 
pairs of elements whose similarity exceeds a 
certain threshold (Guha, Rastogi, Kyuseok 
1998). However, this may cause undesirable 
mergers when there are a large number of pairs 
whose similarities barely exceed the threshold. 
Chameleon clustering combines the two 
approaches. 
K-means clustering is often used on large data 
sets since its complexity is linear in n, the 
number of elements to be clustered. K-means is 
a family of partitional clustering algorithms that 
iteratively assigns each element to one of K 
clusters according to the centroid closest to it 
and recomputes the centroid of each cluster as 
the average of the cluster?s elements. K-means 
has complexity O(K?T?n) and is efficient for 
many clustering tasks. Because the initial 
centroids are randomly selected, the resulting 
clusters vary in quality. Some sets of initial 
centroids lead to poor convergence rates or poor 
cluster quality. 
Bisecting K-means (Steinbach, Karypis, 
Kumar 2000), a variation of K-means, begins 
with a set containing one large cluster consisting 
of every element and iteratively picks the largest 
cluster in the set, splits it into two clusters and 
replaces it by the split clusters. Splitting a cluster 
consists of applying the basic K-means 
algorithm ? times with K=2 and keeping the 
split that has the highest average element-
centroid similarity. 
Hybrid clustering algorithms combine 
hierarchical and partitional algorithms in an 
attempt to have the high quality of hierarchical 
algorithms with the efficiency of partitional 
algorithms. Buckshot (Cutting, Karger, 
Pedersen, Tukey 1992) addresses the problem of 
randomly selecting initial centroids in K-means 
by combining it with average-link clustering. 
Buckshot first applies average-link to a random 
sample of n  elements to generate K clusters. It 
then uses the centroids of the clusters as the 
initial K centroids of K-means clustering. The 
sample size counterbalances the quadratic 
running time of average-link to make Buckshot 
efficient: O(K?T?n + nlogn). The parameters K 
and T are usually considered to be small 
numbers.  
3 Word Similarity 
Following (Lin 1998), we represent each word 
by a feature vector. Each feature corresponds to 
a context in which the word occurs. For 
example, ?threaten with __? is a context. If the 
word handgun occurred in this context, the 
context is a feature of handgun. The value of the 
feature is the pointwise mutual information 
(Manning and Sch?tze 1999 p.178) between the 
feature and the word. Let c be a context and 
Fc(w) be the frequency count of a word w 
occurring in context c. The pointwise mutual 
information between c and w is defined as: 
( )
( ) ( )
N
jF
N
wF
N
wF
cw
j
c
i
i
c
mi ???
=,  
  
where N = ( )??
i j
i jF  is the total frequency 
counts of all words and their contexts. A well-
known problem with mutual information is that 
it is biased towards infrequent words/features. 
We therefore multiplied miw,c with a discounting 
factor: 
( )
( )
( ) ( )
( ) ( ) 11 +???
?
???
?
???
?
???
?
?+ ? ?
? ?
i j
ci
i j
ci
c
c
jF,wFmin
jF,wFmin
wF
wF  
We compute the similarity between two words 
wi and wj using the cosine coefficient (Salton and 
McGill 1983) of their mutual information 
vectors: 
( ) ??
?
?
?
=
c
cw
c
cw
c
cwcw
ji
ji
ji
mimi
mimi
w,wsim
22
 
4 CBC Algorithm 
CBC consists of three phases. In Phase I, we 
compute each element?s top-k similar elements. 
In our experiments, we used k = 20. In Phase II, 
we construct a collection of tight clusters, where 
the elements of each cluster form a committee. 
The algorithm tries to form as many committees 
as possible on the condition that each newly 
formed committee is not very similar to any 
existing committee. If the condition is violated, 
the committee is simply discarded. In the final 
phase of the algorithm, each element is assigned 
to its most similar cluster. 
4.1. Phase I: Find top-similar elements 
Computing the complete similarity matrix 
between pairs of elements is obviously 
quadratic. However, one can dramatically reduce 
the running time by taking advantage of the fact 
that the feature vector is sparse. By indexing the 
features, one can retrieve the set of elements that 
have a given feature. To compute the top similar 
words of a word w, we first sort w?s features 
according to their mutual information with w. 
We only compute pairwise similarities between 
w and the words that share a high mutual 
information feature with w. 
4.2. Phase II: Find committees 
The second phase of the clustering algorithm 
recursively finds tight clusters scattered in the 
similarity space. In each recursive step, the 
algorithm finds a set of tight clusters, called 
committees, and identifies residue elements that 
are not covered by any committee. We say a 
committee covers an element if the element?s 
similarity to the centroid of the committee 
exceeds some high similarity threshold. The 
algorithm then recursively attempts to find more 
committees among the residue elements. The 
output of the algorithm is the union of all 
committees found in each recursive step. The 
details of Phase II are presented in Figure 1. 
In Step 1, the score reflects a preference for 
bigger and tighter clusters. Step 2 gives 
preference to higher quality clusters in Step 3, 
where a cluster is only kept if its similarity to all 
previously kept clusters is below a fixed 
threshold. In our experiments, we set ?1 = 0.35. 
Input: A list of elements E to be clustered, a 
similarity database S from Phase I, thresh-
olds ?1 and ?2. 
Step 1: For each element e ? E 
  Cluster the top similar elements of e from S 
using average-link clustering. 
  For each cluster discovered c compute the 
following score: |c| ? avgsim(c), where 
|c| is the number of elements in c and 
avgsim(c) is the average pairwise simi-
larity between elements in c. 
  Store the highest-scoring cluster in a list L. 
Step 2: Sort the clusters in L in descending order of 
their scores. 
Step 3: Let C be a list of committees, initially 
empty. 
  For each cluster c ? L in sorted order 
  Compute the centroid of c by averaging the 
frequency vectors of its elements and 
computing the mutual information vector 
of the centroid in the same way as we did 
for individual elements.  
  If c?s similarity to the centroid of each 
committee previously added to C is be-
low a threshold ?1, add c to C. 
Step 4: If C is empty, we are done and return C. 
Step 5: For each element e ? E 
  If e?s similarity to every committee in C is 
below threshold ?2, add e to a list of resi-
dues R.   
Step 6: If R is empty, we are done and return C. 
  Otherwise, return the union of C and the 
output of a recursive call to Phase II us-
ing the same input except replacing E 
with R. 
Output: A list of committees. 
Figure 1. Phase II of CBC. 
  
Step 4 terminates the recursion if no committee 
is found in the previous step. The residue 
elements are identified in Step 5 and if no 
residues are found, the algorithm terminates; 
otherwise, we recursively apply the algorithm to 
the residue elements. 
Each committee that is discovered in this 
phase defines one of the final output clusters of 
the algorithm. 
4.3. Phase III: Assign elements to clusters 
In Phase III, every element is assigned to the 
cluster containing the committee to which it is 
most similar. This phase resembles K-means in 
that every element is assigned to its closest 
centroid. Unlike K-means, the number of 
clusters is not fixed and the centroids do not 
change (i.e. when an element is added to a 
cluster, it is not added to the committee of the 
cluster). 
5 Evaluation Methodology 
Many cluster evaluation schemes have been 
proposed. They generally fall under two 
categories: 
? comparing cluster outputs with manually 
generated answer keys (hereon referred to 
as classes); or 
? embedding the clusters in an application 
and using its evaluation measure. 
An example of the first approach considers the 
average entropy of the clusters, which measures 
the purity of the clusters (Steinbach, Karypis, 
and Kumar 2000). However, maximum purity is 
trivially achieved when each element forms its 
own cluster. An example of the second approach 
evaluates the clusters by using them to smooth 
probability distributions (Lee and Pereira 1999). 
Like the entropy scheme, we assume that there 
is an answer key that defines how the elements 
are supposed to be clustered. Let C be a set of 
clusters and A be the answer key. We define the 
editing distance, dist(C, A), as the number of 
operations required to make C consistent with A. 
We say that C is consistent with A if there is a 
one to one mapping between clusters in C and 
the classes in A such that for each cluster c in C, 
all elements of c belong to the same class in A. 
We allow two editing operations: 
? merge two clusters; and 
? move an element from one cluster to 
another. 
Let B be the baseline clustering where each 
element is its own cluster. We define the quality 
of a set of clusters C as follows: 
( )
( )ABdist
ACdist
,
,1?  
Suppose the goal is to construct a clustering 
consistent with the answer key. This measure 
can be interpreted as the percentage of 
operations saved by starting from C versus 
starting from the baseline.  
We aim to construct a clustering consistent 
with A as opposed to a clustering identical to A 
because some senses in A may not exist in the 
corpus used to generate C. In our experiments, 
we extract answer classes from WordNet. The 
word dog belongs to both the Person and Animal 
classes. However, in the newspaper corpus, the 
Person sense of dog is at best extremely rare. 
There is no reason to expect a clustering 
algorithm to discover this sense of dog. The 
baseline distance dist(B, A) is exactly the 
number of elements to be clustered. 
We made the assumption that each element 
belongs to exactly one cluster. The transforma-
tion procedure is as follows: 
1. Suppose there are m classes in the answer 
key. We start with a list of m empty sets, 
each of which is labeled with a class in the 
answer key. 
2. For each cluster, merge it with the set 
whose class has the largest number of 
elements in the cluster (a tie is broken 
arbitrarily). 
3. If an element is in a set whose class is not 
the same as one of the element?s classes, 
move the element to a set where it be-
longs. 
dist(C, A) is the number of operations performed 
using the above transformation rules on C. 
a
b
e
c
d
e
a
c
d
b e
b
a
c
d
e
a
b
c
d
e
A) B)
C) D) E)
Figure 2. An example of applying the transformation rules 
to three clusters. A) The classes in the answer key; B) the 
clusters to be transformed; C) the sets used to reconstruct 
the classes (Rule 1); D) the sets after three merge 
operations (Step 2); E) the sets after one move operation 
(Step 3). 
  
Figure 2 shows an example. In D) the cluster 
containing e could have been merged with either 
set (we arbitrarily chose the second). The total 
number of operations is 4. 
6 Experimental Results 
We generated clusters from a news corpus using 
CBC and compared them with classes extracted 
from WordNet (Miller 1990). 
6.1. Test Data 
To extract classes from WordNet, we first 
estimate the probability of a random word 
belonging to a subhierarchy (a synset and its 
hyponyms). We use the frequency counts of 
synsets in the SemCor corpus (Landes, Leacock, 
Tengi 1998) to estimate the probability of a 
subhierarchy. Since SemCor is a fairly small 
corpus, the frequency counts of the synsets in 
the lower part of the WordNet hierarchy are very 
sparse. We smooth the probabilities by assuming 
that all siblings are equally likely given the 
parent. A class is then defined as the maximal 
subhierarchy with probability less than a 
threshold (we used e-2). 
We used Minipar 1  (Lin 1994), a broad-
coverage English parser, to parse about 1GB 
(144M words) of newspaper text from the TREC 
collection (1988 AP Newswire, 1989-90 LA 
Times, and 1991 San Jose Mercury) at a speed 
of about 500 words/second on a PIII-750 with 
512MB memory. We collected the frequency 
counts of the grammatical relationships 
(contexts) output by Minipar and used them to 
compute the pointwise mutual information 
values from Section 3. The test set is constructed 
by intersecting the words in WordNet with the 
nouns in the corpus whose total mutual 
information with all of its contexts exceeds a 
threshold m. Since WordNet has a low coverage 
of proper names, we removed all capitalized 
nouns. We constructed two test sets: S13403 
consisting of 13403 words (m = 250) and S3566 
consisting of 3566 words (m = 3500). We then 
removed from the answer classes the words that 
did not occur in the test sets. Table 1 summa-
rizes the test sets. The sizes of the WordNet 
classes vary a lot. For S13403 there are 99 classes 
that contain three words or less and the largest 
class contains 3246 words. For S3566, 78 classes 
have three or less words and the largest class 
contains 1181 words. 
                                                 
1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 
6.2. Cluster Evaluation 
We clustered the test sets using CBC and the 
clustering algorithms of Section 2 and applied 
the evaluation methodology from the previous 
section. Table 2 shows the results. The columns 
are our editing distance based evaluation 
measure. Test set S3566 has a higher score for all 
algorithms because it has a higher number of 
average features per word than S13403. 
For the K-means and Buckshot algorithms, we 
set the number of clusters to 250 and the 
maximum number of iterations to 8. We used a 
sample size of 2000 for Buckshot. For the 
Bisecting K-means algorithm, we applied the 
basic K-means algorithm twice (? = 2 in Section 
2) with a maximum of 8 iterations per split. Our 
implementation of Chameleon was unable to 
complete clustering S13403 in reasonable time due 
to its time complexity.  
Table 2 shows that K-means, Buckshot and 
Average-link have very similar performance. 
CBC outperforms all other algorithms on both 
data sets.  
6.3. Manual Inspection 
Let c be a cluster and wn(c) be the WordNet 
class that has the largest intersection with c. The 
precision of c is defined as: 
Table 1. A description of the test sets in our experiments.
DATA 
SET 
TOTAL 
WORDS 
m Average # 
of Features 
TOTAL 
CLASSES 
S13403 13403 250 740.8 202 
S3566 3566 3500 2218.3 150 
 
DATA 
SET 
TOTAL 
WORDS 
M Avg. Features 
per Word 
13403 250 740.8 
3566 3500 2218.3 
Table 2. Cluster quality (%) of several clustering 
algorithms on the test sets. 
ALGORITHM S13403 S3566 
CBC 60.95 65.82 
K-means (K=250) 56.70 62.48 
Buckshot 56.26 63.15 
Bisecting K-means  43.44 61.10 
Chameleon n/a 60.82 
Average-link 56.26 62.62 
Complete-link 49.80 60.29 
Single-link 20.00 31.74 
 
  
( ) ( )
c
cwnc
cprecision
?=  
CBC discovered 943 clusters. We sorted them 
according to their precision. Table 3 shows five 
of the clusters evenly distributed according to 
their precision ranking along with their Top-15 
features with highest mutual-information. The 
words in the clusters are listed in descending 
order of their similarity to the cluster centroid. 
For each cluster c, we also include wn(c). The 
underlined words are in wn(c). The first cluster 
is clearly a cluster of firearms and the second 
one is of pests. In WordNet, the word pest is 
curiously only under the person hierarchy. The 
words stopwatch and houseplant do not belong 
to the clusters but they have low similarity to 
their cluster centroid. The third cluster 
represents some kind of control. In WordNet, the 
legal power sense of jurisdiction is not a 
hyponym of social control as are supervision, 
oversight and governance. The fourth cluster is 
about mixtures. The words blend and mix as the 
event of mixing are present in WordNet but not 
as the result of mixing. The last cluster is about 
consumers. Here is the consumer class in 
WordNet 1.5: 
addict, alcoholic, big spender, buyer, client, 
concert-goer, consumer, customer, cutter, diner, 
drinker, drug addict, drug user, drunk, eater, 
feeder, fungi, head, heroin addict, home buyer, 
junkie, junky, lush, nonsmoker, patron, policy-
holder, purchaser, reader, regular, shopper, 
smoker, spender, subscriber, sucker, taker, user, 
vegetarian, wearer 
In our cluster, only the word client belongs to 
WordNet?s consumer class. The cluster is ranked 
very low because WordNet failed to consider 
words like patient, tenant and renter as 
consumers.  
Table 3 shows that even the lowest ranking 
CBC clusters are fairly coherent. The features 
associated with each cluster can be used to 
classify previously unseen words into one or 
more existing clusters. 
Table 4 shows the clusters containing the word 
cell that are discovered by various clustering 
algorithms from S13403. The underlined words 
represent the words that belong to the cell class 
in WordNet. The CBC cluster corresponds 
almost exactly to WordNet?s cell class. K-means 
and Buckshot produced fairly coherent clusters. 
The cluster constructed by Bisecting K-means is 
obviously of inferior quality. This is consistent 
with the fact that Bisecting K-means has a much 
lower score on S13403 compared to CBC, K-
means and Buckshot. 
Table 3. Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual 
information and the WordNet classes that have the largest intersection with each cluster. 
RANK MEMBERS TOP-15 FEATURES wn(c) 
1 handgun, revolver, shotgun, pistol, rifle, 
machine gun, sawed-off shotgun, 
submachine gun, gun, automatic pistol, 
automatic rifle, firearm, carbine, 
ammunition, magnum, cartridge,  
automatic, stopwatch 
__ blast, barrel of __ , brandish __, fire __, point __, 
pull out __, __ discharge, __ fire, __ go off, arm with 
__, fire with __, kill with __, open fire with __, shoot 
with __, threaten with __ 
artifact / artifact 
236 whitefly, pest, aphid, fruit fly, termite, 
mosquito, cockroach, flea, beetle, killer 
bee, maggot, predator, mite, houseplant, 
cricket 
__ control, __ infestation, __ larvae, __ population, 
infestation of __, specie of __, swarm of __ , attract 
__, breed __, eat __, eradicate __, feed on __, get rid 
of __, repel __, ward off __ 
animal / animate being / 
beast / brute / creature / 
fauna 
471 supervision, discipline, oversight, 
control, governance, decision making, 
jurisdiction 
breakdown in __, lack of __ , loss of __, assume __, 
exercise __, exert __, maintain __, retain __, seize __, 
tighten __, bring under __, operate under __, place 
under __, put under __, remain under __ 
act / human action / 
human activity 
706 blend, mix, mixture, combination, 
juxtaposition, combine, amalgam, 
sprinkle, synthesis, hybrid, melange 
dip in __, marinate in __, pour in __, stir in __, use in 
__, add to __, pour __, stir __, curious __, eclectic __, 
ethnic __, odd __, potent __, unique __, unusual __ 
group / grouping 
941 employee, client, patient, applicant,  
tenant, individual, participant, renter, 
volunteer, recipient, caller, internee, 
enrollee, giver 
benefit for __, care for __, housing for __, benefit to 
__, service to __, filed by __, paid by __, use by __, 
provide for __, require for --, give to __, offer to __, 
provide to __, disgruntled __, indigent __ 
worker 
 
  
7 Conclusion 
We presented a clustering algorithm, CBC, for 
automatically discovering concepts from text. It 
can handle a large number of elements, a large 
number of output clusters, and a large sparse 
feature space. It discovers clusters using well-
scattered tight clusters called committees. In our 
experiments, we showed that CBC outperforms 
several well known hierarchical, partitional, and 
hybrid clustering algorithms in cluster quality. 
For example, in one experiment, CBC 
outperforms K-means by 4.25%. 
By comparing the CBC clusters with WordNet 
classes, we not only find errors in CBC, but also 
oversights in WordNet. 
Evaluating cluster quality has always been a 
difficult task. We presented a new evaluation 
methodology that is based on the editing 
distance between output clusters and classes 
extracted from WordNet (the answer key). 
Acknowledgements 
The authors wish to thank the reviewers for their 
helpful comments. This research was partly 
supported by Natural Sciences and Engineering 
Research Council of Canada grant OGP121338 
and scholarship PGSB207797. 
References 
Cutting, D. R.; Karger, D.; Pedersen, J.; and Tukey, J. W. 1992. 
Scatter/Gather: A cluster-based approach to browsing large 
document collections. In Proceedings of SIGIR-92. pp. 318?329. 
Copenhagen, Denmark. 
Guha, S.; Rastogi, R.; and Kyuseok, S. 1999. ROCK: A robust 
clustering algorithm for categorical attributes. In Proceedings of 
ICDE?99. pp. 512?521. Sydney, Australia. 
Jain, A. K.; Murty, M. N.; and Flynn, P. J. 1999. Data Clustering: A 
Review. ACM Computing Surveys 31(3):264?323. 
Kaufmann, L. and Rousseeuw, P. J. 1987. Clustering by means of 
medoids. In Dodge, Y. (Ed.) Statistical Data Analysis based on the 
L1 Norm. pp. 405?416. Elsevier/North Holland, Amsterdam. 
Karypis, G.; Han, E.-H.; and Kumar, V. 1999. Chameleon: A 
hierarchical  clustering algorithm using dynamic modeling. IEEE 
Computer: Special Issue on Data Analysis and Mining 32(8):68?75. 
Landes, S.; Leacock, C.; and Tengi, R. I. 1998. Building Semantic 
Concordances. In WordNet: An Electronic Lexical Database, edited 
by C. Fellbaum. pp. 199-216. MIT Press. 
Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using corpus 
statistics and WordNet relations for sense identification. 
Computational Linguistics, 24(1):147-165. 
Lee, L. and Pereira, F. 1999. Distributional similarity models: 
Clustering vs. nearest neighbors. In Proceedings of ACL-99. pp. 33-
40. College Park, MD. 
Lin, D. 1994. Principar - an Efficient, Broad-Coverage, Principle-Based 
Parser. In Proceedings of COLING-94. pp. 42-48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar words. In 
Proceedings of COLING/ACL-98. pp. 768-774. Montreal, Canada. 
Lin, D. and Pantel, P. 2001. Induction of semantic classes from natural 
language text. In Proceedings of SIGKDD-01. pp. 317-322. San 
Francisco, CA. 
Manning, C. D. and Sch?tze, H. 1999. Foundations of Statistical 
Natural Language Processing. MIT Press. 
McQueen, J. 1967. Some methods for classification and analysis of 
multivariate observations. In Proceedings of 5th Berkeley Symposium 
on Mathematics, Statistics and Probability, 1:281-298. 
Miller, G. 1990. WordNet: An Online Lexical Database. International 
Journal of Lexicography, 1990. 
Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in 
Open-Domain Question Answering. In Proceedings of NAACL-01 
Workshop on WordNet and Other Lexical Resources. pp. 138-143. 
Pittsburgh, PA. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern Information 
Retrieval. McGraw Hill. 
Steinbach, M.; Karypis, G.; and Kumar, V. 2000. A comparison of 
document clustering techniques. Technical Report #00-034. 
Department of Computer Science and Engineering, University of 
Minnesota.s 
Table 4. The clusters representing the cell concept for several clustering algorithms using S13403. 
ALGORITHMS CLUSTERS THAT HAVE THE LARGEST INTERSECTION WITH THE WORDNET CELL CLASS. 
CBC white blood cell, red blood cell, brain cell, cell, blood cell, cancer cell, nerve cell, embryo, neuron 
K-means cadaver, meteorite, secretion, receptor, serum, handwriting, cancer cell, thyroid, body part, hemoglobin, red blood 
cell, nerve cell, urine, gene, chromosome, embryo, plasma, heart valve, saliva, ovary, white blood cell, intestine, 
lymph node, sperm, heart, colon, cell, blood, bowel, brain cell, central nervous system, spinal cord, blood cell, 
cornea, bladder, prostate, semen, brain, spleen, organ, nervous system, pancreas, tissue, marrow, liver, lung, 
marrow, kidney 
Buckshot cadaver, vagina, meteorite, human body, secretion, lining, handwriting, cancer cell, womb, vein, bloodstream, 
body part, eyesight, polyp, coronary artery, thyroid, membrane, red blood cell, plasma, gene, gland, embryo, 
saliva, nerve cell, chromosome, skin, white blood cell, ovary, sperm, uterus, blood, intestine, heart, spinal cord, 
cell, bowel, colon, blood vessel, lymph node, brain cell, central nervous system, blood cell, semen, cornea, 
prostate, organ, brain, bladder, spleen, nervous system, tissue, pancreas, marrow, liver, lung, bone marrow, kidney 
Bisecting K-means picket line, police academy, sphere of influence, bloodstream, trance, sandbox, downtown, mountain, camera, 
boutique, kitchen sink, kiln, embassy, cellblock, voting booth, drawer, cell, skylight, bookcase, cupboard, 
ballpark, roof, stadium, clubhouse, tub, bathtub, classroom, toilet, kitchen, bathroom, 
WordNet Class blood cell, brain cell, cancer cell, cell, cone, egg, nerve cell, neuron, red blood cell, rod, sperm, white blood cell 
 
Automatically Discovering Word Senses 
Patrick Pantel and Dekang Lin 
Department of Computing Science  
University of Alberta 
Edmonton, Alberta T6G 2E8 Canada 
{ppantel, lindek}@cs.ualberta.ca 
 
 
 
 
Abstract  
We will demonstrate the output of a distribu-
tional clustering algorithm called Clustering 
by Committee that automatically discovers 
word senses from text1. 
1 Introduction 
Using word senses versus word forms is useful in many 
applications such as information retrieval (Voorhees 
1998), machine translation (Hutchins and Sommers 
1992), and question-answering (Pasca and Harabagiu 
2001). 
The Distributional Hypothesis (Harris 1985) states 
that words that occur in the same contexts tend to be 
similar. There have been many approaches to compute 
the similarity between words based on their distribution 
in a corpus (Hindle 1990; Landauer and Dumais 1997; 
Lin 1998). The output of these programs is a ranked list 
of similar words to each word. For example, Lin?s 
approach outputs the following similar words for wine 
and suit: 
wine: beer, white wine, red wine, 
Chardonnay, champagne, fruit, food, 
coffee, juice, Cabernet, cognac, 
vinegar, Pinot noir, milk, vodka,? 
suit: lawsuit, jacket, shirt, pant, dress, 
case, sweater, coat, trouser, claim, 
business suit, blouse, skirt, litiga-
tion, ? 
The similar words of wine represent the meaning of 
wine. However, the similar words of suit represent a 
mixture of its clothing and litigation senses. Such lists 
of similar words do not distinguish between the 
multiple senses of polysemous words. 
                                                     
1 The demonstration is currently available online at 
www.cs.ualberta.ca/~lindek/demos/wordcluster.htm. 
We will demonstrate the output of a distributional 
clustering algorithm called Clustering by Committee 
(CBC) that discovers word senses automatically from 
text. Each cluster that a word belongs to corresponds to 
a sense of the word. The following is a sample output 
from our algorithm: 
(suit 
 0.39 (blouse, slack, legging, sweater) 
 0.20 (lawsuit, allegation, case, charge) 
) 
(plant 
 0.41 (plant, factory, facility, 
refinery) 
 0.20 (shrub, ground cover, perennial, 
bulb) 
) 
(heart 
 0.27 (kidney, bone marrow, marrow, 
liver) 
 0.17 (psyche, consciousness, soul, mind) 
) 
Each entry shows the clusters to which the head-
word belongs along with its similarity to the cluster. 
The lists of words are the top-4 most similar members 
to the cluster centroid. Each cluster corresponds to a 
sense of the headword. 
2 Feature Representation 
Following (Lin 1998), we represent each word by a 
feature vector. Each feature corresponds to a context in 
which the word occurs. For example, ?sip __? is a verb-
object context. If the word wine occurred in this 
context, the context is a feature of wine. These features 
are obtained by parsing a large corpus using Minipar 
(Lin 1994), a broad-coverage English parser. The value 
of the feature is the pointwise mutual information 
(Manning and Sch?tze 1999) between the feature and 
the word. Let c be a context and Fc(w) be the frequency 
count of a word w occurring in context c. The pointwise 
mutual information, miw,c, between c and w is defined 
as: 
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 21-22
                                                         Proceedings of HLT-NAACL 2003
( )
( ) ( )
N
jF
N
wF
N
wF
cw
j
c
i
i
c
mi ???
=,  
where N is the total frequency counts of all words and 
their contexts. We compute the similarity between two 
words wi and wj using the cosine coefficient (Salton and 
McGill 1983) of their mutual information vectors: 
( ) ??
?
?
?
=
c
cw
c
cw
c
cwcw
ji
ji
ji
mimi
mimi
w,wsim
22
 
3 Clustering by Committee 
CBC finds clusters by first discovering the underlying 
structure of the data. It does this by searching for sets 
of representative elements for each cluster, which we 
refer to as committees. The goal is to find committees 
that unambiguously describe the (unknown) target 
classes. By carefully choosing committee members, the 
features of the centroid tend to be the more typical 
features of the target class. For example, our system 
chose the following committee members to compute 
the centroid of the state cluster: Illinois, Michigan, 
Minnesota, Iowa, Wisconsin, Indiana, Nebraska and 
Vermont. States like Washington and New York are not 
part of the committee because they are polysemous. 
The centroid of a cluster is constructed by averaging 
the feature vectors of the committee members. 
CBC consists of three phases. Phase I computes 
each element?s top-k similar elements. In Phase II, we 
do a first pass through the data and discover the 
committees. The goal is that we form tight committees 
(high intra-cluster similarity) that are dissimilar from 
one another (low inter-cluster similarity) and that cover 
the whole similarity space. The method is based on 
finding sub-clusters in the top-similar elements of every 
given element. 
In the final phase of the algorithm, each word is 
assigned to its most similar clusters (represented by a 
committee). Suppose a word w is assigned to a cluster 
c. We then remove from w its features that intersect 
with the features in c. Intuitively, this removes the c 
sense from w, allowing CBC to discover the less 
frequent senses of a word and to avoid discovering 
duplicate senses. The word w is then assigned to its 
next most similar cluster and the process is repeated. 
4 Conclusion 
We will demonstrate the senses discovered by CBC for 
54,685 words on the 3GB ACQUAINT corpus. CBC 
discovered 24,497 polysemous words. 
References 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. 
(ed.) The Philosophy of Linguistics. New York: 
Oxford University Press. pp. 26?47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Hutchins, J. and Sommers, H. 1992. Introduction to 
Machine Translation. Academic Press. 
Landauer, T. K., and Dumais, S. T. 1997. A solution to 
Plato's problem: The Latent Semantic Analysis 
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review, 104:211?
240. 
Lin, D. 1994. Principar - an efficient, broad-coverage, 
principle-based parser. In Proceedings of COLING-
94. pp. 42?48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of 
similar words. In Proceedings of COLING/ACL-98. 
pp. 768?774. Montreal, Canada. 
Manning, C. D. and Sch?tze, H. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press. 
Pasca, M. and Harabagiu, S. 2001. The informative role 
of WordNet in Open-Domain Question Answering. 
In Proceedings of NAACL-01 Workshop on WordNet 
and Other Lexical Resources. pp. 138?143. 
Pittsburgh, PA. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Voorhees, E. M. 1998. Using WordNet for text 
retrieval. In WordNet: An Electronic Lexical 
Database, edited by C. Fellbaum. pp. 285?303. MIT 
Press. 
An Unsupervised Approach to Prepositional Phrase Attachment
using Contextually Similar Words
Patrick Pantel and Dekang Lin
Department of Computing Science
University of Alberta1
Edmonton, Alberta T6G 2H1 Canada
{ppantel, lindek}@cs.ualberta.ca
                                            
1This research was conducted at the University of Manitoba.
Abstract
Prepositional phrase attachment is a
common source of ambiguity in natural
language processing. We present an
unsupervised corpus-based approach to
prepositional phrase attachment that
achieves similar performance to supervised
methods. Unlike previous unsupervised
approaches in which training data is
obtained by heuristic extraction of
unambiguous examples from a corpus, we
use an iterative process to extract training
data from an automatically parsed corpus.
Attachment decisions are made using a
linear combination of features and low
frequency events are approximated using
contextually similar words.
Introduction
Prepositional phrase attachment is a common
source of ambiguity in natural language
processing. The goal is to determine the
attachment site of a prepositional phrase in a
sentence. Consider the following examples:
1. Mary ate the salad with a fork.
2. Mary ate the salad with croutons.
In both cases, the task is to decide whether the
prepositional phrase headed by the preposition
with attaches to the noun phrase (NP) headed by
salad or the verb phrase (VP) headed by ate. In
the first sentence, withattaches to the VP since
Mary is using a fork to eat her salad. In sentence
2, with attaches to the NP since it is the salad
that contains croutons.
Formally, prepositional phrase attachment is
simplified to the following classification task.
Given a 4-tuple of the form (V, N1, P, N2), where
V is the head verb, N1 is the head noun of the
object of V, P is a preposition, and N2 is the head
noun of the prepositional complement, the goal
is to classify as either adverbial attachment
(attaching to V) or adjectival attachment
(attaching to N1). For example, the 4-tuple (eat,
salad, with, fork) has target classification V.
In this paper, we present an unsupervised
corpus-based approach to prepositional phrase
attachment that outperforms previous
unsupervised techniques and approaches the
performance of supervised methods. Unlike
previous unsupervised approaches in which
training data is obtained by heuristic extraction
of unambiguous examples from a corpus, we use
an iterative process to extract training data from
an automatically parsed corpus. The attachment
decision for a 4-tuple (V, N1, P, N2) is made as
follows. First, we replace V and N2 by their
contextually similar words and compute the
average adverbial attachment score. Similarly,
the verage adjectival attachment score is
computed by replacing N1 and N2 by their
contextually similar words. Attachment scores
are obtained using a linear combination of
features of the 4-tuple. Finally, we combine the
average attachment scores with the attachment
score of N2 attaching to the original V and the
attachment score of N2 attaching to the original
N1. The proposed classification represents the
attachment site that scored highest.
1 Previous Work
Altmann and Steedman (1988) showed that
current discourse context is often required for
disambiguating attachments. Recent work shows
that it is generally sufficient to utilize lexical
information (Brill and Resnik, 1994; Collins and
Brooks, 1995; Hindle and Rooth, 1993;
Ratnaparkhi et al, 1994).
One of the earliest corpus-based approaches to
prepositional phrase attachment used lexical
preference by computing co-occurrence
frequencies (lexical associations) of verbs and
nouns with prepositions (Hindle and Rooth,
1993). Training data was obtained by extracting
all phrases of the form (V, N1, P, N2) from a
large parsed corpus.
Supervised methods later improved
attachment accuracy. Ratnaparkhi et al (1994)
used a maximum entropy model considering
only lexical information from within the verb
phrase (ignoring N2). They experimented with
both word features and word class features, their
combination yielding 81.6% attachment
accuracy.
Later, Collins and Brooks (1995) achieved
84.5% accuracy by employing a backed-off
model to smooth for unseen events. They
discovered that P is the most informative lexical
item for attachment disambiguation and keeping
low frequency events increases performance.
A non-statistical supervised approach by Brill
and Resnik (1994) yielded 81.8% accuracy using
a transformation-based approach (Brill, 1995)
and incorporating word-class information. They
report that the top 20 transformations learned
involved specific prepositions supporting
Collins and Brooks? claim that the preposition is
the most important lexical item for resolving the
attachment ambiguity.
The state of the art is a supervised algorithm
that employs a semantically tagged corpus
(Stetina and Nagao, 1997). Each word in a
labelled corpus is sense-tagged using an
unsupervised word-sense disambiguation
algorithm with WordNet (Miller, 1990). Testing
examples are classified using a decision tree
induced from the training examples. They report
88.1% attachment accuracy approaching the
human accuracy of 88.2% (Ratnaparkhi et al,
1994).
The current unsupervised state of the art
achieves 81.9% attachment accuracy
(Ratnaparkhi, 1998). Using an extraction
heuristic, unambiguous prepositional phrase
attachments of the form (V, P, N2) and (N1, P,
N2) are extracted from a large corpus. Co-
occurrence frequencies are then used to
disambiguate examples with ambiguous
attachments.
2 Resources
The input to our algorithm includes a collocation
database and a corpus-based thesaurus, both
available on the Internet2. Below, we briefly
describe these resources.
2.1 Collocation database
Given a word w in a dependency relationship
(such as subject or object), the collocation
database is used to retrieve the words that
occurred in that relationship with w, in a large
corpus, along with their frequencies (Lin,
1998a). Figure 1 shows excerpts of the entries in
                                            
2Available at www.cs.ualberta.ca/~lindek/demos.htm.
eat:
object: almond 1, apple 25, bean 5, beam 1, binge 1,
bread 13, cake 17, cheese 8, dish 14,
disorder 20, egg 31, grape 12, grub 2, hay 3,
junk 1, meat 70, poultry 3, rabbit 4, soup 5,
sandwich 18, pasta 7, vegetable 35, ...
subject: adult 3, animal 8, beetle 1, cat 3, child 41,
decrease 1, dog 24, family 29, guest 7, kid
22, patient 7, refugee 2, rider 1, Russian 1,
shark 2, something 19, We 239, wolf 5, ...
salad:
adj-modifier:assorted 1, crisp 4, fresh 13, good 3, grilled
5, leftover 3, mixed 4, olive 3, prepared 3,
side 4, small 6, special 5, vegetable 3, ...
object-of: add 3, consume 1, dress 1, grow 1, harvest 2,
have 20, like 5, love 1, mix 1, pick 1, place
3, prepare 4, return 3, rinse 1, season 1, serve
8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ...
Figure 1. Excepts of entries in the collocation database for
eat and salad.
Table 1. The top 20 most similar words of eatand salad as
given by (Lin, 1998b).
WORD SIMILAR WORDS (WITH SIMILARITY SCORE)
EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094,
taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep
0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy
0.079, smoke 0.078, harvest 0.076, love 0.076, chop
0.074, sprinkle 0.072, Toss 0.072, chew 0.072
SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta
0.149, dish 0.135, vegetable 0.135, cheese 0.132,
dessert 0.13, entree 0.121, bread 0.116, meat 0.116,
chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11,
dressing 0.109, cake 0.107, steak 0.105, noodle
0.105, bean 0.102
the collocation database for the words eat and
salad. The database contains a total of 11
million unique dependency relationships.
2.2 Corpus-based thesaurus
Using the collocation database, Lin (1998b) used
an unsupervised method to construct a corpus-
based thesaurus consisting of 11839 nouns, 3639
verbs and 5658 adjectives/adverbs. Given a
word w, the thesaurus returns a set of similar
words of w along with their similarity to w. F r
example, the 20 most similar words of eat and
salad are shown in Table 1.
3 Training Data Extraction
We parsed a 125-million word newspaper
corpus with Minipar3, a descendent of Principar
(Lin, 1994). Minipar outputs dependency trees
(Lin, 1999) from the input sentences. For
example, the following sentence is decomposed
into a dependency tree:
Occasionally, the parser generates incorrect
dependency trees. For example, in the above
sentence, the prepositional phrase headed by
with should attach to saw(as opposed to d g).
Two separate sets of training data were then
extracted from this corpus. Below, we briefly
describe how we obtained these data sets.
3.1 Ambiguous Data Set
For each input sentence, Minipar outputs a
single dependency tree. For a sentence
containing one or more prepositions, we use a
program to detect any alternative prepositional
attachment sites. For example, in the above
sentence, the program would detect that wi h
could attach to saw. Using an iterative
algorithm, we initially create a table of co-
occurrence frequencies for 3-tuples of th  f rm
(V, P, N2) and (N1, P, N2). For each k possible
attachment site of a preposition P, we increment
the frequency of the corresponding 3-tuple by
1/k. For example, Table 2 shows the initial co-
occurrence frequency table for the
corresponding 3-tuples of the above sentence.
                                            
3Available at www.cs.ualberta.ca/~lindek/minipar.htm.
In the following iterations of the algorithm, we
update the frequency table as follows. For each k
possible attachment site of a preposition P, we
refine its attachment score using the formulas
described in Section 4: VScore(Vk, Pk, N2k) and
NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k),
where Wk is either Vk or N2k, we update its
frequ ncy as:
( ) ( )( )? =
= k
i iii
kkk
kNkPkW NPWScore
NPWScore
fr
1 2
2
2,,
,,
,,
where Score(Wk, Pk, N2k) = VScore(Wk, Pk, N2k)
if Wk = Vk; otherwise Score(Wk, Pk, N2k) =
NScore(Wk, Pk, N2k).
Suppose that after the initial frequency table is
set NScore(man, in, park) = 1.23, VScore(saw,
with, telescope) = 3.65, and NScore(dog, with,
telescope) = 0.35. Then, the updated co-
occurrence frequencies for (man, in, park) and
(saw, with, telescope) are:
fr(man, in, park) =    23.1
23.1 = 1.0
fr(saw, with, telescope) = 35.065.3
65.3
+ = 0.913
Table 3 shows the updated frequency table
after the first iteration of the algorithm. The
resulting database contained 8,900,000 triples.
3.2 Unambiguous Data Set
As in (Ratnaparkhi, 1998), we constructed a
training data set consisting of only unambiguous
Table 2. Initial co-occurrence frequency table entries for A
man in the park saw a dog with a telescope.
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.5
dog with telescope 0.5
Table 3. Co-occurrence frequency table entries for A man
in the park saw a dog with a telescope after one iteration.
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.913
dog with telescope 0.087
A  man  in  the  park  saw  a  dog  with  a  telescope.
det det det det
pcomppcomp
mod
subj
obj
mod
attachments of the form (V, P, N2) and (N1, P,
N2). We only extract a 3-tuple from a sentence
when our program finds no alternative
attachment site for its preposition. Each
extracted 3-tuple is assigned a frequency count
of 1. For example, in the previous sentence,
(man, in, park) is extracted since it contains only
one attachment site; (dog, with, telescope) is not
extracted since with has an alternative
attachment site. The resulting database
contained 4,400,000 triples.
4 Classification Model
Roth (1998) presented a unified framework for
natural language disambiguation tasks.
Essentially, several language learning algorithms
(e.g. na?ve Bayes estimation, back-off
estimation, transformation-based learning) were
successfully cast as learning linear separators in
their feature space. Roth modelled prepositional
phrase attachment as linear combinations of
features. The features consisted of all 15
possible sub-sequences of the 4-tuple (V, N1, P,
N2) shown in Table 4. The asterix (*) in features
represent wildcards.
Roth used supervised learning to adjust the
weights of the features. In our experiments, we
only considered features that contained P since
the preposition is the most important lexical item
(Collins and Brooks, 1995). Furthermore, we
omitted features that included both V and N1
since their co-occurrence is independent of the
attachment decision. The resulting subset of
features considered in our system is shown in
bold in Table 4 (equivalent to assigning a weight
of 0 or 1 to each feature).
Let |head, rel, mod| represent the frequency,
obtained from the training data, of the head
occurring in the given relationship rel with the
modifier. We then assign a score to each feature
as follows:
1. (*, *, P, *) = log(|*, P, *| / |*, *, *|)
2. (V, *, P, N2) = log(|V, P, N2| / |*, *, *|)
3. (*, N1, P, N2) = log(|N1, P, N2| / |*, *, *|)
4. (V, *, P, *) = log(|V, P, *| / |V, *, *|)
5. (*, N1, P, *) = log(|N1, P, *| / |N1, *, *|)
6. (*, *, P, N2) = log(|*, P, N2| / |*, *, N2|)
1, 2, and 3 are the prior probabilities of P, V P
N2, and N1 P N2 respectively. 4, 5, and 6
represent conditional probabilities P(V, P | V),
P(N1, P | N1), and P(P N2 | N2) respectively.
We estimate the adverbial and adjectival
attachment scores, VScore(V, P, N2) and
NScore(N1, P, N2), as a linear combination of
these features:
VScore(V, P, N2) =(*, *, P, *) + (V, *, P, N2) +
(V, *, P, *) + (*, *, P, N2)
NScore(N1, P, N2) =(*, *, P, *) + (*, N1, P, N2) +
(*, N1, P, *) + (*, *, P, N2)
For example, the attachment scores for (eat,
salad, with, fork) are VScore(eat, with, fork) =
-3.47 and NScore(salad, with, fork) = -4.77. The
model correctly assigns a higher score to the
dv rbial attachment.
5 Contextually Similar Words
The contextually similar words of a word w are
words similar to the intended meaning of w i  its
context. Below, we describe an algorithm for
constructing contextually similar words and we
present a method for approximating the
attachment scores using these words.
5.1 Algorithm
For our purposes, a context of w is simply a
dependency relationship involving w. For
example, a dependency relationship for aw in
the example sentence of Section 3 is
saw:obj:dog.  Figure 2 gives the data flow
diagram for our algorithm for constructing the
contextually similar words of w. We retrieve
from the collocation database the words that
occurred in the same dependency relationship as
w. We refer to this set of words as the cohort of
w for the dependency relationship. Consider the
words eat and salad in the context eat salad.
The cohort of eat consists of verbs that appeared
Table 4. The 15 features for prepositional phrase
attachment.
FEATURES
(V, *, *, *) (V, *, P, *) (*, N1, *, N2)
(V, N1, *, *) (V, *, *, N2) (*, N1, P, N2)
(V, N1, P, *) (V, *, P, N2) (*, *, P, *)
(V, N1, *, N2) (*, N1, *, *) (*, *, *, N2)
(V, N1, P, N2) (*, N1, P, *) (*, *, P, N2)
with object salad in Figure 1 (e.g. add, consume,
cover, ? ) and the cohort of salad consists of
nouns that appeared as object of eat in Figure 1
(e.g. almond, apple, bean,  ?).
Intersecting the set of similar words and the
cohort then forms the set of contextually similar
words of w. For example, Table 5 shows the
contextually similar words of eat and salad in
the context eat salad and the contextually
similar words of fork in the contexts eat with
fork and salad with fork. The words in the first
row are retrieved by intersecting the similar
words of eat in Table 1 with the cohort of eat
while the second row represents the intersection
of the similar words of alad in Table 1 and the
cohort of salad. The third and fourth rows are
determined in a similar manner. In the
nonsensical context salad with fork (in row 4),
no contextually similar words are found.
While previous word sense disambiguation
algorithms rely on a lexicon to provide sense
inventories of words, the contextually similar
words provide a way of distinguishing between
different senses of words without committing to
any particular sense inventory.
5.2 Attachment Approximation
Often, sparse data reduces our confidence in the
attachment scores of Section 4. Using
contextually similar words, we can approximate
these scores. Given the tuple (V, N1, P, N2),
adverbial attachments are approximated as
follows. We first construct a list CSV containing
the contextually similar words of V in context
V:obj:N1 and a list CSN2V containing the
contextually similar words of N2 in context
V:P:N2 (i.e. assuming adverbial attachment). For
each verb v in CSV, we compute VScore(v, P, N2)
and set SV as the average of the largest k of these
scores. Similarly, for each noun  in CSN2V, we
compute VScore(V, P, n) and set SN2V as the
average of the largest k of these scores. Then,
the approximated adverbial attachment score,
Vscore', is:
VScore'(V, P, N2) = max(SV, SN2V)
We approximate the adjectival attachment
score in a similar way. First, we construct a list
CSN1 containing the contextually similar words
of N1 in context V:obj:N1 and a list CSN2N1
containing the contextually similar words of N2
in context N1:P:N2 (i.e. assuming adjectival
attachment). Now, we compute SN1 as the
average of the largest k of NScore(n, P, N2) for
each noun  in CSN1 and SN2N1 as the average of
the largest k of NScore(N1, P, n) for each noun n
in CSN2N1. Then, the approximated adjectival
attachment score, NScore', is:
NScore'(N1, P, N2) = max(SN1, SN2N1)
For example, suppose we wish to approximate
the attachment score for the 4-tuple (eat, salad,
with, fork). First, we retrieve the contextually
similar words of eat and salad in context eat
salad, and the contextually similar words of fork
in contexts eat with fork and salad with fork as
shown in Table 5. Let k = 2. Table 6 shows the
calculation of SV and SN2V while the calculation
of SN1 and SN2N1 is shown in Table 7. Only the
Figure 2. Data flow diagram for identifying the
contextually similar words of a word in a dependency
relationship.
word in dependency
relationship
Similar Words Cohorts
Corpus-Based
Thesaurus
Retrieve
Intersect
Get Similar
Words
Collocation
DB
Contextually
Similar Words
Table 5. Contextually similar words of eat and salad.
WORD CONTEXT CONTEXTUALLY SIMILAR WORDS
EAT eat salad consume, taste, like, serve, pick,
harvest, love, sprinkle, Toss,?
SALAD eat salad soup, sandwich, pasta, dish, cheese,
vegetable, bread, meat, cake, bean, ?
FORK eat with fork spoon, knife, finger
FORK salad with fork ---
top k = 2 scores are shown in these tables. We
have:
VScore' (eat, with, fork) = max(SV, SN2V)
= -2.92
NScore' (salad, with, fork) = max(SN1, SN2N1)
= -4.87
Hence, the approximation correctly prefers the
adverbial attachment to the adjectival
attachment.
6 Attachment Algorithm
Figure 3 describes the prepositional phrase
attachment algorithm. As in previous
approaches, examples with P = of are always
classified as adjectival attachments.
Suppose we wish to approximate the
attachment score for the 4-tuple (eat, salad,
with, fork). From the previous section, Step 1
returns averageV = -2.92 and averageN1 = -4.87.
From Section 4, Step 2 gives aV = -3.47 and
aN1 = -4.77. In our training data, fV = 2.97 and
fN1 = 0, thus Step 3 gives f = 0.914. In Step 4, we
compute:
S(V) = -3.42 and
S(N1) = -4.78
Since S(V) > S(N1), the algorithm correctly
classifies this example as an adverbial
attachment.
Given the 4-tuple (eat, salad, with, croutons),
the algorithm returns S(V) = -4.31 and S(N1) =
-3.88. Hence, the algorithm correctly attaches
the prepositional phrase to the noun salad.
7 Experimental Results
In this section, we describe our test data and the
baseline for our experiments. Finally, we present
our results.
7.1 Test Data
The test data consists of 3097 examples derived
from the manually annotated attachments in the
Penn Treebank Wall Street Journal data
(Ratnaparkhi et al, 1994)4. Each line in the test
data consists of a 4-tuple and a target
classification: V N1 P N2 target.
                                            
4Available at ftp.cis.upenn.edu/pub/adwait/PPattachData.
The data set contains several erroneous tuples
and attachments. For instance, 133 examples
contain the word the as N1 or N2. There are also
improbable attachments such as (sing, birthday,
to, you) with the target attachment birthday.
Table 6. Calculation of SV and SN2V for (eat, salad, with,
fork).
4-TUPLE VSCORE
(mix, salad, with, fork) -2.60
(sprinkle, salad, with, fork) -3.24
SV -2.92
(eat, salad, with, spoon) -3.06
(eat, salad, with, finger) -3.50
SN2V -3.28
Table 7. Calculation of SN1 and SN2N1 for (eat, salad, with,
fork).
4-TUPLE NSCORE
(eat, pasta, with, fork) -4.71
(eat, cake, with, fork) -5.02
SN1 -4.87
--- n/a
--- n/a
SN2N1 n/a
Input: A 4-tuple (V, N1, P, N2)
Step 1: Using the contextually similar words algorithm
and the formulas from Section 5.2 compute:
averageV = VScore'(V, P, N2)
averageN1 = NScore'(N1, P, N2)
Step 2: Compute the adverbial attachment score, av,
and the adjectival attachment score, an1:
aV = VScore(V, P, N2)
aN1 = NScore(N1, P, N2)
Step 3: Retrieve from the training data set the
frequency of the 3-tuples (V, P, N2) and
(N1, P, N2) ? fV and fN1, respectively.
Let f = (fV + fN1 + 0.2) / (fV + fN1 +0.5)
Step 4: Combine the scores of Steps 1-3 to obtain the
final attachment scores:
S(V) = fav + (1 - f)averagev
S(N1) = fan1 + (1 - f)averagen1
Output:The attachment decision: N1 if S(N1) > S(V) or
P = of; V otherwise.
Figure 3. The prepositional phrase attachment algorithm.
7.2 Baseline
Choosing the most common attachment site, N1,
yields an accuracy of 58.96%. However, we
achieve 70.39% accuracy by classifying each
occurrence of P = of as N1, and V otherwise.
Human accuracy, given the full context of a
sentence, is 93.2% and drops to 88.2% when
given only tuples of the form (V, N1, P, N2)
(Ratnaparkhi et al, 1994). Assuming that human
accuracy is the upper bound for automatic
methods, we expect our accuracy to be bounded
above by 88.2% and below by 70.39%.
7.3 Results
We used the 3097-example testing corpus
described in Section 7.1. Table 8 presents the
precision and recall of our algorithm and Table 9
presents a performance comparison between our
system and previous supervised and
unsupervised approaches using the same test
data. We describe the different classifiers below:
clbase: the baseline described in Section 7.2
clR1: uses a maximum entropy model
(Ratnaparkhi et al, 1994)
clBR5: uses transformation-based learning (Brill
and Resnik, 1994)
clCB: uses a backed-off model (Collins and
Brooks, 1995)
clSN: induces a decision tree with a sense-tagged
corpus, using a semantic dictionary
(Stetina and Nagao, 1997)
clHR6: uses lexical preference (Hindle and Rooth,
1993)
clR2: uses a heuristic extraction of unambiguous
attachments (Ratnaparkhi, 1998)
clPL: uses the algorithm described in this paper
Our classifier outperforms all previous
unsupervised techniques and approaches the
performance of supervised algorithm.
We reconstructed the two earlier unsupervised
classifiers clHR and clR2. Table 10 presents the
accuracy of our reconstructed classifiers. The
originally reported accuracy for lR2 is within the
95% confidence interval of our reconstruction.
Our reconstruction of clHR achieved slightly
higher accuracy than the original report.
                                            
5The accuracy is reported in (Collins and Brooks, 1995).
6The accuracy was obtained on a smaller test set but, from
the same source as our test data.
Our classifier used a mixture of the two
training data sets described in Section 3. In
Table 11, we compare the performance of our
system on the following training data sets:
UNAMB: the data set of unambiguous examples
described in Section 3.2
EM0: the data set of Section 3.1 after
frequency table initialization
EM1: EM0 + one iteration of algorithm 3.1
EM2: EM0 + two iterations of algorithm 3.1
EM3: EM0 + three iterations of algorithm 3.1
1/8-EM1:one eighth of the data in EM1
MIX: The concatenation of UNAMB and EM1
Table 11 illustrates a slight but consistent
increase in performance when using contextually
similar words. However, since the confidence
intervals overlap, we cannot claim with certainty
Table 8. Precision and recall for attachment sites V and N1.
CLASS ACTUAL CORRECT INCORRECT PRECISION RECALL
V 1203 994 209 82.63% 78.21%
N1 1894 1617 277 84.31% 88.55%
Table 9. Performance comparison with other approaches.
METHOD LEARNING ACCURACY
CLBASE --- 70.39%
CLR1 supervised 81.6%
CLBR supervised 81.9%
CLCB supervised 84.5%
CLSN supervised 88.1%
CLHR unsupervised 75.8%
CLR2 unsupervised 81.91%
CLPL unsupervised 84.31%
Table 10. Accuracy of our reconstruction of (Hindle &
Rooth, 1993) and (Ratnaparkhi, 1998).
METHOD ORIGINAL
REPORTED
ACCURACY
RECONSTRUCTED
SYSTEM ACCURACY
(95% CONF)
CLHR 75.8% 78.40% ? 1.45%
CLR2 81.91% 82.40% ? 1.34%
that the contextually similar words improve
performance.
In Section 7.1, we mentioned some testing
examples contained N1 = the or N2 = the. For
supervised algorithms, the is represented in the
training set as any other noun. Consequently,
these algorithms collect training data for the nd
performance is not affected. However,
unsupervised methods break down on such
examples. In Table 12, we illustrate the
performance increase of our system when
removing these erroneous examples.
Conclusion and Future Work
The algorithms presented in this paper advance
the state of the art for unsupervised approaches
to prepositional phrase attachment and draws
near the performance of supervised methods.
Currently, we are exploring different functions
for combining contextually similar word
approximations with the attachment scores. A
promising approach considers the mutual
information between the prepositional
relationship of candidate attachments and N2. As
the mutual information decreases, our
confidence in the attachment score decreases
and the contextually similar word approximation
is weighted higher. Also, improving the
construction algorithm for contextually similar
words would possibly improve the accuracy of
the system. One approach first clusters the
similar words. Then, dependency relationships
are used to select the most representative
clusters as the contextually similar words. The
assumption is that more representative similar
words produce better approximations.
Acknowledgements
The authors wish to thank the reviewers for their
helpful comments. This research was partly
supported by Natural Sciences and Engineering
Research Council of Canada grant OGP121338
and scholarship PGSB207797.
References
Altmann, G. and Steedman, M. 1988. Interaction with Context
During Human Sentence Processing. Cognition, 30:191-238.
Brill, E. 1995. Transformation-based Error-driven Learning and
Natural Language Processing: A case study in part of speech
tagging. Computational Linguistics, December.
Brill, E. and Resnik. P. 1994. A Rule-Based Approach to
Prepositional Phrase Attachment Disambiguation. In
Proceedings of COLING-94. Kyoto, Japan.
Collins, M. and Brooks, J. 1995. Prepositional Phrase Attachment
through a Backed-off Model. In Proceedings of the Third
Workshop on Very Large Corpora, pp. 27-38. Cambridge,
Massachusetts.
Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical
Relations. Computational Linguistics, 19(1):103-120.
Lin, D. 1999. Automatic Identification of Non-Compositional
Phrases. In Proceedings of ACL-99, pp. 317-324. College Park,
Maryland.
Lin, D. 1998a. Extracting Collocations from Text Corpora.
Workshop on Computational Terminology. Montreal, Canada.
Lin, D. 1998b. Automatic Retrieval and Clustering of Similar
Words. In Proceedings of COLING-ACL98. Montreal, Canada.
Lin, D. (1994). Principar - an Efficient, Broad-Coverage,
Principle-Based Parser. In Proceedings of COLING-94. Kyoto,
Japan.
Miller, G. 1990. Wordnet: an On-Line Lexical Database.
International Journal of Lexicography, 1990.
Ratnaparkhi, A. 1998. Unsupervised Statistical Models for
Prepositional Phrase Attachment. In Proceedings of COLING-
ACL98. Montreal, Canada.
Ratnaparkhi, A., Reynar, J., and Roukos, S. 1994. A Maximum
Entropy Model for Prepositional Phrase Attachment. In
Proceedings of the ARPA Human Language Technology
Workshop, pp. 250-255. Plainsboro, N.J.
Roth, D. 1998. Learning to Resolve Natural Language
Ambiguities: A Unified Approach. In Proceedings of AAAI-98,
pp. 806-813. Madison, Wisconsin.
Stetina, J. and Nagao, M. 1997. Corpus Based PP Attachment
Ambiguity Resolution with a Semantic Dictionary. In
Proceedings of the Fifth Workshop on Very Large Corpora, pp.
66-80. Beijing and Hong Kong.
Table 11. Performance comparison of different data sets.
DATABASE ACCURACY
WITHOUT
SIMWORDS
(95% CONF)
ACCURACY
WITH
SIMWORDS
(95% CONF)
UNAMBIGUOUS 83.15% ? 1.32% 83.60% ? 1.30%
EM0 82.24% ? 1.35% 82.69% ? 1.33%
EM1 83.76% ? 1.30% 83.92% ? 1.29%
EM2 83.66% ? 1.30% 83.70% ? 1.31%
EM3 83.20% ? 1.32% 83.20% ? 1.32%
1/8-EM1 82.98% ? 1.32% 83.15% ? 1.32%
MIX 84.11% ? 1.29% 84.31% ? 1.28%
Table 12. Performance with removal of the asN1 or N2.
DATA SET ACCURACY
WITHOUT
SIMWORDS
(95% CONF)
ACCURACY
WITH
SIMWORDS
(95% CONF)
WITH THE 84.11% ? 1.29% 84.31% ? 1.32%
WITHOUT THE 84.44% ? 1.31% 84.65% ? 1.30%
Word-for-Word Glossing with Contextually Similar Words 
Patrick Pantel and Dekang Lin 
Department of Computer Science 
University of Manitoba 
Winnipeg, Manitoba R3T 2N2 Canada 
{ppantel, indek} @cs.umanitoba.ca 
Abstract 
Many corpus-based machine translation 
systems require parallel corpora. In this 
paper, we present a word-for-word glossing 
algorithm that requires only a source 
language corpus. To gloss a word, we first 
identify its similar words that occurred in 
the same context in a large corpus. We then 
determine the gloss by maximizing the 
similarity between the set of contextually 
similar words and the different ranslations 
of the word in a bilingual thesaurus. 
1. Introduction 
Word-for-word glossing is the process of 
directly translating each word or term in a 
document without considering the word order. 
Automating this process would benefit many 
NLP applications. For example, in cross- 
language information retrieval, glossing a 
document often provides a sufficient ranslation 
for humans to comprehend the key concepts. 
Furthermore, a glossing algorithm can be used 
for lexical selection in a full-fledged machine 
translation (MT) system. 
Many corpus-based MT systems require 
parallel corpora (Brown et al, 1990; Brown et 
al., 1991; Gale and Church, 1991; Resnik, 
1999). Kikui (1999) used a word sense 
disambiguation algorithm and a non-paralM 
bilingual corpus to resolve translation 
ambiguity. 
In this paper, we present a word-for-word 
glossing algorithm that requires only a source 
language corpus. The intuitive idea behind our 
algorithm is the following. Suppose w is a word 
to be translated. We first identify a set of words 
similar to w that occurred in the same context as 
w in a large corpus. We then use this set (called 
the contextually similar words of w) to select a 
translation for w. For example, the contextually 
similar words of duty in fiduciary duty include 
responsibility, obligation, role, ... This list is 
then used to select a translation for duty. 
In the next section, we describe the resources 
required by our algorithm. In Section 3, we 
present an algorithm for constructing the 
contextually similar words of a word in a 
context. Section 4 presents the word-for-word 
glossing algorithm and Section 5 describes the 
group similarity metric used in our algorithm. In 
Section 6, we present some experimental results 
and finally, in Section 7, we conclude with a 
discussion of future work. 
2. Resources 
The input to our algorithm includes a collocation 
database (Lin, 1998b) and a corpus-based 
thesaurus (Lin, 1998a), which are both available 
on the Interne0. In addition, we require a 
bilingual thesaurus. Below, we briefly describe 
these resources. 
2.1. Collocation database 
Given a word w in a dependency relationship 
(such as subject or object), the collocation 
database can be used to retrieve the words that 
occurred in that relationship with w, in a large 
corpus, along with their frequencies 2. Figure 1 
shows excerpts of the entries in the collocation 
database for the words corporate, duty, and 
fiduciary. The database contains a total of 11 
million unique dependency relationships. 
I Available at www.cs.umanitoba.ca/-lindek/depdb.htm 
and www.cs.umanitoba.ca/-lindek/simdb.htm 
2 We use the term collocation to refer to a pair of 
words that occur in a dependency relationship (rather 
than the linear proximity of a pair of words). 
78 
Table 1. Clustered similar words of  duty as given by (Lin, 
1998a). 
CLUSTER CLUSTERED SIMILAR WORDS OF DUTY 
(WITH SIMILARITY SCORE) 
responsibility 0.16, obligation 0.109, task 0.101, 
function 0.098, role 0.091, post 0.087, position 
0.086, job 0.084, chore 0.08, mission 0.08, 
assignment 0.079, liability 0.077 ....  
tariff0.091, restriction 0.089, tax 0.086, 
regulation 0.085, requirement 0.081, procedure 
0.079, penalty 0.079, quota 0.074, rule 0.07, levy 
0.061 .... 
fee 0.085, salary 0.081, pay 0.064, fine 0.058 
personnel 0.073, staff0.073 
training 0.072, work 0.064, exercise 0.061 
privilege 0.069, right 0.057, license 0.056 
2.2. Corpus-based thesaurus 
Using the collocation database, Lin used an 
unsupervised method to construct a corpus- 
based thesaurus (Lin, 1998a) consisting of  
11839 nouns, 3639 verbs and 5658 
adjectives/adverbs. Given a word w, the 
thesaurus returns a clustered list of similar words 
of w along with their similarity to w. For 
example, the clustered similar words of duty are 
shown in Table 1. 
2.3. Bi l ingual thesaurus 
Using the corpus-based thesaurus and a bilingual 
dictionary, we manually constructed a bilingual 
thesaurus. The entry for a source language word 
w is constructed by manually associating one or 
more clusters of similar words of w to each 
candidate translation of w. We refer to the 
assigned clusters as Words Associated with a 
Translation (WAT). For example, Figure 2 
shows an excerpt of our Engl ish~French 
bilingual thesaurus for the words account  and 
duty. 
Although the WAT assignment is a manual 
process, it is a considerably easier task than 
providing lexicographic definitions. Also, we 
only require entries for source language words 
that have multiple translations. In Section 7, we 
corporate: 
modifier-of: 
duty: 
objeet-of: 
subject-of: 
adj-modifier: 
fiduciary: 
modifier-of: 
client 196, debt 236, development 179, fee 6, 
function 16, headquarter 316, IOU 128, levy 
3, liability 14, manager 203, market 195, 
obligation 1, personnel 7, profit 595, 
responsibility 27, rule 7, staff 113, tax 201, 
training 2, vice president 231 .... 
assume 177, breach 111, carry out 71, do 
114, have 257, impose 114, perform 151 .... 
affect 4, apply 6, include 42, involve 8, keep 
5, officer 22, protect 8, require 13, ... 
active 202, additional 46, administrative 44, 
fiduciary 317, official 66, other 83, ... 
act 2, behavior I, breach 2, claim I, 
company 2, duty 317, irresponsibility 2, 
obligation 32, requirement 1, responsibility 
89, role 2, ... 
Figure 1. Excepts o f  entries in the collocation database for 
the words corporate, duty, and fiduciary. 
account: 
1. compte: 
2. rapport: 
duty: 
1. devoir: 
2. taxe: 
investment, transaction, payment, saving, i 
money, contract, Budget, reserve, security,! 
contribution, debt, property holding 
report, statement, testimony, card, story, 
record, document, data, information, view, 
cheek, figure, article, description, estimate, 
assessment, number, statistic, comment, 
letter, picture, note, ... 
responsibility, obligation, task, function, 
role, post, position, job, chore, mission, 
assignment, liability . . . .  
tariff, restriction, tax, regulation, 
requirement, procedure, penalty, quota, rule, 
levy, ... 
WAT for 
ii~::::= ........................ 
Figure 2. Bilingual thesaurus entries for account and duty. 
discuss a method for automatically assigning the 
WATs. 
3. Contextual ly Similar Words 
The contextually similar words of a word w are 
words similar to the intended meaning of w in its 
context. Figure 3 gives the data flow diagram for 
our algorithm for identifying the contextually 
similar words of w. Data are represented by 
ovals, external resources by double ovals and 
processes by rectangles. 
By parsing a sentence with Minipar 3, we 
extract the dependency relationships involving 
w. For each dependency relationship, we retrieve 
3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm 
79 
Input 
1 I 
Retrieve 
I 
sContextually ~'~ imilar Words,,\] 
Figure 3. Data flow diagram for identifying the 
contextually similar words of a word in context. 
from the collocation database the words that 
occurred in the same dependency relationship as 
w. We refer to this set of words as the cohort of 
w for that dependency relationship. Consider the 
word duty in the contexts corporate duty and 
fiduciary duty. The cohort of duty in corporate 
duty consists of nouns modified by corporate in 
Figure 1 (e.g. client, debt, development . . . .  ) and 
the cohort of duty in fiduciary duty consists of 
nouns modified by fiduciary in Figure 1 (e.g. 
act, behaviour, breach . . . .  ). 
Intersecting the set of similar words and the 
cohort then forms the set of contextually similar 
words of w. For example, Table 2 shows the 
contextually similar words of duty in the 
contexts corporate duty and fiduciary duty. The 
words in the first row are retrieved by 
intersecting the words in Table 1 with the nouns 
modified by corporate in Figure 1. Similarly, 
the second row represents he intersection of the 
words in Table I and the nouns modified by 
fiduciary in Figure 1. 
The first set of contextually similar words in 
Table 2 contains words that are similar to both 
Table 2. The words similar to duty that occurred in the 
contexts corporate duty and fiduciary duty. 
CONTEXT CONTEXTUALLY SIMILAR WORDS OF DUTY 
corporate duty fee, function, levy, liability, obligation, 
personnel, responsibility, rule, staff, tax, 
training 
obligation, requirement, responsibility, role fiducia~ duty 
the responsibility and tax senses of duty, 
reflecting the fact that the meaning of duty is 
indeed ambiguous if corporate duty is its sole 
context. In contrast, the second row in Table 2 
clearly indicates the responsibility sense of duty. 
While previous word sense disambiguation 
algorithms rely on a lexicon to provide sense 
inventories of words, the contextually similar 
words provide a way of distinguishing between 
different senses of words without committing to 
any particular sense inventory. 
4. Overview of the Word-for-Word 
Glossing Algorithm 
Figure 4 illustrates the data flow of the word- 
for-word glossing algorithm and Figure 5 
describes it. 
For example, suppose we wish to translate 
into French the word duty in the context 
corporate fiduciary duty. Step 1 retrieves the 
candidate translations for duty and its WATs 
from Figure 2. In Step 2, we construct two lists 
of contextually similar words, one for the 
dependency context corporate duty and one for 
the dependency context fiduciary duty, shown in 
Table 2. The proposed translation for the context 
is obtained by maximizing the group similarities 
between the lists of contextually similar words 
and the WATs. 
Using the group similarity measure from 
Section 5, Table 3 lists the group similarity 
scores between each list of contextually similar 
words and each WAT as well as the final 
combined score for each candidate translation. 
The combined score for a candidate is the sum 
of the logs of all group similarity scores 
involving its WAT. The correct proposed 
translation for duty in this context is devoir since 
its WAT received the highest score. 
80 
Input 
I Step 1 Step 2 getWATs getCSWLists 
(  w?,?sl 
Step 3 groupSim I 
l 
Matrix of ~%~ 
I Step4 I ?ornbineScores 
Translation )
Figure 4. Data flow diagram for the word-for-word 
glossing algorithm. 
Table 3. Group similarity scores between the contextually 
similar words of duty in corporate duty and fiduciary duty 
with the WATs for candidate translations devoir and taxe. 
CANDIDATE CANDIDATE 
DEVOIR TAXE 
corporate duty 60.3704 16.569 
fiduciary duty 51.2960 4.8325 
Combined Score 8.0381 4.3829 
Figure 6. An example illustrating the difference between 
the interconnectivity and closeness measures. The 
interconnectivity in (a) and (b) remains constant while the 
closeness in (a) is higher than in (b) since there are more 
zero similarity pairs in (b). 
Input: 
Step 1: 
Step 2: 
Step 3: 
Step 4: 
Output: 
A word w to be translated and a set of 
dependency contexts involving w. 
Retrieve the candidate translations ofw and 
the corresponding WATs from the bilingual 
thesaurus. 
Find the contextually similar words of w in 
each dependency context using the algorithm 
from Section 3. 
Compute the group similarity (see details in 
Section 5) between each set of contextually 
similar words and each WAT; the results are 
stored in a matrix t, where t\[i,j\] is the group 
similarity between the ?h list of contextually 
similar words and thef  h WAT. 
Add the logs of the group similarity scores in 
column of t  to obtain a score for each WAT. 
The candidate translation corresponding to
the WAT with the highest score. 
Figure 5. The word-for-word glossing algorithm. 
5. Group Similarity 
The corpus-based thesaurus contains only the 
similarities between individual pairs of words. In 
our algorithm, we require the similarity between 
groups of  words. The group similarity measure 
we use is proposed by Karypis et al (1999). It 
takes as input two groups of  elements, Gl and 
G2, and a similarity matrix, sim, which specifies 
the similarity between individual elements. GI 
and G2 are describable by graphs where the 
vertices are the words and each weighted edge 
between vertices wl and w2 represents the 
similarity, sim(wl, w2), between the words wl 
and Wz. 
Karypis et al consider both the 
interconnectivity and the closeness of the 
groups. The absolute interconnectivity between 
G t and G 2, AI(G t, G2), is defined as the aggregate 
similarity between the two groups: 
x~Gi YEG2 
The absolute closeness between G~ and G2, 
AC(G~, G2), is defined as the average similarity 
between a pair of elements, one from each 
group: 
Ic, lc l 
81 
Table 4. Candidate translations for each testing word along with their frequency of occurrence 
in the test corpus. 
WORD CANDIDATE ENGLISH SENSE FREQUENCY OF 
TRANSLATION OCCURRENCE 
account compte bank account, business 245 
rapport report, statement 55 
duty devoir responsibility, obligation 80 
taxe tax 30 
race course contest 87 
race racial group 23 
suit proems lawsuit 281 
costume garment 17 
check ch6que draft, bank order 105 
contr61e evaluation, verification 25 
record record unsurpassed statistic/performance 98 
enregistremen t recorded ata or documentation 12 
The difference between the absolute 
interconnectivity and the absolute closeness is 
that the latter takes zero similarity pairs into 
account. In Figure 6, the interconnectivity n (a) 
and (b) remains constant. However, the 
closeness in (a) is higher than in (b) since there 
are more zero similarity pairs in (b). 
Karypis et al normalized the absolute 
interconnectivity and closeness by the internal 
interconnectivity and closeness of the individual 
groups. The normalized measures are referred to 
as relative interconnectivity, RI(GI, G2), and 
relative closeness, RC(GI, G2). The internal 
interconnectivity and closeness are obtained by 
first computing a minimal edge bisection of 
each group. An even-sized partition {G', G"} of 
a group G is called a minimal edge bisection of 
G if AI(G', G") is minimal among all such 
partitions. The internal interconnectivity of G, 
II(G), is defined as II(G) = AI(G', G") and the 
internal closeness of G, IC(G), as IC(G) = 
AC(G', G"). 
Minimal edge bisection is performed for all 
WATs and all sets of contextually similar words. 
However, the minimal edge bisection problem is 
NP-complete (Garey and Johnson, 1979). 
Fortunately, state of the art graph partitioning 
algorithms can approximate these bisections in 
polynomial time (Goehring and Saad, 1994; 
Karypis and Kumar, 1999; Kernighan and Lin, 
1970). We used the same approximation 
methods as in (Karypis et al, 1999). 
The similarity between G1 and G2 is then 
defined as follows: 
groupSim(G,, G2)= R/(G,, G2)? RC(G,, G 2 ) 
where 
2AI(G,,G2) 
xI(G,)+ II(G ) 
is the relative interconnectivity and 
RC(G,,G2)= AC(G,,G ) 
IG'I IC(G,)4 IG2I IC(G2) 
IG, I+IG=I IG, I?IG21 
is the relative closeness. 
6. Experimental Results 
The design of our glossing algorithm is 
applicable to any source/destination language 
pair as long as a source language parser is 
available. We considered English-to-French 
translations in our experiments. 
We experimented with six English nouns that 
have multiple French translations: account, duty, 
race, suit, check, and record. Using the 1987 
Wall Street Journal files on the LDC/DCI CD- 
8R 
ROM, we extracted a testing corpus 4consisting 
of the first 100 to 300 sentences containing the 
non-idiomatic usage of the six nouns . Then, we 
manually tagged each sentence with one of the 
candidate translations shown in Table 4. 
Each noun in Table 4 translates more 
frequently to one candidate translation than the 
other. In fact, always choosing the candidate 
procbs as the translation for suit yields 94% 
accuracy. A better measure for evaluating the 
system's classifications considers both the 
algorithm's precision and recall on each 
candidate translation. Table 5 illustrates the 
precision and recall of our glossing algorithm for 
each candidate translation. Albeit precision and 
recall are used to evaluate the quality of the 
classifications, overall accuracy is sufficient for 
comparing different approaches with our system. 
In Section 3, we presented an algorithm for 
identifying the contextually similar words of a 
word in a context using a corpus-based thesaurus 
and a collocation database. Each of the six nouns 
has similar words in the corpus-based thesaurus. 
However, in order to find contextually similar 
words, at least one similar word for each noun 
must occur in the collocation database in a given 
context. Thus, the algorithm for constructing 
contextually similar words is dependent on the 
coverage of the collocation database. We 
estimated this coverage by counting the number 
of times each of the six nouns, in several 
different contexts, has at least one contextually 
similar word. The result is shown in Table 6. 
In Section 5, we described a group similarity 
metric, groupSim, which we use for comparing a
WAT with a set of contextually similar words. 
In Figure 7, we compare the translation accuracy 
of our algorithm using other group similarity 
metrics. Suppose G~ and (/2 are two groups of 
words and w is the word that we wish to 
translate. The metrics used are: 
I. closest& 
sum of similarity of the three closest 
pairs of words from each group. 
4 Available at fip.cs.umanitoba.ca/pub/ppantei/ 
download/wfwgtest.zip 
5 Omitted idiomatic phrases include take into 
account, keep in check, check out, ... 
Table 5. Precision vs. Recall for each candidate translation. 
WORD CANDIDATE PRECISION RECALL 
account compte 0.982 0.902 
rapport 0.680 0.927 
duty devoir 0.951 0.963 
taxe 0.897 0.867 
race course 0.945 0.989 
race 0.947 0.783 
suit proc6s 0.996 0.993 
costume 0.889 0.941 
check ch6que 0.951 0.924 
contr61e 0.714 0.800 
record record 0.968 0.918 
enregistrement 0.529 0.750 
Table 6. The coverage of the collocation database, shown 
by the frequency with which a word in a given context has 
at least one contextually similar word. 
WORD NUMBER OF COVERAGE 
CONTEXTS 
account 1074 95.7% 
duty 343 93.3% 
race 294 92.5% 
suit 332 91.9% 
check 2519 87.5% 
record 1655 92.8% 
2. gs: 
Z sim(x, w )x max sire(x, y)+ Z sire(y, w)x max sire(y, x) 
Z.,'im{x, w)+ Z ~im@, ~) 
3. dC: 
as defined in Section 5. 
4. AI: 
as defined in Section 5. 
5. RC: 
as defined in Section 5. 
6. RI: 
as defined in Section 5. 
83 
Group Similarity Comparison 
01.01 .... ..... 'i ~ ~R o 7 ? i, 06 .5 
0.4 ; ~ I ; 
02 ;~ 
0o 
i}il;;i  iii ijii !ii!! i}ii iii iiiii 
Figure 7. Performance comparison of different group similarity metrics. 
In mostFrequent, we include the results 
obtained if we always choose the translation that 
occurs most frequently in the testing corpus. 
We also compared the accuracy of our 
glossing algorithm with Systran's translation 
system by feeding the testing sentences into 
Systran's web interface 6 and manually 
examining the results. Figure 8 summarizes the 
overall accuracy obtained by each system and 
the baseline on the testing corpus. Systran 
tended to prefer one candidate translation over 
the other and committed the majority of its 
errors on the non-preferred senses. 
Consequently, Systran is very accurate if its 
preferred sense is the frequent sense (as in 
account and duty) but is very inaccurate if its 
preferred sense is the infrequent one (as in race, 
suit, and check). 
7. Conclusion and Future Work 
This paper presents a word-for-word glossing 
algorithm. The gloss of a word is determined by 
maximizing the similarity between the set of 
contextually similar words and the different 
translations of the word in a bilingual thesaurus. 
6 Available at babelfish.altavista.com/cgi-bin/translate 
The algorithm presented in this paper can be 
improved and extended in many ways. At 
present, our glossing algorithm does not take the 
prior probabilities of translations into account. 
For example, in WSJ, the bank account sense of 
account is much more common than the report 
sense. We should thus tend to prefer this sense 
of account. This is achievable by weighting the 
translation scores by the prior probabilities of 
the translations. We are investigating an 
Expectation-Maximization (EM) (Dempster et 
al., 1977) algorithm to learn these prior 
probabilities. Initially, we assume that the 
candidate translations for a word are uniformly 
distributed. After glossing each word in a large 
corpus, we refine the prior probabilities using 
the frequency counts obtained. This process is 
repeated several times until the empirical prior 
probabilities closely approximate the true prior 
probabilities. 
Finally, as discussed in Section 2.3, 
automatically constructing the bilingual 
thesaurus is necessary to gloss whole 
documents. This is attainable by adding a 
corpus-based destination language thesaurus to 
our system. The process of assigning a cluster of 
similar words as a WAT to a candidate 
translation c is as follows. First, we 
84 
1.0 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
\[\] mostFrequent 
? Systran 
\[\]  G oss ng 
Word-for-Word Glossing vs. Systran 
Z 
\]ccour 
0,816; 
0.856; 
0.906; 
[ t~ mostFrequent ? Systran 
\[\] Glossing 
Figure 8. Performance omparison fthe word-for-word glossing algorithm and Systran. 
automatically obtain the candidate translations 
for a word using a bilingual dictionary. With the 
destination language thesaurus, we obtain a list S 
of all words similar to c. With the bilingual 
dictionary, replace each word in S by its source 
language translations. Using the group similarity 
metric from Section 5, assign as the WAT the 
cluster of similar words (obtained from the 
source language thesaurus) most similar to S. 
Acknowledgements 
The authors wish to thank the reviewers for their 
helpful comments. This research was partly 
supported by Natural Sciences and Engineering 
Research Council of Canada grants OGP 121338 
and PGSA207797. 
References 
Peter F. Brown; John Cocke; Stephen A. Della Pietra; 
Vincent J. Della Pietra; Fredrick Jelinek; John D. 
Lafferty; Robert L. Mercer and Paul S. Roossin. 
1990. A Statistical Approach to Machine 
Translation. Computation Linguistics, 16(2). 
Peter F. Brown; Jennifer C. Lai and Robert L. 
Mercer. 1991. Aligning Sentences in Parallel 
Corpora. In Proceedings ofACL91. Berkeley. 
A. P. Dempster; N. M. Laird; & D. B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical 
Society, Series B, 39(I). 
W. A. Gale and K. W. Church. 1991. A Program for 
Aligning Sentences in Bilingual Corpora. In 
Proceedings of ACL91. Berkeley. 
M. R. Garey and D. S. Johnson. 1979. Computers 
and Intractability: A Guide to the Theory of NP- 
Completeness. W H. Freeman. 
T. Goehring and Y. Saad. 1994. Heuristic Algorithms 
for Automatic Graph Partitioning. Technical 
Report. Department of Computer Science, 
University of Minnesota. 
George Karypis and Vipin Kumar. 1999. A Fast and 
High Quality Multilevel Scheme for Partitioning 
Irregular Graphs. SIAM Journal on Scientific 
Computing, 20(1 ). 
George Karypis; Eui-Hong Han and Vipin Kumar. 
1999. Chameleon: A Hierarchical Clustering 
Algorithm Using Dynamic Modeling. IEEE 
Computer: Special Issue on Data Analysis and 
Mining, 32(8).  http:l/www-users.cs.umn.edu/ 
-karypis/publications/Papers/PDF/chameleon.pdf 
B. W. Kernighan and S. Lin. 1970. An Efficient 
Heuristic Procedure for Partitioning Graphs. The 
Bell System Technical Journal. 
Genichiro Kikui. 1999. Resolving Translation 
ambiguity using Non-parallel Bilingual Corpora. 
In Proceedings of ACL99 Workshop on 
Unsupervised Learning in Natural Language 
Processing. 
Dekang Lin. 1998a. Automatic Retrieval and 
Clustering of Similar Wordv. In Proceedings of 
COLING-ACL98. Montreal, Canada. 
Dekang Lin. 1998b. Extracting Collocations from 
Text Corpora. Workshop on Computational 
Terminology. Montreal, Canada. 
Philip Resnik. 1999. Mining the Web for Bilingual 
Text. In Proceedings of ACL99. College Park, 
Maryland. 
85 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 501?509,
Beijing, August 2010
FactRank: Random Walks on a Web of Facts
Alpa Jain
Yahoo! Labs
alpa@yahoo-inc.com
Patrick Pantel
Microsoft Research
ppantel@microsoft.com
Abstract
Fact collections are mostly built using
semi-supervised relation extraction tech-
niques and wisdom of the crowds meth-
ods, rendering them inherently noisy. In
this paper, we propose to validate the re-
sulting facts by leveraging global con-
straints inherent in large fact collections,
observing that correct facts will tend to
match their arguments with other facts
more often than with incorrect ones. We
model this intuition as a graph-ranking
problem over a fact graph and explore
novel random walk algorithms. We
present an empirical study, over a large set
of facts extracted from a 500 million doc-
ument webcrawl, validating the model and
showing that it improves fact quality over
state-of-the-art methods.
1 Introduction
Fact bases, such as those contained in Freebase,
DBpedia, KnowItAll, and TextRunner, are increas-
ingly burgeoning on the Internet, in government,
in high tech companies and in academic laborato-
ries. Bar the accurate manual curation typified by
Cyc (Lenat, 1995), most fact bases are built us-
ing either semi-supervised techniques or wisdom
of the crowds techniques, rendering them inher-
ently noisy. This paper describes algorithms to
validate and re-rank fact bases leveraging global
constraints imposed by the semantic arguments
predicated by the relations.
Facts are defined as instances of n-ary typed re-
lations such as acted-in?movie, actor?, director-
of?movie, director?, born-in?person, date?, and
buy?person, product, person?. In all but very
small fact bases, relations share an argument
type, such as movie for the relations acted-in and
director-of in the above example. The hypothesis
explored in this paper is that when two fact in-
stances from two relations share the same value
for a shared argument type, then the validity of
both facts should be increased. Conversely, we
also hypothesize that an incorrect fact instance
will tend to match a shared argument with other
facts far less frequently. For example, consider
the following four facts from the relations acted-
in, director-of, and is-actor:
t1: acted-in?Psycho, Anthony Perkins?
t2: *acted-in?Walt Disney Pictures, Johnny Depp?
t3: director-of?Psycho, Alfred Hitchcock?
t4: is-actor?Anthony Perkins?
Our confidence in the validity of t1 increases
with the knowledge of t3 and t4 since the argu-
ment movie is shared with t3 and actor with t4.
Similarly, t1 increases our confidence in the va-
lidity of t3 and t4. For t2, we expect to find few
facts that will match a movie argument with Walt
Disney Pictures. Facts that share the actor argu-
ment Johnny Depp with t2 will increase its valid-
ity, but the lack of matches on its movie argument
will decrease its validity.
In this paper, we present FactRank, which for-
malizes the above intuitions by constructing a fact
graph and running various random walk graph-
ranking algorithms over it to re-rank and validate
the facts. A collection of facts is modeled in the
form of a graph where nodes are fact instances and
edges connect nodes that have the same value for a
shared argument type (e.g., t1 would be linked by
an edge to both t3 and t4.) Given a graph represen-
tation of facts, we explore various random walk
algorithms to propagate our confidence in individ-
ual facts through the web of facts. We explore
algorithms such as PageRank (Page et al, 1999)
as well as propose novel algorithms that lever-
age several unique characteristics of fact graphs.
Finally, we present an empirical analysis, over a
large collection of facts extracted from a 500 mil-
501
lion document webcrawl, supporting our model
and confirming that global constraints in a fact
base can be leveraged to improve the quality of
the facts. Our proposed algorithms are agnostic to
the sources of a fact base, however our reported
experiments were carried over a state-of-the-art
semi-supervised extraction system. In summary,
the main contributions of this paper are:
? We formalize the notion of ranking facts in
a holistic manner by applying graph-based
ranking algorithms (Section 2).
? We propose novel ranking algorithms using
random walk models on facts (Section 3).
? We establish the effectiveness of our ap-
proach through an extensive experimental
evaluation over a real-life dataset and show
improvements over state-of-the-art ranking
methods (Section 4).
2 Fact Validation Revisited
We denote an n-ary relation r with typed argu-
ments t1, t2, ? ? ? , tn as r?t1, t2, ? ? ? tn?. In this pa-
per, we limit our focus to unary and binary re-
lations. A fact is an instance of a relation. For
example, acted-in?Psycho, Anthony Perkins? is a
fact from the acted-in?movie, actor? relation.
Definition 2.1 [Fact base]: A fact base is a col-
lection of facts from several relations. Textrunner
and Freebase are example fact bases (note that
they also contain knowledge beyond facts such as
entity lists and ontologies.) 2
Definition 2.2 [Fact farm]: A fact farm is a sub-
set of interconnected relations in a fact base that
share arguments among them. 2
For example, consider a fact base consisting of
facts for relations involving movies, organiza-
tions, products, etc., of which the relations acted-
in and director-of could form a MOVIES fact farm.
Real-world fact bases are built in many ways.
Semi-supervised relation extraction methods in-
clude KnowItAll (Etzioni et al, 2005), TextRun-
ner (Banko and Etzioni, 2008), and many others
such as (Riloff and Jones, 1999; Pantel and Pen-
nacchiotti, 2006; Pas?ca et al, 2006; Mintz et al,
2009). Wisdom of the crowds methods include
DBpedia (Auer et al, 2008) and Freebase which
extracts facts from various open knowledge bases
and allow users to add or edit its content.
Most semi-supervised relation extraction meth-
ods follow (Hearst, 1992). Starting with a rela-
tively small set of seed facts, these extractors it-
eratively learn patterns that can be instantiated to
identify new facts. To reflect their confidence in
an extracted fact, extractors assign an extraction
score with each fact. Methods differ widely in
how they define the extraction score. Similarly,
many extractors assign a pattern score to each
discovered pattern. In each iteration, the high-
est scoring patterns and facts are saved, which are
used to seed the next iteration. After a fixed num-
ber of iterations or when a termination condition
is met, the instantiated facts are ranked by their
extraction score.
Several methods have been proposed to gen-
erate such ranked lists (e.g., (Riloff and Jones,
1999; Banko and Etzioni, 2008; Matuszek et al,
2005; Pantel and Pennacchiotti, 2006; Pas?ca et al,
2006). In this paper, we re-implement the large-
scale state-of-the-art method proposed by Pas?ca et
al. (2006). This pattern learning method generates
binary facts and computes the extraction scores of
a fact based on (a) the scores of the patterns that
generated it, and (b) the distributional similarity
score between the fact and the seed facts. We
computed the distributional similarity between ar-
guments using (Pantel et al, 2009) over a large
crawl of the Web (described in Section 4.1). Other
implementation details follow (Pas?ca et al, 2006).
In our experiments, we observed some interest-
ing ranking problems as illustrated by the follow-
ing example facts for the acted-in relation:
id: Facts (#Rank)
t1: acted-in?Psycho, Anthony Perkins? (#26)
t2: *acted-in?Walt Disney Pictures, Johnny Depp? (#9)
Both t1 and t2 share similar contexts in documents
(e.g., ?movie? film starring ?actor? and ?movie?
starring ?actor?), and this, in turn, boosts the
pattern-based component of the extraction scores
for t1. Furthermore, due to the ambiguity of the
term psycho, the distributional similarity-based
component of the scores for fact t2 is also lower
than that for t1.
502
Relations id : Facts
acted-in t1 : ?Psycho, Anthony Perkins?
t2 : *?Walt Disney Pictures, Johnny Depp?
director-of t3 : ?Psycho, Alfred Hitchcock?
producer-of t4 : ?Psycho, Hilton Green?
is-actor t5 : ?Anthony Perkins?
t6 : ?Johnny Depp?
is-director t7 : ?Alfred Hitchcock?
is-movie t8 : ?Psycho?
Table 1: Facts share arguments across relations
which can be exploited for validation.
Our work in this paper is motivated by the
following observation: the ranked list generated
by an individual extractor does not leverage any
global information that may be available when
considering a fact farm in concert. To under-
stand the information available in a fact farm,
consider a MOVIES fact farm consisting of rela-
tions, such as, acted-in, director-of, producer-of,
is-actor, is-movie, and is-director. Table 1 lists
sample facts that were generated in our experi-
ments for these relations1. In this example, we
observe that for t1 there exist facts in foreign re-
lations, namely, director-of and producer-of that
share the same value for the Movie argument, and
intuitively, facts t3 and t4 add to the validity of t1.
Furthermore, t1 shares the same value for the Ac-
tor argument with t5. Also, t3, which is expected
to boost the validity of t1, itself shares values for
its arguments with facts t4 and t7, which again in-
tuitively adds to the validity of t1. In contrast to
this web of facts generated for t1, the fact t2 shares
only one of its argument value with one other fact,
i.e., t6.
The above example underscores an important
observation: How does the web of facts gener-
ated by a fact farm impact the overall validity of
a fact? To address this question, we hypothesize
that facts that share arguments with many facts are
more reliable than those that share arguments with
few facts. To capture this hypothesis, we model a
web of facts for a farm using a graph-based repre-
sentation. Then, using graph analysis algorithms,
we propagate reliability to a fact using the scores
of other facts that recursively connect to it.
Starting with a fact farm, to validate the facts in
each consisting relation, we:
1The is-actor?actor?, is-director?director?, and is-movie?movie? rela-
tions are equivalent to the relation is-a?c-instance, class? where class ?
{actor, director,movie}.
(1) Identify arguments common to relations in the farm.
(2) Run extraction methods to generate each relation.
(3) Construct a graph-based representation of the extracted
facts using common arguments identified in Step (1)
(see Section 3.1 for details on constructing this graph.)
(4) Perform link analysis using random walk algorithms
over the generated graph, propagating scores to each
fact through the interconnections (see Section 3.2 for
details on various proposed random walk algorithms).
(5) Rank facts in each relation using the scores generated
in Step (4) or by combining them with the original ex-
traction scores.
For the rest of the paper, we focus on generating
better ranked lists than the original rankings pro-
posed by a state-of-the-art extractor.
3 FactRank: Random Walk on Facts
Our approach considers a fact farm holistically,
leveraging the global constraints imposed by the
semantic arguments of the facts in the farm. We
model this idea by constructing a graph represen-
tation of the facts in the farm (Section 3.1) over
which we run graph-based ranking algorithms.
We give a brief overview of one such ranking al-
gorithm (Section 3.2) and present variations of it
for fact re-ranking (Section 3.3). Finally, we in-
corporate the original ranking from the extractor
into the ranking produced by our random walk
models (Section 3.4).
3.1 Graph Representation of Facts
Definition 3.1 We define a fact graph FG(V, E),
with V nodes and E edges, for a fact farm, as a
graph containing facts as nodes and a set of edges
between these nodes. An edge between nodes vi
and vj indicates that the facts share the same
value for an argument that is common to the re-
lations that vi and vj belong to. 2
Figure 1 shows the fact graph for the example
in Table 1 centered around the fact t1.
Note on the representation: The above graph
representation is just one of many possible op-
tions. For instance, instead of representing facts
by nodes, nodes could represent the arguments of
facts (e.g., Psycho) and nodes could be connected
by edges if they occur together in a fact. The task
of studying a ?best? representation remains a fu-
ture work direction. However, we believe that our
proposed methods can be easily adapted to other
such graph representations.
503
<Ps
ych
o, A
ntho
ny P
erki
ns>
<Ps
ych
o, m
ovie
>
<Ps
ych
o, H
ilto
n G
reen
>
<Al
fred
 Hit
chc
hoc
k, d
irec
tor>
<An
thon
y P
erki
ns, 
acto
r>
<Ps
ych
o, A
lfre
d H
itch
coc
k>
Figure 1: Fact graph centered around t1 in Table 1.
3.2 The FactRank Hypothesis
We hypothesize that connected facts increase our
confidence in those facts. We model this idea
by propagating extraction scores through the fact
graph similarly to how authority is propagated
through a hyperlink graph of the Web (used to es-
timate the importance of a webpage). Several link
structure analysis algorithms have been proposed
for this goal, of which we explore a particular ex-
ample, namely, PageRank (Page et al, 1999). The
premise behind PageRank is that given the hyper-
link structure of the Web, when a page v generates
a link to page u, it confers some of its importance
to u. Therefore, the importance of a webpage u
depends on the number of pages that link to u and
furthermore, on the importance of the pages that
link to u. More formally, given a directed graph
G = (V,E) with V vertices and E edges, let I(u)
be the set of nodes that link to a node u and O(v)
be the set of nodes linked by v. Then, the impor-
tance of a node u is defined as:
p(u) =
X
v?I(u)
p(v)
|O(v)| (1)
The PageRank algorithm iteratively updates the
scores for each node in G and terminates when a
convergence threshold is met. To guarantee the al-
gorithm?s convergence, G must be irreducible and
aperiodic (i.e., a connected graph). The first con-
straint can be easily met by converting the adja-
cency matrix for G into a stochastic matrix (i.e.,
all rows sum up to 1.) To address the issue of peri-
odicity, Page et al (1999) suggested the following
modification to Equation 1:
p(u) = 1 ? d|V | + d ?
X
v?I(u)
p(v)
|O(v)| (2)
where d is a damping factor between 0 and 1,
which is commonly set to 0.85. Intuitively, Page-
Rank can be viewed as modeling a ?random
walker? on the nodes inG and the score of a node,
i.e., PageRank, determines the probability of the
walker arriving at this node.
While our method makes use of the PageRank
algorithm, we can also use other graph analysis
algorithms (e.g., HITS (Kleinberg, 1999)). A par-
ticularly important property of the PageRank al-
gorithm is that the stationary scores can be com-
puted for undirected graphs in the same manner
described above, after replacing each undirected
edge by a bi-directed edge. Recall that the edges
in a fact graph are bi-directional (see Figure 1).
3.3 Random Walk Models
Below, we explore various random walk models
to assign scores to each node in a fact graph FG.
3.3.1 Model Implementations
Pln: Our first method applies the traditional Page-
Rank model to FG and computes the score of a
node u using Equation 2.
Traditional PageRank, as is, does not make use
of the strength of the links or the nodes connected
by an edge. Based on this observation, researchers
have proposed several variations of the PageRank
algorithm in order to solve their problems. For
instance, variations of random walk algorithms
have been applied to the task of extracting impor-
tant words from a document (Hassan et al, 2007),
for summarizing documents (Erkan and Radev,
2004), and for ordering user preferences (Liu and
Yang, 2008). Following the same idea, we build
upon the discussion in Section 3.2 and present
random walk models that incorporate the strength
of an edge.
Dst: One improvement over Pln is to distinguish
between nodes in FG using the extraction scores
of the facts associated with them: extraction meth-
ods such as our reimplementation of (Pas?ca et al,
2006) assign scores to each output fact to reflect
its confidence in it (see Section 3.2). Intuitively, a
higher scoring node that connects to u should in-
crease the importance of umore than a connection
from a lower scoring node. Let I(u) be the set of
nodes that link to u and O(v) be the set of nodes
504
linked by v. Then, if w(u) is the extraction score
for the fact represented by node u, the score for
node u is defined:
p(u) = 1 ? d|V | + d ?
X
v?I(u)
w(v) ? p(v)
|O(v)| (3)
where w(v) is the confidence score for the fact
represented by v. Naturally, other (externally de-
rived) extraction scores can also be substituted for
w(v).
Avg: We can further extend the idea of deter-
mining the strength of an edge by combining the
extraction scores of both nodes connected by an
edge. Specifically,
p(u) = 1 ? d|V | + d ?
X
v?I(u)
avg(u, v) ? p(v)
|O(v)| (4)
where avg(u, v) is the average of the extraction
scores assigned to the facts associated with nodes
u and v.
Nde: In addition to using extraction scores, we
can also derive the strength of a node depending
on the number of distinct relations it connects to.
For instance, in Figure 1, t1 is linked to four dis-
tinct relations, namely, director-of, producer-of,
is-actor, is-movie, whereas, t2 is linked to one re-
lation, namely, is-actor. We compute p(u) as:
p(u)=1 ? d|V | +d ?
X
v?I(u)
(? ? w(v)+(1 ? ?) ? r(v)) ? p(v)
|O(v)| (5)
where w(v) is the confidence score for node v and
r(v) is the fraction of total number of relations in
the farm that contain facts with edges to v.
3.3.2 Dangling nodes
In traditional hyperlink graphs for the Web,
dangling nodes (i.e., nodes with no associated
edges) are considered to be of low importance
which is appropriately represented by the scores
computed by the PageRank algorithm. How-
ever, an important distinction from this setting is
that fact graphs are sparse causing them to have
valid facts with no counterpart matching argu-
ments in other relation, thus rendering them dan-
gling. This may be due to several reasons, e.g.,
extractors often suffer from less than perfect recall
and they may miss valid facts. In our experiments,
about 10% and 40% of nodes from acted-in and
director-of, respectively, were dangling nodes.
Handling dangling nodes in our extraction-
based scenario is a particularly challenging issue:
while demoting the validity of dangling nodes
could critically hurt the quality of the facts, lack
of global information prevents us from systemati-
cally introducing them into the re-ranked lists. We
address this issue by maintaining the original rank
positions when re-ranking dangling nodes.
3.4 Incorporating Extractor Ranks
Our proposed random walk ranking methods ig-
nore the ranking information made available by
the original relation extractor (e.g., (Pas?ca et al,
2006) in our implementation). Below, we pro-
pose two ways of combining the ranks suggested
by the original ranked list O and the re-ranked list
G, generated using the algorithms in Section 3.3.
R-Avg: The first combination method computes
the average of the ranks obtained from the two
lists. Formally, if O(i) is the original rank for fact
i and G(i) is the rank for i in the re-ranked list,
the combined rank M(i) is computed as:
M(i) = O(i) +G(i)2 (6)
R-Wgt: The second method uses a weighted aver-
age of the ranks from the individual lists:
M(i) = wo ? O(i) + (1 ? wo) ? G(i)2 (7)
In practice, this linear combination can be learned;
in our experiments, we set them towo = 0.4 based
on our observations over an independent training
set. Several other combination functions could
also be applied to this task. For instance, we ex-
plored the min and max functions but observed lit-
tle improvements.
4 Experimental Evaluation
4.1 Experimental Setup
Extraction method: For our extraction method,
we reimplemented the method described in (Pas?ca
et al, 2006) and further added a validation layer
on top of it based on Wikipedia (we boosted the
scores of a fact if there exists a Wikipedia page
for either of the fact?s arguments, which mentions
the other argument.) This state-of-the-art method
forms a strong baseline in our experiments.
Corpus and farms: We ran our extractor over a
large Web crawl consisting of 500 million English
505
25000 2000025000 150002000025000
f nodes
10000150002000025000
mber o
0500010000150002000025000
Nu
0500010000150002000025000
1
2
4
8
16
32
64
128
256
Node 
degree
0500010000150002000025000
1
2
4
8
16
32
64
128
256
Node 
degree
Figure 2: Degree distribution for MOVIES.
webpages crawled by the Yahoo! search engine.
We removed paragraphs containing fewer than 50
tokens and then removed all duplicate sentences.
The resulting corpus consists of over 5 million
sentences. We defined a farm, MOVIES, with rela-
tions, acted-in, director-of, is-movie, is-actor, and
is-director.
Evaluation methodology: Using our extraction
method over the Web corpus, we generate over
100,000 facts for the above relations. However, to
keep our evaluation manageable, we draw a ran-
dom sample from these facts. Specifically, we
first generate a ranked list using the extraction
scores output by our extractor. We will refer to
this method as Org (original). We then generate
a fact graph over which we will run our methods
from Section 3.3 (each of which will re-rank the
facts). Figure 2 shows the degree, i.e., number
of edges, distribution of the fact graph generated
for MOVIES. We ran Avg, Dst, Nde, R-Avg, and
R-Wgt on this fact graph and using the scores we
re-rank the facts for each of the relations. In Sec-
tion 4.2, we will discuss our results for the acted-
in and director-of relations.
Fact Verification: To verify whether a fact is
valid or not, we recruit human annotators using
the paid service Mechanical Turk. For each fact,
two annotations were requested (keeping the total
cost under $100). The annotators were instructed
to mark incorrect facts as well as disallow any val-
ues that were not ?well-behaved.? For instance,
acted-in?Godfather, Pacino? is correct, but acted-
in?The, Al Pacino? is incorrect. We manually ad-
judicated 32% of the facts where the judges dis-
agreed.
Evaluation metrics: Using the annotated facts,
we construct a goldset S of facts and compute the
precision of a list L as: |L?S||S| . To compare theeffectiveness of the ranked lists, we use average
precision, a standard measure in information re-
trieval for evaluating ranking algorithms, defined
Method Average precision
30% 50% 100%
Org 0.51 0.39 0.38
Pln 0.44 0.35 0.32
Avg 0.55 0.44 0.42
Dst 0.54 0.44 0.41
Nde 0.53 0.40 0.41
R-Avg 0.58 0.46 0.45
R-Wgt 0.60 0.56 0.44
Table 2: Average precision for acted-in for vary-
ing proportion of fact graph of MOVIES.
Method Average precision
30% 50% 100%
Org 0.64 0.69 0.66
Pln 0.69 0.67 0.59
Avg 0.69 0.70 0.64
Dst 0.67 0.69 0.64
Nde 0.69 0.69 0.64
R-Avg 0.70 0.70 0.64
R-Wgt 0.71 0.71 0.69
Table 3: Average precision for director-of for
varying proportion of fact graph of MOVIES.
as: Ap(L) =
P|L|
i=1 P (i)?isrel(i)
P|L|
i=1 isrel(i)
, where P (i) is the
precision of L at rank i, and isrel(i) is 1 if the fact
at rank i is in S, and 0 otherwise. We also study
the precision values at varying ranks in the list.
For robustness, we report the results using 10-fold
cross validation.
4.2 Experimental Results
Effectiveness of graph-based ranking: Our
first experiment studies the overall quality of the
ranked lists generated by each method. Table 2
compares the average precision for acted-in, with
the maximum scores highlighted for each column.
We list results for varying proportions of the orig-
inal fact graph (30%, 50%, and 100%). Due to
our small goldset sizes, these results are not sta-
tistically significant over Org, however we con-
sistently observed a positive trend similar to those
reported in Table 2 over a variety of evaluation
sets generated by randomly building 10-folds of
all the facts.
Overall, the Avg method offers a competitive
alternative to the original ranked list generated
by the extractor Org: not only are the average
precision values for Avg higher than Org, but
as we will see later, the rankings generated by
our graph-based methods exhibits some positive
unique characteristics. These experiments also
506
R Org Pln Avg Dst Nde R-Avg R-Wgt
5 0.44 0.40 0.52 0.48 0.40 0.52 0.56
10 0.36 0.36 0.42 0.38 0.36 0.36 0.36
15 0.287 0.24 0.30 0.28 0.26 0.30 0.30
20 0.26 0.26 0.26 0.26 0.26 0.27 0.27
21 0.27 0.27 0.27 0.27 0.27 0.27 0.27
Table 4: Precision at varying ranks for the acted-
in relation (R stands for Ranks).
R Org Pln Avg Dst Nde R-Avg R-Wgt
5 0.58 0.68 0.70 0.68 0.64 0.66 0.70
10 0.60 0.57 0.59 0.58 0.59 0.6 0.69
15 0.57 0.53 0.58 0.56 0.56 0.56 0.60
20 0.57 0.57 0.58 0.58 0.58 0.58 0.60
25 0.60 0.54 0.56 0.57 0.56 0.57 0.57
30 0.57 0.57 0.57 0.57 0.57 0.58 0.59
33 0.56 0.56 0.56 0.56 0.56 0.56 0.56
Table 5: Precision at varying ranks for the
director-of relation (R stands for Ranks).
confirm our initial observations: using traditional
PageRank (Pln) is not desirable for the task of re-
ranking facts (see Section 3.3). Our modifications
to the PageRank algorithm (e.g., Avg, Dst, Nde)
consistently outperform the traditional PageRank
algorithm (Pln). The results also underscore the
benefit of combining the original extractor ranks
with those generated by our graph-based rank-
ing algorithms with R-Wgt consistently leading to
highest or close to the highest average precision
scores.
In Table 3, we show the average precision val-
ues for director-of. In this case, the summary
statistic, average precision, does not show many
differences between the methods. To take a finer
look into the quality of these rankings, we investi-
gated the precision scores at varying ranks across
the methods. Table 4 and Table 5 show the preci-
sion at varying ranks for acted-in and director-of
respectively. The maximum precision values for
each rank are highlighted.
For acted-in again we see that Avg, R-Avg, R-
Wgt outperform Org and Pln at all ranks, and
Dst outperforms Org at two ranks. While the
method Nde outperforms Org for a few cases, we
expected it to perform better. Error analysis re-
vealed that the sparsity of our fact graph was the
problem. In our MOVIES fact graph, we observed
very few nodes that are linked to all possible re-
lation types, and the scores used by Nde rely on
being able to identify nodes that link to numer-
ous relation types. This problem can be alleviated
#Relation Avg Dst Nde
2 0.35 0.34 0.33
3 0.35 0.35 0.34
4 0.37 0.36 0.35
5 0.38 0.38 0.37
6 0.42 0.41 0.41
Table 6: Average precision for acted-in for vary-
ing number of relations in the MOVIES fact farm.
by reducing the sparsity of the fact graphs (e.g.,
by allowing edges between nodes that are ?simi-
lar enough?), which we plan to explore as future
work. For director-of, Table 5 now shows that for
small ranks (less than 15), a small (but consistent
in our 10-folds) improvement is observed when
comparing our random walk algorithms over Org.
While our proposed algorithms show a con-
sistent improvement for acted-in, the case of
director-of needs further discussion. For both av-
erage precision and precision vs. rank values, Avg,
R-Avg, and R-Wgt are similar or slightly better
than Org. We observed that the graph-based algo-
rithms tend to bring together ?clusters? of noisy
facts that may be spread out in the original ranked
list of facts. To illustrate this point, we show the
ten lowest scoring facts for the director-of rela-
tion. Table 7 shows these ten facts for Org as well
as Avg. These examples highlight the ability of
our graph-based algorithms to demote noisy facts.
Effect of number of relations: To understand
the effect of the number of relations in a farm
(and hence connectivity in a fact graph), we veri-
fied the re-ranking quality of our proposed meth-
ods on various subsets of the MOVIES fact farm.
We generated five different subsets, one with 2 re-
lations, another with 3 relations, and three more
with four, five, and six relations (note that al-
though we have 5 relations in the farm, is-movie
can be used in combination with both acted-in
and director-of, thus yielding six relations to ab-
late.) Table 6 shows the results for acted-in. Over-
all, performance improves as we introduce more
relations (i.e., more connectivity). Once again,
we observe that the performance deteriorates for
sparse graphs: using very few relations results in
degenerating the average precision of the original
ranked list. The issue of identifying the ?right?
characteristics of the fact graph (e.g., number of
relations, degree distribution, etc.) remains future
work.
507
Org Avg
?david mamet, bob rafelson? ? drama, nicholas ray?
?cinderella, wayne sleep? ? drama, mitch teplitsky official?
?mozartdie zauberflte, julie taymor? ? hollywood, marta bautis?
?matthew gross, julie taymor? ? hollywood, marek stacharski?
?steel magnolias, theater project? ? drama, kirk shannon-butts?
?rosie o?donnell, john badham? ? drama, john pietrowski?
?my brotherkeeper, john badham? ? drama, john madden starring?
?goldie hawn, john badham? ? drama, jan svankmajer?
?miramaxbad santa, terry zwigoff? ? drama, frankie sooknanan?
?premonition, alan rudolph? ? drama, dalia hager?
Table 7: Sample facts for director-of at the bot-
tom of the ranked list generated by (a) Org and
(b) Avg.
Evaluation conclusion: We demonstrated the ef-
fectiveness of our graph-based algorithms for re-
ranking facts. In general, Avg outperforms Org
and Pln, and we can further improve the perfor-
mance by using a combination-based ranking al-
gorithm such as R-Wgt. We also studied the im-
pact of the size of the fact graphs on the quality
of the ranked lists and showed that increasing the
density of the fact farms improves the ranking us-
ing our methods.
5 Related Work
Information extraction from text has received sig-
nificant attention in the recent years (Cohen and
McCallum, 2003). Earlier approaches relied
on hand-crafted extraction rules such as (Hearst,
1992), but recent efforts have developed su-
pervised and semi-supervised extraction tech-
niques (Riloff and Jones, 1999; Agichtein and
Gravano, 2000; Matuszek et al, 2005; Pan-
tel and Pennacchiotti, 2006; Pas?ca et al, 2006;
Yan et al, 2009) as well as unsupervised tech-
niques (Davidov and Rappoport, 2008; Mintz
et al, 2009). Most common methods today
use semi-supervised pattern-based learning ap-
proaches that follow (Hearst, 1992), as dis-
cussed in Section 2. Recent work has also ex-
plored extraction-related issues such as, scal-
ability (Pas?ca et al, 2006; Ravichandran and
Hovy, 2002; Pantel et al, 2004; Etzioni et al,
2004), learning extraction schemas (Cafarella et
al., 2007a; Banko et al, 2007), and organizing ex-
tracted facts (Cafarella et al, 2007b). There is
also a lot of work on deriving extraction scores
for facts (Agichtein and Gravano, 2000; Downey
et al, 2005; Etzioni et al, 2004; Pantel and Pen-
nacchiotti, 2006).
These extraction methods are complementary
to our general task of fact re-ranking. Since our
proposd re-ranking algorithms are agnostic to the
methods of generating the initial facts and since
they do not rely on having available corpus statis-
tics, we can use any of the available extractors in
combination with any of the scoring methods. In
this paper, we used Pas?ca et al?s (2006) state-of-
the-art extractor to learn a large set of ranked facts.
Graph-based ranking algorithms have been ex-
plored for a variety of text-centric tasks. Random
walk models have been built for document sum-
marization (Erkan and Radev, 2004), keyword ex-
traction (Hassan et al, 2007), and collaborative
filtering (Liu and Yang, 2008). Closest to our
work is that of Talukdar et al (2008) who pro-
posed random walk algorithms for learning in-
stances of semantic classes from unstructured and
structured text. The focus of our work is on ran-
dom walk models over fact graphs in order to re-
rank collections of facts.
6 Conclusion
In this paper, we show how information avail-
able in a farm of facts can be exploited for re-
ranking facts. As a key contribution of the pa-
per, we modeled fact ranking as a graph ranking
problem. We proposed random walk models that
determine the validity of a fact based on (a) the
number of facts that ?vote? for it, (b) the validity
of the voting facts, and (c) the extractor?s confi-
dence in these voting facts. Our experimental re-
sults demonstrated the effectiveness of our algo-
rithms, thus establishing a stepping stone towards
exploring graph-based frameworks for fact vali-
dation. While this paper forms the basis of em-
ploying random walk models for fact re-ranking,
it also suggests several interesting directions for
future work. We use and build upon PageRank,
however, several alternative algorithms from the
link analysis literature could be adapted for rank-
ing facts. Similarly, we employ a single (simple)
graph-based representation that treats all edges the
same and exploring richer graphs that distinguish
between edges supporting different arguments of
a fact remains future work.
508
References
[Agichtein and Gravano2000] Agichtein, Eugene and Luis
Gravano. 2000. Snowball: Extracting relations from
large plain-text collections. In DL-00.
[Auer et al2008] Auer, S., C. Bizer, G. Kobilarov,
J. Lehmann, R. Cyganiak, and Z. Ives. 2008. Dbpedia: A
nucleus for a web of open data. In ISWC+ASWC 2007.
[Banko and Etzioni2008] Banko, Michele and Oren Etzioni.
2008. The tradeoffs between open and traditional relation
extraction. In ACL-08.
[Banko et al2007] Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matthew Broadhead, and Oren Et-
zioni. 2007. Open information extraction from the web.
In Proceedings of IJCAI-07.
[Cafarella et al2007a] Cafarella, Michael, Dan Suciu, and
Oren Etzioni. 2007a. Navigating extracted data with
schema discovery. In Proceedings of WWW-07.
[Cafarella et al2007b] Cafarella, Michael J., Christopher Re,
Dan Suciu, Oren Etzioni, and Michele Banko. 2007b.
Structured querying of web text: A technical challenge.
In Proceedings of CIDR-07.
[Cohen and McCallum2003] Cohen, William and Andrew
McCallum. 2003. Information extraction from the World
Wide Web (tutorial). In KDD.
[Davidov and Rappoport2008] Davidov, Dmitry and Ari
Rappoport. 2008. Unsupervised discovery of generic re-
lationships using pattern clusters and its evaluation by au-
tomatically generated sat analogy questions. In ACL-08.
[Downey et al2005] Downey, Doug, Oren Etzioni, and
Stephen Soderland. 2005. A probabilistic model of re-
dundancy in information extraction. In Proceedings of
IJCAI-05.
[Erkan and Radev2004] Erkan, Gu?nes? and Dragomir R.
Radev. 2004. Lexrank: Graph-based lexical centrality
as salience in text summarization. JAIR, 22:457?479.
[Etzioni et al2004] Etzioni, Oren, Michael J. Cafarella, Doug
Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in KnowItAll. In
Proceedings of WWW-04.
[Etzioni et al2005] Etzioni, Oren, Michael Cafarella, Doug
Downey, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the web: an
experimental study. Artif. Intell., 165:91?134.
[Hassan et al2007] Hassan, Samer, Rada Mihalcea, and Car-
men Banea. 2007. Random-walk term weighting for im-
proved text classification. ICSC.
[Hearst1992] Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora. In Proceedings of
COLING-92.
[Kleinberg1999] Kleinberg, Jon Michael. 1999. Authorita-
tive sources in a hyperlinked environment. Journal of the
ACM, 46(5):604?632.
[Lenat1995] Lenat, Douglas B. 1995. Cyc: a large-scale in-
vestment in knowledge infrastructure. Commun. ACM,
38(11).
[Liu and Yang2008] Liu, Nathan and Qiang Yang. 2008.
Eigenrank: a ranking-oriented approach to collaborative
filtering. In SIGIR 2008.
[Matuszek et al2005] Matuszek, Cynthia, Michael Witbrock,
Robert C. Kahlert, John Cabral, Dave Schneider, Purvesh
Shah, and Doug Lenat. 2005. Searching for common
sense: Populating cyc from the web. In AAAI-05.
[Mintz et al2009] Mintz, Mike, Steven Bills, Rion Snow, and
Daniel Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In ACL-09.
[Pas?ca et al2006] Pas?ca, Marius, Dekang Lin, Jeffrey
Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organiz-
ing and searching the world wide web of facts - step one:
The one-million fact extraction challenge. In Proceedings
of AAAI-06.
[Page et al1999] Page, Lawrence, Sergey Brin, Rajeev Mot-
wani, and Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the Web. Technical Report
1999/66, Stanford University, Computer Science Depart-
ment.
[Pantel and Pennacchiotti2006] Pantel, Patrick and Marco
Pennacchiotti. 2006. Espresso: leveraging generic pat-
terns for automatically harvesting semantic relations. In
ACL/COLING-06.
[Pantel et al2004] Pantel, Patrick, Deepak Ravichandran,
and Eduard Hovy. 2004. Towards terascale knowledge
acquisition. In COLING-04.
[Pantel et al2009] Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu, and Vishnu Vyas. 2009.
Web-scale distributional similarity and entity set expan-
sion. In EMNLP-09.
[Ravichandran and Hovy2002] Ravichandran, Deepak and
Eduard Hovy. 2002. Learning surface text patterns for
a question answering system. In Proceedings of ACL-08,
pages 41?47. Association for Computational Linguistics.
[Riloff and Jones1999] Riloff, Ellen and Rosie Jones. 1999.
Learning dictionaries for information extraction by multi-
level bootstrapping. In Proceedings of AAAI-99.
[Talukdar et al2008] Talukdar, Partha Pratim, Joseph
Reisinger, Marius Pasca, Deepak Ravichandran, Rahul
Bhagat, and Fernando Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph random
walks. In Proceedings of EMNLP-08.
[Yan et al2009] Yan, Yulan, Yutaka Matsuo, Zhenglu Yang,
and Mitsuru Ishizuka. 2009. Unsupervised relation ex-
traction by mining wikipedia texts with support from web
corpus. In ACL-09.
509
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1477?1488, Dublin, Ireland, August 23-29 2014.
Predicting Interesting Things in Text 
 
 
Michael Gamon 
Microsoft Corp. 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
Arjun Mukherjee 
Department of Computer 
Science 
University of Houston 
Houston, TX 77004 
ar-
jun4787@gmail.com 
Patrick Pantel 
Microsoft Corp. 
One Microsoft Way 
 Redmond, WA 98052 
ppantel@microsoft.com 
 
  
 
Abstract 
While reading a document, a user may encounter concepts, entities, and topics that she is interested in 
exploring more. We propose models of ?interestingness?, which aim to predict the level of interest a user 
has in the various text spans in a document. We obtain naturally occurring interest signals by observing 
user browsing behavior in clicks from one page to another. We cast the problem of predicting interesting-
ness as a discriminative learning problem over this data. We leverage features from two principal sources: 
textual context features and topic features that assess the semantics of the document transition. We learn 
our topic features without supervision via probabilistic inference over a graphical model that captures the 
latent joint topic space of the documents in the transition. We train and test our models on millions of real-
world transitions between Wikipedia documents as observed from web browser session logs. On the task 
of predicting which spans are of most interest to users, we show significant improvement over various 
baselines and highlight the value of our latent semantic model. 
1 Introduction 
Reading inevitably leads people to discover interesting concepts, entities, and topics. Predicting what 
interests a user while reading a document has important applications ranging from augmenting the doc-
ument with supplementary information, to ad placement, to content recommendation. We define the task 
of predicting interesting things (ITs) as ranking text spans in an unstructured document according to 
whether a user would want to know more about them. This desire to learn more serves as our proxy for 
interestingness. 
There are many types of observable behavior that indicate user interest in a text span. The closest one 
to our problem definition is found in web browsing, where users click from one document to another 
via named anchors. The click process is generally governed by the user?s interest (modulo erroneous 
clicks). As such, the anchor name can be viewed as a text span of interest for that user. Furthermore, the 
frequency with which users, in aggregate, click on an anchor serves as a good proxy for the level of 
interest1. 
What is perceived as interesting is influenced by many factors. The semantics of the document and 
candidate IT are important. For example, we find that when users read an article about a movie, they are 
more likely to browse to an article about an actor or character than to another movie or the director. 
Also, user profile and geo-temporal information are relevant. For example, interests can differ depend-
ing on the cultural and socio-economic background of a user as well as the time of the session (e.g., 
weekday versus weekend, daytime versus late night, etc.). 
1 Other naturally occurring expressions of user interest, albeit less fitting to our problem, are found in web search queries, 
social media engagement, and others. 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
                                                 
1477
Strictly speaking, human interestingness is a psychological and cognitive process (Varela et al., 1991). 
Clicks and long dwell times are salient observed behavioral signals of interestingness that have been 
well accepted in the information retrieval literature (Claypool et al., 2001; Mueller and Lockerd, 2001). 
In this paper, we utilize the observed user?s browsing behavior as a supervision signal for modeling 
interestingness. Specifically, we cast the prediction of ITs as a discriminative learning task. We use a 
regression model to predict the likelihood of an anchor in a Wikipedia article to be clicked, which as we 
have seen above can serve as a proxy for interestingness. Based on an empirical study of a sample of 
our data, we use features in our model from the document context (such as the position of the anchor 
text, frequency of the anchor text in the current paragraph, etc.) as well as semantic features that aim to 
capture the latent topic space of the documents in the browsing transition. These semantic features are 
obtained in an unsupervised manner via a joint topic model of source and target documents in browsing 
transitions. We show empirical evidence that our discriminative model is effective in predicting ITs and 
we demonstrate that the automatically learned latent semantic features contribute significantly to the 
model performance. The main contributions of this paper are: 
? We introduce the task of predicting interesting things as identifying what a user likely wants to 
learn more about while reading a document. 
? We use browsing transitions as a proxy for interestingness and model our task using a discrimina-
tive training approach. 
? We propose a semantic probabilistic model of interestingness, which captures the latent aspects 
that drive a user to be interested in browsing from one document to another. Features derived from 
this semantic model are used in our discriminative learner. 
? We show empirical evidence of the effectiveness of our model on an application scenario. 
2 Related Work 
Salience: A notion that might at first glance be confused with interestingness is that of salience (Paranjpe 
2009; Gamon et al. 2013). Salience can be described as the centrality of a term to the content of a 
document. Put another way, it represents what the document is about. Though salience and interesting-
ness can interact, There are clear differences. For example, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average user would probably not be interested in learning more 
about Obama while reading that article. 
Click Prediction: Click prediction models are used pervasively by search engines. Query based click 
prediction aims at computing the probability that a given document in a search-result page is clicked on 
after a user enters some query (Joachims, 2002; Joachims et al., 2005; Agichtein et al., 2006; Guo et al., 
2009a). Click prediction for online advertising is a core signal for estimating the relevance of an ad to a 
search result page or a document (Chatterjee et al., 2003; Broder et al., 2007; Craswell et al., 2008; 
Graepel et al., 2010). Also related are personalized click models, e.g., (Shen et al., 2012), which use 
user-specific click through rate (CTR). Although these applications and our task share the use of CTR 
as a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/fea-
ture at runtime, our task specifically aims at predicting interestingness in the absence of web usage 
features: Our input is completely unstructured and there is no assumption of prior user interaction data. 
Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has re-
semblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009). 
However, these are tailored for blogs and associated comment discussions which is very different from 
our source to destination browsing transition logs. Guo et al., (2009b) used probabilistic models for 
discovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric search 
were explored. Gao et al. (2011) employ statistical machine translation to connect two types of content, 
learning semantic translation of queries to document titles.  None of the above models, however, are 
directly applicable to the joint topic mappings that are involved in source to destination browsing tran-
sitions which are the focus of our work. 
Predicting Popular Content: Modeling interestingness is also related to predicting popular content 
in the Web and content recommenders (Lerman and Hogg, 2010; Szabo and Huberman, 2010; Bandari 
et al., 2012). In contrast to these tasks, we strive to predict what term a user is likely to be interested in 
when reading content. We do not rely on prior browsing history, since we aim to predict interestingness 
1478
in unstructured text with no interaction history. We show in our experiments that a popularity signal 
alone is not a sufficient predictor for interestingness. 
3 The Interestingness Task 
The process of identifying interesting things (ITs) on a page consists of two parts: (1) generating candi-
date things (e.g., entities, concepts, topics); and (2) scoring and ranking these according to interesting-
ness. In this paper, we fix step 1 and focus our effort on step 2, i.e., the assignment of an interestingness 
score to a candidate. We believe that this scope is appropriate in order to understand the factors that 
enter into what is perceived as interesting by a user. Once we have gained an understanding of the 
interestingness scoring problem, however, there are opportunities in identifying candidates automati-
cally, which we leave for future work. 
In this section we begin by formally defining our task. We then introduce our data set of naturally 
occurring interest signals, followed by an investigation of the factors that influence them. 
3.1 Formal Task Definition 
We define our task as follows. Let ??  be the set of all documents and ?? be the set of all candidate text 
spans in all documents in ?? , generated by some candidate generator. Let ???? ? ??  be the set of candi-
dates in ?? ? ?? . We formally define the interestingness task as learning the function below, where 
??(??, ??) is the interestingness of candidate ?? in 2: 
??:?? ? ?? ? ?  (1) 
3.2 Data Set 
User browsing events on the web (i.e., a user clicking from one document to another) form a naturally 
occurring collection of interestingness signals. That is when a user clicks on an anchor in a document, 
we can postulate that the user is interested in learning more about it, modulo erroneous clicks. 
We collect a large set of many millions of such user browsing events from session logs of a commer-
cial web browser. Specifically, we collect from these logs each occurrence of a user click from one 
Wikipedia page to another during a one month period, from all users in all parts of the world. We refer 
to each such event as a transition. For each transition, our browser log provides metadata, including user 
profile information, geo-location information and session information (e.g., time of click, source/target 
dwell time, etc.) Our data set includes millions of transitions between Wikipedia pages.  
For our task we require: (1) a mechanism for generating candidate things; (2) ample clicks to serve 
as a reliable signal of interestingness for training our models; and (3) accessible content. Our focus on 
Wikipedia satisfies all. First, Wikipedia pages tend to contain many anchors, which can serve as the set 
of candidate things to be ranked. Second, Wikipedia attracts enough traffic to obtain robust browsing 
transition data. Finally, Wikipedia provides full content3 dumps. It is important here to note that our 
choice of Wikipedia as a test bed for our experiments does not restrict the general applicability of our 
approach: We propose a semantic model (Section 4.2) for mining latent features relevant to the phenom-
enon of interestingness which is general and can be applied to generic Web document collections. 
Using uniform sampling, we split our data into three sets: a development set (20%), a training set 
(60%) and a test set (20%). We further subdivide the test set by assigning each transition as belonging 
to the HEAD, TORSO, or TAIL, which we compute using inverse CDF sampling on the test set. We do 
so by assigning the most frequently occurring transitions, accounting for 20% of the (source) traffic, to 
the HEAD. Similarly, the least frequently occurring transitions, accounting for 20% of the (source) traf-
fic, are assigned to the TAIL. The remaining transitions are assigned to the TORSO. This three-way 
split reflects a common practice in the IR community and is based on the observation that web traffic 
frequencies show a very skewed distribution, with a small set of web pages attracting a large amount of 
traffic, and a very long tail of infrequently visited sites. Different regions in that distribution often show 
marked differences in behaviour, and models that are useful in one region are not necessarily as useful 
in another. 
2 We fix ??(??, ??) = 0 for all ?? ? ????. 
3 We utilize the May 3, 2013 English Wikipedia dump from http://dumps.wikimedia.org, consisting of roughly 4.1 million 
articles. 
                                                 
1479
3.3 What Factors Influence Interestingness? 
We manually inspected 200 random transitions from our development set. Below, we summarize our 
observations. 
Only few things on a page are interesting: The average number of anchors on a Wikipedia page is 79. 
Of these, only very few are actually clicked by users. For example, the Wikipedia article on the TV 
series ?The Big Bang Theory? leads to clicks on anchors linking to the pages of the series? actors for 
90% of transitions (while these anchors account for only a small fraction of all unique anchors on that 
page). 
The semantics of source and destination pages is important: We manually determined the entity type 
of the Wikipedia articles in our sample, according to schema.org classes. 49% of all source urls in our 
data sample are of the Creative Work category, reflecting the strong popular interest in movies 
(37%), actors (22%), artists (18%), and television series (8%). The next three most prominent categories 
are Organization (12%), Person (11%) and Place (6%). We observed that transitions are influ-
enced by these categories. For example, when the source article category is Movie, the most frequently 
clicked pages are of category Actor (63%) and Character (13%). For source articles of the 
TVSeries category, Actor destination articles account for 86% of clicks. Actor articles lead to 
clicks on Movie articles (45%) and other Actor articles (26%), whereas Artist articles lead to 
clicks on other Artist articles (29%), Movie articles (17%) and MusicRecording articles (18%). 
The structure of the source page plays a role: It is well known that the position of a link on a page 
influences user click behavior: links that are higher on a page or in a more prominent position tend to 
attract more clicks. We noticed similar trends in our data. 
The user plays a role: We hypothesized that users from different geographic and cultural backgrounds 
might exhibit different interests, or that interests are time-bound (e.g., interests on weekends differ from 
those on week days, daytime from nighttime, etc.) Initial experiments showed small effects of these 
factors, however, a more thorough analysis on a larger sample is necessary, which we leave for future 
work. 
4 Modeling Interestingness 
We cast the problem of learning the interestingness function ?? (see Eq. 1) as a discriminative regression 
learning problem. Below, we first describe this model, and then we introduce our semantic topic model 
which serves to provide semantic features for the discriminative learner. 
4.1 Discriminative Model 
Although our task is to predict ITs from unstructured documents, we can leverage the user interactions 
in our data, described in Section 3.2 as our training signal. 
Given a source document ?? ? ?? , and an anchor in s leading to destination document d, we use the 
aggregate click frequency of this anchor as a proxy for its interestingness, i.e.: 
??(??, ??) = ??(??|??)                       (2) 
where ??(??|??) is the probability of a user clicking on the anchor to ?? when viewing ??3F4. We use ??(??|??) 
as our regression target computed from our training data. 
For our learning algorithm, we use boosted decision trees (Friedman, 1999). We tune our hyperpa-
rameters (i.e., number of iterations, learning rate, minimum instances in leaf nodes, and the maximum 
number of leaves) using cross-validation on the development set. Each transition in our training data is 
represented as a vector of features, where the features fall into three basic families: 
1 Anchor features (Anc): position of the anchor in the document, frequency of the anchor, anchor 
density in the paragraph, and whether the anchor text matches the title of the destination page. 
2 User session features (Ses): city, country, postal code, region, state and timezone of the user, as 
well as day of week, hour, and weekend vs. workday of the occurrence of the transition. 
4 For notational convenience, we use ??(??, ??) even though Eq. 1 defines its second argument as being a candidate text span. 
Here, it is implicit that d consists of both the target document and the anchor text (which serves as the candidate text span). 
                                                 
1480
3 Semantic features: sourced in various experimental configurations from (1) Wikipedia page cate-
gories as assigned by Wikipedia editors (Wiki) or from (2) an unsupervised joint topic transition 
model (JTT) of source and destination pages (described in detail in the next section). 
In some experimental configurations we use Wikipedia page categories as semantic features. We show 
in our experiments (see Section 5) that these are highly discriminative. It is important to note that editor-
labeled category information is available in the Wikipedia domain but not in others. In other words, we 
can use this information to verify that semantics indeed is influential for interestingness, but we should 
design our models to not rely on this. We thus build an unsupervised semantic model of source and 
destination pages, which serves the purpose of providing semantic information without any domain-
specific annotation. 
4.2 The Semantics of Interestingness 
As indicated in Section 3, the semantics of source and destination pages, ?? and ??, influence the likeli-
hood that a user is interested in ?? after viewing ??. In this section we propose an unsupervised method 
for modeling the transition semantics between ?? and ??. As outlined in the previous section, this model 
then serves to generate semantic features for our discriminative model of interestingness. 
Referring to the notations in Table 1, we start by positing a distribution over the joint latent transition 
topics (in the higher level of semantic space), ???? for each transition ??. The corresponding source ??(??) 
and destination ??(??) articles of a given transition ?? are assumed to be admixtures of latent topics that are 
conditioned on the joint topic transition distribution, ????. For ease of reference, we will refer to this model 
as the Joint Transition Topic Model (JTT). The variable names and their descriptions are provided in 
Table 1. Figure 1 shows the plate notation of our model and the generative process: 
 
        
     
                        T
? 
zd zs 
ws wd
              
            K
?  
? 
? 
Ns Nd
 
Figure 1. Generative Process and Plate Notation of JTT. 
1. For each topic ??, draw ???? ~ ??????(??) 
2. For each transition ??: 
a. Draw the joint topic transition distribution, ???? ~ 
??????(??) 
b. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????  ~ ????????(????) 
c. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????   ~ ????????(????) 
1481
Variable Description Variable Description 
?? A transition ?? ???? , ???? Set of all topics in src, dest pages 
??(??), ??(??) The src and dest pages of ?? ?? ??, ?? ?? Set of all word tokens in src, dest pages 
????~??????(??) Joint src/dest topic distribution ? = {????} 
Set of all latent joint transition topic dis-
tributions 
????, ???? Latent topics of  ??(??), ??(??) ? = {????} Set of all latent topics 
????, ???? Observed word tokens of  ??(??), ??(??) ????,?? Contribution of topic ?? in transition ?? 
???? ~ ??????(??  
Latent topic-word distributions for 
topic ?? 
????,??
?? , ????,??
??  ??th word of transition ?? in ??(??), ??(??) 
??, ??  Dirichlet parameters for ??, ?? ????,???? , ????,????   Latent topic of ??th word of ?? in ??(??), ??(??) 
????, ???? No. of terms in src and dest pgs of ?? ????(??),??
??  No. of words in ??(??) assigned to topic ?? 
?? = {??} Set of all transitions, ?? ????(??),??
??  No. of words in ??(??) assigned to ?? 
?? No. of topics ????,??
??  No. of times word ?? assigned to ?? in ?? ?? 
??  No. of unique terms in the vocab. ????,????  No. of times word ?? assigned to ?? in ?? ?? 
Table 1. List of notations. 
Exact inference for JTT is intractable. Hence, we use Markov Chain Monte Carlo (MCMC) Gibbs sam-
pling. Rao-Blackwellization (Bishop, 2006) is used to reduce sampling variance by collapsing latent 
variables ?? and ??. Owing to space constraints, we omit the full derivation details. The full joint can be 
written succinctly as follows: 
??(????,????,????, ????) = ??
???????(??),[ ]
??  + ????(??),[ ]
?? + ???
??(??)
??
??=1
? = ??
??(????,[ ]
??  + ????,[ ]
?? +??)
??(??)
??
??=1
? (3) 
Omission of a latter index in the count variables, denoted by [ ], corresponds to the row vector span-
ning over the latter index. The corresponding Gibbs conditional distributions for ???? and ???? are detailed 
below, where the subscript ??(??, ??)? denotes the value of the expression excluding the counts of the 
term (??, ??): 
???????,??
?? = ??| ? ? ?
?????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +??
? ??????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +?????
??=1
?
?????,??
?? ?
?(??,??)
+ ????,??
?? +??
? ??????,??
?? ?
?(??,??)
+ ????,??
?? +?????
??=1
  (4) 
???????,??
?? = ??| ? ? ?
????(??),??
??  +?????(??),??
?? ?
?(??,??)
+??
? ?????(??),??
??  +?????(??),??
?? ?
?(??,??)
+?????
??=1
?
????,??
??  + ?????,??
?? ?
?(??,??)
+??
? ?????,??
??  + ?????,??
?? ?
?(??,??)
+?????
??=1
 (5) 
We learn our joint topic model from a random traffic-weighted sample of 10,000 transitions, which are 
randomly sampled from the development set outlined in Section 3.25. The decision to use this sample 
of 10,000 transitions is based on the observation that there were no statistically significant performance 
gains for models trained on more than 10k transitions. The Dirichlet hyperparameters are set to ?? = 
50/?? and ?? = 0.1 according to the values suggested in (Griffiths and Steyvers, 2004). The number of 
topics, ??, is empirically set to 50. We also conducted pilot experiments with other hyperparameter set-
tings, larger transition sets and more topics but we found no substantial difference in the end-to-end 
performance. Although increasing the number of topics and modeling more volume usually results in 
lowering perplexities and better fitting in topic models (Blei et al., 2003), it can also result in redun-
dancy in topics which may not be very useful for downstream applications (Chen et al., 2013). For all 
reported experiments we use the posterior estimates of our joint model learned according to the above 
settings. In our discriminative interestingness model, we use three classes of features from JTT to cap-
ture the latent topic distributions of the source page, the destination page, and the joint topics for that 
transition. These correspond to source topic features (????, labeled as JTTsrc in charts), destination topic 
features (????, labeled as JTTdst), and transition topic features (?, labeled as JTTtrans). Each of these 
three sets comprises 50 features, for a total of 150.? is the distribution over joint src and dst topics that 
5 Note that we use the development set to train our semantic model since it is ultimately used to generate features for our dis-
criminative learner of Section 4. Since the learner is trained using the training set, this strategy avoids overfitting our seman-
tic model to the training set. 
                                                 
1482
appear in a particular transition. ???? and ???? are the actual topic assignments for individual words in src 
and dst. Upon learning the JTT model, for each K topics, we get a probability of that topic appearing in 
the transition, in the src, and in the dst document (by taking the posterior point estimates for latent 
variables  ?, ????, ???? respectively). The GBDT implementation we use for our discriminative model per-
forms binning of these real-valued features over an ensemble of DTs.  
5 Experiments 
We evaluate our interestingness model on the task of proposing ?? anchors on a page that the user will 
find interesting (highlighting task). Recall the interestingness function ?? from Eq. 1. In the highlighting 
task, a user is reading a document ?? ? ??  and is interested in learning more about a set of anchors. Our 
goal in this task is to select ?? anchors that maximize the cumulative degree of interest of the user, i.e.: 
argmax
??????=(??1,?,????|?????????)
? ??(??, ????)???????????
         (6)  
In other words, we consider the ideal selection to consist of the k most interesting anchors according to 
??(??, ??).We compare the interestingness ranking of our models against a gold standard function, ???, com-
puted from our test set. Recall that we use the aggregate click frequency of an anchor as a proxy for its 
interestingness. As such, the gold standard function for the test set is computed as: 
???(??, ??) = ??(??|??)                      (7) 
where ??(??|??) is the probability of a user clicking on the anchor ?? when viewing ??. 
Given a source document ??, we measure the quality of a model?s interestingness ranking against the 
ideal ranking defined above using the standard nDCG metric (Manning et al., 2008). We use the inter-
estingness score of the gold standard as the relevance score. 
Table 2 shows the nDCG results for two baselines and a range of different feature sets. The first high-
level observation is that the task is difficult, given the low baseline results. Since there are many anchors 
on an average page, picking a random set of anchors yields very low nDCG scores. The nDCG numbers 
of our baselines increase as we move from HEAD to TORSO to TAIL, due to the fact that the average 
number of links per page (not unique) decreases in these sets from 170 to 94 to 416. The second baseline 
illustrates that it is not sufficient to simply pick the top n anchors on a page. 
Next, we see that using our set of anchor features (see Section 4.1) in the regression model greatly 
improves performance over the baselines, with the strongest numbers on the HEAD set and decreasing 
effectiveness in TORSO and TAIL. This shows that the distribution of interesting anchors on a page 
differs according to the popularity of the source content, possibly also with the length of the page. Our 
best performing model is the one using anchor features and all three sets of latent semantic features 
(Table 2, row 6; source, destination, and transition topics). 
The biggest improvement is obtained on the HEAD data. This is not surprising given that the topic 
model is trained on a traffic weighted sample of Wikipedia articles and that HEAD pages tend to have 
more content, making the identification of topics more reliable. Regarding the individual contributions 
of the latent semantic features (Table 2, rows 4, 5), destination features alone hurt performance on the 
HEAD set. Latent semantic source features lead to a boost across the board, and the addition of latent 
semantic transition topic features produces the best model, with gains especially pronounced on the 
HEAD data. Figure 2 further shows the performance of our best configuration across ALL, HEAD, 
TORSO, and TAIL. Interestingly, the TAIL exhibits better performance of the model than the TORSO 
(with the exception of nDCG at rank 3 or higher). We hypothesize that this is because the average num-
ber of anchors in a TAIL page is less than half of that in a TORSO page. 
6 Wikipedia editors tend to spend more time on more frequently viewed documents, hence they tend contain more content and 
more anchors. 
                                                 
1483
 nDCG % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 4.07 4.90 6.24 8.10 3.56 4.83 7.66 10.92 6.20 11.74 19.50 25.82 
Baseline: first n an-
chors 
9.99 12.47 17.72 24.33 7.17 9.87 17.06 23.97 9.06 16.66 27.35 34.82 
Anc 21.46 22.50 25.30 29.47 13.85 16.80 22.85 28.20 10.88 19.16 29.33 36.48 
Anc+JTTdst 13.97 16.33 19.69 23.78 11.37 14.17 19.67 24.66 11.62 19.69 29.69 36.35 
Anc+JTTdst+JTTsrc 26.62 30.03 34.82 39.38 17.05 20.82 27.15 32.48 12.27 21.56 31.88 38.85 
Anc+JTT-
dst+JTTsrc+JTTtrans 
34.49 35.21 38.01 41.80 18.32 21.69 28.03 33.22 13.06 21.68 32.13 38.01 
Table 2. Highlighting performance (% nDCG @ n) for different feature sets across HEAD, TORSO, 
and TAIL. Bold indicates statistically significant best systems (with 95% confidence). 
Not shown in these results are the effects of using user session features. We consistently found that 
these features did not improve upon the configurations where anchor and JTT features are used. We do 
not, however, rule out the potential of such features on this task, especially in light of our data analysis 
observations from Section 3.3, which suggest an effect from these factors. We leave a more in-depth 
study of the potential contribution of these types of features for future research. 
We now address the question how our unsupervised latent semantic features perform compared to the 
editor-assigned categories for Wikipedia pages, for two reasons. First, it is reasonable to consider the 
manually assigned Wikipedia categories as a (fine-grained) oracle for topic assignments. Second, out-
side of Wikipedia, we do not have the luxury of manually assigned categories/topics. As illustrated in 
Figure 3, we found that Wikipedia categories outperform the JTT topic features, but the latter can re-
cover about two thirds of the nDCG gain compared to Wikipedia categories. 
Finally, in the HEAD part of the data, we have enough historical clickthrough data that we could 
directly leverage for prediction. We conducted experiments where we used the prior probability ??(??|??) 
obtained from the development data (both smoothed and unsmoothed). Following this strategy we can 
achieve up to 65% nDCG@10 as shown in Figure 4 where the use of prior history (labeled ?History: 
Target | Source Prior?) is compared to our best model and to baselines. As stressed before, in most real-
life applications, this is not a viable option since anchors or user-interaction logs are unavailable. Even 
in web browsing scenarios, the TORSO and TAIL have no or only very sparse histories. Furthermore, 
the information is not available in a ?cold start? scenario involving new and unseen pages. We also 
examined whether the general popularity of a target page is sufficient to predict an anchor?s interesting-
ness, and we found that this signal performs better than the baselines, but significantly worse than our 
models. This series is labeled ?History: Target Prior? in Figure 4. 
 
Figure 2. NDCG comparison across overall performance (ALL) versus HEAD, TORSO, and TAIL 
subsets, on the Highlighting task. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Best Configuration (Anc+JTT)
NDCG @ Rank (ALL vs. HEAD vs. TORSO vs. TAIL)
ALL
HEAD
TORSO
TAIL
1484
  
Figure 3. JTT features versus Wikipedia category features on Highlighting task. 
 
Figure 4. Highlighting task comparison between baselines, best configuration using JTT, and models 
with historical transitions. 
Our highlights task reflects the main goal of our paper, i.e., to predict interestingness in the context of 
any document, whether it be a web page, an email, or a book. A natural extension of our work, especially 
in our experimental setting with Wikipedia transitions, is to predict the next click of a user, i.e., click 
prediction. 
There is a subtle but important difference between the two tasks. Highlights aims to identify a set of 
interesting nuggets for a source document. A user may ultimately click on only a subset of the nuggets, 
and perhaps not in the order of most interest. Our experimental metric, nDCG, reflects this ranking task 
well. Click prediction is an inherently more difficult task, where we focus on predicting exactly the next 
click of a specific user. Unlike in the highlights task, there is no partial credit for retrieving other inter-
esting anchors. Only the exact clicked anchor is considered a correct result. As such, we utilize a differ-
ent metric than nDCG on this task. We measure our model?s performance on the task of click prediction 
using cumulative precision. Given a unique transition event ?(s,a,d) by a particular user at a particular 
time, we present the transition, minus the gold anchor a and destination d, to our models, which in turn 
predict an ordered list of most likely anchors on which the user will click. The cumulative precision at 
k of a model, is 1 if any of the predicted anchors matched a, and 0 otherwise. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Semantic Features: JTT vs. Wikipedia Categories
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
Anc
Anc+Wiki_Src+Wik
i_tar
Anc+JTT_src+JTT_t
ar+JTT_trans
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Interest Models vs. Access to Historical Transitions
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
History: Target Prior
History: Target |
Source Prior
Anc+JTT_src+JTT_tar
+JTT_trans
1485
Table 3 outlines the results on this task and Figure 5 shows the corresponding chart for our best 
configuration. Note that in the click prediction task, the model performs best on the TAIL, followed by 
TORSO and HEAD. This seems to be a reflection of the fact that in this harder task, the total number of 
anchors per page is the most influential factor in model performance. 
Cumulative  
Precision % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 1.07 2.08 5.29 10.55 1.94 3.91 9.71 19.00 5.97 11.66 26.43 44.94 
Baseline: first n an-
chors 
2.68 5.77 16.73 33.78 4.10 8.19 22.86 42.08 8.77 16.57 36.80 58.52 
Anc 8.40 12.55 22.04 34.22 8.70 14.37 27.56 42.68 10.59 19.08 38.27 59.04 
Anc+JTTdst 5.48 9.19 17.77 29.14 6.93 12.07 23.90 38.00 11.23 19.59 38.46 57.87 
Anc+JTTdst+JTTsrc 9.02 15.65 30.05 44.72 10.11 17.42 32.08 47.07 11.95 21.47 40.96 61.24 
Anc+JTT-
dst+JTTsrc+JTTtrans 
11.53 18.43 31.93 45.36 10.86 18.19 32.96 47.66 12.64 21.58 41.27 61.28 
Table 3. Click prediction results for different feature sets across HEAD, TORSO, and TAIL. Bold indicates sta-
tistically significant best systems (with 95% confidence). 
 
Figure 5. Overall performance (ALL) versus HEAD, TORSO, and TAIL subsets on click prediction. 
6 Conclusion and Future Directions 
We presented a notion of an IT on a page that is grounded in observable browsing behavior during 
content consumption. We implemented a model for prediction of interestingness that we trained and 
tested within the domain of Wikipedia. The model design is generic and not tied to our experimental 
choice of the Wikipedia domain and can be applied to other domains. Our model takes advantage of 
semantic features that we derive from a novel joint topic transition model. This semantic model takes 
into account the topic distributions for the source, destination, and transitions from source to destination. 
We demonstrated that the latent semantic features from our topic model contribute significantly to the 
performance of interestingness prediction, to the point where they perform nearly as well as using editor-
assigned Wikipedia categories as features. We also showed that the transition topics improve results 
over just using source and destination semantic features alone. 
A number of future directions immediately suggest themselves. First, for an application that marks 
interesting ITs on an arbitrary page, we would need a detector for IT candidates. A simple first approach 
would be to use a state-of-the-art Named Entity Recognition (NER) system to cover at least a subset of 
potential candidates. This does not solve the problem entirely, since we know that named entities are 
not the only interesting nuggets ? general terms and concepts can also be of interest to a reader. On the 
other hand we do have reason to believe that entities play a very prominent role in web content con-
sumption, based on the frequency with which entities are searched for (see, for example Lin et al. 2012 
and the references cited therein). Using an NER system as a candidate generator would also allow us to 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Cu
m
ul
at
iv
e 
Pr
ec
is
io
n
Rank
Best Configuration (Anc+JTT)
Click Prediction - Cumulative Precision @ Rank
ALL
HEAD
TORSO
TAIL
1486
add another potentially useful feature to our interestingness prediction model: the type of the entity. One 
could also envision jointly modeling interestingness and candidate detection. 
A second point concerns the observation from the previous section on the different regularities that 
seem to be at play according to the popularity and possibly the length of an article. More detailed ex-
periments are needed to tease out this influence and possibly improve the predictive power of the model. 
User session features did not contribute to model performance when used in conjunction with other 
feature families, but closer investigation of these features is warranted for more personalized models of 
interestingness. Finally, a number of options regarding JTT  could be explored further. Being trained on 
a traffic-weighted sample of articles, the topic model predominantly picks up on popular topics. This 
could be remedied by training on a non-weighted sample, or, more promisingly, on a larger non-
weighted sample with a larger ??, i.e. more permissible total topics. 
References 
Agichtein, E., Brill, E., and Dumais, S. 2006. Improving web search ranking by incorporating user behavior infor-
mation. In Proceedings of SIGIR. pp. 19-26. 
Bandari, R., Asur, S., and Huberman, B. A. 2012. The Pulse of News in Social Media: Forecasting Popularity. In 
Proceedings of ICWSM. 
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent dirichlet allocation. Journal of Machine Learning Re-
search, 3, 993-1022. 
Broder, A., Fontoura, M., Josifovski, V., and Riedel, L. 2007. A semantic approach to contextual advertising. In 
Proceedings of SIGIR. pp. 559-566. 
Bishop, C.M. 2006. Pattern Recognition and Machine Learning. Springer.  
Chatterjee, P., Hoffman, D. L., and Novak, T. P. 2003. Modeling the clickstream: Implications for web-based 
advertising efforts. Marketing Science, 22(4), 520-541. 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M. and Ghosh, R. 2013. Leveraging Multi-Domain Prior 
Knowledge in Topic Models. In Proceedings of IJCAI. pp. 2071-2077. 
Claypool, M., Le, P., Wased, M., & Brown, D. 2001a. Implicit interest indicators. In Proceedings of the 6th inter-
national conference on Intelligent user interfaces (pp. 33-40). ACM. 
Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B. 2008. An experimental comparison of click position-bias 
models. In Proceedings of WSDM. pp. 87-94. 
Erosheva, E., Fienberg, S., and Lafferty, J. 2004. Mixed membership models of scientific publications. In Pro-
ceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1). pp. 5220-5227. 
Friedman, J. H. 1999. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189-
1232, 1999. 
Gamon, M., Yano, T., Song, X., Apacible, J. and Pantel, P. 2013. Identifying Salient Entities in Web Pages. In 
Proceedings CIKM. pp. 2375-2380. 
Gao, J., Toutanova, K., and Yih, W. T. 2011. Clickthrough-based latent semantic models for web search. In Pro-
ceedings of SIGIR. pp. 675-684. 
Graepel, T., Candela, J.Q., Borchert, T., and Herbrich, R. 2010. Web-scale Bayesian Click-Through Rate Predic-
tion for Sponsored Search Advertising in Microsoft?s Bing Search Engine. In Proceedings of ICML. pp. 13-20. 
Griffiths, T.L and Steyvers, M. 2004. Finding Scientific Topics. Proceedings of the National Academy of Science, 
101, suppl 1, 5228-5235. 
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.-M, and Faloutsos, C. 2009a. Click chain model in 
web search. In Proceedings of WWW. pp. 11-20. 
Guo, J., Xu, G., Cheng, X., and Li, H. 2009b. Named entity recognition in query. In Proceedings of SIGIR. pp. 
267-274. 
Joachims, T. 2002. Optimizing search engines using clickthrough data. In Proceedings of KDD. pp. 133-142. 
Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay, G. 2005. Accurately interpreting clickthrough data as 
implicit feedback. In Proceedings of SIGIR. pp. 154-161. 
1487
Lerman, K., and Hogg, T. 2010. Using a model of social dynamics to predict popularity of news. In Proceedings 
of WWW. pp. 621-630. 
Lin, T., Pantel, P., Gamon, M., Kannan, A., and Fuxman, A. 2012. Active objects: actions for entity-centric search. 
In Proceedings of WWW. pp. 589-598. 
Manning, C. D., Raghavan, P., and Schutze, H. 2008. Introduction to Information Retrieval. Cambridge University 
Press. 
Mueller, F., & Lockerd, A. 2001. Cheese: tracking mouse movement activity on websites, a tool for user modeling. 
In CHI'01 extended abstracts on Human factors in computing systems (pp. 279-280). ACM. 
Paranjpe, D. 2009. Learning document aboutness from implicit user feedback and document structure. In Proceed-
ings of CIKM. pp. 365-374. 
Shen, S., Hu, B., Chen, W., and Yang, Q. 2012. Personalized click model through collaborative filtering. In Pro-
ceedings of WSDM. pp. 323-333. 
Szabo, G., and Huberman, B. A. 2010. Predicting the popularity of online content. Com-munications of the 
ACM, 53(8), 80-88. 
Varela, F. J., Thompson, E. T., & Rosch, E. 1991. The embodied mind: Cognitive science and human experi-
ence. The MIT Press. 
Yano, T., Cohen, W. W., & Smith, N. A. 2009. Predicting response to political blog posts with topic models. In 
Proceedings of NAACL. pp. 477-485. 
1488
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Modeling Interestingness with Deep Neural Networks 
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.com 
 
 
Abstract 
This paper presents a deep semantic simi-
larity model (DSSM), a special type of 
deep neural networks designed for text 
analysis, for recommending target docu-
ments to be of interest to a user based on a 
source document that she is reading. We 
observe, identify, and detect naturally oc-
curring signals of interestingness in click 
transitions on the Web between source and 
target documents, which we collect from 
commercial Web browser logs. The DSSM 
is trained on millions of Web transitions, 
and maps source-target document pairs to 
feature vectors in a latent space in such a 
way that the distance between source doc-
uments and their corresponding interesting 
targets in that space is minimized. The ef-
fectiveness of the DSSM is demonstrated 
using two interestingness tasks: automatic 
highlighting and contextual entity search. 
The results on large-scale, real-world da-
tasets show that the semantics of docu-
ments are important for modeling interest-
ingness and that the DSSM leads to signif-
icant quality improvement on both tasks, 
outperforming not only the classic docu-
ment models that do not use semantics but 
also state-of-the-art topic models. 
1 Introduction 
Tasks of predicting what interests a user based on 
the document she is reading are fundamental to 
many online recommendation systems. A recent 
survey is due to Ricci et al. (2011). In this paper, 
we exploit the use of a deep semantic model for 
two such interestingness tasks in which document 
semantics play a crucial role: automatic highlight-
ing and contextual entity search. 
Automatic Highlighting. In this task we want 
a recommendation system to automatically dis-
cover the entities (e.g., a person, location, organi-
zation etc.) that interest a user when reading a doc-
ument and to highlight the corresponding text 
spans, referred to as keywords afterwards. We 
show in this study that document semantics are 
among the most important factors that influence 
what is perceived as interesting to the user. For 
example, we observe in Web browsing logs that 
when a user reads an article about a movie, she is 
more likely to browse to an article about an actor 
or character than to another movie or the director. 
Contextual entity search. After identifying 
the keywords that represent the entities of interest 
to the user, we also want the system to recommend 
new, interesting documents by searching the Web 
for supplementary information about these enti-
ties. The task is challenging because the same key-
words often refer to different entities, and interest-
ing supplementary information to the highlighted 
entity is highly sensitive to the semantic context. 
For example, ?Paul Simon? can refer to many peo-
ple, such as the singer and the senator. Consider 
an article about the music of Paul Simon and an-
other about his life. Related content about his up-
coming concert tour is much more interesting in 
the first context, while an article about his family 
is more interesting in the second. 
At the heart of these two tasks is the notion of 
interestingness. In this paper, we model and make 
use of this notion of interestingness with a deep 
semantic similarity model (DSSM). The model, 
extending from the deep neural networks shown 
recently to be highly effective for speech recogni-
tion (Hinton et al., 2012; Deng et al., 2013) and 
computer vision (Krizhevsky et al., 2012; Mar-
koff, 2014), is semantic because it maps docu-
ments to feature vectors in a latent semantic space, 
also known as semantic representations. The 
model is deep because it employs a neural net-
work with several hidden layers including a spe-
cial convolutional-pooling structure to identify 
keywords and extract hidden semantic features at 
different levels of abstractions, layer by layer. The 
semantic representation is computed through a 
deep neural network after its training by back-
propagation with respect to an objective tailored 
2
to the respective interestingness tasks. We obtain 
naturally occurring ?interest? signals by observ-
ing Web browser transitions, from a source docu-
ment to a target document, in Web usage logs of a 
commercial browser. Our training data is sampled 
from these transitions. 
The use of the DSSM to model interestingness 
is motivated by the recent success of applying re-
lated deep neural networks to computer vision 
(Krizhevshy et al. 2012; Markoff, 2014), speech 
recognition (Hinton et al. 2012), text processing 
(Collobert et al. 2011),  and Web search (Huang 
et al. 2013). Among them, (Huang et al. 2013) is 
most relevant to our work. They also use a deep 
neural network to map documents to feature vec-
tors in a latent semantic space. However, their 
model is designed to represent the relevance be-
tween queries and documents, which differs from 
the notion of interestingness between documents 
studied in this paper. It is often the case that a user 
is interested in a document because it provides 
supplementary information about the entities or 
concepts she encounters when reading another 
document although the overall contents of the sec-
ond documents is not highly relevant. For exam-
ple, a user may be interested in knowing more 
about the history of University of Washington af-
ter reading the news about President Obama?s 
visit to Seattle. To better model interestingness, 
we extend the model of Huang et al. (2013) in two 
significant aspects. First, while Huang et al. treat 
a document as a bag of words for semantic map-
ping, the DSSM treats a document as a sequence 
of words and tries to discover prominent key-
words. These keywords represent the entities or 
concepts that might interest users, via the convo-
lutional and max-pooling layers which are related 
to the deep models used for computer vision 
(Krizhevsky et al., 2013) and speech recognition 
(Deng  et al., 2013a) but are not used in Huang et 
al.?s model. The DSSM then forms the high-level 
semantic representation of the whole document 
based on these keywords. Second, instead of di-
rectly computing the document relevance score 
using cosine similarity in the learned semantic 
space, as in Huang et al. (2013), we feed the fea-
tures derived from the semantic representations of 
documents to a ranker which is trained in a super-
vised manner. As a result, a document that is not 
highly relevant to another document a user is read-
ing (i.e., the distance between their derived feature 
                                                          
1 We stress here that, although the click signal is available to 
form a dataset and a gold standard ranker (to be described in 
vectors is big) may still have a high score of inter-
estingness because the former provides useful in-
formation about an entity mentioned in the latter. 
Such information and entity are encoded, respec-
tively, by (some subsets of) the semantic features 
in their corresponding documents. In Sections 4 
and 5, we empirically demonstrate that the afore-
mentioned two extensions lead to significant qual-
ity improvements for the two interestingness tasks 
presented in this paper.  
Before giving a formal description of the 
DSSM in Section 3, we formally define the inter-
estingness function, and then introduce our data 
set of naturally occurring interest signals. 
2 The Notion of Interestingness 
Let ?  be the set of all documents. Following 
Gamon et al. (2013), we formally define the inter-
estingness modeling task as learning the mapping 
function: 
	 ?: ? ? ? ? ??	 		
where the function ???, ?? is the quantified degree 
of interest that the user has  in the target document 
? ? ? after or while reading the source document 
? ? ?. 
Our notion of a document is meant in its most 
general form as a string of raw unstructured text. 
That is, the interestingness function should not 
rely on any document structure such as title tags, 
hyperlinks, etc., or Web interaction data. In our 
tasks, documents can be formed either from the 
plain text of a webpage or as a text span in that 
plain text, as will be discussed in Sections 4 and 5. 
2.1 Data 
We can observe many naturally occurring mani-
festations of interestingness on the Web. For ex-
ample, on Twitter, users follow shared links em-
bedded in tweets. Arguably the most frequent sig-
nal, however, occurs in Web browsing events 
where users click from one webpage to another 
via hyperlinks. When a user clicks on a hyperlink, 
it is reasonable to assume that she is interested in 
learning more about the anchor, modulo cases of 
erroneous clicks. Aggregate clicks can therefore 
serve as a proxy for interestingness. That is, for a 
given source document, target documents that at-
tract the most clicks are more interesting than doc-
uments that attract fewer clicks1.  
Section 4), our task is to model interestingness between un-
structured documents, i.e., without access to any document 
structure or Web interaction data. Thus, in our experiments, 
3
We collect a large dataset of user browsing 
events from a commercial Web browser. Specifi-
cally, we sample 18 million occurrences of a user 
click from one Wikipedia page to another during 
a one year period. We restrict our browsing events 
to Wikipedia since its pages tend to contain many 
anchors (79 on average, where on average 42 have 
a unique target URL). Thus, they attract enough 
traffic for us to obtain robust browsing transition 
data2. We group together all transitions originat-
ing from the same page and randomly hold out 
20% of the transitions for our evaluation data 
(EVAL), 20% for training the DSSM described in 
Section 3.2 (TRAIN_1), and the remaining 60% 
for training our task specific rankers described in 
Section 3.3 (TRAIN_2). In our experiments, we 
used different settings for the two interestingness 
tasks. Thus, we postpone the detailed description 
of these datasets and other task-specific datasets 
to Sections 4 and 5. 
3 A Deep Semantic Similarity Model 
(DSSM) 
This section presents the architecture of the 
DSSM, describes the parameter estimation, and 
the way the DSSM is used in our tasks. 
                                                          
we remove all structural information (e.g., hyperlinks and 
XML tags) in our documents, except that in the highlighting 
experiments (Section 4) we use anchor texts to simulate the 
candidate keywords to be highlighted. We then convert each 
3.1 Network Architecture 
The heart of the DSSM is a deep neural network 
with convolutional structure, as shown in Figure 
1. In what follows, we use lower-case bold letters, 
such as ?, to denote column vectors, ???? to de-
note the ??? element of ?, and upper-case letters, 
such as ?, to denote matrices. 
Input Layer ?. It takes two steps to convert a doc-
ument ?, which is a sequence of words, into a vec-
tor representation ? for the input layer of the net-
work: (1) convert each word in ? to a word vector, 
and (2) build ? by concatenating these word vec-
tors. To convert a word ? into a word vector, we 
first represent ? by a one-hot vector using a vo-
cabulary that contains ?  high frequent words 
(? ? 150K in this study). Then, following Huang 
et al. (2013), we map ? to a separate tri-letter vec-
tor. Consider the word ?#dog#?, where # is a word 
boundary symbol. The nonzero elements in its tri-
letter vector are ?#do?, ?dog?, and ?og#?. We then 
form the word vector of ? by concatenating its 
one-hot vector and its tri-letter vector. It is worth 
noting that the tri-letter vector complements the 
one-hot vector representation in two aspects. First, 
different OOV (out of vocabulary) words can be 
represented by tri-letter vectors with few colli-
sions. Second, spelling variations of the same 
word can be mapped to the points that are close to 
each other in the tri-letter space. Although the 
number of unique English words on the Web is 
extremely large, the total number of distinct tri-
letters in English is limited (restricted to the most 
frequent 30K in this study). As a result, incorpo-
rating tri-letter vectors substantially improves the 
representation power of word vectors while keep-
ing their size small.  
To form our input layer ? using word vectors, 
we first identify a text span with a high degree of 
relevance, called focus, in ?  using task-specific 
heuristics (see Sections 4 and 5 respectively). Sec-
ond, we form ? by concatenating each word vec-
tor in the focus and a vector that is the summation 
of all other word vectors, as shown in Figure 1. 
Since the length of the focus is much smaller than 
that of its document, ? is able to capture the con-
textual information (for the words in the focus) 
Web document into plain text, which is white-space to-
kenized and lowercased. Numbers are retained and no stem-
ming is performed. 
2 We utilize the May 3, 2013 English Wikipedia dump con-
sisting of roughly 4.1 million articles from http://dumps.wiki-
media.org. 
Figure 1: Illustration of the network architec-
ture and information flow of the DSSM 
 
4
useful to the corresponding tasks, with a manage-
able vector size. 
Convolutional Layer ? . A convolutional layer 
extracts local features around each word ??	in a 
word sequence of length ?  as follows. We first 
generate a contextual vector ??  by concatenating 
the word vectors of ?? and its surrounding words defined by a window (the window size is set to 3 
in this paper). Then, we generate for each word a 
local feature vector ??  using a tanh  activation 
function and a linear projection matrix ??, which 
is the same across all windows ? in the word se-
quence, as: 
?? ? tanh??????? , where	? ? 1? 1) ?) 
Max-pooling Layer ?. The size of the output ? 
depends on the number of words in the word se-
quence. Local feature vectors have to be com-
bined to obtain a global feature vector, with a 
fixed size independent of the document length, in 
order to apply subsequent standard affine layers. 
We design ? by adopting the max operation over 
each ?time? ? of the sequence of vectors computed 
by (1), which forces the network to retain only the 
most useful, partially invariant local features pro-
duced by the convolutional layer: 
???? ? max???,?,??u????? (2) 
where the max operation is performed for each di-
mension of ? across ? ? 1,? , ? respectively.  
That convolutional and max-pooling layers are 
able to discover prominent keywords of a docu-
ment can be demonstrated using the procedure in 
Figure 2 using a toy example. First, the convolu-
tional layer of (1) generates for each word in a 5-
word document a 4-dimensional local feature vec-
tor, which represents a distribution of four topics. 
For example, the most prominent topic of ?? within its three word context window is the first 
topic, denoted by ???1?, and the most prominent 
topic of ?? is ???3?. Second, we use max-pooling of (2) to form a global feature vector, which rep-
resents the topic distribution of the whole docu-
ment. We see that ??1? and ??3? are two promi-
nent topics. Then, for each prominent topic, we 
trace back to the local feature vector that survives 
max-pooling: 
??1? ? max???,?,?????1?? ? ???1?  
??3? ? max???,?,?????3?? ? ???3?.  
Finally, we label the corresponding words of these 
local feature vectors, ?? and ??, as keywords of the document.  
Figure 3 presents a sample of document snip-
pets and their keywords detected by the DSSM ac-
cording to the procedure elaborated in Figure 2. It 
is interesting to see that many names are identified 
as keywords although the DSSM is not designed 
explicitly for named entity recognition. 
Fully-Connected Layers ?  and ? . The fixed 
sized global feature vector ? of (2) is then fed to 
several standard affine network layers, which are 
stacked and interleaved with nonlinear activation 
functions, to extract highly non-linear features ? 
at the output layer. In our model, shown in Figure 
1, we have: 
? ? tanh?????? (3) 
? ? tanh?????? (4) 
where ?? and ?? are learned linear projection matri-ces. 
3.2 Training the DSSM 
To optimize the parameters of the DSSM of Fig-
ure 1, i.e., ? ? ???,??,???, we use a pair-wise rank loss as objective (Yih et al. 2011). Consider 
a source document ?  and two candidate target 
documents ??	and ??, where ?? is more interesting 
than ??  to a user when reading ?. We construct 
two pairs of documents ??, ??? and ??, ???, where the former is preferred and should have a higher 
u1 u2 u3 u4 u5
w1 w2 w3 w4 w5
2
3
4
1
 
w1 w2 w3 w4 w5
v
2
3
4
1
Figure 2: Toy example of (upper) a 5-word 
document and its local feature vectors ex-
tracted using a convolutional layer, and (bot-
tom) the global feature vector of the document 
generated after max-pooling. 
 
 
5
interestingness score. Let ? be the difference of 
their interestingness scores: ?	? ???, ??? ?
???, ??? , where ?  is the interestingness score, computed as the cosine similarity: 
???, ?? ? sim???, ?? ?
?????
???????? 
(5) 
where ?? and ?? are the feature vectors of ? and ?, respectively, which are generated using the 
DSSM, parameterized by ?. Intuitively, we want 
to learn ? to maximize ?. That is, the DSSM is 
learned to represent documents as points in a hid-
den interestingness space, where the similarity be-
tween a document and its interesting documents is 
maximized.  
We use the following logistic loss over ? , 
which can be shown to upper bound the pairwise 
accuracy: 
???; ?? ? log?1 ? exp?????? (6) 
                                                          
3 In our experiments, we observed better results by sampling 
more negative training examples (e.g., up to 100) although 
this makes the training much slower. An alternative approach 
The loss function in (6) has a shape similar to the 
hinge loss used in SVMs. Because of the use of 
the cosine similarity function, we add a scaling 
factor ? that magnifies ? from [-2, 2] to a larger 
range. Empirically, the value of ? makes no dif-
ference as long as it is large enough. In the exper-
iments, we set ? ? 10. Because the loss function 
is differentiable, optimizing the model parameters 
can be done using gradient-based methods. Due to 
space limitations, we omit the derivation of the 
gradient of the loss function, for which readers are 
referred to related derivations (e.g., Collobert et 
al. 2011; Huang et al. 2013; Shen et al. 2014). 
In our experiments we trained DSSMs using 
mini-batch Stochastic Gradient Descent. Each 
mini-batch consists of 256 source-target docu-
ment pairs. For each source document ?, we ran-
domly select from that batch four target docu-
ments which are not paired with ?  as negative 
training samples3. The DSSM trainer is imple-
mented using a GPU-accelerated linear algebra li-
brary, which is developed on CUDA 5.5. Given 
the training set (TRAIN_1 in Section 2), it takes 
approximately 30 hours to train a DSSM as shown 
in Figure 1, on a Xeon E5-2670 2.60GHz machine 
with one Tesla K20 GPU card. 
In principle, the loss function of (6) can be fur-
ther regularized (e.g. by adding a term of 2? norm) 
to deal with overfitting. However, we did not find 
a clear empirical advantage over the simpler early 
stop approach in a pilot study, hence we adopted 
the latter in the experiments in this paper. Our ap-
proach adjusts the learning rate ?  during the 
course of model training. Starting with ? ? 1.0, 
after each epoch (a pass over the entire training 
data), the learning rate is adjusted as ? ? 0.5 ? ? 
if the loss on validation data (held-out from 
TRAIN_1) is not reduced. The training stops if 
either ?  is smaller than a preset threshold 
(0.0001) or the loss on training data can no longer 
be reduced significantly. In our experiments, the 
DSSM training typically converges within 20 
epochs. 
3.3 Using the DSSM 
We experiment with two ways of using the DSSM 
for the two interestingness tasks. First, we use the 
DSSM as a feature generator. The output layer of 
the DSSM can be seen as a set of semantic fea-
tures, which can be incorporated in a boosted tree 
is to approximate the partition function using Noise Contras-
tive Estimation (Gutmann and Hyvarinen 2010). We leave it 
to future work.  
? the comedy festival formerly known as 
the us comedy arts festival is a comedy 
festival held each year in las vegas 
nevada from its 1985 inception to 2008 
. it was held annually at the wheeler 
opera house and other venues in aspen 
colorado . the primary sponsor of the 
festival was hbo with co-sponsorship by 
caesars palace . the primary venue tbs 
geico insurance twix candy bars and 
smirnoff vodka hbo exited the festival 
business in 2007 and tbs became the pri-
mary sponsor the festival includes 
standup comedy performances appearances 
by the casts of television shows? 
 
? bad samaritans is an american comedy
series produced by walt becker kelly
hayes and ross putman . it premiered on 
netflix on march 31 2013 cast and char-
acters . the show focuses on a community 
service parole group and their parole 
officer brian kubach as jake gibson an 
aspiring professional starcraft player 
who gets sentenced to 2000 hours of com-
munity service for starting a forest 
fire during his breakup with drew prior 
to community service he had no real am-
bition in life other than to be a pro-
fessional gamer and become wealthy 
overnight like mark zuckerberg as in 
life his goal during ? 
Figure 3: A sample of document snippets and 
the keywords (in bold) detected by the DSSM. 
 
 
6
based ranker (Friedman 1999) trained discrimina-
tively on the task-specific data. Given a source-
target document pair ??, ??, the DSSM generates 
600 features (300 from the output layers ?? and ?? 
for each ? and ?, respectively). 
Second, we use the DSSM as a direct imple-
mentation of the interestingness function ?. Re-
call from Section 3.2 that in model training, we 
measure the interestingness score for a document 
pair using the cosine similarity between their cor-
responding feature vectors (?? and ??). Similarly 
at runtime, we define	? ? 	sim???, ?? as (5). 
4 Experiments on Highlighting 
Recall from Section 1 that in this task, a system 
must select ? most interesting keywords in a doc-
ument that a user is reading. To evaluate our mod-
els using the click transition data described in Sec-
tion 2, we simulate the task as follows. We use the 
set of anchors in a source document ? to simulate 
the set of candidate keywords that may be of in-
terest to the user while reading ?, and treat the text 
of a document that is linked by an anchor in ? as a 
target document ?. As shown in Figure 1, to apply 
DSSM to a specific task, we need to define the fo-
cus in source and target documents. In this task, 
the focus in s is defined as the anchor text, and the 
focus in t is defined as the first 10 tokens in t. 
We evaluate the performance of a highlighting 
system against a gold standard interestingness 
function ?? which scores the interestingness of an 
anchor as the number of user clicks on ? from the 
anchor in ? in our data. We consider the ideal se-
lection to then consist of the ?  most interesting 
anchors according to ??. A natural metric for this 
task is Normalized Discounted Cumulative Gain 
(NDCG) (Jarvelin and Kekalainen 2000). 
We evaluate our models on the EVAL dataset 
described in Section 2. We utilize the transition 
distributions in EVAL to create three other test 
sets, following the stratified sampling methodol-
ogy commonly employed in the IR community, 
for the frequently, less frequently, and rarely 
viewed source pages, referred to as HEAD, 
TORSO, and TAIL, respectively. We obtain 
these sets by first sorting the unique source docu-
ments according to their frequency of occurrence 
in EVAL. We then partition the set so that HEAD 
corresponds to all transitions from the source 
pages at the top of the list that account for 20% of 
the transitions in EVAL; TAIL corresponds to the 
transitions at the bottom also accounting for 20% 
of the transitions in EVAL; and TORSO corre-
sponds to the remaining transitions. 
4.1 Main Results 
Table 1 summarizes the results of various models 
over the three test sets using NDCG at truncation 
levels 1, 5, and 10. 
Rows 1 to 3 are simple heuristic baselines. 
RAND selects ?  random anchors, 1stK selects 
the first ? anchors and LastK the last ? anchors.  
The other models in Table 1 are boosted tree 
based rankers trained on TRAIN_2 described in 
Section 2. They vary only in their features. The 
ranker in Row 4 uses Non-Semantic Features 
(NSF) only. These features are derived from the 
 # Models HEAD TORSO TAIL 
   @1 @5 @10 @1 @5 @10 @1 @5 @10 
src
  o
nly
 
1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.258 
2 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.348 
3 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.219
4 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.365 
5 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.386 
6 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.372 
8 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382 
src
+ta
r 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.380 
12 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398 
Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test (? ?
0.05). 
 
 
7
source document s and from user session infor-
mation in the browser log. The document features 
include: position of the anchor in the document, 
frequency of the anchor, and anchor density in the 
paragraph.  
The rankers in Rows 5 to 12 use the NSF and 
the semantic features computed from source and 
target documents of a browsing transition. We 
compare semantic features derived from three dif-
ferent sources. The first feature source comes 
from our DSSMs (DSSM and DSSM_BOW) us-
ing the output layers as feature generators as de-
scribed in Section 3.3. DSSM is the model de-
scribed in Section 3 and DSSM_BOW is the 
model proposed by Huang et al. (2013) where 
documents are view as bag of words (BOW) and 
the convolutional and max-pooling layers are not 
used. The two other sources of semantic features 
are used as a point of comparison to the DSSM. 
One is a generative semantic model (Joint Transi-
tion Topic model, or JTT) (Gamon et al. 2013). 
JTT is an LDA-style model (Blei et al. 2003) that 
is trained jointly on source and target documents 
linked by browsing transitions. JTT generates a 
total of 150 features from its latent variables, 50 
each for the source topic model, the target topic 
model and the transition model. The other seman-
tic model of contrast is a manually defined one, 
which we use to assess the effectiveness of auto-
matically learned models against human model-
ers. To this effect, we use the page categories that 
editors assign in Wikipedia as semantic features 
(WCAT). These features number in the multiple 
thousands. Using features such as WCAT is not a 
viable solution in general since Wikipedia catego-
ries are not available for all documents. As such, 
we use it solely as a point of comparison against 
DSSM and JTT. 
We also distinguish between two types of 
learned rankers: those which draw their features 
only from the source (src only) document and 
those that draw their features from both the source 
and target (src+tar) documents. Although our 
task setting allows access to the content of both 
source and target documents, there are practical 
scenarios where a system should predict what in-
terests the user without looking at the target doc-
ument because the extra step of identifying a suit-
able target document for each candidate concept 
or entity of interest is computationally expensive.  
4.2 Analysis of Results 
As shown in Table 1, NSF+DSSM, which incor-
porates our DSSM, is the overall best performing 
system across test sets. The task is hard as evi-
denced by the weak baseline scores. One reason is 
the large average number of candidates per page. 
On HEAD, we found an average of 170 anchors 
(of which 95 point to a unique target URL). For 
TORSO and TAIL, we found the average number 
of anchors to be 94 (52 unique targets) and 41 (19 
unique targets), respectively. 
Clearly, the semantics of the documents form 
important signals for this task: WCAT, JTT, 
DSSM_BOW, and DSSM all significantly boost 
the performance over NSF alone. There are two 
interesting comparisons to consider: (a) manual 
semantics vs. learned semantics; and (b) deep se-
mantic models vs. generative topic models. On 
(a), we observe somewhat surprisingly that the 
learned DSSM produces features that outperform 
the thousands of features coming from manually 
(editor) assigned Wikipedia category features 
(WCAT), in all but the TAIL where the two per-
form statistically the same. In contrast, features 
from the generative model (JTT) perform worse 
than WCAT across the board except on TAIL 
where JTT and WCAT are statistically tied. On 
(b), we observe that DSSM outperforms a state-
of-the-art generative model (JTT) on HEAD and 
TORSO. On TAIL, they are statistically indistin-
guishable. 
We turn now to inspecting the scenario where 
features are only drawn from the source document 
(Rows 1-8 in Table 1). Again we observe that se-
mantic features significantly boost the perfor-
mance against NSF alone, however they signifi-
cantly deteriorate when compared to using fea-
tures from both source and target documents. In 
this scenario, the manual semantics from WCAT 
outperform all other models, but with a diminish-
ing effect as we move from HEAD through 
TORSO to TAIL. DSSM is the best performing 
learned semantic model. 
Finally, we present the results to justify the two 
modifications we made to extend the model of 
Huang et al. (2013) to the DSSM, as described in 
Section 1. First, we see in Table 1 that 
DSSM_BOW, which has the same network struc-
ture of Huang et al.?s model, is much weaker than 
DSSM, demonstrating the benefits of using con-
volutional and max-pooling layers to extract se-
mantic features for the highlighting task. Second, 
we conduct several experiments by using the co-
sine scores between the output layers of DSSM 
for ? and ? as features (following the procedure in 
Section 3.3 for using the DSSM as a direct imple-
mentation of ?). We found that adding the cosine 
8
features to NSF+DSSM does not lead to any im-
provement. We also combined NSF with solely 
the cosine features from DSSM (i.e., without the 
other semantic features drawn from its output lay-
ers). But we still found no improvement over us-
ing NSF alone. Thus, we conclude that for this 
task it is much more effective to feed the features 
derived from DSSM to a supervised ranker than 
directly computing the interestingness score using 
cosine similarity in the learned semantic space, as 
in Huang et al. (2013). 
5 Experiments on Entity Search 
We construct the evaluation data set for this sec-
ond task by randomly sampling a set of documents 
from a traffic-weighted set of Web documents. In 
a second step, we identify the entity names in each 
document using an in-house named entity recog-
nizer. We issue each entity name as a query to a 
commercial search engine, and retain up to the 
top-100 retrieved documents as candidate target 
documents. We form for each entity a source doc-
ument which consists of the entity text and its sur-
rounding text defined by a 200-word window. We 
define the focus (as in Figure 1) in ? as the entity 
text, and the focus in ? as the first 10 tokens in ?. 
The final evaluation data set contains 10,000 
source documents. On average, each source docu-
ment is associated with 87 target documents. Fi-
nally, the source-target document pairs are labeled 
in terms of interestingness by paid annotators. The 
label is on a 5-level scale, 0 to 4, with 4 meaning 
the target document is the most interesting to the 
source document and 0 meaning the target is of no 
interest. 
We test our models on two scenarios. The first 
is a ranking scenario where ?  interesting docu-
ments are displayed to the user. Here, we select 
the top-? ranked documents according to their in-
terestingness scores. We measure the performance 
via NDCG at truncation levels 1 and 3. The sec-
ond scenario is to display to the user all interesting 
results. In this scenario, we select all target docu-
ments with an interestingness score exceeding a 
predefined threshold. We evaluate this scenario 
using ROC analysis and, specifically, the area un-
der the curve (AUC). 
5.1 Main Results 
The main results are summarized in Table 2. Rows 
1 to 6 are single model results, where each model 
is used as a direct implementation of the interest-
ingness function ?. Rows 7 to 9 are ranker results, 
where ? is defined as a boosted tree based ranker 
that incorporates different sets of features ex-
tracted from source and target documents, includ-
ing the features derived from single models. As in 
the highlighting experiments, all the machine-
learned single models, including the DSSM, are 
trained on TRAIN_1, and all the rankers are 
trained on TRAIN_2. 
5.2 Analysis of Results 
BM25 (Rows 1 and 2 in Table 2) is the classic 
document model (Robertson and Zaragoza 2009). 
It uses the bag-of-words document representation 
and the BM25 term weighting function. In our set-
ting, we define the interestingness score of a doc-
ument pair as the dot product of their BM25-
weighted term vectors. To verify the importance 
of using contextual information, we compare two 
different ways of forming the term vector of a 
source document. The first only uses the entity 
text (Row 1). The second (Row 2) uses both the 
entity text and and its surrounding text in a 200-
word window (i.e., the entire source document). 
Results show that the model using contextual in-
formation is significantly better. Therefore, all the 
other models in this section use both the entity 
texts and their surrounding text. 
WTM (Row 3) is our implementation of the 
word translation model for IR (Berger and Laf-
ferty 1999; Gao et al. 2010). WTM defines the in-
terestingness score as: 
???, ?? ? ? ? ????|???????|?????????? ,  
# Models @1 @3 AUC 
1 BM25 (entity)  0.133 0.195 0.583 
2 BM25 0.142 0.227 0.675 
3 WTM 0.191 0.287 0.678 
4 BLTM 0.214 0.306 0.704 
5 DSSM 0.259* 0.356* 0.711* 
6 DSSM_BOW 0.223 0.322 0.699 
7 Baseline ranker 0.283 0.360 0.723 
8 7 + DSSM(1) 0.301# 0.385# 0.758# 
9 7 + DSSM(600) 0.327## 0.402## 0.782##
Table 2: Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test (? ?
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. 
 
 
 
9
where ????|?? is the unigram probability of word 
?? in ?, and ????|??? is the probability of trans-
lating ?? into ??, trained on source-target docu-ment pairs using EM (Brown et al. 1993). The 
translation-based approach allows any pair of 
non-identical but semantically related words to 
have a nonzero matching score. As a result, it sig-
nificantly outperforms BM25. 
BTLM (Row 4) follows the best performing 
bilingual topic model described in Gao et al. 
(2011), which is an extension of PLSA (Hofmann 
1999). The model is trained on source-target doc-
ument pairs using the EM algorithm with a con-
straint enforcing a source document ? and its tar-
get document ? to not only share the same prior 
topic distribution, but to also have similar frac-
tions of words assigned to each topic. BLTM de-
fines the interestingness score between s and t as: 
???, ?? ? ? ? ????|??????|???????? .  
The model assumes the following story of gener-
ating ? from ?. First, for each topic ? a word dis-
tribution ?? is selected from a Dirichlet prior with 
concentration parameter ? . Second, given ? , a 
topic distribution ??  is drawn from a Dirichlet 
prior with parameter ? . Finally, ?  is generated 
word by word. Each word ?? is generated by first 
selecting a topic ?  according to ?? , and then 
drawing a word from ?? . We see that BLTM models interestingness by taking into account the 
semantic topic distribution of the entire docu-
ments. Our results in Table 2 show that BLTM 
outperforms WTM by a significant margin in 
both NDCG and AUC. 
DSSM (Row 5) outperforms all the competing 
single models, including the state-of-the-art topic 
model BLTM. Now, we inspect the difference be-
tween DSSM and BLTM in detail. Although both 
models strive to generate the semantic representa-
tion of a document, they use different modeling 
approaches. BLTM by nature is a generative 
model. The semantic representation in BLTM is a 
distribution of hidden semantic topics. Such a dis-
tribution is learned using Maximum Likelihood 
Estimation in an unsupervised manner, i.e., max-
imizing the log-likelihood of the source-target 
document pairs in the training data. On the other 
hand, DSSM represents documents as points in a 
hidden semantic space using a supervised learning 
method, i.e., paired documents are closer in that 
latent space than unpaired ones. We believe that 
the superior performance of DSSM is largely due 
to the fact that the model parameters are discrimi-
natively trained using an objective that is tailored 
to the interestingness task.  
In addition to the difference in training meth-
ods, DSSM and BLTM also use different model 
structures. BLTM treats a document as a bag of 
words (thus losing some important contextual in-
formation such as word order and inter-word de-
pendencies), and generates semantic representa-
tions of documents using linear projection. 
DSSM, on the other hand, treats text as a sequence 
of words and better captures local and global con-
text, and generates highly non-linear semantic 
features via a deep neural network. To further ver-
ify our analysis, we inspect the results of a variant 
of DSSM, denoted as DSSM_BOW (Row 6), 
where the convolution and max-pooling layers are 
removed. This model treats a document as a bag 
of words, just like BLTM. These results demon-
strate that the effectiveness of DSSM can also be 
attributed to the convolutional architecture in the 
neural network, in addition to being deep and be-
ing discriminative. 
We turn now to discussing the ranker results in 
Rows 7 to 9. The baseline ranker (Row 7) uses 158 
features, including many counts and single model 
scores, such as BM25 and WMT. DSSM (Row 5) 
alone is quite effective, being close in perfor-
mance to the baseline ranker with non-DSSM fea-
tures. Integrating the DSSM score computed in (5) 
as one single feature into the ranker (Row 8) leads 
to a significant improvement over the baseline. 
The best performing combination (Row 9) is ob-
tained by incorporating the DSSM feature vectors 
of source and target documents (i.e., 600 features 
in total) in the ranker. 
We thus conclude that on both tasks, automatic 
highlighting and contextual entity search, features 
drawn from the output layers of our deep semantic 
model result in significant gains after being added 
to a set of non-semantic features, and in compari-
son to other types of semantic models used in the 
past. 
6 Related Work 
In addition to the notion of relevance as described 
in Section 1, related to interestingness is also the 
notion of salience (also called aboutness) (Gamon 
et al. 2013; 2014; Parajpe 2009; Yih et al. 2006). 
Salience is the centrality of a term to the content 
of a document. Although salience and interesting-
ness interact, the two are not the same. For exam-
ple, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average 
user would probably not be interested in learning 
more about Obama while reading that article.  
10
There are many systems that identify popular 
content in the Web or recommend content (e.g., 
Bandari et al. 2012; Lerman and Hogg 2010; 
Szabo and Huberman 2010), which is closely re-
lated to the highlighting task. In contrast to these 
approaches, we strive to predict what term a user 
is likely to be interested in when reading content, 
which may or may not be the same as the most 
popular content that is related to the current docu-
ment. It has empirically been demonstrated in 
Gamon et al. (2013) that popularity is in fact a ra-
ther poor predictor for interestingness. The task of 
contextual entity search, which is formulated as an 
information retrieval problem in this paper, is also 
related to research on entity resolution (Stefanidis 
et al. 2013).  
Latent Semantic Analysis (Deerwester et al. 
1990) is arguably the earliest semantic model de-
signed for IR. Generative topic models widely 
used for IR include PLSA (Hofmann 1990) and 
LDA (Blei et al. 2003). Recently, these models 
have been extended to handle cross-lingual cases, 
where there are pairs of corresponding documents 
in different languages (e.g., Dumais et al. 1997; 
Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). 
By exploiting deep architectures, deep learning 
techniques are able to automatically discover from 
training data the hidden structures and the associ-
ated features at different levels of abstraction use-
ful for a variety of tasks (e.g., Collobert et al. 
2011; Hinton et al. 2012; Socher et al. 2012; 
Krizhevsky et al., 2012; Gao et al. 2014). Hinton 
and Salakhutdinov (2010) propose the most origi-
nal approach based on an unsupervised version of 
the deep neural network to discover the hierar-
chical semantic structure embedded in queries and 
documents. Huang et al. (2013) significantly ex-
tends the approach so that the deep neural network 
can be trained on large-scale query-document 
pairs giving much better performance. The use of 
the convolutional neural network for text pro-
cessing, central to our DSSM, was also described 
in Collobert et al. (2011) and Shen et al. (2014) 
but with very different applications. The DSSM 
described in Section 3 can be viewed as a variant 
of the deep neural network models used in these 
previous studies. 
7 Conclusions 
Modeling interestingness is fundamental to many 
online recommendation systems. We obtain natu-
rally occurring interest signals by observing Web 
browsing transitions where users click from one 
webpage to another. We propose to model this 
?interestingness? with a deep semantic similarity 
model (DSSM), based on deep neural networks 
with special convolutional-pooling structure, 
mapping source-target document pairs to feature 
vectors in a latent semantic space. We train the 
DSSM using browsing transitions between docu-
ments. Finally, we demonstrate the effectiveness 
of our model on two interestingness tasks: auto-
matic highlighting and contextual entity search. 
Our results on large-scale, real-world datasets 
show that the semantics of documents computed 
by the DSSM are important for modeling interest-
ingness and that the new model leads to signifi-
cant improvements on both tasks. DSSM is shown 
to outperform not only the classic document mod-
els that do not use (latent) semantics but also state-
of-the-art topic models that do not have the deep 
and convolutional architecture characterizing the 
DSSM. 
One area of future work is to extend our 
method to model interestingness given an entire 
user session, which consists of a sequence of 
browsing events. We believe that the prior brows-
ing and interaction history recorded in the session 
provides additional signals for predicting interest-
ingness. To capture such signals, our model needs 
to be extended to adequately represent time series 
(e.g., causal relations and consequences of ac-
tions). One potentially effective model for such a 
purpose is based on the architecture of recurrent 
neural networks (e.g., Mikolov et al. 2010; Chen 
and Deng, 2014), which can be incorporated into 
the deep semantic model proposed in this paper. 
Additional Authors 
Yelong Shen (Microsoft Research, One Microsoft 
Way, Redmond, WA 98052, USA, email: 
yeshen@microsoft.com). 
Acknowledgments 
The authors thank Johnson Apacible, Pradeep 
Chilakamarri, Edward Guo, Bernhard Kohlmeier, 
Xiaolong Li, Kevin Powell, Xinying Song and 
Ye-Yi Wang for their guidance and valuable dis-
cussions. We also thank the three anonymous re-
viewers for their comments. 
References 
Bandari, R., Asur, S., and Huberman, B. A. 2012. 
The pulse of news in social media: forecasting 
popularity. In ICWSM. 
11
Bengio, Y., 2009. Learning deep architectures for 
AI. Fundamental Trends in Machine Learning, 
2(1):1?127. 
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet allocation. Journal of Machine 
Learning Research, 3. 
Broder, A., Fontoura, M., Josifovski, V., and 
Riedel, L. 2007. A semantic approach to contex-
tual advertising. In SIGIR. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. 
J., and Mercer, R. L. 1993. The mathematics of 
statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263-
311. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML, pp. 89-96.  
Chen, J. and Deng, L. 2014. A primal-dual method 
for training recurrent neural networks con-
strained by the echo-state property. In ICLR. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P., 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 12. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Indexing 
by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
Deng, L., Hinton, G., and Kingsbury, B. 2013. 
New types of deep neural network learning for 
speech recognition and related applications: An 
overview. In ICASSP. 
Deng, L., Abdel-Hamid, O., and Yu, D., 2013a. A 
deep convolutional neural network using heter-
ogeneous pooling for trading acoustic invari-
ance with phonetic confusion. In ICASSP. 
Dumais, S. T., Letsche, T. A., Littman, M. L., and 
Landauer, T. K. 1997. Automatic cross-linguis-
tic information retrieval using latent semantic 
indexing. In AAAI-97 Spring Symposium Series: 
Cross-Language Text and Speech Retrieval. 
Friedman, J. H. 1999. Greedy function approxi-
mation: a gradient boosting machine. Annals of 
Statistics, 29:1189-1232. 
Gamon, M., Mukherjee, A., Pantel, P. 2014. Pre-
dicting interesting things in text. In COLING. 
Gamon, M., Yano, T., Song, X., Apacible, J. and 
Pantel, P. 2013. Identifying salient entities in 
web pages. In CIKM. 
Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM. pp. 
1139-1148. 
Gao, J., He, X., Yih, W-t., and Deng, L. 2014. 
Learning continuous phrase representations for 
translation modeling. In ACL. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR. pp. 675-684.  
Graves, A., Mohamed, A., and Hinton, G. 2013. 
Speech recognition with deep recurrent neural 
networks. In ICASSP. 
Gutmann, M. and Hyvarinen, A. 2010. Noise-con-
trastive estimation: a new estimation principle 
for unnormalized statistical models. In Proc. 
Int. Conf. on Artificial Intelligence and Statis-
tics (AISTATS2010). 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, 29:82-97. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering binary codes for documents by learning 
deep generative models. Topics in Cognitive 
Science, pp. 1-18. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR. pp. 50-57. 
Huang, P., He, X., Gao, J., Deng, L., Acero, A., 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM. 
Jarvelin, K. and Kekalainen, J. 2000. IR evalua-
tion methods for retrieving highly relevant doc-
uments. In SIGIR. pp. 41-48. 
Krizhevsky, A., Sutskever, I. and Hinton, G. 
2012. ImageNet classification with deep convo-
lutional neural networks. In NIPS. 
Lerman, K., and Hogg, T. 2010. Using a model of 
social dynamics to predict popularity of news. 
In WWW. pp. 621-630. 
Markoff, J. 2014. Computer eyesight gets a lot 
more accurate. In New York Times. 
Mikolov, T.. Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In 
INTERSPEECH. pp. 1045-1048. 
Paranjpe, D. 2009. Learning document aboutness 
from implicit user feedback and document 
structure. In CIKM. 
12
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual document representations from 
discriminative projections. In EMNLP. pp. 251-
261. 
Ricci, F., Rokach, L., Shapira, B., and Kantor, P. 
B. (eds) 2011. Recommender System Handbook, 
Springer. 
Robertson, S., and Zaragoza, H. 2009. The proba-
bilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information 
Retrieval, 3(4):333-389. 
Shen, Y., He, X., Gao. J., Deng, L., and Mesnil, G. 
2014. A latent semantic model with convolu-
tional-pooling structure for information re-
trieval. In CIKM. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic compositionality through recursive 
matrix-vector spaces. In EMNLP. 
Stefanidis, K., Efthymiou, V., Herschel, M., and 
Christophides, V. 2013. Entity resolution in the 
web of data.  CIKM?13 Tutorial. 
Szabo, G., and Huberman, B. A. 2010. Predicting 
the popularity of online content. Communica-
tions of the ACM, 53(8). 
Wu, Q., Burges, C.J.C., Svore, K., and Gao, J. 
2009. Adapting boosting for information re-
trieval measures. Journal of Information Re-
trieval, 13(3):254-270. 
Yih, W., Goodman, J., and Carvalho, V. R. 2006. 
Finding advertising keywords on web pages. In 
WWW. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
 
13
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 602?606,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Responses to Microblog Posts
Yoav Artzi ?
Computer Science & Engineering
University of Washington
Seattle, WA, USA
yoav@cs.washington.edu
Patrick Pantel, Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA, USA
{ppantel,mgamon}@microsoft.com
Abstract
Microblogging networks serve as vehicles for
reaching and influencing users. Predicting
whether a message will elicit a user response
opens the possibility of maximizing the viral-
ity, reach and effectiveness of messages and
ad campaigns on these networks. We propose
a discriminative model for predicting the like-
lihood of a response or a retweet on the Twit-
ter network. The approach uses features de-
rived from various sources, such as the lan-
guage used in the tweet, the user?s social net-
work and history. The feature design process
leverages aggregate statistics over the entire
social network to balance sparsity and infor-
mativeness. We use real-world tweets to train
models and empirically show that they are ca-
pable of generating accurate predictions for a
large number of tweets.
1 Introduction
Microblogging networks are increasingly evolving
into broadcasting networks with strong social as-
pects. The most popular network today, Twitter, re-
ported routing 200 million tweets (status posts) per
day in mid-2011. As the network is increasingly
used as a channel for reaching out and marketing
to its users, content generators aim to maximize the
impact of their messages, an inherently challeng-
ing task. However, unlike for conventionally pro-
duced news, Twitter?s public network allows one to
observe how messages are reaching and influencing
users. One such direct measure of impact are mes-
sage responses.
? This work was conducted at Microsoft Research.
In this work, we describe methods to predict if a
given tweet will elicit a response. Twitter provides
two methods to respond to messages: replies and
retweets (re-posting of a message to one?s follow-
ers). Responses thus serve both as a measure of dis-
tribution and as a way to increase it. Being able to
predict responses is valuable for any content gener-
ator, including advertisers and celebrities, who use
Twitter to increase their exposure and maintain their
brand. Furthermore, this prediction ability can be
used for ranking, allowing the creation of better op-
timized news feeds.
To predict if a tweet will receive a response prior
to its posting we use features of the individual tweet
together with features aggregated over the entire so-
cial network. These features, in combination with
historical activity, are used to train a prediction
model.
2 Related Work
The public nature of Twitter and the unique char-
acteristics of its content have made it an attractive
research topic over recent years. Related work can
be divided into several types:
Twitter Demographics One of the most fertile av-
enues of research is modeling users and their inter-
actions on Twitter. An extensive line of work char-
acterizes users (Pear Analytics, 2009) and quantifies
user influence (Cha et al, 2010; Romero et al, 2011;
Wu et al, 2011; Bakshy et al, 2011). Popescu and
Jain (2011) explored how businesses use Twitter to
connect with their customer base. Popescu and Pen-
nacchiotti (2011) and Qu et al (2011) investigated
602
how users react to events on social media. There
also has been extensive work on modeling conver-
sational interactions on Twitter (Honeycutt and Her-
ring, 2009; Boyd et al, 2010; Ritter et al, 2010;
Danescu-Niculescu-Mizil et al, 2011). Our work
builds on these findings to predict response behavior
on a large scale.
Mining Twitter Social media has been used to de-
tect events (Sakaki et al, 2010; Popescu and Pennac-
chiotti, 2010; Popescu et al, 2011), and even predict
their outcomes (Asur and Huberman, 2010; Culotta,
2010). Similarly to this line of work, we mine the
social network for event prediction. In contrast, our
focus is on predicting events within the network.
Response Prediction There has been significant
work addressing the task of response prediction in
news articles (Tsagkias et al, 2009; Tsagkias et al,
2010) and blogs (Yano et al, 2009; Yano and Smith,
2010; Balasubramanyan et al, 2011). The task of
predicting responses in social networks has been in-
vestigated previously: Hong et al (2011) focused
on predicting responses for highly popular items,
Rowe et al (2011) targeted the prediction of con-
versations and their length and Suh et al (2010) pre-
dicted retweets. In contrast, our work targets tweets
regardless of their popularity and attempts to predict
both replies and retweets. Furthermore, we present
a scalable method to use linguistic lexical features in
discriminative models by leveraging global network
statistics. A related task to ours is that of response
generation, as explored by Ritter et al (2011). Our
work complements their approach by allowing to
detect when the generation of a response is appro-
priate. Lastly, the task of predicting the spread of
hashtags in microblogging networks (Tsur and Rap-
poport, 2012) is also closely related to our work and
both approaches supplement each other as measures
of impact.
Ranking in News Feeds Different approaches
were suggested for ranking items in social media
(Das Sarma et al, 2010; Lakkaraju et al, 2011). Our
work provides an important signal, which can be in-
corporated into any ranking approach.
3 Response Prediction on Twitter
Our goal is to learn a function f that maps a tweet
x to a binary value y ? {0, 1}, where y indicates if
x will receive a response. In this work we make no
distinction between different kinds of responses.
In addition to x, we assume access to a social net-
work S, which we view as a directed graph ?U,E?.
The set of vertices U represents the set of users. For
each u?, u?? ? U , ?u?, u??? ? E if and only if there
exists a following relationship from u? to u??.
For the purpose of defining features we denote xt
as the text of the tweet x and xu ? U the user who
posted x. For training we assume access to a set of
n labeled examples {?xi, yi? : i = 1 . . . n}, where
the label indicates whether the tweet has received a
response or not.
3.1 Features
For prediction we represent a given tweet x using six
feature families:
Historical Features Historical behavior is often
strong evidence of future trends. To account for this
information, we compute the following features: ra-
tio of tweets by xu that received a reply, ratio of
tweets by xu that were retweeted and ratio of tweets
by xu that received both a reply and retweet.
Social Features The immediate audience of a user
xu is his followers. Therefore, incorporating social
features into our model is likely to contribute to its
prediction ability. For a user xu ? U we include
features for the number of followers (indegree in S),
the number of users xu follows (outdegree in S) and
the ratio between the two.
Aggregate Lexical Features To detect lexical
items that trigger certain response behavior we de-
fine features for all bigrams and hashtags in our set
of tweets. To avoid sparsity and maintain a manage-
able feature space we compress the features using
the labels: for each lexical item l we define Rl to
be the set of tweets that include l and received a re-
sponse, and Nl to be the set of tweets that contain l
and received no response. We then define the inte-
ger n to be the rounding of |Rl||Nl| to the nearest integer.
For each such integer we define a feature, which we
increase by 1 when the lexical item l is present in xt.
603
We use this process separately for bigrams and hash-
tags, creating separate sets of aggregate features.
Local Content Features We introduce 45 features
to capture how the content of xt influences response
behavior, including features such as the number of
stop words and the percentage of English words. In
addition we include features specific to Twitter, such
as the number of hash tags and user references.
Posting Features Past analysis of Twitter showed
that posting time influences response potential (Pear
Analytics, 2009). To examine temporal influences,
we include features to account for the user?s local
time and day of the week when x was created.
Sentiment Features To measure how sentiment
influences response behavior we define features that
count the number of positive and negative sentiment
words in xt. To detect sentiment words we use a pro-
prietary Microsoft lexicon of 7K positive and nega-
tive terms.
4 Evaluation
4.1 Learning Algorithm
We experimented with two different learning al-
gorithms: Multiple Additive Regression-Trees
(MART) (Wu et al, 2008) and a maximum entropy
classifier (Berger et al, 1996). Both provide fast
classification, a natural requirement for large-scale
real-time tasks.
4.2 Dataset
In our evaluation we focus on English tweets only.
Since we use local posting time in our features, we
filtered users whose profile did not contain location
information. To collect Tweeter messages we used
the entire public feed of Twitter (often referred to as
the Twitter Firehose). We randomly sampled 943K
tweets from one week of data. We allowed an ex-
tra week for responses, giving a response window
of two weeks. The majority of tweets in our set
(90%) received no response. We used 750K tweets
for training and 188K for evaluation. A separate data
set served as a development set. For the computation
of aggregate lexical features we used 186M tweets
from the same week, resulting in 14M bigrams and
400K hash tags. To compute historical features, we
sampled 2B tweets from the previous three months.
Figure 1: Precision-recall curves for predicting that a
tweet will get a response. The marked area highlights
the area of the curve we focus on in our evaluation.
Figure 2: Precision-recall curves with increasing number
of features removed for the marked area in Figure 1. For
each curve we removed one additional feature set from
the one above it.
4.3 Results
Our evaluation focuses on precision-recall curves
for predicting that a given tweet will get a response.
The curves were generated by varying the confi-
dence measure threshold, which both classifiers pro-
vided. As can be seen in Figure 1, MART outper-
forms the maximum entropy model. We can also see
that it is hard to predict response behavior for most
tweets, but for a large subset we can provide a rela-
tively accurate prediction (highlighted in Figure 1).
The rest of our analysis focuses on this subset and
on results based on MART.
To better understand the contribution of each fea-
ture set, we removed features in a greedy manner.
After learning a model and testing it, we removed
the feature family that was overall most highly
ranked by MART (i.e., was used in high-level splits
in the decision trees) and learned a new model. Fig-
ure 2 shows how removing feature sets degrades pre-
diction performance. Removing historical features
lowers the model?s prediction abilities, although pre-
diction quality remains relatively high. Removing
social features creates a bigger drop in performance.
Lastly, removing aggregate lexical features and lo-
604
cal content features further decreases performance.
At this point, removing posting time features is not
influential. Following the removal of posting time
features, the model includes only sentiment features.
5 Discussion and Conclusion
The first trend seen by removing features is that local
content matters less, or at least is more complex to
capture and use for response prediction. Despite the
influence of chronological trends on posting behav-
ior on Twitter (Pear Analytics, 2009), we were un-
able to show influence of posting time on response
prediction. Historical features were the most promi-
nent in our experiments. Second were social fea-
tures, showing that developing one?s network is crit-
ical for impact. The third most prominent set of fea-
tures, aggregate lexical features, shows that users are
sensitive to certain expressions and terms that tend
to trigger responses.
The natural path for future work is to improve per-
formance using new features. These may include
clique-specific language features, more properties of
the user?s social network, mentions of named enti-
ties and topics of tweets. Another direction is to dis-
tinguish between replies and retweets and to predict
the number of responses and the length of conversa-
tions that a tweet may generate. There is also po-
tential in learning models for the prediction of other
measures of impact, such as hashtag adoption and
inclusion in ?favorites? lists.
Acknowledgments
We would like to thank Alan Ritter, Bill Dolan,
Chris Brocket and Luke Zettlemoyer for their sug-
gestions and comments. We wish to thank Chris
Quirk and Qiang Wu for providing us with access
to their learning software. Thanks to the reviewers
for the helpful comments.
References
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. In Proceedings of the International
Conference on Web Intelligence and Intelligent Agent
Technology.
E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.
2011. Everyone?s an influencer: quantifying influence
on twitter. In Peoceedings of the ACM International
Conference on Web Search and Data Mining.
R. Balasubramanyan, W.W. Cohen, D. Pierce, and D.P.
Redlawsk. 2011. What pushes their buttons? predict-
ing comment polarity from the content of political blog
posts. In Proceedings of the Workshop on Language in
Social Media.
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics.
D. Boyd, S. Golder, and G. Lotan. 2010. Tweet, tweet,
retweet: Conversational aspects of retweeting on twit-
ter. In Proceedings of the International Conference on
System Sciences.
M. Cha, H. Haddadi, F. Benevenuto, and K.P. Gummadi.
2010. Measuring user influence in twitter: The million
follower fallacy. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
A. Culotta. 2010. Towards detecting influenza epidemics
by analyzing twitter messages. In Proceedings of the
Workshop on Social Media Analytics.
C. Danescu-Niculescu-Mizil, M. Gamon, and S. Dumais.
2011. Mark my words!: linguistic style accommoda-
tion in social media. In Proceedings of the Interna-
tional Conference on World Wide Web.
A. Das Sarma, A. Das Sarma, S. Gollapudi, and R. Pan-
igrahy. 2010. Ranking mechanisms in twitter-like fo-
rums. In Proceedings of the ACM International Con-
ference on Web Search and Data Mining.
C. Honeycutt and S.C. Herring. 2009. Beyond mi-
croblogging: Conversation and collaboration via twit-
ter. In Proceedings of the International Conference on
System Sciences.
L. Hong, O. Dan, and B. D. Davison. 2011. Predict-
ing popular messages in twitter. In Proceedings of the
International Conference on World Wide Web.
H. Lakkaraju, A. Rai, and S. Merugu. 2011. Smart
news feeds for social networks using scalable joint la-
tent factor models. In Proceedings of the International
Conference on World Wide Web.
Pear Analytics. 2009. Twitter study.
A.M. Popescu and A. Jain. 2011. Understanding the
functions of business accounts on twitter. In Proceed-
ings of the International Conference on World Wide
Web.
A.M. Popescu and M. Pennacchiotti. 2010. Detect-
ing controversial events from twitter. In Proceedings
of the International Conference on Information and
Knowledge Management.
A.M. Popescu and M. Pennacchiotti. 2011. Dancing
with the stars, nba games, politics: An exploration of
twitter users response to events. In Proceedings of the
605
International AAAI Conference on Weblogs and Social
Media.
A.M. Popescu, M. Pennacchiotti, and D. Paranjpe. 2011.
Extracting events and event descriptions from twit-
ter. In Proceedings of the International Conference
on World Wide Web.
Y. Qu, C. Huang, P. Zhang, and J. Zhang. 2011. Mi-
croblogging after a major disaster in china: a case
study of the 2010 yushu earthquake. In Proceedings
of the ACM Conference on Computer Supported Co-
operative Work.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised
modeling of twitter conversations. In Proceedings of
the Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Ritter, C. Cherry, and B. Dolan. 2011. Data-driven
response generation in social media. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
D. Romero, W. Galuba, S. Asur, and B. Huberman. 2011.
Influence and passivity in social media. Machine
Learning and Knowledge Discovery in Databases,
pages 18?33.
M. Rowe, S. Angeletou, and H. Alani. 2011. Predicting
discussions on the social semantic web. In Proceed-
ings of the Extended Semantic Web Conference.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors. In Proceedings of the International
Conference on World Wide Web.
B. Suh, L. Hong, P. Pirolli, and E. H. Chi. 2010. Want to
be retweeted? large scale analytics on factors impact-
ing retweet in twitter network. In Proceedings of the
IEEE International Conference on Social Computing.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2009.
Predicting the volume of comments on online news
stories. In Proceedings of the ACM Conference on In-
formation and Knowledge Management.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2010.
News comments: Exploring, modeling, and online
prediction. Advances in Information Retrieval, pages
191?203.
O. Tsur and A. Rappoport. 2012. What?s in a hash-
tag?: content based prediction of the spread of ideas
in microblogging communities. In Proceedings of the
ACM International Conference on Web Search and
Data Mining.
Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2008.
Ranking, boosting, and model adaptation. Tecnical
Report, MSR-TR-2008-109.
S. Wu, J.M. Hofman, W.A. Mason, and D.J. Watts. 2011.
Who says what to whom on twitter. In Proceedings of
the International Conference on World Wide Web.
T. Yano and N.A. Smith. 2010. Whats worthy of com-
ment? content and comment volume in political blogs.
Proceedings of the International AAAI Conference on
Weblogs and Social Media.
T. Yano, W.W. Cohen, and N.A. Smith. 2009. Predict-
ing response to political blog posts with topic mod-
els. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
606
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 83?92,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Jigs and Lures: Associating Web Queries with Structured Entities
Patrick Pantel
Microsoft Research
Redmond, WA, USA
ppantel@microsoft.com
Ariel Fuxman
Microsoft Research
Mountain View, CA, USA
arielf@microsoft.com
Abstract
We propose methods for estimating the prob-
ability that an entity from an entity database
is associated with a web search query. Asso-
ciation is modeled using a query entity click
graph, blending general query click logs with
vertical query click logs. Smoothing tech-
niques are proposed to address the inherent
data sparsity in such graphs, including inter-
polation using a query synonymy model. A
large-scale empirical analysis of the smooth-
ing techniques, over a 2-year click graph
collected from a commercial search engine,
shows significant reductions in modeling er-
ror. The association models are then applied
to the task of recommending products to web
queries, by annotating queries with products
from a large catalog and then mining query-
product associations through web search ses-
sion analysis. Experimental analysis shows
that our smoothing techniques improve cover-
age while keeping precision stable, and over-
all, that our top-performing model affects 9%
of general web queries with 94% precision.
1 Introduction
Commercial search engines use query associations
in a variety of ways, including the recommendation
of related queries in Bing, ?something different? in
Google, and ?also try? and related concepts in Ya-
hoo. Mining techniques to extract such query asso-
ciations generally fall into four categories: (a) clus-
tering queries by their co-clicked url patterns (Wen
et al, 2001; Baeza-Yates et al, 2004); (b) leveraging
co-occurrences of sequential queries in web search
query sessions (Zhang and Nasraoui, 2006; Boldi et
al., 2009); (c) pattern-based extraction over lexico-
syntactic structures of individual queries (Pas?ca and
Durme, 2008; Jain and Pantel, 2009); and (d) distri-
butional similarity techniques over news or web cor-
pora (Agirre et al, 2009; Pantel et al, 2009). These
techniques operate at the surface level, associating
one surface context (e.g., queries) to another.
In this paper, we focus instead on associating sur-
face contexts with entities that refer to a particu-
lar entry in a knowledge base such as Freebase,
IMDB, Amazon?s product catalog, or The Library
of Congress. Whereas the former models might as-
sociate the string ?Ronaldinho? with the strings ?AC
Milan? or ?Lionel Messi?, our goal is to associate
?Ronaldinho? with, for example, the Wikipedia en-
tity page ?wiki/AC Milan? or the Freebase entity
?en/lionel mess?. Or for the query string ?ice fish-
ing?, we aim to recommend products in a commer-
cial catalog, such as jigs or lures.
The benefits and potential applications are large.
By knowing the entity identifiers associated with a
query (instead of strings), one can greatly improve
both the presentation of search results as well as
the click-through experience. For example, consider
when the associated entity is a product. Not only
can we present the product name to the web user,
but we can also display the image, price, and re-
views associated with the entity identifier. Once the
entity is clicked, instead of issuing a simple web
search query, we can now directly show a product
page for the exact product; or we can even perform
actions directly on the entity, such as buying the en-
tity on Amazon.com, retrieving the product?s oper-
83
ating manual, or even polling your social network
for friends that own the product. This is a big step
towards a richer semantic search experience.
In this paper, we define the association between a
query string q and an entity id e as the probability
that e is relevant given the query q, P (e|q). Fol-
lowing Baeza-Yates et al (2004), we model rele-
vance as the likelihood that a user would click on
e given q, events which can be observed in large
query-click graphs. Due to the extreme sparsity
of query click graphs (Baeza-Yates, 2004), we pro-
pose several smoothing models that extend the click
graph with query synonyms and then use the syn-
onym click probabilities as a background model.
We demonstrate the effectiveness of our smoothing
models, via a large-scale empirical study over real-
world data, which significantly reduce model errors.
We further apply our models to the task of query-
product recommendation. Queries in session logs
are annotated using our association probabilities and
recommendations are obtained by modeling session-
level query-product co-occurrences in the annotated
sessions. Finally, we demonstrate that our models
affect 9% of general web queries with 94% recom-
mendation precision.
2 Related Work
We introduce a novel application of significant com-
mercial value: entity recommendations for general
Web queries. This is different from the vast body
of work on query suggestions (Baeza-Yates et al,
2004; Fuxman et al, 2008; Mei et al, 2008b; Zhang
and Nasraoui, 2006; Craswell and Szummer, 2007;
Jagabathula et al, 2011), because our suggestions
are actual entities (as opposed to queries or docu-
ments). There is also a rich literature on recom-
mendation systems (Sarwar et al, 2001), including
successful commercial systems such as the Ama-
zon product recommendation system (Linden et al,
2003) and the Netflix movie recommendation sys-
tem (Bell et al, 2007). However, these are entity-
to-entity recommendations systems. For example,
Netflix recommends movies based on previously
seen movies (i.e., entities). Furthermore, these sys-
tems have access to previous transactions (i.e., ac-
tual movie rentals or product purchases), whereas
our recommendation system leverages a different re-
source, namely query sessions.
In principle, one could consider vertical search
engines (Nie et al, 2007) as a mechanism for as-
sociating queries to entities. For example, if we type
the query ?canon eos digital camera? on a commerce
search engine such as Bing Shopping or Google
Products, we get a listing of digital camera entities
that satisfy our query. However, vertical search en-
gines are essentially rankers that given a query, re-
turn a sorted list of (pointers to) entities that are re-
lated to the query. That is, they do not expose actual
association scores, which is a key contribution of our
work, nor do they operate on general search queries.
Our smoothing methods for estimating associ-
ation probabilities are related to techniques de-
veloped by the NLP and speech communities to
smooth n-gram probabilities in language model-
ing. The simplest are discounting methods, such
as additive smoothing (Lidstone, 1920) and Good-
Turing (Good, 1953). Other methods leverage
lower-order background models for low-frequency
events, such as Katz? backoff smoothing (Katz,
1987), Witten-Bell discounting (Witten and Bell,
1991), Jelinek-Mercer interpolation (Jelinek and
Mercer, 1980), and Kneser-Ney (Kneser and Ney,
1995).
In the information retrieval community, Ponte and
Croft (1998) are credited for accelerating the use
of language models. Initial proposals were based
on learning global smoothing models, where the
smoothing of a word would be independent of the
document that the word belongs to (Zhai and Laf-
ferty, 2001). More recently, a number of local
smoothing models have been proposed (Liu and
Croft, 2004; Kurland and Lee, 2004; Tao et al,
2006). Unlike global models, local models leverage
relationships between documents in a corpus. In par-
ticular, they rely on a graph structure that represents
document similarity. Intuitively, the smoothing of a
word in a document is influenced by the smoothing
of the word in similar documents. For a complete
survey of these methods and a general optimization
framework that encompasses all previous proposals,
please see the work of Mei, Zhang et al (2008a).
All the work on local smoothing models has been
applied to the prediction of priors for words in docu-
ments. To the best of our knowledge, we are the first
to establish that query-click graphs can be used to
84
create accurate models of query-entity associations.
3 Association Model
Task Definition: Consider a collection of entities
E. Given a search query q, our task is to compute
P (e|q), the probability that an entity e is relevant to
q, for all e ? E.
We limit our model to sets of entities that can
be accessed through urls on the web, such as Ama-
zon.com products, IMDB movies, Wikipedia enti-
ties, and Yelp points of interest.
Following Baeza-Yates et al (2004), we model
relevance as the click probability of an entity given
a query, which we can observe from click logs of
vertical search engines, i.e., domain-specific search
engines such as the product search engine at Ama-
zon, the local search engine at Yelp, or the travel
search engine at Bing Travel. Clicked results in a
vertical search engine are edges between queries and
entities e in the vertical?s knowledge base. General
search query click logs, which capture direct user
intent signals, have shown significant improvements
when used for web search ranking (Agichtein et al,
2006). Unlike for general search engines, vertical
search engines have typically much less traffic re-
sulting in extremely sparse click logs.
In this section, we define a graph structure for
recording click information and we propose several
models for estimating P (e|q) using the graph.
3.1 Query Entity Click Graph
We define a query entity click graph, QEC(Q?U ?
E,Cu ? Ce), as a tripartite graph consisting of a set
of query nodes Q, url nodes U , entity nodes E, and
weighted edges Cu exclusively between nodes of Q
and nodes of U , as well as weighted edges Ce ex-
clusively between nodes of Q and nodes of E. Each
edge in Cu and Ce represents the number of clicks
observed between query-url pairs and query-entity
pairs, respectively. Let wu(q, u) be the click weight
of the edges in Cu, and we(q, e) be the click weight
of the edges in Ce.
IfCe is very large, then we can model the associa-
tion probability, P (e|q), as the maximum likelihood
estimation (MLE) of observing clicks on e given the
query q:
P?mle(e|q) =
we(q,e)?
e??E we(q,e
?) (3.1)
Figure 1 illustrates an example query entity
graph linking general web queries to entities in a
large commercial product catalog. Figure 1a illus-
trates eight queries in Q with their observed clicks
(solid lines) with products in E1. Some probabil-
ity estimates, assigned by Equation 3.1, include:
P?mle(panfish jigs, e1) = 0, P?mle(ice jigs, e1) = 1,
and P?mle(ice auger, e4) =
ce(ice auger,e4)
ce(ice auger,e3)+ce(ice auger,e4)
.
Even for the largest search engines, query click
logs are extremely sparse, and smoothing techniques
are necessary (Craswell and Szummer, 2007; Gao et
al., 2009). By considering only Ce, those clicked
urls that map to our entity collection E, the sparsity
situation is even more dire. The sparsity of the graph
comes in two forms: a) there are many queries for
which an entity is relevant that will never be seen
in the click logs (e.g., ?panfish jig? in Figure 1a);
and b) the query-click distribution is Zipfian and
most observed edges will have very low click counts
yielding unreliable statistics. In the following sub-
sections, we present a method to expand QEC with
unseen queries that are associated with entities in E.
Then we propose smoothing methods for leveraging
a background model over the expanded click graph.
Throughout our models, we make the simplifying
assumption that the knowledge base E is complete.
3.2 Graph Expansion
Following Gao et al (2009), we address the spar-
sity of edges in Ce by inferring new edges through
traversing the query-url click subgraph, UC(Q ?
U,Cu), which contains many more edges than Ce.
If two queries qi and qj are synonyms or near syn-
onyms2, then we expect their click patterns to be
similar.
We define the synonymy similarity, s(qi, qj) as
the cosine of the angle between qi and qj, the click
pattern vectors of qi and qj , respectively:
cosine(qi,qj) =
qi?qj?
qi?qi?
?
qj?qj
where q is an nu dimensional vector consisting of
the pointwise mutual information between q and
each url u in U , pmi(q, u):
1Clicks are collected from a commerce vertical search en-
gine described in Section 5.1.
2A query qi is a near synonym of a query qj if most relevant
results of qi are also relevant to qj . Section 5.2.1 describes our
adopted metric for near synonymy.
85
ice fishing
ice auger
Eskimo 
Mako 
Auger
Luretech 
Hot Hooks
Hi-Tech
Fish ?N? 
Bucket
icefishingworld.com
iceteam.com
cabelas.com
strikemaster.com
ice fishing tackle
fishusa.com
power auger
ice jigs
fishing bucket
customjigs.com
keeperlures.com
panfish jigs
d rock
Strike-
Lite II 
Auger
Luretech 
Hot Hooks
ice fishing tackle
ice jigs
panfish jigs
? ?eqwe ,? ?eqwe ,?
E
Q
U
? ?ji qqs ,
ice auger
cabelas.com
strikemaster.com
power auger
d rock
? ?uqwu ,a) b)
c)
d)
fishing
ice fishing
ice fishing minnesota
d rock
ice fishing tackle
ice fishing
t 0
t 1
t 3
t 4
t2
(e1)
(e1)
(e2)
(e3)
(e4)
Figure 1: Example QEC graph: (a) Sample queries in Q, clicks connecting queries with urls in U , and clicks to
entities in E; (b) Zoom on edges in Cu illustrating clicks observed on urls with weight wu(q, u) as well as synonymy
edges between queries with similarity score s(qi, qj) (Section 3.2); (c) Zoom on edges in Ce where solid lines indicate
observed clicks with weight we(q, e) and dotted lines indicate inferred clicks with smoothed weight w?e(q, e) (Sec-
tion 3.3); and (d) A temporal sequence of queries in a search session illustrating entity associations propagating from
the QEC graph to the queries in the session (Section 4).
pmi(q, u) = log
(
wu(q,u)?
?
q??Q,u??U wu(q
?,u?)
?
u??U wu(q,u
?)
?
q??Q wu(q
?,u)
)
(3.2)
PMI is known to be biased towards infrequent
events. We apply the discounting factor, ?(q, u),
proposed in (Pantel and Lin, 2002):
?(q,u)= wu(q,u)wu(q,u)+1 ?
min(?q??Q wu(q?,u),
?
u??U wu(q,u
?))
min(?q??Q wu(q?,u),
?
u??U wu(q,u
?))+1
Enrichment: We enrich the original QEC graph
by creating a new edge {q?,e}, where q? ? Q and e ?
E, if there exists a query q where s(q, q?) > ? and
we(q, e) > 0. ? is set experimentally, as described
in Section 5.2.
Figure 1b illustrates similarity edges created be-
tween query ?ice auger? and both ?power auger?
and ?d rock?. Since ?ice auger? was connected to
entities e3 and e4 in the original QEC, our expan-
sion model creates new edges in Ce between {power
auger, e3}, {power auger, e4}, and {d rock, e3}.
For each newly added edge {q,e}, P?mle = 0 ac-
cording to our model from Equation 3.1 since we
have never observed any clicks between q and e. In-
stead, we define a new model that uses P?mle when
clicks are observed and otherwise assigns uniform
probability mass, as:
P?hybr(e|q) =
?
?
?
P?mle(e|q) if ?e?|we(q,e?)>0
1?
e??E ?(q,e
?)
otherwise
(3.3)
where ?(q, e) is an indicator variable which is 1 if
there is an edge between {q, e} in Ce.
This model does not leverage the local synonymy
graph in order to transfer edge weight to unseen
edges. In the next section, we investigate smooth-
ing techniques for achieving this.
3.3 Smoothing
Smoothing techniques can be useful to alleviate data
sparsity problems common in statistical models. In
practice, methods that leverage a background model
(e.g., a lower-order n-gram model) have shown most
promise (Katz, 1987; Witten and Bell, 1991; Je-
linek and Mercer, 1980; Kneser and Ney, 1995). In
this section, we present two smoothing methods, de-
rived from Jelinek-Mercer interpolation (Jelinek and
Mercer, 1980), for estimating the target association
probability P (e|q).
Figure 1c highlights two edges, illustrated with
dashed lines, inserted into Ce during the graph ex-
pansion phase of Section 3.2. w?e(q, e) represents
the weight of our background model, which can be
viewed as smoothed click counts, and are obtained
86
Label Model Reference
UNIF P?unif (e|q) Eq. 3.8
MLE P?mle(e|q) Eq. 3.1
HYBR P?hybr(e|q) Eq. 3.3
INTU P?intu(e|q) Eq. 3.6
INTP P?intp(e|q) Eq. 3.7
Table 1: Models for estimating the association probabil-
ity P (e|q).
by propagating clicks to unseen edges using the syn-
onymy model as follows:
w?e(q, e) =
?
q??Q
s(q,q?)
Nsq
? P?mle(e|q?) (3.4)
where Nsq =
?
q??Q s(q, q
?). By normalizing
the smoothed weights, we obtain our background
model, P?bsim:
P?bsim(e|q) =
w?e(q,e)?
e??E w?e(q,e
?) (3.5)
Below we propose two models for interpolating our
foreground model from Equation 3.1 with the back-
ground model from Equation 3.5.
Basic Interpolation: This smoothing model,
P?intu(e|q), linearly combines our foreground and
background models using a model parameter ?:
P?intu(e|q)=?P?mle(e|q)+(1??)P?bsim(e|q) (3.6)
Bucket Interpolation: Intuitively, edges {q, e} ?
Ce with higher observed clicks, we(q, e), should be
trusted more than those with low or no clicks. A
limitation of P?intu(e|q) is that it weighs the fore-
ground and background models in the same way ir-
respective of the observed foreground clicks. Our
final model, P?intp(e|q) parameterizes the interpola-
tion by the number of observed clicks:
P?intp(e|q)=?[we(q, e)]P?mle(e|q)
+ (1? ?[we(q, e)])P?bsim(e|q)
(3.7)
In practice, we bucket the observed click parame-
ter, we(q, e), into eleven buckets: {1-click, 2-clicks,
..., 10-clicks, more than 10 clicks}.
Section 5.2 outlines our procedure for learn-
ing the model parameters for both P?intu(e|q) and
P?intp(e|q).
3.4 Summary
Table 1 summarizes the association models pre-
sented in this section as well as a strawman that as-
signs uniform probability to all edges in QEC:
P?unif (e|q) =
1
?
e??E ?(q, e
?)
(3.8)
In the following section, we apply these models
to the task of extracting product recommendations
for general web search queries. A large-scale exper-
imental study is presented in Section 5 supporting
the effectiveness of our models.
4 Entity Recommendation
Query recommendations are pervasive in commer-
cial search engines. Many systems extract recom-
mendations by mining temporal query chains from
search sessions and clickthrough patterns (Zhang
and Nasraoui, 2006). We adopt a similar strategy,
except instead of mining query-query associations,
we propose to mine query-entity associations, where
entities come from an entity database as described in
Section 1. Our technical challenge lies in annotating
sessions with entities that are relevant to the session.
4.1 Product Entity Domain
Although our model generalizes to any entity do-
main, we focus now on a product domain. Specifi-
cally, our universe of entities,E, consists of the enti-
ties in a large commercial product catalog, for which
we observe query-click-product clicks, Ce, from the
vertical search logs. Our QEC graph is completed
by extracting query-click-urls from a search engine?s
general search logs, Cu. These datasets are de-
scribed in Section 5.1.
4.2 Recommendation Algorithm
We hypothesize that if an entity is relevant to a
query, then it is relevant to all other queries co-
occurring in the same session. Key to our method
are the models from Section 3.
Step 1 ? Query Annotation: For each query q in a
session s, we annotate it with a set Eq, consisting of
every pair {e, P? (e|q)}, where e ? E such that there
exists an edge {q, e} ? Ce with probability P? (e|q).
Note that Eq will be empty for many queries.
Step 2 ? Session Analysis: We build a query-
entity frequency co-occurrence matrix, A, consist-
ing of n|Q| rows and n|E| columns, where each row
corresponds to a query and each column to an entity.
87
The value of the cell Aqe is the sum over each ses-
sion s, of the maximum edge weight between any
query q? ? s and e3:
Aqe =
?
s?S ?(s, e)
where S consists of all observed search sessions and:
?(s, e) = argmax
P? (e|q?)
({e, P? (e|q?)} ? Eq?),?q
? ? s
Step 3 ? Ranking: We compute ranking scores
between each query q and entity e using pointwise
mutual information over the frequencies in A, simi-
larly to Eq. 3.2.
The final recommendations for a query q are ob-
tained by returning the top-k entities e according to
Step 3. Filters may be applied on: f the frequency
Aqe; and p the pointwise mutual information rank-
ing score between q and e.
5 Experimental Results
5.1 Datasets
We instantiate our models from Sections 3 and 4 us-
ing search query logs and a large catalog of prod-
ucts from a commercial search engine. We form
our QEC graphs by first collecting in Ce aggregate
query-click-entity counts observed over two years
in a commerce vertical search engine. Similarly,
Cu is formed by collecting aggregate query-click-url
counts observed over six months in a web search en-
gine, where each query must have frequency at least
10. Three final QEC graphs are sampled by taking
various snapshots of the above graph as follows: a)
TRAIN consists of 50% of the graph; b) TEST con-
sists of 25% of the graph; c) DEV consists of 25%
of the graph.
5.2 Association Models
5.2.1 Model Parameters
We tune the ? parameters for P?intu and P?intp against
the DEV QEC graph. There are twelve parameters
to be tuned: ? for P?intu and ?(1), ?(2), ..., ?(10),
?(> 10) for P?intp, where ?(x) is the observed
click bucket as described in Section 3.3. For each,
we choose the parameter value that minimizes the
mean-squared error (MSE) of the DEV set, where
3Note that this co-occurrence occurs because q? was anno-
tated with entity e in the same session as q occurred.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
M
S
E
Alpha
Alpha vs. MSE: Heldout Training For Alpha Parameters Basic
Bucket_1
Bucket_2
Bucket_3
Bucket_4
Bucket_5
Bucket_6
Bucket_7
Bucket_8
Bucket_9
Bucket_10
Bucket_11
Figure 2: Alpha tuning on held out data.
Model MSE Var Err/MLE MSEW Var Err/MLE
P?unif 0.0328
? 0.0112 -25.7% 0.0663? 0.0211 -71.8%
P?mle 0.0261 0.0111 ? 0.0386 0.0141 ?
P?hybr 0.0232
? 0.0071 11.1% 0.0385 0.0132 0.03%
P?intu 0.0226? 0.0075 13.4% 0.0369? 0.0133 4.4%
P?intp 0.0213? 0.0068 18.4% 0.0375? 0.0131 2.8%
Table 2: Model analysis: MSE and MSEW with vari-
ance and error reduction relative to P?mle. ? indicates sta-
tistical significance over P?mle with 95% confidence.
model probabilities are computed using the TRAIN
QEC graph. Figure 2 illustrates the MSE ranging
over [0, 0.05, 0.1, ..., 1].
We trained the query synonym model of Sec-
tion 3.2 on the DEV set and hand-annotated 100 ran-
dom synonymy pairs according to whether or not the
pairs were synonyms 2. Setting ? = 0.4 results in a
precision > 0.9.
5.2.2 Analysis
We evaluate the quality of our models in Table 1 by
evaluating their mean-squared error (MSE) against
the target P (e|q) computed on the TEST set:
MSE(P? )=
?
{q,e}?CTe
(PT (e|q)?P? (e|q))2
MSEW (P? )=
?
{q,e}?CTe
wTe (q,e)?(P
T (e|q)?P? (e|q))2
where CTe are the edges in the TEST QEC graph
with weight wTe (q, e), P
T (e|q) is the target proba-
bility computed over the TEST QEC graph, and P?
is one of our models trained on the TRAIN QEC
graph. MSE measures against each edge type,
which makes it sensitive to the long tail of the
click graph. Conversely, MSEW measures against
each edge instance, which makes it a good mea-
sure against the head of the click graph. We expect
our smoothing models to have much more impact
on MSE (i.e., the tail) than on MSEW since head
queries do not suffer from data sparsity.
Table 2 lists the MSE and MSEW results for
each model. We consider P?unif as a strawman and
P?mle as a strong baseline (i.e., without any graph
expansion nor any smoothing against a background
88
00.01
0.02
0.03
0.04
0.05
0.06
M
S
E
Click Bucket (scaled by query - instance coverage)
Mean Squared Error vs. Click Bucket
UNI F
ML E
H YBR
I NTU
I NTP
1 2 43 5 6 7 8 9 10
Figure 3: MSE of each model against the number of
clicks in the TEST corpus. Buckets scaled by query in-
stance coverage of all queries with 10 or fewer clicks.
model). P?unif performs generally very poorly, how-
ever P?mle is much better, with an expected estima-
tion error of 0.16 accounting for an MSE of 0.0261.
As expected, our smoothing models have little im-
provement on the head-sensitive metric (MSEW )
relative to P?mle. In particular, P?hybr performs nearly
identically to P?mle on the head. On the tail, all three
smoothing models significantly outperform P?mle
with P?intp reducing the error by 18.4%. Table 3 lists
query-product associations for five randomly sam-
pled products along with their model scores from
P?mle with P?intp.
Figure 3 provides an intrinsic view into MSE as
a function of the number of observed clicks in the
TEST set. As expected, for larger observed click
counts (>4), all models perform roughly the same,
indicating that smoothing is not necessary. However,
for low click counts, which in our dataset accounts
for over 20% of the overall click instances, we see
a large reduction in MSE with P?intp outperforming
P?intu, which in turn outperforms P?hybr. P?unif per-
forms very poorly. The reason it does worse as the
observed click count rises is that head queries tend to
result in more distinct urls with high-variance clicks,
which in turn makes a uniform model susceptible to
more error.
Figure 3 illustrates that the benefit of the smooth-
ing models is in the tail of the click graph, which
supports the larger error reductions seen in MSE in
Table 2. For associations only observed once, P?intp
reduces the error by 29% relative to P?mle.
We also performed an editorial evaluation of the
query-entity associations obtained with bucket inter-
polation. We created two samples from the TEST
dataset: one randomly sampled by taking click
weights into account, and the other sampled uni-
formly at random. Each set contains results for
Query P?mleP?intp Query P?mleP?intp
Garmin GTM 20 GPS Canon PowerShot SX110 IS
garmin gtm 20 0.44 0.45 canon sx110 0.57 0.57
garmin traffic receiver 0.30 0.27 powershot sx110 0.48 0.48
garmin nuvi 885t 0.02 0.02 powershot sx110 is 0.38 0.36
gtm 20 0 0.33 powershot sx130 is 0 0.33
garmin gtm20 0 0.33 canon power shot sx110 0 0.20
nuvi 885t 0 0.01 canon dig camera review 0 0.10
Samsung PN50A450 50? TV Devil May Cry: 5th Anniversary Col.
samsung 50 plasma hdtv 0.75 0.83 devil may cry 0.76 0.78
samsung 50 0.33 0.32 devilmaycry 0 1.00
50? hdtv 0.17 0.12 High Island Hammock/Stand Combo
samsung plasma tv review 0 0.42 high island hammocks 1.00 1.00
50? samsung plasma hdtv 0 0.35 hammocks and stands 0 0.10
Table 3: Example query-product association scores for a
random sample of five products. Bold queries resulted
from the expansion algorithm in Section 3.2.
100 queries. The former consists of 203 query-
product associations, and the latter of 159 associa-
tions. The evaluation was done using Amazon Me-
chanical Turk4. We created a Mechanical Turk HIT5
where we show to the Mechanical Turk workers the
query and the actual Web page in a Product search
engine. For each query-entity association, we gath-
ered seven labels and considered an association to be
correct if five Mechanical Turk workers gave a pos-
itive label. An association was considered to be in-
correct if at least five workers gave a negative label.
Borderline cases where no label got five votes were
discarded (14% of items were borderline for the uni-
form sample; 11% for the weighted sample). To en-
sure the quality of the results, we introduced 30%
of incorrect associations as honeypots. We blocked
workers who responded incorrectly on the honey-
pots so that the precision on honeypots is 1. The
result of the evaluation is that the precision of the as-
sociations is 0.88 on the weighted sample and 0.90
on the uniform sample.
5.3 Related Product Recommendation
We now present an experimental evaluation of our
product recommendation system using the baseline
model P?mle and our best-performing model P?intp.
The goals of this evaluation are to (1) determine
the quality of our product recommendations; and (2)
assess the impact of our association models on the
product recommendations.
5.3.1 Experimental Setup
We instantiate our recommendation algorithm from
Section 4.2 using session co-occurrence frequencies
4https://www.mturk.com
5HIT stands for Human Intelligence Task
89
Query Set Sample Query Bag Sample
f 10 25 50 100 10 25 50 100
p 10 10 10 10 10 10 10 10
P?mle precision 0.89 0.93 0.96 0.96 0.94 0.94 0.93 0.92
P?intp precision 0.86 0.92 0.96 0.96 0.94 0.94 0.93 0.94
P?mle coverage 0.007 0.004 0.002 0.001 0.085 0.067 0.052 0.039
P?intp coverage 0.008 0.005 0.003 0.002 0.094 0.076 0.059 0.045
Rintp,mle 1.16 1.14 1.13 1.14 1.11 1.13 1.15 1.19
Table 4: Experimental results for product recommenda-
tions. All configurations are for k = 10.
from a one-month snapshot of user query sessions at
a Web search engine, where session boundaries oc-
cur when 60 seconds elapse in between user queries.
We experiment with the recommendation parame-
ters defined at the end of Section 4.2 as follows: k =
10, f ranging from 10 to 100, and p ranging from 3
to 10.
For each configuration, we report coverage as the
total number of queries in the output (i.e., the queries
for which there is some recommendation) divided by
the total number of queries in the log. For our per-
formance metrics, we sampled two sets of queries:
(a) Query Set Sample: uniform random sam-
ple of 100 queries from the unique queries in the
one-month log; and (b) Query Bag Sample:
weighted random sample, by query frequency, of
100 queries from the query instances in the one-
month log. For each sample query, we pooled to-
gether and randomly shuffled all recommendations
by our algorithm using both P?mle and P?intp on each
parameter configuration. We then manually anno-
tated each {query, product} pair as relevant, mildly
relevant or non-relevant. In total, 1127 pairs were
annotated. Interannotator agreement between two
judges on this task yielded a Cohen?s Kappa (Cohen,
1960) of 0.56. We therefore collapsed the mildly
relevant and non-relevant classes yielding two final
classes: relevant and non-relevant. Cohen?s Kappa
on this binary classification is 0.71.
Let CM be the number of relevant (i.e., correct)
suggestions recommended by a configurationM and
let |M | be the number of recommendations returned
by M . Then we define the (micro-) precision of M
as: PM =
CM
C . We define relative recall (Pantel et
al., 2004) between two configurations M1 and M2
as RM1,M2 =
PM1?|M1|
PM2?|M2|
.
5.3.2 Results
Table 4 summarizes our results for some configura-
tions (others omitted for lack of space). Most re-
Query Product Recommendation
wedding gowns 27 Dresses (Movie Soundtrack)
wedding gowns Bridal Gowns: The Basics of Designing, [...] (Book)
wedding gowns Wedding Dress Hankie
wedding gowns The Perfect Wedding Dress (Magazine)
wedding gowns Imagine Wedding Designer (Video Game)
low blood pressure Omron Blood Pressure Monitor
low blood pressure Healthcare Automatic Blood Pressure Monitor
low blood pressure Ridgecrest Blood Pressure Formula - 60 Capsules
low blood pressure Omron Portable Wrist Blood Pressure Monitor
?hello cupcake? cookbook Giant Cupcake Cast Pan
?hello cupcake? cookbook Ultimate 3-In-1 Storage Caddy
?hello cupcake? cookbook 13 Cup Cupcakes and More Dessert Stand
?hello cupcake? cookbook Cupcake Stand Set (Toys)
1 800 flowers Todd Oldham Party Perfect Bouquet
1 800 flowers Hugs and Kisses Flower Bouquet with Vase
Table 5: Sample product recommendations.
markable is the {f = 10, p = 10} configuration
where the P?intp model affected 9.4% of all query
instances posed by the millions of users of a major
search engine, with a precision of 94%. Although
this model covers 0.8% of the unique queries, the
fact that it covers many head queries such as wal-
mart and iphone accounts for the large query in-
stance coverage. Also since there may be many gen-
eral web queries for which there is no appropriate
product in the database, a coverage of 100% is not
attainable (nor desirable); in fact the upper bound
for the coverage is likely to be much lower.
Turning to the impact of the association models
on product recommendations, we note that precision
is stable in our P?intp model relative to our baseline
P?mle model. However, a large lift in relative recall
is observed, up to a 19% increase for the {f = 100,
p = 10} configuration. These results are consistent
with those of Section 5.2, which compared the asso-
ciation models independently of the application and
showed that P?intp outperforms P?mle.
Table 5 shows sample product recommendations
discovered by our P?intp model. Manual inspection
revealed two main sources of errors. First, ambiguity
is introduced both by the click model and the graph
expansion algorithm of Section 3.2. In many cases,
the ambiguity is resolved by user click patterns (i.e.,
users disambiguate queries through their browsing
behavior), but one such error was seen for the query
?shark attack videos? where several Shark-branded
vacuum cleaners are recommended. This is because
of the ambiguous query ?shark? that is found in the
click logs and in query sessions co-occurring with
the query ?shark attack videos?. The second source
of errors is caused by systematic user errors com-
monly found in session logs such as a user acciden-
tally submitting a query while typing. An example
90
session is: {?speedo?, ?speedometer?}where the in-
tended session was just the second query and the un-
intended first query is associated with products such
as Speedo swimsuits. This ultimately causes our sys-
tem to recommend various swimsuits for the query
?speedometer?.
6 Conclusion
Learning associations between web queries and
entities has many possible applications, including
query-entity recommendation, personalization by
associating entity vectors to users, and direct adver-
tising. Although many techniques have been devel-
oped for associating queries to queries or queries
to documents, to the best of our knowledge this is
the first that aims to associate queries to entities
by leveraging click graphs from both general search
logs and vertical search logs.
We developed several models for estimating the
probability that an entity is relevant given a user
query. The sparsity of query entity graphs is ad-
dressed by first expanding the graph with query
synonyms, and then smoothing query-entity click
counts over these unseen queries. Our best per-
forming model, which interpolates between a fore-
ground click model and a smoothed background
model, significantly reduces testing error when com-
pared against a strong baseline, by 18%. On associ-
ations observed only once in our test collection, the
modeling error is reduced by 29% over the baseline.
We applied our best performing model to the
task of query-entity recommendation, by analyz-
ing session co-occurrences between queries and an-
notated entities. Experimental analysis shows that
our smoothing techniques improve coverage while
keeping precision stable, and overall, that our top-
performing model affects 9% of general web queries
with 94% precision.
References
[Agichtein et al2006] Eugene Agichtein, Eric Brill, and
Susan T. Dumais. 2006. Improving web search rank-
ing by incorporating user behavior information. In SI-
GIR, pages 19?26.
[Agirre et al2009] Eneko Agirre, Enrique Alfonseca,
Keith Hall, Jana Kravalova, Marius Pas?ca, and Aitor
Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In
NAACL, pages 19?27.
[Baeza-Yates et al2004] Ricardo Baeza-Yates, Carlos
Hurtado, and Marcelo Mendoza. 2004. Query rec-
ommendation using query logs in search engines. In
Wolfgang Lindner, Marco Mesiti, Can Tu?rker, Yannis
Tzitzikas, and Athena Vakali, editors, EDBT Work-
shops, volume 3268 of Lecture Notes in Computer
Science, pages 588?596. Springer.
[Baeza-Yates2004] Ricardo Baeza-Yates. 2004. Web us-
age mining in search engines. In In Web Mining: Ap-
plications and Techniques, Anthony Scime, editor. Idea
Group, pages 307?321.
[Bell et al2007] R. Bell, Y. Koren, and C. Volinsky.
2007. Modeling relationships at multiple scales to
improve accuracy of large recommender systems. In
KDD, pages 95?104.
[Boldi et al2009] Paolo Boldi, Francesco Bonchi, Carlos
Castillo, Debora Donato, and Sebastiano Vigna. 2009.
Query suggestions using query-flow graphs. In WSCD
?09: Proceedings of the 2009 workshop on Web Search
Click Data, pages 56?63. ACM.
[Cohen1960] Jacob Cohen. 1960. A coefficient of agree-
ment for nominal scales. Educational and Psycholog-
ical Measurement, 20(1):37?46, April.
[Craswell and Szummer2007] Nick Craswell and Martin
Szummer. 2007. Random walks on the click graph.
In SIGIR, pages 239?246.
[Fuxman et al2008] A. Fuxman, P. Tsaparas, K. Achan,
and R. Agrawal. 2008. Using the wisdom of the
crowds for keyword generation. In WWW, pages 61?
70.
[Gao et al2009] Jianfeng Gao, Wei Yuan, Xiao Li, Ke-
feng Deng, and Jian-Yun Nie. 2009. Smoothing click-
through data for web search ranking. In SIGIR, pages
355?362.
[Good1953] Irving John Good. 1953. The population fre-
quencies of species and the estimation of population
parameters. Biometrika, 40(3 and 4):237?264.
[Jagabathula et al2011] S. Jagabathula, N. Mishra, and
S. Gollapudi. 2011. Shopping for products you don?t
know you need. In To appear at WSDM.
[Jain and Pantel2009] Alpa Jain and Patrick Pantel. 2009.
Identifying comparable entities on the web. In CIKM,
pages 1661?1664.
[Jelinek and Mercer1980] Frederick Jelinek and
Robert L. Mercer. 1980. Interpolated estimation
of markov source parameters from sparse data. In In
Proceedings of the Workshop on Pattern Recognition
in Practice, pages 381?397.
[Katz1987] Slava M. Katz. 1987. Estimation of probabil-
ities from sparse data for the language model compo-
nent of a speech recognizer. In IEEE Transactions on
91
Acoustics, Speech and Signal Processing, pages 400?
401.
[Kneser and Ney1995] Reinhard Kneser and Hermann
Ney. 1995. Improved backing-off for m-gram lan-
guage modeling. In In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 181?184.
[Kurland and Lee2004] O. Kurland and L. Lee. 2004.
Corpus structure, language models, and ad-hoc infor-
mation retrieval. In SIGIR, pages 194?201.
[Lidstone1920] George James Lidstone. 1920. Note on
the general case of the bayes-laplace formula for in-
ductive or a posteriori probabilities. Transactions of
the Faculty of Actuaries, 8:182?192.
[Linden et al2003] G. Linden, B. Smith, and J. York.
2003. Amazon.com recommendations: Item-to-item
collaborative filtering. IEEE Internet Computing,
7(1):76?80.
[Liu and Croft2004] X. Liu and W. Croft. 2004. Cluster-
based retrieval using language models. In SIGIR,
pages 186?193.
[Mei et al2008a] Q. Mei, D. Zhang, and C. Zhai. 2008a.
A general optimization framework for smoothing lan-
guage models on graph structures. In SIGIR, pages
611?618.
[Mei et al2008b] Q. Mei, D. Zhou, and Church K. 2008b.
Query suggestion using hitting time. In CIKM, pages
469?478.
[Nie et al2007] Z. Nie, J. Wen, and W. Ma. 2007.
Object-level vertical search. In Conference on Innova-
tive Data Systems Research (CIDR), pages 235?246.
[Pantel and Lin2002] Patrick Pantel and Dekang Lin.
2002. Discovering word senses from text. In
SIGKDD, pages 613?619, Edmonton, Canada.
[Pantel et al2004] Patrick Pantel, Deepak Ravichandran,
and Eduard Hovy. 2004. Towards terascale knowl-
edge acquisition. In COLING, pages 771?777.
[Pantel et al2009] Patrick Pantel, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009. Web-scale distributional similarity and entity
set expansion. In EMNLP, pages 938?947.
[Pas?ca and Durme2008] Marius Pas?ca and Benjamin Van
Durme. 2008. Weakly-supervised acquisition of
open-domain classes and class attributes from web
documents and query logs. In ACL, pages 19?27.
[Ponte and Croft1998] J. Ponte and B. Croft. 1998. A
language modeling approach to information retrieval.
In SIGIR, pages 275?281.
[Sarwar et al2001] B. Sarwar, G. Karypis, J. Konstan,
and J. Reidl. 2001. Item-based collaborative filtering
recommendation system. In WWW, pages 285?295.
[Tao et al2006] T. Tao, X. Wang, Q. Mei, and C. Zhai.
2006. Language model information retrieval with doc-
ument expansion. In HLT/NAACL, pages 407?414.
[Wen et al2001] Ji-Rong Wen, Jian-Yun Nie, and
HongJiang Zhang. 2001. Clustering user queries of a
search engine. In WWW, pages 162?168.
[Witten and Bell1991] I.H. Witten and T.C. Bell. 1991.
The zero-frequency problem: Estimating the proba-
bilities of novel events in adaptive text compression.
IEEE Transactions on Information Theory, 37(4).
[Zhai and Lafferty2001] C. Zhai and J. Lafferty. 2001. A
study of smoothing methods for language models ap-
plied to ad hoc information retrieval. In SIGIR, pages
334?342.
[Zhang and Nasraoui2006] Z. Zhang and O. Nasraoui.
2006. Mining search engine query logs for query rec-
ommendation. In WWW, pages 1039?1040.
92
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 563?571,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mining Entity Types from Query Logs via User Intent Modeling
Patrick Pantel
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
ppantel@microsoft.com
Thomas Lin
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
tlin@cs.washington.edu
Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
mgamon@microsoft.com
Abstract
We predict entity type distributions in Web
search queries via probabilistic inference in
graphical models that capture how entity-
bearing queries are generated. We jointly
model the interplay between latent user in-
tents that govern queries and unobserved en-
tity types, leveraging observed signals from
query formulations and document clicks. We
apply the models to resolve entity types in new
queries and to assign prior type distributions
over an existing knowledge base. Our mod-
els are efficiently trained using maximum like-
lihood estimation over millions of real-world
Web search queries. We show that modeling
user intent significantly improves entity type
resolution for head queries over the state of the
art, on several metrics, without degradation in
tail query performance.
1 Introduction
Commercial search engines are providing ever-
richer experiences around entities. Querying for a
dish on Google yields recipe filters such as cook
time, calories, and ingredients. Querying for a
movie on Yahoo triggers user ratings, cast, tweets
and showtimes. Bing further allows the movie to
be directly added to the user?s Netflix queue. En-
tity repositories such as Freebase, IMDB, Facebook
Pages, Factual, Pricegrabber, and Wikipedia are in-
creasingly leveraged to enable such experiences.
There are, however, inherent problems in the en-
tity repositories: (a) coverage: although coverage of
head entity types is often reliable, the tail can be
sparse; (b) noise: created by spammers, extraction
errors or errors in crowdsourced content; (c) am-
biguity: multiple types or entity identifiers are of-
ten associated with the same surface string; and (d)
over-expression: many entities have types that are
never used in the context of Web search.
There is an opportunity to automatically tailor
knowledge repositories to the Web search scenario.
Desirable capabilities of such a system include: (a)
determining the prior type distribution in Web search
for each entity in the repository; (b) assigning a type
distribution to new entities; (c) inferring the correct
sense of an entity in a particular query context; and
(d) adapting to a search engine and time period.
In this paper, we build such a system by lever-
aging Web search usage logs with large numbers of
user sessions seeking or transacting on entities. We
cast the task as performing probabilistic inference
in a graphical model that captures how queries are
generated, and then apply the model to contextually
recognize entity types in new queries. We motivate
and design several generative models based on the
theory that search users? (unobserved) intents gov-
ern the types of entities, the query formulations, and
the ultimate clicks on Web documents. We show that
jointly modeling user intent and entity type signifi-
cantly outperforms the current state of the art on the
task of entity type resolution in queries. The major
contributions of our research are:
? We introduce the idea that latent user intents
can be an important factor in modeling type dis-
tributions over entities in Web search.
? We propose generative models and inference
procedures using signals from query context,
click, entity, entity type, and user intent.
563
? We propose an efficient learning technique and
a robust implementation of our models, using
real-world query data, and a realistic large set
of entity types.
? We empirically show that our models outper-
form the state of the art and that modeling latent
intent contributes significantly to these results.
2 Related Work
2.1 Finding Semantic Classes
A closely related problem is that of finding the se-
mantic classes of entities. Automatic techniques for
finding semantic classes include unsupervised clus-
tering (Schu?tze, 1998; Pantel and Lin, 2002), hy-
ponym patterns (Hearst, 1992; Pantel et al, 2004;
Kozareva et al, 2008), extraction patterns (Etzioni
et al, 2005), hidden Markov models (Ritter et al,
2009), classification (Rahman and Ng, 2010) and
many others. These techniques typically lever-
age large corpora, while projects such as WordNet
(Miller et al, 1990) and Freebase (Bollacker et al,
2008) have employed editors to manually enumerate
words and entities with their semantic classes.
The aforementioned methods do not use query
logs or explicitly determine the relative probabilities
of different entity senses. A method might learn that
there is independently a high chance of eBay being a
website and an employer, but does not specify which
usage is more common. This is especially problem-
atic, for example, if one wishes to leverage Freebase
but only needs the most commonly used senses (e.g.,
Al Gore is a US Vice President), rather than
all possible obscure senses (Freebase contains 30+
senses, including ones such as Impersonated
Celebrity and Quotation Subject). In
scenarios such as this, our proposed method can in-
crease the usability of systems that find semantic
classes. We also expand upon text corpora meth-
ods in that the type priors can adapt to Web search
signals.
2.2 Query Log Mining
Query logs have traditionally been mined to improve
search (Baeza-Yates et al, 2004; Zhang and Nas-
raoui, 2006), but they can also be used in place of
(or in addition to) text corpora for learning seman-
tic classes. Query logs can contain billions of en-
tries, they provide an independent signal from text
corpora, their timestamps allow the learning of type
priors at specific points in time, and they can contain
information such as clickthroughs that are not found
in text corpora. Sekine and Suzuki (2007) used fre-
quency features on context words in query logs to
learn semantic classes of entities. Pas?ca (2007) used
extraction techniques to mine instances of semantic
classes from query logs. Ru?d et al (2011) found
that cross-domain generalizations learned from Web
search results are applicable to NLP tasks such as
NER. Alfonseca et al (2010) mined query logs to
find attributes of entity instances. However, these
projects did not learn relative probabilities of differ-
ent senses.
2.3 User Intents in Search
Learning from query logs also allows us to lever-
age the concept of user intents. When users sub-
mit search queries, they often have specific intents in
mind. Broder (2002) introduced 3 top level intents:
Informational (e.g., wanting to learn), Navigational
(wanting to visit a site), and Transactional (e.g.,
wanting to buy/sell). Rose and Levinson (2004) fur-
ther divided these into finer-grained subcategories,
and Yin and Shah (2010) built hierarchical tax-
onomies of search intents. Jansen et al (2007), Hu
et al (2009), and Radlinski et al (2010) examined
how to infer the intent of queries. We are not aware
of any other work that has leveraged user intents to
learn type distributions.
2.4 Topic Modeling on Query Logs
The closest work to ours is Guo et al?s (2009) re-
search on Named Entity Recognition in Queries.
Given an entity-bearing query, they attempt to iden-
tify the entity and determine the type posteriors. Our
work significantly scales up the type posteriors com-
ponent of their work. While they only have four
potential types (Movie, Game, Book, Music) for
each entity, we employ over 70 popular types, allow-
ing much greater coverage of real entities and their
types. Because they only had four types, they were
able to hand label their training data. In contrast,
our system self-labels training examples by search-
ing query logs for high-likelihood entities, and must
handle any errors introduced by this process. Our
models also expand upon theirs by jointly modeling
564
entity type with latent user intents, and by incorpo-
rating click signals.
Other projects have also demonstrated the util-
ity of topic modeling on query logs. Carman et
al. (2010) modeled users and clicked documents to
personalize search results and Gao et al (2011) ap-
plied topic models to query logs in order to improve
document ranking for search.
3 Joint Model of Types and User Intents
We turn our attention now to the task of mining the
type distributions of entities and of resolving the
type of an entity in a particular query context. Our
approach is to probabilistically describe how entity-
bearing queries are generated in Web search. We
theorize that search queries are governed by a latent
user intent, which in turn influences the entity types,
the choice of query words, and the clicked hosts. We
develop inference procedures to infer the prior type
distributions of entities in Web search as well as to
resolve the type of an entity in a query, by maximiz-
ing the probability of observing a large collection of
real-world queries and their clicked hosts.
We represent a query q by a triple {n1, e, n2},
where e represents the entity mentioned in the query,
n1 and n2 are respectively the pre- and post-entity
contexts (possibly empty), referred to as refiners.
Details on how we obtain our corpus are presented
in Section 4.2.
3.1 Intent-based Model (IM)
In this section we describe our main model, IM, il-
lustrated in Figure 1. We derive a learning algorithm
for the model in Section 3.2 and an inference proce-
dure in Section 3.3.
Recall our discussion of intents from Section 2.3.
The unobserved semantic type of an entity e in a
query is strongly correlated with the unobserved
user intent. For example, if a user queries for
?song?, then she is likely looking to ?listen to it?,
?download it?, ?buy it?, or ?find lyrics? for it. Our
model incorporates this user intent as a latent vari-
able.
The choice of the query refiner words, n1 and n2,
is also clearly influenced by the user intent. For
example, refiners such as ?lyrics? and ?words? are
more likely to be used in queries where the intent is
For each query/click pair {q, c}
type t ?Multinomial(?)
intent i ?Multinomial(?t)
entity e ?Multinomial(?t)
switch s1 ? Bernoulli(?i)
switch s2 ? Bernoulli(?i)
if (s1) l-context n1 ?Multinomial(?i)
if (s2) r-context n2 ?Multinomial(?i)
click c ?Multinomial(?i)
Table 1: Model IM: Generative process for entity-
bearing queries.
to ?find lyrics? than in queries where the intent is to
?listen?. The same is true for clicked hosts: clicks on
?lyrics.com? and ?songlyrics.com? are more likely
to occur when the intent is to ?find lyrics?, whereas
clicks on ?pandora.com? and ?last.fm? are more
likely for a ?listen? intent.
Model IM leverages each of these signals: latent
intent, query refiners, and clicked hosts. It generates
entity-bearing queries by first generating an entity
type, from which the user intent and entity is gen-
erated. In turn, the user intent is then used to gen-
erate the query refiners and the clicked host. In our
data analysis, we observed that over 90% of entity-
bearing queries did not contain any refiner words n1
and n2. In order to distribute more probability mass
to non-empty context words, we explicitly represent
the empty context using a switch variable that deter-
mines whether a context will be empty.
The generative process for IM is described in Ta-
ble 1. Consider the query ?ymca lyrics?. Our model
first generates the type song, then given the type
it generates the entity ?ymca? and the intent ?find
lyrics?. The intent is then used to generate the pre-
and post-context words ? and ?lyrics?, respectively,
and a click on a host such as ?lyrics.com?.
For mathematical convenience, we assume that
the user intent is generated independently of the
entity itself. Without this assumption, we would
require learning a parameter for each intent-type-
entity configuration, exploding the number of pa-
rameters. Instead, we choose to include these depen-
dencies at the time of inference, as described later.
Recall that q = {n1, e, n2} and let s = {s1, s2},
where s1 = 1 if n1 is not empty and s2 = 1 if n2 is
not empty, 0 otherwise. The joint probability of the
model is the product of the conditional distributions,
as given by:
565
y  
Q
tt
n
2
e
f f 
T
tt
E
Guo?09
y  
Q
tt
n
2
e
ss  
f f 
Ttt E
T
Model M0
y  
Q
tt
n
2
e
w Tcss  
f f 
Ttt E
T
Model M1 Model IM
tt
Q
tt
n
2
qq
T
iie
w Kcss  
f f 
K
y  
T
K
Figure 1: Graphical models for generating entity-bearing queries. Guo?09 represents the current state of the art (Guo
et al, 2009). Models M0 and M1 add an empty context switch and click information, respectively. Model IM further
constrains the query by the latent user intent.
P (t, i, q, c | ?,?,?, ?,?,?) =
P (t | ?)P (i | t,?)P (e | t,?)P (c | i,?)
2?
j=1
P (nj | i,?)
I[sj=1]P (sj |i, ?)
We now define each of the terms in the joint dis-
tribution. Let T be the number of entity types. The
probability of generating a type t is governed by a
multinomial with probability vector ? :
P (t=t?) =
T?
j=1
? I[j=t?]j , s.t.
T?
j=1
?j = 1
where I is an indicator function set to 1 if its condi-
tion holds, and 0 otherwise.
Let K be the number of latent user intents that
govern our query log, where K is fixed in advance.
Then, the probability of intents i is defined as a
multinomial distribution with probability vector ?t
such that ? = [?1, ?2, ..., ?T ] captures the matrix of
parameters across all T types:
P (i=i? | t=t?) =
K?
j=1
?I[j=i?]
t?,j
, s.t. ?t
K?
j=1
?t,j = 1
LetE be the number of known entities. The prob-
ability of generating an entity e is similarly governed
by a parameter ? across all T types:
P (e=e? | t=t?) =
E?
j=1
?I[j=e?]
t?,j
, s.t. ?t
E?
j=1
?t,j = 1
The probability of generating an empty or non-
empty context s given intent i is given by a Bernoulli
with parameter ?i:
P (s | i=i?) = ?I[s=1]
i?
(1? ?i?)
I[s=0]
Let V be the shared vocabulary size of all query
refiner words n1 and n2. Given an intent, i, the
probability of generating a refiner n is given by a
multinomial distribution with probability vector ?i
such that ? = [?1, ?2, ..., ?K ] represents parame-
ters across intents:
P (n=n? | i=i?) =
V?
v=1
?I[v=n?]
i?,v
, s.t. ?i
V?
v=1
?i,v = 1
Finally, we assume there areH possible click val-
ues, corresponding to H Web hosts. A click on a
host is similarly determined by an intent i and is gov-
erned by parameter ? across all K intents:
P (c=c? | i=i?) =
H?
h=1
?I[h=c?]
i?,h
, s.t. ?i
H?
h=1
?i,h = 1
3.2 Learning
Given a query corpus Q consisting of N inde-
pendently and identically distributed queries qj =
{nj1, e
j , nj2} and their corresponding clicked hosts
cj , we estimate the parameters ? , ?, ?, ?, ?, and
? by maximizing the (log) probability of observing
Q. The logP (Q) can be written as:
logP (Q) =
N?
j=1
?
t,i
P j(t, i | q, c) logP j(q, c, t, i)
In the above equation, P j(t, i | q, c) is the poste-
rior distribution over types and user intents for the
jth query. We use the Expectation-Maximization
(EM) algorithm to estimate the parameters. The
parameter updates are obtained by computing the
derivative of logP (Q) with respect to each parame-
ter, and setting the resultant to 0.
The update for ? is given by the average of the
posterior distributions over the types:
566
?t? =
?N
j=1
?
i P
j(t=t?, i | q, c)
?N
j=1
?
t,i P
j(t, i | q, c)
For a fixed type t, the update for ?t is given by
the weighted average of the latent intents, where the
weights are the posterior distributions over the types,
for each query:
?t?,?i =
?N
j=1 P
j(t=t?, i=i? | q, c)
?N
j=1
?
i P
j(t=t?, i | q, c)
Similarly, we can update ?, the parameters that
govern the distribution over entities for each type:
?t?,e? =
?N
j=1
?
i P
j(t=t?, i | q, c)I[ej=e?]
?N
j=1
?
i P
j(t=t?, i | q, c)
Now, for a fixed user intent i, the update for
?i is given by the weighted average of the clicked
hosts, where the weights are the posterior distribu-
tions over the intents, for each query:
?i?,c? =
?N
j=1
?
t P
j(t, i=i? | q, c)I[cj=c?]
?N
j=1
?
t P
j(t, i=i? | q, c)
Similarly, we can update ? and ?, the parameters
that govern the distribution over query refiners and
empty contexts for each intent, as:
?i?,n?=
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[nj1=n?]I[s
j
1=1]+I[n
j
2=n?]I[s
j
2=1]
]
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[sj1=1]+I[s
j
2=1]
]
and
?i? =
?N
j=1
?
t P
j(t, i=i? | q, c)
[
I[s1=1] + I[s2=1]
]
2
?N
j=1
?
t P
j(t, i=i? | q, c)
3.3 Decoding
Given a query/click pair {q, c}, and the learned IM
model, we can apply Bayes? rule to find the poste-
rior distribution, P (t, i | q, c), over the types and
intents, as it is proportional to P (t, i, q, c). We com-
pute this quantity exactly by evaluating the joint for
each combination of t and i, and the observed values
of q and c.
It is important to note that at runtime when a new
query is issued, we have to resolve the entity in the
absence of any observed click. However, we do have
access to historical click probabilities, P (c | q).
We use this information to compute P (t | q) by
marginalizing over i as follows:
P (t | q) =
?
i
H?
j=1
P (t, i | q, cj)P (cj | q) (1)
3.4 Comparative Models
Figure 1 also illustrates the current state-of-the-art
model Guo?09 (Guo et al, 2009), described in Sec-
tion 2.4, which utilizes only query refinement words
to infer entity type distributions. Two extensions to
this model that we further study in this paper are also
shown: Model M0 adds the empty context switch
parameter and Model M1 further adds click infor-
mation. In the interest of space, we omit the update
equations for these models, however they are triv-
ial to adapt from the derivations of Model IM pre-
sented in Sections 3.1 and 3.2.
3.5 Discussion
Full Bayesian Treatment: In the above mod-
els, we learn point estimates for the parameters
(?,?,?, ?,?,?). One can take a Bayesian ap-
proach and treat these parameters as variables (for
instance, with Dirichlet and Beta prior distribu-
tions), and perform Bayesian inference. However,
exact inference will become intractable and we
would need to resort to methods such as variational
inference or sampling. We found this extension un-
necessary, as we had a sufficient amount of training
data to estimate all parameters reliably. In addition,
our approach enabled us to learn (and perform infer-
ence in) the model with large amounts of data with
reasonable computing time.
Fitting to an existing Knowledge Base: Al-
though in general our model decodes type distribu-
tions for arbitrary entities, in many practical cases
it is beneficial to constrain the types to those ad-
missible in a fixed knowledge base (such as Free-
base). As an example, if the entity is ?ymca?,
admissible types may include song, place, and
educational institution. When resolving
types, during inference, one can restrict the search
space to only these admissible types. A desirable
side effect of this strategy is that only valid ambigu-
ities are captured in the posterior distribution.
567
4 Evaluation Methodology
We refer to QL as a set of English Web search
queries issued to a commercial search engine over
a period of several months.
4.1 Entity Inventory
Although our models generalize to any entity reposi-
tory, we experiment in this paper with entities cover-
ing a wide range of web search queries, coming from
73 types in Freebase. We arrived at these types by
grepping for all entities in Freebase within QL, fol-
lowing the procedure described in Section 4.2, and
then choosing the top most frequent types such that
50% of the queries are covered by an entity of one
of these types1.
4.2 Training Data Construction
In order to learn type distributions by jointly mod-
eling user intents and a large number of types, we
require a large set of training examples containing
tagged entities and their potential types. Unlike in
Guo et al (2009), we need a method to automatically
label QL to produce these training cases since man-
ual annotation is impossible for the range of entities
and types that we consider. Reliably recognizing en-
tities in queries is not a solved problem. However,
for training we do not require high coverage of en-
tities in QL, so high precision on a sizeable set of
query instances can be a proper proxy.
To this end, we collect candidate entities in
QL via simple string matching on Freebase entity
strings within our preselected 73 types. To achieve
high precision from this initial (high-recall, low-
precision) candidate set we use a number of heuris-
tics to only retain highly likely entities. The heuris-
tics include retaining only matches on entities that
appear capitalized more than 50% in their occur-
rences in Wikipedia. Also, a standalone score fil-
ter (Jain and Pennacchiotti, 2011) of 0.9 is used,
which is based on the ratio of string occurrence as
1In this process, we omitted any non-core Freebase type
(e.g., /user/* and /base/*), types used for representation
(e.g., /common/* and /type/*), and too general types (e.g.,
/people/person and /location/location) identi-
fied by if a type contains multiple other prominent subtypes.
Finally, we conflated seven of the types that overlapped with
each other into four types (such as /book/written work
and /book/book).
an exact match in queries to how often it occurs as a
partial match.
The resulting queries are further filtered by keep-
ing only those where the pre- and post-entity con-
texts (n1 and n2) were empty or a single word (ac-
counting for a very large fraction of the queries). We
also eliminate entries with clicked hosts that have
been clicked fewer than 100 times over the entire
QL. Finally, for training we filter out any query with
an entity that has more than two potential types2.
This step is performed to reduce recognition er-
rors by limiting the number of potential ambiguous
matches. We experimented with various thresholds
on allowable types and settled on the value two.
The resulting training data consists of several mil-
lion queries, 73 different entity types, and approx-
imately 135K different entities, 100K different re-
finer words, and 40K clicked hosts.
4.3 Test Set Annotation
We sampled two datasets, HEAD and TAIL, each
consisting of 500 queries containing an entity be-
longing to one of the 73 types in our inventory, from
a frequency-weighted random sample and a uniform
random sample of QL, respectively.
We conducted a user study to establish a gold
standard of the correct entity types in each query.
A total of seven different independent and paid pro-
fessional annotators participated in the study. For
each query in our test sets, we displayed the query,
associated clicked host, and entity to the annotator,
along with a list of permissible types from our type
inventory. The annotator is tasked with identifying
all applicable types from that list, or marking the test
case as faulty because of an error in entity identifi-
cation, bad click host (e.g. dead link) or bad query
(e.g. non-English). This resulted in 2,092 test cases
({query, entity, type}-tuples). Each test case was
annotated by two annotators. Inter-annotator agree-
ment as measured by Fleiss? ? was 0.445 (0.498
on HEAD and 0.386 on TAIL), considered moderate
agreement.
From HEAD and TAIL, we eliminated three cat-
egories of queries that did not offer any interesting
type disambiguation opportunities:
? queries that contained entities with only one
2For testing we did not omit any entity or type.
568
HEAD TAIL
nDCG MAP MAPW Prec@1 nDCG MAP MAPW Prec@1
BFB 0.71 0.60 0.45 0.30 0.73 0.64 0.49 0.35
Guo?09 0.79? 0.71? 0.62? 0.51? 0.80? 0.73? 0.66? 0.52?
M0 0.79? 0.72? 0.65? 0.52? 0.82? 0.75? 0.67? 0.57?
M1 0.83? 0.76? 0.72? 0.61? 0.81? 0.74? 0.67? 0.55?
IM 0.87? 0.82? 0.77? 0.73? 0.80? 0.72? 0.66? 0.52?
Table 2: Model analysis on HEAD and TAIL. ? indicates statistical significance over BFB, and ? over both BFB and
Guo?09. Bold indicates statistical significance over all non-bold models in the column. Significance is measured
using the Student?s t-test at 95% confidence.
potential type from our inventory;
? queries where the annotators rated all potential
types as good; and
? queries where judges rated none of the potential
types as good
The final test sets consist of 105 head queries with
359 judged entity types and 98 tail queries with 343
judged entity types.
4.4 Metrics
Our task is a ranking task and therefore the classic
IR metrics nDCG (normalized discounted cumula-
tive gain) and MAP (mean average precision) are
applicable (Manning et al, 2008).
Both nDCG and MAP are sensitive to the rank
position, but not the score (probability of a type) as-
sociated with each rank, S(r). We therefore also
evaluate a weighted mean average precision score
MAPW, which replaces the precision component
of MAP, P (r), for the rth ranked type by:
P (r) =
?r
r?=1 I(r?)S(r?)?r
r?=1 S(r?)
(2)
where I(r) indicates if the type at rank r is judged
correct.
Our fourth metric is Prec@1, i.e. the precision of
only the top-ranked type of each query. This is espe-
cially suitable for applications where a single sense
must be determined.
4.5 Model Settings
We trained all models in Figure 1 using the training
data from Section 4.2 over 100 EM iterations, with
two folds per model. For Model IM, we varied the
number of user intents (K) in intervals from 100 to
400 (see Figure 3), under the assumption that multi-
ple intents would exist per entity type.
We compare our results against two baselines.
The first baseline is an assignment of Freebase types
according to their frequency in our query set BFB,
and the second is Model Guo?09 (Guo et al, 2009)
illustrated in Figure 1.
5 Experimental Results
Table 2 lists the performance of each model on the
HEAD and TAIL sets over each metric defined in
Section 4.4. On head queries, the addition of the
empty context parameter ? and click signal ? to-
gether (Model M1) significantly outperforms both
the baseline and the state-of-the-art model Guo?09.
Further modeling the user intent in Model IM re-
sults in significantly better performance over all
models and across all metrics. Model IM shows
its biggest gains in the first position of its ranking as
evidenced by the Prec@1 metric.
We observe a different behavior on tail queries
where all models significantly outperform the base-
line BFB, but are not significantly different from
each other. In short, the strength of our proposed
model is in improving performance on the head at
no noticeable cost in the tail.
We separately tested the effect of adding the
empty context parameter ?. Figure 2 illustrates the
result on the HEAD data. Across all metrics, ? im-
proved performance over all models3. The more
expressive models benefitted more than the less ex-
pressive ones.
Table 2 reports results for Model IM using K =
200 user intents. This was determined by varying
K and selecting the top-performing value. Figure 3
illustrates the performance of Model IM with dif-
ferent values of K on the HEAD.
3Note that model M0 is just the addition of the ? parameter
over Guo?09.
569
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
M0 M1 IMRe
lati
ve 
gai
n o
f sw
itch
 vs.
 no
 sw
itch
 
Effect of Empty Switch Parameter (s) on HEAD 
No switch
nDCG
MAP
MAPW
Prec@1
Figure 2: The switch parameter ? improves performance
of every model and metric.
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1 Varying K (latent intents) -  TAIL  
0.60.65
0.70.75
0.80.85
0.90.95
1
100 150 200 300 400K  
Model IM -  Varying K (latent intents)  
nDCG
MAP
MAPW
Prec@1
Figure 3: Model performance vs. the number of latent
intents (K).
Our models can also assign a prior type distribu-
tion to each entity by further marginalizing Eq. 1
over query contexts n1 and n2. We measured the
quality of our learned type priors using the subset
of queries in our HEAD test set that consisted of
only an entity without any refiners. The results for
Model IM were: nDCG = 0.86, MAP = 0.80,
MAPW = 0.75, and Prec@1 = 0.70. All met-
rics are statistically significantly better than BFB,
Guo?09 and M0, with 95% confidence. Compared
to Model M1, Model IM is statistically signifi-
cantly better on Prec@1 and not significantly dif-
ferent on the other metrics.
Discussion and Error Analysis: Contrary to
our results, we had expected improvements for
both HEAD and TAIL. Inspection of the TAIL
queries revealed that entities were greatly skewed
towards people (e.g., actor, author, and
politician). Analysis of the latent user in-
tent parameter ? in Model IM showed that most
people types had most of their probability mass
assigned to the same three generic and common in-
tents for people types: ?see pictures of?, ?find bio-
graphical information about?, and ?see video of?. In
other words, latent intents in Model IM are over-
expressive and they do not help in differentiating
people types.
The largest class of errors came from queries
bearing an entity with semantically very similar
types where our highest ranked type was not judged
correct by the annotators. For example, for the
query ?philippine daily inquirer? our system ranked
newspaper ahead of periodical but a judge
rejected the former and approved the latter. For
?ikea catalogue?, our system ranked magazine
ahead of periodical, but again a judge rejected
magazine in favor of periodical.
An interesting success case in the TAIL is high-
lighted by two queries involving the entity ?ymca?,
which in our data can either be a song, place,
or educational institution. Our system
learns the following priors: 0.63, 0.29, and 0.08,
respectively. For the query ?jamestown ymca ny?,
IM correctly classified ?ymca? as a place and for
the query ?ymca palomar? it correctly classified it
as an educational institution. We further
issued the query ?ymca lyrics? and the type song
was then highest ranked.
Our method is generalizable to any entity collec-
tion. Since our evaluation focused on the Freebase
collection, it remains an open question how noise
level, coverage, and breadth in a collection will af-
fect our model performance. Finally, although we
do not formally evaluate it, it is clear that training
our model on different time spans of queries should
lead to type distributions adapted to that time period.
6 Conclusion
Jointly modeling the interplay between the under-
lying user intents and entity types in web search
queries shows significant improvements over the
current state of the art on the task of resolving entity
types in head queries. At the same time, no degrada-
tion in tail queries is observed. Our proposed models
can be efficiently trained using an EM algorithm and
can be further used to assign prior type distributions
to entities in an existing knowledge base and to in-
sert new entities into it.
Although this paper leverages latent intents in
search queries, it stops short of understanding the
nature of the intents. It remains an open problem
to characterize and enumerate intents and to iden-
tify the types of queries that benefit most from intent
models.
570
References
Enrique Alfonseca, Marius Pasca, and Enrique Robledo-
Arnuncio. 2010. Acquisition of instance attributes
via labeled and related instances. In Proceedings of
SIGIR-10, pages 58?65, New York, NY, USA.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Men-
doza. 2004. Query recommendation using query logs
in search engines. In EDBT Workshops, Lecture Notes
in Computer Science, pages 588?596. Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD ?08, pages
1247?1250, New York, NY, USA.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3?10.
Mark James Carman, Fabio Crestani, Morgan Harvey,
and Mark Baillie. 2010. Towards query log based per-
sonalization using topic models. In CIKM?10, pages
1849?1852.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: An
experimental study. volume 165, pages 91?134.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In Proceedings of SIGIR ?11, pages 675?
684, New York, NY, USA. ACM.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proceedings
of SIGIR-09, pages 267?274, New York, NY, USA.
ACM.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545.
Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian tao
Sun, and Zheng Chen. 2009. Understanding user?s
query intent with wikipedia. In WWW, pages 471?480.
Alpa Jain and Marco Pennacchiotti. 2011. Domain-
independent entity extraction from web search query
logs. In Proceedings of WWW ?11, pages 63?64, New
York, NY, USA. ACM.
Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.
2007. Determining the user intent of web search en-
gine queries. In Proceedings of WWW ?07, pages
1149?1150, New York, NY, USA. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. volume 3,
pages 235?244.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07, pages 683?690, New York, NY, USA. ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In SIGKDD, pages 613?619, Ed-
monton, Canada.
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In
COLING, pages 771?777.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations and
clicks. In Proceedings of the 19th international con-
ference on World wide web, WWW ?10, pages 1171?
1172, New York, NY, USA. ACM.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING, pages
931?939.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discov-
ery. In Proceedings of AAAI-09 Spring Symposium on
Learning by Reading and Learning to Read, pages 88?
93.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In Proceedings of
the 13th international conference on World Wide Web,
WWW ?04, pages 13?19, New York, NY, USA. ACM.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL ?11, pages 965?975,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24:97?123,
March.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring on-
tological knowledge from query logs. In Proceedings
of the 16th international conference on World Wide
Web, WWW ?07, pages 1223?1224, New York, NY,
USA. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries. In
WWW, pages 1001?1010.
Z. Zhang and O. Nasraoui. 2006. Mining search en-
gine query logs for query recommendation. In WWW,
pages 1039?1040.
571
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1524?1533,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Smart Selection
Patrick Pantel
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
ppantel@microsoft.com
Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
mgamon@microsoft.com
Ariel Fuxman
Microsoft Research
1065 La Avenida St.
Mountain View, CA 94043, USA
arielf@microsoft.com
Abstract
Natural touch interfaces, common now in
devices such as tablets and smartphones,
make it cumbersome for users to select
text. There is a need for a new text selec-
tion paradigm that goes beyond the high
acuity selection-by-mouse that we have re-
lied on for decades. In this paper, we in-
troduce such a paradigm, called Smart Se-
lection, which aims to recover a user?s in-
tended text selection from her touch input.
We model the problem using an ensemble
learning approach, which leverages mul-
tiple linguistic analysis techniques com-
bined with information from a knowledge
base and a Web graph. We collect a dataset
of true intended user selections and simu-
lated user touches via a large-scale crowd-
sourcing task, which we release to the
academic community. We show that our
model effectively addresses the smart se-
lection task and significantly outperforms
various baselines and standalone linguistic
analysis techniques.
1 Introduction
The process of using a pointing device to select
a span of text has a long history dating back to
the invention of the mouse. It serves to access
functions on text spans, such as copying/pasting,
looking up a word in a dictionary, searching the
Web, or accessing other accelerators. As con-
sumers move from traditional PCs to mobile de-
vices (e.g., tablets and smartphones), touch inter-
action is replacing the pointing devices of yore.
Although more intuitive and arguably a more natu-
ral form of interaction, touch offers much less acu-
ity (colloquially referred to as the fat finger prob-
lem). To select multi-word spans today, mobile
devices require an intricate series of gestures that
results in cumbersome user experiences
1
. Conse-
quently, there is an opportunity to reinvent the way
users select text in such devices.
Our task is, given a single user touch, to pre-
dict the span that the user likely intended to se-
lect. We call this task smart selection. We re-
strict our prediction task to cases where a user in-
tends to perform research on a text span (dictio-
nary/thesaurus lookup, translation, searching). We
specifically consider operations on text spans that
do not form a single unit (i.e., an entity, a concept,
a topic, etc.) to be out of scope. For example, full
sentences, paragraph and page fragments are out
of scope.
Smart selection, as far as we know, is a new re-
search problem. Yet there are many threads of re-
search in the NLP community that identify multi-
word sequences, which have coherent properties.
For example, named-entity recognizers identify
entities such as people/places/organizations, chun-
kers and parsers identify syntactic constituents
such as noun phrases, key phrase detectors or term
segmentors identify term boundaries. While each
of these techniques retrieve meaningful linguistic
units, our problem is a semantic one of recovering
a user?s intent, and as such none alone solves the
entire smart selection problem.
In this paper, we model the problem of smart
selection using an ensemble learning approach.
We leverage various linguistic techniques, such as
those discussed above, and augment them with
other sources of information from a knowledge
1
In order to select a multi-word span, a user would first
have to touch on either word, then drag the left and right
boundary handles to expand it to the adjacent words.
1524
base and a web graph. We evaluate our meth-
ods using a novel dataset constructed for our
task. We construct our dataset of true user-
intended selections by crowdsourcing the task of
a user selecting spans of text in a researching
task. We obtain 13,681 data points. For each in-
tended selection, we construct test cases for each
individual sub-word, simulating the user select-
ing via touch. The resulting testset consists of
33,912 ?simulated selection, intended selection?-
pairs, which we further stratify into head, torso,
and tail subsets. We release the full dataset and
testset to the academic community for further re-
search on this new NLP task. Finally, we empir-
ically show that our ensemble model significantly
improves upon various baseline systems.
In summary, the major contributions of our re-
search are:
? We introduce a new natural language pro-
cessing task, called smart selection, which
aims to address an important problem in text
selection for touch-enabled devices;
? We conduct a large crowd-sourced user study
to collect a dataset of intended selections and
simulated user selections, which we release
to the academic community;
? We propose a machine-learned ensemble
model for smart selection, which combines
various linguistic annotation methods with
information from a large knowledge base and
web graph;
? We empirically show that our model can ef-
fectively address the smart selection task.
2 Related Work
Related work falls into three broad categories: lin-
guistic unit detection, human computer interaction
(HCI), and intent detection.
2.1 Linguistic Unit Detection
Smart selection is closely related to the detection
of syntactic and semantic units: user selections are
often entities, noun phrases, or concepts. A first
approach to solving smart selection is to select an
entity, noun phrase, or concept that subsumes the
user selection. However, no single approach alone
can cover the entire smart selection problem. For
example, consider an approach that uses a state-of-
the-art named-entity recognizer (NER) (Chinchor,
1998; Tjong Kim Sang and De Meulder, 2003;
Finkel et al, 2005; Ratinov and Roth, 2009). We
found in our dataset (see Section 3.2) that only
a quarter of what users intend to select consists
in fact of named entities. Although an NER ap-
proach can be very useful, it is certainly not suf-
ficient. The remainder of the data can be partially
addressed with noun phrase (NP) detectors (Ab-
ney, 1991; Ramshaw and Marcus, 1995; Mu?noz et
al., 1999; Kudo and Matsumoto, 2001) and lists of
items in a knowledge base (KB), but again, each is
not alone sufficient. NP detectors and KB-based
methods are further very susceptible to the gen-
eration of false positives (i.e., text contains many
nested noun phrases and knowledge base items in-
clude highly ambiguous terms).
In our work, we leverage all three techniques in
order to benefit from their complementary cover-
age of user selections. We further create a novel
unit detector, called the hyperlink intent model.
Based on the assumption that Wikipedia anchor
texts are similar in nature to what users would se-
lect in a researching task, it models the problem
of recovering Wikipedia anchor texts from partial
selections.
2.2 Human Computer Interaction
There is a substantial amount of research in the
HCI community on how to facilitate interaction
of a user with touch and speech enabled devices.
To give but a few examples of trends in this field,
Gunawardana et al (2010) address the fat finger
problem in the use of soft keyboards on mobile de-
vices, Kumar et al (2012) explore a novel speech
interaction paradigm for text entry, and Sakamoto
et al (2013) introduce a technique that combines
touch and voice input on a mobile device for im-
proved navigation of user interface elements such
as commands and controls. To the best of our
knowledge, however, the problem of smart selec-
tion as we defined it has not been addressed.
2.3 Intent detection
There is a long line of research in the web lit-
erature on understanding user intent. The clos-
est to smart selection is query recommendation
(Baeza-Yates et al, 2005; Zhang and Nasraoui,
2006; Boldi et al, 2008), where the goal is to sug-
gest queries that may be related to a user?s intent.
Query recommendation techniques are based ei-
ther on clustering queries by their co-clicked URL
patterns (Baeza-Yates et al, 2005) or on leverag-
ing co-occurrences of sequential queries in web
1525
search sessions (Zhang and Nasraoui, 2006; Boldi
et al, 2008; Sadikov et al, 2010). The key dif-
ference from smart selection is that in our task the
output is a selection that is relevant to the context
of the document where the original selection ap-
pears (e.g., by adding terms neighboring the selec-
tion). In query recommendation, however, there is
no notion of a document being read by the user
and, instead, the recommendations are based ex-
clusively on the aggregation of behavior of multi-
ple users.
3 Problem Setting and Data
3.1 Smart Selection Definition
Let D be the set of all documents. We define a
selection to be a character ?offset, length?-tuple in
a document d ? D. Let S be the set of all possible
selections in D and let S
d
be the set of all possible
selections in d.
We define a scored smart selection, ?, in a doc-
ument d, as a pair ? = ?x, y? where x ? S
d
is a
selection and y ? R
+
is a score for the selection.
We formally define the smart selection function
? as producing a ranked scored list of all possi-
ble selections from a document and user selection
pair
2
:
? : D? S? (?
1
, ..., ?
|S
d
|
| x
i
? S
d
, y
i
? y
i+1
)
(1)
Consider a user who selects s in a document d.
Let ? be the target selection that best captures what
the user intended to select. We define the smart
selection task as recovering ? given the pair ?d, s?.
Our problem then is to learn a function ? that best
recovers the target selection from any user selec-
tion.
Note that even for a human, reconstructing an
intended selection from a single word selection is
not trivial. While there are some fairly clear cut
cases such as expanding the selection ?Obama?
to Barack Obama in the sentence ?While in
DC, Barack Obama met with...?, there are cases
where the user intention depends on extrinsic fac-
tors such as the user?s interests. For example, in
a phrase ?University of California at Santa Cruz?
with a selection ?California?, some (albeit proba-
bly few) users may indeed be interested in the state
of California, others in the University
2
The output consists of a ranked list of selections instead
of a single selection to allow experiences such as proposing
an n-best list to the user.
of California system of universities, and
yet others specifically in the University of
California at Santa Cruz. In the next
section, we describe how we obtained a dataset of
true intended user selections.
3.2 Data
In order to obtain a representative dataset for the
smart selection task, we focus on a real-world ap-
plication of users interacting with a touch-enabled
e-reader device. In this application, a user is read-
ing a book and chooses phrases for which she
would like to get information from resources such
as a dictionary, Wikipedia, or web search. Yet, be-
cause of the touch interface, she may only touch
on a single word.
3.2.1 Crowdsourced Intended Selections
We obtain the intended selections through the fol-
lowing crowdsourcing exercise. We use the en-
tire collection of textbooks in English from Wik-
ibooks
3
, a repository of publicly available text-
books. The corpus consists of 2,696 textbooks that
span a large variety of categories such as Comput-
ing, Humanities, Science, etc. We first produce
a uniform random sample of 100 books, and then
sample one paragraph from each book. The result-
ing set of 100 paragraphs is then sent to the crowd-
sourcing system. Each paragraph is evaluated by
100 judges, using a pool of 152 judges. For each
paragraph, we request the judges to select com-
plete phrases for which they would like to ?learn
more in resources such as Wikipedia, search en-
gines and dictionaries?, i.e., our true user intended
selections. As a result of this exercise, we obtain
13,681 judgments, corresponding to 4,067 unique
intended selections. The distribution of number of
unique judges who selected each unique intended
selection, in a log-log scale, is shown in Figure
1. Notice that this is a Zipfian distribution since it
follows a linear trend in the log-log scale.
Intuitively, the likelihood that a phrase is of
interest to a user correlates with the number of
judges who select that phrase. We thus use the
number of judges who selected each phrase as a
proxy for the likelihood that the phrase will be
chosen by users.
The resulting dataset consists of 4,067 ?d, ??-
pairs where d is a Wikibook document paragraph
and ? is an intended selection, along with the num-
ber of judges who selected it. We further assigned
3
Available at http://wikibooks.org.
1526
0
2
4
6
8
10
12
0 1 2 3 4 5 6
LOG 2(
Uniqu
e inte
nded 
select
ions)
LOG2(Unique judges that selected the intended selection)
Figure 1: Zipfian distribution of unique intended
selections vs. the number of judges who selected
them, in log-log scale.
each pair to one of five randomly chosen folds,
which are used for cross-validation experiments.
3.2.2 Testset Construction
We define a test case as a triple ?d, s, ?? where
s is a simulated user selection. For each ?d, ??-
pair in our dataset we construct n correspond-
ing test cases by simulating the user selections
{?d, ?, s
1
?, . . . , ?d, ?, s
n
?}where s
1
, . . . , s
n
corre-
spond to the individual words in ? . In other words,
each word in ? is considered as a candidate user
selection.
We discard all target selections that only a sin-
gle judge annotated since we observed that these
mostly contained errors and noise, such as full sen-
tences or nonsensical long sentence fragments.
Our first testset, labeled T
ALL
, is the resulting
traffic-weighted multiset. That is, each test case
?d, s, ?? appears k times, where k is the number
of judges who selected ? in d. T
ALL
consists of
33,913 test cases.
We further utilize the distribution of judgments
in the creation of three other testsets. Following
the stratified sampling methodology commonly
employed in the IR community, we construct
testsets for the frequently, less frequently, and
rarely annotated intended selections, which we
call HEAD, TORSO, and TAIL, respectively. We
obtain these testsets by first sorting each unique
selection according to their frequency of occur-
rence, and then partitioning the set so that HEAD
corresponds to the elements at the top of the list
that account for 20% of the judgments; TAIL cor-
responds to the elements at the bottom also ac-
counting for 20% of the judgments; and TORSO
corresponds to the remaining elements. The re-
sulting test sets, T
HEAD
, T
TORSO
, T
TAIL
consist of
114, 2115, and 5798 test cases, respectively
4
.
Test sets along with fold assignments
and annotation guidelines are avail-
able at http://research.microsoft.com/en-
us/downloads/eb42522c-068e-404c-b63f-
cf632bd27344/.
3.3 Discussion
Our focus on single word selections is motivated
by the touchscreen scenario presented in Sec-
tion 1. Although our touch simulation assumes
that each word in a target selection is equally likely
to be selected by a user, in fact we expect this dis-
tribution to be non-uniform. For example, users
may tend to select the first or last word more fre-
quently than words in the middle of the target se-
lection. Or perhaps users tend to select nouns and
verbs more frequently than function words. We
consider this out of scope for our paper, but view it
as an important avenue of future investigation. Fi-
nally, for non-touchscreen environments, such as
the desktop case, it would also be interesting to
study the problem on multi-word user selections.
To get an idea of the kind of intended selections
that comprise our dataset, we broke them down ac-
cording to whether they referred to named entities
or not. Perhaps surprisingly, the fraction of named
entities in the dataset is quite low, 24.3%
5
. The
rest of the intended selections mostly correspond
to concepts and topics such as embouchure forma-
tion, vocal fold relaxation, NHS voucher values,
time-domain graphs, etc.
4 Model
As argued in Section 1, existing techniques,
such as NER taggers, chunkers, Knowledge Base
lookup, etc., are geared towards aspects of the
task (i.e., NEs, concepts, KB entries), but not the
task as a whole. We can, however, combine the
outputs of these systems with a learned ?meta-
model?. The meta-model ranks the combined can-
didates according to a criterion that is derived from
data that resembles real usage of smart selection
as closely as possible. This technique is known
4
We stress that T
ALL
is a multi-set, reflecting the over-
all expected user traffic from our 100 judges per paragraph.
T
HEAD
, T
TORSO
, T
TAIL
, in contrast, are not multi-sets since
judgment frequency is already accounted for in the stratifi-
cation process, as commonly done in the IR community.
5
Becker et al (2012) report a similar finding, showing that
only 26% of questions, which a user might ask after reading
a Wikipedia article, are focused on named entities.
1527
in the machine learning community as ensemble
learning (Dietterich, 1997).
Our ensemble approach, described in this sec-
tion, serves as our main implementation of the
smart selection function ? of Equation 1. Each of
the ensemble members are themselves a separate
implementation of ? and will be used as a point
of comparison in our experiments. Below, we de-
scribe the ensemble members before turning to the
ensemble learner.
4.1 Ensemble Members
4.1.1 Hyperlink Intent Model
The Hyperlink Intent Model (HIM), which lever-
ages web graph information, is a machine-learned
system based on the intuition that anchor texts in
Wikipedia are good representations of what users
might want to learn about. We build upon the fact
that Wikipedia editors write anchor texts for enti-
ties, concepts, and things of potential interest for
follow-up to other content. HIM learns to recover
anchor texts from their single word subselections.
Specifically, HIM iteratively decides whether to
expand the current selection (initially a single
word) one word to the left or right via greedy bi-
nary decisions, until a stopping condition is met.
At each step, two binary classifiers are consulted.
The first one scores the left expansion decision
and the second one scores the right expansion de-
cision. In addition, we use the same two classi-
fiers to evaluate the expansion decision ?from the
outside in?, i.e., from the word next to the current
selection (left and right, respectively) to the clos-
est word in the current selection. If the probabil-
ity for expansion of any model exceeds a prede-
fined threshold, then the most probable expansion
is chosen and we continue the iteration with the
newly expanded selection as input. The algorithm
is illustrated in Figure 2.
We automatically create our training set for HIM
by first taking a random sample of 8K Wikipedia
anchor texts. We treat each anchor text as an in-
tended selection, and each word in the anchor text
as a simulated user selection. For each word to the
left (or the right) of the user selection that is part
of the anchor text, we create a positive training ex-
ample. Similarly, for each word to the left (or the
right) that is outside of the anchor text, we create a
negative training example. We include additional
negative examples using random word selections
from Wikipedia content. For this purpose we sam-
Left Context Right Context
Current selection Candidate RightSelectedWord 2
Candidate Left SelectedWord 1
Context 1Context 2
Context 4 Context 3
Figure 2: Hyperlink Intent Model (HIM) decoding
flow for smart selection.
ple random words that are not part of an anchor
text. Our final data consists of 2.6M data points,
with a 1:20 ratio of positive to negative examples
6
.
We use logistic regression as the classification
algorithm for our binary classifiers. The fea-
tures used by each model are computed over three
strings: the current selection s (initially the single-
word simulated user selection), the candidate ex-
pansion word w, and one word over from the
right or left of s. The features fall into five fea-
ture families: (1) character-level features, includ-
ing capitalization, all-cap formatting, character
length, presence of opening/closing parentheses,
presence and position of digits and non-alphabetic
characters, and minimum and average character
uni/bi/trigram frequencies (based on frequency ta-
bles computed offline from Wikipedia article con-
tent); (2) stopword features, which indicate the
presence of a stop word (from a stop word list);
(3) tf.idf scores precomputed from Wikipedia con-
tent statistics; (4) knowledge base features, which
indicate whether a string matches an item or a sub-
string of an item in the knowledge base described
in Section 4.1.2 below; and (5) lexical features,
which capture the actual string of the current se-
lection and the candidate expansion word.
4.1.2 Unit Spotting
Our second qualitative class of ensemble members
use notions of unit that are either based on linguis-
tic constituency or knowledge base presence. The
general process is that any unit that subsumes the
user selection is treated as a smart selection can-
didate. Scoring of candidates is by normalized
length, under the assumption that in general the
most specific (longest) unit is more likely to be the
intended selection.
6
Note that this training set is generated automatically and
is, by design, of a different nature than the manually labeled
data we use to train and test the ensemble model.
1528
Our first unit spotter, labeled NER is geared
towards recognizing named entities. We use
a commercial and proprietary state-of-the-art
NER system, trained using the perceptron algo-
rithm (Collins, 2002) over more than a million
hand-annotated labels.
Our second approach uses purely syntactic in-
formation and treats noun phrases as units. We la-
bel this model as NP. For this purpose we parse the
sentence containing the user selection with a syn-
tactic parser following (Ratnaparkhi, 1999). We
then treat every noun phrase that subsumes the
user selection as a candidate smart selection.
Finally, our third unit spotter, labeled KB, is
based on the assumption that concepts and other
entries in a knowledge base are, by nature, things
that can be of interest to people. For our knowl-
edge base lookup, we use a proprietary graph con-
sisting of knowledge from Wikipedia, Freebase,
and paid feeds from various providers from do-
mains such as entertainment, local, and finance.
4.1.3 Heuristics
Our third family of ensemble members imple-
ments simple heuristics, which tend to be high pre-
cision especially in the HEAD of our data.
The first heuristic, representing the current
touch-enabled selection paradigm seen in many of
today?s tablets and smartphones, is labeled CUR. It
simply assumes that the intended selection is al-
ways the user-selected word.
The second is a capitalization-based heuristic
(CAP), which simply expands every selected capi-
talized word selection to the longest uninterrupted
sequence of capitalized words.
4.2 Ensemble Learning
In this section, we describe how we train our meta-
learner, labeled ENS, which takes as input the can-
didate lists produced by the ensemble members
from Section 4.1, and scores each candidate, pro-
ducing a final scored ranked list.
We use logistic regression as a classification al-
gorithm to address this task. Our 22 features in
ENS consist of three main classes: (1) features
related to the individual ensemble members; (2)
features related to the user selection; and (3) fea-
tures related to the candidate smart selection. For
(1), the features consist of whether a particular
ensemble member generated the candidate smart
selection and its score for that candidate. If the
candidate smart selection is not in the candidate
list of an ensemble member, its score is set to
zero. For both (2) and (3), features account for
length and capitalization properties of the user se-
lection and the candidate smart selection (e.g., to-
ken length, ratio of capitalized tokens, ratio of cap-
italized characters, whether or not the first and last
tokens are capitalized.)
Although training data for the HIM model was
automatically generated from Wikipedia, for ENS
we desire training data that reflects the true ex-
pected user experience. For this, we use five-
fold cross-validation over our data collection de-
scribed in Section 3.2. That is, to decode a fold
with our meta-learner, we train ENS with the other
four folds. Note that every candidate selection for
a ?document, user selection?-pair, ?d, s?, for the
same d and s, are assigned to a single fold, hence
the training process does not see any user selection
from the test set.
5 Experimental Results
5.1 Experimental Setup
Recall our testsets T
ALL
, T
HEAD
, T
TORSO
, and T
TAIL
from Section 3.2.2, where a test case is defined as
a triple ?d, s, ??, and where d is a document, s is a
user selection, and ? is the intended user selection.
In this section, we describe our evaluation metric
and summarize the system configurations that we
evaluate.
5.1.1 Metric
In our evaluation, we apply the smart selection
function ?(d, s) (see Eq. 1) to each test case and
measure how well it recovers ? .
Let A be the set of ?d, ??-pairs from our dataset
described in Section 3.2.1 that corresponds to a
testset T. Let T
?d,??
be the set of all test cases
in T with a fixed d and ? . We define the macro
precision of a smart selection function, P
?
, as fol-
lows:
P
?
=
1
| A |
?
?d,???A
P
?
(d, ?) (2)
P
?
(d, ?) =
1
| T
?d,??
|
?
?d,s,???T
?d,??
P
?
(d, s, ?)
P
?
(d, s, ?) =
1
| ?(d, s) |
?
???(d,s)
I(?, ?)
I(?, ?) =
{
1 if ? = ?x, y? ? x = ?
0 otherwise
1529
CP@1 CP@2 CP@3 CP@4 CP@5
CUR 39.3 - - - -
CAP 48.9 51.0 51.2 51.8 51.8
NER 43.5 - - - -
NP 34.1 50.2 55.5 57.1 57.6
KB 50.2 50.8 50.9 50.9 50.9
HIM 48.1 48.8 48.8 48.8 48.8
ENS 56.8
?
76.0
?
82.6
?
85.2
?
86.6
?
Table 1: Smart selection performance, as a func-
tion of CP, on T
ALL
.
?
and
?
indicate statistical
significance with p = 0.01 and 0.05, respectively.
An oracle ensemble would achieve an upper bound
CP of 87.3%.
We report cumulative macro precision at
rank (CP@k) in our experiments since our
testsets contain a single true user-intended
selection for each test case
7
. However,
this is an overly conservative metric since
in many cases an alternative smart selection
might equally please the user. For example,
if our testset contains a user intended selec-
tion ? = The University of Southern
California, then given the simulated selec-
tion ?California?, both ? and University of
Southern California would most likely
equally satisfy the user intent (whereas the latter
would be considered incorrect in our evaluation).
In fact, the ideal testset would further evaluate the
distance or relevance of the smart selection to the
intended user selection. We would then find per-
haps that Southern California is a more
reasonable smart selection than of Southern
California. However, precisely defining such
a relevance function and designing the guidelines
for a user study is non-trivial and left for future
work.
5.1.2 Systems
In our experiments, we evaluate the follow-
ing systems, each described in detail in Sec-
tion 4: Passthrough (CUR), Capitalization (CAP),
Named-Entity Recognizer (NER), Noun Phrase
(NP), Knowledge Base (KB), Hyperlink Intent
Model (HIM), Ensemble (ENS).
5.2 Results
Table 1 reports the smart selection performance on
the full traffic weighted testset T
ALL
, as a func-
7
Because there is only a single true intended selection for
each test case, Recall@k = CP@k.
tion of CP@k. Our ensemble approach recovers
the true user-intended selection in 56.8% of the
cases. In its top-2 and top-3 ranked smart selec-
tions, the true user-intended selection is retrieved
76.0% and 82.6% of the time, respectively. In po-
sition 1, ENS significantly outperforms all other
systems with 95% confidence. Moreover, we no-
tice that the divergence between ENS and the other
systems greatly increases for K ? 2, where the
significance is now at the 99% level.
The CUR system models the selection paradigm
of today?s consumer touch-enabled devices (i.e., it
assumes that the intented selection is always the
touched word). Without changing the user inter-
face, we report a 45% improvement in predicting
what the user intended to select over this baseline.
If we changed the user interface to allow two or
three options to be displayed to the user, then we
would improve by 93% and 110%, respectively.
For CUR and NER, we report results only at
K = 1 since these systems only ever return a sin-
gle smart selection. Note also that when no named
entity is found by NER, or no noun phrase is found
by NP or no knowledge base entry is found by KB,
the corresponding systems return the original user
selection as their smart selection.
CAP does not vary much across K: when the
intended selection is a capitalized multi-word, the
longest string tends to be the intended selection.
The same holds for KB.
Whereas Table 1 reports the aggregate expected
traffic performance, we further explore the per-
formance against the stratified T
HEAD
, T
TORSO
, and
T
TAIL
testsets. The results are summarized in Ta-
ble 2. As outlined in Section 3.2, the HEAD se-
lections tend to be disproportionately entities and
capitalized terms when compared to the TORSO
and TAIL. Hence CAP, NER and KB perform much
better on the HEAD. In fact, on the HEAD, CAP per-
forms statistically as well as the ENS model. This
means that at position 1, for systems that need to
focus only on the HEAD, a very simple solution is
adequate. For TORSO and TAIL, however, ENS
performs better. At positions 2 and 3, across all
strata, the ENS model significantly outperforms all
other systems (with 99% confidence).
Next, we studied the relative contribution of
each ensemble member to the ENS model. Fig-
ure 3 illustrates the results of the ablation study.
The ensemble member that results in the biggest
performance drop when removed is HIM. Perhaps
1530
HEAD TORSO TAIL
CP@1 CP@2 CP@3 CP@1 CP@2 CP@3 CP@1 CP@2 CP@3
CUR 48.5 - - 36.7 - - 26.6 - -
CAP 74.2 74.7 74.8 43.0 45.0 45.1 26.1 27.4 28.2
NER 60.6 - - 39.2 - - 26.7 - -
NP 52.3 64.9 69.4 31.0 48.2 53.8 20.0 32.2 35.7
KB 66.7 66.7 66.7 47.0 47.9 48.1 29.9 30.1 30.1
HIM 64.4 65.7 65.7 44.7 45.2 45.4 27.9 28.2 28.2
ENS 75.8 91.8
?
96.5
?
52.7
?
73.7
?
81.5
?
32.4
?
50.7
?
58.5
?
Table 2: Smart selection performance, as a function of CP, on the T
HEAD
, T
TORSO
, and T
TAIL
testsets.
?
and
?
indicate statistical significance with p = 0.01 and 0.05, respectively. An oracle ensemble would
achieve an upper bound CP of 98.5%, 86.8% and 64.8% for T
HEAD
, T
TORSO
, and T
TAIL
, respectively.
0.7
0.75
0.8
0.85
0.9
0.95
1
S mart Selection Cumulative Precision @ Rank (ALL)
Ensemble Member Ablation
E NS
-H IM
-KB
-NE R
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
Cumul
ative
 Preci
sion
Rank
Smart Selection Cumulative Precision @ Rank (ALL)
Ensemble Member Ablation
ENS
- HIM
- KB
- NE R
- NP
Figure 3: Ablation of ensemble model members
over T
ALL
. Each consecutive model removes one
member specified in the series name.
surprisingly, a first ablation of either the CAP or
KB model, two of the better individual performing
models from Table 1, leads to an ablated-ENS per-
formance that is nearly identical to the full ENS
model. One possible reason is that both tend to
generate similar candidates (i.e., many entities in
our KB are capitalized). Although the HIM model
as a standalone system does not outperform sim-
ple linguistic unit selection models, it appears to
be the most important contributor to the overall
ensemble.
5.3 Error Analysis: Oracle Ensemble
We begin by assessing an upper bound for our en-
semble, i.e., an oracle ensemble, by assuming that
if a correct candidate is generated by any ensem-
ble member, the oracle ensemble model places it
in first position. For T
ALL
the oracle performance
is 87.3%. In other words, our choice of ensemble
members was able to recover a correct smart se-
lection as a candidate in 87.3% of the user study
cases. For T
HEAD
, T
TORSO
, and T
TAIL
, the oracle
performance is 98.5%, 86.8%, and 64.8%, respec-
tively.
Although our ENS model?s CP@3 is within 2-6
points of the oracle, there is room to significantly
improve our CP@1, see Table 1 and Table 2. We
analyze this opportunity by inspecting a random
sample of 200 test cases where ENS produced an
incorrect smart selection in position 1. The break-
down of these cases is: 1 case from T
HEAD
; 50
cases from T
TORSO
; 149 cases from T
TAIL
, i.e.,
most errors occur in the TAIL.
For 146 of these cases (73%), not a single en-
semble member produced the correct target selec-
tion ? as a candidate. We analyze these cases in
detail in Section 5.4. Of the remaining cases, 25,
10, 9, 4, 4, and 2 were correct in positions 2, 3, 4,
5, 6, 7, respectively. Table 3 lists some examples.
In 18 cases (33%), the result in position 1 is
very reasonable given the context and user selec-
tion (see lines 1-4 in Table 3 for examples). Often
the target selection was also found in second po-
sition. These cases highlight the need for a more
relaxed, relevance-based user study, as pointed out
at the end of Section 5.1.1.
We attributed 7 (13%) of the cases to data prob-
lems: some cases had a punctuation as a sole char-
acter user selection, some had a mishandled es-
caped quotation character, and some had a UTF-8
encoding error.
The remaining 29 (54%) were truly model er-
rors. Some examples are shown in lines 5-8 in Ta-
ble 3. We found three categories of errors here.
First, our model has learned a strong prior on pre-
ferring the original user selection (see example
line 5). From a user experience point of view,
when the model is unsure of itself, it is in fact
better not to alter her selection. Second, we also
learned a strong capitalization prior, i.e., to trust
the CAP member (see example line 6). Finally, we
noticed that we have difficulty handling user selec-
tions consisting of a stopword (we noted determin-
ers, prepositions, and the word ?and?). Adding a
few simple features to ENS based on a stopwords
list or a list of closed-class words should address
this problem.
1531
Text Snippet User Selection ENS 1st Result
1 ?The Russian conquest of the South Caucasus in the 19th century split the
speech community across two states...?
Caucasus South Caucasus
2 ?...are generally something that transportation agencies would like to mini-
mize...?
transportation transportation agencies
3 ?The vocal organ of birds, the syrinx, is located at the base of the blackbird?s
trachea.?
vocal vocal organ
4 ?An example of this may be an idealised waveform like a square wave...? waveform idealised waveform
5 ?Tickets may be purchased from either the ticket counter or from automatic
machines...?
counter counter
6 ?PBXT features include the following: MVCC Support: MVCC stands for
Multi-version Concurrency Control.?
MVCC MVCC Support
7 ?Centers for song production pathways include the High vocal center; ro-
bust nucleus of archistriatum (RA); and the tracheosyringeal part of the hy-
poglossal nucleus...?
robust robust nucleus
8 ?...and get an 11gR2 RAC cluster database running inside virtual ma-
chines...?
cluster RAC cluster
Table 3: Position 1 errors when applying ENS to our test cases. The text snippet is a substring of a
paragraph presented to our judges with the target selection (? ) indicated in bold.
5.4 Error Analysis: Ensemble Members
Over all test cases, the distribution of cases with-
out a correct candidate generated by an ensem-
ble member in the HEAD, TORSO, TAIL is 0.3%,
34.6%, and 65.1%, respectively. We manually in-
spected a random sample of 100 such test cases.
The majority of them, 83%, were large sentence
fragments, which we consider out of scope ac-
cording to our prediction task definition outlined
in Section 1. The average token length of the tar-
get selection ? for these was 15.3. In compari-
son, we estimate the average token length of the
task-admissable cases to be 2.7 tokens. Although
most of these long fragment selections seem to
be noise, a few cases are statements that a user
would reasonably want to know more about, such
as: (i) ?Talks of a merger between the NHL and
the WHA were growing? or (ii) ?NaN + NaN *
1.0i?.
In 10% of the cases, we face a punctuation-
handling issue, and in each case our ensemble was
able to generate a correct candidate when fixing
the punctuation. For example, for the book title
? = What is life?, our ensemble found the
candidate What is life, dropping the ques-
tion mark. For ? = Near Earth Asteroid.
our ensemble found Near Earth Asteroid,
dropping the period. Similar problems occurred
with parentheses and quotation marks.
In two cases, our ensemble members dropped
a leading ?the? token, e.g., for ? = the Hume
Highway, we found Hume Highway.
Finally, 2 cases were UTF-8 encoding mistakes,
leaving five ?true error? cases.
6 Conclusion and Future Work
We introduced a new paradigm, smart selection,
to address the cumbersome text selection capabil-
ities of today?s touch-enabled mobile devices. We
report 45% improvement in predicting what the
user intended to select over current touch-enabled
consumer platforms, such as iOS, Android and
Windows. We release to the community a dataset
of 33, 912 crowdsourced true intended user selec-
tions and corresponding simulated user touches.
There are many avenues for future work, includ-
ing understanding the distribution of user touches
on their intended selection, other interesting sce-
narios (e.g., going beyond the e-reader towards
document editors and web browsers may show dif-
ferent distributions in what users select), leverag-
ing other sources of signal such as a user?s profile,
her interests and her local session context, and ex-
ploring user interfaces that leverage n-best smart
selection prediction lists, for example by provid-
ing selection options to the user after her touch.
With the release of our 33, 912-crowdsourced
dataset and our model analyses, it is our hope that
the research community can help accelerate the
progress towards reinventing the way text selec-
tion occurs today, the initial steps for which we
have taken in this paper.
7 Acknowledgments
The authors thank Aitao Chen for sharing his
NER tagger for our experiments, and Bernhard
Kohlmeier, Pradeep Chilakamarri, Ashok Chan-
dra, David Hamilton, and Bo Zhao for their guid-
ance and valuable discussions.
1532
References
Steven. P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257?278. Kluwer,
Dordrecht.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo
Mendoza. 2005. Query recommendation using
query logs in search engines. In Current Trends
in Database Technology-EDBT 2004 Workshops,
pages 588?596. Springer.
Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the gap: Learning to choose gaps for
question generation. In Proceedings of NAACL HLT
?12, pages 742?751.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applica-
tions. In Proceedings of CIKM ?08, pages 609?618.
ACM.
Nancy A. Chinchor. 1998. Named entity task defini-
tion. In Proceedings of the Seventh Message Under-
standing Conference (MUC-7), Fairfax, VA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Thomas G. Dietterich. 1997. Machine Learning Re-
search - Four Current Directions. AI Magazine,
18:4:97?136.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In In ACL, pages 363?370.
Asela Gunawardana, Tim Paek, and Christopher Meek.
2010. Usability guided key-target resizing for soft
keyboards. In Proceedings of IUI ?10, pages 111?
118.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
?01, pages 1?8.
Anuj Kumar, Tim Paek, and Bongshin Lee. 2012.
Voice typing: A new speech interaction model for
dictation on touchscreen devices. In Proceedings of
CHI?12, pages 2277?2286.
Marcia Mu?noz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP/VLC, pages 168?
178.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the 3rd ACL Workshop on Very
Large Corpora, pages 82?94. Cambridge MA, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-2009, pages 147?155.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34(1-3):151?175, February.
Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon
Halevy. 2010. Clustering query refinements by user
intent. In Proceedings of the 19th international con-
ference on World wide web, pages 841?850. ACM.
Daisuke Sakamoto, Takanori Komatsu, and Takeo
Igarashi. 2013. Voice augmented manipulation: us-
ing paralinguistic information to manipulate mobile
devices. In Mobile HCI, pages 69?78.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142?147. Ed-
monton, Canada.
Zhiyong Zhang and Olfa Nasraoui. 2006. Mining
search engine query logs for query recommendation.
In Proceedings of the 15th international conference
on World Wide Web, pages 1039?1040. ACM.
1533
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 163?171,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Automatically Building Training Examples for Entity Extraction
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA, USA
pennac@yahoo-inc.com
Patrick Pantel
Microsoft Research
Redmond, WA, USA
ppantel@microsoft.com
Abstract
In this paper we present methods for automat-
ically acquiring training examples for the task
of entity extraction. Experimental evidence
show that: (1) our methods compete with a
current heavily supervised state-of-the-art sys-
tem, within 0.04 absolute mean average pre-
cision; and (2) our model significantly out-
performs other supervised and unsupervised
baselines by between 0.15 and 0.30 in abso-
lute mean average precision.
1 Introduction
Entity extraction is a fundamental task in NLP and
related applications. It is broadly defined as the task
of extracting entities of a given semantic class from
texts (e.g., lists of actors, musicians, cities). Search
engines such as Bing, Yahoo, and Google collect
large sets of entities to better interpret queries (Tan
and Peng, 2006), to improve query suggestions (Cao
et al, 2008) and to understand query intents (Hu et
al., 2009). In response, automated techniques for
entity extraction have been proposed (Pas?ca, 2007;
Wang and Cohen, 2008; Chaudhuri et al, 2009; Pan-
tel et al, 2009).
There is mounting evidence that combining
knowledge sources and information extraction sys-
tems yield significant improvements over applying
each in isolation (Pas?ca et al, 2006; Mirkin et al,
2006). This intuition is explored by the Ensem-
ble Semantics (ES) framework proposed by Pennac-
chiotti and Pantel (2009), which outperforms pre-
vious state-of-the-art systems. A severe limitation
of this type of extraction system is its reliance on
editorial judgments for building large training sets
for each semantic class to be extracted. This is
particularly troublesome for applications such as
web search that require large numbers of semantic
classes in order to have a sufficient coverage of facts
and objects (Tan and Peng, 2006). Hand-crafting
training sets across international markets is often in-
feasible. In an exploratory study we estimated that
a pool of editors would need roughly 300 working
days to complete a basic set of 100 English classes
using the ES framework. Critically needed are meth-
ods for automatically building training sets that pre-
serve the extraction quality.
In this paper, we propose simple and intuitively
appealing solutions to automatically build training
sets. Positive and negative training sets for a tar-
get semantic class are acquired by leveraging: i)
?trusted? sources such as structured databases (e.g.,
IMDB or Wikipedia for acquiring a list of Actors);
ii) automatically constructed semantic lexicons; and
iii) instances of semantic classes other than the tar-
get class. Our models focus on extracting training
sets that are large, balanced, and representative of
the unlabeled data. These models can be used in any
extraction setting, where ?trusted? sources of knowl-
edge are available: Today, the popularity of struc-
tured and semi-structured sources such as Wikipedia
and internet databases, makes this approach widely
applicable. As an example, in this paper we show
that our methods can be successfully adapted and
used in the ES framework. This gives us the possi-
bility to test the methods on a large-scale entity ex-
traction task. We replace the manually built training
data in the the ES model with the training data built
163
by our algorithms. We show by means of a large em-
pirical study that our algorithms perform nearly as
good as the fully supervised ES model, within 4% in
absolute mean average precision. Further, we com-
pare the performance of our method against both
Pas?ca et al (2006) and Mirkin et al (2006), show-
ing 17% and 15% improvements in absolute mean
average precision, respectively.
The main contributions of this paper are:
? We propose several general methods for
automatically acquiring labeled training data;
we show that they can be used in a large-scale
extraction framework, namely ES; and
? We show empirical evidence on a large-scale
entity extraction task that our system using
automatically labeled training data performs
nearly as well as the fully-supervised ES
model, and that it significantly outperforms
state-of-the-art systems.
2 Automatic Acquisition of Training Data
Supervised machine learning algorithms require
training data that is: (1) balanced and large enough
to correctly model the problem at hand (Kubat and
Matwin, 1997; Japkowicz and Stephen, 2002); and
(2) representative of the unlabeled data to decode,
i.e., training and unlabeled instances should be ide-
ally drawn from the same distribution (Blumer et al,
1989; Blum and Langley, 1997). If these two prop-
erties are not met, various learning problems, such
as overfitting, can drastically impair predictive ac-
curacy. To address the above properties, a common
approach is to select a subset of the unlabeled data
(i.e., the instances to be decoded), and manually la-
bel them to build the training set.
In this section we propose methods to automate
this task by leveraging the multitude of structured
knowledge bases available on the Web.
Formally, given a target class c, our goal is
to implement methods to automatically build a
training set T (c), composed of both positive and
negative examples, respectively P (c) and N(c);
and to apply T (c) to classify (or rank) a set
of unlabeled data U(c), by using a learning
algorithm. For example, in entity extraction,
given the class Actors, we might have P (c) =
{Brad Pitt, Robert De Niro} and N(c) =
{Serena Williams, Rome, Robert Demiro}.
Below, we define the components of a typical
knowledge acquisition system as in the ES frame-
work, where our methods can be applied :
Sources. Textual repositories of information, ei-
ther structured (e.g., Freebase), semi-structured
(e.g., HTML tables) or unstructured (e.g., a we-
bcrawl). Information sources serve as inputs to the
extraction system, either for the Knowledge Extrac-
tors to generate candidate instances, or for the Fea-
ture Generators to generate features (see below).
Knowledge Extractors (KE). Algorithms re-
sponsible for extracting candidate instances such as
entities or facts. Extractors fall into two categories:
trusted and untrusted. Trusted extractors execute on
structured sources where the contents are deemed to
be highly accurate. Untrusted extractors execute on
unstructured or semi-structured sources and gener-
ally generate high coverage but noisy knowledge.
Feature Generators. Methods that extract evi-
dence (features) of knowledge in order to decide
which extracted candidate instances are correct.
Ranker. A module for ranking the extracted in-
stances using the features generated by the feature
generators. In supervised ML-based rankers, labeled
training instances are required to train the model.
Our goal here is to automatically label training in-
stances thus avoiding the editorial costs.
2.1 Acquiring Positive Examples
Trusted positives: Candidate instances for a class
c that are extracted by a trusted Knowledge Extrac-
tor (e.g., a wrapper induction system over IMDB),
tend to be mostly positive examples. A basic ap-
proach to acquiring a set of positive examples is then
to sample from the unlabeled set U(c) as follows:
P (c) = {i ? U(c) : (?KEi|KEi is trusted} (1)
where KEi is a knowledge extractor that extracted
instance i.
The main advantage of this method is that P (c) is
guaranteed to be highly accurate, i.e., most instances
are true positives. On the downside, instances in
P (c) are not necessarily representative of the un-
trusted KEs. This can highly impact the perfor-
mance of the learning algorithm, which could over-
fit the training data on properties that are specific to
164
the trusted KEs, but that are not representative of the
true population to be decoded (which is largely com-
ing from untrusted KEs).
We therefore enforce that the instances in P (c)
are extracted not only from a trusted KE, but also
from any of the untrusted extractors:
P (c) = {i ? U(c) :?KEi|KEi is trusted ?
?KEj |KEj is untrusted}
(2)
External positives: This method selects the set of
positive examples P (c) from an external repository,
such as an ontology, a database, or an automati-
cally harvested source. The main advantage of this
method is that such resources are widely available
for many knowledge extraction tasks. Yet, the risk
is that P (c) is not representative of the unlabeled in-
stances U(c), as they are drawn from different pop-
ulations.
2.1.1 Acquiring Negative Examples
Acquiring negative training examples is a much
more daunting task (Fagni and Sebastiani, 2007).
The main challenge is to select a set which is a good
representative of the unlabeled negatives in U(c).
Various strategies can be adopted, ranging from
selecting near-miss examples to acquiring generic
ones, each having its own pros and cons. Below
we propose our methods, some building on previous
work described in Section 5.
Near-class negatives: This method selects N(c)
from the population U(C) of the set of classes C
which are semantically similar to c. For example, in
entity extraction, the classes Athletes, Directors
and Musicians are semantically similar to the class
Actors, while Manufacturers and Products are
dissimilar. Similar classes allow us to select negative
examples that are semantic near-misses for the class
c. The hypothesis is the following:
A positive instance extracted for a class similar
to the target class c, is likely to be a near-miss
incorrect instance for c.
To model this hypothesis, we acquire N(c) from
the set of instances having the following two restric-
tions:
1. The instance is most likely correct for C
2. The instance is most likely incorrect for c
Note that restriction (1) alone is not sufficient, as an
instance of C can be at the same time also instance
of c. For example, given the target class Actors, the
instance ?Woody Allen? ? Directors, is not a good
negative example for Actors, since Woody Allen is
both a director and an actor.
In order to enforce restriction (1), we select only
instances that have been extracted by a trusted KE
of C, i.e., the confidence of them being positive is
very high. To enforce (2), we select instances that
have never been extracted by any KE of c. More
formally, we define N(c) as follows:
N(c) =
?
ci?C
P (ci) \ U(c) (3)
The main advantage of this method is that it acquires
negatives that are semantic near-misses of the tar-
get class, thus allowing the learning algorithm to fo-
cus on these borderline cases (Fagni and Sebastiani,
2007). This is a very important property, as most
incorrect instances extracted by unsupervised KEs
are indeed semantic near-misses. On the downside,
the extracted examples are not representative of the
negative examples of the target class c, since they
are drawn from two different distributions.
Generic negatives: This method selects N(c)
from the population U(C) of all classes C different
from the target class c, i.e., both classes semantically
similar and dissimilar to c. The method is very sim-
ilar to the one above, apart from the selection of C,
which now includes any class different from c. The
underlying hypothesis is the following:
A positive instance extracted for a class different
from the target class c, is likely to be an incorrect
instance for c.
This method acquires negatives that are both seman-
tic near-misses and far-misses of the target class.
The learning algorithm is then able to focus both on
borderline cases and on clear-cut incorrect cases, i.e.
the hypothesis space is potentially larger than for the
near-class method. On the downside, the distribu-
tion of c and C are very different. By enlarging the
potential hypothesis space, the risk is then again to
capture hypotheses that overfit the training data on
properties which are not representative of the true
population to be decoded.
165
Same-class negatives: This method selects the
set of negative examples N(c) from the population
U(c). The driving hypothesis is the following:
If a candidate instance for a class c has been ex-
tracted by only one KE and this KE is untrusted,
then the instance is likely to be incorrect, i.e., a
negative example for c.
The above hypothesis stems from an intuitive obser-
vation common to many ensemble-based paradigms
(e.g., ensemble learning in Machine Learning): the
more evidence you have of a given fact, the higher is
the probability of it being actually true. In our case,
the fact that an instance has been extracted by only
one untrusted KE, provides weak evidence that the
instance is correct. N(c) is defined as follows:
N(c) = {i ? U(c) : ?! KEi ?KEi is untrusted}
(4)
The main advantage of this method is that the ac-
quired instances in N(c) are good representatives of
the negatives that will have to be decoded, i.e., they
are drawn from the same distribution U(c). This al-
lows the learning algorithm to focus on the typical
properties of the incorrect examples extracted by the
pool of KEs.
A drawback of this method is that instances in
N(c) are not guaranteed to be true negatives. It fol-
lows that the final training set may be noisy. Two
main strategies can be applied to mitigate this prob-
lem: (1) Use a learning algorithm which is robust to
noise in the training data; and (2) Adopt techniques
to automatically reduce or eliminate noise. We here
adopt the first solution, and leave the second as a
possible avenue for future work, as described in Sec-
tion 6. In Section 4 we demonstrate the amount of
noise in our training data, and show that its impact
is not detrimental for the overall performance of the
system.
3 A Use Case: Entity Extraction
Entity extraction is a fundamental task in NLP
(Cimiano and Staab, 2004; McCarthy and Lehnert,
2005) and web search (Chaudhuri et al, 2009; Hu
et al, 2009; Tan and Peng, 2006), responsible for
extracting instances of semantic classes (e.g., ?Brad
Pitt? and ?Tom Hanks? are instances of the class Ac-
tors). In this section we apply our methods for auto-
matically acquiring training data to the ES entity ex-
traction system described in Pennacchiotti and Pan-
tel (2009).1
The system relies on the following three knowl-
edge extractors. KEtrs: a ?trusted? database
wrapper extractor acquiring entities from sources
such as Yahoo! Movies, Yahoo! Music and Yahoo!
Sports, for extracting respectively Actors, Musicians
and Athletes. KEpat: an ?untrusted? pattern-based
extractor reimplementing Pas?ca et al?s (2006) state-
of-the-art web-scale fact extractor. KEdis: an ?un-
trusted? distributional extractor implementing a vari-
ant of Pantel et al?s (2009).
The system includes four feature generators,
which compute a total of 402 features of various
types extracted from the following sources: (1) a
body of 600 million documents crawled from the
Web at Yahoo! in 2008; (2) one year of web search
queries issued to Yahoo! Search; (3) all HTML inner
tables extracted from the above web crawl; (4) an
official Wikipedia dump from February 2008, con-
sisting of about 2 million articles.
The system adopts as a ranker a supervised
Gradient Boosted Decision Tree regression model
(GBDT) (Friedman, 2001). GBDT is generally con-
sidered robust to noisy training data, and hence is a
good choice given the errors introduced by our auto-
matic training set construction algorithms.
3.1 Training Data Acquisition
The positive and negative components of the training
set for GBDT are built using the methods presented
in Section 2, as follows:
Trusted positives (Ptrs and Pcls): According to
Eq. 2, we acquire a set of positive instances Pcls
as a random sample of the instances extracted by
both KEtrs and either: KEdis, KEpat or both of
them. As a basic variant, we also experiment with
the simpler definition in Eq. 1, i.e. we acquire a set
of positive instances Ptrs as a random sample of the
instances extracted by the trusted extractor KEtrs,
irrespective of KEdis and KEpat.
External positives (Pcbc): Any external repository
of positive examples would serve here. In our spe-
1We here give a summary description of our implementation
of that system. Refer to the original paper for more details.
166
cific implementation, we select a set of positive ex-
amples from the CBC repository (Pantel and Lin,
2002). CBC is a word clustering algorithm that
groups instances appearing in similar textual con-
texts. By manually analyzing the cluster members
in the repository created by CBC, it is easy to pick-
up the cluster(s) representing a target class.
Same-class negatives (Ncls): We select a set of
negative instances as a random sample of the in-
stances extracted by only one extractor, which can
be either of the two untrusted ones, KEdis or
KEpat.
Near-class negatives (Noth): We select a set of
negative instances, as a random sample of the in-
stances extracted by any of our three extractors for a
class different than the one at hand. We also enforce
the condition that instances in Noth must not have
been extracted for the class at hand.
Generic negatives (Ncbc): We automatically se-
lect as generic negatives a random sample of in-
stances appearing in any CBC cluster, except those
containing at least one member of the class at hand
(i.e., containing at least one instance extracted by
one of our KEs for the given class).
4 Experimental Evaluation
In this section, we report experiments comparing
the ranking performance of our different methods
for acquiring training data presented in Section 3,
to three different baselines and a fully supervised
upper-bound.
4.1 Experimental Setup
We evaluate over three semantic classes: Actors
(movie, tv and stage actors); Athletes (profes-
sional and amateur); Musicians (singers, musicians,
composers, bands, and orchestras), so to compare
with (Pennacchiotti and Pantel, 2009). Ranking per-
formance is tested over the test set described in the
above paper, composed of 500 instances, randomly
selected from the instances extracted by KEpat and
KEdis for each of the classes2.
We experiment with various instantiations of the
ES system, each trained on a different training set
2We do not test over instances extracted by KEtrs, as they
do not go though the decoding phase
obtained from our methods. The different system in-
stantiations (i.e., different training sets) are reported
in Table 1 (Columns 1-3). Each training set consists
of 500 positive examples, and 500 negative exam-
ples.
As an upper bound, we use the ES system, where
the training consists of 500 manually annotated in-
stances (Pman and Nman), randomly selected from
those extracted by the KEs. This allows us to di-
rectly check if our automatically acquired training
sets can compete to the human upper-bound. We
also compare to the following baselines.
Baseline 1: An unsupervised rule-based ES sys-
tem, assigning the lowest score to instances ex-
tracted by only one KE, when the KE is untrusted;
and assigning the highest score to any other instance.
Baseline 2: An unsupervised rule-based ES sys-
tem, adopting as KEs the two untrusted extractors
KEpat and KEdis, and a rule-based Ranker that as-
signs scores to instances according to the sum of
their normalized confidence scores.
Baseline 3: An instantiation of our ES system,
trained on Pman and Nman. The only differ-
ence with the upper-bound is that it uses only two
features, namely the confidence score returned by
KEdis and KEpat. This instantiation implements
the system presented in (Mirkin et al, 2006).
For evaluation, we use average precision (AP), a
standard information retrieval measure for evaluat-
ing ranking algorithms:
AP (L) =
?|L|
i=1 P (i) ? corr(i)
?|L|
i=1 corr(i)
(5)
where L is a ranked list produced by a system, P (i)
is the precision of L at rank i, and corr(i) is 1 if the
instance at rank i is correct, and 0 otherwise.
In order to accurately compute statistical signifi-
cance, we divide the test set in 10-folds, and com-
pute the AP mean and variance obtained over the
10-folds. For each configuration, we perform the
random sampling of the training set five times, re-
building the model each time, to estimate the vari-
ance when varying the training sampling.
4.2 Experimental Results
Table 1 reports average precision (AP) results for
different ES instantiations, separately on the three
167
System Training Set AP MAP
Positives Negatives Actors Athletes Musicians
Baseline1 (unsup.) - - 0.562 0.535 0.437 0.511
Baseline2 (unsup.) - - 0.676 0.664 0.576 0.639
Baseline3 (sup.) Pman Nman 0.715 0.697 0.576 0.664
Upper-bound (full-sup.) Pman Nman 0.860? 0.901? 0.786? 0.849?
S1. Pcls Noth 0.751? 0.880? 0.642 0.758?
S2. Pcls Ncbc 0.734? 0.854? 0.644 0.744?
S3. Pcls Ncls 0.842? 0.806? 0.770? 0.806?
S4. Pcls Noth + Ncbc 0.756? 0.853? 0.693? 0.767?
S5. Pcls Ncls + Noth 0.835? 0.807? 0.763? 0.802?
S6. Pcls Ncls + Ncbc 0.838? 0.822? 0.768? 0.809?
S7. Pcls Ncls + Noth + Ncbc 0.838? 0.818? 0.764? 0.807?
Table 1: Average precision (AP) results of systems using different training sets, compared to two usupervised Base-
lines, a supervised Baseline, and a fully supervised upper-bound system. ? indicates statistical significance at the 0.95
level wrt all Baselines. ? indicates statistical significance at the 0.95 level wrt Baseline1 and Baseline 2. ? indicates
statistical significance at the 0.95 level wrt Baseline1.
classes; and the mean average precision (MAP)
computed across the classes. We report results us-
ing Pcls as positive training, and varying the neg-
ative training composition3. Systems S1-S3 use a
single method to build the negatives. Systems S4-
S6 combine two methods (250 examples from one
method, 250 from the other), and S7 combines all
three methods. Table 3 reports additional basic re-
sults when varying the positive training set compo-
sition, and fixing the best performing negative set
(namely Ncls).
Table 1 shows that all systems outperform the
baselines in MAP, with 0.95 statistical significance,
but S2 which is not significant wrt Baseline 3. S6 is
the best performing system, achieving 0.809 MAP,
only 4% below the supervised upper-bound (statis-
tically insignificant at the 0.95 level). These results
indicate that our methods for automatically acquir-
ing training data are highly effective and competitive
with manually crafted training sets.
A class-by-class analysis reveals similar behav-
ior for Actors and Musicians. For these two classes,
the best negative set is Ncls (system S3), achieving
alone the best AP (respectively 0.842 and 0.770 for
Actors and Musicians, 2.1% and 1.6% points below
the upper-bound). Noth and Ncbc show a lower ac-
curacy, more than 10% below Ncls. This suggest
that the most promising strategy for automatically
3For space limitation we cannot report exhaustively all com-
binations.
Negative set False Negatives
Actors Athletes Musicians
Ncls 5% 45% 30%
Noth 0% 10% 10%
Ncbc 0% 0% 15%
Table 2: Percentage of false negatives in different types of
negative sets, across the three experimented classes (esti-
mations over a random sample of 20 examples per class).
acquiring negative training data is to collect exam-
ples from the target class, as they guarantee to be
drawn from the same distribution as the instances to
be decoded. The use of near- and far-misses is still
valuable (AP results are still better than the base-
lines), but less effective.
Results for Athletes give different evidence: the
best performing negative set is Noth, performing
significantly better than Ncls. To investigate this
contrasting result, we manually picked 20 exam-
ples from Ncls, Noth and Ncbc for each class, and
checked their degree of noise, i.e., how many false
negatives they contain. Table 2 reports the results:
these numbers indicate that the Ncls is very noisy
for the Athletes class, while it is more clean for the
other two classes. This suggests that the learning
algorithm, while being robust enough to cope with
the small noise in Ncls for Actors and Musicians, it
starts to diverge when too many false negatives are
presented for training, as it happens for Athletes.
False negatives in Ncls are correct instances ex-
tracted by one untrusted KE alone. The results in
168
Table 2 indicates that our untrusted KEs are more
accurate in extracting instances for Athletes than for
the other classes: accurate enough to make our train-
ing set too noisy, thus decreasing the performance
of S3 wrt S1 and S2. This indicates that the effec-
tiveness of Ncls decreases when the accuracy of the
untrusted KEs is higher.
A good strategy to avoid the above problem is to
pair Ncls with another negative set, either Ncbc or
Noth, as in S5 and S6, respectively. Then, when
the above problem is presented, the learning algo-
rithm can rely on the other negative set to com-
pensate some for the noise. Indeed, when adding
Ncbc to Ncls (system S6) the accuracy over Athletes
improves, while the overall performance across all
classes (MAP) is kept constant wrt the system using
Ncls (S3).
It is interesting that in Table 2, Ncbc and Noth also
have a few false negatives. An intrinsic analysis re-
veals that these are either: (1) Incorrect instances
of the other classes that are actual instances of the
target class; (2) Correct instances of other classes
that are also instances of the target class. Case (1) is
caused by errors of KEs for the other classes (e.g.,
erroneously extracting ?Matthew Flynt? as a Musi-
cian). Case (2) covers cases in which instances are
ambiguous across classes, for example ?Kerry Tay-
lor? is both an Actor and a Musician. This observa-
tion is still surprising, since Eq. 3 explicitly removes
from Ncbc and Noth any correct instance of the tar-
get class extracted by the KEs. The presence of false
negatives is then due to the low coverage of the KEs
for the target class, e.g. the KEs were not able to ex-
tract ?Matthew Flynt? and ?Kerry Taylor? as actors.
Correlations. We computed the Spearman corre-
lation coefficient r among the rankings produced
by the different system instantiations, to verify
how complementary the information enclosed in the
training sets are for building the learning model.
Among the basic systems S1? S3, the highest cor-
relation is between S1 and S2 (r = 0.66 in aver-
age across all classes), which is expected, since they
both apply the principle of acquiring negative ex-
amples from classes other than the target one. S3
exhibits lower correlation with both S1 and S2, re-
spectively r = 0.57 and r = 0.53, suggesting that it
is complementary to them. Also, the best system S6
System Training Set AP MAP
Pos. Neg. Act. Ath. Mus.
S3. Pcls Ncls 0.842 0.806 0.770 0.806
S8. Ptrs Ncls 0.556 0.779 0.557 0.631
S9. Pcbc Ncls 0.633 0.521 0.561 0.571
Table 3: Comparative average precision (AP) results for
systems using different positive sets as training data.
Figure 1: Average precision of system S6 with different
training sizes.
has higher correlation with S3 (r = 0.94) than with
S2 (r = 0.62), indicating that in the combination of
Ncls and Ncbc, most of the model is built on Ncls.
Varying the positive training. Table 3 reports re-
sults when fixing the negative set to the best per-
forming Ncls, and exploring the use of other posi-
tive sets. As expected Pcls largely outperforms Ptrs,
confirming that removing the constraint in Eq. 2 and
using the simpler Eq. 1 makes the training set unrep-
resentative of the unlabeled population. A similar
observation stands for Pcbc. These results indicate
that having a good trusted KE, or even an external
resource of positives, is effective only when select-
ing from the training set examples that are also ex-
tracted by the untrusted KEs.
Varying the training size. In Figure 1 we report
an analysis of the AP achieved by the best perform-
ing System (S6), when varying the training size, i.e.,
changing the cardinality of Pcls and Ncls + Ncbc.
The results show that a relatively small-sized train-
ing set offers good performance, the plateau being
reached already with 500 training examples. This
is an encouraging result, showing that our methods
can potentially be applied also in cases where few
examples are available, e.g., for rare or not well-
represented classes.
169
5 Related Work
Most relevant are efforts in semi-supervised learn-
ing. Semi-supervised systems use both labeled and
unlabeled data to train a machine learning system.
Most common techniques are based on co-training
and self-training. Co-training uses a small set of la-
beled examples to train two classifiers at the same
time. The classifiers use independent views (i.e.
?conditionally independent? feature sets) to repre-
sent the labeled examples. After the learning phase,
the most confident predictions of each classifier
on the unlabeled data are used to increase the la-
beled set of the other. These two phases are re-
peated until a stop condition is met. Co-training
has been successfully applied to various applica-
tions, such as statistical parsing (Sarkar, 2001) and
web pages classification (Yarowsky, 1998). Self-
training techniques (or bootsrapping) (Yarowsky,
1995) start with a small set of labeled data, and it-
eratively classify unlabeled data, selecting the most
confident predictions as additional training. Self-
training has been applied in many NLP tasks, such
as word sense disambiguation (Yarowsky, 1995) and
relation extraction (Hearst, 1992). Unlike typical
semi-supervised approaches, our approach reduces
the needed amount of labeled data not by acting on
the learning algorithm itself (any algorithm can be
used in our approach), but on the method to acquire
the labeled training data.
Our work also relates to the automatic acquisi-
tion of labeled negative training data. Yangarber et
al. (2002) propose a pattern-based bootstrapping ap-
proach for harvesting generalized names (e.g., dis-
eases, locations), where labeled negative examples
for a given class are taken from positive seed exam-
ples of ?competing? classes (e.g. examples of dis-
eases are used as negatives for locations). The ap-
proach is semi-supervised, in that it requires some
manually annotated seeds. The study shows that
using competing categories improves the accuracy
of the system, by avoiding sematic drift, which is
a common cause of divergence in boostrapping ap-
proaches. Similar approaches are used among others
in (Thelen and Riloff, 2002) for learning semantic
lexicons, in (Collins and Singer, 1999) for named-
entity recognition, and in (Fagni and Sebastiani,
2007) for hierarchical text categorization. Some of
our methods rely on the same intuition described
above, i.e., using instances of other classes as nega-
tive training examples. Yet, the ES framework al-
lows us to add further restrictions to improve the
quality of the data.
6 Conclusion
We presented simple and general techniques for au-
tomatically acquiring training data, and then tested
them in the context of the Ensemble Semantics
framework. Experimental results show that our
methods can compete with supervised systems us-
ing manually crafted training data. It is our hope that
these simple and easy-to-implement methods can al-
leviate some of the cost of building machine learn-
ing architectures for supporting open-domain infor-
mation extraction, where the potentially very large
number of classes to be extracted makes infeasible
the use of manually labeled training data.
There are many avenues for future work. Al-
though our reliance on high-quality knowledge
sources is not an issue for many head classes, it
poses a challenge for tail classes such as ?wine con-
noisseurs?, where finding alternative sources of high
precision samples is important. We also plan to ex-
plore techniques to automatically identify and elim-
inate mislabeled examples in the training data as
in (Rebbapragada and Brodley, 2007), and relax the
boolean assumption of trusted/untrusted extractors
into a graded one. Another important issue regards
the discovery of ?near-classes? for collecting near-
classes negatives: we plan to automate this step by
adapting existing techniques as in (McIntosh, 2010).
Finally, we plan to experiment on a larger set of
classes, to show the generalizability of the approach.
Our current work focuses on leveraging auto-
learning to create an extensive taxonomy of classes,
which will constitute the foundation of a very large
knowledge-base for supporting web search.
References
Avrim L. Blum and Pat Langley. 1997. Selection of rel-
evant features and examples in machine learning. Ar-
tificial Intelligence, 97:245?271.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. War-
muth. 1989. Proceedings of ltc-07. Journal of ACM,
36:929?965.
170
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao,
Enhong Chen, and Hang Li. 2008. Context-aware
query suggestion by mining click-through and session
data. In Proceedings of KDD-08, pages 875?883.
Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms for
entities. In Proceedings of WWW-09, pages 151?160.
Philipp Cimiano and Steffen Staab. 2004. Learning by
googling. SIGKDD Explorations, 6(2):24?34.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
WVLC/EMNLP-99, pages 100?110.
Tiziano Fagni and Fabrizio Sebastiani. 2007. On the se-
lection of negative examples for hierarchical text cate-
gorization. In Proceedings of LTC-07, pages 24?28.
Jerome H. Friedman. 2001. Greedy function approxima-
tion: A gradient boosting machine. Annals of Statis-
tics, 29(5):1189?1232.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING-92, pages 539?545.
Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun, and
Zheng Chen. 2009. Understanding user?s query intent
with Wikipedia. In Proceedings of WWW-09, pages
471?480.
N. Japkowicz and S. Stephen. 2002. The class imbalance
problem: A systematic study. Intelligent Data Analy-
sis, 6(5).
M. Kubat and S. Matwin. 1997. Addressing the curse
of inbalanced data sets: One-side sampleing. In Pro-
ceedings of the ICML-1997, pages 179?186. Morgan
Kaufmann.
Joseph F. McCarthy and Wendy G Lehnert. 2005. Using
decision trees for coreference resolution. In Proceed-
ings of IJCAI-1995, pages 1050?1055.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 356?365, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of ACL/COLING-06, pages 579?586.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In Proceedings of
AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-07, pages 683?690, New York, NY,
USA.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD-02, pages
613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 238?247, Singapore.
Association for Computational Linguistics.
Umaa Rebbapragada and Carla E. Brodley. 2007. Class
noise mitigation through instance weighting. In Pro-
ceedings of the 18th European Conference on Machine
Learning.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In NAACL-2001.
Bin Tan and Fuchun Peng. 2006. Unsupervised query
segmentation using generative language models and
wikipedia. In Proceedings of WWW-06, pages 1400?
1405.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 214?221, Philadelphia, PA, USA. As-
sociation for Computational Linguistics.
Richard C. Wang and William W. Cohen. 2008. Itera-
tive set expansion of named entities using the web. In
ICDM ?08: Proceedings of the 2008 Eighth IEEE In-
ternational Conference on Data Mining, pages 1091?
1096, Washington, DC, USA. IEEE Computer Society.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In COLING-2002.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL-1996, pages 189?196.
David Yarowsky. 1998. Combining labeled and unla-
beled data with co-training. In Proceedings of the
Workshop on Computational Learning Theory, pages
92?100.
171
