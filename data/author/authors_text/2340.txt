Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 141?150,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Nested Named Entity Recognition
Jenny Rose Finkel and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel|manning}@cs.stanford.edu
Abstract
Many named entities contain other named
entities inside them. Despite this fact, the
field of named entity recognition has al-
most entirely ignored nested named en-
tity recognition, but due to technological,
rather than ideological reasons. In this pa-
per, we present a new technique for rec-
ognizing nested named entities, by using
a discriminative constituency parser. To
train the model, we transform each sen-
tence into a tree, with constituents for each
named entity (and no other syntactic struc-
ture). We present results on both news-
paper and biomedical corpora which con-
tain nested named entities. In three out
of four sets of experiments, our model
outperforms a standard semi-CRF on the
more traditional top-level entities. At the
same time, we improve the overall F-score
by up to 30% over the flat model, which is
unable to recover any nested entities.
1 Introduction
Named entity recognition is the task of finding en-
tities, such as people and organizations, in text.
Frequently, entities are nested within each other,
such as Bank of China and University of Wash-
ington, both organizations with nested locations.
Nested entities are also common in biomedical
data, where different biological entities of inter-
est are often composed of one another. In the
GENIA corpus (Ohta et al, 2002), which is la-
beled with entity types such as protein and DNA,
roughly 17% of entities are embedded within an-
other entity. In the AnCora corpus of Spanish and
Catalan newspaper text (Mart?? et al, 2007), nearly
half of the entities are embedded. However, work
on named entity recognition (NER) has almost en-
tirely ignored nested entities and instead chosen to
focus on the outermost entities.
We believe this has largely been for practical,
not ideological, reasons. Most corpus designers
have chosen to skirt the issue entirely, and have
annotated only the topmost entities. The widely
used CoNLL (Sang and Meulder, 2003), MUC-6,
and MUC-7 NER corpora, composed of American
and British newswire, are all flatly annotated. The
GENIA corpus contains nested entities, but the
JNLPBA 2004 shared task (Collier et al, 2004),
which utilized the corpus, removed all embedded
entities for the evaluation. To our knowledge, the
only shared task which has included nested enti-
ties is the SemEval 2007 Task 9 (Ma?rquez et al,
2007b), which used a subset of the AnCora corpus.
However, in that task all entities corresponded to
particular parts of speech or noun phrases in the
provided syntactic structure, and no participant di-
rectly addressed the nested nature of the data.
Another reason for the lack of focus on nested
NER is technological. The NER task arose in the
context of the MUC workshops, as small chunks
which could be identified by finite state models
or gazetteers. This then led to the widespread
use of sequence models, first hidden Markov mod-
els, then conditional Markov models (Borthwick,
1999), and, more recently, linear chain conditional
random fields (CRFs) (Lafferty et al, 2001). All
of these models suffer from an inability to model
nested entities.
In this paper we present a novel solution to the
problem of nested named entity recognition. Our
model explicitly represents the nested structure,
allowing entities to be influenced not just by the
labels of the words surrounding them, as in a CRF,
but also by the entities contained in them, and in
which they are contained. We represent each sen-
tence as a parse tree, with the words as leaves, and
with phrases corresponding to each entity (and a
node which joins the entire sentence). Our trees
look just like syntactic constituency trees, such as
those in the Penn TreeBank (Marcus et al, 1993),
141
ROOT
PROT
PROT
NN
PEBP2
PROT
NN
alpha
NN
A1
,
,
PROT
NN
alpha
NN
B1
,
,
CC
and
PROT
NN
alpha
NN
B2
NNS
proteins
VBD
bound
DT
the
DNA
PROT
NN
PEBP2
NN
site
IN
within
DT
the
DNA
NN
mouse
PROT
NN
GM-CSF
NN
promoter
.
.
Figure 1: An example of our tree representation over nested named entities. The sentence is from the
GENIA corpus. PROT is short for PROTEIN.
but they tend to be much flatter. This model allows
us to include parts of speech in the tree, and there-
fore to jointly model the named entities and the
part of speech tags. Once we have converted our
sentences into parse trees, we train a discrimina-
tive constituency parser similar to that of (Finkel
et al, 2008). We found that on top-level enti-
ties, our model does just as well as more conven-
tional methods. When evaluating on all entities
our model does well, with F-scores ranging from
slightly worse than performance on top-level only,
to substantially better than top-level only.
2 Related Work
There is a large body of work on named en-
tity recognition, but very little of it addresses
nested entities. Early work on the GENIA cor-
pus (Kazama et al, 2002; Tsuruoka and Tsujii,
2003) only worked on the innermost entities. This
was soon followed by several attempts at nested
NER in GENIA (Shen et al, 2003; Zhang et
al., 2004; Zhou et al, 2004) which built hidden
Markov models over the innermost named enti-
ties, and then used a rule-based post-processing
step to identify the named entities containing the
innermost entities. Zhou (2006) used a more elab-
orate model for the innermost entities, but then
used the same rule-based post-processing method
on the output to identify non-innermost entities.
Gu (2006) focused only on proteins and DNA, by
building separate binary SVM classifiers for inner-
most and outermost entities for those two classes.
Several techniques for nested NER in GENIA
where presented in (Alex et al, 2007). Their first
approach was to layer CRFs, using the output of
one as the input to the next. For inside-out lay-
ering, the first CRF would identify the innermost
entities, the next layer would be over the words
and the innermost entities to identify second-level
entities, etc. For outside-in layering the first CRF
would identify outermost entities, and then succes-
sive CRFs would identify increasingly nested en-
tities. They also tried a cascaded approach, with
separate CRFs for each entity type. The CRFs
would be applied in a specified order, and then
each CRF could utilize features derived from the
output of previously applied CRFs. This technique
has the problem that it cannot identify nested en-
tities of the same type; this happens frequently in
the data, such as the nested proteins at the begin-
ning of the sentence in Figure 1. They also tried a
joint labeling approach, where they trained a sin-
gle CRF, but the label set was significantly ex-
panded so that a single label would include all of
the entities for a particular word. Their best results
where from the cascaded approach.
Byrne (2007) took a different approach, on his-
torical archive text. She modified the data by con-
catenating adjacent tokens (up to length six) into
potential entities, and then labeled each concate-
nated string using the C&C tagger (Curran and
Clark, 1999). When labeling a string, the ?previ-
ous? string was the one-token-shorter string con-
taining all but the last token of the current string.
For single tokens the ?previous? token was the
longest concatenation starting one token earlier.
SemEval 2007 Task 9 (Ma?rquez et al, 2007b)
included a nested NER component, as well as
noun sense disambiguation and semantic role la-
beling. However, the parts of speech and syn-
tactic tree were given as part of the input, and
named entities were specified as corresponding to
noun phrases in the tree, or particular parts of
speech. This restriction substantially changes the
task. Two groups participated in the shared task,
but only one (Ma?rquez et al, 2007a) worked on
the named entity component. They used a multi-
label AdaBoost.MH algorithm, over phrases in the
142
DNAparent=ROOT
NNparent=DNA,grandparent=ROOT
mouse
@DNAparent=ROOT,prev=NN,first=PROT
PROTparent=DNA,grandparent=ROOT
NNparent=PROT,grandparent=DNA
GM-CSF
NNparent=DNA,grandparent=ROOT
promoter
Figure 2: An example of a subtree after it has been annotated and binarized. Features are computed over
this representation. An @ indicates a chart parser active state (incomplete constituent).
parse tree which, based on their labels, could po-
tentially be entities.
Finally, McDonald et al (2005) presented a
technique for labeling potentially overlapping seg-
ments of text, based on a large margin, multilabel
classification algorithm. Their method could be
used for nested named entity recognition, but the
experiments they performed were on joint (flat)
NER and noun phrase chunking.
3 Nested Named Entity Recognition as
Parsing
Our model is quite simple ? we represent each sen-
tence as a constituency tree, with each named en-
tity corresponding to a phrase in the tree, along
with a root node which connects the entire sen-
tence. No additional syntactic structure is rep-
resented. We also model the parts of speech as
preterminals, and the words themselves as the
leaves. See Figure 1 for an example of a named
entity tree. Each node is then annotated with both
its parent and grandparent labels, which allows
the model to learn how entities nest. We bina-
rize our trees in a right-branching manner, and
then build features over the labels, unary rules,
and binary rules. We also use first-order horizon-
tal Markovization, which allows us to retain some
information about the previous node in the bina-
rized rule. See Figure 2 for an example of an an-
notated and binarized subtree. Once each sentence
has been converted into a tree, we train a discrimi-
native constituency parser, based on (Finkel et al,
2008).
It is worth noting that if you use our model on
data which does not have any nested entities, then
it is precisely equivalent to a semi-CRF (Sarawagi
and Cohen, 2004; Andrew, 2006), but with no
length restriction on entities. Like a semi-CRF, we
are able to define features over entire entities of
arbitrary length, instead of just over a small, fixed
window of words like a regular linear chain CRF.
We model part of speech tags jointly with the
named entities, though the model also works with-
out them. We determine the possible part of
speech tags based on distributional similarity clus-
ters. We used Alexander Clarke?s software,1 based
on (Clark, 2003), to cluster the words, and then
allow each word to be labeled with any part of
speech tag seen in the data with any other word
in the same cluster. Because the parts of speech
are annotated with the parent (and grandparent)
labels, they determine what, if any, entity types
a word can be labeled with. Many words, such as
verbs, cannot be labeled with any entities. We also
limit our grammar based on the rules observed in
the data. The rules whose children include part of
speech tags restrict the possible pairs of adjacent
tags. Interestingly, the restrictions imposed by this
joint modeling (both observed word/tag pairs and
observed rules) actually result in much faster infer-
ence (and therefore faster train and test times) than
a model over named entities alone. This is differ-
ent from most work on joint modeling of multiple
levels of annotation, which usually results in sig-
nificantly slower inference.
3.1 Discriminative Constituency Parsing
We train our nested NER model using the same
technique as the discriminatively trained, condi-
tional random field-based, CRF-CFG parser of
(Finkel et al, 2008). The parser is similar to a
1http://www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
143
Local Features Pairwise Features
labeli distsimi + distsimi?1 + labeli labeli?1 + labeli
wordi + labeli shapei + shapei+1 + labeli wordi + labeli?1 + labeli
wordi?1 + labeli shapei?1 + shapei + labeli wordi?1 + labeli?1 + labeli
wordi+1 + labeli wordi?1 + shapei + labeli wordi+1 + labeli?1 + labeli
distsimi + labeli shapei + wordi+1 + labeli distsimi + labeli?1 + labeli
distsimi?1 + labeli words in a 5 word window distsimi?1 + labeli?1 + labeli
distsimi+1 + labeli prefixes up to length 6 distsimi+1 + labeli?1 + labeli
shapei + labeli suffixes up to length 6 distsimi?1 + distsimi + labeli?1 + labeli
shapei?1 + labeli shapei + labeli?1 + labeli
shapei+1 + labeli shapei?1 + labeli?1 + labeli
shapei+1 + labeli?1 + labeli
shapei?1 + shapei + labeli?1 + labeli
shapei?1 + shapei+1 + labeli?1 + labeli
Table 1: The local and pairwise NER features used in all of our experiments. Consult the text for a full
description of all features, which includes feature classes not in this table.
chart-based PCFG parser, except that instead of
putting probabilities over rules, it puts clique po-
tentials over local subtrees. These unnormalized
potentials know what span (and split) the rule is
over, and arbitrary features can be defined over the
local subtree, the span/split and the words of the
sentence. The inside-outside algorithm is run over
the clique potentials to produce the partial deriva-
tives and normalizing constant which are neces-
sary for optimizing the log likelihood. Optimiza-
tion is done by stochastic gradient descent.
The only real drawback to our model is run-
time. The algorithm is O(n3) in sentence length.
Training on all of GENIA took approximately 23
hours for the nested model and 16 hours for the
semi-CRF. A semi-CRF with an entity length re-
striction, or a regular CRF, would both have been
faster. At runtime, the nested model for GENIA
tagged about 38 words per second, while the semi-
CRF tagged 45 words per second. For compar-
ison, a first-order linear chain CRF trained with
similar features on the same data can tag about
4,000 words per second.
4 Features
When designing features, we first made ones sim-
ilar to the features typically designed for a first-
order CRF, and then added features which are not
possible in a CRF, but are possible in our enhanced
representation. This includes features over entire
entities, features which directly model nested en-
tities, and joint features over entities and parts of
speech. When features are computed over each
label, unary rule, and binary rule, the feature func-
tion is aware of the rule span and split.
Each word is labeled with its distributional sim-
ilarity cluster (distsim), and a string indicating
orthographic information (shape) (Finkel et al,
2005). Subscripts represent word position in the
sentence. In addition to those below, we include
features for each fully annotated label and rule.
Local named entity features. Local named en-
tity features are over the label for a single word.
They are equivalent to the local features in a linear
chain CRF. However, unlike in a linear chain CRF,
if a word belongs to multiple entities then the local
features are computed for each entity. Local fea-
tures are also computed for words not contained in
any entity. Local features are in Table 1.
Pairwise named entity features. Pairwise fea-
tures are over the labels for adjacent words, and
are equivalent to the edge features in a linear chain
CRF. They can occur when pairs of words have
the same label, or over entity boundaries where
the words have different labels. Like with the lo-
cal features, if a pair of words are contained in, or
straddle the border of, multiple entities, then the
features are repeated for each. The pairwise fea-
tures we use are shown in Table 1.
Embedded named entity features. Embedded
named entity features occur in binary rules where
one entity is the child of another entity. For our
embedded features, we replicated the pairwise fea-
tures, except that the embedded named entity was
treated as one of the words, where the ?word?
(and other annotations) were indicative of the type
of entity, and not the actual string that is the en-
tity. For instance, in the subtree in Figure 2, we
would compute wordi+labeli?1+labeli as PROT-
DNA-DNA for i = 18 (the index of the word GM-
CSF). The normal pairwise feature at the same po-
144
GENIA ? Testing on All Entities
Nested NER Model Semi-CRF Model
# Test (train on all entities) (train on top-level entities)
Entities Precision Recall F1 Precision Recall F1
Protein 3034 79.04 69.22 73.80 78.63 64.04 70.59
DNA 1222 69.61 61.29 65.19 71.62 57.61 63.85
RNA 103 86.08 66.02 74.73 79.27 63.11 70.27
Cell Line 444 73.82 56.53 64.03 76.59 59.68 67.09
Cell Type 599 68.77 65.44 67.07 72.12 59.60 65.27
Overall 5402 75.39 65.90 70.33 76.17 61.72 68.19
Table 2: Named entity results on GENIA, evaluating on all entities.
GENIA ? Testing on Top-level Entities Only
Nested NER Model Semi-CRF Model
# Test (train on all entities) (train on top-level entities)
Entities Precision Recall F1 Precision Recall F1
Protein 2592 78.24 72.42 75.22 76.16 72.61 74.34
DNA 1129 70.40 64.66 67.41 71.21 62.00 66.29
RNA 103 86.08 66.02 74.73 79.27 63.11 70.27
Cell Line 420 75.54 58.81 66.13 76.59 63.10 69.19
Cell Type 537 69.36 70.39 69.87 71.11 65.55 68.22
Overall 4781 75.22 69.02 71.99 74.57 68.27 71.28
Table 3: Named entity results on GENIA, evaluating on only top-level entities.
sition would be GM-CSF-DNA-DNA.
Whole entity features. We had four whole en-
tity features: the entire phrase; the preceding and
following word; the preceding and following dis-
tributional similarity tags; and the preceding dis-
tributional similarity tag with the following word.
Local part of speech features. We used the
same POS features as (Finkel et al, 2008).
Joint named entity and part of speech features.
For the joint features we replicated the POS fea-
tures, but included the parent of the POS, which
either is the innermost entity type, or would indi-
cate that the word is not in any entities.
5 Experiments
We performed two sets of experiments, the first set
over biomedical data, and the second over Spanish
and Catalan newspaper text. We designed our ex-
periments to show that our model works just as
well on outermost entities, the typical NER task,
and also works well on nested entities.
5.1 GENIA Experiments
5.1.1 Data
We performed experiments on the GENIA v.3.02
corpus (Ohta et al, 2002). This corpus contains
2000 Medline abstracts (?500k words), annotated
with 36 different kinds of biological entities, and
with parts of speech. Previous NER work using
this corpus has employed 10-fold cross-validation
for evaluation. We wanted to explore different
model variations (e.g., level of Markovization, and
different sets of distributional similarity cluster-
ings) and feature sets, so we needed to set aside
a development set. We split the data by putting
the first 90% of sentences into the training set, and
the remaining 10% into the test set. This is the
exact same split used to evaluate part of speech
tagging in (Tsuruoka et al, 2005). For develop-
ment we used the first half of the data to train, and
the next quarter of the data to test.2 We made the
same modifications to the label set as the organiz-
ers of the JNLPBA 2004 shared task (Collier et
al., 2004). They collapsed all DNA subtypes into
DNA; all RNA subtypes into RNA; all protein sub-
types into protein; kept cell line and cell type; and
removed all other entities. However, they also re-
moved all embedded entities, while we kept them.
As discussed in Section 3, we annotated each
word with a distributional similarity cluster. We
used 200 clusters, trained using 200 million words
from PubMed abstracts. During development, we
found that fewer clusters resulted in slower infer-
2This split may seem strange: we had originally intended
a 50/25/25 train/dev/test split, until we found the previously
used 90/10 split.
145
JNLPBA 2004 ? Testing on Top-level Entities Only
Nested NER Model Semi-CRF Model Zhou & Su (2004)
# Test (train on all entities) (train on top-level entities)
Entities Precision Recall F1 Precision Recall F1 Precision Recall F1
Protein 4944 66.98 74.58 70.57 68.15 62.68 65.30 69.01 79.24 73.77
DNA 1030 62.96 66.50 64.68 65.45 52.23 58.10 66.84 73.11 69.83
RNA 115 63.06 60.87 61.95 64.55 61.74 63.11 64.66 63.56 64.10
Cell line 487 49.92 60.78 54.81 49.61 52.16 50.85 53.85 65.80 59.23
Cell type 1858 75.12 65.34 69.89 73.29 55.81 63.37 78.06 72.41 75.13
Overall 8434 66.78 70.57 68.62 67.50 59.27 63.12 69.42 75.99 72.55
Table 4: Named entity results on the JNLPBA 2004 shared task data. Zhou and Su (2004) was the best
system at the shared task, and is still state-of-the-art on the dataset.
ROOT
SP
A
At
AQ
doble
double
NC
partido
match
FC
,
,
ORGANIZATION
DA
el
the
ORGANIZATION
NP
Barc?a
Barc?a
VS
es
is
DA
el
the
AQ
favorito
favorite
FE
?
?
FC
,
,
VM
afirma
states
PERSON
PERSON
NP
Makaay
Makaay
PERSON
FC
,
,
NC
delantero
attacker
SP
del
of
ORGANIZATION
NP
Deportivo
Deportivo
FP
.
.
Figure 3: An example sentence from the AnCora corpus, along with its English translation.
ence with no improvement in performance.
5.1.2 Experimental Setup
We ran several sets of experiments, varying be-
tween all entities, or just top-level entities, for
training and testing. As discussed in Section 3, if
we train on just top-level entities then the model is
equivalent to a semi-CRF. Semi-CRFs are state-
of-the-art and provide a good baseline for per-
formance on just the top-level entities. Semi-
CRFs are strictly better than regular, linear chain
CRFs, because they can use all of the features and
strucutre of a linear chain CRF, but also utilize
whole-entity features (Andrew, 2006). We also
evaluated the semi-CRF model on all entities. This
may seem like an unfair evaluation, because the
semi-CRF has no way of recovering the nested en-
tities, but we wanted to illustrate just how much
information is lost when using a flat representa-
tion.
5.1.3 Results
Our named entity results when evaluating on all
entities are shown in Table 2 and when evaluat-
ing on only top-level entities are shown in Table 3.
Our nested model outperforms the flat semi-CRF
on both top-level entities and all entities.
While not our main focus, we also evaluated
our models on parts of speech. The model trained
on just top level entities achieved POS accuracy
of 97.37%, and the one trained on all entities
achieved 97.25% accuracy. The GENIA tagger
(Tsuruoka et al, 2005) achieves 98.49% accuracy
using the same train/test split.
5.1.4 Additional JNLPBA 2004 Experiments
Because we could not compare our results on the
NER portion of the GENIA corpus with any other
work, we also evaluated on the JNLPBA corpus.
This corpus was used in a shared task for the
BioNLP workshop at Coling in 2004 (Collier et
al., 2004). They used the entire GENIA corpus for
training, and modified the label set as discussed in
Section 5.1.1. They also removed all embedded
entities, and kept only the top-level ones. They
then annotated new data for the test set. This
dataset has no nested entities, but because the
training data is GENIA we can still train our model
on the data annotated with nested entities, and then
evaluate on their test data by ignoring all embed-
ded entities found by our named entity recognizer.
146
AnCora Spanish ? Testing on All Entities
Nested NER Model Semi-CRF Model
# Test (train on all entities) (train on top-level entities)
Entities Precision Recall F1 Precision Recall F1
Person 1778 65.29 78.91 71.45 75.10 32.73 45.59
Organization 2137 86.43 56.90 68.62 47.02 26.20 33.65
Location 1050 78.66 46.00 58.05 84.94 13.43 23.19
Date 568 87.13 83.45 85.25 79.43 29.23 42.73
Number 991 81.51 80.52 81.02 66.27 28.15 39.52
Other 512 17.90 64.65 28.04 10.77 16.60 13.07
Overall 7036 62.38 66.87 64.55 51.06 25.77 34.25
Table 5: Named entity results on the Spanish portion of AnCora, evaluating on all entities.
AnCora Spanish ? Testing on Top-level Entities Only
Nested NER Model Semi-CRF Model
# Test (train on all entities) (train on top-level entities)
Entities Precision Recall F1 Precision Recall F1
Person 1050 57.42 66.67 61.70 71.23 52.57 60.49
Organization 1060 77.38 40.66 53.31 44.33 49.81 46.91
Location 279 72.49 36.04 48.15 79.52 24.40 37.34
Date 290 72.29 57.59 64.11 71.77 51.72 60.12
Number 519 57.17 49.90 53.29 54.87 44.51 49.15
Other 541 11.30 38.35 17.46 9.51 26.88 14.04
Overall 3739 50.57 49.72 50.14 46.07 44.61 45.76
Table 6: Named entity results on the Spanish portion of AnCora, evaluating on only top-level entities.
This experiment allows us to show that our named
entity recognizer works well on top-level entities,
by comparing it with prior work. Our model also
produces part of speech tags, but the test data is
not annotated with POS tags, so we cannot show
POS tagging results on this dataset.
One difficulty we had with the JNLPBA exper-
iments was with tokenization. The version of GE-
NIA distributed for the shared task is tokenized
differently from the original GENIA corpus, but
we needed to train on the original corpus as it is
the only version with nested entities. We tried our
best to retokenize the original corpus to match the
distributed data, but did not have complete suc-
cess. It is worth noting that the data is actually to-
kenized in a manner which allows a small amount
of ?cheating.? Normally, hyphenated words, such
as LPS-induced, are tokenized as one word. How-
ever, if the portion of the word before the hyphen
is in an entity, and the part after is not, such as
BCR-induced, then the word is split into two to-
kens: BCR and -induced. Therefore, when a word
starts with a hyphen it is a strong indicator that the
prior word and it span the right boundary of an en-
tity. Because the train and test data for the shared
task do not contain nested entities, fewer words
are split in this manner than in the original data.
We did not intentionally exploit this fact in our
feature design, but it is probable that some of our
orthographic features ?learned? this fact anyway.
This probably harmed our results overall, because
some hyphenated words, which straddled bound-
aries in nested entities and would have been split
in the original corpus (and were split in our train-
ing data), were not split in the test data, prohibiting
our model from properly identifying them.
For this experiment, we retrained our model on
the entire, retokenized, GENIA corpus. We also
retrained the distributional similarity model on the
retokenized data. Once again, we trained one
model on the nested data, and one on just the top-
level entities, so that we can compare performance
of both models on the top-level entities. Our full
results are shown in Table 4, along with the cur-
rent state-of-the-art (Zhou and Su, 2004). Besides
the tokenization issues harming our performance,
Zhou and Su (2004) also employed clever post-
processing to improve their results.
5.2 AnCora Experiments
5.2.1 Data
We performed experiments on the NER portion
of AnCora (Mart?? et al, 2007). This corpus has
Spanish and Catalan portions, and we evaluated
on both. The data is also annotated with parts
of speech, parse trees, semantic roles and word
147
AnCora Catalan ? Testing on All Entities
Nested NER Model Semi-CRF Model
# Test (train all entities) (train top-level entities only)
Entities Precision Recall F1 Precision Recall F1
Person 1303 89.01 50.35 64.31 70.08 46.20 55.69
Organization 1781 68.95 83.77 75.64 65.32 41.77 50.96
Location 1282 76.78 72.46 74.56 75.49 36.04 48.79
Date 606 84.27 81.35 82.79 70.87 38.94 50.27
Number 1128 86.55 83.87 85.19 75.74 38.74 51.26
Other 596 85.48 8.89 16.11 64.91 6.21 11.33
Overall 6696 78.09 68.23 72.83 70.39 37.60 49.02
Table 7: Named entity results on the Catalan portion of AnCora, evaluating on all entities.
AnCora Catalan ? Testing on Top-level Entities Only
Nested NER Model Semi-CRF Model
# Test (train all entities) (train top-level entities only)
Entities Precision Recall F1 Precision Recall F1
Person 801 67.44 47.32 55.61 62.63 67.17 64.82
Organization 899 52.21 74.86 61.52 57.68 73.08 64.47
Location 659 54.86 67.68 60.60 62.42 57.97 60.11
Date 296 62.54 66.55 64.48 59.46 66.89 62.96
Number 528 62.35 70.27 66.07 63.08 68.94 65.88
Other 342 49.12 8.19 14.04 45.61 7.60 13.03
Overall 3525 57.67 59.40 58.52 60.53 61.42 60.97
Table 8: Named entity results on the Catalan portion of AnCora, evaluating on only top-level entities.
senses. The corpus annotators made a distinction
between strong and weak entities. They define
strong named entities as ?a word, a number, a date,
or a string of words that refer to a single individual
entity in the real world.? If a strong NE contains
multiple words, it is collapsed into a single token.
Weak named entities, ?consist of a noun phrase,
being it simple or complex? and must contain a
strong entity. Figure 3 shows an example from the
corpus with both strong and weak entities. The
entity types present are person, location, organi-
zation, date, number, and other. Weak entities are
very prevalent; 47.1% of entities are embedded.
For Spanish, files starting with 7?9 were the test
set, 5?6 were the development test set, and the re-
mainder were the development train set. For Cata-
lan, files starting with 8?9 were the test set, 6?7
were the development test set, and the remainder
were the development train set. For both, the de-
velopment train and test sets were combined to
form the final train set. We removed sentences
longer than 80 words. Spanish has 15,591 train-
ing sentences, and Catalan has 14,906.
5.2.2 Experimental Setup
The parts of speech provided in the data include
detailed morphological information, using a sim-
ilar annotation scheme to the Prague TreeBank
(Hana and Hanova?, 2002). There are around 250
possible tags, and experiments on the development
data with the full tagset where unsuccessful. We
removed all but the first two characters of each
POS tag, resulting in a set of 57 tags which more
closely resembles that of the Penn TreeBank (Mar-
cus et al, 1993). All reported results use our mod-
ified version of the POS tag set.
We took only the words as input, none of the
extra annotations. For both languages we trained a
200 cluster distributional similarity model over the
words in the corpus. We performed the same set
of experiments on AnCora as we did on GENIA.
5.2.3 Results and Discussion
The full results for Spanish when testing on all en-
tities are shown in Table 5, and for only top-level
entities are shown in Table 6. For part of speech
tagging, the nested model achieved 95.93% accu-
racy, compared with 95.60% for the flatly trained
model. The full results for Catalan when testing on
all entities are shown in Table 7, and for only top-
level entities are shown in Table 8. POS tagging
results were even closer on Catalan: 96.62% for
the nested model, and 96.59% for the flat model.
It is not surprising that the models trained on
all entities do significantly better than the flatly
trained models when testing on all entities. The
148
story is a little less clear when testing on just top-
level entities. In this case, the nested model does
4.38% better than the flat model on the Spanish
data, but 2.45% worse on the Catalan data. The
overall picture is the same as for GENIA: model-
ing the nested entities does not, on average, reduce
performance on the top-level entities, but a nested
entity model does substantially better when evalu-
ated on all entities.
6 Conclusions
We presented a discriminative parsing-based
method for nested named entity recognition,
which does well on both top-level and nested enti-
ties. The only real drawback to our method is that
it is slower than common flat techniques. While
most NER corpus designers have defenestrated
embedded entities, we hope that in the future this
will not continue, as large amounts of information
are lost due to this design decision.
Acknowledgements
Thanks to Mihai Surdeanu for help with the An-
Cora data. The first author was supported by
a Stanford Graduate Fellowship. This paper is
based on work funded in part by the Defense Ad-
vanced Research Projects Agency through IBM.
The content does not necessarily reflect the views
of the U.S. Government, and no official endorse-
ment should be inferred.
References
Beatrice Alex, Barry Haddow, and Claire Grover. 2007.
Recognising nested named entities in biomedical text. In
BioNLP Workshop at ACL 2007, pages 65?72.
Galen Andrew. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In Pro-
ceedings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2006).
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York Uni-
versity.
Kate Byrne. 2007. Nested named entity recognition in his-
torical archive text. In ICSC ?07: Proceedings of the Inter-
national Conference on Semantic Computing, pages 589?
596.
Alexander Clark. 2003. Combining distributional and mor-
phological information for part of speech induction. In
Proceedings of the tenth Annual Meeting of the European
Association for Computational Linguistics (EACL), pages
59?66.
Nigel Collier, J. Kim, Y. Tateisi, T. Ohta, and Y. Tsuruoka, ed-
itors. 2004. Proceedings of the International Joint Work-
shop on NLP in Biomedicine and its Applications.
J. R. Curran and S. Clark. 1999. Language independent NER
using a maximum entropy tagger. In CoNLL 1999, pages
164?167.
Jenny Finkel, Shipra Dingare, Christopher Manning, Malv-
ina Nissim, Beatrice Alex, and Claire Grover. 2005. Ex-
ploring the boundaries: Gene and protein identification in
biomedical text. In BMC Bioinformatics 6 (Suppl. 1).
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Man-
ning. 2008. Efficient, feature-based conditional random
field parsing. In ACL/HLT-2008.
Baohua Gu. 2006. Recognizing nested named entities in GE-
NIA corpus. In BioNLP Workshop at HLT-NAACL 2006,
pages 112?113.
Jir??? Hana and Hana Hanova?. 2002. Manual for morpholog-
ical annotation. Technical Report TR-2002-14, UK MFF
CKL.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proceedings
of the Workshop on Natural Language Processing in the
Biomedical Domain (ACL 2002).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In ICML
2001, pages 282?289. Morgan Kaufmann, San Francisco,
CA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
L. Ma?rquez, L. Padre`, M. Surdeanu, and L. Villarejo. 2007a.
UPC: Experiments with joint learning within semeval task
9. In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007).
L. Ma?rquez, L. Villarejo, M.A. Mart??, and M. Taule`. 2007b.
Semeval-2007 task 09: Multilevel semantic annotation of
Catalan and Spanish. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations (SemEval-
2007).
M.A. Mart??, M. Taule`, M. Bertran, and L. Ma?rquez. 2007.
Ancora: Multilingual and multilevel annotated corpora.
MS, Universitat de Barcelona.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Flexible text segmentation with structured mul-
tilabel classification. In HLT ?05: Proceedings of the
conference on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages 987?
994.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The
GENIA corpus: an annotated research abstract corpus in
molecular biology domain. In Proceedings of the second
international conference on Human Language Technology
Research, pages 82?86.
149
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings of
CoNLL-2003.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov
conditional random fields for information extraction. In In
Advances in Neural Information Processing Systems 17,
pages 1185?1192.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-
Lim Tan. 2003. Effective adaptation of a hidden markov
model-based named entity recognizer for biomedical do-
main. In Proceedings of the ACL 2003 workshop on Nat-
ural language processing in biomedicine. Association for
Computational Linguistics (ACL 2003).
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Boost-
ing precision and recall of dictionary-based protein name
recognition. In Proceedings of the ACL-03 Workshop on
Natural Language Processing in Biomedicine, pages 41?
48.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, Tomoko
Ohta, John McNaught, Sophia Ananiadou, and Jun?ichi
Tsujii. 2005. Developing a robust part-of-speech tag-
ger for biomedical text. In Advances in Informatics -
10th Panhellenic Conference on Informatics, LNCS 3746,
pages 382?392.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2004. Enhancing HMM-based biomedical named
entity recognition by studying special phenomena. Jour-
nal of Biomedical Informatics, 37(6):411?422.
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recognition.
In Joint Workshop on Natural Language Processing in
Biomedicine and Its Applications at Coling 2004.
Guodong Zhou, Jie Zhang, Jian Su, Dan Shen, and Chewlim
Tan. 2004. Recognizing names in biomedical texts: a
machine learning approach. Bioinformatics, 20(7):1178?
1190.
Guodong Zhou. 2006. Recognizing names in biomedical
texts using mutual information independence model and
SVM plus sigmoid. International Journal of Medical In-
formatics, 75:456?467.
150
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 326?334,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Joint Parsing and Named Entity Recognition
Jenny Rose Finkel and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel|manning}@cs.stanford.edu
Abstract
For many language technology applications,
such as question answering, the overall sys-
tem runs several independent processors over
the data (such as a named entity recognizer, a
coreference system, and a parser). This eas-
ily results in inconsistent annotations, which
are harmful to the performance of the aggre-
gate system. We begin to address this prob-
lem with a joint model of parsing and named
entity recognition, based on a discriminative
feature-based constituency parser. Our model
produces a consistent output, where the named
entity spans do not conflict with the phrasal
spans of the parse tree. The joint represen-
tation also allows the information from each
type of annotation to improve performance
on the other, and, in experiments with the
OntoNotes corpus, we found improvements of
up to 1.36% absolute F1 for parsing, and up to
9.0% F1 for named entity recognition.
1 Introduction
In order to build high quality systems for complex
NLP tasks, such as question answering and textual
entailment, it is essential to first have high quality
systems for lower level tasks. A good (deep analy-
sis) question answering system requires the data to
first be annotated with several types of information:
parse trees, named entities, word sense disambigua-
tion, etc. However, having high performing, low-
level systems is not enough; the assertions of the
various levels of annotation must be consistent with
one another. When a named entity span has crossing
brackets with the spans in the parse tree it is usually
impossible to effectively combine these pieces of in-
formation, and system performance suffers. But, un-
fortunately, it is still common practice to cobble to-
gether independent systems for the various types of
annotation, and there is no guarantee that their out-
puts will be consistent.
This paper begins to address this problem by
building a joint model of both parsing and named
entity recognition. Vapnik has observed (Vapnik,
1998; Ng and Jordan, 2002) that ?one should solve
the problem directly and never solve a more gen-
eral problem as an intermediate step,? implying that
building a joint model of two phenomena is more
likely to harm performance on the individual tasks
than to help it. Indeed, it has proven very diffi-
cult to build a joint model of parsing and seman-
tic role labeling, either with PCFG trees (Sutton and
McCallum, 2005) or with dependency trees. The
CoNLL 2008 shared task (Surdeanu et al, 2008)
was intended to be about joint dependency parsing
and semantic role labeling, but the top performing
systems decoupled the tasks and outperformed the
systems which attempted to learn them jointly. De-
spite these earlier results, we found that combining
parsing and named entity recognition modestly im-
proved performance on both tasks. Our joint model
produces an output which has consistent parse struc-
ture and named entity spans, and does a better job at
both tasks than separate models with the same fea-
tures.
We first present the joint, discriminative model
that we use, which is a feature-based CRF-CFG
parser operating over tree structures augmented with
NER information. We then discuss in detail how
we make use of the recently developed OntoNotes
corpus both for training and testing the model, and
then finally present the performance of the model
and some discussion of what causes its superior per-
formance, and how the model relates to prior work.
326
NP
DT
the
NP
NNP
[District
PP
IN
of
NP
NNP
Columbia] GPE
=?
NP
DT
the
NamedEntity-GPE*
NP-GPE
NNP-GPE
District
PP-GPE
IN-GPE
of
NP-GPE
NNP-GPE
Columbia
Figure 1: An example of a (sub)tree which is modified for input to our learning algorithm. Starting from the normalized
tree discussed in section 4.1, a new NamedEntity node is added, so that the named entity corresponds to a single
phrasal node. That node, and its descendents, have their labels augmented with the type of named entity. The * on the
NamedEntity node indicates that it is the root of the named entity.
2 The Joint Model
When constructing a joint model of parsing and
named entity recognition, it makes sense to think
about how the two distinct levels of annotation may
help one another. Ideally, a named entity should cor-
respond to a phrase in the constituency tree. How-
ever, parse trees will occasionally lack some explicit
structure, such as with right branching NPs. In these
cases, a named entity may correspond to a contigu-
ous set of children within a subtree of the entire
parse. The one thing that should never happen is for
a named entity span to have crossing brackets with
any spans in the parse tree.
For named entities, the joint model should help
with boundaries. The internal structure of the named
entity, and the structural context in which it ap-
pears, can also help with determining the type of
entity. Finding the best parse for a sentence can be
helped by the named entity information in similar
ways. Because named entities should correspond
to phrases, information about them should lead to
better bracketing. Also, knowing that a phrase is a
named entity, and the type of entity, may help in get-
ting the structural context, and internal structure, of
that entity correct.
2.1 Joint Representation
After modifying the OntoNotes dataset to ensure
consistency, which we will discuss in Section 4, we
augment the parse tree with named entity informa-
tion, for input to our learning algorithm. In the cases
where a named entity corresponds to multiple con-
tiguous children of a subtree, we add a new Name-
dEntity node, which is the new parent to those chil-
dren. Now, all named entities correspond to a single
phrasal node in the entire tree. We then augment the
labels of the phrasal node and its descendents with
the type of named entity. We also distinguish be-
tween the root node of an entity, and the descendent
nodes. See Figure 1 for an illustration. This repre-
sentation has several benefits, outlined below.
2.1.1 Nested Entities
The OntoNotes data does not contain any nested en-
tities. Consider the named entity portions of the
rules seen in the training data. These will look, for
instance, like none ? none person, and organization
? organization organization. Because we only al-
low named entity derivations which we have seen in
the data, nested entities are impossible. However,
there is clear benefit in a representation allowing
nested entities. For example, it would be beneficial
to recognize that the United States Supreme Court is
a an organization, but that it also contains a nested
GPE.1 Fortunately, if we encounter data which has
been annotated with nested entities, this representa-
tion will be able to handle them in a natural way.
In the given example, we would have a derivation
which includes organization ? GPE organization.
This information will be helpful for correctly la-
beling nested entities such as New Jersey Supreme
Court, because the model will learn how nested en-
tities tend to decompose.
2.1.2 Feature Representation for Named
Entities
Currently, named entity recognizers are usually con-
structed using sequence models, with linear chain
1As far as we know, GENIA (Kim et al, 2003) is the only
corpus currently annotated with nested entities.
327
conditional random fields (CRFs) being the most
common. While it is possible for CRFs to have links
that are longer distance than just between adjacent
words, most of the benefit is from local features,
over the words and labels themselves, and from fea-
tures over adjacent pairs of words and labels. Our
joint representation allows us to port both types of
features from such a named entity recognizer. The
local features can be computed at the same time the
features over parts of speech are computed. These
are the leaves of the tree, when only the named en-
tity for the current word is known.2 The pairwise
features, over adjacent labels, are computed at the
same time as features over binary rules. Binariza-
tion of the tree is necessary for efficient computa-
tion, so the trees consist solely of unary and bi-
nary productions. Because of this, for all pairs of
adjacent words within an entity, there will be a bi-
nary rule applied where one word will be under the
left child and the other word will be under the right
child. Therefore, we compute features over adjacent
words/labels when computing the features for the bi-
nary rule which joins them.
2.2 Learning the Joint Model
We construct our joint model as an extension to the
discriminatively trained, feature-rich, conditional
random field-based, CRF-CFG parser of (Finkel and
Manning, 2008). Their parser is similar to a chart-
based PCFG parser, except that instead of putting
probabilities over rules, it puts clique potentials over
local subtrees. These unnormalized potentials know
what span (and split) the rule is over, and arbitrary
features can be defined over the local subtree, the
span/split and the words of the sentence. The inside-
outside algorithm is run over the clique potentials to
produce the partial derivatives and normalizing con-
stant which are necessary for optimizing the log like-
lihood.
2.3 Grammar Smoothing
Because of the addition of named entity annota-
tions to grammar rules, if we use the grammar
as read off the treebank, we will encounter prob-
lems with sparseness which severely degrade per-
formance. This degradation occurs because of CFG
2Note that features can include information about other
words, because the entire sentence is observed. The features
cannot include information about the labels of those words.
rules which only occur in the training data aug-
mented with named entity information, and because
of rules which only occur without the named entity
information. To combat this problem, we added ex-
tra rules, unseen in the training data.
2.3.1 Augmenting the Grammar
For every rule encountered in the training data which
has been augmented with named entity information,
we add extra copies of that rule to the grammar. We
add one copy with all of the named entity informa-
tion stripped away, and another copy for each other
entity type, where the named entity augmentation
has been changed to the other entity type.
These additions help, but they are not sufficient.
Most entities correspond to noun phrases, so we took
all rules which had an NP as a child, and made
copies of that rule where the NP was augmented
with each possible entity type. These grammar ad-
ditions sufficed to improve overall performance.
2.3.2 Augmenting the Lexicon
The lexicon is augmented in a similar manner to
the rules. For every part of speech tag seen with a
named entity annotation, we also add that tag with
no named entity information, and a version which
has been augmented with each type of named entity.
It would be computationally infeasible to allow
any word to have any part of speech tag. We there-
fore limit the allowed part of speech tags for com-
mon words based on the tags they have been ob-
served with in the training data. We also augment
each word with a distributional similarity tag, which
we discuss in greater depth in Section 3, and al-
low tags seen with other words which belong to the
same distributional similarity cluster. When decid-
ing what tags are allowed for each word, we initially
ignore named entity information. Once we deter-
mine what base tags are allowed for a word, we also
allow that tag, augmented with any type of named
entity, if the augmented tag is present in the lexicon.
3 Features
We defined features over both the parse rules and the
named entities. Most of our features are over one or
the other aspects of the structure, but not both.
Both the named entity and parsing features utilize
the words of the sentence, as well as orthographic
and distributional similarity information. For each
word we computed a word shape which encoded
328
information about capitalization, length, and inclu-
sion of numbers and other non-alphabetic charac-
ters. For the distributional similarity information,
we had to first train a distributional similarity model.
We trained the model described in (Clark, 2000),
with code downloaded from his website, on several
hundred million words from the British national cor-
pus, and the English Gigaword corpus. The model
we trained had 200 clusters, and we used it to assign
each word in the training and test data to one of the
clusters.
For the named entity features, we used a fairly
standard feature set, similar to those described in
(Finkel et al, 2005). For parse features, we used the
exact same features as described in (Finkel and Man-
ning, 2008). When computing those features, we re-
moved all of the named entity information from the
rules, so that these features were just over the parse
information and not at all over the named entity in-
formation.
Lastly, we have the joint features. We included as
features each augmented rule and each augmented
label. This allowed the model to learn that certain
types of phrasal nodes, such as NPs are more likely
to be named entities, and that certain entities were
more likely to occur in certain contexts and have par-
ticular types of internal structure.
4 Data
For our experiments we used the LDC2008T04
OntoNotes Release 2.0 corpus (Hovy et al, 2006).
The OntoNotes project leaders describe it as ?a
large, multilingual richly-annotated corpus con-
structed at 90% internanotator agreement.? The cor-
pus has been annotated with multiple levels of anno-
tation, including constituency trees, predicate struc-
ture, word senses, coreference, and named entities.
For this work, we focus on the parse trees and named
entities. The corpus has English and Chinese por-
tions, and we used only the English portion, which
itself has been split into seven sections: ABC, CNN,
MNB, NBC, PRI, VOA, and WSJ. These sections
represent a mix of speech and newswire data.
4.1 Data Inconsistencies
While other work has utilized the OntoNotes corpus
(Pradhan et al, 2007; Yu et al, 2008), this is the
first work to our knowledge to simultaneously model
the multiple levels of annotation available. Because
this is a new corpus, still under development, it is
not surprising that we found places where the data
was inconsistently annotated, namely with crossing
brackets between named entity and tree annotations.
In the places where we found inconsistent anno-
tation it was rarely the case that the different lev-
els of annotation were inherently inconsistent, but
rather inconsistency results from somewhat arbitrary
choices made by the annotators. For example, when
the last word in a sentence ends with a period, such
as Corp., one period functions both to mark the ab-
breviation and the end of the sentence. The conven-
tion of the Penn Treebank is to separate the final pe-
riod and treat it as the end of sentence marker, but
when the final word is also part of an entity, that
final period was frequently included in the named
entity annotation, resulting in the sentence terminat-
ing period being part of the entity, and the entity not
corresponding to a single phrase. See Figure 2 for an
illustration from the data. In this case, we removed
the terminating period from the entity, to produce a
consistent annotation.
Overall, we found that 656 entities, out of 55,665
total, could not be aligned to a phrase, or multiple
contiguous children of a node. We identified and
corrected the following sources of inconsistencies:
Periods and abbreviations. This is the problem
described above with the Corp. example. We
corrected it by removing the sentence terminat-
ing final period from the entity annotation.
Determiners and PPs. Noun phrases composed of
a nested noun phrase and a prepositional phrase
were problematic when they also consisted of a
determiner followed by an entity. We dealt with
this by flattening the nested NP, as illustrated in
Figure 3. As we discussed in Section 2.1, this
tree will then be augmented with an additional
node for the entity (see Figure 1).
Adjectives and PPs. This problem is similar to the
previous problem, with the difference being
that there are also adjectives preceding the en-
tity. The solution is also similar to the solution
to the previous problem. We moved the adjec-
tives from the nested NP into the main NP.
These three modifications to the data solved most,
but not all, of the inconsistencies. Another source
of problems was conjunctions, such as North and
South Korea, where North and South are a phrase,
329
SNP
NNP
[Mr.
NNP
Todt]PER
VP
VBD
had
VP
VBN
been
NP
NP
NN
president
PP
IN
of
NP
NNP
[Insilco
NNP
Corp
.
.]ORG
Figure 2: An example from the data of inconsistently labeled named entity and parse structure. The inclusion of the
final period in the named entity results in the named entity structure having crossing brackets with the parse structure.
NP
NP
DT
the
NNP
[District
PP
IN
of
NP
NNP
Columbia] GPE
NP
DT
the
NP
NNP
[District
PP
IN
of
NP
NNP
Columbia] GPE
(a) (b)
Figure 3: (a) Another example from the data of inconsistently labeled named entity and parse structure. In this
instance, we flatten the nested NP, resulting in (b), so that the named entity corresponds to a contiguous set of children
of the top-level NP.
but South Korea is an entity. The rest of the er-
rors seemed to be due to annotation errors and other
random weirdnesses. We ended up unable to make
0.4% of the entities consistent with the parses, so we
omitted those entities from the training and test data.
One more change we made to the data was with
respect to possessive NPs. When we encountered
noun phrases which ended with (POS ?s) or (POS ?),
we modified the internal structure of the NP. Origi-
nally, these NPs were flat, but we introduced a new
nested NP which contained the entire contents of the
original NP except for the POS. The original NP la-
bel was then changed to PossNP. This change is mo-
tivated by the status of ?s as a phrasal affix or clitic:
It is the NP preceding ?s that is structurally equiva-
lent to other NPs, not the larger unit that includes ?s.
This change has the additional benefit in this context
that more named entities will correspond to a single
phrase in the parse tree, rather than a contiguous set
of phrases.
4.2 Named Entity Types
The data has been annotated with eighteen types of
entities. Many of these entity types do not occur
very often, and coupled with the relatively small
amount of data, make it difficult to learn accurate
entity models. Examples are work of art, product,
and law. Early experiments showed that it was dif-
ficult for even our baseline named entity recognizer,
based on a state-of-the-art CRF, to learn these types
of entities.3 As a result, we decided to merge all
but the three most dominant entity types into into
one general entity type called misc. The result was
four distinct entity types: person, organization, GPE
(geo-political entity, such as a city or a country), and
misc.
3The difficulties were compounded by somewhat inconsis-
tent and occasionally questionable annotations. For example,
the word today was usually labeled as a date, but about 10% of
the time it was not labeled as anything. We also found several
strange work of arts, including Stanley Cup and the U.S.S. Cole.
330
Training Testing
Range # Sent. Range # Sent.
ABC 0?55 1195 56?69 199
CNN 0?375 5092 376?437 1521
MNB 0?17 509 18?25 245
NBC 0?29 552 30?39 149
PRI 0?89 1707 90?112 394
VOA 0?198 1512 199?264 383
Table 1: Training and test set sizes for the six datasets in
sentences. The file ranges refer to the numbers within the
names of the original OntoNotes files.
5 Experiments
We ran our model on six of the OntoNotes datasets
described in Section 4,4 using sentences of length
40 and under (approximately 200,000 annotated En-
glish words, considerably smaller than the Penn
Treebank (Marcus et al, 1993)). For each dataset,
we aimed for roughly a 75% train / 25% test split.
See Table 1 for the the files used to train and test,
along with the number of sentences in each.
For comparison, we also trained the parser with-
out the named entity information (and omitted the
NamedEntity nodes), and a linear chain CRF using
just the named entity information. Both the base-
line parser and CRF were trained using the exact
same features as the joint model, and all were op-
timized using stochastic gradient descent. The full
results can be found in Table 2. Parse trees were
scored using evalB (the extra NamedEntity nodes
were ignored when computing evalB for the joint
model), and named entities were scored using entity
F-measure (as in the CoNLL 2003 conlleval).5
While the main benefit of our joint model is the
ability to get a consistent output over both types of
annotations, we also found that modeling the parse
4These datasets all consistently use the new conventions for
treebank annotation, while the seventh WSJ portion is currently
still annotated in the original 1990s style, and so we left the
WSJ portion aside.
5Sometimes the parser would be unable to parse a sentence
(less than 2% of sentences), due to restrictions in part of speech
tags. Because the underlying grammar (ignoring the additional
named entity information) was the same for both the joint and
baseline parsers, it is the case that whenever a sentence is un-
parseable by either the baseline or joint parser it is in fact un-
parsable by both of them, and would affect the parse scores of
both models equally. However, the CRF is able to named entity
tag any sentence, so these unparsable sentences had an effect
on the named entity score. To combat this, we fell back on
the baseline CRF model to get named entity tags for unparsable
sentences.
and named entities jointly resulted in improved per-
formance on both. When looking at these numbers,
it is important to keep in mind that the sizes of the
training and test sets are significantly smaller than
the Penn Treebank. The largest of the six datasets,
CNN, has about one seventh the amount of training
data as the Penn Treebank, and the smallest, MNB,
has around 500 sentences from which to train. Parse
performance was improved by the joint model for
five of the six datasets, by up to 1.36%. Looking
at the parsing improvements on a per-label basis,
the largest gains came from improved identication
of NML consituents, from an F-score of 45.9% to
57.0% (on all the data combined, for a total of 420
NML constituents). This label was added in the new
treebank annotation conventions, so as to identify in-
ternal left-branching structure inside previously flat
NPs. To our surprise, performance on NPs only in-
creased by 1%, though over 12,949 constituents, for
the largest improvement in absolute terms. The sec-
ond largest gain was on PPs, where we improved by
1.7% over 3,775 constituents. We tested the signif-
icance of our results (on all the data combined) us-
ing Dan Bikel?s randomized parsing evaluation com-
parator6 and found that both the precision and recall
gains were significant at p ? 0.01.
Much greater improvements in performance were
seen on named entity recognition, where most of
the domains saw improvements in the range of 3?
4%, with performance on the VOA data improving
by nearly 9%, which is a 45% reduction in error.
There was no clear trend in terms of precision ver-
sus recall, or the different entity types. The first
place to look for improvements is with the bound-
aries for named entities. Once again looking at all of
the data combined, in the baseline model there were
203 entities where part of the entity was found, but
one or both boundaries were incorrectly identified.
The joint model corrected 72 of those entities, while
incorrectly identifying the boundaries of 37 entities
which had previously been correctly identified. In
the baseline NER model, there were 243 entities for
which the boundaries were correctly identified, but
the type of entity was incorrect. The joint model cor-
rected 80 of them, while changing the labels of 39
entities which had previously been correctly identi-
fied. Additionally, 190 entities were found which
the baseline model had missed entirely, and 68 enti-
6Available at http://www.cis.upenn.edu/ dbikel/software.html
331
Parse Labeled Bracketing Named Entities Training
Precision Recall F1 Precision Recall F1 Time
ABC Just Parse 70.18% 70.12% 70.15% ? 25m
Just NER ? 76.84% 72.32% 74.51%
Joint Model 69.76% 70.23% 69.99% 77.70% 72.32% 74.91% 45m
CNN Just Parse 76.92% 77.14% 77.03% ? 16.5h
Just NER ? 75.56% 76.00% 75.78%
Joint Model 77.43% 77.99% 77.71% 78.73% 78.67% 78.70% 31.7h
MNB Just Parse 63.97% 67.07% 65.49% ? 12m
Just NER ? 72.30% 54.59% 62.21%
Joint Model 63.82$ 67.46% 65.59% 71.35% 62.24% 66.49% 19m
NBC Just Parse 59.72% 63.67% 61.63% ? 10m
Just NER ? 67.53% 60.65% 63.90%
Joint Model 60.69% 65.34% 62.93% 71.43% 64.81% 67.96% 17m
PRI Just Parse 76.22% 76.49% 76.35% ? 2.4h
Just NER ? 82.07% 84.86% 83.44%
Joint Model 76.88% 77.95% 77.41% 86.13% 86.56% 86.34% 4.2h
VOA Just Parse 76.56% 75.74% 76.15% ? 2.3h
Just NER ? 82.79% 75.96% 79.23%
Joint Model 77.58% 77.45% 77.51% 88.37% 87.98% 88.18% 4.4h
Table 2: Full parse and NER results for the six datasets. Parse trees were evaluated using evalB, and named entities
were scored using macro-averaged F-measure (conlleval).
ties were lost. We tested the statistical significance
of the gains (of all the data combined) using the
same sentence-level, stratified shuffling technique as
Bikel?s parse comparator and found that both preci-
sion and recall gains were significant at p < 10?4.
An example from the data where the joint model
helped improve both parse structure and named en-
tity recognition is shown in Figure 4. The output
from the individual models is shown in part (a), with
the output from the named entity recognizer shown
in brackets on the words at leaves of the parse. The
output from the joint model is shown in part (b),
with the named entity information encoded within
the parse. In this example, the named entity Egyp-
tian Islamic Jihad helped the parser to get its sur-
rounding context correct, because it is improbable
to attach a PP headed by with to an organization.
At the same time, the surrounding context helped
the joint model correctly identify Egyptian Islamic
Jihad as an organization and not a person. The
baseline parser also incorrectly added an extra level
of structure to the person name Osama Bin Laden,
while the joint model found the correct structure.
6 Related Work
A pioneering antecedent for our work is (Miller et
al., 2000), who trained a Collins-style generative
parser (Collins, 1997) over a syntactic structure aug-
mented with the template entity and template rela-
tions annotations for the MUC-7 shared task. Their
sentence augmentations were similar to ours, but
they did not make use of features due to the gen-
erative nature of their model. This approach was not
followed up on in other work, presumably because
around this time nearly all the activity in named
entity and relation extraction moved to the use of
discriminative sequence models, which allowed the
flexible specification of feature templates that are
very useful for these tasks. The present model is
able to bring together both these lines of work, by
integrating the strengths of both approaches.
There have been other attempts in NLP to jointly
model multiple levels of structure, with varying de-
grees of success. Most work on joint parsing and se-
mantic role labeling (SRL) has been disappointing,
despite obvious connections between the two tasks.
Sutton and McCallum (2005) attempted to jointly
model PCFG parsing and SRL for the CoNLL 2005
shared task, but were unable to improve perfor-
mance on either task. The CoNLL 2008 shared task
(Surdeanu et al, 2008) was joint dependency pars-
ing and SRL, but the top performing systems de-
coupled the tasks, rather than building joint models.
Zhang and Clark (2008) successfully built a joint
332
VP
VBD
were
NP
NP
NNS
members
PP
IN
of
NP
NP
the [Egyptian Islamic Jihad]PER
PP
IN
with
NP
NP
NNS
ties
PP
TO
to
NP
NML
NNP
[Osama
NNP
Bin
NNP
Laden]PER
(a)
VP
VBD
were
NP
NNS
members
PP
IN
of
NP
DT
the
NamedEntity-ORG*
Egyptian Islamic Jihad
PP
IN
with
NP
NP
NNS
ties
PP
TO
to
NP-PER*
NNP-PER
Osama
NNP-PER
Bin
NNP-PER
Laden
(b)
Figure 4: An example for which the joint model helped with both parse structure and named entity recognition. The
individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra
internal structure to Osama Bin Laden. The joint model (b) gets both the structure and the named entity correct.
model of Chinese word segmentation and parts of
speech using a single perceptron.
An alternative approach to joint modeling is to
take a pipelined approach. Previous work on linguis-
tic annotation pipelines (Finkel et al, 2006; Holling-
shead and Roark, 2007) has enforced consistency
from one stage to the next. However, these models
are only used at test time; training of the compo-
nents is still independent. These models also have
the potential to suffer from search errors and are not
guaranteed to find the optimal output.
7 Conclusion
We presented a discriminatively trained joint model
of parsing and named entity recognition, which im-
proved performance on both tasks. Our model
is based on a discriminative constituency parser,
with the data, grammar, and features carefully con-
structed for the joint task. In the future, we would
like to add other levels of annotation available in
the OntoNotes corpus to our model, including word
sense disambiguation and semantic role labeling.
Acknowledgements
The first author is supported by a Stanford Gradu-
ate Fellowship. This paper is based on work funded
in part by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred. We also
wish to thank the creators of OntoNotes, without
which this project would not have been possible.
333
References
Alexander Clark. 2000. Inducing syntactic categories by
context distribution clustering. In Proc. of Conference
on Computational Natural Language Learning, pages
91?94, Lisbon, Portugal.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL 1997.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Efficient, feature-based conditional random field pars-
ing. In ACL/HLT-2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL 2005.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate bayesian inference for linguistic
annotation pipelines. In EMNLP 2006.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In ACL 2007.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In HLT-NAACL 2006.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichi
Tsujii. 2003. Genia corpus ? a semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl.
1):i180?i182.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In In 6th Applied Natu-
ral Language Processing Conference, pages 226?233.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs. generative classifiers: A comparison of logistic
regression and naive bayes. In Advances in Neural In-
formation Processing Systems (NIPS).
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in ontonotes. International Conference on Semantic
Computing, 0:446?453.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL), Manchester, UK.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In Conference on Nat-
ural Language Learning (CoNLL).
V. N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
Liang-Chih Yu, Chung-Hsien Wu, and Eduard Hovy.
2008. OntoNotes: Corpus cleanup of mistaken agree-
ment using word sense disambiguation. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 1057?1064.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
ACL 2008.
334
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 602?610,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Bayesian Domain Adaptation
Jenny Rose Finkel and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel|manning}@cs.stanford.edu
Abstract
Multi-task learning is the problem of maxi-
mizing the performance of a system across a
number of related tasks. When applied to mul-
tiple domains for the same task, it is similar to
domain adaptation, but symmetric, rather than
limited to improving performance on a target
domain. We present a more principled, better
performing model for this problem, based on
the use of a hierarchical Bayesian prior. Each
domain has its own domain-specific parame-
ter for each feature but, rather than a constant
prior over these parameters, the model instead
links them via a hierarchical Bayesian global
prior. This prior encourages the features to
have similar weights across domains, unless
there is good evidence to the contrary. We
show that the method of (Daume? III, 2007),
which was presented as a simple ?prepro-
cessing step,? is actually equivalent, except
our representation explicitly separates hyper-
parameters which were tied in his work. We
demonstrate that allowing different values for
these hyperparameters significantly improves
performance over both a strong baseline and
(Daume? III, 2007) within both a conditional
random field sequence model for named en-
tity recognition and a discriminatively trained
dependency parser.
1 Introduction
The goal of multi-task learning is to improve perfor-
mance on a set of related tasks, when provided with
(potentially varying quantities of) annotated data for
each of the tasks. It is very closely related to domain
adaptation, a far more common task in the natural
language processing community, but with two pri-
mary differences. Firstly, in domain adaptation the
different tasks are actually just different domains.
Secondly, in multi-task learning the focus is on im-
proving performance across all tasks, while in do-
main adaptation there is a distinction between source
data and target data, and the goal is to improve per-
formance on the target data. In the present work we
focus on domain adaptation, but like the multi-task
setting, we wish to improve performance across all
domains and not a single target domains. The word
domain is used here somewhat loosely: it may refer
to a topical domain or to distinctions that linguists
might term mode (speech versus writing) or regis-
ter (formal written prose versus SMS communica-
tions). For example, one may have a large amount
of parsed newswire, and want to use it to augment
a much smaller amount of parsed e-mail, to build a
higher quality parser for e-mail data. We also con-
sider the extension to the task where the annotation
is not the same, but is consistent, across domains
(that is, some domains may be annotated with more
information than others).
This problem is important because it is omni-
present in real life natural language processing tasks.
Annotated data is expensive to produce and limited
in quantity. Typically, one may begin with a con-
siderable amount of annotated newswire data, some
annotated speech data, and a little annotated e-mail
data. It would be most desirable if the aggregated
training data could be used to improve the perfor-
mance of a system on each of these domains.
From the baseline of building separate systems
for each domain, the obvious first attempt at domain
adaptation is to build a system from the union of the
training data, and we will refer to this as a second
baseline. In this paper we propose a more principled,
formal model of domain adaptation, which not only
outperforms previous work, but maintains attractive
602
performance characteristics in terms of training and
testing speed. We also show that the domain adapta-
tion work of (Daume? III, 2007), which is presented
as an ad-hoc ?preprocessing step,? is actually equiv-
alent to our formal model. However, our representa-
tion of the model conceptually separates some of the
hyperparameters which are not separated in (Daume?
III, 2007), and we found that setting these hyperpa-
rameters with different values from one another was
critical for improving performance.
We apply our model to two tasks, named entity
recognition, using a linear chain conditional random
field (CRF), and dependency parsing, using a dis-
criminative, chart-based model. In both cases, we
find that our model improves performance over both
baselines and prior work.
2 Hierarchical Bayesian Domain
Adaptation
2.1 Motivation
We call our model hierarchical Bayesian domain
adaptation, because it makes use of a hierarchical
Bayesian prior. As an example, take the case of
building a logistic classifier to decide if a word is
part of a person?s name. There will be a param-
eter (weight) for each feature, and usually there is
a zero-mean Gaussian prior over the parameter val-
ues so that they don?t get too large.1 In the stan-
dard, single-domain, case the log likelihood of the
data and prior is calculated, and the optimal pa-
rameter values are found. Now, let?s extend this
model to the case of two domains, one containing
American newswire and the other containing British
newswire. The data distributions will be similar for
the two domains, but not identical. In our model,
we have separate parameters for each feature in each
domain. We also have a top level parameter (also
to be learned) for each feature. For each domain,
the Gaussian prior over the parameter values is now
centered around these top level parameters instead
of around zero. A zero-mean Gaussian prior is then
placed over the top level parameters. In this ex-
ample, if some feature, say word=?Nigel,? only ap-
pears in the British newswire, the corresponding
weight for the American newswire will have a sim-
ilar value. This happens because the evidence in
the British domain will push the British parameter
1This can be regarded as a Bayesian prior or as weight reg-
ularization; we adopt the former perspective here.
to have a high value, and this will in turn influence
the top-level parameter to have a high value, which
will then influence the American newswire to have
a high value, because there will be no evidence in
the American data to override the prior. Conversely,
if some feature is highly indicative of isName=true
for the British newswire, and of isName=false for
the American newswire, then the British parameter
will have a high (positive) value while the American
parameter will have a low (negative) value, because
in both cases the domain-specific evidence will out-
weigh the effect of the prior.
2.2 Formal Model
Our domain adaptation model is based on a hierar-
chical Bayesian prior, through which the domain-
specific parameters are tied. The model is very
general-purpose, and can be applied to any discrim-
inative learning task for which one would typically
put a prior with a mean over the parameters. We will
build up to it by first describing a general, single-
domain, discriminative learning task, and then we
will show how to modify this model to construct
our hierarchical Bayesian domain adaptation model.
In a typical discriminative probabilistic model, the
learning process consists of optimizing the log con-
ditional likelihood of the data with respect to the pa-
rameters, Lorig(D ;?). This likelihood function can
take on many forms: logistic regression, a condi-
tional Markov model, a conditional random field, as
well as others. It is common practice to put a zero-
mean Gaussian prior over the parameters, leading to
the following objective, for which we wish to find
the optimal parameter values:
argmax
?
(
Lorig(D ;?)??
i
?2i
2? 2
)
(1)
From a graphical models perspective, this looks like
Figure 1(a), where ? is the mean for the prior (in our
case, zero), ? 2 is the variance for the prior, ? are the
parameters, or feature weights, and D is the data.
Now we will extend this single-domain model into
a multi-domain model (illustrated in Figure 1(b)).
Each feature weight ?i is replicated once for each
domain, as well as for a top-level set of parame-
ters. We will refer to the parameters for domain
d as ?d , with individual components ?d,i, the top-
level parameters as ??, and all parameters collec-
tively as ? . All of the power of our model stems
from the relationship between these sets of param-
603
?? ?
D
N
?
?? ??
?d ?d
Dd
N M
?
?? ??
?txt ?txt ?sp ?sp
?d ?d ?d ?d
Dd Dd
(a) (b) (c)
Figure 1: (a) No domain adaptation. The model parameters, ? , are normally distributed, with mean ? (typically zero)
and variance ?2. The likelihood of the data,D , is dependent on the model parameters. The form of the data distribution
depends on the underlying model (e.g., logistic regression, or a CRF). (b) Our hierarchical domain adaptation model.
The top-level parameters, ??, are normally distributed, with mean ? (typically zero) and variance ?2? . There is a plate
for each domain. Within each plate, the domain-specific parameters, ?d are normally distributed, with mean ?? and
variance ?2d . (c) Our hierarchical domain adaptation model, with an extra level of structure. In this example, the
domains are further split into text and speech super-domains, each of which has its own set of parameters (?txt and ?txt
for text and ?sp and ?sp for speech). ?d is normally distributed with mean ?txt if domain d is in the text super-domain,
and ?sp if it is in the speech super-domain.
eters. First, we place a zero-mean Gaussian prior
over the top level parameters ??. Then, these top
level parameters are used as the mean for a Gaussian
prior placed over each of the domain-specific param-
eters ?d . These domain-specific parameters are then
the parameters used in the original conditional log
likelihood functions for each domain. The domain-
specific parameter values jointly influence an appro-
priate value for the higher-level parameters. Con-
versely, the higher-level parameters will largely de-
termine the domain-specific parameters when there
is little or no evidence from within a domain, but can
be overriden by domain-specific evidence when it
clearly goes against the general picture (for instance
Leeds is normally a location, but within the sports
domain is usually an organization (football team)).
The beauty of this model is that the degree of in-
fluence each domain exerts over the others, for each
parameter, is based on the amount of evidence each
domain has about that parameter. If a domain has
a lot of evidence for a feature weight, then that evi-
dence will outweigh the effect of the prior. However,
when a domain lacks evidence for a parameter the
opposite occurs, and the prior (whose value is deter-
mined by evidence in the other domains) will have a
greater effect on the parameter value.
To achieve this, we modify the objective func-
tion. We now sum over the log likelihood for all do-
mains, including a Gaussian prior for each domain,
but which is now centered around ??, the top-level
parameters. Outside of this summation, we have a
Gaussian prior over the top-level parameters which
is identical to the prior in the original model:
Lhier(D ;?) = (2)
?
d
(
Lorig(Dd ;?d)??
i
(?d,i ???,i)2
2? 2d
)
??
i
(??,i)2
2? 2?
where ? 2d and ? 2? are variances on the priors over
the parameters for all the domains, as well as the
top-level parameters. The graphical models repre-
sentation is shown in Figure 1(b).
One potential source of confusion is with respect
to the directed or undirected nature of our domain
adaptation model, and the underlying model of the
data. Our hierarchical Bayesian domain adaptation
model is directed, as illustrated in Figure 1. How-
ever, somewhat counterintuitively, the underlying
(original) model of the data can be either directed
or undirected, and for our experiments we use undi-
604
rected, conditional random field-based models. The
directed domain adaptation model can be viewed
as a model of the parameters, and those parameter
weights are used by the underlying data model. In
Figure 1, the entire data model is represented by a
single node, D , conditioned on the parameters, ? or
?d . The form of that model can then be almost any-
thing, including an undirected model.
From an implementation perspective, the objec-
tive function is not much more difficult to implement
than the original single-domain model. For all of our
experiments, we optimized the log likelihood using
L-BFGS, which requires the function value and par-
tial derivatives of each parameter. The new partial
derivatives for the domain-specific parameters (but
not the top-level parameters) utilize the same par-
tial derivatives as in the original model. The only
change in the calculations is with respect to the pri-
ors. The partial derivatives for the domain-specific
parameters are:
?Lhier(D ;?)
??d,i
= ?Ld(Dd ,?d)??d,i ?
?d,i ???,i
? 2d
(3)
and the derivatives for the top level parameters ??
are:
?Lhier(D ;?)
???,i
=
(
?
d
??,i ??d,i
? 2d
)
? ??,i
? 2?
(4)
This function is convex. Once the optimal param-
eters have been learned, the top level parameters
can be discarded, since the runtime model for each
domain is the same as the original (single-domain)
model, parameterized by the parameters learned for
that domain in the hierarchical model. However, it
may be useful to retain the top-level parameters for
use in adaptation to further domains in the future.
In our model there are d extra hyper-parameters
which can be tuned. These are the variances ? 2d for
each domain. When this value is large then the prior
has little influence, and when set high enough will be
equivalent to training each model separately. When
this value is close to zero the prior has a strong in-
fluence, and when it is sufficiently close to zero then
it will be equivalent to completely tying the param-
eters, such that ?d1,i = ?d2,i for all domains. Despite
having many more parameters, for both of the tasks
on which we performed experiments, we found that
our model did not take much more time to train that
a baseline model trained on all of the data concate-
nated together.
2.3 Model Generalization
The model as presented thus far can be viewed
as a two level tree, with the top-level parameters
at the root, and the domain-specific ones at the
leaves. However, it is straightforward to generalize
the model to any tree structure. In the generalized
version, the domain-specific parameters would still
be at the leaves, the top-level parameters at the root,
but new mid-level parameters can be added based
on beliefs about how similar the various domains
are. For instance, if one had four datasets, two of
which contained speech data and two of which con-
tained newswire, then it might be sensible to have
two sets of mid-level parameters, one for the speech
data and one for the newswire data, as illustrated in
Figure 1(c). This would allow the speech domains
to influence one another more than the newswire do-
mains, and vice versa.
2.4 Formalization of (Daume? III, 2007)
As mentioned earlier, our model is equivalent to that
presented in (Daume? III, 2007), and can be viewed
as a formal version of his model.2 In his presenta-
tion, the adapation is done through feature augmen-
tation. Specifically, for each feature in the original
version, a new version is created for each domain, as
well as a general, domain-independent version of the
feature. For each datum, two versions of each orig-
inal feature are present: the version for that datum?s
domain, and the domain independent one.
The equivalence between the two models can be
shown with simple arithmetic. Recall that the log
likelihood of our model is:
?
d
(
Lorig(Dd ;?d)??
i
(?d,i ???,i)2
2? 2d
)
??
i
(??,i)2
2? 2?
We now introduce a new variable ?d = ?d ???, and
plug it into the equation for log likelihood:
?
d
(
Lorig(Dd ;?d +??)??
i
(?d,i)2
2? 2d
)
??
i
(??,i)2
2? 2?
The result is the model of (Daume? III, 2007), where
the ?d are the domain-specific feature weights, and
?d are the domain-independent feature weights. In
his formulation, the variances ? 2d = ? 2? for all do-
mains d.
This separation of the domain-specific and inde-
pendent variances was critical to our improved per-
formance. When using a Gaussian prior there are
2Many thanks to David Vickrey for pointing this out to us.
605
two parameters set by the user: the mean, ? (usu-
ally zero), and the variance, ? 2. Technically, each
of these parameters is actually a vector, with an en-
try for each feature, but almost always the vectors
are uniform and the same parameter is used for each
feature (there are exceptions, e.g. (Lee et al, 2007)).
Because Daume? III (2007) views the adaptation as
merely augmenting the feature space, each of his
features has the same prior mean and variance, re-
gardless of whether it is domain specific or indepen-
dent. He could have set these parameters differently,
but he did not.3 In our presentation of the model,
we explicitly represent different variances for each
domain, as well as the top level parameters. We
found that specifying different values for the domain
specific versus domain independent variances sig-
nificantly improved performance, though we found
no gains from using different values for the differ-
ent domain specific variances. The values were set
based on development data.
3 Named Entity Recognition
For our first set of experiments, we used a linear-
chain, conditional random field (CRF) model,
trained for named entity recognition (NER). The use
of CRFs for sequence modeling has become stan-
dard so we will omit the model details; good expla-
nations can be found in a number of places (Lafferty
et al, 2001; Sutton and McCallum, 2007). Our fea-
tures were based on those in (Finkel et al, 2005).
3.1 Data
We used three named entity datasets, from the
CoNLL 2003, MUC-6 and MUC-7 shared tasks.
CoNLL is British newswire, while MUC-6 and
MUC-7 are both American newswire. Arguably
MUC-6 and MUC-7 should not count as separate
domains, but because they were annotated sepa-
rately, for different shared tasks, we chose to treat
them as such, and feel that our experimental results
justify the distinction. We used the standard train
and test sets for each domain, which for CoNLL cor-
responds to the (more difficult) testb set. For details
about the number of training and test words in each
dataset, please see Table 1.
One interesting challenge in dealing with both
CoNLL and MUC data is that the label sets differ.
3Although he alludes to the potential for something similar
in the last section of his paper, when discussing the kerneliza-
tion interpretation of his approach.
# Train # Test
Words Words
MUC-6 165,082 15,032
MUC-7 89,644 64,490
CoNLL 203,261 46,435
Table 1: Number of words in the training and test sets for
each of the named entity recognition datasets.
CoNLL has four classes: person, organization, lo-
cation, and misc. MUC data has seven classes: per-
son, organization, location, percent, date, time, and
money. They overlap in the three core classes (per-
son, organization, and location), but CoNLL has
one additional class and MUC has four additional
classes.
The differences in the label sets led us to perform
two sets of experiments for the baseline and hier-
archical Bayesian models. In the first set of exper-
iments, at training time, the model allows any la-
bel from the union of the label sets, regardless of
whether that label was legal for the domain. At test
time, we would ignore guesses made by the model
which were inconsistent with the allowed labels for
that domain.4 In the second set of experiments, we
restricted the model at training time to only allow
legal labels for each domain. At test time, the do-
main was specified, and the model was once again
restricted so that words would never be tagged with
a label outside of that domain?s label set.
3.2 Experimental Results and Discussion
In our experiments, we compared our model to sev-
eral strong baselines, and the full set of results is in
Table 2. The models we used were:
TARGET ONLY. Trained and tested on only the data
for that domain.
ALL DATA. Trained and tested on data from all do-
mains, concatenated into one large dataset.
ALL DATA*. Same as ALL DATA, but restricted
possible labels for each word based on domain.
DAUME07. Trained and tested using the same tech-
nique as (Daume? III, 2007). We note that they
present results using per-token label accuracy,
while we used the more standard entity preci-
sion, recall, and F score (as in the CoNLL 2003
shared task).
4We treated them identically to the background symbol. So,
for instance, labelling a word a date in the CoNLL data had no
effect on the score.
606
Named Entity Recognition
Model Precision Recall F1
MUC-6
TARGET ONLY 86.74 80.10 83.29
ALL DATA* 85.04 83.49 84.26
ALL DATA 86.00 82.71 84.32
DAUME07* 87.83 83.41 85.56
DAUME07 87.81 82.23 85.46
HIER BAYES* 88.59 84.97 86.74
HIER BAYES 88.77 85.14 86.92
MUC-7
TARGET ONLY 81.17 70.23 75.30
ALL DATA* 81.66 76.17 78.82
ALL DATA 82.20 70.91 76.14
DAUME07* 83.33 75.42 79.18
DAUME07 83.51 75.63 79.37
HIER BAYES* 82.90 76.95 79.82
HIER BAYES 83.17 77.02 79.98
CoNLL
TARGET ONLY 85.55 84.72 85.13
ALL DATA* 86.34 84.45 85.38
ALL DATA 86.58 83.90 85.22
DAUME07* 86.09 85.06 85.57
DAUME07 86.35 85.26 85.80
HIER BAYES* 86.33 85.06 85.69
HIER BAYES 86.51 85.13 85.81
Table 2: Named entity recognition results for each of the
models. With the exception of the TARGET ONLY model,
all three datasets were combined when training each of
the models.
DAUME07*. Same as DAUME07, but restricted
possible labels for each word based on domain.
HIER BAYES. Our hierarchical Bayesian domain
adaptation model.
HIER BAYES*. Same as HIER BAYES, but re-
stricted possible labels for each word based on
the domain.
For all of the baseline models, and for the top
level-parameters in the hierarchical Bayesian model,
we used ? = 1. For the domain-specific parameters,
we used ?d = 0.1 for all domains.
The HIER BAYES model outperformed all base-
lines for both of the MUC datasets, and tied with
the DAUME07 for CoNLL. The largest improvement
was on MUC-6, where HIER BAYES outperformed
DAUME07*, the second best model, by 1.36%. This
improvement is greater than the improvement made
by that model over the ALL DATA* baseline. To as-
sess significance we used a document-level paired
t-test (over all of the data combined), and found that
HIER BAYES significantly outperformed all of the
baselines (not including HIER BAYES*) with greater
than 95% confidence.
For both the HIER BAYES and DAUME07 mod-
els, we found that performance was better for the
variant which did not restrict possible labels based
on the domain, while the ALL DATA model did ben-
efit from the label restriction. For H IER BAYES and
DAUME07, this result may be due to the structure
of the models. Because both models have domain-
specific features, the models likely learned that these
labels were never actually allowed. However, when
a feature does not occur in the data for a particular
domain, then the domain-specific parameter for that
feature will have positive weight due to evidence
present in the other domains, which at test time can
lead to assigning an illegal label to a word. This
information that a word may be of some other (un-
known to that domain) entity type may help prevent
the model from mislabeling the word. For example,
in CoNLL, nationalities, such as Iraqi and Ameri-
can, are labeled as misc. If a previously unseen na-
tionality is encountered in the MUC testing data, the
MUC model may be tempted to label is as a location,
but this evidence from the CoNLL data may prevent
that, by causing it to instead be labeled misc, a label
which will subsequently be ignored.
In typical domain adaptation work, showing gains
is made easier by the fact that the amount of train-
ing data in the target domain is comparatively small.
Within the multi-task learning setting, it is more
challenging to show gains over the ALL DATA base-
line. Nevertheless, our results show that, so long as
the amount of data in each domain is not widely dis-
parate, it is possible to achieve gains on all of the
domains simultaneously.
4 Dependency Parsing
4.1 Parsing Model
We also tested our model on an untyped dependency
parsing task, to see how it performs on a more struc-
turally complex task than sequence modeling. To
our knowledge, the discriminatively trained depen-
dency model we used has not been previously pub-
lished, but it is very similar to recent work on dis-
criminative constituency parsing (Finkel and Man-
ning, 2008). Due to space restrictions, we cannot
give a complete treatment of the model, but will give
an overview.
607
We built a CRF-based model, optimizing the like-
lihood of the parse, conditioned on the words and
parts of speech of the sentence. At the heart of
our model is the Eisner dependency grammar chart-
parsing algorithm (Eisner, 1996), which allows for
efficient computation of inside and outside scores.
The Eisner algorithm, originally designed for gen-
erative parsing, decomposes the probability of a de-
pendency parse into the probabilities of each attach-
ment of a dependent to its parent, and the proba-
bilities of each parent stopping taking dependents.
These probabilities can be conditioned on the child,
parent, and direction of the dependency. We used
a slight modification of the algorithm which allows
each probability to also be conditioned on whether
there is a previous dependent. While the unmodified
version of the algorithm includes stopping probabil-
ities, conditioned on the parent and direction, they
have no impact on which parse for a particular sen-
tence is most likely, because all words must eventu-
ally stop taking dependents. However, in the modi-
fied version, the stopping probability is also condi-
tioned on whether or not there is a previous depen-
dent, so this probability does make a difference.
While the Eisner algorithm computes locally nor-
malized probabilities for each attachment decision,
our model computes unnormalized scores. From
a graphical models perspective, our parsing model
is undirected, while the original model is directed.5
The score for a particular tree decomposes the same
way in our model as in the original Eisner model,
but it is globally normalized instead of locally nor-
malized. Using the inside and outside scores we can
compute partial derivatives for the feature weights,
as well as the value of the normalizing constant
needed to determine the probability of a particular
parse. This is done in a manner completely analo-
gous to (Finkel and Manning, 2008). Partial deriva-
tives and the function value are all that is needed to
find the optimal feature weights using L-BFGS.6
Features are computed over each attachment and
stopping decision, and can be conditioned on the
5The dependencies themselves are still directed in both
cases, it is just the underlying graphical model used to compute
the likelihood of a parse which changes from a directed model
to an undirected model.
6In (Finkel and Manning, 2008) we used stochastic gradient
descent to optimize our weights because our function evaluation
was too slow to use L-BFGS. We did not encounter this problem
in this setting.
parent, dependent (or none, if it is a stopping deci-
sion), direction of attachment, whether there is a pre-
vious dependent in that direction, and the words and
parts of speech of the sentence. We used the same
features as (McDonald et al, 2005), augmented with
information about whether or not a dependent is the
first dependent (information they did not have).
4.2 Data
For our dependency parsing experiments, we used
LDC2008T04 OntoNotes Release 2.0 data (Hovy
et al, 2006). This dataset is still in development,
and includes data from seven different domains, la-
beled for a number of tasks, including PCFG trees.
The domains span both newswire and speech from
multiple sources. We converted the PCFG trees
into dependency trees using the Collins head rules
(Collins, 2003). We also omitted the WSJ portion
of the data, because it follows a different annotation
scheme from the other domains.7 For each of the
remaining six domains, we aimed for an 75/25 data
split, but because we divided the data using the pro-
vided sections, this split was fairly rough. The num-
ber of training and test sentences for each domain
are specified in the Table 3, along with our results.
4.3 Experimental Results and Discussion
We compared the same four domain adaptation
models for dependency parsing as we did for the
named entity experiments, once again setting ? =
1.0 and ?d = 0.1. Unlike the named entity experi-
ments however, there were no label set discrepencies
between the domains, so only one version of each
domain adaptation model was necessary, instead of
the two versions in that section.
Our full dependency parsing results can be found
in Table 3. Firstly, we found that DAUME07, which
had outperformed the ALL DATA baseline for the
sequence modeling task, performed worse than the
7Specifically, all the other domains use the ?new? Penn
Treebank annotation style, whereas the WSJ data is still in the
?traditional? annotation style, familiar from the past decade?s
work in Penn Treebank parsing. The major changes are in
hyphenation and NP structure. In the new annotation style,
many hyphenated words are separated into multiple tokens, with
a new part-of-speech tag given to the hyphens, and leftward-
branching structure inside noun phrases is indicated by use of
a new NML phrasal category. The treatment of hyphenated
words, in particular, makes the two annotation styles inconsis-
tent, and so we could not work with all the data together.
608
Dependency Parsing
Training Testing TARGET ALL HIER
Range # Sent Range # Sent ONLY DATA DAUME07 BAYES
ABC 0?55 1195 56?69 199 83.32% 88.97% 87.30% 88.68%
CNN 0?375 5092 376?437 1521 85.53% 87.09% 86.41% 87.26%
MNB 0?17 509 18?25 245 77.06% 86.41% 84.70% 86.71%
NBC 0?29 552 30?39 149 76.21% 85.82% 85.01% 85.32%
PRI 0?89 1707 90?112 394 87.65% 90.28% 89.52% 90.59%
VOA 0?198 1512 199?264 383 89.17% 92.11% 90.67% 92.09%
Table 3: Dependency parsing results for each of the domain adaptation models. Performance is measured as unlabeled
attachment accuracy.
baseline here, indicating that the transfer of infor-
mation between domains in the more structurally
complicated task is inherently more difficult. Our
model?s gains over the ALL DATA baseline are
quite small, but we tested their significance using a
sentence-level paired t-test (over all of the data com-
bined) and found them to be significant at p < 10?5.
We are unsure why some domains improved while
others did not. It is not simply a consequence of
training set size, but may be due to qualities of the
domains themselves.
5 Related Work
We already discussed the relation of our work to
(Daume? III, 2007) in Section 2.4. Another piece of
similar work is (Chelba and Acero, 2004), who also
modify their prior. Their work is limited to two do-
mains, a source and a target, and their algorithm has
a two stage process: First, train a classifier on the
source data, and then use the learned weights from
that classifier as the mean for a Gaussian prior when
training a new model on just the target data.
Daume? III and Marcu (2006) also took a Bayesian
approach to domain adaptation, but structured their
model in a very different way. In their model, it is
assumed that each datum within a domain is either a
domain-specific datum, or a general datum, and then
domain-specific and general weights were learned.
Whether each datum is domain-specific or general
is not known, so they developed an EM based algo-
rithm for determining this information while simul-
taneously learning the feature weights. Their model
had good performance, but came with a 10 to 15
times slowdown at training time. Our slowest de-
pendency parser took four days to train, making this
model close to infeasible for learning on that data.
Outside of the NLP community there has been
much similar work making use of hierarchical
Bayesian priors to tie parameters across multiple,
similar tasks. Evgeniou et al (2005) present a sim-
ilar model, but based on support vector machines,
to predict the exam scores of students. Elidan et
al. (2008) make us of an undirected Bayesian trans-
fer hierarchy to jointly model the shapes of differ-
ent mammals. The complete literature on related
multi-task learning is too large to fully discuss here,
but we direct the reader to (Baxter, 1997; Caruana,
1997; Yu et al, 2005; Xue et al, 2007). For a more
general discussion of hierarchical priors, we recom-
mend Chapter 5 of (Gelman et al, 2003) and Chap-
ter 12 of (Gelman and Hill, 2006).
6 Conclusion and Future Work
In this paper we presented a new model for domain
adaptation, based on a hierarchical Bayesian prior,
which allows information to be shared between do-
mains when information is sparse, while still allow-
ing the data from a particular domain to override the
information from other domains when there is suf-
ficient evidence. We outperformed previous work
on a sequence modeling task, and showed improve-
ments on dependency parsing, a structurally more
complex problem, where previous work failed. Our
model is practically useful and does not require sig-
nificantly more time to train than a baseline model
using the same data (though it does require more
memory, proportional to the number of domains). In
the future we would like to see if the model could be
adapted to improve performance on data from a new
domain, potentially by using the top-level weights
which should be less domain-dependent.
Acknowledgements
The first author is supported by a Stanford Graduate
Fellowship. We also thank David Vickrey for his
helpful comments and observations.
609
References
J. Baxter. 1997. A bayesian/information theoretic model
of learning to learn via multiple task sampling. In Ma-
chine Learning, volume 28.
R. Caruana. 1997. Multitask learning. In Machine Learn-
ing, volume 28.
Ciprian Chelba and Alex Acero. 2004. Adaptation of a
maximum entropy capitalizer: Little data can help a
lot. In EMNLP 2004.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Computa-
tional Linguistics (ACL), Prague, Czech Republic.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING-96), Copenhagen.
Gal Elidan, Benjamin Packer, Geremy Heitz, and Daphne
Koller. 2008. Convex point estimation using undi-
rected bayesian transfer hierarchies. In UAI 2008.
T. Evgeniou, C. Micchelli, and M. Pontil. 2005. Learn-
ing multiple tasks with kernel methods. In Journal of
Machine Learning Research.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Efficient, feature-based conditional random field pars-
ing. In ACL/HLT-2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL 2005.
Andrew Gelman and Jennifer Hill. 2006. Data Analysis
Using Regression and Multilevel/Hierarchical Models.
Cambridge University Press.
A. Gelman, J. B. Carlin, H. S. Stern, and Donald
D. B. Rubin. 2003. Bayesian Data Analysis. Chapman
& Hall.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In HLT-NAACL 2006.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML 2001.
Su-In Lee, Vassil Chatalbashev, David Vickrey, and
Daphne Koller. 2007. Learning a meta-level prior for
feature relevance from multiple related tasks. In ICML
?07: Proceedings of the 24th international conference
on Machine learning.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL 2005.
Charles Sutton and Andrew McCallum. 2007. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, Intro-
duction to Statistical Relational Learning. MIT Press.
Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krish-
napuram. 2007. Multi-task learning for classification
with dirichlet process priors. J. Mach. Learn. Res., 8.
Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005.
Learning gaussian processes from multiple tasks. In
ICML ?05: Proceedings of the 22nd international con-
ference on Machine learning.
610
Proceedings of the 43rd Annual Meeting of the ACL, pages 363?370,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Incorporating Non-local Information into Information
Extraction Systems by Gibbs Sampling
Jenny Rose Finkel, Trond Grenager, and Christopher Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel, grenager, mannning}@cs.stanford.edu
Abstract
Most current statistical natural language process-
ing models use only local features so as to permit
dynamic programming in inference, but this makes
them unable to fully account for the long distance
structure that is prevalent in language use. We
show how to solve this dilemma with Gibbs sam-
pling, a simple Monte Carlo method used to per-
form approximate inference in factored probabilis-
tic models. By using simulated annealing in place
of Viterbi decoding in sequence models such as
HMMs, CMMs, and CRFs, it is possible to incorpo-
rate non-local structure while preserving tractable
inference. We use this technique to augment an
existing CRF-based information extraction system
with long-distance dependency models, enforcing
label consistency and extraction template consis-
tency constraints. This technique results in an error
reduction of up to 9% over state-of-the-art systems
on two established information extraction tasks.
1 Introduction
Most statistical models currently used in natural lan-
guage processing represent only local structure. Al-
though this constraint is critical in enabling tractable
model inference, it is a key limitation in many tasks,
since natural language contains a great deal of non-
local structure. A general method for solving this
problem is to relax the requirement of exact infer-
ence, substituting approximate inference algorithms
instead, thereby permitting tractable inference in
models with non-local structure. One such algo-
rithm is Gibbs sampling, a simple Monte Carlo algo-
rithm that is appropriate for inference in any factored
probabilistic model, including sequence models and
probabilistic context free grammars (Geman and Ge-
man, 1984). Although Gibbs sampling is widely
used elsewhere, there has been extremely little use
of it in natural language processing.1 Here, we use
it to add non-local dependencies to sequence models
for information extraction.
Statistical hidden state sequence models, such
as Hidden Markov Models (HMMs) (Leek, 1997;
Freitag and McCallum, 1999), Conditional Markov
Models (CMMs) (Borthwick, 1999), and Condi-
tional Random Fields (CRFs) (Lafferty et al, 2001)
are a prominent recent approach to information ex-
traction tasks. These models all encode the Markov
property: decisions about the state at a particular po-
sition in the sequence can depend only on a small lo-
cal window. It is this property which allows tractable
computation: the Viterbi, Forward Backward, and
Clique Calibration algorithms all become intractable
without it.
However, information extraction tasks can benefit
from modeling non-local structure. As an example,
several authors (see Section 8) mention the value of
enforcing label consistency in named entity recogni-
tion (NER) tasks. In the example given in Figure 1,
the second occurrence of the token Tanjug is mis-
labeled by our CRF-based statistical NER system,
because by looking only at local evidence it is un-
clear whether it is a person or organization. The first
occurrence of Tanjug provides ample evidence that
it is an organization, however, and by enforcing la-
bel consistency the system should be able to get it
right. We show how to incorporate constraints of
this form into a CRF model by using Gibbs sam-
pling instead of the Viterbi algorithm as our infer-
ence procedure, and demonstrate that this technique
yields significant improvements on two established
IE tasks.
1Prior uses in NLP of which we are aware include: Kim et
al. (1995), Della Pietra et al (1997) and Abney (1997).
363
the news agency Tanjug reported . . . airport , Tanjug said .
Figure 1: An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset.
2 Gibbs Sampling for Inference in
Sequence Models
In hidden state sequence models such as HMMs,
CMMs, and CRFs, it is standard to use the Viterbi
algorithm, a dynamic programming algorithm, to in-
fer the most likely hidden state sequence given the
input and the model (see, e.g., Rabiner (1989)). Al-
though this is the only tractable method for exact
computation, there are other methods for comput-
ing an approximate solution. Monte Carlo methods
are a simple and effective class of methods for ap-
proximate inference based on sampling. Imagine
we have a hidden state sequence model which de-
fines a probability distribution over state sequences
conditioned on any given input. With such a model
M we should be able to compute the conditional
probability PM (s|o) of any state sequence s =
{s0, . . . , sN} given some observed input sequence
o = {o0, . . . , oN}. One can then sample se-
quences from the conditional distribution defined by
the model. These samples are likely to be in high
probability areas, increasing our chances of finding
the maximum. The challenge is how to sample se-
quences efficiently from the conditional distribution
defined by the model.
Gibbs sampling provides a clever solution (Ge-
man and Geman, 1984). Gibbs sampling defines a
Markov chain in the space of possible variable as-
signments (in this case, hidden state sequences) such
that the stationary distribution of the Markov chain
is the joint distribution over the variables. Thus it
is called a Markov Chain Monte Carlo (MCMC)
method; see Andrieu et al (2003) for a good MCMC
tutorial. In practical terms, this means that we
can walk the Markov chain, occasionally outputting
samples, and that these samples are guaranteed to
be drawn from the target distribution. Furthermore,
the chain is defined in very simple terms: from each
state sequence we can only transition to a state se-
quence obtained by changing the state at any one
position i, and the distribution over these possible
transitions is just
PG(s(t)|s(t?1)) = PM (s(t)i |s
(t?1)
?i ,o). (1)
where s?i is all states except si. In other words, the
transition probability of the Markov chain is the con-
ditional distribution of the label at the position given
the rest of the sequence. This quantity is easy to
compute in any Markov sequence model, including
HMMs, CMMs, and CRFs. One easy way to walk
the Markov chain is to loop through the positions i
from 1 to N , and for each one, to resample the hid-
den state at that position from the distribution given
in Equation 1. By outputting complete sequences
at regular intervals (such as after resampling all N
positions), we can sample sequences from the con-
ditional distribution defined by the model.
This is still a gravely inefficient process, how-
ever. Random sampling may be a good way to es-
timate the shape of a probability distribution, but it
is not an efficient way to do what we want: find
the maximum. However, we cannot just transi-
tion greedily to higher probability sequences at each
step, because the space is extremely non-convex. We
can, however, borrow a technique from the study
of non-convex optimization and use simulated an-
nealing (Kirkpatrick et al, 1983). Geman and Ge-
man (1984) show that it is easy to modify a Gibbs
Markov chain to do annealing; at time t we replace
the distribution in (1) with
PA(s(t)|s(t?1)) =
PM (s(t)i |s
(t?1)
?i ,o)1/ct
?
j PM (s
(t)
j |s
(t?1)
?j ,o)1/ct
(2)
where c = {c0, . . . , cT } defines a cooling schedule.
At each step, we raise each value in the conditional
distribution to an exponent and renormalize before
sampling from it. Note that when c = 1 the distri-
bution is unchanged, and as c ? 0 the distribution
364
Inference CoNLL Seminars
Viterbi 85.51 91.85
Gibbs 85.54 91.85
Sampling 85.51 91.85
85.49 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.86
Mean 85.51 91.85
Std. Dev. 0.01 0.004
Table 1: An illustration of the effectiveness of Gibbs sampling,
compared to Viterbi inference, for the two tasks addressed in
this paper: the CoNLL named entity recognition task, and the
CMU Seminar Announcements information extraction task. We
show 10 runs of Gibbs sampling in the same CRF model that
was used for Viterbi. For each run the sampler was initialized
to a random sequence, and used a linear annealing schedule that
sampled the complete sequence 1000 times. CoNLL perfor-
mance is measured as per-entity F1, and CMU Seminar An-
nouncements performance is measured as per-token F1.
becomes sharper, and when c = 0 the distribution
places all of its mass on the maximal outcome, hav-
ing the effect that the Markov chain always climbs
uphill. Thus if we gradually decrease c from 1 to
0, the Markov chain increasingly tends to go up-
hill. This annealing technique has been shown to
be an effective technique for stochastic optimization
(Laarhoven and Arts, 1987).
To verify the effectiveness of Gibbs sampling and
simulated annealing as an inference technique for
hidden state sequence models, we compare Gibbs
and Viterbi inference methods for a basic CRF, with-
out the addition of any non-local model. The results,
given in Table 1, show that if the Gibbs sampler is
run long enough, its accuracy is the same as a Viterbi
decoder.
3 A Conditional Random Field Model
Our basic CRF model follows that of Lafferty et al
(2001). We choose a CRF because it represents the
state of the art in sequence modeling, allowing both
discriminative training and the bi-directional flow of
probabilistic information across the sequence. A
CRF is a conditional sequence model which rep-
resents the probability of a hidden state sequence
given some observations. In order to facilitate ob-
taining the conditional probabilities we need for
Gibbs sampling, we generalize the CRF model in a
Feature NER TF
Current Word X X
Previous Word X X
Next Word X X
Current Word Character n-gram all length ? 6
Current POS Tag X
Surrounding POS Tag Sequence X
Current Word Shape X X
Surrounding Word Shape Sequence X X
Presence of Word in Left Window size 4 size 9
Presence of Word in Right Window size 4 size 9
Table 2: Features used by the CRF for the two tasks: named
entity recognition (NER) and template filling (TF).
way that is consistent with the Markov Network lit-
erature (see Cowell et al (1999)): we create a linear
chain of cliques, where each clique, c, represents the
probabilistic relationship between an adjacent pair
of states2 using a clique potential ?c, which is just
a table containing a value for each possible state as-
signment. The table is not a true probability distribu-
tion, as it only accounts for local interactions within
the clique. The clique potentials themselves are de-
fined in terms of exponential models conditioned on
features of the observation sequence, and must be
instantiated for each new observation sequence. The
sequence of potentials in the clique chain then de-
fines the probability of a state sequence (given the
observation sequence) as
PCRF(s|o) ?
N
?
i=1
?i(si?1, si) (3)
where ?i(si?1, si) is the element of the clique po-
tential at position i corresponding to states si?1 and
si.3
Although a full treatment of CRF training is be-
yond the scope of this paper (our technique assumes
the model is already trained), we list the features
used by our CRF for the two tasks we address in
Table 2. During training, we regularized our expo-
nential models with a quadratic prior and used the
quasi-Newton method for parameter optimization.
As is customary, we used the Viterbi algorithm to
infer the most likely state sequence in a CRF.
2CRFs with larger cliques are also possible, in which case
the potentials represent the relationship between a subsequence
of k adjacent states, and contain |S|k elements.
3To handle the start condition properly, imagine also that we
define a distinguished start state s0.
365
The clique potentials of the CRF, instantiated for
some observation sequence, can be used to easily
compute the conditional distribution over states at
a position given in Equation 1. Recall that at posi-
tion i we want to condition on the states in the rest
of the sequence. The state at this position can be
influenced by any other state that it shares a clique
with; in particular, when the clique size is 2, there
are 2 such cliques. In this case the Markov blanket
of the state (the minimal set of states that renders
a state conditionally independent of all other states)
consists of the two neighboring states and the obser-
vation sequence, all of which are observed. The con-
ditional distribution at position i can then be com-
puted simply as
PCRF(si|s?i,o) ? ?i(si?1, si)?i+1(si, si+1) (4)
where the factor tables F in the clique chain are al-
ready conditioned on the observation sequence.
4 Datasets and Evaluation
We test the effectiveness of our technique on two es-
tablished datasets: the CoNLL 2003 English named
entity recognition dataset, and the CMU Seminar
Announcements information extraction dataset.
4.1 The CoNLL NER Task
This dataset was created for the shared task of the
Seventh Conference on Computational Natural Lan-
guage Learning (CoNLL),4 which concerned named
entity recognition. The English data is a collection
of Reuters newswire articles annotated with four en-
tity types: person (PER), location (LOC), organi-
zation (ORG), and miscellaneous (MISC). The data
is separated into a training set, a development set
(testa), and a test set (testb). The training set con-
tains 945 documents, and approximately 203,000 to-
kens. The development set has 216 documents and
approximately 51,000 tokens, and the test set has
231 documents and approximately 46,000 tokens.
We evaluate performance on this task in the man-
ner dictated by the competition so that results can be
properly compared. Precision and recall are evalu-
ated on a per-entity basis (and combined into an F1
score). There is no partial credit; an incorrect entity
4Available at http://cnts.uia.ac.be/conll2003/ner/.
boundary is penalized as both a false positive and as
a false negative.
4.2 The CMU Seminar Announcements Task
This dataset was developed as part of Dayne Fre-
itag?s dissertation research Freitag (1998).5 It con-
sists of 485 emails containing seminar announce-
ments at Carnegie Mellon University. It is annotated
for four fields: speaker, location, start time, and end
time. Sutton and McCallum (2004) used 5-fold cross
validation when evaluating on this dataset, so we ob-
tained and used their data splits, so that results can
be properly compared. Because the entire dataset is
used for testing, there is no development set. We
also used their evaluation metric, which is slightly
different from the method for CoNLL data. Instead
of evaluating precision and recall on a per-entity ba-
sis, they are evaluated on a per-token basis. Then, to
calculate the overall F1 score, the F1 scores for each
class are averaged.
5 Models of Non-local Structure
Our models of non-local structure are themselves
just sequence models, defining a probability distri-
bution over all possible state sequences. It is pos-
sible to flexibly model various forms of constraints
in a way that is sensitive to the linguistic structure
of the data (e.g., one can go beyond imposing just
exact identity conditions). One could imagine many
ways of defining such models; for simplicity we use
the form
PM (s|o) ?
?
???
?#(?,s,o)? (5)
where the product is over a set of violation types ?,
and for each violation type ? we specify a penalty
parameter ??. The exponent #(?, s,o) is the count
of the number of times that the violation ? occurs
in the state sequence s with respect to the observa-
tion sequence o. This has the effect of assigning
sequences with more violations a lower probabil-
ity. The particular violation types are defined specif-
ically for each task, and are described in the follow-
ing two sections.
This model, as defined above, is not normalized,
and clearly it would be expensive to do so. This
5Available at http://nlp.shef.ac.uk/dot.kom/resources.html.
366
PER LOC ORG MISC
PER 3141 4 5 0
LOC 6436 188 3
ORG 2975 0
MISC 2030
Table 3: Counts of the number of times multiple occurrences of
a token sequence is labeled as different entity types in the same
document. Taken from the CoNLL training set.
PER LOC ORG MISC
PER 1941 5 2 3
LOC 0 167 6 63
ORG 22 328 819 191
MISC 14 224 7 365
Table 4: Counts of the number of times an entity sequence is
labeled differently from an occurrence of a subsequence of it
elsewhere in the document. Rows correspond to sequences, and
columns to subsequences. Taken from the CoNLL training set.
doesn?t matter, however, because we only use the
model for Gibbs sampling, and so only need to com-
pute the conditional distribution at a single position
i (as defined in Equation 1). One (inefficient) way
to compute this quantity is to enumerate all possi-
ble sequences differing only at position i, compute
the score assigned to each by the model, and renor-
malize. Although it seems expensive, this compu-
tation can be made very efficient with a straightfor-
ward memoization technique: at all times we main-
tain data structures representing the relationship be-
tween entity labels and token sequences, from which
we can quickly compute counts of different types of
violations.
5.1 CoNLL Consistency Model
Label consistency structure derives from the fact that
within a particular document, different occurrences
of a particular token sequence are unlikely to be la-
beled as different entity types. Although any one
occurrence may be ambiguous, it is unlikely that all
instances are unclear when taken together.
The CoNLL training data empirically supports the
strength of the label consistency constraint. Table 3
shows the counts of entity labels for each pair of
identical token sequences within a document, where
both are labeled as an entity. Note that inconsis-
tent labelings are very rare.6 In addition, we also
6A notable exception is the labeling of the same text as both
organization and location within the same document. This is a
consequence of the large portion of sports news in the CoNLL
want to model subsequence constraints: having seen
Geoff Woods earlier in a document as a person is
a good indicator that a subsequent occurrence of
Woods should also be labeled as a person. How-
ever, if we examine all cases of the labelings of
other occurrences of subsequences of a labeled en-
tity, we find that the consistency constraint does not
hold nearly so strictly in this case. As an exam-
ple, one document contains references to both The
China Daily, a newspaper, and China, the country.
Counts of subsequence labelings within a document
are listed in Table 4. Note that there are many off-
diagonal entries: the China Daily case is the most
common, occurring 328 times in the dataset.
The penalties used in the long distance constraint
model for CoNLL are the Empirical Bayes estimates
taken directly from the data (Tables 3 and 4), except
that we change counts of 0 to be 1, so that the dis-
tribution remains positive. So the estimate of a PER
also being an ORG is 53151 ; there were 5 instance of
an entity being labeled as both, PER appeared 3150
times in the data, and we add 1 to this for smoothing,
because PER-MISC never occured. However, when
we have a phrase labeled differently in two differ-
ent places, continuing with the PER-ORG example,
it is unclear if we should penalize it as PER that is
also an ORG or an ORG that is also a PER. To deal
with this, we multiply the square roots of each esti-
mate together to form the penalty term. The penalty
term is then multiplied in a number of times equal
to the length of the offending entity; this is meant to
?encourage? the entity to shrink.7 For example, say
we have a document with three entities, Rotor Vol-
gograd twice, once labeled as PER and once as ORG,
and Rotor, labeled as an ORG. The likelihood of a
PER also being an ORG is 53151 , and of an ORG also
being a PER is 53169 , so the penalty for this violation
is (
?
5
3151 ?
?
5
3151 )2. The likelihood of a ORG be-
ing a subphrase of a PER is 2842 . So the total penalty
would be 53151 ? 53169 ? 2842 .
dataset, so that city names are often also team names.
7While there is no theoretical justification for this, we found
it to work well in practice.
367
5.2 CMU Seminar Announcements
Consistency Model
Due to the lack of a development set, our consis-
tency model for the CMU Seminar Announcements
is much simpler than the CoNLL model, the num-
bers where selected due to our intuitions, and we did
not spend much time hand optimizing the model.
Specifically, we had three constraints. The first is
that all entities labeled as start time are normal-
ized, and are penalized if they are inconsistent. The
second is a corresponding constraint for end times.
The last constraint attempts to consistently label the
speakers. If a phrase is labeled as a speaker, we as-
sume that the last word is the speaker?s last name,
and we penalize for each occurrance of that word
which is not also labeled speaker. For the start and
end times the penalty is multiplied in based on how
many words are in the entity. For the speaker, the
penalty is only multiplied in once. We used a hand
selected penalty of exp?4.0.
6 Combining Sequence Models
In the previous section we defined two models of
non-local structure. Now we would like to incor-
porate them into the local model (in our case, the
trained CRF), and use Gibbs sampling to find the
most likely state sequence. Because both the trained
CRF and the non-local models are themselves se-
quence models, we simply combine the two mod-
els into a factored sequence model of the following
form
PF (s|o) ? PM (s|o)PL(s|o) (6)
where M is the local CRF model, L is the new non-
local model, and F is the factored model.8 In this
form, the probability again looks difficult to com-
pute (because of the normalizing factor, a sum over
all hidden state sequences of length N ). However,
since we are only using the model for Gibbs sam-
pling, we never need to compute the distribution ex-
plicitly. Instead, we need only the conditional prob-
ability of each position in the sequence, which can
be computed as
PF (si|s?i,o) ? PM (si|s?i,o)PL(si|s?i,o). (7)
8This model double-generates the state sequence condi-
tioned on the observations. In practice we don?t find this to
be a problem.
CoNLL
Approach LOC ORG MISC PER ALL
B&M LT-RMN ? ? ? ? 80.09
B&M GLT-RMN ? ? ? ? 82.30
Local+Viterbi 88.16 80.83 78.51 90.36 85.51
NonLoc+Gibbs 88.51 81.72 80.43 92.29 86.86
Table 5: F1 scores of the local CRF and non-local models on the
CoNLL 2003 named entity recognition dataset. We also provide
the results from Bunescu and Mooney (2004) for comparison.
CMU Seminar Announcements
Approach STIME ETIME SPEAK LOC ALL
S&M CRF 97.5 97.5 88.3 77.3 90.2
S&M Skip-CRF 96.7 97.2 88.1 80.4 90.6
Local+Viterbi 96.67 97.36 83.39 89.98 91.85
NonLoc+Gibbs 97.11 97.89 84.16 90.00 92.29
Table 6: F1 scores of the local CRF and non-local models on
the CMU Seminar Announcements dataset. We also provide
the results from Sutton and McCallum (2004) for comparison.
At inference time, we then sample from the Markov
chain defined by this transition probability.
7 Results and Discussion
In our experiments we compare the impact of adding
the non-local models with Gibbs sampling to our
baseline CRF implementation. In the CoNLL named
entity recognition task, the non-local models in-
crease the F1 accuracy by about 1.3%. Although
such gains may appear modest, note that they are
achieved relative to a near state-of-the-art NER sys-
tem: the winner of the CoNLL English task reported
an F1 score of 88.76. In contrast, the increases pub-
lished by Bunescu and Mooney (2004) are relative
to a baseline system which scores only 80.9% on
the same task. Our performance is similar on the
CMU Seminar Announcements dataset. We show
the per-field F1 results that were reported by Sutton
and McCallum (2004) for comparison, and note that
we are again achieving gains against a more compet-
itive baseline system.
For all experiments involving Gibbs sampling, we
used a linear cooling schedule. For the CoNLL
dataset we collected 200 samples per trial, and for
the CMU Seminar Announcements we collected 100
samples. We report the average of all trials, and in all
cases we outperform the baseline with greater than
95% confidence, using the standard t-test. The trials
had low standard deviations - 0.083% and 0.007% -
and high minimun F-scores - 86.72%, and 92.28%
368
- for the CoNLL and CMU Seminar Announce-
ments respectively, demonstrating the stability of
our method.
The biggest drawback to our model is the com-
putational cost. Taking 100 samples dramatically
increases test time. Averaged over 3 runs on both
Viterbi and Gibbs, CoNLL testing time increased
from 55 to 1738 seconds, and CMU Seminar An-
nouncements testing time increases from 189 to
6436 seconds.
8 Related Work
Several authors have successfully incorporated a
label consistency constraint into probabilistic se-
quence model named entity recognition systems.
Mikheev et al (1999) and Finkel et al (2004) in-
corporate label consistency information by using ad-
hoc multi-stage labeling procedures that are effec-
tive but special-purpose. Malouf (2002) and Curran
and Clark (2003) condition the label of a token at
a particular position on the label of the most recent
previous instance of that same token in a prior sen-
tence of the same document. Note that this violates
the Markov property, but is achieved by slightly re-
laxing the requirement of exact inference. Instead
of finding the maximum likelihood sequence over
the entire document, they classify one sentence at a
time, allowing them to condition on the maximum
likelihood sequence of previous sentences. This ap-
proach is quite effective for enforcing label consis-
tency in many NLP tasks, however, it permits a for-
ward flow of information only, which is not suffi-
cient for all cases of interest. Chieu and Ng (2002)
propose a solution to this problem: for each to-
ken, they define additional features taken from other
occurrences of the same token in the document.
This approach has the added advantage of allowing
the training procedure to automatically learn good
weightings for these ?global? features relative to the
local ones. However, this approach cannot easily
be extended to incorporate other types of non-local
structure.
The most relevant prior works are Bunescu and
Mooney (2004), who use a Relational Markov Net-
work (RMN) (Taskar et al, 2002) to explicitly mod-
els long-distance dependencies, and Sutton and Mc-
Callum (2004), who introduce skip-chain CRFs,
which maintain the underlying CRF sequence model
(which (Bunescu and Mooney, 2004) lack) while
adding skip edges between distant nodes. Unfortu-
nately, in the RMN model, the dependencies must
be defined in the model structure before doing any
inference, and so the authors use crude heuristic
part-of-speech patterns, and then add dependencies
between these text spans using clique templates.
This generates a extremely large number of over-
lapping candidate entities, which then necessitates
additional templates to enforce the constraint that
text subsequences cannot both be different entities,
something that is more naturally modeled by a CRF.
Another disadvantage of this approach is that it uses
loopy belief propagation and a voted perceptron for
approximate learning and inference ? ill-founded
and inherently unstable algorithms which are noted
by the authors to have caused convergence prob-
lems. In the skip-chain CRFs model, the decision
of which nodes to connect is also made heuristi-
cally, and because the authors focus on named entity
recognition, they chose to connect all pairs of identi-
cal capitalized words. They also utilize loopy belief
propagation for approximate learning and inference.
While the technique we propose is similar math-
ematically and in spirit to the above approaches, it
differs in some important ways. Our model is im-
plemented by adding additional constraints into the
model at inference time, and does not require the
preprocessing step necessary in the two previously
mentioned works. This allows for a broader class of
long-distance dependencies, because we do not need
to make any initial assumptions about which nodes
should be connected, and is helpful when you wish
to model relationships between nodes which are the
same class, but may not be similar in any other way.
For instance, in the CMU Seminar Announcements
dataset, we can normalize all entities labeled as a
start time and penalize the model if multiple, non-
consistent times are labeled. This type of constraint
cannot be modeled in an RMN or a skip-CRF, be-
cause it requires the knowledge that both entities are
given the same class label.
We also allow dependencies between multi-word
phrases, and not just single words. Additionally,
our model can be applied on top of a pre-existing
trained sequence model. As such, our method does
not require complex training procedures, and can
369
instead leverage all of the established methods for
training high accuracy sequence models. It can in-
deed be used in conjunction with any statistical hid-
den state sequence model: HMMs, CMMs, CRFs, or
even heuristic models. Third, our technique employs
Gibbs sampling for approximate inference, a simple
and probabilistically well-founded algorithm. As a
consequence of these differences, our approach is
easier to understand, implement, and adapt to new
applications.
9 Conclusions
We have shown that a constraint model can be effec-
tively combined with an existing sequence model in
a factored architecture to successfully impose var-
ious sorts of long distance constraints. Our model
generalizes naturally to other statistical models and
other tasks. In particular, it could in the future
be applied to statistical parsing. Statistical context
free grammars provide another example of statistical
models which are restricted to limiting local struc-
ture, and which could benefit from modeling non-
local structure.
Acknowledgements
This work was supported in part by the Advanced
Researchand Development Activity (ARDA)?s
Advanced Question Answeringfor Intelligence
(AQUAINT) Program. Additionally, we would like
to that our reviewers for their helpful comments.
References
S. Abney. 1997. Stochastic attribute-value grammars. Compu-
tational Linguistics, 23:597?618.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. 2003.
An introduction to MCMC for machine learning. Machine
Learning, 50:5?43.
A. Borthwick. 1999. A Maximum Entropy Approach to Named
Entity Recognition. Ph.D. thesis, New York University.
R. Bunescu and R. J. Mooney. 2004. Collective information
extraction with relational Markov networks. In Proceedings
of the 42nd ACL, pages 439?446.
H. L. Chieu and H. T. Ng. 2002. Named entity recognition:
a maximum entropy approach using global information. In
Proceedings of the 19th Coling, pages 190?196.
R. G. Cowell, A. Philip Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. 1999. Probabilistic Networks and Expert Sys-
tems. Springer-Verlag, New York.
J. R. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proceedings of the 7th
CoNLL, pages 164?167.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Induc-
ing features of random fields. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 19:380?393.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and C. D. Man-
ning. 2004. Exploiting context for biomedical entity recog-
nition: from syntax to the web. In Joint Workshop on Natural
Language Processing in Biomedicine and Its Applications at
Coling 2004.
D. Freitag and A. McCallum. 1999. Information extraction
with HMMs and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction.
D. Freitag. 1998. Machine learning for information extraction
in informal domains. Ph.D. thesis, Carnegie Mellon Univer-
sity.
S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs
distributions, and the Bayesian restoration of images. IEEE
Transitions on Pattern Analysis and Machine Intelligence,
6:721?741.
M. Kim, Y. S. Han, and K. Choi. 1995. Collocation map
for overcoming data sparseness. In Proceedings of the 7th
EACL, pages 53?59.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220:671?680.
P. J. Van Laarhoven and E. H. L. Arts. 1987. Simulated Anneal-
ing: Theory and Applications. Reidel Publishers.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
Random Fields: Probabilistic models for segmenting and
labeling sequence data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann, San Francisco, CA.
T. R. Leek. 1997. Information extraction using hidden Markov
models. Master?s thesis, U.C. San Diego.
R. Malouf. 2002. Markov models for language-independent
named entity recognition. In Proceedings of the 6th CoNLL,
pages 187?190.
A. Mikheev, M. Moens, and C. Grover. 1999. Named entity
recognition without gazetteers. In Proceedings of the 9th
EACL, pages 1?8.
L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?286.
C. Sutton and A. McCallum. 2004. Collective segmentation
and labeling of distant entities in information extraction. In
ICML Workshop on Statistical Relational Learning and Its
connections to Other Fields.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceedings of
the 18th Conference on Uncertianty in Artificial Intelligence
(UAI-02), pages 485?494, Edmonton, Canada.
370
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272?279,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Infinite Tree
Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
{jrfinkel, grenager, manning}@cs.stanford.edu
Abstract
Historically, unsupervised learning tech-
niques have lacked a principled technique
for selecting the number of unseen compo-
nents. Research into non-parametric priors,
such as the Dirichlet process, has enabled in-
stead the use of infinite models, in which the
number of hidden categories is not fixed, but
can grow with the amount of training data.
Here we develop the infinite tree, a new infi-
nite model capable of representing recursive
branching structure over an arbitrarily large
set of hidden categories. Specifically, we
develop three infinite tree models, each of
which enforces different independence as-
sumptions, and for each model we define a
simple direct assignment sampling inference
procedure. We demonstrate the utility of
our models by doing unsupervised learning
of part-of-speech tags from treebank depen-
dency skeleton structure, achieving an accu-
racy of 75.34%, and by doing unsupervised
splitting of part-of-speech tags, which in-
creases the accuracy of a generative depen-
dency parser from 85.11% to 87.35%.
1 Introduction
Model-based unsupervised learning techniques have
historically lacked good methods for choosing the
number of unseen components. For example, k-
means or EM clustering require advance specifica-
tion of the number of mixture components. But
the introduction of nonparametric priors such as the
Dirichlet process (Ferguson, 1973) enabled develop-
ment of infinite mixture models, in which the num-
ber of hidden components is not fixed, but emerges
naturally from the training data (Antoniak, 1974).
Teh et al (2006) proposed the hierarchical Dirich-
let process (HDP) as a way of applying the Dirichlet
process (DP) to more complex model forms, so as to
allow multiple, group-specific, infinite mixture mod-
els to share their mixture components. The closely
related infinite hidden Markov model is an HMM
in which the transitions are modeled using an HDP,
enabling unsupervised learning of sequence models
when the number of hidden states is unknown (Beal
et al, 2002; Teh et al, 2006).
We extend this work by introducing the infinite
tree model, which represents recursive branching
structure over a potentially infinite set of hidden
states. Such models are appropriate for the syntactic
dependency structure of natural language. The hid-
den states represent word categories (?tags?), the ob-
servations they generate represent the words them-
selves, and the tree structure represents syntactic de-
pendencies between pairs of tags.
To validate the model, we test unsupervised learn-
ing of tags conditioned on a given dependency tree
structure. This is useful, because coarse-grained
syntactic categories, such as those used in the Penn
Treebank (PTB), make insufficient distinctions to be
the basis of accurate syntactic parsing (Charniak,
1996). Hence, state-of-the-art parsers either supple-
ment the part-of-speech (POS) tags with the lexical
forms themselves (Collins, 2003; Charniak, 2000),
manually split the tagset into a finer-grained one
(Klein and Manning, 2003a), or learn finer grained
tag distinctions using a heuristic learning procedure
(Petrov et al, 2006). We demonstrate that the tags
learned with our model are correlated with the PTB
POS tags, and furthermore that they improve the ac-
curacy of an automatic parser when used in training.
2 Finite Trees
We begin by presenting three finite tree models, each
with different independence assumptions.
272
C? pik
H ?k
z1
z2 z3
x1 x2 x3
Figure 1: A graphical representation of the finite
Bayesian tree model with independent children. The
plate (rectangle) indicates that there is one copy of
the model parameter variables for each state k ? C .
2.1 Independent Children
In the first model, children are generated indepen-
dently of each other, conditioned on the parent. Let
t denote both the tree and its root node, c(t) the list
of children of t, ci(t) the ith child of t, and p(t) the
parent of t. Each tree t has a hidden state zt (in a syn-
tax tree, the tag) and an observation xt (the word).1
The probability of a tree is given by the recursive
definition:2
Ptr(t) = P(xt|zt)
?
t??c(t)
P(zt? |zt)Ptr(t?)
To make the model Bayesian, we must define ran-
dom variables to represent each of the model?s pa-
rameters, and specify prior distributions for them.
Let each of the hidden state variables have C possi-
ble values which we will index with k. Each state k
has a distinct distribution over observations, param-
eterized by ?k, which is distributed according to a
prior distribution over the parameters H:
?k|H ? H
We generate each observation xt from some distri-
bution F (?zt) parameterized by ?zt specific to its
corresponding hidden state zt. If F (?k)s are multi-
nomials, then a natural choice for H would be a
Dirichlet distribution.3
The hidden state zt? of each child is distributed
according to a multinomial distribution pizt specific
to the hidden state zt of the parent:
xt|zt ? F (?zt)
zt? |zt ? Multinomial(pizt)
1To model length, every child list ends with a distinguished
stop node, which has as its state a distinguished stop state.
2We also define a distinguished node t0, which generates the
root of the entire tree, and P (xt0 |zt0) = 1.
3A Dirichlet distribution is a distribution over the possible
parameters of a multinomial distributions, and is distinct from
the Dirichlet process.
Each multinomial over children pik is distributed ac-
cording to a Dirichlet distribution with parameter ?:
pik|? ? Dirichlet(?, . . . , ?)
This model is presented graphically in Figure 1.
2.2 Simultaneous Children
The independent child model adopts strong indepen-
dence assumptions, and we may instead want mod-
els in which the children are conditioned on more
than just the parent?s state. Our second model thus
generates the states of all of the children c(t) simul-
taneously:
Ptr(t) = P(xt|zt)P((zt?)t??c(t)|zt)
?
t??c(t)
Ptr(t?)
where (zt?)t??c(t) indicates the list of tags of the chil-
dren of t. To parameterize this model, we replace the
multinomial distribution pik over states with a multi-
nomial distribution ?k over lists of states.4
2.3 Markov Children
The very large domain size of the child lists in the
simultaneous child model may cause problems of
sparse estimation. Another alternative is to use a
first-order Markov process to generate children, in
which each child?s state is conditioned on the previ-
ous child?s state:
Ptr(t) = P(xt|zt)
?|c(t)|
i=1
P(zci(t)|zci?1(t), zt)Ptr(t?)
For this model, we augment all child lists with a dis-
tinguished start node, c0(t), which has as its state
a distinguished start state, allowing us to capture
the unique behavior of the first (observed) child. To
parameterize this model, note that we will need to
define C(C + 1) multinomials, one for each parent
state and preceding child state (or a distinguished
start state).
3 To Infinity, and Beyond . . .
This section reviews needed background material
for our approach to making our tree models infinite.
3.1 The Dirichlet Process
Suppose we model a document as a bag of words
produced by a mixture model, where the mixture
components might be topics such as business, pol-
itics, sports, etc. Using this model we can generate a
4This requires stipulating a maximum list length.
273
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
P(xi = "game")
P(xi = "profit")
Figure 2: Plot of the density function of a Dirich-
let distribution H (the surface) as well as a draw
G (the vertical lines, or sticks) from a Dirichlet
process DP(?0,H) which has H as a base mea-
sure. Both distributions are defined over a sim-
plex in which each point corresponds to a particular
multinomial distribution over three possible words:
?profit?, ?game?, and ?election?. The placement of
the sticks is drawn from the distribution H , and is
independent of their lengths, which is drawn from a
stick-breaking process with parameter ?0.
document by first generating a distribution over top-
ics pi, and then for each position i in the document,
generating a topic zi from pi, and then a word xi
from the topic specific distribution ?zi . The word
distributions ?k for each topic k are drawn from a
base distribution H . In Section 2, we sample C
multinomials ?k from H . In the infinite mixture
model we sample an infinite number of multinomi-
als from H , using the Dirichlet process.
Formally, given a base distribution H and a con-
centration parameter ?0 (loosely speaking, this con-
trols the relative sizes of the topics), a Dirichlet pro-
cess DP(?0,H) is the distribution of a discrete ran-
dom probability measure G over the same (possibly
continuous) space that H is defined over; thus it is a
measure over measures. In Figure 2, the sticks (ver-
tical lines) show a draw G from a Dirichlet process
where the base measure H is a Dirichlet distribution
over 3 words. A draw comprises of an infinite num-
ber of sticks, and each corresponding topic.
We factor G into two coindexed distributions: pi,
a distribution over the integers, where the integer
represents the index of a particular topic (i.e., the
height of the sticks in the figure represent the proba-
bility of the topic indexed by that stick) and ?, rep-
resenting the word distribution of each of the top-
N
?
?0 H
pi ?k
zi
xi
pi|?0 ? GEM(?0)
?k|H ? H
zi|pi ? pi
xi|zi,? ? F (?zi) N
?
? ?0
? H
pij ?k
zji
xji
(a) (b)
Figure 3: A graphical representation of a simple
Dirichlet process mixture model (left) and a hierar-
chical Dirichlet process model (right). Note that we
show the stick-breaking representations of the mod-
els, in which we have factored G ? DP(?0,H) into
two sets of variables: pi and ?.
ics (i.e., the location of the sticks in the figure). To
generate pi we first generate an infinite sequence of
variables pi? = (pi?k)?k=1, each of which is distributed
according to the Beta distribution:
pi?k|?0 ? Beta(1, ?0)
Then pi = (pik)?k=1 is defined as:
pik = pi?k
?k?1
i=1
(1? pi?i)
Following Pitman (2002) we refer to this process as
pi ? GEM(?0). It should be noted that
??
k=1 pik =
1,5 and P (i) = pii. Then, according to the DP,
P (?i) = pii. The complete model, is shown graphi-
cally in Figure 3(a).
To build intuition, we walk through the process of
generating from the infinite mixture model for the
document example, where xi is the word at posi-
tion i, and zi is its topic. F is a multinomial dis-
tribution parameterized by ?, and H is a Dirichlet
distribution. Instead of generating all of the infinite
mixture components (pik)?k=1 at once, we can build
them up incrementally. If there are K known top-
ics, we represent only the known elements (pik)Kk=1
and represent the remaining probability mass piu =
5This is called the stick-breaking construction: we start with
a stick of unit length, representing the entire probability mass,
and successively break bits off the end of the stick, where the
proportional amount broken off is represented by pi?k and the
absolute amount is represented by pik.
274
?1 ?2 ?3 ?4 ?5 ?6 ?7 . . .
? :
pij :
. . .
Figure 4: A graphical representation of pij , a broken
stick, which is distributed according to a DP with a
broken stick ? as a base measure. Each ?k corre-
sponds to a ?k.
1 ? (?Kk=1 pik). Initially we have piu = 1 and
? = ().
For the ith position in the document, we first draw
a topic zi ? pi. If zi 6= u, then we find the coin-
dexed topic ?zi . If zi = u, the unseen topic, we
make a draw b ? Beta(1, ?0) and set piK+1 = bpiu
and pinewu = (1 ? b)piu. Then we draw a parame-
ter ?K+1 ? H for the new topic, resulting in pi =
(pi1, . . . , piK+1, pinewu ) and ? = (?1, . . . , ?K+1). A
word is then drawn from this topic and emitted by
the document.
3.2 The Hierarchical Dirichlet Process
Let?s generalize our previous example to a corpus
of documents. As before, we have a set of shared
topics, but now each document has its own charac-
teristic distribution over these topics. We represent
topic distributions both locally (for each document)
and globally (across all documents) by use of a hier-
archical Dirichlet process (HDP), which has a local
DP for each document, in which the base measure is
itself a draw from another, global, DP.
The complete HDP model is represented graphi-
cally in Figure 3(b). Like the DP, it has global bro-
ken stick ? = (?k)?k=1 and topic specific word dis-
tribution parameters ? = (?k)?k=1, which are coin-
dexed. It differs from the DP in that it also has lo-
cal broken sticks pij for each group j (in our case
documents). While the global stick ? ? GEM(?)
is generated as before, the local sticks pij are dis-
tributed according to a DP with base measure ?:
pij ? DP(?0,?).
We illustrate this generation process in Figure 4.
The upper unit line represents ?, where the size of
segment k represents the value of element ?k, and
the lower unit line represents pij ? DP(?0,?) for a
particular group j. Each element of the lower stick
was sampled from a particular element of the upper
stick, and elements of the upper stick may be sam-
pled multiple times or not at all; on average, larger
elements will be sampled more often. Each element
?k, as well as all elements of pij that were sampled
from it, corresponds to a particular ?k. Critically,
several distinct pij can be sampled from the same
?k and hence share ?k; this is how components are
shared among groups.
For concreteness, we show how to generate a cor-
pus of documents from the HDP, generating one
document at a time, and incrementally construct-
ing our infinite objects. Initially we have ?u = 1,
? = (), and piju = 1 for all j. We start with the
first position of the first document and draw a local
topic y11 ? pi1, which will return u with probabil-
ity 1. Because y11 = u we must make a draw from
the base measure, ?, which, because this is the first
document, will also return u with probability 1. We
must now break ?u into ?1 and ?newu , and break pi1u
into pi11 and pinew1u in the same manner presented for
the DP. Since pi11 now corresponds to global topic
1, we sample the word x11 ? Multinomial(?1). To
sample each subsequent word i, we first sample the
local topic y1i ? pi1. If y1i 6= u, and pi1y1i corre-
sponds to ?k in the global stick, then we sample the
word x1i ? Multinomial(?k). Once the first docu-
ment has been sampled, subsequent documents are
sampled in a similar manner; initially piju = 1 for
document j, while ? continues to grow as more doc-
uments are sampled.
4 Infinite Trees
We now use the techniques from Section 3 to create
infinite versions of each tree model from Section 2.
4.1 Independent Children
The changes required to make the Bayesian inde-
pendent children model infinite don?t affect its ba-
sic structure, as can be witnessed by comparing the
graphical depiction of the infinite model in Figure 5
with that of the finite model in Figure 1. The in-
stance variables zt and xt are parameterized as be-
fore. The primary change is that the number of
copies of the state plate is infinite, as are the number
of variables pik and ?k.
Note also that each distribution over possible
child states pik must also be infinite, since the num-
ber of possible child states is potentially infinite. We
achieve this by representing each of the pik variables
as a broken stick, and adopt the same approach of
275
?|? ? GEM(?)
pik|?0,? ? DP(?0,?)
?k|H ? H
?
? ?
?0 pik
H ?k
z1
z2 z3
x1 x2 x3
Figure 5: A graphical representation of the infinite
independent child model.
sampling each pik from a DP with base measure ?.
For the dependency tree application, ?k is a vector
representing the parameters of a multinomial over
words, and H is a Dirichlet distribution.
The infinite hidden Markov model (iHMM) or
HDP-HMM (Beal et al, 2002; Teh et al, 2006) is
a model of sequence data with transitions modeled
by an HDP.6 The iHMM can be viewed as a special
case of this model, where each state (except the stop
state) produces exactly one child.
4.2 Simultaneous Children
The key problem in the definition of the simulta-
neous children model is that of defining a distribu-
tion over the lists of children produced by each state,
since each child in the list has as its domain the posi-
tive integers, representing the infinite set of possible
states. Our solution is to construct a distribution Lk
over lists of states from the distribution over individ-
ual states pik. The obvious approach is to sample the
states at each position i.i.d.:
P((zt?)t??c(t)|pi) =
?
t??c(t)
P(zt? |pi) =
?
t??c(t)
pizt?
However, we want our model to be able to rep-
resent the fact that some child lists, ct, are more
or less probable than the product of the individual
child probabilities would indicate. To address this,
we can sample a state-conditional distribution over
child lists ?k from a DP with Lk as a base measure.
6The original iHMM paper (Beal et al, 2002) predates, and
was the motivation for, the work presented in Teh et al (2006),
and is the origin of the term hierarchical Dirichlet process.
However, they used the term to mean something slightly differ-
ent than the HDP presented in Teh et al (2006), and presented a
sampling scheme for inference that was a heuristic approxima-
tion of a Gibbs sampler.
Thus, we augment the basic model given in the pre-
vious section with the variables ? , Lk, and ?k:
Lk|pik ? Deterministic, as described above
?k|?, Lk ? DP(?, Lk)
ct|?k ? ?k
An important consequence of defining Lk locally
(instead of globally, using ? instead of the piks) is
that the model captures not only what sequences of
children a state prefers, but also the individual chil-
dren that state prefers; if a state gives high proba-
bility to some particular sequence of children, then
it is likely to also give high probability to other se-
quences containing those same states, or a subset
thereof.
4.3 Markov Children
In the Markov children model, more copies of the
variable pi are needed, because each child state must
be conditioned both on the parent state and on the
state of the preceding child. We use a new set of
variables piki, where pi is determined by the par-
ent state k and the state of the preceding sibling i.
Each of the piki is distributed as pik was in the basic
model: piki ? DP(?0,?).
5 Inference
Our goal in inference is to draw a sample from the
posterior over assignments of states to observations.
We present an inference procedure for the infinite
tree that is based on Gibbs sampling in the direct
assignment representation, so named because we di-
rectly assign global state indices to observations.7
Before we present the procedure, we define a few
count variables. Recall from Figure 4 that each state
k has a local stick pik, each element of which cor-
responds to an element of ?. In our sampling pro-
cedure, we only keep elements of pik and ? which
correspond to states observed in the data. We define
the variable mjk to be the number of elements of the
finite observed portion of pik which correspond to ?j
and njk to be the number of observations with state
k whose parent?s state is j.
We also need a few model-specific counts. For the
simultaneous children model we need njz, which is
7We adapt one of the sampling schemes mentioned by Teh
et al (2006) for use in the iHMM. This paper suggests two
sampling schemes for inference, but does not explicitly present
them. Upon discussion with one of the authors (Y. W. Teh,
2006, p.c.), it became clear that inference using the augmented
representation is much more complicated than initially thought.
276
the number of times the state sequence z occurred
as the children of state j. For the Markov chil-
dren model we need the count variable n?jik which
is the number of observations for a node with state
k whose parent?s state is j and whose previous sib-
ling?s state is i. In all cases we represent marginal
counts using dot-notation, e.g., n?k is the total num-
ber of nodes with state k, regardless of parent.
Our procedure alternates between three distinct
sampling stages: (1) sampling the state assignments
z, (2) sampling the counts mjk, and (3) sampling
the global stick ?. The only modification of the pro-
cedure that is required for the different tree mod-
els is the method for computing the probability
of the child state sequence given the parent state
P((zt?)t??c(t)|zt), defined separately for each model.
Sampling z. In this stage we sample a state for
each tree node. The probability of node t being as-
signed state k is given by:
P(zt = k|z?t,?) ? P(zt = k, (zt?)t??s(t)|zp(t))
? P((zt?)t??c(t)|zt = k) ? f?xtk (xt)
where s(t) denotes the set of siblings of t, f?xtk (xt)
denotes the posterior probability of observation xt
given all other observations assigned to state k, and
z?t denotes all state assignments except zt. In other
words, the probability is proportional to the product
of three terms: the probability of the states of t and
its siblings given its parent zp(t), the probability of
the states of the children c(t) given zt, and the pos-
terior probability of observation xt given zt. Note
that if we sample zt to be a previously unseen state,
we will need to extend ? as discussed in Section 3.2.
Now we give the equations for P((zt?)t??c(t)|zt)
for each of the models. In the independent child
model the probability of generating each child is:
Pind(zci(t) = k|zt = j) =
njk + ?0?k
nj? + ?0
Pind((zt?)t??c(t)|zt = j) =
?
t??c(t)
Pind(zt? |zt = j)
For the simultaneous child model, the probability of
generating a sequence of children, z, takes into ac-
count how many times that sequence has been gen-
erated, along with the likelihood of regenerating it:
Psim((zt?)t??c(t) = z|zt = j) =
njz + ?Pind(z|zt = j)
nj? + ?
Recall that ? denotes the concentration parameter
for the sequence generating DP. Lastly, we have the
DT NN IN DT NN VBD PRP$ NN TO VB NN EOS
The man in the corner taught his dachshund to play golf EOS
Figure 6: An example of a syntactic dependency tree
where the dependencies are between tags (hidden
states), and each tag generates a word (observation).
Markov child model:
Pm(zci(t) = k|zci?1(t) = i, zt = j) =
n?jik + ?0?k
n?ji? + ?0
Pm((zt?)t??c(t)|zt) =
?|c(t)|
i=1
Pm(zci(t)|zci?1(t), zt)
Finally, we give the posterior probability of an ob-
servation, given that F (?k) is Multinomial(?k), and
that H is Dirichlet(?, . . . , ?). Let N be the vocab-
ulary size and n?k be the number of observations x
with state k. Then:
f?xtk (xt) =
n?xtk + ?
n??k + N?
Sampling m. We use the following procedure,
which slightly modifies one from (Y. W. Teh, 2006,
p.c.), to sample each mjk:
SAMPLEM(j, k)
1 if njk = 0
2 then mjk = 0
3 else mjk = 1
4 for i? 2 to njk
5 do if rand() < ?0?0+i?1
6 then mjk = mjk + 1
7 return mjk
Sampling ?. Lastly, we sample ? using the Di-
richlet distribution:
(?1, . . . , ?K , ?u) ? Dirichlet(m?1, . . . ,m?K , ?0)
6 Experiments
We demonstrate infinite tree models on two dis-
tinct syntax learning tasks: unsupervised POS learn-
ing conditioned on untagged dependency trees and
learning a split of an existing tagset, which improves
the accuracy of an automatic syntactic parser.
For both tasks, we use a simple modification of
the basic model structure, to allow the trees to gen-
erate dependents on the left and the right with dif-
ferent distributions ? as is useful in modeling natu-
ral language. The modification of the independent
child tree is trivial: we have two copies of each of
277
the variables pik, one each for the left and the right.
Generation of dependents on the right is completely
independent of that for the left. The modifications of
the other models are similar, but now there are sepa-
rate sets of pik variables for the Markov child model,
and separate Lk and ?k variables for the simultane-
ous child model, for each of the left and right.
For both experiments, we used dependency trees
extracted from the Penn Treebank (Marcus et al,
1993) using the head rules and dependency extrac-
tor from Yamada and Matsumoto (2003). As is stan-
dard, we used WSJ sections 2?21 for training, sec-
tion 22 for development, and section 23 for testing.
6.1 Unsupervised POS Learning
In the first experiment, we do unsupervised part-of-
speech learning conditioned on dependency trees.
To be clear, the input to our algorithm is the de-
pendency structure skeleton of the corpus, but not
the POS tags, and the output is a labeling of each
of the words in the tree for word class. Since the
model knows nothing about the POS annotation, the
new classes have arbitrary integer names, and are
not guaranteed to correlate with the POS tag def-
initions. We found that the choice of ?0 and ?
(the concentration parameters) did not affect the out-
put much, while the value of ? (the parameter for
the base Dirichlet distribution) made a much larger
difference. For all reported experiments, we set
?0 = ? = 10 and varied ?.
We use several metrics to evaluate the word
classes. First, we use the standard approach of
greedily assigning each of the learned classes to the
POS tag with which it has the greatest overlap, and
then computing tagging accuracy (Smith and Eisner,
2005; Haghighi and Klein, 2006).8 Additionally, we
compute the mutual information of the learned clus-
ters with the gold tags, and we compute the cluster
F-score (Ghosh, 2003). See Table 1 for results of
the different models, parameter settings, and met-
rics. Given the variance in the number of classes
learned it is a little difficult to interpret these results,
but it is clear that the Markov child model is the
best; it achieves superior performance to the inde-
pendent child model on all metrics, while learning
fewer word classes. The poor performance of the
simultaneous model warrants further investigation,
but we observed that the distributions learned by that
8The advantage of this metric is that it?s comprehensible.
The disadvantage is that it?s easy to inflate by adding classes.
Model ? # Classes Acc. MI F1
Indep. 0.01 943 67.89 2.00 48.29
0.001 1744 73.61 2.23 40.80
0.0001 2437 74.64 2.27 39.47
Simul. 0.01 183 21.36 0.31 21.57
0.001 430 15.77 0.09 13.80
0.0001 549 16.68 0.12 14.29
Markov 0.01 613 68.53 2.12 49.82
0.001 894 75.34 2.31 48.73
Table 1: Results of part unsupervised POS tagging
on the different models, using a greedy accuracy
measure.
model are far more spiked, potentially due to double
counting of tags, since the sequence probabilities are
already based on the local probabilities.
For comparison, Haghighi and Klein (2006) re-
port an unsupervised baseline of 41.3%, and a best
result of 80.5% from using hand-labeled prototypes
and distributional similarity. However, they train on
less data, and learn fewer word classes.
6.2 Unsupervised POS Splitting
In the second experiment we use the infinite tree
models to learn a refinement of the PTB tags. We
initialize the set of hidden states to the set of PTB
tags, and then, during inference, constrain the sam-
pling distribution over hidden state zt at each node t
to include only states that are a refinement of the an-
notated PTB tag at that position. The output of this
training procedure is a new annotation of the words
in the PTB with the learned tags. We then compare
the performance of a generative dependency parser
trained on the new refined tags with one trained on
the base PTB tag set. We use the generative de-
pendency parser distributed with the Stanford fac-
tored parser (Klein and Manning, 2003b) for the
comparison, since it performs simultaneous tagging
and parsing during testing. In this experiment, un-
labeled, directed, dependency parsing accuracy for
the best model increased from 85.11% to 87.35%, a
15% error reduction. See Table 2 for the full results
over all models and parameter settings.
7 Related Work
The HDP-PCFG (Liang et al, 2007), developed at
the same time as this work, aims to learn state splits
for a binary-branching PCFG. It is similar to our
simultaneous child model, but with several impor-
tant distinctions. As discussed in Section 4.2, in our
model each state has a DP over sequences, with a
base distribution that is defined over the local child
278
Model ? Accuracy
Baseline ? 85.11
Independent 0.01 86.18
0.001 85.88
Markov 0.01 87.15
0.001 87.35
Table 2: Results of untyped, directed dependency
parsing, where the POS tags in the training data have
been split according to the various models. At test
time, the POS tagging and parsing are done simulta-
neously by the parser.
state probabilities. In contrast, Liang et al (2007)
define a global DP over sequences, with the base
measure defined over the global state probabilities,
?; locally, each state has an HDP, with this global
DP as the base measure. We believe our choice to
be more linguistically sensible: in our model, for a
particular state, dependent sequences which are sim-
ilar to one another increase one another?s likelihood.
Additionally, their modeling decision made it diffi-
cult to define a Gibbs sampler, and instead they use
variational inference. Earlier, Johnson et al (2007)
presented adaptor grammars, which is a very simi-
lar model to the HDP-PCFG. However they did not
confine themselves to a binary branching structure
and presented a more general framework for defin-
ing the process for splitting the states.
8 Discussion and Future Work
We have presented a set of novel infinite tree models
and associated inference algorithms, which are suit-
able for representing syntactic dependency structure.
Because the models represent a potentially infinite
number of hidden states, they permit unsupervised
learning algorithms which naturally select a num-
ber of word classes, or tags, based on qualities of
the data. Although they require substantial techni-
cal background to develop, the learning algorithms
based on the models are actually simple in form, re-
quiring only the maintenance of counts, and the con-
struction of sampling distributions based on these
counts. Our experimental results are preliminary but
promising: they demonstrate that the model is capa-
ble of capturing important syntactic structure.
Much remains to be done in applying infinite
models to language structure, and an interesting ex-
tension would be to develop inference algorithms
that permit completely unsupervised learning of de-
pendency structure.
Acknowledgments
Many thanks to Yeh Whye Teh for several enlight-
ening conversations, and to the following mem-
bers (and honorary member) of the Stanford NLP
group for comments on an earlier draft: Thad
Hughes, David Hall, Surabhi Gupta, Ani Nenkova,
Sebastian Riedel. This work was supported by a
Scottish Enterprise Edinburgh-Stanford Link grant
(R37588), as part of the EASIE project, and by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Phase II Program.
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametrics. Annals of Statistics,
2:1152?1174.
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural Infor-
mation Processing Systems, pages 577?584.
E. Charniak. 1996. Tree-bank grammars. In AAAI 1996, pages
1031?1036.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
HLT-NAACL 2000, pages 132?139.
M. Collins. 2003. Head-driven statistical models for natural lan-
guage parsing. Computational Linguistics, 29(4):589?637.
T. S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. Ghosh. 2003. Scalable clustering methods for data mining. In
N. Ye, editor, Handbook of Data Mining, chapter 10, pages
247?277. Lawrence Erlbaum Assoc.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In HLT-NAACL 2006.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Adaptor
grammars: A framework for specifying compositional non-
parametric Bayesian models. In NIPS 2007.
D. Klein and C. D. Manning. 2003a. Accurate unlexicalized
parsing. In ACL 2003.
D. Klein and C. D. Manning. 2003b. Factored A* search for
models over sequences and trees. In IJCAI 2003.
P. Liang, S. Petrov, D. Klein, and M. Jordan. 2007. Nonpara-
metric PCFGs using Dirichlet processes. In EMNLP 2007.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL
44/COLING 21, pages 433?440.
J. Pitman. 2002. Poisson-Dirichlet and GEM invariant distribu-
tions for split-and-merge transformations of an interval par-
tition. Combinatorics, Probability and Computing, 11:501?
514.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In ACL 2005.
Y. W. Teh, M.I. Jordan, M. J. Beal, and D.M. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American Statis-
tical Association, 101:1566?1581.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT, pages 195?206.
279
Proceedings of ACL-08: HLT, pages 959?967,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Efficient, Feature-based, Conditional Random Field Parsing
Jenny Rose Finkel, Alex Kleeman, Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
jrfinkel@cs.stanford.edu, akleeman@stanford.edu, manning@cs.stanford.edu
Abstract
Discriminative feature-based methods are
widely used in natural language processing,
but sentence parsing is still dominated by gen-
erative methods. While prior feature-based
dynamic programming parsers have restricted
training and evaluation to artificially short sen-
tences, we present the first general, feature-
rich discriminative parser, based on a condi-
tional random field model, which has been
successfully scaled to the full WSJ parsing
data. Our efficiency is primarily due to the
use of stochastic optimization techniques, as
well as parallelization and chart prefiltering.
On WSJ15, we attain a state-of-the-art F-score
of 90.9%, a 14% relative reduction in error
over previous models, while being two orders
of magnitude faster. On sentences of length
40, our system achieves an F-score of 89.0%,
a 36% relative reduction in error over a gener-
ative baseline.
1 Introduction
Over the past decade, feature-based discriminative
models have become the tool of choice for many
natural language processing tasks. Although they
take much longer to train than generative models,
they typically produce higher performing systems,
in large part due to the ability to incorporate ar-
bitrary, potentially overlapping features. However,
constituency parsing remains an area dominated by
generative methods, due to the computational com-
plexity of the problem. Previous work on discrim-
inative parsing falls under one of three approaches.
One approach does discriminative reranking of the
n-best list of a generative parser, still usually de-
pending highly on the generative parser score as
a feature (Collins, 2000; Charniak and Johnson,
2005). A second group of papers does parsing by a
sequence of independent, discriminative decisions,
either greedily or with use of a small beam (Ratna-
parkhi, 1997; Henderson, 2004). This paper extends
the third thread of work, where joint inference via
dynamic programming algorithms is used to train
models and to attempt to find the globally best parse.
Work in this context has mainly been limited to use
of artificially short sentences due to exorbitant train-
ing and inference times. One exception is the re-
cent work of Petrov et al (2007), who discrimina-
tively train a grammar with latent variables and do
not restrict themselves to short sentences. However
their model, like the discriminative parser of John-
son (2001), makes no use of features, and effectively
ignores the largest advantage of discriminative train-
ing. It has been shown on other NLP tasks that mod-
eling improvements, such as the switch from gen-
erative training to discriminative training, usually
provide much smaller performance gains than the
gains possible from good feature engineering. For
example, in (Lafferty et al, 2001), when switching
from a generatively trained hidden Markov model
(HMM) to a discriminatively trained, linear chain,
conditional random field (CRF) for part-of-speech
tagging, their error drops from 5.7% to 5.6%. When
they add in only a small set of orthographic fea-
tures, their CRF error rate drops considerably more
to 4.3%, and their out-of-vocabulary error rate drops
by more than half. This is further supported by John-
son (2001), who saw no parsing gains when switch-
959
ing from generative to discriminative training, and
by Petrov et al (2007) who saw only small gains of
around 0.7% for their final model when switching
training methods.
In this work, we provide just such a framework for
training a feature-rich discriminative parser. Unlike
previous work, we do not restrict ourselves to short
sentences, but we do provide results both for training
and testing on sentences of length ? 15 (WSJ15) and
for training and testing on sentences of length ? 40,
allowing previous WSJ15 results to be put in context
with respect to most modern parsing literature. Our
model is a conditional random field based model.
For a rule application, we allow arbitrary features
to be defined over the rule categories, span and split
point indices, and the words of the sentence. It is
well known that constituent length influences parse
probability, but PCFGs cannot easily take this infor-
mation into account. Another benefit of our feature
based model is that it effortlessly allows smooth-
ing over previously unseen rules. While the rule
may be novel, it will likely contain features which
are not. Practicality comes from three sources. We
made use of stochastic optimization methods which
allow us to find optimal model parameters with very
few passes through the data. We found no differ-
ence in parser performance between using stochastic
gradient descent (SGD), and the more common, but
significantly slower, L-BFGS. We also used limited
parallelization, and prefiltering of the chart to avoid
scoring rules which cannot tile into complete parses
of the sentence. This speed-up does not come with a
performance cost; we attain an F-score of 90.9%, a
14% relative reduction in errors over previous work
on WSJ15.
2 The Model
2.1 A Conditional Random Field Context Free
Grammar (CRF-CFG)
Our parsing model is based on a conditional ran-
dom field model, however, unlike previous TreeCRF
work, e.g., (Cohn and Blunsom, 2005; Jousse et al,
2006), we do not assume a particular tree structure,
and instead find the most likely structure and la-
beling. This is similar to conventional probabilis-
tic context-free grammar (PCFG) parsing, with two
exceptions: (a) we maximize conditional likelihood
of the parse tree, given the sentence, not joint like-
lihood of the tree and sentence; and (b) probabil-
ities are normalized globally instead of locally ?
the graphical models depiction of our trees is undi-
rected.
Formally, we have a CFG G, which consists of
(Manning and Schu?tze, 1999): (i) a set of termi-
nals {wk},k = 1, . . . ,V ; (ii) a set of nonterminals
{Nk},k = 1, . . . ,n; (iii) a designated start symbol
ROOT ; and (iv) a set of rules, {? = N i ? ? j}, where
? j is a sequence of terminals and nonterminals. A
PCFG additionally assigns probabilities to each rule
? such that ?i? j P(N i ? ? j) = 1. Our conditional
random field CFG (CRF-CFG) instead defines local
clique potentials ?(r|s;?), where s is the sentence,
and r contains a one-level subtree of a tree t, corre-
sponding to a rule ? , along with relevant information
about the span of words which it encompasses, and,
if applicable, the split position (see Figure 1). These
potentials are relative to the sentence, unlike a PCFG
where rule scores do not have access to words at the
leaves of the tree, or even how many words they
dominate. We then define a conditional probabil-
ity distribution over entire trees, using the standard
CRF distribution, shown in (1). There is, however,
an important subtlety lurking in how we define the
partition function. The partition function Zs, which
makes the probability of all possible parses sum to
unity, is defined over all structures as well as all la-
belings of those structures. We define ?(s) to be the
set of all possible parse trees for the given sentence
licensed by the grammar G.
P(t|s;?) = 1
Zs ?r?t ?(r|s;?) (1)
where
Zs = ?t??(s) ?r?t ? ?(r|s;?)
The above model is not well-defined over all
CFGs. Unary rules of the form N i ? N j can form
cycles, leading to infinite unary chains with infinite
mass. However, it is standard in the parsing liter-
ature to transform grammars into a restricted class
of CFGs so as to permit efficient parsing. Binariza-
tion of rules (Earley, 1970) is necessary to obtain
cubic parsing time, and closure of unary chains is re-
quired for finding total probability mass (rather than
just best parses) (Stolcke, 1995). To address this is-
sue, we define our model over a restricted class of
960
SNP
NN
Factory
NNS
payrolls
VP
VBD
fell
PP
IN
in
NN
September
Phrasal rules
r1 = S0,5 ? NP0,2 VP2,5 | Factory payrolls fell in September
r3 = VP2,5 ? VBD2,3 PP3,5 | Factory payrolls fell in September
. . .
Lexicon rules
r5 = NN0,1 ? Factory | Factory payrolls fell in September
r6 = NNS1,2 ? payrolls | Factory payrolls fell in September
. . .
(a) PCFG Structure (b) Rules r
Figure 1: A parse tree and the corresponding rules over which potentials and features are defined.
CFGs which limits unary chains to not have any re-
peated states. This was done by collapsing all al-
lowed unary chains to single unary rules, and dis-
allowing multiple unary rule applications over the
same span.1 We give the details of our binarization
scheme in Section 5. Note that there exists a gram-
mar in this class which is weakly equivalent with any
arbitrary CFG.
2.2 Computing the Objective Function
Our clique potentials take an exponential form. We
have a feature function, represented by f (r,s), which
returns a vector with the value for each feature. We
denote the value of feature fi by fi(r,s) and our
model has a corresponding parameter ?i for each
feature. The clique potential function is then:
?(r|s;?) = exp?i ?i fi(r,s) (2)
The log conditional likelihood of the training data
D , with an additional L2 regularization term, is then:
L (D ;?) =
(
?
(t,s)?D
(
?
r?t
?
i
?i fi(r,s)
)
?Zs
)
+?
i
?2i
2? 2
(3)
And the partial derivatives of the log likelihood, with
respect to the model weights are, as usual, the dif-
ference between the empirical counts and the model
expectations:
?L
??i
=
(
?
(t,s)?D
(
?
r?t
fi(r,s)
)
?E? [ fi|s]
)
+ ?i
? 2
(4)
1In our implementation of the inside-outside algorithm, we
then need to keep two inside and outside scores for each span:
one from before and one from after the application of unary
rules.
The partition function Zs and the partial derivatives
can be efficiently computed with the help of the
inside-outside algorithm.2 Zs is equal to the in-
side score of ROOT over the span of the entire sen-
tence. To compute the partial derivatives, we walk
through each rule, and span/split, and add the out-
side log-score of the parent, the inside log-score(s)
of the child(ren), and the log-score for that rule and
span/split. Zs is subtracted from this value to get the
normalized log probability of that rule in that posi-
tion. Using the probabilities of each rule applica-
tion, over each span/split, we can compute the ex-
pected feature values (the second term in Equation
4), by multiplying this probability by the value of
the feature corresponding to the weight for which we
are computing the partial derivative. The process is
analogous to the computation of partial derivatives
in linear chain CRFs. The complexity of the algo-
rithm for a particular sentence is O(n3), where n is
the length of the sentence.
2.3 Parallelization
Unlike (Taskar et al, 2004), our algorithm has the
advantage of being easily parallelized (see footnote
7 in their paper). Because the computation of both
the log likelihood and the partial derivatives involves
summing over each tree individually, the compu-
tation can be parallelized by having many clients
which each do the computation for one tree, and one
central server which aggregates the information to
compute the relevant information for a set of trees.
Because we use a stochastic optimization method,
as discussed in Section 3, we compute the objec-
tive for only a small portion of the training data at
a time, typically between 15 and 30 sentences. In
2In our case the values in the chart are the clique potentials
which are non-negative numbers, but not probabilities.
961
this case the gains from adding additional clients
decrease rapidly, because the computation time is
dominated by the longest sentences in the batch.
2.4 Chart Prefiltering
Training is also sped up by prefiltering the chart. On
the inside pass of the algorithm one will see many
rules which cannot actually be tiled into complete
parses. In standard PCFG parsing it is not worth fig-
uring out which rules are viable at a particular chart
position and which are not. In our case however this
can make a big difference.We are not just looking
up a score for the rule, but must compute all the fea-
tures, and dot product them with the feature weights,
which is far more time consuming. We also have to
do an outside pass as well as an inside one, which
is sped up by not considering impossible rule appli-
cations. Lastly, we iterate through the data multi-
ple times, so if we can compute this information just
once, we will save time on all subsequent iterations
on that sentence. We do this by doing an inside-
outside pass that is just boolean valued to determine
which rules are possible at which positions in the
chart. We simultaneously compute the features for
the possible rules and then save the entire data struc-
ture to disk. For all but the shortest of sentences,
the disk I/O is easily worth the time compared to re-
computation. The first time we see a sentence this
method is still about one third faster than if we did
not do the prefiltering, and on subsequent iterations
the improvement is closer to tenfold.
3 Stochastic Optimization Methods
Stochastic optimization methods have proven to be
extremely efficient for the training of models involv-
ing computationally expensive objective functions
like those encountered with our task (Vishwanathan
et al, 2006) and, in fact, the on-line backpropagation
learning used in the neural network parser of Hen-
derson (2004) is a form of stochastic gradient de-
scent. Standard deterministic optimization routines
such as L-BFGS (Liu and Nocedal, 1989) make little
progress in the initial iterations, often requiring sev-
eral passes through the data in order to satisfy suffi-
cient descent conditions placed on line searches. In
our experiments SGD converged to a lower objective
function value than L-BFGS, however it required far
0 5 10 15 20 25 30 35 40 45 50
?3.5
?3
?2.5
?2
?1.5
?1
?0.5
0 x 10
5
Passes
Lo
g 
Li
ke
lih
oo
d
SGD
L?BFGS
Figure 2: WSJ15 objective value for L-BFGS and SGD
versus passes through the data. SGD ultimately con-
verges to a lower objective value, but does equally well
on test data.
fewer iterations (see Figure 2) and achieved compa-
rable test set performance to L-BFGS in a fraction of
the time. One early experiment on WSJ15 showed a
seven time speed up.
3.1 Stochastic Function Evaluation
Utilization of stochastic optimization routines re-
quires the implementation of a stochastic objective
function. This function, ?L is designed to approx-
imate the true function L based off a small subset
of the training data represented by Db. Here b, the
batch size, means that Db is created by drawing b
training examples, with replacement, from the train-
ing set D . With this notation we can express the
stochastic evaluation of the function as ?L (Db;?).
This stochastic function must be designed to ensure
that:
E
[
?ni ?L (D(i)b ;?)
]
= L (D ;?)
Note that this property is satisfied, without scaling,
for objective functions that sum over the training
data, as it is in our case, but any priors must be
scaled down by a factor of b/ |D |. The stochastic
gradient, ?L (D(i)b ;?), is then simply the derivative
of the stochastic function value.
3.2 Stochastic Gradient Descent
SGD was implemented using the standard update:
?k+1 = ?k ??k?L (D(k)b ;?k)
962
And employed a gain schedule in the form
?k = ?0
?
? + k
where parameter ? was adjusted such that the gain is
halved after five passes through the data. We found
that an initial gain of ?0 = 0.1 and batch size be-
tween 15 and 30 was optimal for this application.
4 Features
As discussed in Section 5 we performed experi-
ments on both sentences of length ? 15 and length
? 40. All feature development was done on the
length 15 corpus, due to the substantially faster
train and test times. This has the unfortunate effect
that our features are optimized for shorter sentences
and less training data, but we found development
on the longer sentences to be infeasible. Our fea-
tures are divided into two types: lexicon features,
which are over words and tags, and grammar fea-
tures which are over the local subtrees and corre-
sponding span/split (both have access to the entire
sentence). We ran two kinds of experiments: a dis-
criminatively trained model, which used only the
rules and no other grammar features, and a feature-
based model which did make use of grammar fea-
tures. Both models had access to the lexicon fea-
tures. We viewed this as equivalent to the more
elaborate, smoothed unknown word models that are
common in many PCFG parsers, such as (Klein and
Manning, 2003; Petrov et al, 2006).
We preprocessed the words in the sentences to ob-
tain two extra pieces of information. Firstly, each
word is annotated with a distributional similarity tag,
from a distributional similarity model (Clark, 2000)
trained on 100 million words from the British Na-
tional Corpus and English Gigaword corpus. Sec-
ondly, we compute a class for each word based on
the unknown word model of Klein and Manning
(2003); this model takes into account capitaliza-
tion, digits, dashes, and other character-level fea-
tures. The full set of features, along with an expla-
nation of our notation, is listed in Table 1.
5 Experiments
For all experiments, we trained and tested on the
Penn treebank (PTB) (Marcus et al, 1993). We used
Binary Unary
Model States Rules Rules
WSJ15 1,428 5,818 423
WSJ15 relaxed 1,428 22,376 613
WSJ40 7,613 28,240 823
Table 2: Grammar size for each of our models.
the standard splits, training on sections 2 to 21, test-
ing on section 23 and doing development on section
22. Previous work on (non-reranking) discrimina-
tive parsing has given results on sentences of length
? 15, but most parsing literature gives results on ei-
ther sentences of length ? 40, or all sentences. To
properly situate this work with respect to both sets
of literature we trained models on both length ?
15 (WSJ15) and length ? 40 (WSJ40), and we also
tested on all sentences using the WSJ40 models. Our
results also provide a context for interpreting previ-
ous work which used WSJ15 and not WSJ40.
We used a relatively simple grammar with few ad-
ditional annotations. Starting with the grammar read
off of the training set, we added parent annotations
onto each state, including the POS tags, resulting in
rules such as S-ROOT ? NP-S VP-S. We also added
head tag annotations to VPs, in the same manner as
(Klein and Manning, 2003). Lastly, for the WSJ40
runs we used a simple, right branching binarization
where each active state is annotated with its previous
sibling and first child. This is equivalent to children
of a state being produced by a second order Markov
process. For the WSJ15 runs, each active state was
annotated with only its first child, which is equiva-
lent to a first order Markov process. See Table 5 for
the number of states and rules produced.
5.1 Experiments
For both WSJ15 and WSJ40, we trained a genera-
tive model; a discriminative model, which used lexi-
con features, but no grammar features other than the
rules themselves; and a feature-based model which
had access to all features. For the length 15 data we
also did experiments in which we relaxed the gram-
mar. By this we mean that we added (previously un-
seen) rules to the grammar, as a means of smoothing.
We chose which rules to add by taking existing rules
and modifying the parent annotation on the parent
of the rule. We used stochastic gradient descent for
963
Table 1: Lexicon and grammar features. w is the word and t the tag. r represents a particular rule along with span/split
information; ? is the rule itself, rp is the parent of the rule; wb, ws, and we are the first, first after the split (for binary
rules) and last word that a rule spans in a particular context. All states, including the POS tags, are annotated with
parent information; b(s) represents the base label for a state s and p(s) represents the parent annotation on state s.
ds(w) represents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the
unknown word class.
Lexicon Features Grammar Features
t Binary-specific features
b(t) ?
?t,w? ?b(p(rp)),ds(ws)? ?b(p(rp)),ds(ws?1,dsws)?
?t, lc(w)? ?b(p(rp)),ds(we)? PP feature:
?b(t),w? unary? if right child is a PP then ?r,ws?
?b(t), lc(w)? simplified rule: VP features:
?t,ds(w)? base labels of states if some child is a verb tag, then rule,
?t,ds(w?1)? dist sim bigrams: with that child replaced by the word
?t,ds(w+1)? all dist. sim. bigrams below
?b(t),ds(w)? rule, and base parent state Unaries which span one word:
?b(t),ds(w?1)? dist sim bigrams:
?b(t),ds(w+1)? same as above, but trigrams ?r,w?
?p(t),w? heavy feature: ?r,ds(w)?
?t,unk(w)? whether the constituent is ?big? ?b(p(r)),w?
?b(t),unk(w)? as described in (Johnson, 2001) ?b(p(r)),ds(w)?
these experiments; the length 15 models had a batch
size of 15 and we allowed twenty passes through
the data.3 The length 40 models had a batch size
of 30 and we allowed ten passes through the data.
We used development data to decide when the mod-
els had converged. Additionally, we provide gener-
ative numbers for training on the entire PTB to give
a sense of how much performance suffered from the
reduced training data (generative-all in Table 4).
The full results for WSJ15 are shown in Table 3
and for WSJ40 are shown in Table 4. The WSJ15
models were each trained on a single Dual-Core
AMD OpteronTM using three gigabytes of RAM and
no parallelization. The discriminatively trained gen-
erative model (discriminative in Table 3) took ap-
proximately 12 minutes per pass through the data,
while the feature-based model (feature-based in Ta-
ble 3) took 35 minutes per pass through the data.
The feature-based model with the relaxed grammar
(relaxed in Table 3) took about four times as long
as the regular feature-based model. The discrimina-
3Technically we did not make passes through the data, be-
cause we sampled with replacement to get our batches. By this
we mean having seen as many sentences as are in the data, de-
spite having seen some sentences multiple times and some not
at all.
tively trained generative WSJ40 model (discrimina-
tive in Table 4) was trained using two of the same
machines, with 16 gigabytes of RAM each for the
clients.4 It took about one day per pass through
the data. The feature-based WSJ40 model (feature-
based in Table 4) was trained using four of these
machines, also with 16 gigabytes of RAM each for
the clients. It took about three days per pass through
the data.
5.2 Discussion
The results clearly show that gains came from both
the switch from generative to discriminative train-
ing, and from the extensive use of features. In Fig-
ure 3 we show for an example from section 22 the
parse trees produced by our generative model and
our feature-based discriminative model, and the cor-
rect parse. The parse from the feature-based model
better exhibits the right branching tendencies of En-
glish. This is likely due to the heavy feature, which
encourages long constituents at the end of the sen-
tence. It is difficult for a standard PCFG to learn this
aspect of the English language, because the score it
assigns to a rule does not take its span into account.
4The server does almost no computation.
964
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
development set ? length ? 15 test set ? length ? 15
Taskar 2004 89.7 90.2 90.0 ? ? ? 89.1 89.1 89.1 ? ? ?
Turian 2007 ? ? ? ? ? ? 89.6 89.3 89.4 ? ? ?
generative 86.9 85.8 86.4 46.2 0.34 81.2 87.6 85.8 86.7 49.2 0.33 81.9
discriminative 89.1 88.6 88.9 55.5 0.26 85.5 88.9 88.0 88.5 56.6 0.32 85.0
feature-based 90.4 89.3 89.9 59.5 0.24 88.3 91.1 90.2 90.6 61.3 0.24 86.8
relaxed 91.2 90.3 90.7 62.1 0.24 88.1 91.4 90.4 90.9 62.0 0.22 87.9
Table 3: Development and test set results, training and testing on sentences of length ? 15 from the Penn treebank.
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
test set ? length ? 40 test set ? all sentences
Petrov 2007 ? ? 88.8 ? ? ? ? ? 88.3 ? ? ?
generative 83.5 82.0 82.8 25.5 1.57 53.4 82.8 81.2 82.0 23.8 1.83 50.4
generative-all 83.6 82.1 82.8 25.2 1.56 53.3 ? ? ? ? ? ?
discriminative 85.1 84.5 84.8 29.7 1.41 55.8 84.2 83.7 83.9 27.8 1.67 52.8
feature-based 89.2 88.8 89.0 37.3 0.92 65.1 88.2 87.8 88.0 35.1 1.15 62.3
Table 4: Test set results, training on sentences of length ? 40 from the Penn treebank. The generative-all results were
trained on all sentences regardless of length
6 Comparison With Related Work
The most similar related work is (Johnson, 2001),
which did discriminative training of a generative
PCFG. The model was quite similar to ours, except
that it did not incorporate any features and it re-
quired the parameters (which were just scores for
rules) to be locally normalized, as with a genera-
tively trained model. Due to training time, they used
the ATIS treebank corpus , which is much smaller
than even WSJ15, with only 1,088 training sen-
tences, 294 testing sentences, and an average sen-
tence length of around 11. They found no signif-
icant difference in performance between their gen-
eratively and discriminatively trained parsers. There
are two probable reasons for this result. The training
set is very small, and it is a known fact that gener-
ative models tend to work better for small datasets
and discriminative models tend to work better for
larger datasets (Ng and Jordan, 2002). Additionally,
they made no use of features, one of the primary
benefits of discriminative learning.
Taskar et al (2004) took a large margin approach
to discriminative learning, but achieved only small
gains. We suspect that this is in part due to the gram-
mar that they chose ? the grammar of (Klein and
Manning, 2003), which was hand annotated with the
intent of optimizing performance of a PCFG. This
grammar is fairly sparse ? for any particular state
there are, on average, only a few rules with that state
as a parent ? so the learning algorithm may have suf-
fered because there were few options to discriminate
between. Starting with this grammar we found it dif-
ficult to achieve gains as well. Additionally, their
long training time (several months for WSJ15, ac-
cording to (Turian and Melamed, 2006)) made fea-
ture engineering difficult; they were unable to really
explore the space of possible features.
More recent is the work of (Turian and Melamed,
2006; Turian et al, 2007), which improved both the
training time and accuracy of (Taskar et al, 2004).
They define a simple linear model, use boosted de-
cision trees to select feature conjunctions, and a line
search to optimize the parameters. They use an
agenda parser, and define their atomic features, from
which the decision trees are constructed, over the en-
tire state being considered. While they make exten-
sive use of features, their setup is much more com-
plex than ours and takes substantially longer to train
? up to 5 days on WSJ15 ? while achieving only
small gains over (Taskar et al, 2004).
The most recent similar research is (Petrov et al,
2007). They also do discriminative parsing of length
40 sentences, but with a substantially different setup.
Following up on their previous work (Petrov et al,
2006) on grammar splitting, they do discriminative
965
SS
NP
PRP
He
VP
VBZ
adds
NP
DT
This
VP
VBZ
is
RB
n?t
NP
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
n?t
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
n?t
NP
NP
CD
1987
VP
VBN
revisited
(a) generative output (b) feature-based discriminative output (c) gold parse
Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse.
parsing with latent variables, which requires them
to optimize a non-convex function. Instead of us-
ing a stochastic optimization technique, they use L-
BFGS, but do coarse-to-fine pruning to approximate
their gradients and log likelihood. Because they
were focusing on grammar splitting they, like (John-
son, 2001), did not employ any features, and, like
(Taskar et al, 2004), they saw only small gains from
switching from generative to discriminative training.
7 Conclusions
We have presented a new, feature-rich, dynamic pro-
gramming based discriminative parser which is sim-
pler, more effective, and faster to train and test than
previous work, giving us new state-of-the-art per-
formance when training and testing on sentences of
length ? 15 and the first results for such a parser
trained and tested on sentences of length ? 40. We
also show that the use of SGD for training CRFs per-
forms as well as L-BFGS in a fraction of the time.
Other recent work on discriminative parsing has ne-
glected the use of features, despite their being one of
the main advantages of discriminative training meth-
ods. Looking at how other tasks, such as named
entity recognition and part-of-speech tagging, have
evolved over time, it is clear that greater gains are to
be gotten from developing better features than from
better models. We have provided just such a frame-
work for improving parsing performance.
Acknowledgments
Many thanks to Teg Grenager and Paul Heymann
for their advice (and their general awesomeness),
and to our anonymous reviewers for helpful com-
ments.
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM, by the Disruptive Technology Office
(DTO) Phase III Program for Advanced Question
Answering for Intelligence (AQUAINT) through
Broad Agency Announcement (BAA) N61339-06-
R-0034, and by a Scottish Enterprise Edinburgh-
Stanford Link grant (R37588), as part of the EASIE
project.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL 43, pages 173?180.
Alexander Clark. 2000. Inducing syntactic categories by
context distribution clustering. In Proc. of Conference
on Computational Natural Language Learning, pages
91?94, Lisbon, Portugal.
Trevor Cohn and Philip Blunsom. 2005. Semantic
role labelling with tree conditional random fields. In
CoNLL 2005.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML 17, pages 175?182.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In ACL 42, pages 96?
103.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Meeting of the Associ-
ation for Computational Linguistics, pages 314?321.
Florent Jousse, Re?mi Gilleron, Isabelle Tellier, and Marc
Tommasi. 2006. Conditional Random Fields for XML
966
trees. In ECML Workshop on Mining and Learning in
Graphs.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Associa-
tion of Computational Linguistics (ACL).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
ICML 2001, pages 282?289. Morgan Kaufmann, San
Francisco, CA.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503?528.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs. generative classifiers: A comparison of logistic
regression and naive bayes. In Advances in Neural In-
formation Processing Systems (NIPS).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In ACL 44/COLING 21,
pages 433?440.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Dis-
criminative log-linear grammars with latent variables.
In NIPS.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
EMNLP 2, pages 1?10.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21:165?
202.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher D. Manning. 2004. Max-margin
parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Joseph Turian and I. Dan Melamed. 2006. Advances in
discriminative parsing. In ACL 44, pages 873?880.
Joseph Turian, Ben Wellington, and I. Dan Melamed.
2007. Scalable discriminative learning for natural lan-
guage parsing and translation. In Advances in Neural
Information Processing Systems 19, pages 1409?1416.
MIT Press.
S. V. N. Vishwanathan, Nichol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML 23, pages 969?976.
967
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45?48,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enforcing Transitivity in Coreference Resolution
Jenny Rose Finkel and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{jrfinkel|manning}@cs.stanford.edu
Abstract
A desirable quality of a coreference resolution
system is the ability to handle transitivity con-
straints, such that even if it places high like-
lihood on a particular mention being corefer-
ent with each of two other mentions, it will
also consider the likelihood of those two men-
tions being coreferent when making a final as-
signment. This is exactly the kind of con-
straint that integer linear programming (ILP)
is ideal for, but, surprisingly, previous work
applying ILP to coreference resolution has not
encoded this type of constraint. We train a
coreference classifier over pairs of mentions,
and show how to encode this type of constraint
on top of the probabilities output from our
pairwise classifier to extract the most probable
legal entity assignments. We present results
on two commonly used datasets which show
that enforcement of transitive closure consis-
tently improves performance, including im-
provements of up to 3.6% using the b3 scorer,
and up to 16.5% using cluster f-measure.
1 Introduction
Much recent work on coreference resolution, which
is the task of deciding which noun phrases, or men-
tions, in a document refer to the same real world
entity, builds on Soon et al (2001). They built a
decision tree classifier to label pairs of mentions as
coreferent or not. Using their classifier, they would
build up coreference chains, where each mention
was linked up with the most recent previous men-
tion that the classifier labeled as coreferent, if such
a mention existed. Transitive closure in this model
was done implicitly. If John Smith was labeled
coreferent with Smith, and Smith with Jane Smith,
then John Smith and Jane Smith were also corefer-
ent regardless of the classifier?s evaluation of that
pair. Much work that followed improved upon this
strategy, by improving the features (Ng and Cardie,
2002b), the type of classifier (Denis and Baldridge,
2007), and changing mention links to be to the most
likely antecedent rather than the most recent posi-
tively labeled antecedent (Ng and Cardie, 2002b).
This line of work has largely ignored the implicit
transitivity of the decisions made, and can result in
unintuitive chains such as the Smith chain just de-
scribed, where each pairwise decision is sensible,
but the final result is not.
Ng and Cardie (2002a) and Ng (2004) highlight
the problem of determining whether or not common
noun phrases are anaphoric. They use two clas-
sifiers, an anaphoricity classifier, which decides if
a mention should have an antecedent and a pair-
wise classifier similar those just discussed, which
are combined in a cascaded manner. More recently,
Denis and Baldridge (2007) utilized an integer lin-
ear programming (ILP) solver to better combine the
decisions made by these two complementary clas-
sifiers, by finding the globally optimal solution ac-
cording to both classifiers. However, when encoding
constraints into their ILP solver, they did not enforce
transitivity.
The goal of the present work is simply to show
that transitivity constraints are a useful source of
information, which can and should be incorporated
into an ILP-based coreference system. For this goal,
we put aside the anaphoricity classifier and focus
on the pairwise classifier and transitivity constraints.
We build a pairwise logistic classifier, trained on all
pairs of mentions, and then at test time we use an
ILP solver equipped with transitivity constraints to
find the most likely legal assignment to the variables
which represent the pairwise decisions.1 Our re-
sults show a significant improvement compared to
the na??ve use of the pairwise classifier.
Other work on global models of coreference (as
1A legal assignment is one which respects transitive closure.
45
opposed to pairwise models) has included: Luo et al
(2004) who used a Bell tree whose leaves represent
possible partitionings of the mentions into entities
and then trained a model for searching the tree; Mc-
Callum and Wellner (2004) who defined several con-
ditional random field-based models; Ng (2005) who
took a reranking approach; and Culotta et al (2006)
who use a probabilistic first-order logic model.
2 Coreference Resolution
For this task we are given a document which is an-
notated with a set of mentions, and the goal is to
cluster the mentions which refer to the same entity.
When describing our model, we build upon the no-
tation used by Denis and Baldridge (2007).
2.1 Pairwise Classification
Our baseline systems are based on a logistic classi-
fier over pairs of mentions. The probability of a pair
of mentions takes the standard logistic form:
P (x?i,j?|mi,mj ; ?) =
(
1 + e?f(mi,mj)??
)?1 (1)
where mi and mj correspond to mentions i and j
respectively; f(mi,mj) is a feature function over a
pair of mentions; ? are the feature weights we wish
to learn; and x?i,j? is a boolean variable which takes
value 1 if mi and mj are coreferent, and 0 if they are
not. The log likelihood of a document is the sum of
the log likelihoods of all pairs of mentions:
L(x|m; ?) =
?
mi,mj?m2
log P (x?i,j?|mi,mj; ?)
(2)
where m is the set of mentions in the document, and
x is the set of variables representing each pairwise
coreference decision x?i,j?. Note that this model is
degenerate, because it assigns probability mass to
nonsensical clusterings. Specifically, it will allow
x?i,j? = x?j,k? = 1 while x?i,k? = 0.
Prior work (Soon et al, 2001; Denis and
Baldridge, 2007) has generated training data for
pairwise classifiers in the following manner. For
each mention, work backwards through the preced-
ing mentions in the document until you come to a
true coreferent mention. Create negative examples
for all intermediate mentions, and a positive exam-
ple for the mention and its correct antecedent. This
approach made sense for Soon et al (2001) because
testing proceeded in a similar manner: for each men-
tion, work backwards until you find a previous men-
tion which the classifier thinks is coreferent, add
a link, and terminate the search. The COREF-ILP
model of Denis and Baldridge (2007) took a dif-
ferent approach at test time: for each mention they
would work backwards and add a link for all pre-
vious mentions which the classifier deemed coref-
erent. This is equivalent to finding the most likely
assignment to each x?i,j? in Equation 2. As noted,
these assignments may not be a legal clustering be-
cause there is no guarantee of transitivity. The tran-
sitive closure happens in an ad-hoc manner after
this assignment is found: any two mentions linked
through other mentions are determined to be coref-
erent. Our SOON-STYLE baseline used the same
training and testing regimen as Soon et al (2001).
Our D&B-STYLE baseline used the same test time
method as Denis and Baldridge (2007), however at
training time we created data for all mention pairs.
2.2 Integer Linear Programming to Enforce
Transitivity
Because of the ad-hoc manner in which transitiv-
ity is enforced in our baseline systems, we do not
necessarily find the most probable legal clustering.
This is exactly the kind of task at which integer
linear programming excels. We need to first for-
mulate the objective function which we wish the
ILP solver to maximize at test time.2 Let p?i,j? =
log P (x?i,j?|mi,mj ; ?), which is the log probabil-
ity that mi and mj are coreferent according to the
pairwise logistic classifier discussed in the previous
section, and let p??i,j? = log(1 ? p?i,j?), be the log
probability that they are not coreferent. Our objec-
tive function is then the log probability of a particu-
lar (possibly illegal) variable assignment:
max
?
mi,mj?m2
p?i,j? ?x?i,j?? p??i,j? ? (1?x?i,j?) (3)
We add binary constraints on each of the variables:
x?i,j? ? {0, 1}. We also add constraints, over each
triple of mentions, to enforce transitivity:
(1 ? x?i,j?) + (1 ? x?j,k?) ? (1 ? x?i,k?) (4)
2Note that there are no changes from the D&B-STYLE base-
line system at training time.
46
This constraint ensures that whenever x?i,j? =
x?j,k? = 1 it must also be the case that x?i,k? = 1.
3 Experiments
We used lp solve3 to solve our ILP optimization
problems. We ran experiments on two datasets. We
used the MUC-6 formal training and test data, as
well as the NWIRE and BNEWS portions of the ACE
(Phase 2) corpus. This corpus had a third portion,
NPAPER, but we found that several documents where
too long for lp solve to find a solution.4
We added named entity (NE) tags to the data us-
ing the tagger of Finkel et al (2005). The ACE data
is already annotated with NE tags, so when they con-
flicted they overrode the tags output by the tagger.
We also added part of speech (POS) tags to the data
using the tagger of Toutanova et al (2003), and used
the tags to decide if mentions were plural or sin-
gular. The ACE data is labeled with mention type
(pronominal, nominal, and name), but the MUC-
6 data is not, so the POS and NE tags were used
to infer this information. Our feature set was sim-
ple, and included many features from (Soon et al,
2001), including the pronoun, string match, definite
and demonstrative NP, number and gender agree-
ment, proper name and appositive features. We had
additional features for NE tags, head matching and
head substring matching.
3.1 Evaluation Metrics
The MUC scorer (Vilain et al, 1995) is a popular
coreference evaluation metric, but we found it to be
fatally flawed. As observed by Luo et al (2004),
if all mentions in each document are placed into a
single entity, the results on the MUC-6 formal test
set are 100% recall, 78.9% precision, and 88.2%
F1 score ? significantly higher than any published
system. The b3 scorer (Amit and Baldwin, 1998)
was proposed to overcome several shortcomings of
the MUC scorer. However, coreference resolution
is a clustering task, and many cluster scorers al-
ready exist. In addition to the MUC and b3 scorers,
we also evaluate using cluster f-measure (Ghosh,
2003), which is the standard f-measure computed
over true/false coreference decisions for pairs of
3From http://lpsolve.sourceforge.net/
4Integer linear programming is, after all, NP-hard.
mentions; the Rand index (Rand, 1971), which is
pairwise accuracy of the clustering; and variation
of information (Meila, 2003), which utilizes the en-
tropy of the clusterings and their mutual information
(and for which lower values are better).
3.2 Results
Our results are summarized in Table 1. We show
performance for both baseline classifiers, as well as
our ILP-based classifier, which finds the most prob-
able legal assignment to the variables representing
coreference decisions over pairs of mentions. For
comparison, we also give the results of the COREF-
ILP system of Denis and Baldridge (2007), which
was also based on a na??ve pairwise classifier. They
used an ILP solver to find an assignment for the vari-
ables, but as they note at the end of Section 5.1, it is
equivalent to taking all links for which the classifier
returns a probability ? 0.5, and so the ILP solver is
not really necessary. We also include their JOINT-
ILP numbers, however that system makes use of an
additional anaphoricity classifier.
For all three corpora, the ILP model beat both
baselines for the cluster f-score, Rand index, and
variation of information metrics. Using the b3 met-
ric, the ILP system and the D&B-STYLE baseline
performed about the same on the MUC-6 corpus,
though for both ACE corpora, the ILP system was
the clear winner. When using the MUC scorer, the
ILP system always did worse than the D&B-STYLE
baseline. However, this is precisely because the
transitivity constraints tend to yield smaller clusters
(which increase precision while decreasing recall).
Remember that going in the opposite direction and
simply putting all mentions in one cluster produces
a MUC score which is higher than any in the table,
even though this clustering is clearly not useful in
applications. Hence, we are skeptical of this mea-
sure?s utility and provide it primarily for compari-
son with previous work. The improvements from
the ILP system are most clearly shown on the ACE
NWIRE corpus, where the b3 f-score improved 3.6%,
and the cluster f-score improved 16.5%.
4 Conclusion
We showed how to use integer linear program-
ming to encode transitivity constraints in a corefer-
47
MUC SCORER b3 SCORER CLUSTER
MODEL P R F1 P R F1 P R F1 RAND VOI
MUC-6
D&B-STYLE BASELINE 84.8 59.4 69.9 79.7 54.4 64.6 43.8 44.4 44.1 89.9 1.78
SOON-STYLE BASELINE 91.5 51.5 65.9 94.4 46.7 62.5 88.2 31.9 46.9 93.5 1.65
ILP 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 93.2 1.65
ACE ? NWIRE
D&B COREF-ILP 74.8 60.1 66.8 ? ? ? ?
D&B JOINT-ILP 75.8 60.8 67.5 ? ? ? ?
D&B-STYLE BASELINE 73.3 67.6 70.4 70.1 71.4 70.8 31.1 54.0 39.4 91.7 1.42
SOON-STYLE BASELINE 85.3 37.8 52.4 94.1 56.9 70.9 67.7 19.8 30.6 95.5 1.38
ILP 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 96.5 1.09
ACE ? BNEWS
D&B COREF-ILP 75.5 62.2 68.2 ? ? ? ?
D&B JOINT-ILP 78.0 62.1 69.2 ? ? ? ?
D&B-STYLE BASELINE 77.9 51.1 61.7 80.3 64.2 71.4 35.5 33.8 34.6 0.89 1.32
SOON-STYLE BASELINE 90.0 43.2 58.3 95.6 58.4 72.5 83.3 21.5 34.1 0.93 1.09
ILP 87.8 46.8 61.1 93.5 59.9 73.1 77.5 26.1 39.1 0.93 1.06
Table 1: Results on all three datasets with all five scoring metrics. For VOI a lower number is better.
ence classifier which models pairwise decisions over
mentions. We also demonstrated that enforcing such
constraints at test time can significantly improve per-
formance, using a variety of evaluation metrics.
Acknowledgments
Thanks to the following members of the Stanford
NLP reading group for helpful discussion: Sharon
Goldwater, Michel Galley, Anna Rafferty.
This paper is based on work funded by the Dis-
ruptive Technology Office (DTO) Phase III Program
for Advanced Question Answering for Intelligence
(AQUAINT).
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
A. Culotta, M. Wick, and A. McCallum. 2006. First-
order probabilistic models for coreference resolution.
In NAACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In HLT-NAACL, Rochester, New York.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo-
rating non-local information into information extrac-
tion systems by Gibbs sampling. In ACL.
J. Ghosh. 2003. Scalable clustering methods for data
mining. In N. Ye, editor, Handbook of Data Mining,
chapter 10, pages 247?277. Lawrence Erlbaum Assoc.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the Bell tree. In
ACL.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In NIPS.
M. Meila. 2003. Comparing clusterings by the variation
of information. In COLT.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In COLING.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In ACL.
V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove coreference resolution: issues in representation
and optimization. In ACL.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
ACL.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. In Journal of the American Sta-
tistical Association, 66, pages 846?850.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. In
Computational Linguistics, 27(4).
K. Toutanova, D. Klein, and C. Manning. 2003. Feature-
rich part-of-speech tagging with a cyclic dependency
network. In HLT-NAACL 2003.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6.
48
Exploiting Context for Biomedical Entity Recognition:
From Syntax to the Web
Jenny Finkel,* Shipra Dingare,? Huy Nguyen,*
Malvina Nissim,? Christopher Manning,* and Gail Sinclair?
*Department of Computer Science
Stanford University
Stanford, CA 93405-9040
United States
{jrfinkel|htnguyen|manning}
@cs.stanford.edu
?Institute for Communicating and
Collaborative Systems
University of Edinburgh
Edinburgh EH8 9LW
United Kingdom
{sdingar1|mnissim|csincla1}
@inf.ed.ac.uk
Abstract
We describe a machine learning system for the
recognition of names in biomedical texts. The sys-
tem makes extensive use of local and syntactic fea-
tures within the text, as well as external resources
including the web and gazetteers. It achieves an F-
score of 70% on the Coling 2004 NLPBA/BioNLP
shared task of identifying five biomedical named en-
tities in the GENIA corpus.
1 Introduction
The explosion of information in the fields of molec-
ular biology and genetics has provided a unique
opportunity for natural language processing tech-
niques to aid researchers and curators of databases
in the biomedical field by providing text mining
services. Yet typical natural language processing
tasks such as named entity recognition, informa-
tion extraction, and word sense disambiguation are
particularly challenging in the biomedical domain
with its highly complex and idiosyncratic language.
With the increasing use of shared tasks and shared
evaluation procedures (e.g., the recent BioCreative,
TREC, and KDD Cup), it is rapidly becoming clear
that performance in this domain is markedly lower
than the field has come to expect from the standard
domain of newswire. The Coling 2004 shared task
focuses on the problem of Named Entity Recogni-
tion, requiring participating systems to identify the
five named entities of protein, RNA, DNA, cell line,
and cell type in the GENIA corpus of MEDLINE
abstracts (Ohta et al, 2002). In this paper we de-
scribe a machine learning system incorporating a di-
verse set of features and various external resources
to accomplish this task. We describe our system in
detail and also discuss some sources of error.
2 System Description
Our system is a Maximum Entropy Markov Model,
which further develops a system earlier used for the
CoNLL 2003 shared task (Klein et al, 2003) and the
2004 BioCreative critical assessment of information
extraction systems, a task that involved identifying
gene and protein name mentions but not distinguish-
ing between them (Dingare et al, 2004). Unlike
the above two tasks, many of the entities in the cur-
rent task do not have good internal cues for distin-
guishing the class of entity: various systematic pol-
ysemies and the widespread use of acronyms mean
that internal cues are lacking. The challenge was
thus to make better use of contextual features, in-
cluding local and syntactic features, and external re-
sources in order to succeed at this task.
2.1 Local Features
We used a variety of features describing the imme-
diate content and context of each word, including
the word itself, the previous and next words, word
prefixes and suffix of up to a length of 6 characters,
word shapes, and features describing the named en-
tity tags assigned to the previous words. Word
shapes refer to a mapping of each word onto equiva-
lence classes that encodes attributes such as length,
capitalization, numerals, greek letters, and so on.
For instance, ?Varicella-zoster? would become Xx-
xxx, ?mRNA? would become xXXX, and ?CPA1?
would become XXXd. We also incorporated part-of-
speech tagging, using the TnT tagger(Brants, 2000)
retrained on the GENIA corpus gold standard part-
of-speech tagging. We also used various interaction
terms (conjunctions) of these base-level features in
various ways. The full set of local features is out-
lined in Table 1.
2.2 External Resources
We made use of a number of external resources, in-
cluding gazetteers, web-querying, use of the sur-
rounding abstract, and frequency counts from the
British National Corpus.
88
Word Features wi, wi?1, wi+1
Disjunction of 5 prev words
Disjunction of 5 next words
TnT POS POSi, POSi?1, POSi+1
Prefix/suffix Up to a length of 6
Abbreviations abbri
abbri?1 + abbri
abbri + abbri+1
abbri?1 + abbri + abbri+1
Word Shape shapei, shapei?1, shapei+1
shapei?1 + shapei
shapei + shapei+1
shapei?1 + shapei + shapei+1
Prev NE NEi?1, NEi?2 + NEi?1
NEi?3 + NEi?2 + NEi?1
Prev NE + Word NEi?1 + wi
Prev NE + POS NEi?1 + POSi?1 + POSi
NEi?2 + NEi?1 + POSi?2 +
POSi?1 + POSi
Prev NE + Shape NEi?1 + shapei
NEi?1 + shapei+1
NEi?1 + shapei?1 + shapei
NEi?2 + NEi?1 + shapei?2 +
shapei?1 + shapei
Paren-Matching Signals when one parenthesis
in a pair has been assigned a
different tag than the other in a
window of 4 words
Table 1: Local Features (+ indicates conjunction)
2.2.1 Frequency
Many entries in gazetteers are ambiguous words,
occasionally used in the sense that the gazetteer
seeks to represent, but at least as frequently not.
So while the information that a token was seen in
a gazetteer is an unreliable indicator of whether it
is an entity, less frequent words are less likely to be
ambiguous than more frequent ones. Additionally,
more frequent words are likely to have been seen
often in the training data and the system should be
better at classifying them, while less frequent words
are a common source of error and their classifica-
tion is more likely to benefit from the use of external
resources. We assigned each word in the training
and testing data a frequency category correspond-
ing to its frequency in the British National Corpus,
a 100 million word balanced corpus, and used con-
junctions of this category and certain other features.
2.2.2 Gazetteers
Our gazetteer contained only gene names and was
compiled from lists from biomedical websites (such
as LocusLink) as well as from the Gene Ontol-
ogy and the data provided for the BioCreative 2004
tasks. The final gazetteer contained 1,731,496 en-
tries. Because it contained only gene names, and for
the reasons discussed earlier, we suspect that it was
not terribly useful for identifying the presences of
entities, but rather that it mainly helped to establish
the exact beginning and ending point of multi-word
entities recognized mainly through other features.
2.2.3 Web
For each of the named entity classes, we built in-
dicative contexts, such as ?X mRNA? for RNA, or
?X ligation? for protein. For each entity X which
had a frequency lower than 10 in the British Na-
tional Corpus, we submitted instantiations of each
pattern to the web, using the Google API, and ob-
tained the number of hits. The pattern that returned
the highest number of hits determined the feature
value (e.g., ?web-protein?, or ?web-RNA?). If no
hits were returned by any pattern, a value ?O-web?
was assigned. This value was also assigned to all
words whose frequency was higher than 10 (using
yet another value for words with higher frequency
did not improve the tagger?s performance).
2.2.4 Abstracts
A number of NER systems have made effective use
of how the same token was tagged in different parts
of the same document (see (Curran and Clark, 2003)
and (Mikheev et al, 1999)). A token which appears
in an unindicative context in one sentence may ap-
pear in a very obvious context in another sentence
in the same abstract. To leverage this we tagged
each abstract twice, providing for each token a fea-
ture indicating whether it was tagged as an entity
elsewhere in the abstract. This information was
only useful when combined with information on fre-
quency.
2.3 Deeper Syntactic Features
While the local features discussed earlier are all
fairly surface level, our system also makes use of
deeper syntactic features. We fully parsed the train-
ing and testing data using the Stanford Parser of
(Klein and Manning, 2003) operating on the TnT
part-of-speech tagging ? we believe that the un-
lexicalized nature of this parser makes it a partic-
ularly suitable statistical parser to use when there
is a large domain mismatch between the training
material (Wall Street Journal text) and the target
domain, but have not yet carefully evaluated this.
Then, for each word in the sentence which is in-
side a noun phrase, the head and governor of the
noun phrase are extracted. These features are not
very useful when identifying only two classes (such
as GENE and OTHER in the BioCreative task), but
they were quite useful for this task because of the
large number of classes which the system needed to
distinguish between. Because the classifier is now
89
choosing between classes where members can look
very similar, longer distance information can pro-
vide a better representation of the context in which
the word appears. For instance, the word phospho-
rylation occurs in the training corpus 492 times, 482
of which it is was classified as other. However,
it is the governor of 738 words, of which 443 are
protein, 292 are other and only 3 are cell
line.
We also made use of abbreviation matching to
help ensure consistency of labels. Abbreviations
and long forms were extracted from the data using
the method of (Schwartz and Hearst, 2003). This
data was combined with a list of other abbreviations
and long forms extracted from the BioCreative 2004
task. Then all occurrences of either the long or short
forms in the data was labeled. These labels were in-
cluded in the system as features and helped to im-
prove boundary detection.
2.4 Adjacent Entities
When training our classifier, we merged the B- and
I- labels for each class, so it did not learn how to
differentiate between the first word of a class and
internal word. There were several motivations for
doing this. Foremost was memory concerns; our fi-
nal system trained on just the six classes had 1.5
million features ? we just did not have the resources
to train it over more classes without giving up many
of our features. Our second motivation was that by
merging the beginning and internal labels for a par-
ticular class, the classifier would see more examples
of that class and learn better how to identify it. The
drawback of this move is that when two entities be-
longing to the same class are adjacent, our classifier
will automatically merge them into one entity. We
did attempt to split them back up using NP chunks,
but this severely reduced performance.
3 Results and Discussion
Our results on the evaluation data and a confusion
matrix are shown in Tables 2 and 4. Table 4 sug-
gests areas for further work. Collapsing the B- and
I- tags does cost us quite a bit. Otherwise confusions
between some named entity and being nothing are
most of the errors, although protein/DNA and cell-
line/cell-type confusions are also noticeable.
Analysis of performance in biomedical Named
Entity Recognition tends to be dominated by the
perceived poorness of the results, stemming from
the twin beliefs that performance of roughly ninety
percent is the state-of-the-art and that performance
of 100% (or close to that) is possible and the goal
to be aimed for. Both of these beliefs are ques-
tionable, as the top MUC 7 performance of 93.39%
Entity Precision Recall F-Score
Fully Correct
protein 77.40% 68.48% 72.67%
DNA 66.19% 69.62% 67.86%
RNA 72.03% 65.89% 68.83%
cell line 59.00% 47.12% 52.40%
cell type 62.62% 76.97% 69.06%
Overall 71.62% 68.56% 70.06%
Left Boundary Correct
protein 82.89% 73.34% 77.82%
DNA 68.47% 72.01% 70.19%
RNA 75.42% 68.99% 72.06%
cell line 63.80% 50.96% 56.66%
cell type 63.93% 78.57% 70.49%
Overall 75.72% 72.48% 74.07%
Right Boundary Correct
protein 84.70% 74.96% 79.53%
DNA 74.43% 78.29% 76.31%
RNA 78.81% 72.09% 75.30%
cell line 70.2% 56.07% 62.34%
cell type 71.68% 88.10% 79.05%
Overall 79.65% 76.24% 77.91%
Table 2: Results on the evaluation data
(Mikheev et al, 1998) in the domain of newswire
text used an easier performance metric where incor-
rect boundaries were given partial credit, while both
the biomedical NER shared tasks to date have used
an exact match criterion where one is doubly penal-
ized (both as a FP and as a FN) for incorrect bound-
aries. However, the difference in metric clearly can-
not account entirely for the performance discrep-
ancy between newswire NER and biomedical NER.
Biomedical NER appears to be a harder task due
to the widespread ambiguity of terms out of con-
text, the complexity of medical language, and the
apparent need for expert domain knowledge. These
are problems that more sophisticated machine learn-
ing systems using resources such as ontologies and
deep processing might be able to overcome. How-
ever, one should also consider the inherent ?fuzzi-
ness? of the classification task. The few existing
studies of inter-annotator agreement for biomedi-
cal named entities have measured agreement be-
tween 87%(Hirschman, 2003) and 89%(Demetrious
and Gaizauskas, 2003). As far as we know there
are no inter-annotator agreement results for the GE-
NIA corpus, and it is necessary to have such results
before properly evaluating the performance of sys-
tems. In particular, the fact that BioNLP sought to
distinguish between gene and protein names, when
these are known to be systematically ambiguous,
and when in fact in the GENIA corpus many enti-
ties were doubly classified as ?protein molecule or
90
DNA RNA cell line cell type protein
gold\ans B- I- B- I- B- I- B- I- B- I- O
B-DNA 723 39 0 0 1 0 0 0 154 1 138
I-DNA 52 1390 0 0 0 0 0 0 19 71 257
B-RNA 1 0 89 3 0 0 0 0 14 0 11
I-RNA 0 1 5 164 0 0 0 0 2 0 15
B-cell line 3 0 0 0 319 41 37 5 12 1 82
I-cell line 0 6 0 0 24 713 5 104 0 14 123
B-cell type 1 0 0 0 164 22 1228 90 31 5 380
I-cell type 0 0 0 0 13 383 88 2101 8 27 371
B-protein 48 5 10 3 20 1 19 3 4200 192 566
I-protein 6 66 0 11 0 10 2 25 245 3630 779
O 170 240 25 26 85 142 184 132 1042 656 78945
Table 3: Our confusion matrix over the evaluation data
human B-cell type
monocytes I-cell type
human O
monocytes B-cell type
macrophages B-cell type
primary B-cell type JJ
T I-cell type NN
lymphocytes I-cell type NNS
primary O JJ
peripheral B-cell type JJ
blood I-cell type NN
lymphocytes I-cell type NNS
Table 4: Examples of annotation inconsistencies
region? and ?DNA molecule or region?, suggests
that inter-annotator agreement could be low, and
that many entities in fact have more than one classi-
fication.
One area where GENIA appears inconsistent is
in the labeling of preceding adjectives. The data
was selected by querying for the term human, yet
the term is labeled inconsistently, as is shown in Ta-
ble 4. Of the 1790 times the term human occurred
before or at the beginning of an entity in the train-
ing data, it was not classified as part of the entity
110 times. In the test data, there is only on instance
(out of 130) where the term is excluded. Adjectives
are excluded approximately 25% of the time in both
the training and evaluation data. There are also in-
consistencies when two entities are separated by the
word and.
4 Acknowledgements
This paper is based on work supported in part by a
Scottish Enterprise Edinburgh-Stanford Link Grant
(R36759), as part of the SEER project, and in part
the National Science Foundation under the Knowl-
edge Discovery and Dissemination program.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In ANLP 6, pages 224?231.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-03), pages 164?167.
George Demetrious and Rob Gaizauskas. 2003. Corpus
resources for development and evaluation of a biolog-
ical text mining system. In Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Jenny Rose Finkel, Christopher Man-
ning, Malvina Nissim, and Beatrice Alex. 2004. Ex-
ploring the boundaries: Gene and protein identifica-
tion in biomedical text. In Proceedings of the BioCre-
ative Workshop.
Lynette Hirschman. 2003. Using biological resources to
bootstrap text mining.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423?430.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In CoNLL 7, pages 180?
183.
Andrei Mikheev, Claire Grover, and Mark Moens. 1998.
Description of the LTG system used for MUC-7. In
Proceedings of MUC-7.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In Pro-
ceedings of the ninth conference on European chap-
ter of the Association for Computational Linguistics,
pages 1?8. Association for Computational Linguis-
tics.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of he Human Language Technology Confer-
ence, pages 73?77.
Ariel Schwartz and Marti Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing, Kauai, Jan.
91
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 618?626,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Solving the Problem of Cascading Errors: Approximate Bayesian
Inference for Linguistic Annotation Pipelines
Jenny Rose Finkel, Christopher D. Manning and Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel, manning, ang}@cs.stanford.edu
Abstract
The end-to-end performance of natural
language processing systems for com-
pound tasks, such as question answering
and textual entailment, is often hampered
by use of a greedy 1-best pipeline archi-
tecture, which causes errors to propagate
and compound at each stage. We present
a novel architecture, which models these
pipelines as Bayesian networks, with each
low level task corresponding to a variable
in the network, and then we perform ap-
proximate inference to find the best la-
beling. Our approach is extremely sim-
ple to apply but gains the benefits of sam-
pling the entire distribution over labels at
each stage in the pipeline. We apply our
method to two tasks ? semantic role la-
beling and recognizing textual entailment
? and achieve useful performance gains
from the superior pipeline architecture.
1 Introduction
Almost any system for natural language under-
standing must recover hidden linguistic structure
at many different levels: parts of speech, syntac-
tic dependencies, named entities, etc. For exam-
ple, modern semantic role labeling (SRL) systems
use the parse of the sentence, and question answer-
ing requires question type classification, parsing,
named entity tagging, semantic role labeling, and
often other tasks, many of which are dependent
on one another and must be pipelined together.
Pipelined systems are ubiquitous in NLP: in ad-
dition to the above examples, commonly parsers
and named entity recognizers use part of speech
tags and chunking information, and also word seg-
mentation for languages such as Chinese. Almost
no NLP task is truly standalone.
Most current systems for higher-level, aggre-
gate NLP tasks employ a simple 1-best feed for-
ward architecture: they greedily take the best out-
put at each stage in the pipeline and pass it on to
the next stage. This is the simplest architecture to
build (particularly if reusing existing component
systems), but errors are frequently made during
this pipeline of annotations, and when a system
is given incorrectly labeled input it is much harder
for that system to do its task correctly. For ex-
ample, when doing semantic role labeling, if no
syntactic constituent of the parse actually corre-
sponds to a given semantic role, then that seman-
tic role will almost certainly be misidentified. It
is therefore disappointing, but not surprising, that
F-measures on SRL drop more than 10% when
switching from gold parses to automatic parses
(for instance, from 91.2 to 80.0 for the joint model
of Toutanova (2005)).
A common improvement on this architecture is
to pass k-best lists between processing stages, for
example (Sutton and McCallum, 2005; Wellner et
al., 2004). Passing on a k-best list gives useful
improvements (e.g., in Koomen et al (2005)), but
efficiently enumerating k-best lists often requires
very substantial cognitive and engineering effort,
e.g., in (Huang and Chiang, 2005; Toutanova et
al., 2005).
At the other extreme, one can maintain the
entire space of representations (and their proba-
bilities) at each level, and use this full distribu-
tion to calculate the full distribution at the next
level. If restricting oneself to weighted finite state
transducers (WFSTs), a framework applicable to a
number of NLP applications (as outlined in Kart-
tunen (2000)), a pipeline can be compressed down
618
into a single WFST, giving outputs equivalent
to propagating the entire distribution through the
pipeline. In the worst case there is an exponential
space cost, but in many relevant cases composition
is in practice quite practical. Outside of WFSTs,
maintaining entire probability distributions is usu-
ally infeasible in NLP, because for most intermedi-
ate tasks, such as parsing and named entity recog-
nition, there is an exponential number of possible
labelings. Nevertheless, for some models, such as
most parsing models, these exponential labelings
can be compactly represented in a packed form,
e.g., (Maxwell and Kaplan, 1995; Crouch, 2005),
and subsequent stages can be reengineered to work
over these packed representations, e.g., (Geman
and Johnson, 2002). However, doing this normally
also involves a very high cognitive and engineer-
ing effort, and in practice this solution is infre-
quently adopted. Moreover, in some cases, a sub-
sequent module is incompatible with the packed
representation of a previous module and an ex-
ponential amount of work is nevertheless required
within this architecture.
Here we present an attractive middle ground
in dealing with linguistic pipelines. Rather than
only using the 1 or k most likely labelings at each
stage, we would indeed like to take into account
all possible labelings and their probabilities, but
we would like to be able to do so without a lot of
thinking or engineering. We propose that this can
be achieved by use of approximate inference. The
form of approximate inference we use is very sim-
ple: at each stage in the pipeline, we draw a sam-
ple from the distribution of labels, conditioned on
the samples drawn at previous stages. We repeat
this many times, and then use the samples from
the last stage, which corresponds to the ultimate,
higher-level task, to form a majority vote classifier.
As the number of samples increases, this method
will approximate the complete distribution. Use of
the method is normally a simple modification to an
existing piece of code, and the method is general.
It can be applied not only to all pipelines, but to
multi-stage algorithms which are not pipelines as
well.
We apply our method to two problems: seman-
tic role labeling and recognizing textual entail-
ment. For semantic role labeling we use a two
stage pipeline which parses the input sentence, and
for recognizing textual entailment we use a three
stage pipeline which tags the sentence with named
entities and then parses it before passing it to the
entailment decider.
2 Approach
2.1 Overview
In order to do approximate inference, we model
the entire pipeline as a Bayesian network. Each
stage in the pipeline corresponds to a variable in
the network. For example, the parser stage cor-
responds to a variable whose possible values are
all possible parses of the sentence. The probabil-
ities of the parses are conditioned on the parent
variables, which may just be the words of the sen-
tence, or may be the part of speech tags output by
a part of speech tagger.
The simple linear structure of a typical linguis-
tic annotation network permits exact inference that
is quadratic in the number of possible labels at
each stage, but unfortunately our annotation vari-
ables have a very large domain. Additionally,
some networks may not even be linear; frequently
one stage may require the output from multiple
previous stages, or multiple earlier stages may be
completely independent of one another. For ex-
ample, a typical QA system will do question type
classification on the question, and from that ex-
tract keywords which are passed to the informa-
tion retreival part of the system. Meanwhile, the
retreived documents are parsed and tagged with
named entities; the network rejoins those outputs
with the question type classification to decide on
the correct answer. We address these issues by
using approximate inference instead of exact in-
ference. The structure of the nodes in the network
permits direct sampling based on a topological sort
of the nodes. Samples are drawn from the condi-
tional distributions of each node, conditioned on
the samples drawn at earlier nodes in the topolog-
ical sort.
2.2 Probability of a Complete Labeling
Before we can discuss how to sample from these
Bayes nets, we will formalize how to move from
an annotation pipeline to a Bayes net. Let A be
the set of n annotators A1, A2, ..., An (e.g., part
of speech tagger, named entity recognizer, parser).
These are the variables in the network. For annota-
tor ai, we denote the set of other annotators whose
input is directly needed as Parents(Ai) ? A
and a particular assignment to those variables is
parents(Ai). The possible values for a particu-
619
lar annotator Ai are ai (e.g., a particular parse tree
or named entity tagging). We can now formulate
the probability of a complete annotation (over all
annotators) in the standard way for Bayes nets:
PBN(a1, a2, ..., an) =
N
?
i=1
P (ai|parents(Ai))
(1)
2.3 Approximate Inference in Bayesian
Networks
This factorization of the joint probability distri-
bution facilitates inference. However, exact in-
ference is intractable because of the number of
possible values for our variables. Parsing, part of
speech tagging, and named entity tagging (to name
a few) all have a number of possible labels that is
exponential in the length of the sentence, so we
use approximate inference. We chose Monte Carlo
inference, in which samples drawn from the joint
distribution are used to approximate a marginal
distribution for a subset of variables in the dis-
tribution. First, the nodes are sorted in topologi-
cal order. Then, samples are drawn for each vari-
able, conditioned on the samples which have al-
ready been drawn. Many samples are drawn, and
are used to estimate the joint distribution.
Importantly, for many language processing
tasks our application only needs to provide the
most likely value for a high-level linguistic an-
notation (e.g., the guessed semantic roles, or an-
swer to a question), and other annotations such as
parse trees are only present to assist in performing
that task. The probability of the final annotation is
given by:
PBN(an) =
?
a1,a2,...,an?1
PBN(a1, a2, ..., an) (2)
Because we are summing out all variables other
than the final one, we effectively use only the sam-
ples drawn from the final stage, ignoring the labels
of the variables, to estimate the marginal distribu-
tion over that variable. We then return the label
which had the highest number of samples. For
example, when trying to recognize textual entail-
ment, we count how many times we sampled ?yes,
it is entailed? and how many times we sampled
?no, it is not entailed? and return the answer with
more samples.
When the outcome you are trying to predict is
binary (as is the case with RTE) or n-ary for small
n, the number of samples needed to obtain a good
estimate of the posterior probability is very small.
This is true even if the spaces being sampled from
during intermediate stages are exponentially large
(such as the space of all parse trees). Ng and
Jordan (2001) show that under mild assumptions,
with only N samples the relative classification er-
ror will be at most O( 1N ) higher than the error of
the Bayes optimal classifier (in our case, the clas-
sifier which does exact inference). Even if the out-
come space is not small, the sampling technique
we present can still be very useful, as we will see
later for the case of SRL.
3 Generating Samples
The method we have outlined requires the ability
to sample from the conditional distributions in the
factored distribution of (1): in our case, the prob-
ability of a particular linguistic annotation, condi-
tioned on other linguistic annotations. Note that
this differs from the usual annotation task: taking
the argmax. But for most algorithms the change is
a small and easy change. We discuss how to ob-
tain samples efficiently from a few different anno-
tation models: probabilistic context free grammars
(PCFGs), and conditional random fields (CRFs).
3.1 Sampling Parses
Bod (1995) discusses parsing with probabilistic
tree substitution grammars, which, unlike simple
PCFGs, do not have a one-to-one mapping be-
tween output parse trees and a derivation (a bag of
rules) that produced it, and hence the most-likely
derivation may not correspond to the most likely
parse tree. He therefore presents a bottom-up ap-
proach to sampling derivations from a derivation
forest, which does correspond to a sample from the
space of parse trees. Goodman (1998) presents a
top-down version of this algorithm. Although we
use a PCFG for parsing, it is the grammar of (Klein
and Manning, 2003), which uses extensive state-
splitting, and so there is again a many-to-one cor-
respondence between derivations and parses, and
we use an algorithm similar to Goodman?s in our
work.
PCFGs put probabilities on each rule, such as
S ? NP VP and NN ? ?dog?. The probability of
a parse is the product of the probabilities of the
rules used to construct the parse tree. A dynamic
programing algorithm, the inside algorithm, can
be used to find the probability of a sentence. The
620
inside probability ?k(p, q) is the probability that
words p through q, inclusive, were produced by
the non-terminal k. So the probability of the sen-
tence The boy pet the dog. is equal to the inside
probability ?S(1, 6), where the first word, w1 is
The and the sixth word, w6, is [period]. It is also
useful for our purposes to view this quantity as the
sum of the probabilities of all parses of the sen-
tence which have S as the start symbol. The prob-
ability can be defined recursively (Manning and
Schu?tze, 1999) as follows:
?k(p, q) =
?
?
?
?
?
?
?
?
?
P (Nk ? wp) if p = q
?
r,s
q?1
?
d=p
P (Nk ? N rN s)?r(p, d)?s(d + 1, q)
otherwise
(3)
where Nk, N r and N s are non-terminal symbols
and wp is the word at position p. We have omit-
ted the case of unary rules for simplicity since it
requires a closure operation.
These probabilities can be efficiently computed
using a dynamic program. or memoization of each
value as it is calculated. Once we have computed
all of the inside probabilities, they can be used to
generate parses from the distribution of all parses
of the sentence, using the algorithm in Figure 1.
This algorithm is called after all of the inside
probabilities have been calculated and stored, and
take as parameters S, 1, and length(sentence). It
works by building the tree, starting from the root,
and recursively generating children based on the
posterior probabilities of applying each rule and
each possible position on which to split the sen-
tences. Intuitively, the algorithm is given a non-
terminal symbol, such as S or NP, and a span of
words, and has to decide (a) what rule to apply to
expand the non-terminal, and (b) where to split the
span of words, so that each non-terminal result-
ing from applying the rule has an associated word
span, and the process can repeat. The inside prob-
abilities are calculated just once, and we can then
generate many samples very quickly; DrawSam-
ples is linear in the number of words, and rules.
3.2 Sampling Named Entity Taggings
To do named entity recognition, we chose to use
a conditional random field (CRF) model, based on
Lafferty et al (2001). CRFs represent the state of
function DRAWSAMPLE(Nk, r, s)
if r = s
tree.label = Nk
tree.child = word(r)
return (tree)
for each rule m ? {m? : head(m?) = Nk}
N i ? lChild(m)
Nj ? rChild(m)
for q ? r to s? 1
scores(m,q)? P (m)?i(r, q)?j(q + 1, s)
(m, q)? SAMPLEFROM(scores)
tree.label = head(m)
tree.lChild = DRAWSAMPLE(lChild(m), r, q)
tree.rChild = DRAWSAMPLE(rChild(m), q + 1, s)
return (tree)
Figure 1: Pseudo-code for sampling parse trees from a PCFG.
This is a recursive algorithm which starts at the root of the
tree and expands each node by sampling from the distribu-
tion of possible rules and ways to split the span of words. Its
arguments are a non-terminal and two integers corresponding
to word indices, and it is initially called with arguments S, 1,
and the length of the sentence. There is a call to sampleFrom,
which takes an (unnormalized) probability distribution, nor-
malizes it, draws a sample and then returns the sample.
the art in sequence modeling ? they are discrimi-
natively trained, and maximize the joint likelihood
of the entire label sequence in a manner which
allows for bi-directional flow of information. In
order to describe how samples are generated, we
generalize CRFs in a way that is consistent with
the Markov random field literature. We create a
linear chain of cliques, each of which represents
the probabilistic relationship between an adjacent
set of n states using a factor table containing |S|n
values. These factor tables on their own should
not be viewed as probabilities, unnormalized or
otherwise. They are, however, defined in terms of
exponential models conditioned on features of the
observation sequence, and must be instantiated for
each new observation sequence. The probability
of a state sequence is then defined by the sequence
of factor tables in the clique chain, given the ob-
servation sequence:
PCRF(s|o) =
1
Z(o)
N
?
i=1
Fi(si?n . . . si) (4)
where Fi(si?n . . . si) is the element of the fac-
tor table at position i corresponding to states si?n
through si, and Z(o) is the partition function
which serves to normalize the distribution.1 To in-
1To handle the start condition properly, imagine also that
we define a set of distinguished start states s?(n?1) . . . s0.
621
fer the most likely state sequence in a CRF it is
customary to use the Viterbi algorithm.
We then apply a process called clique tree cal-
ibration, which involves passing messages be-
tween the cliques (see Cowell et al (2003) for
a full treatment of this topic). After this pro-
cess has completed, the factor tables can be
viewed as unnormalized probabilities, which can
be used to compute conditional probabilities,
PCRF(si|si?n . . . si?1, o). Once these probabili-
ties have been calculated, generating samples is
very simple. First, we draw a sample for the label
at the first position,2 and then, for each subsequent
position, we draw a sample from the distribution
for that position, conditioned on the label sampled
at the previous position. This process results in
a sample of a complete labeling of the sequence,
drawn from the posterior distribution of complete
named entity taggings.
Similarly to generating sample parses, the ex-
pensive part is calculating the probabilities; once
we have them we can generate new samples very
quickly.
3.3 k-Best Lists
At first glance, k-best lists may seem like they
should outperform sampling, because in effect
they are the k best samples. However, there are
several important reasons why one might prefer
sampling. One reason is that the k best paths
through a word lattice, or the k best derivations in
parse forest do not necessarily correspond to the
k best sentences or parse trees. In fact, there are
no known sub-exponential algorithms for the best
outputs in these models, when there are multiple
ways to derive the same output.3 This is not just a
theoretical concern ? the Stanford parser uses such
a grammar, and we found that when generating a
50-best derivation list that on average these deriva-
tions corresponded to about half as many unique
parse trees. Our approach circumvents this issue
entirely, because the samples are generated from
the actual output distribution.
Intuition also suggests that sampling should
give more diversity at each stage, reducing the
likelihood of not even considering the correct out-
put. Using the Brown portion of the SRL test
set (discussed in sections 4 and 6.1), and 50-
samples/50-best, we found that on average the 50-
2Conditioned on the distinguished start states.
3Many thanks to an anonymous reviewer for pointing out
this argument.
samples system considered approximately 25%
more potential SRL labelings than the 50-best sys-
tem.
When pipelines have more than two stages, it
is customary to do a beam search, with a beam
size of k. This means that at each stage in the
pipeline, more and more of the probability mass
gets ?thrown away.? Practically, this means that
as pipeline length increases, there will be in-
creasingly less diversity of labels from the earlier
stages. In a degenerate 10-stage, k-best pipeline,
where the last stage depends mainly on the first
stage, it is probable that all but a few labelings
from the first stage will have been pruned away,
leaving something much smaller than a k-best
sample, possibly even a 1-best sample, as input to
the final stage. Using approximate inference to es-
timate the marginal distribution over the last stage
in the pipeline, such as our sampling approach, the
pipeline length does not have this negative impact
or affect the number of samples needed. And un-
like k-best beam searches, there is an entire re-
search community, along with a large body of lit-
erature, which studies how to do approximate in-
ference in Bayesian networks and can provide per-
formance bounds based on the method and the
number of samples generated.
One final issue with the k-best method arises
when instead of a linear chain pipeline, one is us-
ing a general directed acyclic graph where a node
can have multiple parents. In this situation, doing
the k-best calculation actually becomes exponen-
tial in the size of the largest in-degree of a node ?
for a node with n parents, you must try all kn com-
binations of the values for the parent nodes. With
sampling this is not an issue; each sample can be
generated based on a topological sort of the graph.
4 Semantic Role Labeling
4.1 Task Description
Given a sentence and a target verb (the predicate)
the goal of semantic role labeling is to identify and
label syntactic constituents of the parse tree with
semantic roles of the predicate. Common roles
are agent, which is the thing performing the ac-
tion, patient, which is the thing on which the ac-
tion is being performed, and instrument, which is
the thing with which the action is being done. Ad-
ditionally, there are modifier arguments which can
specify the location, time, manner, etc. The fol-
lowing sentence provides an example of a predi-
622
cate and its arguments:
[The luxury auto maker]agent [last
year]temp [sold]pred [1,214 cars]patient
in [the U.S]location.
Semantic role labeling is a key component for
systems that do question answering, summariza-
tion, and any other task which directly uses a se-
mantic interpretation.
4.2 System Description
We modified the system described in Haghighi
et al (2005) and Toutanova et al (2005) to test
our method. The system uses both local models,
which score subtrees of the entire parse tree inde-
pendently of the labels of other nodes not in that
subtree, and joint models, which score the entire
labeling of a tree with semantic roles (for a partic-
ular predicate).
First, the task is separated into two stages, and
local models are learned for each. At the first
stage, the identification stage, a classifier labels
each node in the tree as either ARG, meaning that
it is an argument (either core or modifier) to the
predicate, or NONE, meaning that it is not an argu-
ment. At the second stage, the classification stage,
the classifier is given a set of arguments for a pred-
icate and must label each with its semantic role.
Next, a Viterbi-like dynamic algorithm is used
to generate a list of the k-best joint (identification
and classification) labelings according to the lo-
cal models. The algorithm enforces the constraint
that the roles should be non-overlapping. Finally,
a joint model is constructed which scores a com-
pletely labeled tree, and it is used to re-rank the k-
best list. The separation into local and joint mod-
els is necessary because there are an exponential
number of ways to label the entire tree, so using
the joint model alone would be intractable. Ide-
ally, we would want to use approximate inference
instead of a k-best list here as well. Particle fil-
tering would be particularly well suited - particles
could be sampled from the local model and then
reweighted using the joint model. Unfortunately,
we did not have enough time modify the code of
(Haghighi et al, 2005) accordingly, so the k-best
structure remained.
To generate samples from the SRL system, we
take the scores given to the k-best list, normalize
them to sum to 1, and sample from them. One
consequence of this, is that any labeling not on the
k-best list has a probability of 0.
5 Recognizing Textual Entailment
5.1 Task Description
In the task of recognizing textual entailment
(RTE), also commonly referred to as robust textual
inference, you are provided with two passages, a
text and a hypothesis, and must decide whether the
hypothesis can be inferred from the text. The term
robust is used because the task is not meant to be
domain specific. The term inference is used be-
cause this is not meant to be logical entailment, but
rather what an intelligent, informed human would
infer. Many NLP applications would benefit from
the ability to do robust textual entailment, includ-
ing question answering, information retrieval and
multi-document summarization. There have been
two PASCAL workshops (Dagan et al, 2005) with
shared tasks in the past two years devoted to RTE.
We used the data from the 2006 workshop, which
contains 800 text-hypothesis pairs in each of the
test and development sets4 (there is no training
set). Here is an example from the development
set from the first RTE challenge:
Text: Researchers at the Harvard School of Pub-
lic Health say that people who drink coffee
may be doing a lot more than keeping them-
selves awake ? this kind of consumption ap-
parently also can help reduce the risk of dis-
eases.
Hypothesis: Coffee drinking has health benefits.
The positive and negative examples are bal-
anced, so the baseline of guessing either all yes
or all no would score 50%. This is a hard task ? at
the first challenge no system scored over 60%.
5.2 System Description
MacCartney et al (2006) describe a system for do-
ing robust textual inference. They divide the task
into three stages ? linguistic analysis, graph align-
ment, and entailment determination. The first of
these stages, linguistic analysis is itself a pipeline
of parsing and named entity recognition. They use
the syntactic parse to (deterministically) produce
a typed dependency graph for each sentence. This
pipeline is the one we replace. The second stage,
graph alignment consists of trying to find good
alignments between the typed dependency graphs
4The dataset and further information from both
challenges can be downloaded from http://www.pascal-
network.org/Challenges/RTE2/Datasets/
623
NER parser RTE
Figure 2: The pipeline for recognizing textual entailment.
for the text and hypothesis. Each possible align-
ment has a score, and the alignment with the best
score is propagated forward. The final stage, en-
tailment determination, is where the decision is
actually made. Using the score from the align-
ment, as well as other features, a logistic model
is created to predict entailment. The parameters
for this model are learned from development data.5
While it would be preferable to sample possible
alignments, their system for generating alignment
scores is not probabilistic, and it is unclear how
one could convert between alignment scores and
probabilities in a meaningful way.
Our modified linguistic analysis pipeline does
NER tagging and parsing (in their system, the
parse is dependent on the NER tagging because
some types of entities are pre-chunked before
parsing) and treats the remaining two sections of
their pipeline, the alignment and determination
stages, as one final stage. Because the entailment
determination stage is based on a logistic model, a
probability of entailment is given and sampling is
straightforward.
6 Experimental Results
In our experiments we compare the greedy
pipelined approach with our sampling pipeline ap-
proach.
6.1 Semantic Role Labeling
For the past two years CoNLL has had shared
tasks on SRL (Carreras and Ma`rquez (2004) and
Carreras and Ma`rquez (2005)). We used the
CoNLL 2005 data and evaluation script. When
evaluating semantic role labeling results, it is com-
mon to present numbers on both the core argu-
ments (i.e., excluding the modifying arguments)
and all arguments. We follow this convention and
present both sets of numbers. We give precision,
5They report their results on the first PASCAL dataset,
and use only the development set from the first challenge for
learning weights. When we test on the data from the second
challenge, we use all data from the first challenge and the
development data from the second challenge to learn these
weights.
SRL Results ? Penn Treebank Portion
Core Args Precision Recall F-measure
Greedy 79.31% 77.7% 78.50%
K-Best 80.05% 78.45% 79.24%
Sampling 80.13% 78.25% 79.18%
All Args Precision Recall F-measure
Greedy 78.49% 74.77% 76.58%
K-Best 79.58% 74.90% 77.16%
Sampling 79.81% 74.85% 77.31%
SRL Results ? Brown Portion
Core Args Precision Recall F-measure
Greedy 68.28% 67.72% 68.0%
K-Best 69.25% 69.02% 69.13%
Sampling 69.35% 68.93% 69.16%
All Args Precision Recall F-measure
Greedy 66.6% 60.45% 63.38%
K-Best 68.82% 61.03% 64.69%
Sampling 68.6% 61.11% 64.64%
Table 1: Results for semantic role labeling task. The sampled
numbers are averaged over several runs, as discussed.
recall and F-measure, which are based on the num-
ber of arguments correctly identified. For an argu-
ment to be correct both the span and the classifica-
tion must be correct; there is no partial credit.
To generate sampled parses, we used the Stan-
ford parser (Klein and Manning, 2003). The
CoNLL data comes with parses from Charniak?s
parser (Charniak, 2000), so we had to re-parse
the data and retrain the SRL system on these new
parses, resulting in a lower baseline than previ-
ously presented work. We choose to use Stan-
ford?s parser because of the ease with which we
could modify it to generate samples. Unfortu-
nately, its performance is slightly below that of the
other parsers.
The CoNLL data has two separate test sets; the
first is section 23 of the Penn Treebank (PTB),
and the second is ?fresh sentences? taken from the
Brown corpus. For full results, please see Table 1.
On the Penn Treebank portion we saw an absolute
F-score improvement of 0.7% on both core and all
arguments. On the Brown portion of the test set we
saw an improvement of 1.25% on core and 1.16%
on all arguments. In this context, a gain of over
1% is quite large: for instance, the scores for the
top 4 systems on the Brown data at CoNLL 2005
were within 1% of each other. For both portions,
we generated 50 samples, and did this 4 times, av-
eraging the results. We most likely saw better per-
formance on the Brown portion than the PTB por-
tion because the parser was trained on the Penn
Treebank training data, so the most likely parses
will be of higher quality for the PTB portion of
the test data than for the Brown portion. We also
624
RTE Results
Accuracy Average Precision
Greedy 59.13% 59.91%
Sampling 60.88% 61.99%
Table 2: Results for recognizing textual entailment. The sam-
pled numbers are averaged over several runs, as discussed.
ran the pipeline using a 50-best list, and found the
two results to be comparable.
6.2 Textual Entailment
For the second PASCAL RTE challenge, two dif-
ferent types of performance measures were used
to evaluate labels and confidence of the labels for
the text-hypothesis pairs. The first measure is ac-
curacy ? the percentage of correct judgments. The
second measure is average precision. Responses
are sorted based on entailment confidence and then
average precision is calculated by the following
equation:
1
R
n
?
i=1
E(i)# correct up to pair ii (5)
where n is the size of the test set, R is the number
of positive (entailed) examples, E(i) is an indi-
cator function whose value is 1 if the ith pair is
entailed, and the is are sorted based on the entail-
ment confidence. The intention of this measure is
to evaluate how well calibrated a system is. Sys-
tems which are more confident in their correct an-
swers and less confident in their incorrect answers
will perform better on this measure.
Our results are presented in Table 2. We gen-
erated 25 samples for each run, and repeated the
process 7 times, averaging over runs. Accuracy
was improved by 1.5% and average precision by
2%. It does not come as a surprise that the average
precision improvement was larger than the accu-
racy improvement, because our model explicitly
estimates its own degree of confidence by estimat-
ing the posterior probability of the class label.
7 Conclusions and Future Work
We have presented a method for handling lan-
guage processing pipelines in which later stages
of processing are conditioned on the results of
earlier stages. Currently, common practice is to
take the best labeling at each point in a linguistic
analysis pipeline, but this method ignores informa-
tion about alternate labelings and their likelihoods.
Our approach uses all of the information available,
and has the added advantage of being extremely
simple to implement. By modifying your subtasks
to generate samples instead of the most likely la-
beling, our method can be used with very little ad-
ditional overhead. And, as we have shown, such
modifications are usually simple to make; further,
with only a ?small? (polynomial) number of sam-
ples k, under mild assumptions the classification
error obtained by the sampling approximation ap-
proaches that of exact inference. (Ng and Jordan,
2001) In contrast, an algorithm that keeps track
only of the k-best list enjoys no such theoretical
guarantee, and can require an exponentially large
value for k to approach comparable error. We also
note that in practice, k-best lists are often more
complicated to implement and more computation-
ally expensive (e.g. the complexity of generat-
ing k sample parses or CRF outputs is substan-
tially lower than that of generating the k best parse
derivations or CRF outputs).
The major contribution of this work is not
specific to semantic role labeling or recognizing
textual entailment. We are proposing a general
method to deal with all multi-stage algorithms. It
is common to build systems using many different
software packages, often from other groups, and to
string together the 1-best outputs. If, instead, all
NLP researchers wrote packages which can gen-
erate samples from the posterior, then the entire
NLP community could use this method as easily
as they can use the greedy methods that are com-
mon today, and which do not perform as well.
One possible direction for improvement of this
work would be to move from a Bayesian network
to an undirected Markov network. This is desir-
able because influence should be able to flow in
both directions in this pipeline. For example, the
semantic role labeler should be able to tell the
parser that it did not like a particular parse, and
this should influence the probability assigned to
that parse. The main difficulty here lies in how
to model this reversal of influence. The problem
of using parse trees to help decide good semantic
role labelings is well studied, but the problem of
using semantic role labelings to influence parses is
not. Furthermore, this requires building joint mod-
els over adjacent nodes, which is usually a non-
trivial task. However, we feel that this approach
would improve performance even more on these
pipelined tasks and should be pursued.
625
8 Acknowledgements
We would like to thank our anonymous review-
ers for their comments and suggestions. We
would also like to thank Kristina Toutanova, Aria
Haghighi and the Stanford RTE group for their as-
sistance in understanding and using their code.
This paper is based on work funded in part by a
Stanford School of Engineering fellowship and in
part by the Defense Advanced Research Projects
Agency through IBM. The content does not nec-
essarily reflect the views of the U.S. Government,
and no official endorsement should be inferred.
References
Rens Bod. 1995. The problem of computing the most proba-
ble tree in data-oriented parsing and stochastic tree gram-
mars. In Proceedings of EACL 1995.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL 2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL 2005.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 14th National Conference
on Artificial Intelligence.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 2003. Probabilistic Networks and
Expert Systems. Springer.
Richard Crouch. 2005. Packed rewriting for mapping se-
mantics to KR. In Proceedings of the 6th International
Workshop on Computational Semantics.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL recognizing textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on Rec-
ognizing Textual Entailment.
Stuart Geman and Mark Johnson. 2002. Dynamic program-
ming for parsing and estimation of stochastic unification-
based grammars. In Proceedings of ACL 2002.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, Kristina Toutanova, and Christopher D. Man-
ning. 2005. A joint model for semantic role labeling. In
Proceedings of CoNLL 2005.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of the 9th International Workshop on
Parsing Technologies.
Lauri Karttunen. 2000. Applications of finite-state trans-
ducers in natural-language processing. In Proceesings of
the Fifth International Conference on Implementation and
Application of Automata.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL 2003.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen tau
Yih. 2005. Generalized inference with multiple semantic
role labeling systems. In Proceedings of CoNLL 2005,
pages 181?184.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic models
for segmenting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on Ma-
chine Learning, pages 282?289.
Bill MacCartney, Trond Grenager, Marie de Marneffe, Daniel
Cer, and Christopher D. Manning. 2006. Learning to rec-
ognize features of valid textual entailments. In Proceed-
ings of NAACL-HTL 2006.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts.
John T. Maxwell, III and Ronald M. Kaplan. 1995. A method
for disjunctive constraint satisfaction. In Mary Dalrymple,
Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-
nen, editors, Formal Issues in Lexical-Functional Gram-
mar, number 47 in CSLI Lecture Notes Series, chapter 14,
pages 381?481. CSLI Publications.
Andrew Ng and Michael Jordan. 2001. Convergence rates of
the voting Gibbs classifier, with application to Bayesian
feature selection. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In Proceedings of CoNLL
2005, pages 225?228.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL 2005.
Kristina Toutanova. 2005. Effective statistical models for syn-
tactic and semantic disambiguation. Ph.D. thesis, Stan-
ford University.
Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael
Hay. 2004. An integrated, conditional model of informa-
tion extraction and coreference with application to citation
matching. In Proceedings of the 20th Annual Conference
on Uncertainty in Artificial Intelligence.
626
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 720?728,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical Joint Learning:
Improving Joint Parsing and Named Entity Recognition
with Non-Jointly Labeled Data
Jenny Rose Finkel and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel|manning}@cs.stanford.edu
Abstract
One of the main obstacles to produc-
ing high quality joint models is the lack
of jointly annotated data. Joint model-
ing of multiple natural language process-
ing tasks outperforms single-task models
learned from the same data, but still under-
performs compared to single-task models
learned on the more abundant quantities
of available single-task annotated data. In
this paper we present a novel model which
makes use of additional single-task anno-
tated data to improve the performance of
a joint model. Our model utilizes a hier-
archical prior to link the feature weights
for shared features in several single-task
models and the joint model. Experiments
on joint parsing and named entity recog-
nition, using the OntoNotes corpus, show
that our hierarchical joint model can pro-
duce substantial gains over a joint model
trained on only the jointly annotated data.
1 Introduction
Joint learning of multiple types of linguistic struc-
ture results in models which produce more consis-
tent outputs, and for which performance improves
across all aspects of the joint structure. Joint
models can be particularly useful for producing
analyses of sentences which are used as input for
higher-level, more semantically-oriented systems,
such as question answering and machine trans-
lation. These high-level systems typically com-
bine the outputs from many low-level systems,
such as parsing, named entity recognition (NER)
and coreference resolution. When trained sepa-
rately, these single-task models can produce out-
puts which are inconsistent with one another, such
as named entities which do not correspond to any
nodes in the parse tree (see Figure 1 for an ex-
ample). Moreover, one expects that the different
types of annotations should provide useful infor-
mation to one another, and that modeling them
jointly should improve performance. Because a
named entity should correspond to a node in the
parse tree, strong evidence about either aspect of
the model should positively impact the other as-
pect.
However, designing joint models which actu-
ally improve performance has proven challeng-
ing. The CoNLL 2008 shared task (Surdeanu
et al, 2008) was on joint parsing and semantic
role labeling, but the best systems (Johansson and
Nugues, 2008) were the ones which completely
decoupled the tasks. While negative results are
rarely published, this was not the first failed at-
tempt at joint parsing and semantic role label-
ing (Sutton and McCallum, 2005). There have
been some recent successes with joint modeling.
Zhang and Clark (2008) built a perceptron-based
joint segmenter and part-of-speech (POS) tagger
for Chinese, and Toutanova and Cherry (2009)
learned a joint model of lemmatization and POS
tagging which outperformed a pipelined model.
Adler and Elhadad (2006) presented an HMM-
based approach for unsupervised joint morpho-
logical segmentation and tagging of Hebrew, and
Goldberg and Tsarfaty (2008) developed a joint
model of segmentation, tagging and parsing of He-
brew, based on lattice parsing. No discussion of
joint modeling would be complete without men-
tion of (Miller et al, 2000), who trained a Collins-
style generative parser (Collins, 1997) over a syn-
tactic structure augmented with the template entity
and template relations annotations for the MUC-7
shared task.
One significant limitation for many joint mod-
els is the lack of jointly annotated data. We built
a joint model of parsing and named entity recog-
nition (Finkel and Manning, 2009b), which had
small gains on parse performance and moderate
gains on named entity performance, when com-
pared with single-task models trained on the same
data. However, the performance of our model,
trained using the OntoNotes corpus (Hovy et al,
2006), fell short of separate parsing and named
720
FRAG
INTJ
UH
Like
NP
NP
DT
a
NN
gross
PP
IN
of
NP
QP
DT
a
CD
[billion
NNS
dollars]MONEY
NP
JJ
last
NN
year
Figure 1: Example from the data where separate parse and named entity models give conflicting output.
entity models trained on larger corpora, annotated
with only one type of information.
This paper addresses the problem of how to
learn high-quality joint models with smaller quan-
tities of jointly-annotated data that has been aug-
mented with larger amounts of single-task an-
notated data. To our knowledge this work is
the first attempt at such a task. We use a hi-
erarchical prior to link a joint model trained on
jointly-annotated data with other single-task mod-
els trained on single-task annotated data. The key
to making this work is for the joint model to share
some features with each of the single-task models.
Then, the singly-annotated data can be used to in-
fluence the feature weights for the shared features
in the joint model. This is an important contribu-
tion, because it provides all the benefits of joint
modeling, but without the high cost of jointly an-
notating large corpora. We applied our hierarchi-
cal joint model to parsing and named entity recog-
nition, and it reduced errors by over 20% on both
tasks when compared to a joint model trained on
only the jointly annotated data.
2 Related Work
Our task can be viewed as an instance of multi-task
learning, a machine learning paradigm in which
the objective is to simultaneously solve multiple,
related tasks for which you have separate labeled
training data. Many schemes for multitask learn-
ing, including the one we use here, are instances
of hierarchical models. There has not been much
work on multi-task learning in the NLP com-
munity; Daume? III (2007) and Finkel and Man-
ning (2009a) both build models for multi-domain
learning, a variant on domain adaptation where
there exists labeled training data for all domains
and the goal is to improve performance on all of
them. Ando and Zhang (2005) utilized a multi-
task learner within their semi-supervised algo-
rithm to learn feature representations which were
useful across a large number of related tasks. Out-
side of the NLP community, Elidan et al (2008)
used an undirected Bayesian transfer hierarchy
to jointly model the shapes of multiple mammal
species. Evgeniou et al (2005) applied a hier-
archical prior to modeling exam scores of stu-
dents. Other instances of multi-task learning in-
clude (Baxter, 1997; Caruana, 1997; Yu et al,
2005; Xue et al, 2007). For a more general discus-
sion of hierarchical models, we direct the reader to
Chapter 5 of (Gelman et al, 2003) and Chapter 12
of (Gelman and Hill, 2006).
3 Hierarchical Joint Learning
In this section we will discuss the main con-
tribution of this paper, our hierarchical joint
model which improves joint modeling perfor-
mance through the use of single-task models
which can be trained on singly-annotated data.
Our experiments are on a joint parsing and named
entity task, but the technique is more general and
only requires that the base models (the joint model
and single-task models) share some features. This
section covers the general technique, and we will
cover the details of the parsing, named entity, and
joint models that we use in Section 4.
3.1 Intuitive Overview
As discussed, we have a joint model which re-
quires jointly-annotated data, and several single-
task models which only require singly-annotated
data. The key to our hierarchical model is that the
joint model must have features in common with
each of the single models, though it can also have
features which are only present in the joint model.
721
PARSE JOINT NER
?
?? ??
?p ?p
Dp
?j ?j
Dj
?n ?n
Dn
Figure 2: A graphical representation of our hierar-
chical joint model. There are separate base models
for just parsing, just NER, and joint parsing and
NER. The parameters for these models are linked
via a hierarchical prior.
Each model has its own set of parameters (feature
weights). However, parameters for the features
which are shared between the single-task models
and the joint model are able to influence one an-
other via a hierarchical prior. This prior encour-
ages the learned weights for the different models
to be similar to one another. After training has
been completed, we retain only the joint model?s
parameters. Our resulting joint model is of higher
quality than a comparable joint model trained on
only the jointly-annotated data, due to all of the ev-
idence provided by the additional single-task data.
3.2 Formal Model
We have a set M of three base models: a
parse-only model, an NER-only model and a
joint model. These have corresponding log-
likelihood functions Lp(Dp; ?p), Ln(Dn; ?n), and
Lj(Dj ; ?j), where the Ds are the training data for
each model, and the ?s are the model-specific pa-
rameter (feature weight) vectors. These likelihood
functions do not include priors over the ?s. For
representational simplicity, we assume that each
of these vectors is the same size and corresponds
to the same ordering of features. Features which
don?t apply to a particular model type (e.g., parse
features in the named entity model) will always
be zero, so their weights have no impact on that
model?s likelihood function. Conversely, allowing
the presence of those features in models for which
they do not apply will not influence their weights
in the other models because there will be no evi-
dence about them in the data. These three models
are linked by a hierarchical prior, and their fea-
ture weight vectors are all drawn from this prior.
The parameters ?? for this prior have the same di-
mensionality as the model-specific parameters ?m
and are drawn from another, top-level prior. In our
case, this top-level prior is a zero-mean Gaussian.1
The graphical representation of our hierarchical
model is shown in Figure 2. The log-likelihood of
this model is
Lhier-joint(D; ?) = (1)
?
m?M
(
Lm(Dm; ?m)?
?
i
(?m,i ? ??,i)2
2?2m
)
?
?
i
(??,i ? ?i)2
2?2?
The first summation in this equation computes the
log-likelihood of each model, using the data and
parameters which correspond to that model, and
the prior likelihood of that model?s parameters,
based on a Gaussian prior centered around the
top-level, non-model-specific parameters ??, and
with model-specific variance ?m. The final sum-
mation in the equation computes the prior likeli-
hood of the top-level parameters ?? according to a
Gaussian prior with variance ?? and mean ? (typ-
ically zero). This formulation encourages each
base model to have feature weights similar to the
top-level parameters (and hence one another).
The effects of the variances ?m and ?? warrant
some discussion. ?? has the familiar interpretation
of dictating how much the model ?cares? about
feature weights diverging from zero (or ?). The
model-specific variances, ?m, have an entirely dif-
ferent interpretation. They dictate how how strong
the penalty is for the domain-specific parameters
to diverge from one another (via their similarity to
??). When ?m are very low, then they are encour-
aged to be very similar, and taken to the extreme
this is equivalent to completely tying the parame-
ters between the tasks. When ?m are very high,
then there is less encouragement for the parame-
ters to be similar, and taken to the extreme this is
equivalent to completely decoupling the tasks.
We need to compute partial derivatives in or-
der to optimize the model parameters. The partial
derivatives for the parameters for each base model
m are given by:
?Lhier(D; ?)
??m,i
= ?Lm(Dm, ?m)??m,i
? ?m,i ? ??,i?2d (2)
where the first term is the partial derivative ac-
cording to the base model, and the second term is
1Though we use a zero-mean Gaussian prior, this top-
level prior could take many forms, including an L1 prior, or
another hierarchical prior.
722
the prior centered around the top-level parameters.
The partial derivatives for the top level parameters
?? are:
?Lhier(D; ?)
???,i
=
(
?
m?M
??,i ? ?m,i
?2m
)
? ??,i ? ?i?2?
(3)
where the first term relates to how far each model-
specific weight vector is from the top-level param-
eter values, and the second term relates how far
each top-level parameter is from zero.
When a model has strong evidence for a feature,
effectively what happens is that it pulls the value
of the top-level parameter for that feature closer to
the model-specific value for it. When it has little
or no evidence for a feature then it will be pulled
in the direction of the top-level parameter for that
feature, whose value was influenced by the models
which have evidence for that feature.
3.3 Optimization with Stochastic Gradient
Descent
Inference in joint models tends to be slow, and of-
ten requires the use of stochastic optimization in
order for the optimization to be tractable. L-BFGS
and gradient descent, two frequently used numer-
ical optimization algorithms, require computing
the value and partial derivatives of the objective
function using the entire training set. Instead,
we use stochastic gradient descent. It requires a
stochastic objective function, which is meant to be
a low computational cost estimate of the real ob-
jective function. In most NLP models, such as lo-
gistic regression with a Gaussian prior, computing
the stochastic objective function is fairly straight-
forward: you compute the model likelihood and
partial derivatives for a randomly sampled subset
of the training data. When computing the term
for the prior, it must be rescaled by multiplying
its value and derivatives by the proportion of the
training data used. The stochastic objective func-
tion, where D? ? D is a randomly drawn subset of
the full training set, is given by
Lstoch(D; ?) = Lorig(D?; ?)?
|D?|
|D|
?
i
(??,i)2
2?2?
(4)
This is a stochastic function, and multiple calls to
it with the same D and ? will produce different
values because D? is resampled each time. When
designing a stochastic objective function, the crit-
ical fact to keep in mind is that the summed values
and partial derivatives for any split of the data need
to be equal to that of the full dataset. In practice,
stochastic gradient descent only makes use of the
partial derivatives and not the function value, so
we will focus the remainder of the discussion on
how to rescale the partial derivatives.
We now describe the more complicated case
of stochastic optimization with a hierarchical ob-
jective function. For the sake of simplicity, let
us assume that we are using a batch size of one,
meaning |D?| = 1 in the above equation. Note
that in the hierarchical model, each datum (sen-
tence) in each base model should be weighted
equally, so whichever dataset is the largest should
be proportionally more likely to have one of its
data sampled. For the sampled datum d, we then
compute the function value and partial derivatives
with respect to the correct base model for that da-
tum. When we rescale the model-specific prior, we
rescale based on the number of data in that model?s
training set, not the total number of data in all the
models combined. Having uniformly randomly
drawn datum d ? ?m?MDm, let m(d) ? M
tell us to which model?s training data the datum
belongs. The stochastic partial derivatives will
equal zero for all model parameters ?m such that
m 6= m(d), and for ?m(d) it becomes:
?Lhier-stoch(D; ?)
??m(d),i
= (5)
?Lm(d)({d}; ?m(d))
??m(d),i
? 1|Dm(d)|
(?m(d),i ? ??,i
?2d
)
Now we will discuss the stochastic partial deriva-
tives with respect to the top-level parameters ??,
which requires modifying Equation 3. The first
term in that equation is a summation over all
the models. In the stochastic derivative we only
perform this computation for the datum?s model
m(d), and then we rescale that value based on the
number of data in that datum?s model |Dm(d)|. The
second term in that equation is rescaled by the to-
tal number of data in all models combined. The
stochastic partial derivatives with respect to ?? be-
come:
?Lhier-stoch(D; ?)
???,i
= (6)
1
|Dm(d)|
(??,i ? ?m(d),i
?2m
)
? 1?
m?M
|Dm|
(
??,i
?2?
)
where for conciseness we omit ? under the as-
sumption that it equals zero.
An equally correct formulation for the partial
derivative of ?? is to simply rescale Equation 3
by the total number of data in all models. Early
experiments found that both versions gave simi-
lar performance, but the latter was significantly
723
B-PER
Hilary
I-PER
Clinton
O
visited
B-GPE
Haiti
O
.
(a)
PER
Hilary Clinton
O
visited
GPE
Haiti
O
.
(b)
ROOT
PER
PER-i
Hilary
PER-i
Clinton
O
visited
GPE
GPE-i
Haiti
O
.
(c)
Figure 3: A linear-chain CRF (a) labels each word,
whereas a semi-CRF (b) labels entire entities. A
semi-CRF can be represented as a tree (c), where i
indicates an internal node for an entity.
slower to compute because it required summing
over the parameter vectors for all base models in-
stead of just the vector for the datum?s model.
When using a batch size larger than one, you
compute the given functions for each datum in the
batch and then add them together.
4 Base Models
Our hierarchical joint model is composed of three
separate models, one for just named entity recog-
nition, one for just parsing, and one for joint pars-
ing and named entity recognition. In this section
we will review each of these models individually.
4.1 Semi-CRF for Named Entity Recognition
For our named entity recognition model we use a
semi-CRF (Sarawagi and Cohen, 2004; Andrew,
2006). Semi-CRFs are very similar to the more
popular linear-chain CRFs, but with several key
advantages. Semi-CRFs segment and label the
text simultaneously, whereas a linear-chain CRF
will only label each word, and segmentation is im-
plied by the labels assigned to the words. When
doing named entity recognition, a semi-CRF will
have one node for each entity, unlike a regular
CRF which will have one node for each word.2
See Figure 3a-b for an example of a semi-CRF
and a linear-chain CRF over the same sentence.
Note that the entity Hilary Clinton has one node
in the semi-CRF representation, but two nodes in
the linear-chain CRF. Because different segmen-
tations have different model structures in a semi-
CRF, one has to consider all possible structures
(segmentations) as well as all possible labelings.
It is common practice to limit segment length in
order to speed up inference, as this allows for the
use of a modified version of the forward-backward
algorithm. When segment length is not restricted,
the inference procedure is the same as that used
in parsing (Finkel and Manning, 2009c).3 In this
work we do not enforce a length restriction, and
directly utilize the fact that the model can be trans-
formed into a parsing model. Figure 3c shows a
parse tree representation of a semi-CRF.
While a linear-chain CRF allows features over
adjacent words, a semi-CRF allows them over ad-
jacent segments. This means that a semi-CRF can
utilize all features used by a linear-chain CRF, and
can also utilize features over entire segments, such
as First National Bank of New York City, instead of
just adjacent words like First National and Bank
of. Let y be a vector representing the labeling for
an entire sentence. yi encodes the label of the ith
segment, along with the span of words the seg-
ment encompasses. Let ? be the feature weights,
and f(s, yi, yi?1) the feature function over adja-
cent segments yi and yi?1 in sentence s.4 The log
likelihood of a semi-CRF for a single sentence s is
given by:
L(y|s; ?) = 1Zs
|y|?
i=1
exp{? ? f(s, yi, yi?1)} (7)
The partition function Zs serves as a normalizer.
It requires summing over the set ys of all possible
segmentations and labelings for the sentence s:
Zs =
?
y?ys
|y|?
i=1
exp{? ? f(s, yi, yi?1)} (8)
2Both models will have one node per word for non-entity
words.
3While converting a semi-CRF into a parser results in
much slower inference than a linear-chain CRF, it is still sig-
nificantly faster than a treebank parser due to the reduced
number of labels.
4There can also be features over single entities, but these
can be encoded in the feature function over adjacent entities,
so for notational simplicity we do not include an additional
term for them.
724
FRAG
INTJ
UH
Like
NP
NP
DT
a
NN
gross
PP
IN
of
NP-MONEY
QP-MONEY-i
DT-MONEY-i
a
CD-MONEY-i
billion
NNS-MONEY-i
dollars
NP
JJ
last
NN
year
Figure 4: An example of a sentence jointly annotated with parse and named entity information. Named
entities correspond to nodes in the tree, and the parse label is augmented with the named entity informa-
tion.
Because we use a tree representation, it is
easy to ensure that the features used in the NER
model are identical to those in the joint parsing
and named entity model, because the joint model
(which we will discuss in Section 4.3) is also
based on a tree representation where each entity
corresponds to a single node in the tree.
4.2 CRF-CFG for Parsing
Our parsing model is the discriminatively trained,
conditional random field-based context-free gram-
mar parser (CRF-CFG) of (Finkel et al, 2008).
The relationship between a CRF-CFG and a PCFG
is analogous to the relationship between a linear-
chain CRF and a hidden Markov model (HMM)
for modeling sequence data. Let t be a com-
plete parse tree for sentence s, and each lo-
cal subtree r ? t encodes both the rule from
the grammar, and the span and split informa-
tion (e.g NP(7,9) ? JJ(7,8)NN(8,9) which covers
the last two words in Figure 1). The feature func-
tion f(r, s) computes the features, which are de-
fined over a local subtree r and the words of the
sentence. Let ? be the vector of feature weights.
The log-likelihood of tree t over sentence s is:
L(t|s; ?) = 1Zs
?
r?t
exp{? ? f(r, s)} (9)
To compute the partition function Zs, which
serves to normalize the function, we must sum
over ?(s), the set of all possible parse trees for
sentence s. The partition function is given by:
Zs =
?
t???(s)
?
r?t?
exp{? ? f(r, s)}
We also need to compute the partial derivatives
which are used during optimization. Let fi(r, s)
be the value of feature i for subtree r over sen-
tence s, and let E?[fi|s] be the expected value of
feature i in sentence s, based on the current model
parameters ?. The partial derivatives of ? are then
given by
?L
??i
=
?
(t,s)?D
((?
r?t
fi(r, s)
)
? E?[fi|s]
)
(10)
Just like with a linear-chain CRF, this equation
will be zero when the feature expectations in the
model equal the feature values in the training data.
A variant of the inside-outside algorithm is used
to efficiently compute the likelihood and partial
derivatives. See (Finkel et al, 2008) for details.
4.3 Joint Model of Parsing and Named Entity
Recognition
Our base joint model for parsing and named entity
recognition is the same as (Finkel and Manning,
2009b), which is also based on the discriminative
parser discussed in the previous section. The parse
tree structure is augmented with named entity in-
formation; see Figure 4 for an example. The fea-
tures in the joint model are designed in a man-
ner that fits well with the hierarchical joint model:
some are over just the parse structure, some are
over just the named entities, and some are over the
joint structure. The joint model shares the NER
and parse features with the respective single-task
models. Features over the joint structure only ap-
pear in the joint model, and their weights are only
indirectly influenced by the singly-annotated data.
In the parsing model, the grammar consists of
only the rules observed in the training data. In the
joint model, the grammar is augmented with ad-
725
Training Testing
Range # Sent. Range # Sent.
ABC 0?55 1195 56?69 199
MNB 0?17 509 18?25 245
NBC 0?29 589 30?39 149
PRI 0?89 1704 90?112 394
VOA 0?198 1508 199?264 385
Table 1: Training and test set sizes for the five
datasets in sentences. The file ranges refer to
the numbers within the names of the original
OntoNotes files.
ditional joint rules which are composed by adding
named entity information to existing parse rules.
Because the grammars are based on the observed
data, and the two models have different data, they
will have somewhat different grammars. In our hi-
erarchical joint model, we added all observed rules
from the joint data (stripped of named entity infor-
mation) to the parse-only grammar, and we added
all observed rules from the parse-only data to the
grammar for the joint model, and augmented them
with named entity information in the same manner
as the rules observed in the joint data.
Earlier we said that the NER-only model uses
identical named entity features as the joint model
(and similarly for the parse-only model), but this
is not quite true. They use identical feature tem-
plates, such as word, but different realizations
of those features will occur with the different
datasets. For instance, the NER-only model may
have word=Nigel as a feature, but because Nigel
never occurs in the joint data, that feature is never
manifested and no weight is learned for it. We deal
with this similarly to how we dealt with the gram-
mar: if a named entity feature occurs in either the
joint data or the NER-only data, then both mod-
els will learn a weight for that feature. We do the
same thing for the parse features. This modeling
decision gives the joint model access to potentially
useful features to which it would not have had ac-
cess if it were not part of the hierarchical model.5
5 Experiments and Discussion
We compared our hierarchical joint model to a reg-
ular (non-hierarchical) joint model, and to parse-
only and NER-only models. Our baseline ex-
periments were modeled after those in (Finkel
and Manning, 2009b), and while our results were
not identical (we updated to a newer release of
the data), we had similar results and found the
same general trends with respect to how the joint
5In the non-hierarchical setting, you could include those
features in the optimization, but, because there would be no
evidence about them, their weights would be zero due to reg-
ularization.
model improved on the single models. We used
OntoNotes 3.0 (Hovy et al, 2006), and made the
same data modifications as (Finkel and Manning,
2009b) to ensure consistency between the parsing
and named entity annotations. Table 2 has our
complete set of results, and Table 1 gives the num-
ber of training and test sentences. For each sec-
tion of the data (ABC, MNB, NBC, PRI, VOA)
we ran experiments training a linear-chain CRF
on only the named entity information, a CRF-CFG
parser on only the parse information, a joint parser
and named entity recognizer, and our hierarchi-
cal model. For the hierarchical model, we used
the CNN portion of the data (5093 sentences) for
the extra named entity data (and ignored the parse
trees) and the remaining portions combined for the
extra parse data (and ignored the named entity an-
notations). We used ?? = 1.0 and ?m = 0.1,
which were chosen based on early experiments on
development data. Small changes to ?m do not
appear to have much influence, but larger changes
do. We similarly decided how many iterations to
run stochastic gradient descent for (20) based on
early development data experiments. We did not
run this experiment on the CNN portion of the
data, because the CNN data was already being
used as the extra NER data.
As Table 2 shows, the hierarchical model did
substantially better than the joint model overall,
which is not surprising given the extra data to
which it had access. Looking at the smaller cor-
pora (NBC and MNB) we see the largest gains,
with both parse and NER performance improving
by about 8% F1. ABC saw about a 6% gain on
both tasks, and VOA saw a 1% gain on both. Our
one negative result is in the PRI portion: parsing
improves slightly, but NER performance decreases
by almost 2%. The same experiment on develop-
ment data resulted in a performance increase, so
we are not sure why we saw a decrease here. One
general trend, which is not surprising, is that the
hierarchical model helps the smaller datasets more
than the large ones. The source of this is two-
fold: lower baselines are generally easier to im-
prove upon, and the larger corpora had less singly-
annotated data to provide improvements, because
it was composed of the remaining, smaller, sec-
tions of OntoNotes. We found it interesting that
the gains tended to be similar on both tasks for all
datasets, and believe this fact is due to our use of
roughly the same amount of singly-annotated data
for both parsing and NER.
One possible conflating factor in these experi-
ments is that of domain drift. While we tried to
726
Parse Labeled Bracketing Named Entities
Precision Recall F1 Precision Recall F1
ABC Just Parse 69.8% 69.9% 69.8% ?
Just NER ? 77.0% 75.1% 76.0%
Baseline Joint 70.2% 70.5% 70.3% 79.2% 76.5% 77.8%
Hierarchical Joint 75.5% 74.4% 74.9% 85.1% 82.7% 83.9%
MNB Just Parse 61.7% 65.5% 63.6% ?
Just NER ? 69.6% 49.0% 57.5%
Baseline Joint 61.7% 66.2% 63.9% 70.9% 63.5% 67.0%
Hierarchical Joint 72.6% 70.2% 71.4% 74.4% 75.5% 74.9%
NBC Just Parse 59.9% 63.9% 61.8% ?
Just NER ? 63.9% 60.9% 62.4%
Baseline Joint 59.3% 64.2% 61.6% 68.9% 62.8% 65.7%
Hierarchical Joint 70.4% 69.9% 70.2% 72.9% 74.0% 73.4%
PRI Just Parse 78.6% 77.0% 76.9% ?
Just NER ? 81.3% 77.8% 79.5%
Baseline Joint 78.0% 78.6% 78.3% 86.3% 86.0% 86.2%
Hierarchical Joint 79.2% 78.5% 78.8% 84.2% 85.5% 84.8%
VOA Just Parse 77.5% 76.5% 77.0% ?
Just NER ? 85.2% 80.3% 82.7%
Baseline Joint 77.2% 77.8% 77.5% 87.5% 86.7% 87.1%
Hierarchical Joint 79.8% 77.8% 78.8% 87.7% 88.9% 88.3%
Table 2: Full parse and NER results for the six datasets. Parse trees were evaluated using evalB, and
named entities were scored using micro-averaged F-measure (conlleval).
get the most similar annotated data available ? data
which was annotated by the same annotators, and
all of which is broadcast news ? these are still dif-
ferent domains. While this is likely to have a nega-
tive effect on results, we also believe this scenario
to be a more realistic than if it were to also be data
drawn from the exact same distribution.
6 Conclusion
In this paper we presented a novel method for
improving joint modeling using additional data
which has not been labeled with the entire joint
structure. While conventional wisdom says that
adding more training data should always improve
performance, this work is the first to our knowl-
edge to incorporate singly-annotated data into a
joint model, thereby providing a method for this
additional data, which cannot be directly used by
the non-hierarchical joint model, to help improve
joint modeling performance. We built single-task
models for the non-jointly labeled data, designing
those single-task models so that they have features
in common with the joint model, and then linked
all of the different single-task and joint models
via a hierarchical prior. We performed experi-
ments on joint parsing and named entity recogni-
tion, and found that our hierarchical joint model
substantially outperformed a joint model which
was trained on only the jointly annotated data.
Future directions for this work include automat-
ically learning the variances, ?m and ?? in the hi-
erarchical model, so that the degree of information
sharing between the models is optimized based on
the training data available. We are also interested
in ways to modify the objective function to place
more emphasis on learning a good joint model, in-
stead of equally weighting the learning of the joint
and single-task models.
Acknowledgments
Many thanks to Daphne Koller for discussions
which led to this work, and to Richard Socher
for his assistance and input. Thanks also to our
anonymous reviewers and Yoav Goldberg for use-
ful feedback on an earlier draft of this paper.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) un-
der prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the view of
the Air Force Research Laboratory (AFRL). The
first author is additionally supported by a Stanford
Graduate Fellowship.
727
References
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological disam-
biguation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th annual
meeting of the Association for Computational Linguistics,
pages 665?672, Morristown, NJ, USA. Association for
Computational Linguistics.
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In ACL ?05: Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics,
pages 1?9, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Galen Andrew. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In Pro-
ceedings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2006).
J. Baxter. 1997. A bayesian/information theoretic model of
learning to learn via multiple task sampling. In Machine
Learning, volume 28.
R. Caruana. 1997. Multitask learning. In Machine Learning,
volume 28.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL 1997.
Hal Daume? III. 2007. Frustratingly easy domain adaptation.
In Conference of the Association for Computational Lin-
guistics (ACL), Prague, Czech Republic.
Gal Elidan, Benjamin Packer, Geremy Heitz, and Daphne
Koller. 2008. Convex point estimation using undirected
bayesian transfer hierarchies. In UAI 2008.
T. Evgeniou, C. Micchelli, and M. Pontil. 2005. Learning
multiple tasks with kernel methods. In Journal of Machine
Learning Research.
Jenny Rose Finkel and Christopher D. Manning. 2009a. Hi-
erarchical bayesian domain adaptation. In Proceedings
of the North American Association of Computational Lin-
guistics (NAACL 2009).
Jenny Rose Finkel and Christopher D. Manning. 2009b. Joint
parsing and named entity recognition. In Proceedings of
the North American Association of Computational Lin-
guistics (NAACL 2009).
Jenny Rose Finkel and Christopher D. Manning. 2009c.
Nested named entity recognition. In Proceedings of
EMNLP 2009.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Man-
ning. 2008. Efficient, feature-based conditional random
field parsing. In ACL/HLT-2008.
Andrew Gelman and Jennifer Hill. 2006. Data Analysis Us-
ing Regression and Multilevel/Hierarchical Models. Cam-
bridge University Press.
A. Gelman, J. B. Carlin, H. S. Stern, and Donald D. B. Rubin.
2003. Bayesian Data Analysis. Chapman & Hall.
Yoav Goldberg and Reut Tsarfaty. 2008. A single genera-
tive model for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL-08: HLT, pages
371?379, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In HLT-NAACL 2006.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based syntactic-semantic analysis with propbank and
nombank. In CoNLL ?08: Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing, pages 183?187, Morristown, NJ, USA. Association
for Computational Linguistics.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In In 6th Applied Natural
Language Processing Conference, pages 226?233.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov
conditional random fields for information extraction. In In
Advances in Neural Information Processing Systems 17,
pages 1185?1192.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL),
Manchester, UK.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In Conference on Natural
Language Learning (CoNLL).
Kristina Toutanova and Colin Cherry. 2009. A global model
for joint lemmatization and part-of-speech prediction. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 486?494, Suntec, Singapore, August. Association
for Computational Linguistics.
Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishna-
puram. 2007. Multi-task learning for classification with
dirichlet process priors. J. Mach. Learn. Res., 8.
Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005. Learn-
ing gaussian processes from multiple tasks. In ICML ?05:
Proceedings of the 22nd international conference on Ma-
chine learning.
Yue Zhang and Stephen Clark. 2008. Joint word segmenta-
tion and POS tagging using a single perceptron. In ACL
2008.
728
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Stanford CoreNLP Natural Language Processing Toolkit
Christopher D. Manning
Linguistics & Computer Science
Stanford University
manning@stanford.edu
Mihai Surdeanu
SISTA
University of Arizona
msurdeanu@email.arizona.edu
John Bauer
Dept of Computer Science
Stanford University
horatio@stanford.edu
Jenny Finkel
Prismatic Inc.
jrfinkel@gmail.com
Steven J. Bethard
Computer and Information Sciences
U. of Alabama at Birmingham
bethard@cis.uab.edu
David McClosky
IBM Research
dmcclosky@us.ibm.com
Abstract
We describe the design and use of the
Stanford CoreNLP toolkit, an extensible
pipeline that provides core natural lan-
guage analysis. This toolkit is quite widely
used, both in the research NLP community
and also among commercial and govern-
ment users of open source NLP technol-
ogy. We suggest that this follows from
a simple, approachable design, straight-
forward interfaces, the inclusion of ro-
bust and good quality analysis compo-
nents, and not requiring use of a large
amount of associated baggage.
1 Introduction
This paper describe the design and development of
Stanford CoreNLP, a Java (or at least JVM-based)
annotation pipeline framework, which provides
most of the common core natural language pro-
cessing (NLP) steps, from tokenization through to
coreference resolution. We describe the original
design of the system and its strengths (section 2),
simple usage patterns (section 3), the set of pro-
vided annotators and how properties control them
(section 4), and how to add additional annotators
(section 5), before concluding with some higher-
level remarks and additional appendices. While
there are several good natural language analysis
toolkits, Stanford CoreNLP is one of the most
used, and a central theme is trying to identify the
attributes that contributed to its success.
2 Original Design and Development
Our pipeline system was initially designed for in-
ternal use. Previously, when combining multiple
natural language analysis components, each with
their own ad hoc APIs, we had tied them together
with custom glue code. The initial version of the
Tokeniza)on*
Sentence*Spli0ng*
Part4of4speech*Tagging*
Morphological*Analysis*
Named*En)ty*Recogni)on*
Syntac)c*Parsing*
Other*Annotators*
Coreference*Resolu)on**
Raw*text*
Execu)
on*Flow
* Annota)on*Object*
Annotated*text*
(tokenize)*
(ssplit)*
(pos)*
(lemma)*
(ner)*
(parse)*
(dcoref)*
(gender, sentiment)!
Figure 1: Overall system architecture: Raw text
is put into an Annotation object and then a se-
quence of Annotators add information in an analy-
sis pipeline. The resulting Annotation, containing
all the analysis information added by the Annota-
tors, can be output in XML or plain text forms.
annotation pipeline was developed in 2006 in or-
der to replace this jumble with something better.
A uniform interface was provided for an Annota-
tor that adds some kind of analysis information to
some text. An Annotator does this by taking in an
Annotation object to which it can add extra infor-
mation. An Annotation is stored as a typesafe het-
erogeneous map, following the ideas for this data
type presented by Bloch (2008). This basic archi-
tecture has proven quite successful, and is still the
basis of the system described here. It is illustrated
in figure 1. The motivations were:
? To be able to quickly and painlessly get linguis-
tic annotations for a text.
? To hide variations across components behind a
common API.
? To have a minimal conceptual footprint, so the
system is easy to learn.
? To provide a lightweight framework, using plain
Java objects (rather than something of heav-
ier weight, such as XML or UIMA?s Common
Analysis System (CAS) objects).
55
In 2009, initially as part of a multi-site grant
project, the system was extended to be more easily
usable by a broader range of users. We provided
a command-line interface and the ability to write
out an Annotation in various formats, including
XML. Further work led to the system being re-
leased as free open source software in 2010.
On the one hand, from an architectural perspec-
tive, Stanford CoreNLP does not attempt to do ev-
erything. It is nothing more than a straightforward
pipeline architecture. It provides only a Java API.
1
It does not attempt to provide multiple machine
scale-out (though it does provide multi-threaded
processing on a single machine). It provides a sim-
ple concrete API. But these requirements satisfy
a large percentage of potential users, and the re-
sulting simplicity makes it easier for users to get
started with the framework. That is, the primary
advantage of Stanford CoreNLP over larger frame-
works like UIMA (Ferrucci and Lally, 2004) or
GATE (Cunningham et al., 2002) is that users do
not have to learn UIMA or GATE before they can
get started; they only need to know a little Java.
In practice, this is a large and important differ-
entiator. If more complex scenarios are required,
such as multiple machine scale-out, they can nor-
mally be achieved by running the analysis pipeline
within a system that focuses on distributed work-
flows (such as Hadoop or Spark). Other systems
attempt to provide more, such as the UIUC Cu-
rator (Clarke et al., 2012), which includes inter-
machine client-server communication for process-
ing and the caching of natural language analyses.
But this functionality comes at a cost. The system
is complex to install and complex to understand.
Moreover, in practice, an organization may well
be committed to a scale-out solution which is dif-
ferent from that provided by the natural language
analysis toolkit. For example, they may be using
Kryo or Google?s protobuf for binary serialization
rather than Apache Thrift which underlies Cura-
tor. In this case, the user is better served by a fairly
small and self-contained natural language analysis
system, rather than something which comes with
a lot of baggage for all sorts of purposes, most of
which they are not using.
On the other hand, most users benefit greatly
from the provision of a set of stable, robust, high
1
Nevertheless, it can call an analysis component written in
other languages via an appropriate wrapper Annotator, and
in turn, it has been wrapped by many people to provide Stan-
ford CoreNLP bindings for other languages.
quality linguistic analysis components, which can
be easily invoked for common scenarios. While
the builder of a larger system may have made over-
all design choices, such as how to handle scale-
out, they are unlikely to be an NLP expert, and
are hence looking for NLP components that just
work. This is a huge advantage that Stanford
CoreNLP and GATE have over the empty tool-
box of an Apache UIMA download, something
addressed in part by the development of well-
integrated component packages for UIMA, such
as ClearTK (Bethard et al., 2014), DKPro Core
(Gurevych et al., 2007), and JCoRe (Hahn et al.,
2008). However, the solution provided by these
packages remains harder to learn, more complex
and heavier weight for users than the pipeline de-
scribed here.
These attributes echo what Patricio (2009) ar-
gued made Hibernate successful, including: (i) do
one thing well, (ii) avoid over-design, and (iii)
up and running in ten minutes or less! Indeed,
the design and success of Stanford CoreNLP also
reflects several other of the factors that Patricio
highlights, including (iv) avoid standardism, (v)
documentation, and (vi) developer responsiveness.
While there are many factors that contribute to the
uptake of a project, and it is hard to show causal-
ity, we believe that some of these attributes ac-
count for the fact that Stanford CoreNLP is one of
the more used NLP toolkits. While we certainly
have not done a perfect job, compared to much
academic software, Stanford CoreNLP has gained
from attributes such as clear open source licens-
ing, a modicum of attention to documentation, and
attempting to answer user questions.
3 Elementary Usage
A key design goal was to make it very simple to
set up and run processing pipelines, from either
the API or the command-line. Using the API, run-
ning a pipeline can be as easy as figure 2. Or,
at the command-line, doing linguistic processing
for a file can be as easy as figure 3. Real life is
rarely this simple, but the ability to get started us-
ing the product with minimal configuration code
gives new users a very good initial experience.
Figure 4 gives a more realistic (and complete)
example of use, showing several key properties of
the system. An annotation pipeline can be applied
to any text, such as a paragraph or whole story
rather than just a single sentence. The behavior of
56
Annotator pipeline = new StanfordCoreNLP();
Annotation annotation = new Annotation(
"Can you parse my sentence?");
pipeline.annotate(annotation);
Figure 2: Minimal code for an analysis pipeline.
export StanfordCoreNLP_HOME /where/installed
java -Xmx2g -cp $StanfordCoreNLP_HOME/*
edu.stanford.nlp.StanfordCoreNLP
-file input.txt
Figure 3: Minimal command-line invocation.
import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;
public class StanfordCoreNlpExample {
public static void main(String[] args) throws IOException {
PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");
Properties props = new Properties();
props.setProperty("annotators",
"tokenize, ssplit, pos, lemma, ner, parse");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(
"This is a short sentence. And this is another.");
pipeline.annotate(annotation);
pipeline.xmlPrint(annotation, xmlOut);
// An Annotation is a Map and you can get and use the
// various analyses individually. For instance, this
// gets the parse tree of the 1st sentence in the text.
List<CoreMap> sentences = annotation.get(
CoreAnnotations.SentencesAnnotation.class);
if (sentences != null && sentences.size() > 0) {
CoreMap sentence = sentences.get(0);
Tree tree = sentence.get(TreeAnnotation.class);
PrintWriter out = new PrintWriter(System.out);
out.println("The first sentence parsed is:");
tree.pennPrint(out);
}
}
}
Figure 4: A simple, complete example program.
annotators in a pipeline is controlled by standard
Java properties in a Properties object. The most
basic property to specify is what annotators to run,
in what order, as shown here. But as discussed be-
low, most annotators have their own properties to
allow further customization of their usage. If none
are specified, reasonable defaults are used. Run-
ning the pipeline is as simple as in the first exam-
ple, but then we show two possibilities for access-
ing the results. First, we convert the Annotation
object to XML and write it to a file. Second, we
show code that gets a particular type of informa-
tion out of an Annotation and then prints it.
Our presentation shows only usage in Java, but
the Stanford CoreNLP pipeline has been wrapped
by others so that it can be accessed easily from
many languages, including Python, Ruby, Perl,
Scala, Clojure, Javascript (node.js), and .NET lan-
guages, including C# and F#.
4 Provided annotators
The annotators provided with StanfordCoreNLP
can work with any character encoding, making use
of Java?s good Unicode support, but the system
defaults to UTF-8 encoding. The annotators also
support processing in various human languages,
providing that suitable underlying models or re-
sources are available for the different languages.
The system comes packaged with models for En-
glish. Separate model packages provide support
for Chinese and for case-insensitive processing of
English. Support for other languages is less com-
plete, but many of the Annotators also support
models for French, German, and Arabic (see ap-
pendix B), and building models for further lan-
guages is possible using the underlying tools. In
this section, we outline the provided annotators,
focusing on the English versions. It should be
noted that some of the models underlying annota-
tors are trained from annotated corpora using su-
pervised machine learning, while others are rule-
based components, which nevertheless often re-
quire some language resources of their own.
tokenize Tokenizes the text into a sequence of to-
kens. The English component provides a PTB-
style tokenizer, extended to reasonably handle
noisy and web text. The corresponding com-
ponents for Chinese and Arabic provide word
and clitic segmentation. The tokenizer saves the
character offsets of each token in the input text.
cleanxml Removes most or all XML tags from
the document.
ssplit Splits a sequence of tokens into sentences.
truecase Determines the likely true case of tokens
in text (that is, their likely case in well-edited
text), where this information was lost, e.g., for
all upper case text. This is implemented with
a discriminative model using a CRF sequence
tagger (Finkel et al., 2005).
pos Labels tokens with their part-of-speech (POS)
tag, using a maximum entropy POS tagger
(Toutanova et al., 2003).
lemma Generates the lemmas (base forms) for all
tokens in the annotation.
gender Adds likely gender information to names.
ner Recognizes named (PERSON, LOCATION,
ORGANIZATION, MISC) and numerical
(MONEY, NUMBER, DATE, TIME, DU-
RATION, SET) entities. With the default
57
annotators, named entities are recognized
using a combination of CRF sequence taggers
trained on various corpora (Finkel et al., 2005),
while numerical entities are recognized using
two rule-based systems, one for money and
numbers, and a separate state-of-the-art system
for processing temporal expressions (Chang
and Manning, 2012).
regexner Implements a simple, rule-based NER
over token sequences building on Java regular
expressions. The goal of this Annotator is to
provide a simple framework to allow a user to
incorporate NE labels that are not annotated in
traditional NL corpora. For example, a default
list of regular expressions that we distribute
in the models file recognizes ideologies (IDE-
OLOGY), nationalities (NATIONALITY), reli-
gions (RELIGION), and titles (TITLE).
parse Provides full syntactic analysis, including
both constituent and dependency representa-
tion, based on a probabilistic parser (Klein and
Manning, 2003; de Marneffe et al., 2006).
sentiment Sentiment analysis with a composi-
tional model over trees using deep learning
(Socher et al., 2013). Nodes of a binarized tree
of each sentence, including, in particular, the
root node of each sentence, are given a senti-
ment score.
dcoref Implements mention detection and both
pronominal and nominal coreference resolution
(Lee et al., 2013). The entire coreference graph
of a text (with head words of mentions as nodes)
is provided in the Annotation.
Most of these annotators have various options
which can be controlled by properties. These can
either be added to the Properties object when cre-
ating an annotation pipeline via the API, or spec-
ified either by command-line flags or through a
properties file when running the system from the
command-line. As a simple example, input to the
system may already be tokenized and presented
one-sentence-per-line. In this case, we wish the
tokenization and sentence splitting to just work by
using the whitespace, rather than trying to do any-
thing more creative (be it right or wrong). This can
be accomplished by adding two properties, either
to a properties file:
tokenize.whitespace: true
ssplit.eolonly: true
in code:
/** Simple annotator for locations stored in a gazetteer. */
package org.foo;
public class GazetteerLocationAnnotator implements Annotator {
// this is the only method an Annotator must implement
public void annotate(Annotation annotation) {
// traverse all sentences in this document
for (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {
// loop over all tokens in sentence (the text already tokenized)
List<CoreLabel> toks = sentence.get(TokensAnnotation.class);
for (int start = 0; start < toks.size(); start++) {
// assumes that the gazetteer returns the token index
// after the match or -1 otherwise
int end = Gazetteer.isLocation(toks, start);
if (end > start) {
for (int i = start; i < end; i ++) {
toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");
}
}
}
}
}
}
Figure 5: An example of a simple custom anno-
tator. The annotator marks the words of possibly
multi-word locations that are in a gazetteer.
props.setProperty("tokenize.whitespace", "true");
props.setProperty("ssplit.eolonly", "true");
or via command-line flags:
-tokenize.whitespace -ssplit.eolonly
We do not attempt to describe all the properties
understood by each annotator here; they are avail-
able in the documentation for Stanford CoreNLP.
However, we note that they follow the pattern of
being x.y, where x is the name of the annotator
that they apply to.
5 Adding annotators
While most users work with the provided annota-
tors, it is quite easy to add additional custom an-
notators to the system. We illustrate here both how
to write an Annotator in code and how to load it
into the Stanford CoreNLP system. An Annotator
is a class that implements three methods: a sin-
gle method for analysis, and two that describe the
dependencies between analysis steps:
public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
The information in an Annotation is updated in
place (usually in a non-destructive manner, by
adding new keys and values to the Annotation).
The code for a simple Annotator that marks loca-
tions contained in a gazetteer is shown in figure 5.
2
Similar code can be used to write a wrapper Anno-
tator, which calls some pre-existing analysis com-
ponent, and adds its results to the Annotation.
2
The functionality of this annotator is already provided by
the regexner annotator, but it serves as a simple example.
58
While building an analysis pipeline, Stanford
CoreNLP can add additional annotators to the
pipeline which are loaded using reflection. To pro-
vide a new Annotator, the user extends the class
edu.stanford.nlp.pipeline.Annotator
and provides a constructor with the signature
(String, Properties). Then, the user adds
the property
customAnnotatorClass.FOO: BAR
to the properties used to create the pipeline. If
FOO is then added to the list of annotators, the
class BAR will be loaded to instantiate it. The
Properties object is also passed to the constructor,
so that annotator-specific behavior can be initial-
ized from the Properties object. For instance, for
the example above, the properties file lines might
be:
customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotator
annotators: tokenize,ssplit,locgaz
locgaz.maxLength: 5
6 Conclusion
In this paper, we have presented the design
and usage of the Stanford CoreNLP system, an
annotation-based NLP processing pipeline. We
have in particular tried to emphasize the proper-
ties that we feel have made it successful. Rather
than trying to provide the largest and most engi-
neered kitchen sink, the goal has been to make it
as easy as possible for users to get started using
the framework, and to keep the framework small,
so it is easily comprehensible, and can easily be
used as a component within the much larger sys-
tem that a user may be developing. The broad us-
age of this system, and of other systems such as
NLTK (Bird et al., 2009), which emphasize acces-
sibility to beginning users, suggests the merits of
this approach.
A Pointers
Website: http://nlp.stanford.edu/software/
corenlp.shtml
Github: https://github.com/stanfordnlp/CoreNLP
Maven: http://mvnrepository.com/artifact/edu.
stanford.nlp/stanford-corenlp
License: GPL v2+
Stanford CoreNLP keeps the models for ma-
chine learning components and miscellaneous
other data files in a separate models jar file. If you
are using Maven, you need to make sure that you
list the dependency on this models file as well as
the code jar file. You can do that with code like the
following in your pom.xml. Note the extra depen-
dency with a classifier element at the bottom.
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
</dependency>
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
<classifier>models</classifier>
</dependency>
B Human language support
We summarize the analysis components supported
for different human languages in early 2014.
Annotator Ara- Chi- Eng- Fre- Ger-
bic nese lish nch man
Tokenize X X X X X
Sent. split X X X X X
Truecase X
POS X X X X X
Lemma X
Gender X
NER X X X
RegexNER X X X X X
Parse X X X X X
Dep. Parse X X
Sentiment X
Coref. X
C Getting the sentiment of sentences
We show a command-line for sentiment analysis.
$ cat sentiment.txt
I liked it.
It was a fantastic experience.
The plot move rather slowly.
$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators
tokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/
english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/
englishPCFG.ser.gz ... done [1.4 sec].
Adding annotator sentiment
Ready to process: 1 files, skipped 0, total 1
Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt ... writing to /Users/manning/Software/
stanford-corenlp-full-2014-01-04/sentiment.txt.xml {
Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt [0.583 seconds]
} [1.219 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.0 sec.
59
MorphaAnnotator: 0.0 sec.
ParserAnnotator: 0.4 sec.
SentimentAnnotator: 0.1 sec.
TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec.
Pipeline setup: 3.0 sec.
Total time for StanfordCoreNLP pipeline: 4.2 sec.
$ grep sentiment sentiment.txt.xml
<sentence id="1" sentimentValue="3" sentiment="Positive">
<sentence id="2" sentimentValue="4" sentiment="Verypositive">
<sentence id="3" sentimentValue="1" sentiment="Negative">
D Use within UIMA
The main part of using Stanford CoreNLP within
the UIMA framework (Ferrucci and Lally, 2004)
is mapping between CoreNLP annotations, which
are regular Java classes, and UIMA annotations,
which are declared via XML type descriptors
(from which UIMA-specific Java classes are gen-
erated). A wrapper for CoreNLP will typically de-
fine a subclass of JCasAnnotator ImplBase whose
process method: (i) extracts UIMA annotations
from the CAS, (ii) converts UIMA annotations to
CoreNLP annotations, (iii) runs CoreNLP on the
input annotations, (iv) converts the CoreNLP out-
put annotations into UIMA annotations, and (v)
saves the UIMA annotations to the CAS.
To illustrate part of this process, the ClearTK
(Bethard et al., 2014) wrapper converts CoreNLP
token annotations to UIMA annotations and saves
them to the CAS with the following code:
int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);
int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);
String pos = tokenAnn.get(PartOfSpeechAnnotation.class);
String lemma = tokenAnn.get(LemmaAnnotation.class);
Token token = new Token(jCas, begin, end);
token.setPos(pos);
token.setLemma(lemma);
token.addToIndexes();
where Token is a UIMA type, declared as:
<typeSystemDescription>
<name>Token</name>
<types>
<typeDescription>
<name>org.cleartk.token.type.Token</name>
<supertypeName>uima.tcas.Annotation</supertypeName>
<features>
<featureDescription>
<name>pos</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>lemma</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
</types>
</typeSystemDescription>
References
Steven Bethard, Philip Ogren, and Lee Becker. 2014.
ClearTK 2.0: Design patterns for machine learning
in UIMA. In LREC 2014.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Joshua Bloch. 2008. Effective Java. Addison Wesley,
Upper Saddle River, NJ, 2nd edition.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC 2012.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
learned to stop worrying and love NLP pipelines).
In LREC 2012.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In ACL 2002.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, pages 449?454.
David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL 43, pages 363?370.
I. Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,
M. Weimer, and T. Zesch. 2007. Darmstadt knowl-
edge processing repository based on UIMA. In
First Workshop on Unstructured Information Man-
agement Architecture at GLDV 2007, T?ubingen.
U. Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,
Poprat M, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the Julie lab UIMA component
registry. In LREC 2008.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, Advances in
Neural Information Processing Systems, volume 15,
pages 3?10. MIT Press.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Anthony Patricio. 2009. Why this project is success-
ful? https://community.jboss.org/wiki/
WhyThisProjectIsSuccessful.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP 2013, pages 1631?1642.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL 3, pages 252?259.
60
