Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Building a Competitive Opinion Summarization System: 
Challenges and Keys 
 
 
Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andr?s Montoyo 
Department of Software and Computing Systems 
University of Alicante 
Apartado de Correos 99, E-03080, Alicante, Spain 
{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es 
 
 
 
Abstract 
This paper presents an overview of our participation in 
the TAC 2008 Opinion Pilot Summarization task, as 
well as the proposed and evaluated post-competition 
improvements. We first describe our opinion 
summarization system and the results obtained. Further 
on, we identify the system?s weak points and suggest 
several improvements, focused both on information 
content, as well as linguistic and readability aspects. We 
obtain encouraging results, especially as far as F-
measure is concerned, outperforming the competition 
results by approximately 80%. 
1 Introduction 
The Opinion Summarization Pilot (OSP) task 
within the TAC 2008 competition consisted in 
generating summaries from answers to opinion 
questions retrieved from blogs (the Blog061 
collection). The questions were organized around 
25 targets ? persons, events, organizations etc.  
Additionally, a set of text snippets that contained 
the answers to the questions were provided by the 
organizers, their use being optional. An example of 
target, question and provided snippet is given in 
Figure 1. 
 
 
 
 
 
Figure 1. Examples of target, question and snippet 
 
                                                           
*Elena Lloret is funded by the FPI program (BES-2007-
16268) from the Spanish Ministry of Science and Innovation, 
under the project TEXT-MESS (TIN-2006-15265)  
1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html 
The techniques employed by the participants were 
mainly based on the already existing 
summarization systems. While most participants 
added new features (sentiment, pos/neg sentiment, 
pos/neg opinion) to account for the presence of 
positive opinions or negative ones - CLASSY 
(Conroy and Schlessinger, 2008); CCNU (He et 
al.,2008);  LIPN (Bossard et al, 2008);  IIITSum08 
(Varma et al, 2008) -, efficient methods were 
proposed focusing on the retrieval and filtering 
stage, based on polarity ? DLSIUAES (Balahur et 
al., 2008) - or on separating information rich 
clauses - italica (Cruz et al, 2008). In general, 
previous work in opinion mining includes 
document level sentiment classification using 
supervised (Chaovalit and Zhou, 2005) and 
unsupervised methods (Turney, 2002), machine 
learning techniques and sentiment classification 
considering rating scales (Pang, Lee and 
Vaithyanathan, 2002), and scoring of features 
(Dave, Lawrence and Pennock, 2003). Other 
research has been conducted in analysing 
sentiment at a sentence level using bootstrapping 
techniques (Riloff and Wiebe, 2003), finding 
strength of opinions (Wilson, Wiebe and Hwa, 
2004), summing up orientations of opinion words 
in a sentence (Kim and Hovy, 2004), and 
identifying opinion holders (Stoyanov and Cardie, 
2006). Finally, fine grained, feature-based opinion 
summarization is defined in (Hu and Liu, 2004).  
2 Opinion Summarization System 
In order to tackle the OSP task, we considered the 
use of two different methods for opinion mining 
and summarization, differing mainly with respect 
to the use of the optional text snippets provided. 
Our first approach (the Snippet-driven Approach) 
Target : George Clooney 
Question: Why do people like George Clooney? 
Snippet 1: 1050 BLOG06-20060125-015-
0025581509 he is a great actor 
72
used these snippets, whereas the second one (Blog-
driven Approach) found the answers directly in the 
corresponding blogs. A general overview of the 
system?s architecture is shown in Figure 2, where 
three main parts can be distinguished: the question 
processing stage, the snippets processing stage 
(only carried out for the first approach), and the 
final summary generation module. Next, the main 
steps involved in each process will be explained in 
more detail.  
 
Figure 2. System architecture 
 
The first step was to determine the polarity of each 
question, extract the keywords from each of them 
and finally, build some patterns of reformulation. 
The latter were defined in order to give the final 
summary an abstract nature, rather than a simple 
joining of sentences. The polarity of the question 
was determined using a set of created patterns, 
whose goal was to extract for further classification 
the nouns, verbs, adverbs or adjectives indicating 
some kind of polarity (positive or negative). These 
extracted words, together with their determiners, 
were classified using the emotions lists in 
WordNet Affect (Strapparava and Valitutti, 2005), 
jointly with the emotions lists of attitudes, triggers 
resource (Balahur and Montoyo, 2008 [1]), four 
created lists of attitudes, expressing criticism, 
support, admiration and rejection and two 
categories for value (good and bad), taking for the 
opinion mining systems in (Balahur and Montoyo, 
2008 [2]). Moreover, the focus of each question 
was automatically extracted using the Freeling2 
Named Entity Recognizer module. This 
information was used to determine whether or not 
all the questions within the same topic had the 
same focus, as well as be able to decide later on 
which text snippet belonged to which question.  
Regarding the given text snippets, we also 
computed their polarity and their focus. The 
                                                           
2
 http://garraf.epsevg.upc.es/freeling/ 
polarity was calculated as a vector similarity 
between the snippets and vectors constructed from 
the list of sentences contained in the ISEAR corpus 
(Scherer and Wallbot, 1997), WordNet Affect 
emotion lists of anger, sadness, disgust and joy and 
the emotion triggers resource, using Pedersen's 
Text Similarity Package.3  
Concerning the blogs, our opinion mining and 
summarization system is focused only on plain 
text; therefore, as pre processing stage, we 
removed all unnecessary tags and irrelevant 
information, such as links, images etc. Further on, 
we split the remaining text into individual 
sentences. A matching between blogs' sentences 
and text snippets was performed so that a 
preliminary set of potential meaningful sentences 
was recorded for further processing. To achieve 
this, snippets not literally contained in the blogs 
were tokenized and stemmed using Porter's 
Stemmer,4 and stop words were removed in order 
to find the most similar possible sentence 
associated with it. Subsequently, by means of the 
same Pedersen Text Similarity Package as for 
computing the snippets' polarity, we computed the 
similarity between the given snippets and this 
created set of potential sentences. We extracted the 
complete blog sentences to which each snippet was 
related. Further on, we extracted the focus for each 
blog phrase sentence as well. Then, we filtered 
redundant sentences using a na?ve similarity based 
approach. Once we obtained the possible answers, 
we used Minipar5 to filter out incomplete 
sentences.  
Having computed the polarity for the questions and 
snippets, and set out the final set of sentences to 
produce the summary, we bound each sentence to 
its corresponding question, and we grouped all 
sentences which were related to the same question 
together, so that we could generate the language 
for this group, according to the patterns of 
reformulation previously mentioned. Finally, the 
speech style was changed to an impersonal one, in 
order to avoid directly expressed opinion 
sentences. A POS-tagger tool (TreeTagger6) was 
used to identify third person verbs and change 
them to a neutral style. A set of rules to identify 
                                                           
3http://www.d.umn.edu/~tpederse/text-similarity.html 
4http://tartarus.org/~martin/PorterStemmer/ 
5http://www.cs.ualberta.ca/~lindek/minipar.htm 
6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/ 
73
pronouns was created, and they were also changed 
to the more general pronoun ?they? and its 
corresponding forms, to avoid personal opinions.  
3 Evaluation 
Table 1 shows the final results obtained by our 
approaches in the TAC 2008 Opinion Pilot (the 
rank among the 36 participating systems is shown 
in brackets for each evaluation measure). Both of 
our approaches were totally automatic, and the 
only difference between them was the use of the 
given snippets in the first one (A1) and not in the 
second (A2). The column numbers stand for the 
following average scores: summarizerID (1); 
pyramid F-score (Beta=1) (2), grammaticality (3); 
non-redundancy (4); structure/coherence 
(including focus and referential clarity) (5); overall 
fluency/readability (6); overall responsiveness (7). 
 
1 2 3 4 5 6 7 
A1 0.357 
(7) 
4.727 
(8) 
5.364 
(28) 
3.409 
(4) 
3.636 
(16) 
5.045 
(5) 
A2 0.155 
(23) 
3.545 
(36) 
4.364 
(36) 
3.091 
(13) 
2.636 
(36) 
2.227 
(28) 
Table 1. Evaluation results 
 
As it can be noticed from Table 1, our system 
performed well regarding F-measure, the first run 
being classified 7th among the 36 evaluated. As far 
as the structure and coherence are concerned, the 
results were also good, placing the first approach 
in the fourth. Also worth mentioning is the good 
performance obtained regarding the overall 
responsiveness, where A1 ranked 5th. Generally 
speaking, the results for A1 showed well-balanced 
among all the criteria evaluated, except for non 
redundancy and grammaticality.  For the second 
approach, results were not as good, due to the 
difficulty in selecting the appropriate opinion blog 
sentence by only taking into account the keywords 
of the question.  
4 Post-competition tests, experiments 
and improvements 
When an exhaustive examination of the nuggets 
used for evaluating the summaries was done, we 
found some problems that are worth mentioning. 
 
a) Some nuggets with high score did not exist in 
the snippet list (e.g. ?When buying from 
CARMAX, got a better than blue book trade-in 
on old car? (0.9)).  
b) Some nuggets for the same target express the 
same idea, despite their not being identical 
(e.g. ?NAFTA needs to be renegotiated to 
protect Canadian sovereignty? and ?Green 
Party: Renegotiate NAFTA to protect 
Canadian Sovereignty?). 
c) The meaning of one nugget can be deduced 
from another's (e.g. ?reasonably healthy food? 
and ?sandwiches are healthy?). 
d) Some nuggets are not very clear in meaning 
(e.g. ?hot?, ?fun?). 
e) A snippet can be covered by several nuggets 
(e.g. both nuggets ?it is an honest book? and 
?it is a great book? correspond to the same 
snippet ?It was such a great book- honest and 
hard to read (content not language 
difficulty)?). 
 
On the other hand, regarding the use of the 
optional snippets, the main problem to address is to 
remove redundancy, because many of them are 
repeated for the same target, and we have to 
determine which snippet represents better the idea 
for the final summary, in order to avoid noisy 
irrelevant information. 
4.1 Measuring the Performance of a 
Generic Summarization System 
Several participants in the TAC 2008 edition 
performed the OSP task by using generic 
summarization systems. Most were adjusted by 
integrating an opinion classifier module so that the 
task could be fulfilled, but some were not (Bossard 
et al, 2008), (Hendrickx and Bosma, 2008). This 
fact made us realize that a generic summarizer 
could be used to achieve this task. We wanted to 
analyze the effects of such a kind of summarizer to 
produce opinion summaries. We followed the 
approach described in (Lloret et al, 2008). The 
main idea employed is to score sentences of a 
document with regard to the word frequency count 
(WF), which can be combined with a Textual 
Entailment (TE) module.  
Although the first approach suggested for opinion 
summarization obtained much better results in the 
evaluation than the second one (see Section 3.1), 
we decided to run the generic system over both 
approaches, with and without applying TE, to 
74
provide a more extent analysis and conclusions. 
After preprocessing the blogs and having all the 
possible candidate sentences grouped together, we 
considered these as the input for the generic 
summarizer. The goal of these experiments was to 
determine whether the techniques used for a 
generic summarizer would have a positive 
influence in selecting the main relevant 
information to become part of the final summary.  
4.2 Results and Discussion 
We re-evaluated the summaries generated by the 
generic system following the nuggets? list provided 
by the TAC 2008 organization, and counting 
manually the number of nuggets that were covered 
in the summaries. This was a tedious task, but it 
could not be automatically performed because of 
the fact that many of the provided nuggets were 
not found in the original blog collection. After the 
manual matching of nuggets and sentences, we 
computed the average Recall, Precision and F-
measure (Beta =1) in the same way as in the TAC 
2008 was done, according to the number and 
weight of the nuggets that were also covered in the 
summary. Each nugget had a weight ranging from 
0 to 1 reflecting its importance, and it was counted 
only once, even though the information was 
repeated within the summary.  
The average for each value was calculated taking 
into account the results for all the summaries in 
each approach. Unfortunately, we could not 
measure criteria such as readability or coherence as 
they were manually evaluated by human experts.  
Table 2 points out the results for all the approaches 
reported. We have also considered the results 
derived from our participation in the TAC 2008 
conference (OpSum-1 and OpSum-2), in order to 
analyze whether they have been improved or not. 
From these results it can be stated that the TE 
module in conjunction with the WF counts, have 
been very appropriate in selecting the most 
important information of a document. Although it 
can be thought that applying TE can remove some 
meaningful sentences which contained important 
information, results show the opposite. It benefits 
the Precision value, because a shorter summary 
contains greater ratio of relevant information. On 
the other hand, taking into consideration the F-
measure value only, it can be seen that the 
approach combining TE and WF, for the sentences 
in the first approach, has beaten significantly the 
best F-measure result among the participants of 
TAC 2008 (please see Table 3), increasing its 
performance by 20% (with respect to WF only), 
and improving by approximately 80% with respect 
to our first approach submitted to TAC 2008.    
However, a simple generic summarization system 
like the one we have used here is not enough to 
produce opinion oriented summaries, since 
semantic coherence given by the grouping of 
positive and negative opinions is not taken into 
account. Therefore, the opinion classification stage 
must be added in the same manner as used in the 
competition. 
 
SYSTEM RECALL PRECISION F-MEASURE 
OpSum-1 0.592 0.272 0.357 
OpSum-2 0.251 0.141 0.155 
WF-1 0.705 0.392 0.486 
TE+WF -1  0.684 0.630  0.639 
WF -2 0.322 0.234  0.241 
TE+WF-2 0.292 0.282 0.262 
Table 2. Comparison of the results 
4.3 Improving the quality of summaries 
In the evaluation performed by the TAC 
organization, a manual quality evaluation was also 
carried out. In this evaluation the important aspects 
were grammaticality, non-redundancy, structure 
and coherence, readability, and overall 
responsiveness. Although our participating systems 
obtained good F-measure values, in other scores, 
especially in grammaticality and non-redundancy, 
the results achieved were very low. Focusing all 
our efforts in improving the first approach, 
OpSum-1, non-redundancy and grammaticality 
verification had to be performed. In this approach, 
we wanted to test how much of the redundant 
information would be possible to remove by using 
a Textual Entailment system similar to (Iftene and 
Balahur-Dobrescu, 2007), without it affecting the 
quality of the remaining data. As input for the TE 
system, we considered the snippets retrieved from 
the original blog posts. We applied the entailment 
verification on each of the possible pairs, taking in 
turn all snippets as Text and Hypothesis with all 
other snippets as Hypothesis and Text, 
respectively. Thus, as output, we obtained the list 
of snippets from which we eliminated those that 
75
are entailed by any of the other snippets. We 
further eliminated those snippets which had a high 
entailment score with any of the remaining 
snippets. 
 
SYSTEM F-MEASURE 
Best system  0.534 
Second best system 0.490 
OpSum-1 + TE  0.530 
OpSum-1 0.357 
Table 3. F-measure results after improving the system 
 
Table 3 shows that applying TE before generating 
the final summary leads to very good results 
increasing the F-measure by 48.50% with respect 
to the original first approach. Moreover, it can be 
seen form Table 3 that our improved approach 
would have ranked in the second place among all 
the participants, regarding F-measure. The main 
problem with this approach is the long processing 
time. We can apply Textual Entailment in the 
manner described within the generic 
summarization system presented, successively 
testing the relation as Snippet1 entails Snippet2?, 
Snippet1+Snippet2 entails Snippet3? and so on. 
The problem then becomes the fact that this 
approach is random, since different snippets come 
from different sources, so there is no order among 
them. Further on, we have seen that many 
problems arise from the fact that extracting 
information from blogs introduces a lot of noise. In 
many cases, we had examples such as: 
At 4:00 PM John said Starbucks coffee tastes great 
John said Starbucks coffee tastes great, always get one 
when reading New York Times. 
To the final summary, the important information 
that should be added is ?Starbucks coffee tastes 
great?. Our TE system contains a rule specifying 
that the existence or not of a Named Entity in the 
hypothesis and its not being mentioned in the text 
leads to the decision of ?NO? entailment. For the 
example given, both snippets are maintained, 
although they contain the same data.  
Another issue to be addressed is the extra 
information contained in final summaries that is 
not scored as nugget. As we have seen from our 
data, much of this information is also valid and 
correctly answers the questions. Therefore, what 
methods can be employed to give more weight to 
some and penalize others automatically?  
Regarding the grammaticality criteria, once we had 
a summary generated we used the module 
Language Tool7 as a post-processing step. The 
errors that we needed correcting included the 
number matching between nouns and determiners 
as well as among subject and predicate, upper case 
for sentence start, repeated words or punctuation 
marks and lack of punctuation marks. The rules 
present in the module and that we ?switched off?, 
due to the fact that they produced more errors, 
were those concerning the limit in the number of 
consecutive nouns and the need for an article 
before a noun (since it always seemed to want to 
correct ?Vista? for ?the Vista? a.o.). We evaluated 
by observing the mistakes that the texts contained, 
and counting the number of remaining or 
introduced errors in the output. The results 
obtained can be seen in Table 4. 
 
Problem Rightly corrected 
 
Wrongly 
corrected 
Match S-P 90% 10% 
Noun-det 75% 25% 
Upper case 80% 20% 
Repeated words 100% 0% 
Repeated ?.? 80% 20% 
Spelling mistakes 60% 40% 
Unpaired ??/() 100% 0% 
Table 4. Grammaticality analysis 
 
The greatest problem encountered was the fact that 
bigrams are not detected and agreement is not 
made in cases in which the noun does not appear 
exactly after the determiner. All in all, using this 
module, the grammaticality of our texts was 
greatly improved. 
5 Conclusions and future work 
The Opinion Pilot in the TAC 2008 competition 
was a difficult task, involving the development of 
systems including components for QA, IR, polarity 
classification and summarization. Our contribution 
presented in this paper resides in proposing an 
opinion mining and summarization method using 
different approaches and resources, evaluating 
each of them in turn. We have shown that using a 
generic summarization system, we obtain 80% 
improvement over the results obtained in the 
competition, with coherence being maintained by 
using the same polarity classification mechanisms. 
                                                           
7http://community.languagetool.org/ 
76
Using redundancy removal with TE, as opposed to 
our initial polarity strength based sentence filtering 
improved the system performance by almost 50%.    
Finally, we showed that grammaticality can be 
checked and improved using an independent 
solution given by Language Tool.  
Further work includes the improvement of the 
polarity classification component by using 
machine learning over annotated corpora and other 
techniques, such as anaphora resolution. As we 
could see, the well functioning of this component 
ensures logic, structure and coherence to the 
produced summaries. Moreover, we plan to study 
the manner in which opinion sentences of 
blogs/bloggers can be coherently combined. 
References  
Balahur, A., Lloret, E., Ferr?ndez, ?., Montoyo, A., 
Palomar, M., Mu?oz, R., The DLSIUAES Team?s 
Participation in the TAC 2008 Tracks. In 
Proceedings of the Text Analysis Conference (TAC), 
2008. 
Balahur, A. and Montoyo, A. [1]. An Incremental 
Multilingual Approach to Forming a Culture 
Dependent Emotion Triggers Database. In 
Proceedings of the 8th International Conference on 
Terminology and Knowledge Engineering, 2008. 
Balahur, A. and Montoyo, A. [2]. Multilingual Feature--
driven Opinion Mining and Summarization from 
Customer Reviews. In Lecture Notes in Computer 
Science 5039, pg. 345-346. 
Bossard, A., G?n?reux, M. and  Poibeau, T.. Description 
of the LIPN systems at TAC 2008: Summarizing 
information and opinions. In Proceedings of the Text 
Analysis Conference (TAC), 2008. 
Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a 
Comparison between Supervised and Unsupervised 
Classification Approaches. In Proceedings of HICSS-
05, the 38th Hawaii International Conference on 
System Sciences. 
Cruz, F., Troyani, J.A., Ortega, J., Enr?quez, F. The 
Italica System at TAC 2008 Opinion Summarization 
Task. In Proceedings of the Text Analysis 
Conference (TAC), 2008. 
Cui, H., Mittal, V., Datar, M. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of the 21st National 
Conference on Artificial Intelligence AAAI 2006.  
Dave, K., Lawrence, S., Pennock, D. 2003. Mining the 
Peanut Gallery: Opinion Extraction and Semantic 
Classification of Product Reviews. In Proceedings of 
WWW-03.  
Lloret, E., Ferr??ndez, O., Mu?oz, R. and Palomar, M. A 
Text Summarization Approach under the Influence of 
Textual Entailment. In Proceedings of the 5th 
International Workshop on Natural Language 
Processing and Cognitive Science (NLPCS 2008), 
pages 22?31, 2008. 
Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E. 
2005. Mining Customer Opinions from Free Text. 
Lecture Notes in Computer Science. 
He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008: 
Proceeding on Using Semantic Method for 
Automated Summarization Yield. In Proceedings of 
the Text Analysis Conference (TAC), 2008. 
Hendrickx, I. and Bosma, W.. Using coreference links 
and sentence compression in graph-based 
summarization. In Proceedings of the Text Analysis 
Conference (TAC), 2008.     
Hu, M., Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of 19th National 
Conference on Artificial Intelligence AAAI. 
Iftene, A., Balahur-Dobrescu, A. Hypothesis 
Transformation and Semantic Variability Rules for 
Recognizing Textual Entailment. In Proceedings of 
the ACL 2007 Workshop on Textual Entailment and 
Paraphrasis, 2007. 
Kim, S.M., Hovy, E. 2004. Determining the Sentiment 
of Opinions. In Proceedings of COLING 2004. 
Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP-02, the 
Conference on Empirical Methods in Natural 
Language Processing. 
Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns 
for Subjective Expressions. In Proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing.  
Scherer, K. and Wallbott, H.G. The ISEAR 
Questionnaire and Codebook, 1997.  
Stoyanov, V., Cardie, C. 2006. Toward Opinion 
Summarization: Linking the Sources. In: COLING-
ACL 2006 Workshop on Sentiment and Subjectivity 
in Text. 
Strapparava, C. and Valitutti, A. "WordNet-Affect: an 
affective extension of WordNet". In Proceedings 
ofthe 4th International Conference on Language 
Resources and Evaluation, 2004, pp. 1083-1086.  
Turney, P., 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceedings of the 40th 
Annual Meeting of the ACL 
Varma, V., Pingali, P., Katragadda, R., Krisha, S., 
Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty, 
H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT 
Hyderabad at TAC 2008. In Proceedings of the Text 
Analysis Conference (TAC), 2008.  
Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are 
you? Finding strong and weak opinion clauses. In: 
Proceedings of AAAI 2004. 
77
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157?160,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Opinion and Generic Question Answering Systems: a Performance 
Analysis 
 
 
Alexandra Balahur 1,2 
1DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
2IPSC, EC Joint Research Centre 
Via E. Fermi, 21027, Ispra 
abalahur@dlsi.ua.es 
Ester Boldrini 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
patricio@dlsi.ua.es 
 
Abstract 
The importance of the new textual genres such 
as blogs or forum entries is growing in parallel 
with the evolution of the Social Web. This pa-
per presents two corpora of blog posts in Eng-
lish and in Spanish, annotated according to the 
EmotiBlog annotation scheme. Furthermore, 
we created 20 factual and opinionated ques-
tions for each language and also the Gold 
Standard for their answers in the corpus. The 
purpose of our work is to study the challenges 
involved in a mixed fact and opinion question 
answering setting by comparing the perform-
ance of two Question Answering (QA) sys-
tems as far as mixed opinion and factual set-
ting is concerned. The first one is open do-
main, while the second one is opinion-
oriented. We evaluate separately the two sys-
tems in both languages and propose possible 
solutions to improve QA systems that have to 
process mixed questions. 
Introduction and motivation 
In the last few years, the number of blogs has 
grown exponentially. Thus, the Web contains 
more and more subjective texts. A research from 
the Pew Institute shows that 75.000 blogs are 
created daily (Pang and Lee, 2008). They ap-
proach a great variety of topics (computer sci-
ence, sociology, political science or economics) 
and are written by different types of people, thus 
are a relevant resource for large community be-
havior analysis. Due to the high volume of data 
contained in blogs, new Natural Language Proc-
essing (NLP) resources, tools and methods are 
needed in order to manage their language under-
standing. Our fist contribution consists in carry-
ing out a multilingual research, for English and 
Spanish. Secondly, many sources are present in 
blogs, as people introduce quotes from newspa-
per articles or other information to support their 
arguments and make references to previous posts 
in the discussion thread. Thus, when performing 
a task such as Question Answering (QA), many 
new aspects have to be taken into consideration. 
Previous studies in the field (Stoyanov, Cardie 
and Wiebe, 2005) showed that certain types of 
queries, which are factual in nature, require the 
use of Opinion Mining (OM) resources and tech-
niques to retrieve the correct answers. A further 
contribution this paper brings is the analysis and 
definition of the criteria for the discrimination 
among types of factual versus opinionated ques-
tions. Previous researchers mainly concentrated 
on newspaper collections. We formulated and 
annotated of a set of questions and answers over 
a multilingual blog collection. A further contri-
bution is the evaluation and comparison of two 
different approaches to QA a fact-oriented one 
and another designed for opinion QA scenarios.  
Related work 
Research in building factoid QA systems has a 
long history. However, it is only recently that 
studies have started to focus also on the creation 
and development of QA systems for opinions. 
Recent years have seen the growth of interest in 
this field, both by the research performed and the 
publishing of various studies on the requirements 
157
and peculiarities of opinion QA systems (Stoy-
anov, Cardie and Wiebe, 2005), (Pustejovsky 
and Wiebe, 2006), as well as the organization of 
international conferences that promote the crea-
tion of effective QA systems both for general and 
subjective texts, as, for example, the Text Analy-
sis Conference (TAC)1. Last year?s TAC 2008 
Opinion QA track proposed a mixed setting of 
factoid (?rigid list?) and opinion questions 
(?squishy list?), to which the traditional systems 
had to be adapted. The Alyssa system (Shen et 
al., 2007), classified the polarity of the question 
and of the extracted answer snippet, using a Sup-
port Vector Machines classifier trained on the 
MPQA corpus (Wiebe, Wilson and Cardie, 
2005), English NTCIR2 data and rules based on 
the subjectivity lexicon (Wilson, Wiebe and 
Hoffman, 2005). The PolyU (Wenjie et al, 
2008) system determines the sentiment orienta-
tion with two estimated language models for the 
positive versus negative categories. The 
QUANTA (Li, 2008) system detects the opinion 
holder, the object and the polarity of the opinion 
using a semantic labeler based on PropBank3 and 
some manually defined patterns.  
Evaluation 
In order to carry out our evaluation, we em-
ployed a corpus of blog posts presented in 
(Boldrini et al, 2009). It is a collection of blog 
entries in English, Spanish and Italian. However, 
for this research we used the first two languages. 
We annotated it using EmotiBlog (Balahur et al, 
2009) and we also created a list of 20 questions 
for each language. Finally, we produced the Gold 
Standard, by labeling the corpus with the correct 
answers corresponding to the questions. 
1.1 Questions 
No TYPE QUESTION 
 
1 
 
F 
 
F 
What international organization do people criticize for 
its policy on carbon emissions? 
?Cu?l fue uno de los primeros pa?ses que se preocup? 
por el problema medioambiental? 
 
 
2 
 
 
O 
 
 
F 
What motivates people?s negative opinions on the 
Kyoto Protocol? 
?Cu?l es el pa?s con mayor responsabilidad de la 
contaminaci?n mundial seg?n la opini?n p?blica? 
 
 
3 
 
 
F 
 
 
F 
What country do people praise for not signing the 
Kyoto Protocol? 
?Qui?n piensa que la reducci?n de la contaminaci?n se 
deber?a apoyar en los consejos de los cient?ficos? 
 
 
4 
 
 
F 
 
 
F 
What is the nation that brings most criticism to the 
Kyoto Protocol? 
?Qu? administraci?n act?a totalmente en contra de la 
lucha contra el cambio clim?tico? 
                                                 
1 http://www.nist.gov/tac/ 
2 http://research.nii.ac.jp/ntcir/ 
3 http://verbs.colorado.edu/~mpalmer/projects/ace.html 
 
 
5 
 
 
O 
 
 
F 
What are the reasons for the success of the Kyoto 
Protocol? 
?Qu? personaje importante est? a favor de la 
colaboraci?n del estado en la lucha contra el 
calentamiento global? 
 
 
6 
 
 
O 
 
 
F 
What arguments do people bring for their criticism of 
media as far as the Kyoto Protocol is concerned? 
?A qu? pol?ticos americanos culpa la gente por la 
grave situaci?n en la que se encuentra el planeta? 
 
7 
 
O 
 
F 
Why do people criticize Richard Branson? 
?A qui?n reprocha la gente el fracaso del Protocolo de 
Kyoto? 
 
8 
 
F 
 
F 
What president is criticized worldwide for his reaction 
to the Kyoto Protocol? 
?Qui?n acusa a China por provocar el mayor da?o al 
medio ambiente? 
 
9 
 
F 
 
O 
What American politician is thought to have developed 
bad environmental policies? 
?C?mo ven los expertos el futuro? 
 
10 
 
F 
 
O 
What American politician has a positive opinion on the 
Kyoto protocol? 
C?mo se considera el atentado del 11 de septiembre? 
 
11 
 
O 
 
O 
What negative opinions do people have on Hilary 
Benn? 
?Cu?l es la opini?n sobre EEUU? 
 
12 
 
O 
 
O 
Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol and other environmental issues? 
?De d?nde viene la riqueza de EEUU? 
 
13 
 
F 
 
O 
What country disregards the importance of the Kyoto 
Protocol? 
?Por qu? la guerra es negativa? 
 
14 
 
F 
 
O 
What country is thought to have rejected the Kyoto 
Protocol due to corruption? 
?Por qu? Bush se retir? del Protocolo de Kyoto? 
 
15 
 
F/
O 
 
O 
What alternative environmental friendly resources do 
people suggest to use instead of gas en the future? 
?Cu?l fue la posici?n de EEUU sobre el Protocolo de 
Kyoto? 
 
16 
 
F/
O 
 
O 
 Is Arnold Schwarzenegger pro or against the reduction 
of CO2 emissions? 
?Qu? piensa Bush sobre el cambio clim?tico? 
 
17 
 
F 
 
O 
What American politician supports the reduction of 
CO2 emissions? 
?Qu? impresi?n da Bush? 
 
18 
 
F/
O 
 
O 
What improvements are proposed to the Kyoto Proto-
col? 
?Qu? piensa China del calentamiento global? 
 
19 
 
F/
O 
 
O 
What is Bush accused of as far as political measures 
are concerned? 
?Cu?l es la opini?n de Rusia sobre el Protocolo de 
Kyoto? 
 
20 
 
F/
O 
 
O 
What initiative of an international body is thought to be 
a good continuation for the Kyoto Protocol? 
?Qu? cree que es necesario hacer Yvo Boer? 
 
Table 1: List of question in English and Spanish 
 
As it can be seen in the table above, we created 
factoid (F) and opinion (O) queries for English 
and for Spanish; however, there are some that 
could be defined between factoid and opinion 
(F/O) and the system can retrieve multiple an-
swers after having selected, for example, the po-
larity of the sentences in the corpus. 
1.2 Performance of the two systems 
We evaluated and compared the generic QA sys-
tem of the University of Alicante (Moreda et al, 
2008) and the opinion QA system presented in 
(Balahur et al, 2008), in which Named Entity 
Recognition with LingPipe4 and FreeLing5 was 
                                                 
4 http://alias-i.com/lingpipe/ 
5 http://garraf.epsevg.upc.es/freeling/ 
158
added, in order to boost the scores of answers 
containing NEs of the question Expected Answer 
Type (EAT). Table 2 presents the results ob-
tained for English and Table 3 for Spanish. We 
indicate the id of the question (Q), the question 
type (T) and the number of answer of the Gold 
Standard (A). We present the number of the re-
trieved questions by the traditional system 
(TQA) and by the opinion one (OQA). We take 
into account the first 1, 5, 10 and 50 answers. 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
   TQA OQA TQA OQA TQA OQA TQA OQA 
1 F 5 0 0 0 2 0 3 4 4 
2 O 5 0 0 0 1 0 1 0 3 
3 F 2 1 1 2 1 2 1 2 1 
4 F 10 1 1 2 1 6 2 10 4  
5 O 11 0 0 0 0 0 0 0 0 
6 O 2 0 0 0 0 0 1 0 2 
7 O 5 0 0 0 0 0 1 0 3 
8 F 5 1 0 3 1 3 1 5 1 
9 F 5 0 1 0 2 0 2 1 3 
10 F 2 1 0 1 0 1 1 2 1 
11 O 2 0 1 0 1 0 1 0 1 
12 O 3 0 0 0 1 0 1 0 1 
13 F 1 0 0 0 0 0 0 0 1 
14 F 7 1 0 1 1 1 2 1 2 
15 F/O 1 0 0 0 0 0 1 0 1 
16 F/O 6 0 1 0 4 0 4 0 4 
17 F 10 0 1 0 1 4 1 0 2 
18 F/O 1 0 0 0 0 0 0 0 0 
19 F/O 27 0 1 0 5 0 6 0 18 
20 F/O 4 0 0 0 0 0 0 0 0 
 
Table 2: Results for English 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
    TQA  OQA  TQA  OQA  TQA  OQA  TQA  OQA 
1 F 9 1 0 0 1 1 1 1 3 
2 F 13 0 1 2 3 0 6 11 7 
3 F 2 0 1 0 2 0 2 2 2 
4 F 1 0 0 0 0 0 0 1 0 
5 F 3 0 0 0 0 0 0 1 0 
6 F 2 0 0 0 1 0 1 2 1 
7 F 4 0 0 0 0 1 0 4 0 
8 F 1 0 0 0 0 0 0 1 0 
9 O 5 0 1 0 2 0 2 0 4 
10 O 2 0 0 0 0 0 0 0 0 
11 O 5 0 0 0 1 0 2 0 3 
12 O 2 0 0 0 1 0 1 0 1 
13 O 8 0 1 0 2 0 2 0 4 
14 O 25 0 1 0 2 0 4 0 8 
15 O 36 0 1 0 2 0 6 0 15 
16 O 23 0 0 0 0 0 0 0 0 
17 O 50 0 1 0 5 0 6 0 10 
18 O 10 0 1 0 1 0 2 0 2 
19 O 4 0 1 0 1 0 1 0 1 
20 O 4 0 1 0 1 0 1 0 1 
 
Table 3: Results for Spanish 
1.3 Results and discussion 
There are many problems involved when trying 
to perform mixed fact and opinion QA. The first 
can be the ambiguity of the questions e.g. ?De 
d?nde viene la riqueza de EEUU?. The answer 
can be explicitly stated in one of the blog sen-
tences, or a system might have to infer them 
from assumptions made by the bloggers and their 
comments. Moreover, most of the opinion ques-
tions have longer answers, not just a phrase snip-
pet, but up to 2 or 3 sentences. As we can ob-
serve in Table 2, the questions for which the 
TQA system performed better were the pure fac-
tual ones (1, 3, 4, 8, 10 and 14), although in some 
cases (question number 14) the OQA system re-
trieved more correct answers.  At the same time, 
opinion queries, although revolving around NEs, 
were not answered by the traditional QA system, 
but were satisfactorily answered by the opinion 
QA system (2, 5, 6, 7, 11, 12). Questions 18 and 
20 were not correctly answered by any of the two 
systems. We believe the reason is that question 
18 was ambiguous as far as polarity of the opin-
ions expressed in the answer snippets (?im-
provement? does not translate to either ?positive? 
or ?negative?) and question 20 referred to the 
title of a project proposal that was not annotated 
by any of the tools used. Thus, as part of the fu-
ture work in our OQA system, we must add a 
component for the identification of quotes and 
titles, as well as explore a wider range of polar-
ity/opinion scales. Furthermore, questions 15, 16, 
18, 19 and 20 contain both factual as well as 
opinion aspects and the OQA system performed 
better than the TQA, although in some cases, 
answers were lost due to the artificial boosting of 
the queries containing NEs of the EAT (Ex-
pected Answer Type). Therefore, it is obvious 
that an extra method for answer ranking should 
be used, as Answer Validation techniques using 
Textual Entailment. In Table 3, the OQA missed 
some of the answers due to erroneous sentence 
splitting, either separating text into two sentences 
where it was not the case or concatenating two 
consecutive sentences; thus missing out on one 
of two consecutively annotated answers. Exam-
ples are questions number 16 and 17, where 
many blog entries enumerated the different ar-
guments in consecutive sentences. Another 
source of problems was the fact that we gave a 
high weight to the presence of the NE of the 
sought type within the retrieved snippet and in 
some cases the name was misspelled in the blog 
entries, whereas in other NER performed by 
159
FreeLing either attributed the wrong category to 
an entity, failed to annotate it or wrongfully an-
notated words as being NEs.  Not of less impor-
tance is the question duality aspect in question 
17. Bush is commented in more than 600 sen-
tences; therefore, when polarity is not specified, 
it is difficult to correctly rank the answers. Fi-
nally, also the problems of temporal expressions 
and the coreference need to be taken into ac-
count.  
Conclusions and future work 
In this article, we created a collection of both 
factual and opinion queries in Spanish and Eng-
lish. We labeled the Gold Standard of the an-
swers in the corpora and subsequently we em-
ployed two QA systems, one open domain, one 
for opinion questions. Our main objective was to 
compare the performances of these two systems 
and analyze their errors, proposing solutions to 
creating an effective QA system for both factoid 
an opinionated queries. We saw that, even using 
specialized resources, the task of QA is still chal-
lenging. Opinion QA can benefit from a snippet 
retrieval at a paragraph level, since in many 
cases the answers were not simple parts of sen-
tences, but consisted in two or more consecutive 
sentences. On the other hand, we have seen cases 
in which each of three different consecutive sen-
tences was a separate answer to a question. Our 
future work contemplates the study of the impact 
anaphora resolution and temporality on opinion 
QA, as well as the possibility to use Answer 
Validation techniques for answer re-ranking. 
 
Acknowledgments 
 
The authors would like to thank Paloma Moreda, 
Hector Llorens, Estela Saquete and Manuel 
Palomar for evaluating the questions on their QA 
system. This research has been partially funded 
by the Spanish Government under the project 
TEXT-MESS (TIN 2006-15265-C06-01), by the 
European project QALL-ME (FP6 IST 033860) 
and by the University of Alicante, through its 
doctoral scholarship. 
References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo, 
and Patricio Mart?nez-Barco, 2009. Cross-topic 
Opinion Mining for Real-time Human-Computer 
Interaction. In Proceedings of the 6th Workshop in 
Natural Language Processing and Cognitive Sci-
ence, ICEIS 2009 Conference, Milan, Italy. 
Alexandra Balahur, Elena Lloret, Oscar Ferrandez, 
Andr?s Montoyo, Manuel Palomar, Rafael Mu?oz. 
2008. The DLSIUAES Team?s Participation in the 
TAC 2008 Tracks. In Proceedings of the Text 
Analysis Conference (TAC 2008). 
Ester Boldrini, Alexandra Balahur, Patricio Mart?nez-
Barco, and Andr?s Montoyo. 2009. EmotiBlog: An 
Annotation Scheme for Emotion Detection and 
Analysis in Non-Traditional Textual Genres. To 
appear in Proceedings of the 5th Conference on 
data Mining. Las Vegas, Nevada, USA. 
W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC 
2008. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu, 
Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie 
Huang. THU QUANTA at TAC 2008 QA and RTE 
track. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Bo Pang, and Lilian. Lee, Opinion mining and senti-
ment analysis. Foundations and Trends R. In In-
formation Retrieval Vol. 2, Nos. 1?2 (2008) 1?135, 
2008. 
James Pustejovsky and Janyce. Wiebe. Introduction 
to Special Issue on Advances in Question Answer-
ing. In Language Resources and Evaluation (2005) 
39: 119?122. Springer, 2006. 
Dan Shen, Jochen L. Leidner, Andreas Merkel, Diet-
rich Klakow. The Alyssa system at TREC QA 2007: 
Do we need Blog06? In Proceedings of The Six-
teenth Text Retrieval Conference (TREC 2007), 
Gaithersburg, MD, USA, 2007 
Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe. 
Multi-Perspective Question Answering Using the 
OpQA Corpus. In Proceedings of HLT/EMNLP. 
2005. 
Paloma Moreda, Hector Llorens, Estela Saquete, 
Manuel Palomar. 2008. Automatic Generalization 
of a QA Answer Extraction Module Based on Se-
mantic Roles. In: AAI - IBERAMIA, Lisbon, Portu-
gal, pages 233-242, Springer. 
Janyce. Wiebe, Theresa Wilson, and Claire Cardie 
Annotating expressions of opinions and emotions 
in language. Language Resources and Evaluation, 
volume 39, issue 2-3, pp. 165-210, 2005. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
Recognising Contextual Polarity in Phrase-level 
sentiment Analysis. In Proceedings of Human lan-
guage Technologies Conference/Conference on 
Empirical methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2005. 
160
The University of Alicante systems at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpt. of Software and Computing Systems
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Iulia Nica and Antonia Mart?? ?
Dpt. of General Linguistics
Universidad de Barcelona, Spain
iulia@clic.fil.ub.es
amarti@ub.edu
Abstract
The DLSI-UA team is currently working on sev-
eral word sense disambiguation approaches, both
supervised and unsupervised. These approaches are
based on different ways to use both annotated and
unannotated data, and several resources generated
from or exploiting WordNet (Miller et al, 1993),
WordNet Domains, EuroWordNet (EWN) and addi-
tional corpora. This paper presents a view of differ-
ent system results for Word Sense Disambiguation
in different tasks of SENSEVAL-3.
1 Introduction
Word Sense Disambiguation (WSD) is an open re-
search field in Natural Language Processing (NLP).
The task of WSD consists in assigning the correct
sense to words in a particular context using an elec-
tronic dictionary as the source of words definitions.
This is a difficult problem that is receiving a great
deal of attention from the research community.
At the Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
SENSEVAL-2, several supervised and unsupervised
systems took part. The more successful systems re-
lay on corpus-based and supervised learning meth-
ods. At SENSEVAL-2 the average level of accu-
racy achieved rounded 70%, which is insufficient
for such other NLP tasks as information retrieval,
machine translation, or question answering.
The DLSI-UA systems were applied to three
SENSEVAL-3 tasks: English all-words, English lex-
ical sample and Spanish Lexical Sample. Our sys-
tems use both corpus-based and knowledge-based
approaches: Maximum Entropy(ME) (Lau et al,
1993; Berger et al, 1996; Ratnaparkhi, 1998) is
a corpus-based and supervised method based on
linguistic features; ME is the core of a bootstrap-
ping algorithm that we call re-training inspired
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
by co-training (Blum and Mitchell, 1998); Rele-
vant Domains (RD) (Montoyo et al, 2003) is a
resource built from WordNet Domains (Magnini
and Cavaglia, 2000) that is used in an unsuper-
vised method that assigns domain and sense la-
bels; Specification Marks(SP) (Montoyo and Palo-
mar, 2000) exploits the relations between synsets
stored in WordNet (Miller et al, 1993) and does not
need any training corpora; Commutative Test (CT)
(Nica et al, 2003), based on the Sense Discrimi-
nators device derived from EWN (Vossen, 1998),
disambiguates nouns inside their syntactic patterns,
with the help of information extracted from raw cor-
pus.
A resume of which methods and how were used
in which SENSEVAL-3 tasks is shown in Table 1.
DLSI-UA Method Combined
Systems Results
ALL-NOSU RD No
LS-ENG-SU Re-t No
LS-ENG-
NOSU
RD No
LS-SPA-SU ME+Re-t No
LS-SPA-NOSU SM + ME Nouns: SM
Verbs and adj.: ME
LS-SPA- Pattern-Nica Nouns: SM
PATTERN + ME Verbs and adj.: ME
Table 1: DLSI-UA Systems at SENSEVAL-3
Most of these methods are relatively new and our
goal when participating at SENSEVAL-3 is to evalu-
ate for the first time such approaches. At the mo-
ment of writing this paper we can conclude that
these are promising contributions in order to im-
prove current WSD systems.
In the following section each method is described
briefly. Then, details of how the SENSEVAL-3 train
and testing data were processed are shown. Next,
the scores obtained by each system are explained.
Finally, some conclusions and future work are pre-
sented.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Methods and Algorithms
In this section we describe the set of methods and
techniques that we used to build the four systems
that had participated in SENSEVAL-3.
2.1 Re-training and Maximum Entropy
In this section, we describe our bootstrapping
method, which we call re-training. Our method
is derived from the co-training method. Our re-
training system is based on two different views of
the data (as is also the case for co-training), de-
fined using several groups of features from those de-
scribed in Figure 1, with several filters that ensure a
high confidence sense labelling.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
? ANPA codes (Spanish only)
? IPTC codes (Spanish only)
Figure 1: Features Used for the Supervised Learn-
ing
These two views consist of two weak ME learn-
ers, based on different sets of linguistic features,
for every possible sense of a target word. We de-
cided to use ME as the core of our bootstrapping
method because it has shown to be competitive in
WSD when compared to other machine learning ap-
proaches (Sua?rez and Palomar, 2002; Ma`rquez et
al., 2003).
The main difference with respect co-training is
that the two views are used in parallel in order to
get a consensus of what label to assign to a particu-
lar context. Additional filters will ultimately deter-
mine which contexts will then be added to the next
training cycle.
Re-training performs several binary partial train-
ings with positive and negative examples for each
sense. These classifications must be merged in a
unique label for such contexts with enough evidence
of being successfully classified. This ?evidence? re-
lies on values of probability assigned by the ME
module to positive and negative labels, and the fact
that the unlabeled example is classified as positive
for a unique sense only. The set of new labeled ex-
amples feeds the training corpora of the next itera-
tion with positive and negative examples. The stop-
ping criteria is a certain number of iterations or the
failure to obtain new examples from the unlabeled
corpus.
2.2 Specification Marks
Specification Marks is an unsupervised WSD
method over nouns. Its context is the group of words
that co-occur with the word to be disambiguated in
the sentence and their relationship to the noun to
be disambiguated. The disambiguation is resolved
with the use of the WordNet lexical knowledge base.
The underlying hypothesis of the method we
present here is that the higher the similarity between
two words, the larger the amount of information
shared by two concepts. In this case, the informa-
tion commonly shared by two concepts is indicated
by the most specific concept that subsumes them
both in the taxonomy.
The input for the WSD module is a group of
nouns W = {w1, w2, ..., wn} in a context. Each
word wi is sought in WordNet, each having an asso-
ciated set of possible senses Si ={Si1, Si2, ..., Sin},
and each sense having a set of concepts in the IS-A
taxonomy (hypernymy/hyponymy relations). First,
the common concept to all the senses of the words
that form the context is gathered. This concept is
marked by the initial specification mark (ISM). If
this initial specification mark does not resolve the
ambiguity of the word, we then descend through
the WordNet hierarchy, from one level to another,
assigning new specification marks. The number of
concepts contained within the subhierarchy is then
counted for each specification mark. The sense that
corresponds to the specification mark with the high-
est number of words is the one chosen as the sense
disambiguated within the given context
We define six heuristics for our system: Heuris-
tic of Hypernym, Heuristic of Definition, Heuristic
of Common Specification Mark, Heuristic of Gloss
Hypernym, Heuristic of Hyponym and Heuristic of
Gloss Hyponym.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula (w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 Pattern-Nica
This is an unsupervised method only for Spanish
nouns exploiting both EuroWordNet and corpus.
In this method we adopt a different approach to
WSD: the occurrence to be disambiguated is con-
sidered not separately, but integrated into a syn-
tactic pattern, and its disambiguation is carried
out in relation to this pattern. A syntactic pat-
tern is a triplet X-R-Y, formed by two lexical con-
tent units X and Y and an eventual relational el-
ement R, which corresponds to a syntactic rela-
tion between X and Y. Examples: [X=canal-noun
R=de-preposition Y=televisio?n-noun], [X=pasaje-
noun R=? Y=ae?reo-adjective]. The strategy is
based on the hypothesis that syntactic patterns in
which an ambiguous occurrence participates have
decisive influence on its meaning. We also assume
that inside a syntactic pattern a word will tend to
have the same sense: the ?quasi one sense per syn-
tactic pattern? hypothesis. The method works as fol-
lows:
Step 1, the identification of the syntactic patterns
of the ambiguous occurrence;
Step 2, the extraction of information related to it:
from corpus and from the sentential context;
Step 3, the application of the WSD algorithm on
the different information previously obtained;
Step 4, the final sense assignment by combining
the partial sense proposals from step 3.
For step 1, we POS-tag the test sentence and ex-
tract the sequences that correspond to previously de-
fined combinations of POS tags. We only kept the
patterns with frequency 5 or superior.
In step 2, we use a search corpus previously POS-
tagged. For every syntactic pattern of the ambigu-
ous occurrence X, we obtain from corpus two sets of
words: the substitutes of X into the pattern (S1) and
the nouns that co-occur with the pattern in any sen-
tence from the corpus (S2), In both cases, we keep
only the element with frequency 5 or superior.
We perform step 3 by means of the heuristics de-
fined by the Commutative Test (CT) algorithm ap-
plied on each set from 2. The algorithm is related
to the Sense Discriminators (SD) lexical device, an
adaptation of the Spanish WordNet, consisting in a
set of sense discriminators for every sense of a given
noun in WordNet. The Commutative Test algorithm
lays on the hypothesis that if an ambiguous occur-
rence can be substituted in a syntactic pattern by a
sense discriminator, then it can have the sense cor-
responding to that sense discriminator.
For step 4, we first obtain a sense assignment in
relation with each syntactic pattern, by intersecting
the sense proposals from the two heuristics corre-
sponding to a pattern; then we choose the most fre-
quent sense between those proposed by the differ-
ent syntactic patterns; finally, if there are more final
proposed senses, we choose the most frequent sense
on the base of sense numbers in WordNet.
The method we propose for nouns requires only a
large corpus, a minimal preprocessing phase (POS-
tagging) and very little grammatical knowledge, so
it can easily be adapted to other languages. Sense
assignment is performed exploiting information ex-
tracted from corpus, thus we make an intensive use
of sense untagged corpora for the disambiguation
process.
3 Tasks Processing
At this point we explain for each task the systems
processing. The results of each system are shown in
Table2:
DLSI-UA Systems Precision Recall
LS-SPA-SU 84% 84%
LS-ENG-SU 82% 32%
ALL-NOSU 34% 28%
LS-ENG-NOSU 32% 20%
LS-SPA-NOSU 62% 62%
LS-SPA-PATTERN 84% 47%
Table 2: Results at SENSEVAL-3
3.1 DLSI-UA-LS-SPA-SU
Our system, based on re-training and maximum en-
tropy methods, processed both sense labelled and
unlabelled Spanish Lexical Sample data in three
consecutive steps:
Step 1, analyzing the train corpus: words which
most frequent sense is under 70% were selected.
For each one of these words, each feature was used
in a 3-fold cross-validation in order to determine the
best set of features for re-training.
Step 2, feeding training corpora: for these se-
lected words, based on the results of the previous
step, each training corpus was enriched with new
examples from the unlabelled data using re-training.
Step 3, classifying the test data: for the selected
words, re-training was used again to obtain a first set
of answers with, a priori, a label with a high level of
confidence; the remaining contexts that re-training
could not classify were processed with the ME sys-
tem using a unique set of features for all words.
The lemmatization and POS information supplied
into the SENSEVAL-3 Spanish data were the infor-
mation used for defining the features of the system.
0ur system obtained an accuracy of 0.84 for the
Spanish lexical sample task. Unfortunately, a shal-
low analysis of the answers revealed that the UA.5
system performed slightly worse than if only the ba-
sic ME system were used1. This fact means that the
new examples extracted from the unlabelled data in-
troduced too much noise into the classifiers. Be-
cause this anomalous behavior was present only on
some words, a complete study of such new exam-
ples must be done. Probably, the number of itera-
tions done by re-training over unlabelled data were
too low and the enrichment of the training corpora
not large enough.
1The ME system, without using re-training, has not com-
peted at SENSEVAL-3: our own scoring of these set of answers
reported an accuracy of 0.856
3.2 DLSI-UA-LS-ENG-SU
In the English Lexical Sample task our system goal
was to prove that the re-training method ensures a
high level of precision.
By means of a 3-fold cross-validation of the train
data, the features were ordered from higher to lower
precision. Based on this information, four execu-
tions of re-training over the test data were done with
different selections of features for the two views of
the method. Each execution feed the learning cor-
pora of the next one with new examples, those that
re-training considered as the most probably correct.
For this system Minipar parser (Lin, 1998)was
used to properly add syntactic information to the
training and testing data.
Almost 40% of the test contexts were la-
belled by our system, obtaining these scores (for
?fine-grained? and ?coarse-grained?, respectively):
0.782/0.828 precision and 0.310/0.329 recall. In our
opinion, such results must be interpreted as very
positive because the re-training method is able to
satisfy a high level of precision if the parameters of
the system are correctly set.
3.3 DLSI-UA-ALL-NOSU and
DLSI-UA-LS-ENG-NOSU
In the English All Words and English Lexical Sam-
ple tasks RD system was performed with informa-
tion obtained from Relevant Domains resource us-
ing for the disambiguation process all the 165 do-
main labels.
For All Words task we used as input information
all nouns, verbs, adjectives and adverbs present in
a 100 words window around the word to be disam-
biguated. So our system obtained a 34% of preci-
sion and a reduced recall around 28%.
For Lexical Sample task we used all nouns, verbs,
adjectives and adverbs present in the context of each
instance obtaining around 32% precision.
We obtained a reduced precision due to we use all
the domains label hierarchy. In some experiments
realized on SENSEVAL-2 data, our system obtained
a more high precision when grouping domains into
the first three levels. Therefore we expect with re-
ducing the number of domains labels, an improve-
ment on precision.
3.4 DLSI-UA-LS-SPA-NOSU
We used a combined system for Spanish Lexical
Sample task, using the SM method for disambiguat-
ing nouns and the ME method for disambiguating
verbs and adjectives. We obtained around 62% pre-
cision and a 62% recall.
3.5 DLSI-UA-LS-SPA-PATTERN
Our goal when participating in this task was to
demonstrate that the applying of syntactic patterns
to WSD maintains high levels of precision.
In this task we used also a combined system for
Spanish Lexical Sample task, using Pattern-Nica
method for disambiguating nouns and ME method
for disambiguating verbs and adjectives. We ob-
tained around 84% precision and a 47% recall.
4 Conclusions
The supervised systems for the English and Span-
ish lexical sample tasks are very competitive. Al-
though the processing of the train and test data was
different for each task, both systems rely on re-
training, a bootstrapping method, that uses a max-
imum entropy-based WSD module.
The results for the English task prove that re-
training is capable of maintaining a high level of
precision. Nevertheless, for the Spanish task, al-
though the scores achieved were excellent, the sys-
tem must be redesigned in order to improve the clas-
sifiers.
The re-training method is a proposal that we are
trying to incorporate into text retrieval and ques-
tion answering systems that could take advantage of
sense disambiguation of a subset of words.
The unsupervised systems presented here does
not appear to be sufficient for a stand-alone WSD
solution. Wether these methods can be combined
with other supervised methods to improve their re-
sults requires further investigation.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92?100,
Madison, Wisconsin, July. ACM Press.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
Andre?s Montoyo and Manuel Palomar. 2000. Word
Sense Disambiguation with Specification Marks
in Unrestricted Texts. In Proceedings of 11th In-
ternational Workshop on Database and Expert
Systems Applications (DEXA 2000), pages 103?
107, Greenwich, London, UK, September. IEEE
Computer Society.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
Iulia Nica, Antonia Mart??, and Andre?s Mon-
toyo. 2003. Colaboracio?n entre informacio?n
paradigma?tica y sintagma?tica en la desam-
biguacio?n sema?ntica automa?tica. XIX Congreso
de la SEPLN 2003.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
Piek Vossen. 1998. EuroWordNet: Building a Mul-
tilingual Database with WordNets for European
Languages. The ELRA Newsletter, 3(1).
The R2D2 Team at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpto. de Lenguajes y Sistemas. Informa?ticos
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Manuel Garc??a, M. Teresa Mart??n ?
M. ?Angel Garc??a and L. Alfonso Uren?a
Dpto. de Informa?tica
Universidad de Jae?n, Spain
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
Davide Buscaldi, Paolo Rosso ?
Antonio Molina, Ferra?n Pla? and Encarna Segarra
Dpto. de Sistemas Informa?ticos y Computacio?n
Univ. Polit. de Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method. . .
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
1 Introduction
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al, 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al, 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al, 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
2 Systems description
In this section the systems that have participated at
SENSEVAL-3 will be described.
2.1 Maximum Entropy
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Schu?tze,
1999). ME probability models have been success-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al, 2001; Montoyo and Sua?rez, 2001;
Sua?rez and Palomar, 2002), and as meta-learner in
(Ilhan et al, 2001).
Our ME-based system has been shown competi-
tive (Ma`rquez et al, 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
Because the ME system needs annotated data
for the training, Semcor (Miller et al, 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al, 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
2.2 UPV-SHMM-AW
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al, 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 LVQ-JA ?EN-ELS
The LVQ-JA ?EN-ELS system (Garc??a-Vega et al,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf ? idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
sim(wk, xi) = wk ? xi| wk | ? | xi | (2)
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector || x? wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
wc(t+ 1) = wc(t) + s ? ?(t) ? [x(t)? wc(t)] (3)
where s = 0, if k 6= c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = ?1, if
they do not (c 6= d). ?(t) is the learning rate, and
0 < ?(t) < 1 is a monotically decreasing func-
tion of time. It is recommended that ?(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and ?(t) continues decreasing
to a given threshold, u, very close to 0.
2.5 CIAOSENSO
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al, 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns? senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ?1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ? 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
3 Tasks Processing
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
3.1 English Lexical Sample Task
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA ?EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
3.2 English All Words Task
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn?t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
4 Conclusions
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
References
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ?Recent Advances in Natural
Language Processing? (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Manuel Garc??a-Vega, Mar??a Teresa Mart??n-
Valdivia, and Luis Alfonso Uren?a. 2003.
Aprendizaje competitivo lvq para la desam-
biguacio?n le?xica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125?132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87?90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings of ARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
Andre?s Montoyo and Armando Sua?rez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131?134, Toulouse, France, July. ACL-SIGLEX.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado Le?xico y Ana?lisis
Sinta?ctico Superficial basado en Modelos Es-
tad??sticos. Tesis doctoral, Departamento de Sis-
temas Informa?ticos y Computacio?n. Universidad
de Polite?cnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273?276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199?
230.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 19?26,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Language Independent Approach for Name Categorization and
Discrimination
Zornitsa Kozareva
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
zkozareva@dlsi.ua.es
Sonia Va?zquez
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
svazquez@dlsi.ua.es
Andre?s Montoyo
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
We present a language independent ap-
proach for fine-grained categorization and
discrimination of names on the basis of text
semantic similarity information. The exper-
iments are conducted for languages from the
Romance (Spanish) and Slavonic (Bulgar-
ian) language groups. Despite the fact that
these languages have specific characteristics
as word-order and grammar, the obtained
results are encouraging and show that our
name entity method is scalable not only to
different categories, but also to different lan-
guages. In an exhaustive experimental eval-
uation, we have demonstrated that our ap-
proach yields better results compared to a
baseline system.
1 Introduction
1.1 Background
Named Entity (NE) recognition concerns the detec-
tion and classification of names into a set of cate-
gories. Presently, most of the successful NE ap-
proaches employ machine learning techniques and
handle simply the person, organization, location and
miscellaneous categories. However, the need of
the current Natural Language Applications impedes
specialized NE extractors which can help for in-
stance an information retrieval system to determine
that a query about ?Jim Henriques guitars? is related
to the person ?Jim Henriques? with the semantic cat-
egory musician, and not ?Jim Henriques? the com-
poser. Such classification can aid the system to rank
or return relevant answers in a more accurate and
appropriate way.
So far, the state-of-art NE recognizers identify
that ?Jim Henriques? is a person, but do not sub-
categorize it. There are numerous drawbacks re-
lated to the fine-grained NE issue. First, the sys-
tems need hand annotated data which are not avail-
able for multiple categories, because their creation is
time-consuming, requires supervision by experts, a
predefined fine-grained hierarchical structure or on-
tology. Second, there is a significant lack of freely
available or developed resources for languages other
than English, and especially for the Eastern Euro-
pean ones.
The World Wide Web is a vast, multilingual
source of unstructured information which we con-
sult daily in our native language to understand what
the weather in our city is or how our favourite soccer
team performed. Therefore, the need of multilingual
and specialized NE extractors remains and we have
to focus on the development of language indepen-
dent approaches.
Together with the specialized NE categorization,
we face the problem of name ambiguity which is
related to queries for different people, locations or
companies that share the same name. For instance,
Cambridge is a city in the United Kingdom, but
also in the United States of America. ACL refers
to ?The Association of Computational Linguistics?,
?The Association of Christian Librarians? or to the
?Automotive Components Limited?. Googling the
name ?Boyan Bonev? returns thousands of docu-
ments where some are related to a member of a robot
vision group in Alicante, a teacher at the School
19
of Biomedical Science, a Bulgarian schoolboy that
participated in computer science competition among
others. So far, we have to open the documents one
by one, skim the text and decide to which ?Boyan
Bonev? the documents are related to. However, if
we resolve the name disambiguation issue, this can
lead to an automatic clustering of web pages talking
about the same individual, location or ogranization.
1.2 Related Work
Previously, (Pedersen et al, 2005) tackled the name
discrimination task by developing a language inde-
pendent approach based on the context in which the
ambiguous name occurred. They construct second
order co-occurrence features according to which the
entities are clustered and associated to different un-
derlying names. The performance of this method
ranges from 51% to 73% depending on the pair of
named entities that have to be disambiguated. Simi-
lar approach was developed by (Bagga and Baldwin,
1998), who created first order context vectors that
represent the instance in which the ambiguous name
occurs. Their approach is evaluated on 35 different
mentions of John Smith, and the f-score is 84%.
For fine-grained person NE categorization, (Fleis-
chman and Hovy, 2002) carried out a supervised
learning for which they deduced features from the
local context in which the entity resides, as well as
semantic information derived from the topic signa-
tures and WordNet. According to their results, to
improve the 70% coverage for person name catego-
rization, more sophisticated features are needed, to-
gether with a more solid data generation procedure.
(Tanev and Magnini, 2006) classified geographic
location and person names into several subclasses.
They use syntactic information and observed how
often a syntactic pattern co-occurs with certain
member of a given class. Their method reaches 65%
accuracy. (Pasca, 2004) presented a lightly super-
vised lexico-syntactic method for named entity cat-
egorization which reaches 76% when evaluated with
unstructured text of Web documents.
(Mann, 2002) populated a fine-grained proper
noun ontology using common noun patterns and fol-
lowing the hierarchy of WordNet. They studied the
influence of the newly generated person ontology in
a Question Answering system. According to the ob-
tained results, the precision of the ontology is high,
but still suffers in coverage. A similar approach for
the population of the CyC Knowledge Base (KB)
was presented in (Shah et al, 2006). They used
information from the Web and other electronically
available text corpora to gather facts about particu-
lar named entities, to validate and finally to add them
to the CyC KB.
In this paper, we present a new text semantic simi-
larity approach for fine-grained person name catego-
rization and discrimination which is similar to those
of (Pedersen et al, 2005) and (Bagga and Baldwin,
1998), but instead of simple word co-occurrences,
we consider the whole text segment and relate the
deduced semantic information of Latent Seman-
tic Analysis (LSA) to trace the text cohesion be-
tween thousands of sentences containing named en-
tities which belong to different fine-grained cate-
gories or individuals. Our method is based on the
word sense discrimination hypothesis of Miller and
Charles (1991) according to which words with sim-
ilar meaning are used in similar context, hence in
our approach we assume that the same person or
the same fine-grained person category appears in the
similar context.
2 NE categorization and discrimination
with Latent Semantic Analysis
LSA has been applied successfully in many areas
of Natural Language Processing such as Informa-
tion Retrieval (Deerwester et al, 1990), Informa-
tion Filtering (Dumais, 1995) , Word Sense Disam-
biguation (Shu?tze, 1998) among others. This is pos-
sible because LSA is a fully automatic mathemati-
cal/statistical technique for extracting and inferring
relations of expected contextual usage of words in
discourse. It uses no humanly constructed dictionar-
ies or knowledge bases, semantic networks, syntac-
tic or morphological analyzers, because it takes only
as input raw text which is parsed into words and is
separated into meaningful passages. On the basis of
this information, LSA extracts a list of semantically
related word pairs or rank documents related to the
same topic.
LSA represents explicitly terms and documents
in a rich, highly dimensional space, allowing the
underlying ?latent?, semantic relationships between
terms and documents to be exploited. LSA relies
20
on the constituent terms of a document to suggest
the document?s semantic content. However, the LSA
model views the terms in a document as somewhat
unreliable indicators of the concepts contained in the
document. It assumes that the variability of word
choice partially obscures the semantic structure of
the document. By reducing the original dimen-
sionality of the term-document space with Singular
Value Decomposition to a matrix of 300 columns,
the underlying, semantic relationships between doc-
uments are revealed, and much of the ?noise? (dif-
ferences in word usage, terms that do not help distin-
guish documents, etc.) is eliminated. LSA statisti-
cally analyzes the patterns of word usage across the
entire document collection, placing documents with
similar word usage patterns near to each other in the
term-document space, and allowing semantically-
related documents to be closer even though they may
not share terms.
Taking into consideration these properties of
LSA, we thought that instead of constructing the
traditional term-document matrix, we can construct
a term-sentence matrix with which we can find a
set of sentences that are semantically related and
talk about the same person. The rows of the term-
sentence matrix correspond to the words of the sen-
tence where the NE has to be categorized or discrim-
inated (we call this sentence target sentence), while
the columns correspond to the rest of the sentences
with NEs. The cells of the matrix show the num-
ber of times a given word from the target sentence
co-occurs in the rest of the sentences. When two
columns of the term-sentence matrix are similar, this
means that the two sentences contain similar words
and are therefore likely to be semantically related.
When two rows are similar, then the corresponding
words occur in most of the same sentences and are
likely to be semantically related.
In this way, we can obtain semantic evidence
about the words which characterize a given person.
For instance, a football player is related to words
as ball, match, soccer, goal, and is seen in phrases
such as ?X scores a goal?, ?Y is penalized?. Mean-
while, a surgeon is related to words as hospital, pa-
tient, operation, surgery and is seen in phrases such
as ?X operates Y?, ?X transplants?. Evidently, the
category football player can be distinguished easily
from that of the surgeon, because both person names
occur and relate semantically to different words.
Another advantage of LSA is its property of lan-
guage independence, and the ability to link sev-
eral flexions or declanations of the same term.
This is especially useful for the balto-slavonic lan-
guages which have rich morphology. Once the term-
sentence approach is developed, practically there is
no restrain for LSA to be applied and extended to
other languages. As our research focuses not only
on the resolution of the NE categorization and dis-
crimination problems as a whole, but also on the lan-
guage independence issue, we considered the LSA?s
usage are very appropriate.
3 Development Data Set
For the development of our name discrimination and
classification approach, we used the Spanish lan-
guage. The corpora we worked with is the EFE94-95
Spanish news corpora, which were previously used
in the CLEF competitions1. In order to identify the
named entities in the corpora, we used a machine
learning based named entity recognizer (Kozareva et
al., 2007).
For the NE categorization and discrimination ex-
periments, we used six different named entities, for
which we assumed a-priory to belong to one of the
two fine-grained NE categories PERSON SINGER
and PERSON PRESIDENT. The president names
are Bill Clinton, George Bush and Fidel Castro, and
the singer names are Madonna, Julio Iglesias and
Enrique Iglesias. We have selected these names for
our experiment, because of their high frequency in
the corpora and low level of ambiguity.
Once we have selected the names, we have col-
lected a context of 10, 25, 50 and 100 words from
the left and from the right of the NEs. This is done
in order to study the influence of the context for the
NE discrimination and categorization tasks, and es-
pecially how the context window affects LSA?s per-
formance. We should note that the context for the
NEs is obtained from the text situated between the
text tags. During the creation of the context win-
dow, we used only the words that belong to the docu-
ment in which the NE is detected. This restriction is
imposed, because if we use words from previous or
following documents, this can influence and change
1http://www.clef-campaign.org/
21
the domain and the topic in which the NE is seen.
Therefore, NE examples for which the number of
context words does not correspond to 10, 25, 50 or
100 are directly discarded.
From the compiled data, we have randomly se-
lected different NE examples and we have created
two data sets: one with 100 and another with 200
examples per NE. In the fine-grained classification,
we have substituted the occurrence of the presi-
dent and singer names with the obfuscated form
President Singer. While for the NE discrim-
ination task, we have replaced the names with the
M EI JI BC GB FC label. The first label indicates
that a given sentence can belong to the president or
to the singer category, while the second label indi-
cates that behind it can stand one of the six named
entities. The NE categorization and discrimination
experiments are carried out in a completely unsuper-
vised way, meaning that we did not use the correct
name and name category until evaluation.
4 Experimental Evaluation
4.1 Experimental Settings
As mentioned in Section 2, to establish the semantic
similarity relation between a sentence with an ob-
fuscated name and the rest of the sentences, we use
LSA2. The output of LSA is a list of sentences that
best matches the target sentence (e.g. the sentence
with the name that has to be classified or discrim-
inated) ordered by their semantic similarity score.
Strongly similar sentences have values close to 1,
and dissimilar sentences have values close to 0.
In order to group the most semantically similar
sentences which we expect to refer to the same per-
son or the same fine-grained category, we apply the
graph-based clustering algorithm PoBOC (Cleuziou
et al, 2004). We construct a new quadratic sentence-
sentence similarity matrix where the rows stand for
the sentence we want to classify, the columns stand
for the sentences in the whole corpus and the values
of the cells represent the semantic similarity scores
derived from LSA.
On the basis of this information, PoBOC forms
two clusters whose performance is evaluated in
terms of precision, recall, f-score and accuracy
which can be derived from Table 1.
2http://infomap-nlp.sourceforge.net/
number of Correct PRESIDENT Correct SINGER
Assigned PRESIDENT a b
Assigned SINGER c d
Table 1: Contingency table
We have used the same experimental setting for
the name categorization and discrimination prob-
lems.
4.2 Spanish name categorization
In Table 2, we show the results for the Spanish fine-
grained categorization. The detailed results are for
the context window of 50 words with 100 and 200
examples. All runs, outperform a simple baseline
system which returns for half of the examples the
fine-grained category PRESIDENT and for the rest
SINGER. This 50% baseline performance is due to
the balanced corpus we have created. In the column
diff., we show the difference between the 50% base-
line and the f-score of the category. As can be seen
the f-scores reaches 90%, which is with 40% more
than the baseline. According to the z? statistics with
confidence level of 0.975, the improvement over the
baseline is statistically significant.
SPANISH
cont/ex Category P. R. A. F. diff.
50/100
PRESIDENT 90.38 87.67 88.83 89.00
SINGER 87.94 90.00 88.33 88.96 +39.00
50/200
PRESIDENT 90.10 94.33 91.92 92.18
SINGER 94.04 89.50 91.91 91.71 +42.00
Table 2: Spanish NE categorization
During the error analysis, we found out that the
PERSON PRESIDENT and PERSON SINGER cat-
egories are distinguishable and separable because
of the well-established semantic similarity relation
among the words with which the NE occurs.
A pair of president sentences has lots of strongly
related words such as president:meeting, presi-
dent:government, which indicates high text cohe-
sion, while the majority of words in a president?
singer pair are weakly related, for instance presi-
dent:famous, president:concert. But still we found
out ambiguous pairs such as president:company,
where the president relates to a president of a coun-
try, while the company refers to a musical enter-
22
name c10 c25 c50 c100
Madonna 63.63 61.61 63.16 79.45
Julio Iglesias 58.96 56.68 66.00 79.19
Enrique Iglesias 77.27 80.17 84.36 90.54
Bill Clinton 52.72 48.81 74.74 73.91
George Bush 49.45 41.38 60.20 67.90
Fidel Castro 61.20 62.44 77.08 82.41
Table 3: Spanish NE discrimination
prize. Such information confuses LSA?s categoriza-
tion process and decreases the NE categorization
performance.
4.3 Spanish name discrimination
In a continuation, we present in Table 3 the f-scores
for the Spanish NE discrimination task with the 10,
25, 50 and 100 context windows. The results show
that the semantic similarity method we employ is
very reliable and suitable not only for the NE cat-
egorization, but also for the NE discrimination. A
baseline which always returns one and the same per-
son name during the NE discrimination task is 17%.
From the table can be seen that all names outperform
this baseline. The f-score performance per individ-
ual name ranges from 42% to 90%. The results are
very good, as the conflated names (three presidents
and three singers) can be easily obfuscated, because
they share the same domain and occur with the same
semantically related words.
The three best discriminated names are Enrique
Iglesias, Fidel Castro and Madonna. The name Fidel
Castro is easily discriminated due to its characteriz-
ing words Cuba, CIA, Cuban president, revolution,
tyrant. All sentences having these words or syn-
onyms related to them are associated to Fidel Cas-
tro.
Bill Clinton occurred many times with the words
democracy, Boris Yeltsin, Halifax, Chelsea (the
daughter of Bill Clinton), White House, while
George Bush appeared with republican, Ronald
Reigan, Pentagon, war in Vietnam, Barbara Bush
(the wife of George Bush).
During the data compilation process, the exam-
ples for Enrique Iglesias are considered to belong to
the Spanish singer. However, in reality some exam-
ples of Enrique Iglesias talked about the president of
a financial company in Uruguay or political issues.
Therefore, this name was confused with Bill Clin-
ton, because they shared semantically related words
such as bank, general secretary, meeting, decision,
appointment.
The discrimination process for the singer names is
good, though Madonna and Julio Iglesias appeared
in the context of concerts, famous, artist, maga-
zine, scene, backstage. The characterizing words for
Julio Iglesias are Chabeli (the daughter of Julio Igle-
sias), Spanish, Madrid, Iberoamerican. The name
Madonna occurred with words related to a picture
of Madonna, a statue in a church of Madonna, the
movie Evita.
Looking at the effect of the context window for
the NE discrimination task, it can be seen that the
best performances of 90% for Enrique Iglesias, 82%
for Fidel Castro and 79% for Madonna are achieved
with 100 words from the left and from the right of
the NE. This shows that the larger context has better
discrimination power.
4.4 Discussion
After the error analysis, we saw that the performance
of our approach depends on the quality of the data
source we worked with. Although, we have selected
names with low degree of ambiguity, during the data
compilation process for which we assumed that they
refer 100% to the SINGER or PRESIDENT cate-
gories, during the experiments we found out that one
and the same name can refer to three different in-
dividuals. This was the case of Madonna and En-
rique Iglesias. From one side this impeded the fine-
grained categorization and discrimination processes,
but opened a new line for research.
In conclusion, the conducted experiments re-
vealed a series of important observations. The first
one is that the LSA?s term-sentence approach per-
forms better with a higher number of examples, be-
cause they provide more semantic information. In
addition to the number of examples, the experiments
show that the influence of the context window for the
name discrimination is significant. The discrimina-
tion power is better for larger context windows and
this is also related to the expressiveness of the lan-
guage.
Second, our name categorization and discrimina-
tion approach outperforms the baseline with 30%.
Finally, LSA is a very appropriate approximation
for the resolution of the NE categorization and dis-
23
crimination tasks. LSA also gives logical explana-
tion about the classification decision of the person
names, providing a set of words characterizing the
category or simply a list of words describing the in-
dividual we want to classify.
5 Adaptation to Bulgarian
5.1 Motivation
So far, we have discussed and described the develop-
ment and the performance of our approach with the
Spanish language. The obtained results and observa-
tions, serve as a base for the context extraction and
the experimental setup for the rest of the languages
which we want to study. However, to verify the mul-
tilingual performance of the approach, we decided
to carry out an experiment with a language which is
very different from the Romance family.
For this reason, we choose the Bulgarian lan-
guage, which is the earliest written Slavic language.
It dates back from the creation of the old Bulgarian
alphabet Glagolista, which was later replaced by the
Cyrillic alphabet. The most typical characteristics of
the Bulgarian language are the elimination of noun
declension, suffixed definite article, lack of a verb
infinitive and complicated verb system.
The Bulgarian name discrimination data is ex-
tracted from the news corpus Sega2002. This corpus
is originally prepared and used in the CLEF compe-
titions. The corpus consists of news articles orga-
nized in different XML files depending on the year,
month, and day of the publication of the news. We
merged all files into a single one, and considered
only the text between the text tags. In order to ease
the text processing and to avoid encoding problems,
we transliterated the Cyrillic characters into Latin
ones.
The discrimination data in this experiment con-
sists of the city, country, party, river and mountain
categories. We were interested in studying not only
the multilingual issue of our approach, but also how
scalable it is with other categories. The majority
of the categories are locations and only one corre-
sponds to organization. In Table 4, we shows the
number of names which we extracted for each one
of the categories.
5.2 Bulgarian data
The cities include the capital of Bulgaria ? Sofia, the
second and third biggest Bulgarian cities ? Plovdiv
and Varna, a city from the southern parts of Bulgaria
? Haskovo, the capital of England ? London and
the capital of Russia ? Moskva. The occurrences of
these examples are conflated in the ambiguous name
CITY.
For countries we choose Russia (Rusiya)3, Ger-
many (Germaniya), France (Franciya), Turkey (Tur-
ciya) and England (Angliya). The five names are
conflated into COUNTRY.
The organizations we worked with are the two
leading Bulgarian political parties. BSP (Balgar-
ska Socialisticeska Partija, or Bulgarian Socialist
Party) is the left leaning party and the successor to
the Bulgarian Communist Party. SDS (Sayuz na
demokratichnite sili, or The Union of Democratic
Forces) is the right leaning political party. The two
organizations are conflated into PARTY.
For the RIVER category we choose Danube
(Dunav) which is the second longest river in Eu-
rope and passes by Bulgaria, Maritsa which is the
longest river that runs solely in the interior of the
Balkans, Struma and Mesta which run in Bulgaria
and Greece.
The final category consists of the oldest Bulgarian
mountain situated in the southern part of Bulgaria ?
Rhodope (Rodopi), Rila which is the highest moun-
tain in Bulgaria and on the whole Balkan Penin-
sula, and Pirin which is the second highest Bulgarian
mountain after Rila. The three mountain names are
conflated and substituted with the label MOUNTAIN.
5.3 Bulgarian name discrimination
The experimental settings coincide with those pre-
sented in Section 4 and the obtained results are
shown in Table 4. The performance of our approach
ranges from 32 to 81%. For the five categories, the
best performance is achieved for those names that
have the majority number of examples.
For instance, for the CITY category, the best per-
formance of 79% is reached with Sofia. TAs we
have previously mentioned, this is due to the fact that
LSA has more evidence about the context in which
Sofia appears. It is interesting to note that the city
3this is the Bulgarian transliteration for Russia
24
Category Instance Total P R F
City
Plovdiv 1822 44.42 83.87 58.08
Sofiya 5633 71.39 89.79 79.54
Varna 1042 32.02 82.64 46.17
Haskovo 140 21.09 69.29 32.33
London 751 31.32 84.82 45.74
Moskva 1087 39.47 88.22 54.53
Country
Rusiya 2043 55.83 86.19 67.77
Germaniya 1588 40.72 77.96 53.50
Francia 1352 37.27 77.81 50.39
Turciya 1162 43.23 84.08 57.10
Angliya 655 29.67 72.67 42.14
Party
BSP 2323 42.54 99.35 59.57
SDS 3916 64.86 98.85 78.32
River
Dunav 403 85.39 76.92 80.94
Marica 203 77.88 83.25 80.47
Mesta 81 63.64 95.06 76.24
Struma 37 56.67 91.89 70.10
Mountain
Rila 101 70.22 91.09 79.31
Pirin 294 75.11 57.48 65.12
Rodopi 135 71.04 96.29 81.76
Table 4: Bulgarian NE discrimination
Varna forms part of weak named entities such as the
University of Varna, the Major house of Varna. Al-
though, this strong entity is embedded into the weak
ones, practically Varna changes its semantic cate-
gory from a city into university, major house. This
creates additional ambiguity in our already conflated
and ambiguous names. In order to improve the per-
formance, we need a better data generation process
where the mixture of weak and strong entities will
be avoided.
The same effect of best classification for major-
ity sense is observed with the COUNTRY category.
The best performance of 67% is obtained for Rus-
sia. The other country which is distinguished sig-
nificantly well is Turkey. The 57% performance is
from 5 to 10% higher compared to the performances
of Germany, England and France. This is due to the
context in which the names occur. Turkey is related
to trading with Bulgaria and emigration, meanwhile
the other countries appear in the context of the Eu-
ropean Union, the visit of the Bulgarian president in
these countries.
During the error analysis, we noticed that in the
context of the political parties, SDS appeared many
times in with the names of the political leader or the
representatives of the BSP party and vice versa. This
impeded LSA?s classification, because of the similar
context.
Among all categories, RIVER and MOUNTAIN
obtained the best performances. The rivers Dunav
and Maritsa reached 80%, while the mountains
Rodopi achieved 81.76% f-score. Looking at the
discrimination results for the other names in these
categories, it can be seen that their performances are
much higher compared to the names of the CITY,
COUNTY and PARTY categories. This experiment
shows that the discrimination power is related to the
type of the NE category we want to resolve.
6 Conclusions
In this paper, we have presented a language indepen-
dent approach for person name categorization and
discrimination. This approach is based on the sen-
tence semantic similarity information derived from
LSA. The approach is evaluated with different NE
examples for the Spanish and Bulgarian languages.
We have observed the discrimination performance of
LSA not only with the SINGER and PRESIDENT
companies, but also with the CITY, COUNTRY,
MOUNTAIN, RIVER and PARTY. This is the first
approach which focuses on the resolution of these
categories for the Bulgarian language.
The obtained results both for Spanish and Bulgar-
ian are very promising. The baselines are outper-
formed with 25%. The person fine-grained catego-
rization reaches 90% while the name discrimination
varies from 42% to 90%. This variability is related
to the degree of the name ambiguity among the con-
flated names and similar behaviour is observed in the
co-occurence approach of (Pedersen et al, 2005).
During the experimental evaluation, we found out
that the 100% name purity (e.g. that one name be-
longs only to one and the same semantic category)
which we accept during the data creation in real-
ity contains 9% noise. These observations are con-
firmed in the additional experimental study we have
conducted with the Bulgarian language. According
to the obtained results, our text semantic similarity
approach performs very well and practically there
is no restrain to be adapted to other languages, data
sets or even new categories.
7 Future Work
In the future, we want to relate the name discrimi-
nation and categorization processes, by first encoun-
tering the different underlying meanings of a name
25
and then grouping together the sentences that belong
to the same semantic category. This process will in-
crease the performance of the NE fine-grained cat-
egorization, and will reduce the errors we encoun-
tered during the classification of the singers Enrique
Iglesias and Madonna. In addition to this experi-
ment, we want to cluster web pages on the basis of
name ambiguity. For instance, we want to process
the result for the Google?s query George Miller, and
form three separate clusters obtained on the basis of
a fine-grained and name discrimination. Thus we
can form the clusters for GeorgeMiller the congress-
man, the movie director and the father of WordNet.
This study will include also techniques for automatic
cluster stopping.
Moreover, LSA?s ability of language indepen-
dence can be exploited to resolve cross-language
NE categorization and discrimination from which
we can extract cross-language pairs of semantically
related words characterizing a person e.g. George
Bush is seen with White House in English, la Casa
Blanca in Spanish, a Casa Branka in Portuguese and
Beliat Dom in Bulgarian.
With LSA, we can also observe the time consis-
tency property of a person which changes its se-
mantic category across time. For instance, a stu-
dent turns into a PhD student, teaching assistant and
then university professor, or as in the case of Arnold
Schwarzenegger from actor to governor.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their useful comments and suggestions.
This work was partially funded by the European
Union under the project QALLME number FP6 IST-
033860 and by the Spanish Ministry of Science and
Technology under the project TEX-MESS number
TIN2006-15265-C06-01.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the Thirty-Sixth Annual Meeting of
the ACL and Seventeenth International Conference on
Computational Linguistics, pages 79?85.
G. Cleuziou, L. Martin, and C. Vrain. 2004. Poboc: An
overlapping clustering algorithm, application to rule-
based classification and textual data. In ECAI, pages
440?444.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Informa-
tion Science, volume 41, pages 391?407.
S. Dumais. 1995. Using lsi for information filtering:
Trec-3 experiments. In The Third Text Retrieval Con-
ference (TREC-3), pages 219?230.
M. Fleischman and E. Hovy. 2002. Fine grained classifi-
cation of named entities. In Proceedings of the 19th in-
ternational conference on Computational linguistics,
pages 1?7.
Z. Kozareva, O. Ferra?ndeza, A. Montoyo, R. Mun?oz,
A. Sua?rez, and J. Go?mez. 2007. Combining data-
driven systems for improving named entity recogni-
tion. Data and Knowledge Engineering, 61(3):449?
466, June.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
G. Miller and W. Charles. 1991. Contextual correlates of
semantic similarity. In Language and Cognitive Pro-
cesses, pages 1?28.
M. Pasca. 2004. Acquisition of categorized named enti-
ties for web search. In CIKM ?04: Proceedings of the
thirteenth ACM international conference on Informa-
tion and knowledge management, pages 137?145.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In CI-
CLing, pages 226?237.
P. Shah, D. Schneider, C. Matuszek, R.C. Kahlert,
B. Aldag, D. Baxter, J. Cabral, M. Witbrock, and
J. Curtis. 2006. Automated population of cyc: Ex-
tracting information about named-entities from the
web. In Proceedings of the Nineteenth International
FLAIRS Conference, pages 153?158.
H. Shu?tze. 1998. Automatic word sense discrimination.
In Journal of computational linguistics, volume 24.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proceeding of
11th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 17?24.
26
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334?337,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZBSA: A Headline Emotion Classification through Web Information
Zornitsa Kozareva, Borja Navarro, Sonia Va?zquez, Andre?s Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,borja,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents a headline emotion clas-
sification approach based on frequency and
co-occurrence information collected from
the World Wide Web. The content words of
a headline (nouns, verbs, adverbs and adjec-
tives) are extracted in order to form different
bag of word pairs with the joy, disgust, fear,
anger, sadness and surprise emotions. For
each pair, we compute the Mutual Informa-
tion Score which is obtained from the web
occurrences of an emotion and the content
words. Our approach is based on the hypoth-
esis that group of words which co-occur to-
gether across many documents with a given
emotion are highly probable to express the
same emotion.
1 Introduction
The subjective analysis of a text is becoming impor-
tant for many Natural Language Processing (NLP)
applications such as Question Answering, Informa-
tion Extraction, Text Categorization among others
(Shanahan et al, 2006). The resolution of this prob-
lem can lead to a complete, realistic and coher-
ent analysis of the natural language, therefore ma-
jor attention is drawn to the opinion, sentiment and
emotion analysis, and to the identification of be-
liefs, thoughts, feelings and judgments (Quirk et al,
1985), (Wilson and Wiebe, 2005).
The aim of the Affective Text task is to clas-
sify a set of news headlines into six types of emo-
tions: ?anger?, ?disgust?, ?fear?, ?joy?, ?sadness?
and ?surprise?. In order to be able to conduct
such multi-category analysis, we believe that first
we need a comprehensive theory of what a human
emotion is, and then we need to understand how the
emotion is expressed and transmitted within the nat-
ural language. These aspects rise the need of syn-
tactic, semantic, textual and pragmatic analysis of
a text (Polanyi and Zaenen, 2006). However, some
of the major drawbacks in this field are related to
the manual or automatic acquisition of subjective ex-
pressions, as well as to the lack of resources in terms
of coverage.
For this reason, our current emotion classification
approach is based on frequency and co-occurrence
bag of word counts collected from the World Wide
Web. Our hypothesis is that words which tend to co-
occur across many documents with a given emotion
are highly probable to express this emotion.
The rest of the paper is organized as follows. In
Section 2 we review some of the related work, in
Section 3 we describe our web-based emotion classi-
fication approach for which we show a walk-through
example in Section 4. A discussion of the obtained
results can be found in Section 5 and finally we con-
clude in Section 6.
2 Related work
Our approach for emotion classification is based on
the idea of (Hatzivassiloglou and McKeown, 1997)
and is similar to those of (Turney, 2002) and (Tur-
ney and Littman, 2003). According to Hatzivas-
siloglou and McKeown (1997), adjectives with the
same polarity tended to appear together. For exam-
ple the negative adjectives ?corrupt and brutal? co-
334
occur very often.
The idea of tracing polarity through adjective co-
occurrence is adopted by Turney (2002) for the bi-
nary (positive and negative) classification of text re-
views. They take two adjectives, for instance ?ex-
cellent? and ?poor? in a way that the first adjective
expresses positive meaning, meanwhile the second
one expresses negative. Then, they extract all ad-
jectives from the review text and combine them with
?excellent? and ?poor?. The co-occurrences of these
words are searched on the web, and then the Mutual
Information score for the two groups of adjectives
is measured. When the adjective of the review ap-
pear more often with ?excellent?, then the review is
classified as positive, and when the adjectives appear
more often with ?poor?, then the review is classified
as negative.
Following Hatzivassiloglou and McKeown (1997)
and Turney (2002), we decided to observe how often
the words from the headline co-occur with each one
of the six emotions. This study helped us deduce
information according to which ?birthday? appears
more often with ?joy?, while ?war? appears more
often with ?fear?.
Some of the differences between our approach
and those of Turney (2002) are mentioned below:
? objectives: Turney (2002) aims at binary text
classification, while our objective is six class
classification of one-liner headlines. Moreover,
we have to provide a score between 0 and 100
indicating the presence of an emotion, and not
simply to identify what the emotion in the text
is. Apart from the difficulty introduced by the
multi-category classification, we have to deal
with a small number of content words while
Turney works with large list of adjectives.
? word class: Turney (2002) measures polarity
using only adjectives, however in our approach
we consider the noun, the verb, the adverb and
the adjective content words. The motivation
of our study comes from (Polanyi and Zaenen,
2006), according to which each content word
can express sentiment and emotion. In addition
to this issue we saw that most of the headlines
contain only nouns and verbs, because they ex-
press objectivity.
? search engines: Turney (2002) uses the Al-
tavista web browser, while we consider and
combine the frequency information acquired
from three web search engines.
? word proximity: For the web searches, Tur-
ney (2002) uses the NEAR operator and con-
siders only those documents that contain the
adjectives within a specific proximity. In our
approach, as far as the majority of the query
words appear in the documents, the frequency
count is considered.
? queries: The queries of Turney (2002) are made
up of a pair of adjectives, and in our approach
the query contains the content words of the
headline and an emotion.
There are other emotion classification approaches
that use the web as a source of information. For
instance, (Taboada et al, 2006) extracted from the
web co-occurrences of adverbs, adjectives, nouns
and verbs. Gamon and Aue (2005) were looking
for adjectives that did not co-occur at sentence level.
(Baroni and Vegnaduzzo, 2004) and (Grefenstette
et al, 2004) gathered subjective adjectives from the
web calculating the Mutual Information score.
Other important works on sentiment analysis are
those of (Wilson et al, 2005) and (Wiebe et al,
2005; Wilson and Wiebe, 2005), who used linguistic
information such as syntax and negations to deter-
mine polarity. Kim and Hovy (2006) integrated verb
information from FrameNet and incorporated it into
semantic role labeling.
3 Web co-occurrences
In order to determine the emotions of a
headline, we measure the Pointwise Mu-
tual Information (MI) of ei and cwj as
MI(ei, cwj) = log2 hits(ei,cwj)hits(ei)hits(cwj) , where ei ?
{anger, disgust, fear, joy, sadness, surprise}
and cwj are the content words of the headline j.
For each headline, we have six MI scores which
indicate the presence of the emotion. MI is used
in our experiments because it provides information
about the independence of an emotion and a bag of
words.
To collect the frequency and co-occurrence counts
of the headline words, we need large and massive
335
data repositories. To surmount the data sparsity
problem, we used as corpus the World Wide Web
which is constantly growing and daily updated.
Our statistical information is collected from three
web search engines: MyWay1, AlltheWeb2 and Ya-
hoo3. It is interesting to note that the emotion dis-
tribution provided by each one of the search engines
for the same headline has different scores. For this
reason, we decided to compute an intermediate MI
score as aMI =
?n
s=1 MI(ei,cwj)
s .
In the trail data, besides the MI score of an emo-
tion and all headline content words, we have calcu-
lated the MI for an emotion and each one of the con-
tent words. This allowed us to determine the most
sentiment oriented word in the headline and then we
use this predominant emotion to weight the associ-
ation sentiment score for the whole text. Unfortu-
nately, we could not provide results for the test data
set, due to the high number of emotion-content word
pairs and the increment in processing time and re-
turned responses of the search engines.
4 Example for Emotion Classification
As a walk through example, we use the Mortar as-
sault leaves at least 18 dead headline which is taken
from the trial data. The first step in our emotion clas-
sification approach consists in the determination of
the part-of-speech tags for the one-liner. The non-
content words are stripped away, and the rest of the
words are taken for web queries. To calculate the MI
score of a headline, we query the three search en-
gines combining ?mortar, assault, leave, dead? with
the anger, joy, disgust, fear, sadness and surprise
emotions. The obtained results are normalized in a
range from 0 to 100 and are shown in Table 1.
MyWay AllWeb Yahoo Av. G.Stand.
anger 19 22 24 22 22
disgust 5 6 7 6 2
fear 44 50 53 49 60
joy 15 19 20 18 0
sadness 28 36 36 33 64
surprise 4 5 6 5 0
Table 1: Performance of the web-based emotion
classification for a trail data headline
1www.myway.com
2www.alltheweb.com
3www.yahoo.com
As can be seen from the table, the three search
engines provide different sentiment distribution for
the same headline, therefore in our final experiment
we decided to calculate intermediate MI. Comparing
our results to those of the gold standard, we can say
that our approach detects significantly well the fear,
sadness and angry emotions.
5 Results and Discussion
Table 2 shows the obtained results for the affective
test data. The low performance of our approach
is explainable by the minimal knowledge we have
used. An interesting conclusion deduced from the
trail and test emotion data is that the system detects
better the negative feelings such as anger, disgust,
fear and sadness, in comparison to the positive emo-
tions such as joy and surprise. This makes us believe
that according to the web most of the word-emotion
combinations we queried are related to the expres-
sion of negative emotions.
UA-ZBSA Fine-grained Coarse-grained
Pearson Acc. P. R.
Anger 23.20 86.40 12.74 21.66
Disgust 16.21 97.30 0.00 0.00
Fear 23.15 75.30 16.23 26.27
Joy 2.35 81.80 40.00 2.22
Sadness 12.28 88.90 25.00 0.91
Surprise 7.75 84.60 13.70 16.56
Table 2: Performance of the web-based emotion
classification for the whole test data set
In the test run, we could not apply the emotion-
word weighting, however we believe that it has
a significant impact over the final performance.
Presently, we were looking for the distribution of all
content words and the emotions, but in the future we
would like to transform all words into adjectives and
then conduct web queries.
Furthermore, we would like to combine the re-
sults from the web emotion classification with the
polarity information given by SentiWordNet4. A-
priory we want to disambiguate the headline content
words and to determine the polarities of the words
and their corresponding senses. For instance, the ad-
jective ?new? has eleven senses, where new#a#3 and
new#a#5 express negativism, new#a#4 and new#a#9
positivism and the rest of the senses are objective.
4http://sentiwordnet.isti.cnr.it/
336
So far we did not consider the impact of valence
shifter (Polanyi and Zaenen, 2006) and we were un-
able to detect that a negative adverb or adjective
transforms the emotion from positive into negative
and vice versa. We are also interested in studying
how to conduct queries not as a bag of words but
bind by syntactic relations (Wilson et al, 2005).
6 Conclusion
Emotion classification is a challenging and difficult
task in Natural Language Processing. For our first
attempt to detect the amount of angry, fear, sadness,
surprise, disgust and joy emotions, we have pre-
sented a simple web co-occurrence approach. We
have combined the frequency count information of
three search engines and we have measured the Mu-
tual Information score between a bag of content
words and emotion.
According to the yielded results, the presented ap-
proach can determine whether one sentiment is pre-
dominant or not, and most of the correct sentiment
assignments correspond to the negative emotions.
However, we need to improve the approach in many
aspects and to incorporate more knowledge-rich re-
sources, as well as to tune the 0-100 emotion scale.
Acknowledgements
This research has been funded by QALLME number
FP6 IST-033860 and TEX-MESS number TIN2006-
15265-C06-01.
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identi-
fying subjective adjectives through web-based mutual
information. In Ernst Buchberger, editor, Proceedings
of KONVENS 2004, pages 17?24.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing (ACL
2005), pages 57?64.
Gregory Grefenstette, Yan Qu, James G. Shanahana, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, pages 1?8.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifter. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Applications, chapter 1, pages 1?
10. Springer.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2006.
Computing Attitude and Affect in Text: Theory and Ap-
plications. Springer.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Methods for creating semantic orientation
databases. In Proceeding of LREC-06, the 5th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 427?432.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 417?424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2-3):165?
210.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Ann Arbor, editor,
Proceedings of the Workshop on Frontiers in Corpus
Annotation II: Pie in the Sky, pages 53?60.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
337
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 338?341,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZSA: Web Page Clustering on the basis of Name Disambiguation
Zornitsa Kozareva, Sonia Vazquez, Andres Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents an approach for web
page clustering. The different underlying
meanings of a name are discovered on the
basis of the title of the web page, the body
content, the common named entities across
the documents and the sub-links. This in-
formation is feeded into a K-Means cluster-
ing algorithm which groups together the web
pages that refer to the same individual.
1 Introduction
Ambiguity is the task of building up multiple alter-
native linguistic structures for a single input. Most
of the approaches focus on word sense disambigua-
tion (WSD), where the sense of a word has to be
determined depending on the context in which it is
used.
The same problem arises for named entities
shared by different people or for grandsons named
after their grandparents. For instance, querying the
name ?Michael Hammond? in the World Wide Web
where there are huge quantities of massive and un-
structured data, a search engine retrieves thousands
of documents related to this name. However, there
are several individuals sharing the name ?Michael
Hammon?. One is a biology professor at the Univer-
sity of Arizona, another is at the University of War-
wick, there is a mathematician from Toronto among
others. The question is which one of these refer-
ents we are actually looking for and interested in.
Presently, to be able to answer to this question, we
have to skim the content of the documents and re-
trieve the correct answers on our own.
To automate this process, the named entities
can be disambiguated and the different underlying
meanings of the name can be found. On the basis
of this information, the web pages can be clustered
together and organized in a hierarchical structure
which can ease the documents? browsing. This is
also the objective of the Web People Search (WePS)
task (Artiles et al, 2007). What makes the WePS
task even more challenging is the fact that in con-
trast to WSD where the number of senses of a word
are predefined, in WePS we do not know the exact
number of different individuals.
For the resolution of the WePS task, we have de-
veloped a web page clustering approach using the
title and the body content of the web pages. In ad-
dition, we group together the documents that share
many location, person and organization names, as
well as those that point out to the same sub-links.
The rest of the paper is organized as follows. In
Section 2 we describe various approaches for name
disambiguation and discrimination. Our approach
is shown in Section 3, the obtained results and a dis-
cussion are provided in Section 4 and finally we con-
clude in Section 5.
2 Related Work
Early work in the field of name disambiguation
is that of (Bagga and Baldwin, 1998) who pro-
posed cross-document coreference resolution algo-
rithm which uses vector space model to resolve the
ambiguities between people sharing the same name.
The approach is evaluated on 35 different mentions
of John Smith and reaches 85% f-score.
Mann and Yarowski (2003) developed an unsu-
338

HTML/XML cleaning 
Search 
Web 
Retrieved Documents 
Preprocessing 
Title 
Context information 
Body 
Text Proper names 
Links 
Clusters 
K-means Cluster analysis WEKA 
LSA matrix transformation 
Clustering 
On the basis of name disambiguation 
Matrix from context 
Figure 1: Architecture of the WePS System
pervised approach to name discrimination where bi-
ographical features (age, date of birth), familiar re-
lationships (wife, son, daughter) and associations
(country, company, organization) are considered.
Therefore, in our approach we use person, organiza-
tion and location names in order to construct a social
similarity network between two documents.
Another unsupervised clustering technique for
name discrimination of web pages is that of Peder-
sen and Kulkarni (2007). They used contextual vec-
tors derived from bigrams, and measured the impact
of several association measures. During the evalu-
ation, some names were easily discriminable com-
pared to others categories for which was even diffi-
cult to find and obtain discriminative feature. We
worked with their unigram model (Purandare and
Pedersen, 2004) to cluster the web pages using the
text content between the title tags.
3 Web Person Disambiguation
Our web people clustering approach is presented in
Figure 1 and consists of the following steps:
? HTML cleaning: all html tags are stripped
away, the javascript code is eliminated, the non
closed WePS tags are repaired, the missing be-
gin/end body tags are included and then the
content between the title, the body and the an-
chor tags is extracted.
? name matching: the location, person and orga-
nization names in the body texts are identified
with the GATE1 system (Cunningham, 2005).
Each named entity of a document is matched
with its corresponding named entity category
from the rest of the web pages. This infor-
mation is used to calculate the social semantic
similarity of the person, the location and the or-
ganization names. Our hypothesis is that doc-
uments with similar names tend to refer to the
same individual. The output of this module is
a matrix with binary values, where 1 stands for
the documents which share more than the half
of their proper names, and 0 otherwise.
? links: for each document, we extract the links
situated between the anchor tags. Since the
links are too specific, we wrote an url function
which transform a given web page d1 with URL
http://www.cs.ualberta.ca/?lindek/index.htm
into www.cs.ualberta.ca/?lindek,
and the web page d2 with URL
http://www.cs.ualberta.ca/?lindek/demos.htm
into www.cs.ualberta.ca/?lindek. According
to our approach, the two web pages d1 and d2
are linked to each other if their link structures
(LS) intersect, that is LS(d1)?LS(d2) 6= 0.
The output of this module is a matrix with
binary values, where 1 stands for two web
pages having more than 3 links in common and
0 otherwise.
? titles: for each document, we extract the text
between the title tags. We create a unigram
matrix which is feed into SenseClusters2. We
use automatic cluster stopping criteria with the
gap statistics which groups the web pages into
several clusters according to the context of the
titles. From the obtained clusters, we generate
a new matrix with binary values, where 1 corre-
sponds to the documents which were put in the
1http://sourceforge.net/projects/gate
2http://marimba.d.umn.edu/cgi-bin/SC-cgi/index.cgi
339
same cluster according to SenseClusters and 0
otherwise.
? bodies: the text between the body tags is ex-
tracted, tokenized and the part-of-speech (POS)
tags 3 are determined. The original text is trans-
formed by encoding the POS tag information as
follows: ?water#v the#det flowers#n and#conj
pass#v me#pron the#det glass#n of#prep wa-
ter#n?. This corpus transformation is done, be-
cause we want the Latent Semantic Analysis
(LSA) module to consider the syntactic cate-
gories of the words and to construct a more
reliable semantic space. For instance, in the
example above, there are two different repre-
sentations of water: the noun and the verb,
while without the corpus transformation LSA
sees only the string water.
? LSA4: the semantic similarity score for the
web-pages is calculated with Latent Semantic
Analysis (LSA). From the encoded body texts,
we build up a matrix, where the rows repre-
sent the words of the web-page collection, the
columns stand for the web-pages we want to
cluster and the cells show the number of times a
word of the corpus occurs in a web page. In or-
der to reduce the noise and the data sparsity, we
apply the Singular Value Decomposition algo-
rithm by reducing the original vector space into
300 dimensions. The output of the LSA mod-
ule is a matrix, which represents the semantic
similarity among the web pages.
? knowledge combination: the outputs of the
name matching, link, title and body modules
are combined into a new matrix 100 ? 400 di-
mensional matrix. The rows correspond to the
number of web pages and the columns repre-
sent the obtained values of the link, title, body
and name modules. This matrix is fed into
the K-means clustering algorithm which deter-
mines the final web page clustering.
? K-means5: the clustering of N web pages
into K disjoint subsets Sj containing Nj data
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4infomap-nlp.sourceforge.net/
5http://www.cs.waikato.ac.nz/ml/weka/
points is done by the minimization of the sum-
of-squares criterion J = ?Kj=1
?
n?Sj |xn ?
muj |2, where xn is a vector representing the
nth data point and muj is the geometric cen-
troid of the data points in Sj . The informa-
tion matrix from which the web page cluster-
ing is performed includes the similarity infor-
mation for the title, link, proper name and body.
The current implementation of K-means (Wit-
ten and Frank, 2005) does not have an au-
tomatic cluster stopping criteria, therefore the
number of clusters is set up manually.
4 Results and Discussion
Table 1 shows the obtained results for the test data
set. The average performance of our system is 56%
and we ranked on 10-th position from 16 participat-
ing teams. Although, we have used different sources
of information and various approximations, in the
future we have to surmount a number of obstacles.
One of the limitations comes from the usage of the
text snippets situated between the body tags. There
are a number of web pages which do not contain any
text. The semantic space for these documents cannot
be built with LSA and their similarity score is zero.
Despite the fact that we have eliminated the stop
words from the documents and we have transformed
the web pages by encoding the syntactic categories,
the classification power of LSA was different for the
ambiguous names and for the web pages. To some
extend this is due to the varying number of words
in the web pages. In the future, we want to con-
duct experiments with a fixed context windows for
all documents.
In this task, the number of senses (e.g. number
of different individuals that share the same name)
is unknown, and one of the major drawbacks in our
approach is related to the setting up of the number
of clusters. The K-Means clustering algorithm we
used, did not include an automatic cluster stopping
criteria, and we had to set up the number of clus-
ters manually. To be able to do that, we have ob-
served the average number of clusters per name in
the trial data. We have evaluated the performance
of our approach with several different numbers of
clusters. According to the obtained results, the best
clusters are 25 and 50. We used the same number
340
Name Purity Inverse
Purity
F
?=0.5
F
?=0.2
Mark Johnson 0,55 0,74 0,63 0,69
Sharon Goldwater 0,96 0,23 0,37 0,27
Robert Moore 0,36 0,67 0,47 0,57
Leon Barrett 0,62 0,51 0,56 0,52
Dekang Lin 0,99 0,43 0,60 0,49
Stephen Clark 0,52 0,75 0,62 0,69
Frank Keller 0,38 0,67 0,48 0,58
Jerry Hobbs 0,54 0,63 0,58 0,61
James Curran 0,53 0,61 0,57 0,59
Chris Brockett 0,73 0,40 0,51 0,44
Thomas Fraser 0,66 0,57 0,61 0,58
John Nelson 0,68 0,76 0,72 0,74
James Hamilton 0,56 0,60 0,58 0,59
William Dickson 0,59 0,78 0,67 0,73
James Morehead 0,36 0,64 0,46 0,56
Patrick Killen 0,56 0,69 0,62 0,66
George Foster 0,46 0,70 0,56 0,64
James Davidson 0,58 0,71 0,64 0,68
Arthur Morgan 0,77 0,47 0,59 0,51
Thomas Kirk 0,26 0,90 0,41 0,60
Patrick Killen 0,56 0,69 0,62 0,66
Harry Hughes 0,66 0,54 0,59 0,56
Jude Brown 0,64 0,63 0,64 0,63
Stephan Johnson 0,56 0,80 0,66 0,73
Marcy Jackson 0,40 0,73 0,52 0,63
Karen Peterson 0,56 0,72 0,63 0,68
Neil Clark 0,68 0,36 0,47 0,40
Jonathan Brooks 0,53 0,76 0,63 0,70
Violet Howard 0,58 0,75 0,65 0,71
Global average 0,58 0,64 0,58 0,60
Table 1: Evaluation results
of clusters for the test data, however this is a rough
parameter estimation.
5 Conclusion
Person name disambiguation is a very important task
whose resolution can improve the performance of
the search engine by grouping together web pages
which refer to different individuals that share the
same name.
For our participation in the WePS task, we pre-
sented a name disambiguation approach which uses
only the information extracted from the web pages.
We conducted an experimental study with the trail
data set, according to which the combination of
the title, the body, the proper names and sub-links
reaches the best performance. Our current approach
can be improved with the incorporation of automatic
cluster stopping criteria.
So far we did not take advantage of the document
ranking and the returned snippets, but we want to in-
corporate this information by measuring the snippet
similarity on the basis of relevant domain informa-
tion (Kozareva et al, 2007).
Acknowledgements
Many thanks to Ted Pedersen for useful comments
and suggestions. This work was partially funded
by the European Union under the project QALLME
number FP6 IST-033860 and by the Spanish Min-
istry of Science and Technology under the project
TEX-MESS number TIN2006-15265-C06-01.
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The semeval-
2007 weps evaluation: Establishing a benchmark for
the web people search task. In Proceedings of Semeval
2007, Association for Computational Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of ACL, pages 79?85.
H. Cunningham. 2005. Information Extraction, Auto-
matic. Encyclopedia of Language and Linguistics, 2nd
Edition.
Z. Kozareva, S. Vazquez, and A. Montoyo. 2007. The
usefulness of conceptual representation for the iden-
tification of semantic variability expressions. In Pro-
ceedings of the Eighth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, (CICLing-2007).
G. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning at HLT-
NAACL 2003, pages 33?40.
T. Pedersen and A. Kulkarni. 2007. Discovering identi-
ties in web contexts with unsupervised clustering. In
Proceedings of the IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data.
A. Purandare and T. Pedersen. 2004. Senseclusters -
finding clusters that represent word senses. In AAAI,
pages 1030?1031.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, volume 2.
Morgan Kaufmann.
341
Coling 2010: Poster Volume, pages 27?35,
Beijing, August 2010
Going Beyond Traditional QA Systems: Challenges and Keys 
in Opinion Question Answering 
Alexandra Balahur 
Dept. of Software and Computing Systems  
University of Alicante 
abalahur@dlsi.ua.es 
Ester Boldrini  
Dept. of Software and Computing Systems  
University of Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
Dept. of Software and Computing Systems  
University of Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
Dept. of Software and Computing Systems  
University of Alicante 
patricio@dlsi.ua.es 
Abstract  
The treatment of factual data has been 
widely studied in different areas of Nat-
ural Language Processing (NLP). How-
ever, processing subjective information 
still poses important challenges. This 
paper presents research aimed at assess-
ing techniques that have been suggested 
as appropriate in the context of subjec-
tive - Opinion Question Answering 
(OQA). We evaluate the performance of 
an OQA with these new components 
and propose methods to optimally tackle 
the issues encountered. We assess the 
impact of including additional resources 
and processes with the purpose of im-
proving the system performance on two 
distinct blog datasets. The improve-
ments obtained for the different combi-
nation of tools are statistically signifi-
cant. We thus conclude that the pro-
posed approach is adequate for the OQA 
task, offering a good strategy to deal 
with opinionated questions. 
1 Introduction 
The State of the Blogosphere 2009 survey pub-
lished by Technorati 1 concludes that in the past 
years the blogosphere has gained a high influ-
ence on a high variety of topics, ranging from 
cooking and gardening, to economics, politics 
and scientific achievements. The development 
                                                 
1 http://technorati.com/ 
of the Social Web and the new communication 
frameworks also influenced the way informa-
tion is transmitted through communities. Blogs 
are part of the so-called new textual genres. 
They have distinctive features when compared 
to the traditional ones, such as newspaper ar-
ticles. Blog language contains formal and in-
formal expressions, and other elements, as re-
peated punctuation or emoticons (used to stress 
upon different text elements). With the growth 
in the content of the blogosphere, the quantity 
of subjective data of the Web is increasing ex-
ponentially (Cui et al, 2006). As it is being up-
dated in real-time, this data becomes a source of 
timely information on many topics, exploitable 
by different applications. In order to properly 
manage the content of this subjective informa-
tion, its processing must be automated. The 
NLP task, which deals with the classification of 
opinionated content is called Sentiment Analy-
sis (SA). Research in this field aims at discover-
ing appropriate mechanisms to properly re-
trieve, extract and classify opinions expressed in 
text. While techniques to retrieve objective in-
formation have been widely studied, imple-
mented and evaluated, opinion-related tasks still 
represent an important challenge. As a conse-
quence, the aim of our research is to study, im-
plement and evaluate appropriate methods for 
the task of Question Answering (QA) in the 
opinion treatment framework.  
2 Motivation and Contribution 
Research in opinion-related tasks gained impor-
tance in the past years. However, there are still 
many aspects that require analysis and im-
27
provement, especially for approaches that com-
bine SA with other NLP tasks such as QA or 
automatic summarization. The TAC 2008 Opi-
nion Pilot task and the subsequent research per-
formed on the competition data have demon-
strated that answering opinionated questions 
and summarizing subjective information are 
significantly different from the equivalent tasks 
in the same context, but dealing with factual 
data.  This finding was confirmed by the recent 
work by (Kabadjov et al, 2009). The first moti-
vation of our work is the need to detect and ex-
plore the challenges raised by opinion QA 
(OQA), as compared to factual QA. To this aim, 
we analyze the improvements that can be 
brought at the different steps of the OQA 
process: question treatment (identification of 
expected polarity ? EPT, expected source ? ES 
and expected target ?ET-), opinion retrieval (at 
the level of one and three-sentences long snip-
pets, using topic-related words or using paraph-
rases), opinion analysis (using topic detection 
and anaphora resolution). This preliminary re-
search is motivated by the conclusions drawn by 
previous studies (Balahur et al, 2009). Our pur-
pose is to verify if the inclusion of new ele-
ments and methods - source and target detection 
(using semantic role labeling (SRL)), topic de-
tection (using Latent Semantic Analysis), pa-
raphrasing and joint topic-sentiment analysis 
(classification of the opinion expressed only in 
sentences related to the topic), followed by ana-
phora resolution (using a system whose perfor-
mance is not optimal), affects the results of the 
system and how. Our contribution to this respect 
is the identification of the challenges related to 
OQA compared to traditional QA. A further 
contribution consists in adding the appropriate 
methods, tools and resources to resolve the 
identified challenges. With the purpose of test-
ing the effect of each tool, resource and tech-
nique, we carry out a separate and a global 
evaluation. An additional motivation of our 
work is the fact that although previous ap-
proaches showed that opinion questions have 
longer answers than factual ones, the research 
done in OQA so far has only considered a sen-
tence-level approach. Another contribution this 
paper brings is the retrieval at 1 and 3-sentence 
level and the retrieval based on similarity to 
query paraphrases enriched with topic-related 
words). We believe retrieving longer text could 
cause additional problems such as redundancy, 
coreference and temporal expressions or the 
need to apply contextual information. Paraph-
rasing, on the other hand, had account for lan-
guage variability in a more robust manner; 
however, the paraphrase collections that are 
available at the moment are known to be noisy. 
The following sections are structured as fol-
lows: Section 3 presents the related work in the 
field and the competitions organized for systems 
tackling the OQA task. In Section 4 we describe 
the corpora used for the experiments we carried 
out and the set of questions asked over each of 
them. Section 5 presents the experimental set-
tings and the different system configurations we 
assessed. Section 6 shows the results of the 
evaluations, discusses the improvements and 
drops in performance using different configura-
tions. We finally conclude on our approaches in 
Section 7, proposing the lines for future work. 
3 Related Work 
QA can be defined as the task in which given a 
set of questions and a collection of documents, 
an automatic NLP system is employed to re-
trieve the answer to the queries in Natural Lan-
guage (NL). Research focused on building fac-
toid QA systems has a long tradition; however, 
it is only recently that researchers have started 
to focus on the development of OQA systems. 
(Stoyanov et al, 2005) and (Pustejovsky and 
Wiebe, 2006) studied the peculiarities of opi-
nion questions. (Cardie et al, 2003) employed 
opinion summarization to support a Multi-
Perspective QA system, aiming at identifying 
the opinion-oriented answers for a given set of 
questions. (Yu and Hatzivassiloglou, 2003) se-
parated opinions from facts and summarized 
them as answer to opinion questions. (Kim and 
Hovy, 2005) identified opinion holders, which 
are a key component in retrieving the correct 
answers to opinion questions. Due to the rea-
lized importance of blog data, recent years have 
also marked the beginning of NLP research fo-
cused on the development of opinion QA sys-
tems and the organization of international con-
ferences encouraging the creation of effective 
QA systems both for fact and subjective texts. 
The TAC 20082 QA track proposed a collection 
                                                 
2 http://www.nist.gov/tac/ 
28
of factoid and opinion queries called ?rigid list? 
(factoid) and ?squishy list? (opinion) respective-
ly, to which the traditional QA systems had to 
be adapted. Some participating systems treated 
opinionated questions as ?other? and thus they 
did not employ opinion specific methods. How-
ever, systems that performed better in the 
?squishy list? questions than in the ?rigid list? 
implemented additional components to classify 
the polarity of the question and of the extracted 
answer snippet. The Alyssa system (Shen et al 
2007) uses a Support Vector Machines (SVM) 
classifier trained on the MPQA corpus (Wiebe 
et al, 2005), English NTCIR3 data and rules 
based on the subjectivity lexicon (Wilson et al, 
2005). (Varma et al, 2008) performed query 
analysis to detect the polarity of the question 
using defined rules. Furthermore, they filter 
opinion from fact retrieved snippets using a 
classifier based on Na?ve Bayes with unigram 
features, assigning for each sentence a score that 
is a linear combination between the opinion and 
the polarity scores. The PolyU (Venjie et al, 
2008) system determines the sentiment orienta-
tion of the sentence using the Kullback-Leibler 
divergence measure with the two estimated lan-
guage models for the positive versus negative 
categories. The QUANTA (Li et al, 2008) sys-
tem performs opinion question sentiment analy-
sis by detecting the opinion holder, the object 
and the polarity of the opinion. It uses a seman-
tic labeler based on PropBank 4  and manually 
defined patterns. Regarding the sentiment clas-
sification, they extract and classify the opinion 
words. Finally, for the answer retrieval, they 
score the retrieved snippets depending on the 
presence of topic and opinion words and only 
choose as answer the top ranking results. Other 
related work concerns opinion holder and target 
detection. NTCIR 7 and 8 organized MOAT 
(the Multilingual Opinion Analysis Task), in 
which most participants employed machine 
learning approaches using syntactic patterns 
learned on the MPQA corpus (Wiebe et al, 
2005). Starting from the abovementioned re-
search, our aim is to take a step forward to 
present approaches and employ opinion specific 
methods focused on improving the performance 
of our OQA. We perform the retrieval at 1 sen-
                                                 
3 http://research.nii.ac.jp/ntcir/ 
4http://verbs.colorado.edu/~mpalmer/projects/ace.html 
tence and 3 sentence-level and also determine 
the expected source (ES) and the expected tar-
get (ET) of the questions, which are fundamen-
tal to properly retrieve the correct answer. These 
two elements are selected employing semantic 
roles (SR). The expected answer type (EAT) is 
determined using Machine Learning (ML) using 
Support Vector Machine (SVM), by taking into 
account the interrogation formula, the subjectiv-
ity of the verb and the presence of polarity 
words in the target SR. In the case of expected 
opinionated answers, we also compute the ex-
pected polarity type (EPT) ? by applying opi-
nion mining (OM) on the affirmative version of 
the question (e.g. for the question ?Why do 
people prefer Starbucks to Dunkin Donuts??, 
the affirmative version is ?People prefer Star-
bucks to Dunkin Donuts because X?). These 
experiments are presented in more detail in  
Section 5.  
4 Corpora 
In order to carry out the present research for 
detecting and solving the complexities of opi-
nion QA, we employed two corpora of blog 
posts: EmotiBlog (Boldrini et al, 2009a) and 
the TAC 2008 Opinion Pilot test collection (part 
of the Blog06 corpus). 
The TAC 2008 Opinion Pilot test collection is 
composed by documents with the answers to the 
opinion questions given on 25 targets. EmotiB-
log is a collection of blog posts in English ex-
tracted form the Web. As a consequence, it 
represents a genuine example of this textual ge-
nre. It consists in a monothematic corpus about 
the Kyoto Protocol, annotated with the im-
proved version of EmotiBlog (Boldrini et al, 
2009b). It is well know that Opinion Mining 
(OM) is a very complex task due to the high 
variability of the language employed. Thus, our 
objective is to build an annotation model that is 
able to capture the whole range of phenomena 
specific to subjectivity expression. Additional 
criteria employed when choosing the elements 
to be annotated were effectiveness and noise 
minimization. Thus, from the first version of the 
model, the elements which did not prove to be 
statistically relevant have been eliminated. The 
elements that compose the improved version of 
the annotation model are presented in Table 1.   
 
29
Elements Description 
Obj. speech Confidence, comment, source, target. 
Subj. speech Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Adjec-
tives/Adverbs 
Confidence, comment, level, emotion, 
phenomenon, modifier/not, polarity, 
source and target. 
Verbs/ Names Confidence, comment, level, emotion, 
phenomenon, polarity, mode, source 
and target. 
Anaphora Confidence, comment, type, source and 
target. 
Capital letter/ 
Punctuation 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Phenomenon Confidence, comment, type, colloca-
tion, saying, slang, title, and rhetoric. 
Reader/Author 
Interpr. (obj.) 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Emotions Confidence, comment, accept, anger, 
anticipation, anxiety, appreciation, bad, 
bewilderment, comfort, compassion? 
Table 1: EmotiBlog improved structure 
 
The first distinction consists in separating objec-
tive and subjective speech. Subsequently, a fin-
er-grained annotation is employed for each of 
the two types of data. Objective sentences are 
annotated with source and target (when neces-
sary, also the level of confidence of the annota-
tor and a comment). Subjective elements can be 
annotated at a sentence level, but they also have 
to be labeled at a word and/or phrase level. 
EmotiBlog also contains annotations of anapho-
ra at a cross-document level (to interpret the 
storyline of the posts) and the sentence type 
(simple sentence or title, but also saying or col-
location). Finally, the Reader and the Writer 
interpretation have to be marked in objective 
sentences. These elements are employed to 
mark and interpret correctly an apparent objec-
tive discourse, whose aim is to implicitly ex-
press an opinion (e.g. ?The camera broke in two 
days?). The first is useful to extract what is the 
interpretation of the reader (for example if the 
writer says The result of their governing was an 
increase of 3.4% in the unemployment rate in-
stead of The result of their governing was a dis-
aster for the unemployment rate) and the second 
to understand the background of the reader (i.e.. 
These criminals are not able to govern instead 
of saying the x party is not able to govern). 
From this sentence, for example, the reader can 
deduce the political ideas of the writer. The 
questions whose answers are annotated with 
EmotiBlog are the subset of opinion questions in 
English presented in (Balahur et al, 2009). The 
complete list of questions is shown in Table 2.  
 
N Question 
2 What motivates people?s negative opinions on the 
Kyoto Protocol? 
5 What are the reasons for the success of the Kyoto 
Protocol? 
6 What arguments do people bring for their criticism 
of media as far as the Kyoto Protocol is concerned? 
7 Why do people criticize Richard Branson? 
11 What negative opinions do people have on Hilary 
Benn? 
12 Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol? 
15 What alternative environmental friendly resources 
do people suggest to use instead of gas en the future? 
16 Is Arnold Schwarzenegger pro or against the reduc-
tion of CO2 emissions? 
18 What improvements are proposed to the Kyoto Pro-
tocol? 
19 What is Bush accused of as far as political measures 
are concerned? 
20 What initiative of an international body is thought to 
be a good continuation for the Kyoto Protocol? 
Table 2: Questions over the EmotiBlog  
corpus 
 
The main difference between the two corpora 
employed is that Emotiblog is monothematic, 
containing only posts about the Kyoto Protocol, 
while the TAC 2008 corpus contains documents 
on a multitude of subjects. Therefore, different 
techniques must be adjusted in order to treat 
each of them.  
5 Experiments 
5.1 Question Analysis 
In order to be able to extract the correct answer 
to opinion questions, different elements must be 
considered. As stated in (Balahur et al, 2009) 
we need to determine both the expected answer 
type (EAT) of the question ? as in the case of 
factoid ones - as well as new elements ? such as 
expected polarity type (EPT). However, opi-
nions are directional ? i.e., they suppose the ex-
istence of a source and a target to which they 
are addressed. Thus, we introduce two new 
elements in the question analysis ? expected 
source (ES) and expected target (ET). These 
two elements are selected by applying SR and 
choosing the source as the agent in the sentence 
and the direct object (patient) as the target of the 
opinion. Of course, the source and target of the 
30
opinions expressed can also be found in other 
roles, but at this stage we only consider these 
cases. The expected answer type (EAT) (e.g. 
opinion or other) is determined using Machine 
Learning (ML) using Support Vector Machine 
(SVM), by taking into account the interrogation 
formula, the subjectivity of the verb and the 
presence of polarity words in the target SR. In 
the case of expected opinionated answers, we 
also compute the expected polarity type (EPT) ? 
by applying OM on the affirmative version of 
the question. An example of such a transforma-
tion is: given the question ?What are the rea-
sons for the success of the Kyoto Protocol??, 
the affirmative version of the question is ?The 
reasons for the success of the Kyoto Protocol 
are X?.  
5.2 Candidate Snippet Retrieval 
In the answer retrieval stage, we employ four 
strategies:  
1. Using the JIRS (JAVA Information Re-
trieval System) IR engine (G?mez et al, 
2007) to find relevant snippets. JIRS re-
trieves passages (of the desired length), 
based on searching the question struc-
tures (n-grams) instead of the keywords, 
and comparing them.  
2. Using the ?Yahoo? search engine to re-
trieve the first 20 documents that are 
most related to the query. Subsequently, 
we apply LSA on the retrieved docu-
ments and extract the words that are 
most related to the topic. Finally, we 
expand the query using words that are 
very similar to the topic and retrieve 
snippets that contain at least one of 
them and the ET. 
3. Generating equivalent expressions for 
the query, using the DIRT paraphrase 
collection (Lin and Pantel, 2001) and 
retrieving candidate snippets of length 1 
and 3 (length refers to the number of 
sentences retrieved) that are similar to 
each of the new generated queries and 
contain the ET. Similarity is computed 
using the cosine measure. Examples of 
alternative queries for ?People like 
George Clooney? are ?People adore 
George Clooney?, ?People enjoy 
George Clooney?, ?People prefer 
George Clooney?. 
4. Enriching the equivalent expressions for 
the query in 3. with the topic-related 
words discovered in 2. using LSA. 
5.3 Polarity and topic-polarity classifica-
tion of snippets 
In order to determine the correct answers from 
the collection of retrieved snippets, we must 
filter for the next processing stage only the can-
didates that have the same polarity as the ques-
tion EPT. For polarity detection, we use a com-
bined system employing SVM ML on unigram 
and bigram features trained on the NTCIR 
MOAT 7 data and an unsupervised lexicon-
based system. In order to compute the features 
for each of the unigrams and bigrams, we com-
pute the tf-idf scores. 
The unsupervised system uses the Opinion 
Finder lexicon to filter out subjective sentences 
? that contain more than two subjective words 
or a subjective word and a valence shifter (ob-
tained from the General Inquirer resource). Sub-
sequently, it accounts for the presence of opi-
nionated words from four different lexicons ? 
MicroWordNet (Cerini et al, 2007), WordNet 
Affect (Strapparava and Valitutti, 2004) Emo-
tion Triggers (Balahur and Montoyo, 2008) and 
General Inquirer (Stone et al, 1966). For the 
joint topic-polarity analysis, we first employ 
LSA to determine the words that are strongly 
associated to the topic, as described in Section 
5.2 (second list item). Consequently, we com-
pute the polarity of the sentences that contain at 
least one topic word and the question target. 
5.4 Filtering using SR 
Finally, answers are filtered using the Semrol 
system for SR labeling described in (Moreda, 
2008). Subsequently, we filter all snippets with 
the required target and source as agent or pa-
tient. Semrol receives as input plain text with 
information about grammar, syntax, word 
senses, Named Entities and constituents of each 
verb. The system output is the given text, in 
which the semantic roles information of each 
constituent is marked. Ambiguity is resolved 
31
depending on the machine algorithm employed, 
which in this case is TIMBL5. 
6 Evaluation and Discussion 
We evaluate our approaches on both the Emo-
tiBlog question collection, as well as on the 
TAC 2008 Opinion Pilot test set. We compare 
them against the performance of the system eva-
luated in (Balahur et al, 2009) and the best 
(Copeck et al, 2008) and worst (Varma et al, 
2008) scoring systems (as far as F-measure is 
concerned) in the TAC 2008 task.  For both the 
TAC 2008 and EmotiBlog sets of questions, we 
employ the SR system in SA and determine the 
ES, ET and EPT. Subsequently, for each of the 
two corpora, we retrieve 1-phrase and 3-phrase 
snippets. The retrieval of the of the EmotiBlog 
candidate snippets is done using query expan-
sion with LSA and filtering according to the ET. 
Further on, we apply sentiment analysis (SA) 
using the approach described in Section 5.3 and 
select only the snippets whose polarity is the 
same as the determined question EPT. The re-
sults are presented in Table 3.  
 
Q 
N
o. 
N
o.  
A 
Baseline 
(Balahur et al, 
2009) 
1 phrase + 
ET+SA 
3 phrases 
+ET+SA 
  @ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@
2
0 
2 5 0 2 3 4 1 2 3 4 1 2 3 4 
5 1
1 
0 0 0 0 0 2 2 2 1 2 3 4 
6 2 0 0 1 2 1 1 2 2 0 1 2 2 
7 5 0 0 1 3 1 1 1 3 0 2 2 4 
1
1 
2 1 1 1 1 0 0 0 0 0 0 0 1 
1
2 
3 0 1 1 1 0 1 2 3 0 0 1 2 
1
5 
1 0 0 1 1 0 0 1 1 1 1 1 1 
1
6 
6 1 4 4 4 0 1 1 2 1 2 2 6 
1
8 
1 0 0 0 0 0 0 0 0 0 0 0 0 
1
9 
2
7 
1 5 6 1
8 
0 1 1 2 0 1 1 1 
2
0 
4 0 0 0 0 0 0 1 1 0 0 1 2 
Table 3: Results for questions over  
EmotiBlog 
 
                                                 
5
http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.2_Manual
.pdf and http://ilk.uvt.nl/timbl/ 
The retrieval of the TAC 2008 1-phrase and 3-
phrase candidate snippets was done using JIRS 
and, in a second approach, using the cosine si-
milarity measure between alternative queries 
generated using paraphrases and candidate 
snippets. Subsequently, we performed different 
evaluations, in order to assess the impact of us-
ing different resources and tools. Since the TAC 
2008 had a limit of the output of 7000 charac-
ters, in order to compute a comparable F-
measure, at the end of each processing chain, 
we only considered the snippets for the 1-phrase 
retrieval and for the 3-phases one until this limit 
was reached. 
1. In the first evaluation, we only apply the 
sentiment analysis tool and select the snip-
pets that have the same polarity as the ques-
tion EPT and the ET is found in the snippet.  
(i.e. What motivates peoples negative opi-
nions on the Kyoto Protocol? The Kyoto 
Protocol becomes deterrence to economic 
development and international cooperation/ 
Secondly, in terms of administrative aspect, 
the Kyoto Protocol is difficult to implement.  
- same EPT and ET) 
We also detected cases of same polarity but 
no ET, e.g. These attempts mean annual ex-
penditures of $700 million in tax credits in 
order to endorse technologies, $3 billion in 
developing research and $200 million in 
settling technology into developing coun-
tries ? EPT negative but not same ET. 
2. In the second evaluation, we add the result 
of the LSA process to filter out the snippets 
from 1., containing the words related to the 
topic starting from the retrieval performed 
by Yahoo, which extracts the first 20 docu-
ments about the topic. 
3. In the third evaluation, we filter the results 
in 2 by applying the Semrol system and set-
ting the condition that the ET and ES are the 
agent or the patient of the snippet. 
4. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases (as explained in 
the third method in section 5.2.). We subse-
quently filtered these results based on their 
polarity  (so that it corresponds to the EPT) 
and on the condition that the source and tar-
get of the opinion (identified through SRL 
using Semrol) correspond to the ES and ET.  
32
5. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases, enriched with the 
topic words determined using LSA. We 
subsequently filtered these results based on 
their polarity (so that it corresponds to the 
EPT) and on the condition that the source 
and target of the opinion (identified through 
SRL using Semrol) correspond to the ES 
and ET.  
 
System F-measure 
Best TAC 0.534 
Worst TAC 0.101 
JIRS + SA+ET (1 phrase)  0.377 
JIRS + SA+ET (3 phrases)  0.431 
JIRS + SA+ET+LSA (1 phrase)  0.489 
JIRS + SA+ET+LSA (3 phrases)  0.505 
JIRS + SA+ET+LSA+SR (1 
phrase)  
0. 533 
JIRS + SA+ET+LSA+SR (3 
phrases) 
0.571 
PAR+SA+ET+SR(1 phrase) 0.345 
PAR+SA+ET+SR(2 phrase) 0.386 
PAR_LSA+SA+ET+SR (1 phra-
se) 
0.453 
PAR_LSA+SA+ET+SR (3 phra-
ses) 
0.434 
Table 4: Results for the TAC 2008 test set 
 
From the results obtained (Table 3 and Table 4), 
we can draw the following conclusions. Firstly, 
the hypothesis that OQA requires the retrieval 
of longer snippets was confirmed by the im-
proved results, both in the case of EmotiBlog, as 
well as the TAC 2008 corpus. Secondly, opi-
nion questions require the use of joint topic-
sentiment analysis. As we can see from the re-
sults, the use of topic-related words when com-
puting of the affect influences the results in a 
positive manner and joint topic-sentiment anal-
ysis is especially useful for the cases of ques-
tions asked on a monothematic corpus. Thirdly, 
another conclusion that we can draw is that tar-
get and source detection are highly relevant 
steps at the time of answer filtering, not only 
helping the more accurate retrieval of answers, 
but also at placing at the top of the retrieval the 
relevant results (as more relevant information is 
contained within these 7000 characters). The 
use of paraphrases at the retrieval stage was 
shown to produce a significant drop in results, 
which we explain by the noise introduced and 
the fact that more non-relevant answer candi-
dates were introduced among the results. None-
theless, as we can see from the overall relatively 
low improvement in the results, much remains 
to be done in order to appropriately tackle 
OQA. As seen in the results, there are still ques-
tions for which no answer is found (e.g. 18). 
This is due to the fact that the treatment of such 
questions requires the use of inference tech-
niques that are presently unavailable (i.e. define 
terms such as ?improvement?, possibly as ?X 
better than Y?, in which case opinion extraction 
from comparative sentences should be intro-
duced in the model).  
The results obtained when using all the compo-
nents for the 3-sentence long snippets signifi-
cantly improve the results obtained by the best 
system participating in the TAC 2008 Opinion 
Pilot competition (determined using a paired t-
test for statistical significance, with confidence 
level 5%). Finally, from the analysis of the er-
rors, we could see that even though some tools 
are in theory useful and should produce higher 
improvements ? such as SR ? their performance 
in reality does not produce drastically higher 
results. The idea to use paraphrases for query 
expansion also proved to decrease the system 
performance. From preliminary results obtained 
using JavaRap6  for coreference resolution, we 
also noticed that the performance of the OQA 
lowered, although theoretically it should have 
improved. 
7 Conclusions ad Future Work 
In this paper, we presented and evaluated differ-
ent methods and techniques with the objective 
of improving the task of QA in the context of 
opinion data. From the evaluations performed 
using different NLP resources and tools, we 
concluded that joint topic-sentiment analysis, as 
well as the target and source identification, are 
crucial for the correct performance of this task. 
We have also demonstrated that by retrieving 
longer answers, the results have improved. We 
tested, within a simple setting, the impact of 
using paraphrases in the context of opinion 
questions and saw that their use lowered the 
system results. Although such paraphrase col-
                                                 
6http://wing.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.ht
m 
33
lections include a lot of noise and have been 
shown to decrease system performance even in 
the case of factual questions, we believe that 
other types of paraphrasing methods should be 
investigated in the context of OQA. We thus 
showed that opinion QA requires the develop-
ment of appropriate strategies at the different 
stages of the task (recognition of subjective 
questions, detection of subjective content of the 
questions, source and target identification, re-
trieval and classification of the candidate an-
swer data). Due to the high level of complexity 
of subjective language, our future work will be 
focused on testing higher-performing tools for 
coreference resolution, other (opinion) paraph-
rases collections and paraphrasing methods and 
the employment of external knowledge sources 
that refine the semantics of queries. We also 
plan to include other SA methods and extend 
the semantic roles considered for ET and ES, 
with the purpose of checking if they improve or 
not the performance of the QA system. 
 
Acknowledgements 
This paper has been partially supported by Mi-
nisterio de Ciencia e Innovaci?n - Spanish Gov-
ernment (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat Va-
lenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/286). 
References 
Balahur, A. and Montoyo, A. 2008. Applying a 
Culture Dependent Emotion Triggers Data-
base for Text Valence and Emotion 
Classification. In Proceedings of the AISB 
2008 Symposium on Affective Language in 
Human and Machine, Aberdeen, Scotland. 
Balahur, A., Lloret, E., Ferr?ndez, O., Montoyo, 
A., Palomar, M., and Mu?oz, R. 2008. The 
DLSIUAES Team?s Participation in the TAC 
2008 Tracks. In Proceedings of the Text 
Analysis Conference 2008 Workshop. 
Balahur, A., Boldrini, E., Montoyo A. and 
Mart?nez-Barco P. 2009. Opinion and Generic 
Question Answering Systems: a Performance 
Analysis. In Proceedings of ACL. Singapur.  
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and  Montoyo. A. 2009a. EmotiBlog: an An-
notation Scheme for Emotion Detection and 
Analysis in Non-traditional Textual Genre. In 
Proceedings of DMIN 2009, Las Vegas. Ne-
vada. 
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and Montoyo. A. 2009b. EmotiBlog: a fine-
grained model for emotion detection in non-
traditional textual genre. In Proceedings of 
WOMSA 2009. Seville. 
Cardie, C., Wiebe, J., Wilson, T. and Litman, D. 
2003. Combining Low-Level and Summary 
Representations of Opinions for Multi-
Perspective Question Answering. AAAI 
Spring Symposium on New Directions in 
Question Answering. 
Cerini, S., Compagnoni, V., Demontis, A., 
Formentelli, M. and Gandini, C. 2007. Mi-
cro-WNOp: A gold standard for the evalua-
tion of automatically compiledlexical re-
sources for opinion mining. In: A.Sanso 
(ed.): Language resources and linguistic 
theory: Typology, Second Language Acqui-
sition, English Linguistics. Milano. IT. 
Copeck, T.,  Kazantseva, A., Kennedy, A., 
Kunadze, A., Inkpen, D. and Szpakowicz, 
S. 2008. Update Summary Update. In Pro-
ceedings of the Text Analysis Conference 
(TAC) 2008. 
Cui, H., Mittal, V. and Datar, M. 2006. Com-
parative Experiments on Sentiment Classifi-
cation for Online Product Review. Proceed-
ings, The Twenty-First National Conference 
on Artificial Intelligence and the Eighteenth 
Innovative Applications of Artificial Intelli-
gence Conference. Boston, Massachusetts, 
USA. 
G?mez, J.M., Rosso, P. and Sanchis, E. 2007. 
JIRS Language-Independent Passage Re-
trieval System: A Comparative Study. 5th 
International Conference on Natural 
Language Proceeding (ICON 2007). 
Kabadjov, M., Balahur, A. And Boldrini, E. 
2009. Sentiment Intensity: Is It a Good 
Summary Indicator?. Proceedings of the 4th 
Language Technology Conference LTC, pp. 
380-384. Poznan, Poland, 6-8.11.2009. 
Kim, S. M. and Hovy, E. 2005. Identifying 
Opinion Holders for Question Answering in 
Opinion Texts. Proceedings of the 
Workshop on Question Answering in 
Restricted Domain at the Conference of the 
American Association of Artificial 
Intelligence (AAAI-05).  Pittsburgh, PA. 
34
Li, F., Zheng, Z.,Yang T., Bu, F., Ge, R., Zhu, 
X., Zhang, X., and Huang, M. 2008. THU 
QUANTA at TAC 2008. QA and RTE track. 
In Proceedings of the Text Analysis 
Conference (TAC). 
Lin, D. and Pantel, P. 2001. Discovery of 
Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-
360. 
Moreda. P. 2008. Los Roles Sem?nticos en la 
Tecnolog?a del Lengauje Humano: Anota-
ci?n y Aplicaci?n. Doctoral Thesis. Univer-
sity of Alicante. 
Pustejovsky, J. and Wiebe, J. 2006. Introduction 
to Special Issue on Advances in Question 
Answering. Language Resources and Eval-
uation (2005), (39). 
Shen, D., Wiegand, M., Merkel, A., Kazalski, 
S., Hunsicker, S., Leidner, J. L. and 
Klakow, D. 2007. The Alyssa System at 
TREC QA 2007: Do We Need Blog06? In 
Proceedings of the Sixteenth Text Retrieval 
Conference (TREC 2007), Gaithersburg, 
MD, USA. 
Strapparava, C. and Valitutti, A. 2004. Word-
Net-Affect: an affective extension of Word-
Net. In Proceedings of 4th International Con-
ference on Language Resources and Evalua-
tion (LREC 2004), pages 1083 ? 1086, Lis-
bon. 
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. 
Multiperspective question answering using 
the opqa corpus. In Proceedings of the 
Human Language Technology Conference 
and the Conference on Empirical Methods 
in Natural Language Processing 
(HLT/EMNLP 2005). 
Varma, V., Pingali, P., Katragadda, S., Krishna, 
R., Ganesh, S., Sarvabhotla, K. Garapati, 
H., Gopisetty, H., Reddy, K. and 
Bharadwaj, R. 2008. IIIT Hyderabad at 
TAC 2008. In Proceedings of Text Analysis 
Conference (TAC).  
Wenjie, L., Ouyang, Y., Hu, Y. and Wei, F. 
2008. PolyU at TAC 2008. In Proceedings 
of the Text Analysis Conference (TAC). 
Wiebe, J., Wilson, T., and Cardie, C. 2005. 
Annotating expressions of opinions and 
emotions in language. Language Resources 
and Evaluation, volume 39, issue 2-3, pp. 
165-210. 
Wilson, T., J. Wiebe, and Hoffmann, P. 2005. 
Recognizing Contextual Polarity in Phrase-
level sentiment Analysis. In Proceedings of 
the Human Language Technologies 
Conference/Conference on Empirical 
Methods in Natural Language Processing 
(HLT/ EMNLP). 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions. In Proceedings of 
EMNLP-03. 
Wiebe, J., Wilson, T., and Cardie, C. (2005). 
Annotating expressions of opinions and 
emotions in language. In Language 
Resources and Evaluation. Vol. 39. 
35
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 427?432,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UMCC-DLSI: Integrative resource for disambiguation task
Yoan Gutie?rrez and Antonio Ferna?ndez
DI, University of Matanzas
Autopista a Varadero km 31/2
Matanzas, Cuba
yoan.gutierrez,antonio.fernandez@umcc.cu
Andre?s Montoyo and Sonia Va?zquez
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
montoyo,svazquez@dlsi.ua.es
Abstract
This paper describes the UMCC-DLSI
system in SemEval-2010 task number 17
(All-words Word Sense Disambiguation
on Specific Domain). The main purpose
of this work is to evaluate and compare
our computational resource of WordNet?s
mappings using 3 different methods:
Relevant Semantic Tree, Relevant
Semantic Tree 2 and an Adaptation of
k-clique?s Technique. Our proposal is
a non-supervised and knowledge-based
system that uses Domains Ontology and
SUMO.
1 Introduction
Ambiguity is the task of building up multiple
alternative linguistic structures for a single
input (Kozareva et al, 2007). Word Sense
Disambiguation (WSD) is a key enabling-
technology that automatically chooses the
intended sense of a word in context. In this task,
one of the most used lexical data base is WordNet
(WN) (Fellbaum, 1998). WN is an online lexical
reference system whose design is inspired by
current psycholinguistic theories of human lexical
memory. Due to the great popularity of WN
in Natural Language Processing (NLP), several
authors (Magnini and Cavaglia, 2000), (Niles
and Pease, 2001), (Niles and Pease, 2003),
(Valitutti, 2004) have proposed to incorporate to
the semantic net of WN, some taxonomies that
characterize, in one or several concepts, the senses
of each word. In spite of the fact that there have
been developed a lot of WordNet?s mappings,
there isn?t one unique resource to integrate all
of them in a single system approach. To solve
this need we have developed a resource that joins
WN1, the SUMO Ontology2, WordNet Domains3
and WordNet Affect4. Our purpose is to test the
advantages of having all the resources together for
the resolution of the WSD task.
The rest of the paper is organized as follows.
In Section 2 we describe the architecture of the
integrative resource. Our approach is shown in
Section 3. Next section presents the obtained
results and a discussion. And finally the
conclusions in Section 5.
2 Background and techniques
2.1 Architecture of the integrative resource
Our integrative model takes WN 1.6 as nucleus
and links to it the SUMO resource. Moreover,
WordNet Domains 2.0 (WND) and WordNet
Affect 1.1 (WNAffects) are also integrated but
mapped instead to WN 2.0. From the model
showed in Figure 1, a computational resource has
been built in order to integrate the mappings above
mentioned.
The model integrator?s proposal provides
a software that incorporates bookstores of
programming classes, capable to navigate inside
the semantic graph and to apply any type of
possible algorithm to a net. The software
architecture allows to update WN?s version.
In order to maintain the compatibility with other
resources mapped to WN, we have decided to use
WN 1.6 version. However, the results can be
offered in anyone of WN?s versions.
1http://www.cogsci.princeton.edu/ wn/
2http://suo.ieee.org
3http://wndomains.fbk.eu/
4http://wndomains.fbk.eu/wnaffect.html
427
 Figure 1: WordNet integrative model
2.2 The k-clique?s Technique
Formally, a clique is the maximum number of
actors who have all possible ties presented among
themselves. A ?Maximal complete sub-graph? is
such a grouping, expanded to include as many
actors as possible.
?A k-clique is a subset of vertices C such that,
for every i, j ? C, the distance d(i, j)
k
. The 1-
clique is identical to a clique, because the distance
between the vertices is one edge. The 2-clique
is the maximal complete sub-graph with a path
length of one or two edges?. (Cavique et al, 2009)
3 The Proposal
Our proposal consists in accomplishing three runs
with different algorithms. Both first utilize the
domain?s vectors; the third method utilizes k-
cliques? techniques.
This work is divided in several stages:
1. Pre-processing of the corpus (lemmatization
with Freeling) (Atserias et al, 2006).
2. Context selection (For the first (3.1), and
the third (3.3) run the context window was
constituted by the sentence that contains
the word to disambiguate; in the second
run the context window was constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the
next one).
3. Obtaining the domain vector, this vector is
used in first and the second runs (when
the lemma of the words in the analyzed
sentence is obtained, the integrative resource
of WordNet?s Mappings is used to get the
respective senses from each lemma).
4. Obtaining the all resource vector: SUMO,
Affects, and Domain resource. This is only
for the third run (3.3).
5. Relevant Semantic Tree construction
(Addition of concepts parents to the vectors.
For the first (3.1) and second (3.2) runs only
Domain resource is used; for the third (3.3)
run all the resources are used).
6. Selection of the correct senses (the first and
the second runs use the same way to do the
selection; the third run is different. We make
an exception: For the verb ?be? we select the
sense with the higher frequency according to
Freeling.
3.1 Relevant Semantic Tree
With this proposal we measure how much a
concept is correlated to the sentence, similar to
Reuters Vector (Magnini et al, 2002), but with
a different equation. This proposal has a partial
similarity with the Conceptual Density (Agirre
and Rigau, 1996) and DRelevant (Va?zquez et al,
2004) to get the concepts from a hierarchy that
they associate with the sentence.
In order to determine the Association Ratio
(RA) of a domain in relation to the sentence, the
Equation 1 is used.
RA(D, f) =
n
?
i=1
RA(D, f
i
) (1)
where:
RA(D,w) = P (D,w) ? log
2
P (D,w)
P (D)
(2)
f : is a set of words w.
f
i
: is a i-th word of the phrase f .
P (D,w): is joint probability distribution.
P (D): is marginal probability.
From now, vectors are created using the
Senseval-2?s corpus. Next, we show an example:
For the phrase: ?But it is unfair to dump
on teachers as distinct from the educational
establishment?.
By means of the process Pres-processing
analyzed in previous stage 1 we get the lemma and
the following vector.
428
Phrase [unfair; dump; teacher, distinct,
educational; establishment]
Each lemma is looked for in WordNet?s
integrative resource of mappings and it is
correlated with concepts of WND.
Vector
RA Domains
0.9 Pedagogy
0.9 Administration
0.36 Buildings
0.36 Politics
0.36 Environment
0.36 Commerce
0.36 Quality
0.36 Psychoanalysis
0.36 Economy
Table 1: Initial Domain Vector
After obtaining the Initial Domain Vector we
apply the Equation 3 in order to build the Relevant
Semantic Tree related to the phrase.
DN(CI,Df) = RA CI ?
MP (CI,Df)
TD
(3)
Where DN : is a normalized distance
CI: is the Initial Concept which you want to
add the ancestors.
Df : is Parent Domain.
RA CI: is a Association Ratio of the child
Concept.
TD: is Depth of the hierarchic tree of the
resource to use.
MP : is Minimal Path.
Applying the Equation 3 the algorithm to decide
which parent domain will be added to the vector is
shown here:
if (DN(CI,Df) > 0)
{
if ( Df not exist)
Df is added to the vector with DN value;
else
Df value = Df value + DN ;
}
As a result the Table 2 is obtained.
This vector represents the Domain tree
associated to the phrase.
After the Relevant Semantic Tree is obtained,
the Domain Factotum is eliminated from the tree.
Due to the large amount of WordNet synsets,
Vector
RA Domains
1.63 Social Science
0.9 Administration
0.9 Pedagogy
0.8 RootDomain
0.36 Psychoanalysis
0.36 Economy
0.36 Quality
0.36 Politics
0.36 Buildings
0.36 Commerce
0.36 Environment
0.11 Factotum
0.11 Psychology
0.11 Architecture
0.11 Pure Science
Table 2: Final Domain Vector
 
Figure 2: Relevant semantic tree
that do not belong to a specific domain, but
rather they can appear in almost all of them, the
Factotum domain has been created. It basically
includes two types of synsets: Generic synsets,
which are hard to classify in a particular domain;
and Stop Senses synsets which appear frequently
in different contexts, such as numbers, week
days, colors, etc. (Magnini and Cavaglia, 2000),
(Magnini et al, 2002). Words that contain this
synsets are frequently in the phrases, therefore the
senses associated to this domain are not selected.
After processing the patterns that characterize
the sentence, the following stage is to determine
the correct senses, so that the next steps ensue:
1. Senses that do not coincide with the
grammatical category of Freeling are
removed.
429
2. For each word to disambiguate all candidate
senses are obtained. Of each sense
the relevant vector are obtained using the
Equation 4, and according to the previous
Equation 3 parent concepts are added.
RA(D, s) = P (D, s) ? log
2
P (D, s)
P (D)
(4)
where s: is a sense of word.
P (D, s): is joint probability distribution
between Domain concept D and the sense s.
P (D): is marginal probability of the Domain
concept.
3. The one that accumulates the bigger value of
relevance is assigned as correct sense. The
following process is applied:
For each coincidence of the elements in the
senses? domain vector with the domain vector
of the sentence, the RA value of the analyzed
elements is accumulated. The process is
described in the Equation 5.
AC(s, V RA) =
?
k
V RA[V s
k
]
?
i=1
V RA
i
(5)
where AC: The RA value accumulated for
the analyzed elements.
V RA: Vector of relevant domains of the
sentence with the format: V RA [domain ?
value RA].
V s: Vector of relevant domain of the sense
with the format: V s [domain].
V s
k
: Is a k-th domain of the vector V s.
V RA[V s
k
]: Represents the value of RA
assigned to the domain V sk for the value
V RA.
The
?
i=1
V RA
i
term normalizes the result.
3.2 Relevant Semantic Tree 2
This run is the same as the first one with a
little difference, the context window is constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the next
one.
3.3 Adaptation of k-clique?s technique to the
WSD
They are applied, of the section 3, the steps from
the 1 to the 5, where the semantic trees of concepts
are obtained.
Then they are already obtained for all the
words of the context, all the senses discriminated
according to Freeling (Atserias et al, 2006).
Then a sentence?s net of knowledge is built
by means of minimal paths among each sense
and each concept at trees. Next the k-clique?s
technique is applied to the net of knowledge to
obtain cohesive subsets of nodes.
To obtain the correct sense of each word it is
looked, as proposed sense, the sense belonging to
the subset containing more quantities of nodes and
if it has more than a sense for the same word,
the more frequent sense is chosen according to
Freeling.
4 Results and Discussion
The conducted experiments measure the
influence of the aforementioned resources in
the disambiguation task. We have evaluated them
individually and as a whole. In the Table 3 it
is represented each one of the inclusions and
combinations experimented with the Relevant
Semantic Tree method.
Resources Precision Recall Attempted
WNAffect 0.242 0.237 97.78%
SUMO 0.267 0.261 98.5%
WND 0.328 0.322 98.14%
WND &
SUMO
0.308 0.301 97.78%
WND &
SUMO &
WNAffect
0.308 0.301 97.78%
Table 3: Evaluation of integrated resources
As it can be observed, in the evaluation for
specific domain corpus the best results are reached
when only domain resource is used. But this
is not a conclusion about the resources inclusion
because the use of this method for global domain,
for example with the task English All words from
Senseval-2 (Agirre et al, 2010), the experiment
adding all the resources showed good results. This
is due to the fact that the global domain includes
information of different contexts, exactly what
is representing in the mentioned resources. For
430
this reason, in the experiment with global domain
and the inclusion of all the resource obtained
better results than using this method with specific
domain, 42% of recall and 45% of precision
(Gutie?rrez, 2010).
For example, with the k-clique?s technique,
utilizing the English All word task from Senseval-
2?s corpus, the results for the test with global
dominion were: with single domain inclusion 40
% of precision and recall; but with the three
resources 41.7 % for both measures.
Table 4 shows the obtained results for the test
data set. The average performance of our system
is 32% and we ranked on 27-th position from
27 participating systems. Although, we have
used different sources of information and various
approximations, in the future we have to surmount
a number of obstacles.
One of the limitations comes from the usage
of the POS-tagger Freeling which introduces
some errors in the grammatical discrimination.
Representing a loss of 3.7% in the precision of our
system.
The base of knowledge utilized in the task was
WordNet 1.6; but the competition demanded the
results with WordNet 3.0. In order to achieve
this we utilized mappings among versions where
119 of 1398 resulting senses emitted by Semeval-
2 were did not found. This represents an 8.5%.
In our proposal, the sense belonging to the
Factotum Domain was eliminated, what disabled
that the senses linked to this domain went
candidates to be recovered. 777 senses of 1398
annotated like correct for Semeval-2 belong to
domain Factotum, what represents that the 66%
were not recovered by our system. Considering
the senses that are not correlated to Factotum,
that is, that correlate to another domains, we are
speaking about 621 senses to define; The system
would emit results of a 72,4%. Senses selected
correctly were 450, representing a 32%. However,
189 kept on like second candidates to be elected.
This represents a 13.5%. If a technique of more
precise decision takes effect, the results of the
system could be increased largely.
5 Conclusion and future works
For our participation in the Semeval-2 task
17 (All-words Word Sense Disambiguation on
Specific Domain), we presented three methods
for disambiguation approach which uses an
Methods Precision Recall Attempted
Relevant
Domains
Tree
0.328 0.322 98.14%
Relevant
Semantic
Tree 2
0.321 0.315 98.14%
Relevant
Cliques
0.312 0.303 97.35%
Table 4: Evaluation results
integrative resource of WordNet mappings. We
conducted an experimental study with the trail
data set, according to which the Relevant Semantic
Tree reaches the best performance. Our current
approach can be improved with the incorporation
of more granularities in the hierarchy of WordNet
Domains. Because it was demonstrated that
to define correct senses associated to specific
domains an improvement of 72.4% is obtained.
At this moment, only domain information is used
in our first and second method. Besides was
demonstrated for specific domains, the inclusion
of several resources worsened the results with the
first and second proposal method, the third one has
been not experimented yet. Despite the fact that
we have knowledge of SUMO, WordNet-Affect
and WordNet Domain in our third method we still
not obtain a relevant result.
It would be convenient to enrich our resource
with other resources like Frame-Net, Concept-Net
or others with the objective of characterizing even
more the senses of the words.
Acknowledgments
This paper has been supported partially by
Ministerio de Ciencia e Innovacio?n - Spanish
Government (grant no. TIN2009-13391-C04-
01), and Conselleria d?Educacio? - Generalitat
Valenciana (grant no. PROMETEO/2009/119 and
ACOMP/2010/288).
References
Eneko Agirre and German Rigau. 1996. Word
sense disambiguation using conceptual density. In
Proceedings of the 16th International Conference
on Computational Linguistic (COLING?96),
Copenhagen, Denmark.
Eneko Agirre, Oier Lopez de Lacalle, Christiane
Fellbaum, Shu-kai Hsieh, Maurizio Tesconi, Monica
431
Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word
sense disambiguation on a specific domain. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010), Association
for Computational Linguistics.
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonza?lez, Llu??s Padro?, and Muntsa Padro?.
2006. Freeling 1.3: Syntactic and semantic services
in an open-source nlp library. In Proceedings
of the fifth international conference on Language
Resources and Evaluation (LREC 2006), ELRA.
Lu??s Cavique, Armando B. Mendes, and Jorge M.
Santos. 2009. An algorithm to discover the k-
clique cover in networks. In EPIA ?09: Proceedings
of the 14th Portuguese Conference on Artificial
Intelligence, pages 363?373, Berlin, Heidelberg.
Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Yoan Gutie?rrez. 2010. Resolucio?n de ambiguedad
sema?ntica mediante el uso de vectores de conceptos
relevantes.
Zornitsa Kozareva, Sonia Va?zquez, and Andre?s
Montoyo. 2007. Ua-zsa: Web page clustering on
the basis of name disambiguation. In Semeval I. 4th
International Wordshop on Semantic Evaluations.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating subject field codes into wordnet. In
Proceedings of Third International Conference on
Language Resources and Evaluation (LREC-2000).
Bernardo Magnini, Carlo Strapparava, Giovanni
Pezzulo, and Alfio Gliozzo. 2002. Comparing
ontology-based and corpus-based domain
annotations in wordnet. In Proceedings of the
First International WordNet Conference, pages
146?154.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In FOIS, pages 2?9.
Ian Niles and Adam Pease. 2003. Linking lexicons
and ontologies: Mapping wordnet to the suggested
upper merged ontology. In IKE, pages 412?416.
Ro Valitutti. 2004. Wordnet-affect: an affective
extension of wordnet. In Proceedings of the 4th
International Conference on Language Resources
and Evaluation, pages 1083?1086.
Sonia Va?zquez, Andre?s Montoyo, and German Rigau.
2004. Using relevant domains resource for word
sense disambiguation. In IC-AI, pages 784?789.
432
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 444?447,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
OpAL: Applying Opinion Mining Techniques for the Disambiguation of 
Sentiment Ambiguous Adjectives in SemEval-2 Task 18  
 
Alexandra Balahur 
University of Alicante 
Department of Software and  
Computing Systems 
abalahur@dlsi.ua.es 
Andr?s Montoyo 
University of Alicante 
Department of Software and  
Computing Systems 
montoyo@dlsi.ua.es 
  
 
  
 
Abstract 
 
The task of extracting the opinion expressed in 
text is challenging due to different reasons. 
One of them is that the same word (in particu-
lar, adjectives) can have different polarities 
depending on the context. This paper presents 
the experiments carried out by the OpAL team 
for the participation in the SemEval 2010 Task 
18 ? Disambiguation of Sentiment Ambiguous 
Adjectives. Our approach is based on three dif-
ferent strategies: a) the evaluation of the polar-
ity of the whole context using an opinion min-
ing system; b) the assessment of the polarity of 
the local context, given by the combinations 
between the closest nouns and the adjective to 
be classified; c) rules aiming at refining the lo-
cal semantics through the spotting of modifi-
ers. The final decision for classification is tak-
en according to the output of the majority of 
these three approaches.  The method used 
yielded good results, the OpAL system run 
ranking fifth among 16 in micro accuracy and 
sixth in macro accuracy.   
1 Credits  
This research has been supported by Ministerio 
de Ciencia e Innovaci?n - Spanish Government 
(grant no. TIN2009-13391-C04-01), and Consel-
leria d'Educaci?n-Generalitat Valenciana (grant 
no. PROMETEO/2009/119 and ACOMP/2010/ 
288).  
2 Introduction 
Recent years have marked the beginning and ex-
pansion of the Social Web, in which people free-
ly express and respond to opinion on a whole 
variety of topics. Moreover, at the time of taking 
a decision, more and more people search for in-
formation and opinions expressed on the Web on 
their matter of interest and base their final deci-
sion on the information found (Pang and Lee, 
2008). Nevertheless, the high quantity of data 
that has to be analysed imposed the development 
of specialized Natural Language Processing 
(NLP) systems that automatically extract, classi-
fy and summarize the opinions available on the 
web on different topics. Research in this field, of 
opinion mining (sentiment analysis), has ad-
dressed the problem of extracting and classifying 
opinions from different perspectives and at dif-
ferent levels, depending on various factors. 
While determining the overall opinion on a mov-
ie is sufficient for taking the decision to watch it 
or not, when buying a product, people are inter-
ested in the individual opinions on the different 
product characteristics.  Especially in this con-
text, opinion mining systems are confronted with 
a difficult problem: the fact that the adjectives 
used to express opinion have different polarities 
depending on the characteristic they are men-
tioned with. For example, ?high price? is nega-
tive, while ?high resolution? is positive. There-
fore, specialized methods have to be employed to 
correctly determine the contextual polarity of 
such words and thus accurately assign polarity to 
the opinion.    
This is the aim of the SemEval 2010 Task 18 ? 
Disambiguation of Sentiment Ambiguous Adjec-
tives (Wu and Jin, 2010). In the following sec-
tions, we first present state-of-the art approaches 
towards polarity classification of opinions, sub-
sequently describing our approach in the SemEv-
al task. Finally, we present the results we ob-
tained in the evaluation and our plans for future 
work.    
444
3 State of the Art  
Subjectivity analysis is defined by (Wiebe, 1994) 
as the ?linguistic expression of somebody?s opi-
nions, sentiments, emotions, evaluations, beliefs 
and speculations?. Sentiment analysis, on the 
other hand, is defined as the task of extracting, 
from a text, the opinion expressed on an object 
(product, person, topic etc.) and classifying it as 
positive, negative or neutral. The task of senti-
ment analysis, considered a step further to sub-
jectivity analysis, is more complex than the lat-
ter, because it involves an extra step: the classifi-
cation of the retrieved opinion words according 
to their polarity. There are a series of techniques 
that were used to obtain lexicons of subjective 
words ? e.g. the Opinion Finder lexicon (Wilson 
et al, 2005) and opinion words with associated 
polarity. (Hu and Liu, 2004) start with a set of 
seed adjectives (?good? and ?bad?) and apply 
synonymy and antonymy relations in WordNet. 
A similar approach was used in building Word-
Net Affect (Strapparava and Valitutti, 2004), 
starting from a larger set of seed affective words, 
classified according to the six basic categories of 
emotion (joy, sadness, fear, surprise, anger and 
disgust) and expanding the lexicon using paths in 
WordNet. Another related method was used in 
the creation of SentiWordNet (Esuli and Sebas-
tiani, 2005), using a set of seed words whose po-
larity was known and expanded using gloss simi-
larity. The collection of appraisal terms in (Whi-
telaw et al, 2005), the terms also have polarity 
assigned. MicroWNOp (Cerini et al, 2007), 
another lexicon containing opinion words with 
their associated polarity, was built on the basis of 
a set of terms extracted from the General Inquirer 
lexicon and subsequently adding all the synsets 
in WordNet where these words appear. Other 
methods built sentiment lexicons using the local 
context of words. (Pang et al, 2002) built a lex-
icon of sentiment words with associated polarity 
value, starting with a set of classified seed adjec-
tives and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. (Turney, 2002) classifies words ac-
cording to their polarity on the basis of the idea 
that terms with similar orientation tend to co-
occur in documents. Thus, the author computes 
the Pointwise Mutual Information score between 
seed words and new words on the basis of the 
number of AltaVista hits returned when querying 
the seed word and the word to be classified with 
the ?NEAR? operator. In our work in (Balahur 
and Montoyo, 2008a), we compute the polarity 
of new words using ?polarity anchors? (words 
whose polarity is known beforehand) and Nor-
malized Google Distance (Cilibrasi and Vitanyi, 
2006) scores. Another approach that uses the po-
larity of the local context for computing word 
polarity is (Popescu and Etzioni, 2005), who use 
a weighting function of the words around the 
context to be classified.   
4 The OpAL system at SemEval 2010 
Task 18 
In the SemEval 2010 Task 18, the participants 
were given a set of contexts in Chinese, in which  
14 dynamic sentiment ambiguous adjectives are 
selected. They are: ?|big, ?|small, ?|many, ?
|few, ?|high, ?|low, ?|thick, ?|thin, ?|deep, 
?|shallow, ?|heavy, ? |light, ??|huge, ??
|grave. The task was to automatically classify the 
polarity of these adjectives, i.e. to detect whether 
their sense in the context is positive or negative. 
The contexts were given in two forms: as plain 
text, in which the adjective to be classified was 
marked; in the second for, the text was tokenized 
and the tokens were tagged with part of speech 
(POS). There was no training set provided.  
  Our approach uses a set of opinion mining re-
sources and an opinion mining system that is 
implemented to work for English. This is why, 
the first step we took in our approach was to 
translate the given contexts into English using 
the Google Translator1. In order to perform this 
task, we first split the initial file into 10 smaller 
files, using a specialized program ? GSplit32.  
The OpAL adjective polarity disambiguation 
system combines supervised methods with unsu-
pervised ones.  In order to judge the polarity of 
the adjectives, it uses three types of judgments. 
The first one is the general polarity of the con-
text, determined by our in-house opinion mining 
system - based on SVM machine learning on the 
NTCIR data and the EmotiBlog (Boldrini et al, 
2009) annotations and different subjectivity, opi-
nion and emotion lexica (Opinion Finder, Mi-
croWordNet Opinion, General Inquirer, Word-
Net Affect, emotion triggers (Balahur and Mon-
toyo, 2008b). The second one is the local polari-
ty, given by the highest number of results ob-
tained when issuing queries containing the clos-
est noun with the adjective to be disambiguated 
followed by the conjunction ?AND? and a prede-
fined set of 6 adjectives whose polarity is non-
                                                 
1 http://translate.google.com/ 
2 www.gdgsoft.com/gsplit/ 
445
ambiguous ? 3 positive - ?positive?, ?beautiful?, 
?good? and 3 negative ? ?negative?, ?ugly?, 
?bad?. An example of such queries is ?price high 
and good?. The third component is made up of 
rules, depending on the presence of specific 
modifiers in a window of 4 words before the ad-
jective.  The final verdict is given based on the 
vote given by the majority of the three compo-
nents, explained in detail in the next sections: 
4.1 The OpAL opinion mining component 
First, we process each context using Minipar3. 
We compute, for each word in a sentence, a se-
ries of features, computed from the NTCIR 7 
data and the EmotiBlog annotations. These 
words are used to compute vectors of features for 
each of the individual contexts: 
 the part of speech (POS)  
 opinionatedness/intensity - if the word is 
annotated as opinion word, its polarity, i.e. 1 
and -1 if the word is positive or negative, re-
spectively and 0 if it is not an opinion word, 
its intensity (1, 2 or 3) and 0 if it is not a 
subjective word 
 syntactic relatedness with other opinion 
word ? if it is directly dependent of an opi-
nion word or modifier (0 or 1), plus the po-
larity/intensity and emotion of this word (0 
for all the components otherwise) 
  role in 2-word, 3-word, 4-word and sen-
tence annotations: opinionatedness, intensity 
and emotion of the other words contained in 
the annotation, direct dependency relations 
with them if they exist and 0 otherwise.  
We add to the opinion words annotated in 
EmotiBlog the list of opinion words found in the 
Opinion Finder, Opinion Finder, MicroWordNet 
Opinion, General Inquirer, WordNet Affect, 
emotion triggers lexical resources. We train the 
model using the SVM SMO implementation in 
Weka4. 
4.2 Assessing local polarity using Google 
queries 
This approach aimed at determining the polarity 
of the context immediately surrounding the ad-
jective to be classified. To that aim, we con-
structed queries using the noun found before the 
adjective in the context given, and issued six dif-
ferent queries on Google, together with six pre-
defined adjectives whose polarity is known (3 
                                                 
3 http://webdocs.cs.ualberta.ca/~lindek/minipar.htm 
4 http://www.cs.waikato.ac.nz/ml/weka/ 
positive - ?positive?, ?beautiful?, ?good? and 3 
negative ? ?negative?, ?ugly?, ?bad?). The form 
of the queries was ?noun+adjective+AND+pre-
defined adjective?.  The local polarity was consi-
dered as the one for which the query issued the 
highest number of total results (total number of 
results for the 3 queries corresponding to the pos-
itive adjectives or to the negative adjectives, re-
spectively).  
4.3 Modifier rules for contextual polarity  
This rule accounts for the original, most fre-
quently used polarity of the given adjectives (e.g. 
high is positive, low is negative). For each of 
them, we define its default polarity. Subsequent-
ly, we determine whether in the window of 4 
words around the adjective there are any modifi-
ers (valence shifters). If this is the case, and they 
have an opposite value of polarity, the adjective 
is assigned a polarity value opposite from its de-
fault one (e.g. too high is negative).  We employ 
a list of 82 positive and 87 negative valence shif-
ters.  
5  Evaluation  
Table 1 and Table 2 present the results obtained 
by the OpAL system in the SemEval 2010 Task 
18 competition. The system ranked fifth, with a 
Micro accuracy of 0.76037 and sixth, with a Ma-
cro accuracy of 0.7037.  
 
System name Micro accura-
cy 
98-35_result 0.942064 
437-381_HITSZ_CITYU_ 
Task18_Run1.key 
0.936236 
437-380_HITSZ_CITYU_ 
Task18_Run2.key 
0.93315 
53-211_dsaa 0.880699 
186-325_OpAL_results.txt 0.76037 
291-389_submission4.txt 0.724717 
291-388_submission3.txt 0.715461 
437-382_HITSZ_CITYU_ 
Task18_Run3 
0.665752 
 Table 1: Results - top 8 runs (micro accuracy) 
 
System name Macro  accu-
racy 
437-380_HITSZ_CITYU_ 
Task18_Run2.key 0.957881 
437-381_HITSZ_CITYU_ 
Task18_Run1.key 0.953238 
98-35_result 0.929308 
53-211_dsaa 0.861964 
446
291-388_submission3.txt 0.755387 
186-325_OpAL_results.txt 0.703777 
291-389_submission4.txt 0.698037 
460383_New_Task18_ 
Chinese_test_pos_QiuLikun_R.rar 0.695448 
Table 2: Results ? top 8 runs (macro accuracy) 
 
Since the gold standard was not provided, we 
were not able to perform an exhaustive analysis 
of the errors. However, from a random inspec-
tion of the system results, we could see that a 
large number of errors was due to the translation 
? through which modifiers are placed far from 
the word they determine or the words are not 
translated with their best equivalent.  
6 Conclusions and future work 
In this article we presented our approach towards 
the disambiguation of polarity ambiguous adjec-
tives depending on the context in which they ap-
pear. The OpAL system?s run was based on three 
subcomponents working in English ? one assess-
ing the overall polarity of the context using an 
opinion mining system, the second assessing the 
local polarity using Google queries formed by 
expressions containing the noun present in the 
context before the adjective to be classified and 
the third one evaluating contextual polarity based 
on the adjective?s default value and the modifiers 
around it. The final output is based on the vote 
given by the majority of the three components. 
The approach had a good performance, the 
OpAL system run ranking fifth among 16 runs. 
Future work includes the separate evaluation of 
the three components and their combination in a 
unique approach, using machine learning, as well 
as a thorough assessment of errors that are due to 
translation.   
References  
Balahur, A. and Montoyo, A. 2008a. A feature-driven 
approach to opinion mining and classification. In 
Proceedings of the NLPKE 2008. 
Balahur, A. and Montoyo, A. 2008b. Applying a cul-
ture dependent emotion triggers database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 40(40). 
Boldrini, E., Balahur, A., Mart?nez-Barco, P., and 
Montoyo, A. 2009. EmotiBlog: an annotation 
scheme for emotion detection and analysis in non-
traditional textual genres. In Proceedings of the 
5th International Conference on Data Mining 
(DMIN 2009). 
Cerini, S., Compagnoni, V., Demontis, A., Formentel-
li, M., and Gandini, G. 2007. Micro-WNOp: A gold 
standard for the evaluation of automatically com-
piled lexical resources for opinion mining. 
Cilibrasi, D. and Vitanyi, P. 2006. Automatic Mean-
ing Discovery Using Google. IEEE Journal of 
Transactions on Knowledge and Data Engineering. 
Esuli, A. and Sebastiani, F. 2006. SentiWordNet: a 
publicly available resource for opinion mining. In 
Proceedings of the 6th International Conference on 
Language Resources and Evaluation. 
Hu, M. and Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of Nineteenth 
National Conference on Artificial Intellgience 
AAAI-2004. 
Pang, B. and Lee, L. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2), pp. 1?135, 2008 
Pang, B., Lee, L., and Vaithyanathan, S. 2002. 
Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of 
EMNLP-02, the Conference on Empirical Methods 
in Natural Language Processing. 
Popescu, A. M. and Etzioni, O. 2005. Extracting 
product features and opinions from reviews. In In 
Proceedings of HLTEMNLP 2005. 
Stone, P., Dumphy, D. C., Smith, M. S., and Ogilvie, 
D. M. 1966. The General Inquirer: A Computer 
Approach to Content Analysis. The MIT Press. 
Strapparava, C. and Valitutti, A. 2004.WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC 2004). 
Turney, P. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguis-
tics. 
Whitelaw, C., Garg, N., and Argamon, S. 2005. Using 
appraisal groups for sentiment analysis. In Pro-
ceedings of the CIKM 2005. 
Wiebe, J. (1994). Tracking point of view in narrative. 
Computational Linguistics, 20. 
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Re-
cognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of HLT-EMNLP 
2005. 
Wu, Y., Jin, P. 2010. SemEval-2010 Task 18: Disam-
biguating Sentiment Ambiguous Adjectives. In Pro-
ceedings of the SemEval 2010 Workshop, ACL 
2010.  
447
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 608?616,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UMCC_DLSI: Multidimensional Lexical-Semantic Textual Similarity 
 
Antonio Fern?ndez, Yoan Guti?rrez, 
Alexander Ch?vez, H?ctor D?vila, Andy 
Gonz?lez, Rainel Estrada , Yenier Casta?eda 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
 
Sonia V?zquez 
Andr?s Montoyo, Rafael Mu?oz,  
 
DLSI, University of Alicante 
Carretera de San Vicente S/N 
Alicante, Spain 
 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the first Semantic Textual 
Similarity task (STS) of SemEval-2012. Our 
supervised system uses different kinds of 
semantic and lexical features to train classifiers 
and it uses a voting process to select the correct 
option. Related to the different features we can 
highlight the resource ISR-WN1 used to extract 
semantic relations among words and the use of 
different algorithms to establish semantic and 
lexical similarities. In order to establish which 
features are the most appropriate to improve 
STS results we participated with three runs 
using different set of features. Our best 
approach reached the position 18 of 89 runs, 
obtaining a general correlation coefficient up to 
0.72. 
1. Introduction 
SemEval 2012 competition for evaluating Natural 
Language Processing (NLP) systems presents a 
new task called Semantic Textual Similarity (STS) 
(Agirre et al, 2012). In STS the participating 
systems must examine the degree of semantic 
equivalence between two sentences. The goal of 
this task is to create a unified framework for the 
evaluation of semantic textual similarity modules 
and to characterize their impact on NLP 
applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that STS 
                                                   
1 Integration of Semantic Resource based on WordNet. 
assumes bidirectional graded equivalence between 
the pair of textual snippets. In the case of TE the 
equivalence is directional (e.g. a student is a 
person, but a person is not necessarily a student). 
In addition, STS differs from TE and Paraphrase in 
that, rather than being a binary yes/no decision, 
STS is a similarity-graded notion (e.g. a student 
and a person are more similar than a dog and a 
person). This bidirectional gradation is useful for 
NLP tasks such as Machine Translation, 
Information Extraction, Question Answering, and 
Summarization. Several semantic tasks could be 
added as modules in the STS framework, ?such as 
Word Sense Disambiguation and Induction, 
Lexical Substitution, Semantic Role Labeling, 
Multiword Expression detection and handling, 
Anaphora and Co-reference resolution, Time and 
Date resolution and Named Entity Recognition, 
among others?2  
1.1. Description of 2012 pilot task 
In STS, all systems were provided with a set of 
sentence pairs obtained from a segmented corpus. 
For each sentence pair, s1 and s2, all participants 
had to quantify how similar s1 and s2 were, 
providing a similarity score. The output of 
different systems was compared to the manual 
scores provided by SemEval-2012 gold standard 
file, which range from 5 to 0 according to the next 
criterions3:  
? (5) ?The two sentences are equivalent, as they 
mean the same thing?. 
                                                   
2
 http://www.cs.york.ac.uk/semeval-2012/task6/ 
3
 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
608
? (4) ?The two sentences are mostly equivalent, 
but some unimportant details differ?. 
? (3) ?The two sentences are roughly equivalent, 
but some important information 
differs/missing?. 
? (2) ?The two sentences are not equivalent, but 
share some details?. 
? (1) ?The two sentences are not equivalent, but 
are on the same topic?. 
? (0) ?The two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 2 shows the 
architecture of our system and a description of the 
different runs. In section 3 we describe the 
algorithms and methods used to obtain the features 
for our system, and Section 4 describe the training 
phase. The obtained results and a discussion are 
provided in Section 5, and finally the conclusions 
and future works in Section 6. 
2. System architecture and description of 
the runs 
As we can see in Figure 1 our three runs begin 
with the pre-processing of SemEval 2012?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS tagged using Freeling tool 
(Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group of 
features. 
The Run 1 (MultiSemLex) is our main run. 
This takes into account all extracted features and 
trains a model with a Voting classifier composed 
by the following techniques: Bagging (using M5P), 
Bagging (using REPTree), Random SubSpace 
(using REPTree) and MP5. The training corpus has 
been provided by SemEval-2012 competition, in 
concrete by the Semantic Textual Similarity task.  
The Runs 2 and 3 use the same classifier, but 
including different features. Run 2 (MultiLex) uses 
(see Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 3.1, 
Lexical-Semantic Alignment (LS-A) described in 
section 3.2 and Sentiment Polarity (SP) described 
in section 3.3.  
On the other hand, the Run 3 (MultiSem) uses 
features extracted only from Semantic Alignment 
(SA) described in section 3.4 and the textual edit 
distances named QGram-Distances. 
 
Figure 1. System Architecture. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two sentences. 
Finally, we test our system with the SemEval 
2012 test set (see Table 7 with the results of our 
three runs). The following section describes the 
features extraction process. 
      Run 1 
      Voting  
      Classifier 
Training set from 
SemEval 2012 
Pre-Processing (using Freeling) 
 
Run 3 
Voting classifier 
Run 2 
Voting classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
Sentiment 
Polarity 
Jaro QGram Rel. Concept . . . 
Tokenizing Lemmatizing POS tagging 
SemEval 2012 
Test set 
     Training Process (using Weka) 
609
3. Description of the features used in the 
Machine Learning System 
Sometimes, when two sentences are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired by this fact we 
developed various algorithms, which measure the 
level of overlapping by computing a quantity of 
matching words (the quantity of lemmas that 
correspond exactly by its morphology) in a pair of 
sentences. In our system, we used lexical and 
semantic similarity measures as features for a 
MLS. Other features were extracted from a lexical-
semantic sentences alignment and a variant using 
only a semantic alignment.  
3.1. Similarity measures 
We have used well-known string based similarity 
measures like: Needleman-Wunch (NW) (sequence 
alignment), Smith-Waterman (SW) (sequence 
alignment), Jaro, Jaro-Winkler (JaroW), Chapman-
Mean-Length (CMLength), QGram-Distance 
(QGramD), Block-Distance (BD), Jaccard 
Similarity (JaccardS), Monge-Elkan (ME) and 
Overlap-Coefficient (OC). These algorithms have 
been obtained from an API (Application Program 
Interface) SimMetrics library v1.54 for .NET 2.0. 
Copyright (c) 2006 by Chris Parkinson. We 
obtained 10 features for our MLS from these 
similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order to 
obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as the 
LED between two sentences and the normalized 
variant named NomLED. Contrary to (Tatu et al, 
2006), we do not remove the punctuation or stop 
words from the sentences, neither consider 
different cost for transformation operation, and we 
used all the operations (deletion, insertion and 
substitution). The second one is a variant that we 
named Double Levenshtein?s Edit Distance 
(DLED). For this algorithm, we used LED to 
measure the distance between the sentences, but to 
compare the similarity between the words, we used 
LED again. Another feature is the normalized 
variant of DLED named NomDLED. 
The unique difference between classic LED 
algorithm and DLED is the comparison of 
                                                   
4
 http://sourceforge.net/projects/simmetrics/ 
similitude between two words. With LED should 
be: ?[?] = ?[?], whereas for our DLED we 
calculate words similarity also with LED (e.g. ????(?[?], ?[?]) <= 2). Values above a decision 
threshold (experimentally 2) mean unequal words. 
We obtain as result two new different features 
from these algorithms. 
Another distance we used is an extension of 
LED named Extended Distance (EDx) (see 
(Fern?ndez Orqu?n et al, 2009) for details). This 
algorithm is an extension of the Levenshtein?s 
algorithm, with which penalties are applied by 
considering what kind of operation or 
transformation is carried out (insertion, deletion, 
substitution, or non-operation) in what position, 
along with the character involved in the operation. 
In addition to the cost matrixes used by 
Levenshtein?s algorithm, EDx also obtains the 
Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes for 
determining similarity between strings in a single 
iteration. It is worth noting that the inclusion of all 
these penalizations makes the EDx algorithm a 
good candidate for our approach. In our previous 
work (Fern?ndez Orqu?n et al, 2009), EDx 
demonstrated excellent results when it was 
compared with other distances as (Levenshtein, 
1965), (Needleman and Wunsch, 1970), (Winkler, 
1999). How to calculate EDx is briefly described 
as follows (we refer reader to (Fern?ndez Orqu?n et 
al., 2009) for a further description): 
EDx = ??  ?????????????,???????(???????)????????? ?? ; (1) 
 
 
Where: ? - Transformations accomplished on the words (?, ?, ?, ?). ? - Not operations at all, ? - Insertion, ? - Deletion, ? - Substitution.  
We formalize ? as a vector: 
? =
???
??(0,0)(1,0) :: ??(0,1)(1,1) :: ?????
??
 
?1 and ?2 - The examined words ?1j - The j-th character of the word ?1 
610
?2k - The k-th character of the word ?2 ? - The weight of each character 
We can vary all this weights in order to make a 
flexible penalization to the interchangeable 
characters.  ??1j - The weight of characters at ?1j ??2k - The weight of characters at ?2k ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ; ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ? - The biggest word length of the language ? - Edit operations length ?i - Operation at (?) position ???? - Greatest value of ? ranking 
? = ? 2????(2???? + 1)???????  (2) 
As we can see in the equation (1), the term ?(??) ? ???????, ?(???)? is the Cartesian product that 
analyzes the importance of doing i-th operation 
between the characters at j-th and k-th position 
The term (2R??? + 1)??? in equation (1) penalizes 
the position of the operations. The most to the left 
hand the operation is the highest the penalization 
is. The term ? (see equation (2) normalizes the 
EDx into [0,1] interval. This measure is also used 
as a feature for the system. 
We also used as a feature the Minimal 
Semantic Distances (Breadth First Search (BFS)) 
obtained between the most relevant concepts of 
both sentences. The relevant concepts pertain to 
semantic resources ISR-WN (Guti?rrez et al, 
2011a; 2010b), as WordNet (Miller et al, 1990), 
WordNet Affect (Strapparava and Valitutti, 2004), 
SUMO (Niles and Pease, 2001) and Semantic 
Classes (Izquierdo et al, 2007). Those concepts 
were obtained after having applied the Association 
Ratio (AR) measure between concepts and words 
over each sentence. The obtained distances for 
each resource SUMO, WordNet Affect, WordNet 
and Semantic Classes are named SDist, AffDist, 
WNDist and SCDist respectively. 
ISR-WN, takes into account different kind of 
labels linked to WN: Level Upper Concepts 
(SUMO), Domains and Emotion labels. In this 
work, our purpose is to use a semantic network, 
which links different semantic resources aligned to 
WN. After several tests, we decided to apply ISR-
WN. Although others resources provide different 
semantic relations, ISR-WN has the highest 
quantity of semantic dimensions aligned, so it is a 
suitable resource to run our algorithm.  
Using ISR-WN we are able to extract 
important information from the interrelations of 
four ontological resources: WN, WND, WNA and 
SUMO. ISR-WN resource is based on WN1.6 or 
WN2.0 versions. In the last updated version, 
Semantic Classes and SentiWordNet were also 
included. Furthermore, ISR-WN provides a tool 
that allows the navigation across internal links. At 
this point, we can discover the multidimensionality 
of concepts that exists in each sentence. In order to 
establish the concepts associated to each sentence 
we apply Relevant Semantic Trees (Guti?rrez et 
al., 2010a; Guti?rrez et al, 2011b) approach using 
the provided links of ISR-WN. We refer reader to 
(Guti?rrez et al, 2010a) for a further description. 
3.2. Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried to 
align the sentences by its lemmas. If the lemmas 
coincide we look for coincidences among parts of 
speech, and then the phrase is realigned using both. 
If the words do not share the same part of speech, 
they will not be aligned. Until here, we only have 
taken into account a lexical alignment. From now 
on, we are going to apply a semantic variant. After 
all the process, the non-aligned words will be 
analyzed taking into account its WorldNet?s 
relations (synonymy, hyponymy, hyperonymy, 
derivationally ? related ? form, similar-to, verbal 
group, entailment and cause-to relation); and a set 
of equivalencies like abbreviations of months, 
countries, capitals, days and coins. In the case of 
the relation of hyperonymy and hyponymy, the 
words will be aligned if there is a word in the first 
sentence that is in the same relation (hyperonymy 
or hyponymy) of another one in the second 
sentence. For the relations of ?cause-to? and 
?implication? the words will be aligned if there is a 
word in the first sentence that causes or implicates 
another one of the second sentence. All the other 
types of relations will be carried out in 
bidirectional way, that is, there is an alignment if a 
word of the first sentence is a synonymous of 
another one belonging to the second one or vice 
versa. Finally, we obtain a value we called 
alignment relation. This value is calculated as ??? =  ??? / ????. Where ??? is the final 
611
alignment value, ??? is the number of aligned 
word and ???? is the number of words of the 
shorter phrase. This value is also another feature 
for our system. 
3.3. Sentiment Polarity Feature 
Another feature is obtained calculating 
SentiWordNet Polarities matches of the analyzed 
sentences (see (Guti?rrez et al, 2011c) for detail). 
This analysis has been applied from several 
dimensions (WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes) 
where the words with sentimental polarity offer to 
the relevant concepts (for each conceptual resource 
from ISR-WN (e.g. WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes)) 
its polarity values. Other analysis were the 
integration of all results of polarity in a measure 
and further a voting process where all polarities 
output are involved (for more details see 
(Fern?ndez et al, 2012)). 
The final measure corresponds to ?? =????? + ?????, where ????1 is a polarity value of 
the sentence ?1 and ????? is a polarity value of the 
sentence ?2. The negative, neutral, and positive 
values of polarities are represented as -1, 0 and 1 
respectively. 
3.4. Semantic Alignment 
This alignment method depends on calculating the 
semantic similarity between sentences based on an 
analysis of the relations, in ISR-WN, of the words 
that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according to 
their parts of speech (noun, verb, adjective, and 
adverbs.).  
We take 30% of the most probable senses of 
every word and we treat them as a group. The 
distance between two groups will be the minimal 
distance between senses of any pair of words 
belonging to the group. For example: 
 
Figure 2. Minimal Distance between ?Run? and 
?Chase?. 
In the example of Figure 2 the ???? = 2 is 
selected for the pair ?Run-Chase?, because this 
pair has the minimal cost=2.  
For nouns and the words that are not found in 
WordNet like common nouns or Christian names, 
the distance is calculated in a different way. In this 
case, we used LED. 
Let's see the following example: 
We could take the pair 99 of corpus MSRvid 
(from training set) with a litter of transformation in 
order to a better explanation of our method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using the algorithm showed in the 
example of Figure 2, a matrix with the distances 
between all groups of both sentences is created 
(see Table 1). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 1. Distances between the groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of the 
smaller sentence is checked with an element of the 
biggest sentence and the rest is marked as words 
that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, so 
the number of non-aligned words is 2. There is 
only one perfect match: ?group-group? (match 
with ???? = 0). The length of the shortest sentence 
is 4. The Table 2 shows the results of this analysis. 
Number of 
exact 
coincidences 
(Same) 
Total Distances 
of optimal 
Matching 
(Cost) 
Number of 
non-
aligned 
Words 
(Dif) 
Number of 
lemmas of 
shorter 
sentence 
(Min) 
1 5 2 4 
Table 2. Features extracted from the analyzed sentences. 
This process has to be repeated for the verbs, 
nouns (see Table 3), adjectives, and adverbs. On 
the contrary, the tables have to be created only 
with the similar groups of the sentences. Table 3 
Lemma: Chase 
 
 
 
 
Lemma: Run 
 
 
 
 
Dist=2 
2 
3 
5 
Sense 1 
Sense 2 
Sense 1 
Sense 2 
4 
612
shows features extracted from the analysis of 
nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 3. Distances between the groups of nouns. 
Number of 
exact 
coincidences 
(SameN) 
Total 
Distances of 
optimal 
Matching 
(CostN) 
Number of 
non-aligned 
Words 
(DifN) 
Number of 
lemmas of 
shorter 
sentence 
(MinN) 
1 3 0 3 
Table 4. Feature extracted the analysis of nouns. 
Several attributes are extracted from the pair of 
sentences. Four attributes from the entire 
sentences, four attributes considering only verbs, 
only nouns, only adjectives, and only adverbs. 
These attributes are:  
? Number of exact coincidences (Same) 
? Total distance of optimal matching (Cost). 
? Number of words that do not match (Dif). 
? Number of lemmas of the shortest sentence 
(Min). 
As a result, we finally obtain 20 attributes from 
this alignment method. For each part-of-speech, 
the attributes are represented adding to its names 
the characters N, V, A and R to represent features 
for nouns, verbs, adjectives, and adverbs 
respectively. 
It is important to remark that this alignment 
process searches to solve, for each word from the 
rows (see Table 3) its respectively word from the 
columns. 
4. Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
(MSRpar, MSRvid and SMTeuroparl) as a training 
corpus. Using 10 fold cross validation with the 
classifier mentioned in section 2 (experimentally 
selected). 
As we can see in Table 5, the features: FAV, 
EDx, CMLength, QGramD, BD, Same, SameN, 
obtain values over 0.50 of correlation. The more 
relevant are EDx and QGramD, which were 
selected as a lexical base for the experiment in Run 
3. It is important to remark that feature SameN and 
Same only using number of exact coincidences 
obtain an encourage value of correlation. 
 
Feature Correlation Feature Correlation Feature Correlation 
Correlation using all 
features 
(correspond to Run 1) 
FAV 0.5064 ME 0.4971 CostV 0.1517 
0.8519 
LED 0.4572 OC 0.4983 SameN 0.5307 
DLED 0.4782 SDist 0.4037 MinN 0.4149 
NormLED 0.4349 AffDist 0.4043 DifN 0.1132 
NormDLED 0.4457 WNDist 0.2098 CostN 0.1984 
EDx 0.596 SCDist  0.1532 SameA 0.4182 
NW 0.2431 PV 0.0342 MinA 0.4261 
SW 0.2803 Same 0.5753 DifA 0.3818 
Jaro 0.3611 Min 0.5398 CostA 0.3794 
JaroW 0.2366 Dif 0.2588 SameR 0.3586 
CMLength 0.5588 Cost 0.2568 MinR 0.362 
QGramD 0.5749 SameV 0.3004 DifR 0.3678 
BD 0.5259 MinV 0.4227 CostR 0.3461 
JaccardS 0.4849 DifV 0.2634 
  
 
Table 5. Correlation of individual features over all training sets. 
 
We decide to include the Sentiment Polarity as 
a feature, because our previous results on Textual 
Entailment task in (Fern?ndez et al, 2012). But, 
contrary to what we obtain in this paper, the 
influence of the polarity (PV) for this task is very 
low, its contribution working together with other 
features is not remarkable, but neither negative 
(Table 6), So we decide remaining in our system. 
In oder to select the lexical base for Run 3 
(MultiSem, features marked in bold) we compared 
the individual influences of the best lexical 
features (EDx, QGramD, CMLength), obtaining 
613
the 0.82, 0.83, 0.81 respectively (Table 6). Finally, 
we decided to use QGramD. 
The conceptual features SDist, AffDist, 
WNDist, SCDist do not increase the similarity 
score, this is due to the generality of the obtained 
concept, losing the essential characteristic between 
both sentences. Just like with PV we decide to 
keep them in our system. 
As we can see in Table 5, when all features are 
taking into account the system obtain the best 
score. 
Feature Pearson (MSRpar, MSRvid and SMTeuroparl) 
SDist 
 
       
0.8509 
AffDist 
 
       
WNDist 
 
       
SCDist 
 
       
EDx 
 
      
0.8507 
PV 
 
   
 
  QGramD 
 
    
0.8491 
CMLength
 
 
0.8075 
   
Same 
0.7043 
0.795 0.829 0.8302 0.8228 
Min 
Dif 
Cost 
SameV 
0.576 MinV DifV 
CostV 
SameN 
0.5975 MinN DifN 
CostN 
SameA 
0.4285 MinA DifA 
CostA 
SameR 
0.3778 MinR DifR 
CostR 
Table 6. Features influence.  
Note: Gray cells mean features that are not taking into 
account. 
5. Result and discussion 
Semantic Textual Similarity task of SemEval-2012 
offered three official measures to rank the 
systems5: 
1. ALL: Pearson correlation with the gold 
standard for the five datasets, and 
corresponding rank. 
2. ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
                                                   
5
 http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update 
standard using least squares, and 
corresponding rank. 
3. Mean: Weighted mean across the five datasets, 
where the weight depends on the number of 
pairs in the dataset. 
4. Pearson for individual datasets. 
Using these measures, our main run (Run 1) 
obtained the best results (see Table 7). This 
demonstrates the importance of tackling this 
problem from a multidimensional lexical-semantic 
point of view. 
Run MSRpar MSRvid SMT-eur On-WN SMT-
news 
1 0.6205 0.8104 0.4325 0.6256 0.4340 
2 0.6022 0.7709 0.4435 0.4327 0.4264 
3 0.5269 0.7756 0.4688 0.6539 0.5470 
Table 7. Official SemEval 2012 results. 
Run ALL Rank ALLnrm RankNrm Mean RankMean 
1 0.7213 18 0.8239 14 0.6158 15 
2 0.6630 26 0.7922 46 0.5560 49 
3 0.6529 29 0.8115 23 0.6116 16 
Table 8. Ranking position of our runs in SemEval 2012. 
The Run 2 uses a lot of lexical analysis and not 
much of semantic analysis. For this reason, the 
results for Run 2 is poorer (in comparison to the 
Run 3) (see Table 7) for the test sets: SMT-eur, 
On-WN and SMT-news. Of course, these tests 
have more complex semantic structures than the 
others. However, for test MSRpar it function better 
and for test MSRvid it functions very similar to 
Run 3. 
Otherwise, the Run 3 uses more semantic 
analysis that Run 2 (it uses all features mentioned 
except feature marked in bold on Table 6) and only 
one lexical similarity measure (QGram-Distance). 
This makes it to work better for test sets SMT-eur, 
On-WN and SMT-news (see Table 7). It is 
important to remark that this run obtains important 
results for the test SMT-news, positioning this 
variant in the fifth place of 89 runs. Moreover, it is 
interesting to notice (Table 7) that when mixing the 
semantic features with the lexical one (creating 
Run 1) it makes the system to improve its general 
results, except for the test: SMT-eur, On-WN and 
SMT-news in comparison with Run 3. For these 
test sets seem to be necessary more semantic 
analysis than lexical in order to improve similarity 
estimation. We assume that Run 1 is non-balance 
according to the quantity of lexical and semantic 
features, because this run has a high quantity of 
614
lexical and a few of semantic analysis. For that 
reason, Run 3 has a better performance than Run 1 
for these test sets. 
Even when the semantic measures demonstrate 
significant results, we do not discard the lexical 
help on Run 3. After doing experimental 
evaluations on the training phase, when lexical 
feature from QGram-Distance is not taken into 
account, the Run 3 scores decrease. This 
demonstrates that at least a lexical base is 
necessary for the Semantic Textual Similarity 
systems. 
6. Conclusion and future works 
This paper introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional interpretation 
of a text. 
As mentioned in section 2 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 7 that 
all runs obtained encouraging results. Our best run 
was placed between the first 18th positions of the 
ranking of Semeval 2012 (from 89 Runs) in all 
cases. Table 8 shows the reached positions for the 
three different runs and the ranking according to 
the rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment, Semantic Alignment, and Sentiment 
Polarity Cross-checking. 
We have conducted the semantic features 
extraction in a multidimensional context using the 
resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWorNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we show 
that this approach can be used to correctly classify 
several examples from the STS task of SemEval-
2012. Comparing with the best run (UKP_Run2 
(see Table 9)) of the ranking our main run has very 
closed results. In two times we increased the best 
UKP?s run (UKP_Run 2), for MSRvid test set in 
0.2824 points and for On-WN test set in 0.1319 
points (see Table 10).  
Run ALL Rank ALLnrm RankNrm Mean RankMean 
(UKP) 
Run 2 0.8239 1 0.8579 2 0.6773 1 
Table 9. The best run of SemEval 2012. 
It is important to remark that we do not expand 
any corpus to train the classifier of our system. 
This fact locates us at disadvantage according to 
other teams that do it. 
Run ALL MSRpar MSRvid SMT-
eur 
On-
WN 
SMT-
news 
(UKP) 
Run 2 0.8239 0.8739 0.528 0.6641 0.4937 0.4937 
(Our) 
Run 1 0.721 0.6205 0.8104 0.4325 0.6256 0.434 
Table 10. Comparison of our distance with the best. 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think that 
with this improvement we can increase the results 
obtained with texts like those in On-WN test set. 
Acknowledgments 
This paper has been supported partially by 
Ministerio de Ciencia e Innovaci?n - Spanish 
Government (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat 
Valenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/288). 
Reference 
Antonio Fern?ndez, Yoan Guti?rrez, Rafael Mu?oz and 
Andr?s Montoyo. 2012. Approaching Textual 
Entailment with Sentiment Polarity. In  ICAI'12 - The 
2012 International Conference on Artificial 
Intelligence, Las Vegas, Nevada, USA.  
Antonio Celso Fern?ndez Orqu?n, D?az Blanco Josval, 
Alfredo Fundora Rolo and Rafael Mu?oz Guillena. 
2009. Un algoritmo para la extracci?n de 
caracter?sticas lexicogr?ficas en la comparaci?n de 
palabras. In  IV Convenci?n Cient?fica Internacional 
CIUM, Matanzas, Cuba.  
Carlo Strapparava and Alessandro Valitutti. 2004. 
WordNet-Affect: an affective extension of WordNet. 
In Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC 2004), 
Lisbon,  1083-1086.  
Daniel S. Hirschberg. 1977. Algorithms for the longest 
common subsequence problem Journal of the ACM, 
24: 664?675. 
615
Dan I. Moldovan and Vasile Rus. 2001. Explaining 
Answers with Extended WordNet ACL. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A 
Pilot on Semantic Textual Similarity. In Proceedings 
of the 6th International Workshop on Semantic 
Evaluation (SemEval 2012), in conjunction with the 
First Joint Conference on Lexical and Computational 
Semantics (*SEM 2012), Montreal, Canada, ACL.  
George A. Miller, Richard Beckwith, Christiane 
Fellbaum, Derek Gross and Katherine Miller. 1990. 
Introduction to WordNet: An On-line Lexical 
Database International Journal of Lexicography, 
3(4):235-244. 
Harold W. Kuhn. 1955. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly,  2: 83?97. 
Ian Niles and Adam Pease. 2001. Origins of the IEEE 
Standard Upper Ontology. In  Working Notes of the 
IJCAI-2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA.  
Jordi Atserias, Bernardino Casas, Elisabet Comelles, 
Meritxell Gonz?lez, Llu?s Padr? and Muntsa Padr?. 
2006. FreeLing 1.3: Syntactic and semantic services 
in an open source NLP library. In  Proceedings of the 
fifth international conference on Language Resources 
and Evaluation (LREC 2006), Genoa, Italy.  
Marta Tatu, Brandon Iles, John Slavick, Novischi 
Adrian and Dan Moldovan. 2006. COGEX at the 
Second Recognizing Textual Entailment Challenge. 
In Proceedings of the Second PASCAL Recognising 
Textual Entailment Challenge Workshop, Venice, 
Italy,  104-109. 
Rub?n Izquierdo, Armando Su?rez and German Rigau. 
2007. A Proposal of Automatic Selection of Coarse-
grained Semantic Classes for WSD Procesamiento 
del Lenguaje Natural,  39: 189-196. 
Saul B. Needleman and Christian D. Wunsch. 1970. A 
general method applicable to the search for 
similarities in the amino acid sequence of two 
proteins Journal of Molecular Biology,  48(3): 443-
453. 
Vladimir Losifovich Levenshtein. 1965. Binary codes 
capable of correcting spurious insertions and 
deletions of ones. Problems of information 
Transmission.  pp. 8-17.  
William E. Winkler. 1999. The state of record linkage 
and current research problems. Technical Report. 
U.S. Census Bureau, Statistical Research Division. 
Yoan Guti?rrez, Antonio Fern?ndez, And?s Montoyo 
and Sonia V?zquez. 2010a. UMCC-DLSI: Integrative 
resource for disambiguation task. In  Proceedings of 
the 5th International Workshop on Semantic 
Evaluation, Uppsala, Sweden, Association for 
Computational Linguistics,  427-432.  
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010b. Integration of semantic 
resources based on WordNet XXVI Congreso de la 
Sociedad Espa?ola para el Procesamiento del 
Lenguaje Natural,  45: 161-168. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2011a. Enriching the Integration 
of Semantic Resources based on WordNet 
Procesamiento del Lenguaje Natural,  47: 249-257. 
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011b. Improving WSD using ISR-WN with Relevant 
Semantic Trees and SemCor Senses Frequency. In  
Proceedings of the International Conference Recent 
Advances in Natural Language Processing 2011, 
Hissar, Bulgaria, RANLP 2011 Organising 
Committee,  233--239.  
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011c. Sentiment Classification Using Semantic 
Features Extracted from WordNet-based Resources. 
In  Proceedings of the 2nd Workshop on 
Computational Approaches to Subjectivity and 
Sentiment Analysis (WASSA 2.011), Portland, 
Oregon., Association for Computational Linguistics,  
139--145.  
616
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 109?118, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Textual Similarity based on Lexical-Semantic features 
 
 
Alexander Ch?vez, Antonio Fern?ndez Orqu?n, 
H?ctor D?vila, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ?  
Matanzas, Cuba.  
{alexander.chavez, tony, 
hector.davila, yoan.gutierrez, 
armando.collazo, jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera de 
San Vicente S/N Alicante, Spain. 
{montoyo,rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the Semantic Textual 
Similarity task (STS) of SemEval-2013. Our 
supervised system uses different types of 
lexical and semantic features to train a 
Bagging classifier used to decide the correct 
option. Related to the different features we 
can highlight the resource ISR-WN used to 
extract semantic relations among words and 
the use of different algorithms to establish 
semantic and lexical similarities. In order to 
establish which features are the most 
appropriate to improve STS results we 
participated with three runs using different 
set of features. Our best run reached the 
position 44 in the official ranking, obtaining 
a general correlation coefficient of 0.61. 
1 Introduction 
SemEval-2013 (Agirre et al, 2013) presents the 
task Semantic Textual Similarity (STS) again. In 
STS, the participating systems must examine the 
degree of semantic equivalence between two 
sentences. The goal of this task is to create a 
unified framework for the evaluation of semantic 
textual similarity modules and to characterize 
their impact on NLP applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that 
STS assumes bidirectional graded equivalence 
between the pair of textual snippets. 
In case of TE, the equivalence is directional 
(e.g. a student is a person, but a person is not 
necessarily a student). In addition, STS differs 
from TE and Paraphrase in that, rather than 
being a binary yes/no decision, STS is a 
similarity-graded notion (e.g. a student is more 
similar to a person than a dog to a person).  
This graded bidirectional is useful for NLP 
tasks such as Machine Translation (MT), 
Information Extraction (IE), Question 
Answering (QA), and Summarization. Several 
semantic tasks could be added as modules in the 
STS framework, ?such as Word Sense 
Disambiguation and Induction, Lexical 
Substitution, Semantic Role Labeling, Multiword 
Expression detection and handling, Anaphora 
and Co-reference resolution, Time and Date 
resolution and Named Entity, among others?1  
1.1 Description of 2013 pilot task 
This edition of SemEval-2013 remain with the 
same classification approaches that in their first 
version in 2012. The output of different systems 
was compared to the reference scores provided 
by SemEval-2013 gold standard file, which 
range from five to zero according to the next 
criterions2: (5) ?The two sentences are 
equivalent, as they mean the same thing?. (4) 
?The two sentences are mostly equivalent, but 
some unimportant details differ?. (3) ?The two 
sentences are roughly equivalent, but some 
important information differs/missing?. (2) ?The 
two sentences are not equivalent, but share some 
details?. (1) ?The two sentences are not 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
2 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
109
equivalent, but are on the same topic?. (0) ?The 
two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 3 shows the 
Related Works. Section 4 presents our system 
architecture and description of the different runs. 
In section 4 we describe the different features 
used in our system. Results and a discussion are 
provided in Section 5 and finally we conclude in 
Section 6. 
2 Related Works 
There are more extensive literature on measuring 
the similarity between documents than to 
between sentences. Perhaps the most recently 
scenario is constituted by the competition of 
SemEval-2012 task 6: A Pilot on Semantic 
Textual Similarity (Aguirre and Cerd, 2012). In 
SemEval-2012, there were used different tools 
and resources like stop word list, multilingual 
corpora, dictionaries, acronyms, and tables of 
paraphrases, ?but WordNet was the most used 
resource, followed by monolingual corpora and 
Wikipedia? (Aguirre and Cerd, 2012). 
According to Aguirre, Generic NLP tools were 
widely used. Among those that stand out were 
tools for lemmatization and POS-tagging 
(Aguirre and Cerd, 2012). On a smaller scale 
word sense disambiguation, semantic role 
labeling and time and date resolution. In 
addition, Knowledge-based and distributional 
methods were highly used. Aguirre and Cerd 
remarked on (Aguirre and Cerd, 2012) that 
alignment and/or statistical machine translation 
software, lexical substitution, string similarity, 
textual entailment and machine translation 
evaluation software were used to a lesser extent. 
It can be noted that machine learning was widely 
used to combine and tune components. 
Most of the knowledge-based methods ?obtain 
a measure of relatedness by utilizing lexical 
resources and ontologies such as WordNet 
(Miller et al, 1990b) to measure definitional 
overlap, term distance within a graphical 
taxonomy, or term depth in the taxonomy as a 
measure of specificity? (Banea et al, 2012). 
Some scholars as in (Corley and Mihalcea, 
June 2005) have argue ?the fact that a 
comprehensive metric of text semantic similarity 
should take into account the relations between 
words, as well as the role played by the various 
entities involved in the interactions described by 
each of the two sentences?. This idea is resumed 
in the Principle of Compositionality, this 
principle posits that the meaning of a complex 
expression is determined by the meanings of its 
constituent expressions and the rules used to 
combine them (Werning et al, 2005). Corley 
and Mihalcea in this article combined metrics of 
word-to-word similarity, and language models 
into a formula and they pose that this is a 
potentially good indicator of the semantic 
similarity of the two input texts sentences. They 
modeled the semantic similarity of a sentence as 
a function of the semantic similarity of the 
component words (Corley and Mihalcea, June 
2005). 
One of the top scoring systems at SemEval-
2012 (?ari? et al, 2012) tended to use most of 
the aforementioned resources and tools. They 
predict the human ratings of sentence similarity 
using a support-vector regression model with 
multiple features measuring word-overlap 
similarity and syntax similarity. They also 
compute the similarity between sentences using 
the semantic alignment of lemmas. First, they 
compute the word similarity between all pairs of 
lemmas from first to second sentence, using 
either the knowledge-based or the corpus-based 
semantic similarity. They named this method 
Greedy Lemma Aligning Overlap. 
Daniel B?r presented the UKP system, which 
performed best in the Semantic Textual 
Similarity (STS) task at SemEval-2012 in two 
out of three metrics. It uses a simple log-linear 
regression model, trained on the training data, to 
combine multiple text similarity measures of 
varying complexity. 
3 System architecture and description 
of the runs 
As we can see in Figure 1, our three runs begin 
with the pre-processing of SemEval-2013?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS-tagged using Freeling 2.2 
tool (Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group 
of features. 
110
Figure 1. System Architecture. 
The Run 1 (named MultiSemLex) is our main 
run. This takes into account all extracted features 
and trains a model with a Bagging classifier 
(Breiman, 1996) (using REPTree). The training 
corpus has been provided by SemEval-2013 
competition, in concrete by the Semantic Textual 
Similarity task.  
The Run 2 (named MultiLex) and Run 3 
(named MultiSem) use the same classifier, but 
including different features. Run 2 uses (see 
Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 
4.1, and Lexical-Semantic Alignment (LS-A) 
described in section 4.2. 
On the other hand, Run 3 uses features 
extracted only from Semantic Alignment (SA) 
described in section 4.3. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two phrases. 
Finally, we test our system with the SemEval-
2013 test set (see Table 14 with the results of our 
three runs). The following section describes the 
features extraction process. 
4 Description of the features used in the 
Machine Learning System 
Many times when two phrases are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired in this fact we 
developed various algorithms, which measure 
the level of overlapping by computing a quantity 
of matching words in a pair of phrases. In our 
system, we used as features for a MLS lexical 
and semantic similarity measures. Other features 
were extracted from a lexical-semantic sentences 
alignment and a variant using only a semantic 
alignment. 
4.1 Similarity measures 
We have used well-known string based 
similarity measures like: Needleman-Wunch 
(sequence alignment), Smith-Waterman 
(sequence alignment), Smith-Waterman-Gotoh, 
Smith-Waterman-Gotoh-Windowed-Affine, 
Jaro, Jaro-Winkler, Chapman-Length-Deviation, 
Chapman-Mean-Length, QGram-Distance, 
Block-Distance, Cosine Similarity, Dice 
Similarity, Euclidean Distance, Jaccard 
Similarity, Matching Coefficient, Monge-Elkan 
and Overlap-Coefficient. These algorithms have 
been obtained from an API (Application 
Program Interface) SimMetrics library v1.5 for 
.NET 2.03. We obtained 17 features for our MLS 
from these similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order 
to obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as 
the LED between two sentences. Contrary to 
(Tatu et al, 2006), we do not remove the 
punctuation or stop words from the sentences, 
                                                          
3 Copyright (c) 2006 by Chris Parkinson, available in 
http://sourceforge.net/projects/simmetrics/ 
Run1.Bagging Classifier 
Training set from 
SemEval 2013 
Pre-Processing (using Freeling) 
 
Run 3 Bagging  
classifier 
Run 2 Bagging 
classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
 
Jaro QGra
m 
Rel. 
Concept 
. . . 
Tokenizing Lemmatizing POS tagging 
SemEval 
2013 Test 
set 
     Training Process (using Weka) 
111
neither consider different cost for transformation 
operation, and we used all the operations 
(deletion, insertion and substitution).  
The second one is a variant that we named 
Double Levenshtein?s Edit Distance (DLED) 
(see Table 9 for detail). For this algorithm, we 
used LED to measure the distance between the 
phrases, but in order to compare the words, we 
used LED again (Fern?ndez et al, 2012; 
Fern?ndez Orqu?n et al, 2009). 
Another distance we used is an extension of 
LED named Extended Distance (in spanish 
distancia extendida (DEx)) (see (Fern?ndez et 
al., 2012; Fern?ndez Orqu?n et al, 2009) for 
details). This algorithm is an extension of the 
Levenshtein?s algorithm, with which penalties 
are applied by considering what kind of 
transformation (insertion, deletion, substitution, 
or non-operation) and the position it was carried 
out, along with the character involved in the 
operation. In addition to the cost matrixes used 
by Levenshtein?s algorithm, DEx also obtains 
the Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes 
for determining similarity between strings in a 
single iteration. It is worth noting that the 
inclusion of all these penalizations makes the 
DEx algorithm a good candidate for our 
approach.  
In our previous work (Fern?ndez Orqu?n et al, 
2009), DEx demonstrated excellent results when 
it was compared with other distances as 
(Levenshtein, 1965), (Neeedleman and Wunsch, 
1970), (Winkler, 1999). We also used as a 
feature the Minimal Semantic Distances 
(Breadth First Search (BFS)) obtained between 
the most relevant concepts of both sentences. 
The relevant concepts pertain to semantic 
resources ISR-WN (Guti?rrez et al, 2011; 
2010a), as WordNet (Miller et al, 1990a), 
WordNet Affect (Strapparava and Valitutti, 
2004), SUMO (Niles and Pease, 2001) and 
Semantic Classes (Izquierdo et al, 2007). Those 
concepts were obtained after having applied the 
Association Ratio (AR) measure between 
concepts and words over each sentence. (We 
refer reader to (Guti?rrez et al, 2010b) for a 
further description). 
Another attribute obtained by the system was a 
value corresponding with the sum of the smaller 
distances (using QGram-Distance) between the 
words or the lemmas of the phrase one with each 
words of the phrase two. 
As part of the attributes extracted by the 
system, was also the value of the sum of the 
smaller distances (using Levenshtein) among 
stems, chunks and entities of both phrases. 
4.2 Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried 
to align the phrases by its lemmas. If the lemmas 
coincide we look for coincidences among parts-
of-speech4 (POS), and then the phrase is 
realigned using both. If the words do not share 
the same POS, they will not be aligned. To this 
point, we only have taken into account a lexical 
alignment. From now on, we are going to apply 
a semantic variant. After all the process, the non-
aligned words will be analyzed taking into 
account its WordNet?s relations (synonymy, 
hyponymy, hyperonymy, derivationally-related-
form, similar-to, verbal group, entailment and 
cause-to relation); and a set of equivalences like 
abbreviations of months, countries, capitals, days 
and currency. In case of hyperonymy and 
hyponymy relation, words are going to be 
aligned if there is a word in the first sentence 
that is in the same relation (hyperonymy or 
hyponymy) with another one in the second 
sentence. For the relations ?cause-to? and 
?implication? the words will be aligned if there 
is a word in the first sentence that causes or 
implicates another one in the second sentence. 
All the other types of relations will be carried 
out in bidirectional way, that is, there is an 
alignment if a word of the first sentence is a 
synonymous of another one belonging to the 
second one or vice versa. 
Finally, we obtain a value we called alignment 
relation. This value is calculated as ??? =
 ??? / ????. Where ??? is the final 
alignment value, ??? is the number of aligned 
words, and ???? is the number of words of the 
shorter phrase. The  ??? value is also another 
feature for our system. Other extracted attributes 
they are the quantity of aligned words and the 
quantity of not aligned words. The core of the 
alignment is carried out in different ways, which 
                                                          
4 (noun, verb, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, etc.) 
112
are obtained from several attributes.  Each way 
can be compared by: 
? the part-of-speech. 
? the morphology and the part-of-speech. 
? the lemma and the part-of-speech. 
? the morphology, part-of-speech, and 
relationships of WordNet. 
? the lemma, part-of-speech, and 
relationships of WordNet. 
4.3 Semantic Alignment 
This alignment method depends on calculating 
the semantic similarity between sentences based 
on an analysis of the relations, in ISR-WN, of 
the words that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according 
to their POS, creating different groups. 
The distance between two words will be the 
distance, based on WordNet, of the most 
probable sense of each word in the pair, on the 
contrary of our previously system in SemEval 
2012. In that version, we assumed the selected 
sense after apply a double Hungarian Algorithm 
(Kuhn, 1955), for more details  please refer to 
(Fern?ndez et al, 2012). The distance is 
computed according to the equation (1): 
?(?, ?) = ? ? ? ?(?[?], ?[? + 1])?=??=0 ; (1) 
Where ? is the collection of synsets 
corresponding to the minimum path between 
nodes ? and ?, ? is the length of ? subtracting 
one, ? is a function that search the relation 
connecting ? and ? nodes, ? is a weight 
associated to the relation searched by ? (see 
Table 1). 
Relation Weight 
Hyponym, Hypernym 2 
Member_Holonym, Member_Meronym, 
Cause, Entailment 
5 
Similar_To 10 
Antonym 200 
Other relation different to Synonymy 60 
Table 1. Weights applied to WordNet relations. 
Table 1 shows the weights associated to 
WordNet relations between two synsets. 
Let us see the following example: 
? We could take the pair 99 of corpus 
MSRvid (from training set of SemEval-
2013) with a littler transformation in 
order to a better explanation of our 
method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using equation (1), a matrix with the 
distances between all groups of both phrases is 
created (see Table 2). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 2. Distances between groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of 
the first sentence is checked with each element 
of the second sentence, and the rest is marked as 
words that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, 
so the number of non-aligned words is two. 
There is only one perfect match: ?group-group? 
(match with cost=0). The length of the shortest 
sentence is four. The Table 3 shows the results 
of this analysis. 
Number of exact 
coincidence 
Total Distances of 
optimal Matching 
Number of 
non-aligned 
Words 
1 5 2 
Table 3. Features from the analyzed sentences. 
This process has to be repeated for nouns (see 
Table 4), verbs, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, 
digits and date times. On the contrary, the tables 
have to be created only with the similar groups 
of the sentences. Table 4 shows features 
extracted from the analysis of nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 4. Distances between groups of nouns. 
113
Number of 
exact 
coincidence 
Total Distances 
of optimal 
Matching 
Number of non-aligned 
Words 
1 3 0 
Table 5. Feature extracted from analysis of nouns. 
Several attributes are extracted from the pair of 
sentences (see Table 3 and Table 5). Three 
attributes considering only verbs, only nouns, 
only adjectives, only adverbs, only prepositions, 
only conjunctions, only pronouns, only 
determinants, only modifiers, only digits, and 
only date times. These attributes are:  
? Number of exact coincidences 
? Total distance of matching 
? Number of words that do not match 
Many groups have particular features 
according to their parts-of-speech. The group of 
the nouns has one more feature that indicates if 
the two phrases have the same number (plural or 
singular). For this feature, we take the average of 
the number of each noun in the phrase like a 
number of the phrase.  
For the group of adjectives we added a feature 
indicating the distance between the nouns that 
modify it from the aligned adjectives, 
respectively.  
For the verbs, we search the nouns that precede 
it, and the nouns that are next of the verb, and 
we define two groups. We calculated the 
distance to align each group with every pair of 
aligned verbs. The verbs have other feature that 
specifies if all verbs are in the same verbal time.  
With the adverbs, we search the verb that is 
modified by it, and we calculate their distance 
from all alignment pairs.  
With the determinants and the adverbs we 
detect if any of the alignment pairs are 
expressing negations (like don?t, or do not) in 
both cases or not. Finally, we determine if the 
two phrases have the same principal action. For 
all this new features, we aid with Freeling tool. 
As a result, we finally obtain 42 attributes from 
this alignment method. It is important to remark 
that this alignment process searches to solve, for 
each word from the rows (see Table 4) it has a 
respectively word from the columns. 
4.4 Description of the alignment feature 
From the alignment process, we extract different 
features that help us a better result of our MLS. 
Table 6 shows the group of features with lexical 
and semantic support, based on WordNet 
relation (named F1). Each of they were named 
with a prefix, a hyphen and a suffix. Table 7 
describes the meaning of every prefix, and Table 
8 shows the meaning of the suffixes. 
Features 
CPA_FCG, CPNA_FCG, SIM_FCG, CPA_LCG, 
CPNA_LCG, SIM_LCG, CPA_FCGR, 
CPNA_FCGR, SIM_FCGR, CPA_LCGR, 
CPNA_LCGR, SIM_LCGR 
Table 6. F1. Semantic feature group. 
Prefixes Descriptions 
CPA Number of aligned words. 
CPNA Number of non-aligned words. 
SIM Similarity 
Table 7. Meaning of each prefixes. 
Prefixes Compared words for? 
FCG Morphology and POS 
LCG Lemma and POS 
FCGR Morphology, POS and WordNet relation. 
LCGR Lemma, POS and WordNet relation. 
Table 8. Suffixes for describe each type of alignment. 
Features Descriptions 
LevForma Levenshtein Distance between two 
phrases comparing words by 
morphology 
LevLema The same as above, but now 
comparing by lemma. 
LevDoble Idem, but comparing again by 
Levenshtein and accepting words 
match if the distance is ? 2. 
DEx Extended Distance 
NormLevF, 
NormLevL 
Normalized forms of LevForma and 
LevLema. 
Table 9. F2. Lexical alignment measures. 
Features 
NWunch, SWaterman, SWGotoh, SWGAffine, Jaro, 
JaroW, CLDeviation, CMLength, QGramD, BlockD, 
CosineS, DiceS, EuclideanD, JaccardS, MaCoef, 
MongeElkan, OverlapCoef. 
Table 10. Lexical Measure from SimMetrics library. 
Features Descriptions 
AxAQGD_L All against all applying QGramD 
and comparing by lemmas of the 
words. 
AxAQGD_F Same as above, but applying 
QGramD and comparing by 
morphology. 
AxAQGD_LF Idem, not only comparing by lemma 
but also by morphology. 
AxALev_LF All against all applying Levenhstein 
114
comparing by morphology and 
lemmas. 
AxA_Stems Idem, but applying Levenhstein 
comparing by the stems of the 
words. 
Table 11. Aligning all against all. 
Other features we extracted were obtained 
from the following similarity measures (named 
F2) (see Table 9 for detail). 
We used another group named F3, with lexical 
measure extracted from SimMetric library (see 
Table 10 for detail). 
Finally we used a group of five feature (named 
F4), extracted from all against all alignment (see 
Table 11 for detail). 
4.5 Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
as a training corpus. Using ten-fold cross 
validation with the classifier mentioned in 
section 3 (experimentally selected). 
As we can see in Table 12, the attributes 
corresponding with the Test 1 (only lexical 
attributes) obtain 0.7534 of correlation. On the 
other side, the attributes of the Test 2 (lexical 
features with semantic support) obtain 0.7549 of 
correlation, and all features obtain 0.7987. Being 
demonstrated the necessity to tackle the problem 
of the similarity from a multidimensional point 
of view (see Test 3 in the Table 12). 
Features 
Correlation on the training data of SemEval-
2013 
Test 1 Test 2 Test 3 
F1 
 0.7549 
0.7987 
F2 
F3 0.7534  
F4   
Table 12. Features influence. Gray cells mean 
features are not taking into account. 
5 Result and discussion 
Semantic Textual Similarity task of SemEval-
2013 offered two official measures to rank the 
systems5: Mean- the main evaluation value, 
Rank- gives the rank of the submission as 
ordered by the "mean" result. 
                                                          
5http://ixa2.si.ehu.es/sts/index.php?option=com_content&vi
ew=article&id=53&Itemid=61 
Test data for the core test datasets, coming 
from the following: 
Corpus Description 
Headlineas: news headlines mined from several news 
sources by European Media Monitor 
using the RSS feed. 
OnWN: mapping of lexical resources OnWN. The 
sentences are sense definitions from 
WordNet and OntoNotes. 
FNWN: the sentences are sense definitions from 
WordNet and FrameNet. 
SMT: SMT dataset comes from DARPA GALE 
HTER and HyTER. One sentence is a 
MT output and the other is a reference 
translation where a reference is generated 
based on human post editing. 
Table 13. Test Core Datasets. 
Using these measures, our second run (Run 2) 
obtained the best results (see Table 14). As we 
can see in Table 14, our lexical run has obtained 
our best result, given at the same time worth 
result in our other runs. This demonstrates that 
tackling this problem with combining multiple 
lexical similarity measure produce better results 
in concordance to this specific test corpora. 
To explain Table 14 we present following 
descriptions: caption in top row mean: 1- 
Headlines, 2- OnWN, 3- FNWN, 4- SMT and 5- 
mean. 
Run 1 R 2 R 3 R 4 R 5 R 
1 0.5841 60 0.4847 54 0.2917 52 0.2855 66 0.4352 58 
2 0.6168 55 0.5557 39 0.3045 50 0.3407 28 0.4833 44 
3 0.3846 85 0.1342 88 -0.0065 85 0.2736 72 0.2523 87 
Table 14. Official SemEval-2013 results over test 
datasets. Ranking (R). 
The Run 1 is our main run, which contains the 
junction of all attributes (lexical and semantic 
attributes).  Table 14 shows the results of all the 
runs for a different corpus from test phase. As 
we can see, Run 1 did not obtain the best results 
among our runs. 
Otherwise, Run 3 uses more semantic analysis 
than Run 2, from this; Run 3 should get better 
results than reached over the corpus of FNWN, 
because this corpus is extracted from FrameNet 
corpus (Baker et al, 1998) (a semantic network). 
FNWN provides examples with high semantic 
content than lexical. 
Run 3 obtained a correlation coefficient of 
0.8137 for all training corpus of SemEval 2013, 
115
while Run 2 and Run 1 obtained 0.7976 and 
0.8345 respectively with the same classifier 
(Bagging using REPTree, and cross validation 
with ten-folds). These results present a 
contradiction between test and train evaluation. 
We think it is consequence of some obstacles 
present in test corpora, for example:  
In headlines corpus there are great quantity of 
entities, acronyms and gentilics that we not take 
into account in our system. 
The corpus FNWN presents a non-balance 
according to the length of the phrases. 
In OnWN -test corpus-, we believe that some 
evaluations are not adequate in correspondence 
with the training corpus. For example, in line 7 
the goal proposed was 0.6, however both phrases 
are semantically similar. The phrases are: 
? the act of lifting something 
? the act of climbing something. 
We think that 0.6 are not a correct evaluation 
for this example. Our system result, for this 
particular case, was 4.794 for Run 3, and 3.814 
for Run 2, finally 3.695 for Run 1. 
6 Conclusion and future works 
This paper have introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional 
interpretation of a text. 
As mentioned in section 3 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 14 
that all runs obtained encouraging results. Our 
best run was situated at 44th position of 90 runs 
of the ranking of SemEval-2013.  Table 12 and 
Table 14 show the reached positions for the three 
different runs and the ranking according to the 
rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment and Semantic Alignment. 
We have conducted the semantic features 
extraction in a multidimensional context using 
the resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWordNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we 
show that this approach can be used to correctly 
classify several examples from the STS task of 
SemEval-2013. Compared with the best run of 
the ranking (UMBC_EBIQUITY- ParingWords) 
(see Table 15) our main run has very close 
results in headlines (1), and SMT (4) core test 
datasets. 
Run 1 2 3 4 5 6 
(First) 0.7642 0.7529 0.5818 0.3804 0.6181 1 
(Our) 
RUN 2 
0.6168 0.5557 0.3045 0.3407 0.4833 44 
Table 15. Comparison with best run (SemEval 2013). 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think 
that with this improvement we can increase the 
results obtained with texts like those in OnWN 
test set. 
6.1 Team Collaboration 
Is important to remark that our team has been 
working up in collaboration with INAOE 
(Instituto Nacional de Astrof?sica, ?ptica y 
Electr?nica) and LIPN (Laboratoire 
d'Informatique de Paris-Nord), Universit? Paris 
13 universities, in order to encourage the 
knowledge interchange and open shared 
technology. Supporting this collaboration, 
INAOE-UPV (Instituto Nacional de Astrof?sica, 
?ptica y Electr?nica and Universitat Polit?cnica 
de Val?ncia) team, in concrete in INAOE-UPV-
run 3 has used our semantic distances for nouns, 
adjectives, verbs and adverbs, as well as lexical 
attributes like LevDoble, NormLevF, NormLevL 
and Ext (see influence of these attributes in 
Table 12). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
116
Reference 
Agirre, E.; D. Cer; M. Diab and W. Guo. *SEM 2013 
Shared Task: Semantic Textual Similarity 
including a Pilot on Typed-Similarity. *SEM 
2013: The Second Joint Conference on Lexical and 
Computational Semantics, Association for 
Computational Linguistics, 2013.  
Aguirre, E. and D. Cerd. SemEval 2012 Task 6:A 
Pilot on Semantic Textual Similarity. First Join 
Conference on Lexical and Computational 
Semantic (*SEM), Montr?al, Canada, Association 
for Computational Linguistics., 2012. 385-393 p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. 
Baker, C. F.; C. J. Fillmore and J. B. Lowe. The 
berkeley framenet project. Proceedings of the 17th 
international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, 1998. 86-90 p.  
Banea, C.; S. Hassan; M. Mohler and R. Mihalcea. 
UNT:A Supervised Synergistic Approach to 
SemanticText Similarity. First Joint Conference on 
Lexical and Computational Semantics (*SEM), 
Montr?al. Canada, Association for Computational 
Linguistics, 2012. 635?642 p.  
Breiman, L. Bagging predictors Machine learning, 
1996, 24(2): 123-140. 
Corley, C. and R. Mihalcea. Measuring the Semantic 
Similarity of Texts, Association for Computational 
Linguistic. Proceedings of the ACL Work shop on 
Empirical Modeling of Semantic Equivalence and 
Entailment, pages 13?18, June 2005. 
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; 
A. Gonz?lez; R. Estrada; Y. Casta?eda; S. 
V?zquez; A. Montoyo and R. Mu?oz. 
UMCC_DLSI: Multidimensional Lexical-
Semantic Textual Similarity. {*SEM 2012}: The 
First Joint Conference on Lexical and 
Computational Semantics -- Volume 1: 
Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Fern?ndez Orqu?n, A. C.; J. D?az Blanco; A. Fundora 
Rolo and R. Mu?oz Guillena. Un algoritmo para la 
extracci?n de caracter?sticas lexicogr?ficas en la 
comparaci?n de palabras. IV Convenci?n 
Cient?fica Internacional CIUM, Matanzas, Cuba, 
2009.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010a. 161-168 p. 1135-
5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschberg, D. S. Algorithms for the longest common 
subsequence problem J. ACM, 1977, 24: 664?675. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Kuhn, H. W. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly, 1955, 2: 83?97. 
Levenshtein, V. I. Binary codes capable of correcting 
spurious insertions and deletions of ones. Problems 
of information Transmission. 1965. pp. 8-17 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller. Five papers on WordNet. 
Princenton University, Cognositive Science 
Laboratory, 1990a. 
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-
line Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990b. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Neeedleman, S. and C. Wunsch A general method 
applicable to the search for similarities in the 
amino acid sequence of two proteins Mol. Biol, 
1970, 48(443): 453. 
Niles, I. and A. Pease. Origins of the IEEE Standard 
Upper Ontology. Working Notes of the IJCAI-
2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA., 2001. 
117
?ari?, F.; G. Glava?; Mladenkaran; J. ?najder and B. 
D. Basi?. TakeLab: Systems for Measuring 
Semantic Text Similarity.  Montr?al, Canada, First 
Join Conference on Lexical and Computational 
Semantic (*SEM), pages 385-393. Association for 
Computational Linguistics., 2012.  
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of 
the 4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p. 
Tatu, M.; B. Iles; J. Slavick; N. Adrian and D. 
Moldovan. COGEX at the Second Recognizing 
Textual Entailment Challenge. Proceedings of the 
Second PASCAL Recognising Textual Entailment 
Challenge Workshop, Venice, Italy, 2006. 104-109 
p.  
Werning, M.; E. Machery and G. Schurz. The 
Compositionality of Meaning and Content, 
Volume 1: Foundational issues. ontos verlag 
[Distributed in] North and South America by 
Transaction Books, 2005. p. Linguistics & 
philosophy, Bd. 1. 3-937202-52-8. 
Winkler, W. The state of record linkage and current 
research problems. Technical Report, Statistical 
Research Division, U.S, Census Bureau, 1999. 
 
 
118
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 93?97, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(EPS): Paraphrases Detection Based on Semantic 
Distance 
 
H?ctor D?vila, Antonio Fern?ndez Orqu?n, 
Alexander Ch?vez, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba. 
{hector.davila, tony, 
alexander.chavez, yoan.gutierrez, 
armando.collazo, 
jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera 
de San Vicente S/N Alicante, Spain. 
{montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI-(EPS) system, which 
participated in the first Evaluating Phrasal 
Semantics of SemEval-2013. Our supervised 
system uses different kinds of semantic 
features to train a bagging classifier used to 
select the correct similarity option. Related to 
the different features we can highlight the 
resource WordNet used to extract semantic 
relations among words and the use of different 
algorithms to establish semantic similarities. 
Our system obtains promising results with a 
precision value around 78% for the English 
corpus and 71.84% for the Italian corpus. 
1 Introduction 
It is well known finding words similarity, even 
when it is lexical or semantic can improve 
entailment recognition and paraphrase 
identification; and ultimately lead to improvements 
in a wide range of applications in Natural 
Language Processing (NLP). Several areas like 
question answering, query expansion, information 
retrieval, and many others, depend on phrasal 
semantics (PS). PS, is concerned with how the 
meaning of a sentence is composed both from the 
meaning of the constituent words, and from extra 
meaning contained within the structural 
organization of the sentence itself (Dominey, 
2005). 
The aim of SemEval 2013 competition is also 
discovering similarity, specifically in Evaluating 
Phrasal Semantics (EPS). The goal of this task is to 
evaluate how well systems can judge the semantic 
similarity of a word and a short sequence of words. 
That is, given a set of pairs of this type; classify it 
on negative (if the meaning of the word is 
semantically different to the meaning of the 
sequence) or positive (if the meaning of the 
sequence, as a whole, is semantically close to the 
meaning of the word).  
Based on this, we developed a system capable to 
detect if two phrases are semantically close. 
The rest of this paper, specifically section 2 is a 
brief Related Work. Section 3 describes the system 
architecture and our run. Continuing with section 4 
we describe the training phase. Following that, 
section 5 presents the results and discussion for our 
Machine Learning System. Finally we conclude 
and propose our future works (Section 6). 
2 Related Work 
There have been many WordNet-based similarity 
measures, among other highlights the work of 
researchers like (Budanitsky and Hirst, 2006; 
Leacock and Chodorow, 1998; Mihalcea et al, 
2006; Richardson et al, 1994). 
On the other hand, WordNet::Similarity1 
(Pedersen et al, 2004) has been used by other 
researchers in an interesting array of domains. 
WordNet::Similarity implements measures of 
similarity and relatedness between a pair of 
concepts (or synsets2) based on the structure and 
content of WordNet. According to (Pedersen et al, 
2004), three of the six measures of similarity are 
based on the information content of the least 
                                                     
1http://sourceforge.net/projects/wn-similarity/ 
2 A group of English words into sets of synonyms. 
93
common subsumer (LCS). These measures include 
res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang 
and Conrath, 1997). 
Pursuant to Pedersen, there are three other 
similarity measures based on path lengths between 
a pair of concepts: lch (Leacock and Chodorow, 
1998), wup (Wu and Palmer, 1994), and path. 
Our proposal differs from those of 
WordNet::Similarity and other measures of 
similarity in the way we selected the relevant 
WordNet relations (see section 3.2 for detail). 
Unlike others, our measure assign weight to 
WordNet relations (any we consider relevant) 
depending to the place they occupy in the 
minimum path and the previously visited relations. 
Besides these, the novelty of our approach is 
using the weights as a function of semantic 
relations in a minimal distance path and also the 
method we used to arrive to those weight functions 
or rules. 
3 System Architecture and description of 
the run 
As we can see in Figure 1 our run begin with the 
pre-processing of SemEval 2013?s training set. 
Every sentence pair is tokenized, lemmatized and 
POS-tagged using Freeling 2.2 tool (Atserias et al, 
2006). Afterwards, several methods and algorithms 
are applied in order to extract all features for our 
Machine Learning System (MLS). The system 
trains the classifier using a model based on 
bagging (using JRip3). The training corpus has 
been provided by SemEval-2013 competition, in 
concrete by the EPS task. As a result, we obtain a 
trained model capable to detect if one phrase 
implies other. Finally, we test our system with the 
SemEval 2013 test set (see Table 2 with the results 
of our run). The following section describes the 
features extraction process. 
3.1 Description of the features used in the 
Machine Learning System 
In order to detect entailment between a pair of 
phrases, we developed an algorithm that searches a 
semantic distance, according to WordNet (Miller et 
al., 1990), between each word in the first phrase 
with each one in the second phrase. 
We used four features which intend to measure 
the level of proximity between both sentences: 
                                                     
3 JRip is an inference and rules-based learner. 
? The minimum distance to align the first 
phrase with the second (MinDist). See section 
3.2 for details. 
? The maximal distance to align the first phrase 
with the second (MaxDist). 
? The average of all distances results to align 
the first phrase with the second one. 
(AverageDistance). 
? The absolute relative error of all distances 
results to align the first phrase with the 
second respect to the average of them. 
 
Figure 1. System Architecture. 
Other features included are the most frequent 
relations contained in the shorted path of the 
minimum distance; result to align the first phrase 
with the second one. Following table shows the 
relations selected as most frequent. 
A weight was added to each of them, according 
to the place it occupy in the shortest path between 
two synsets. The shortest path was calculated using 
Breadth -First-Search algorithm (BFS) (Cormen et 
al., 2001). 
In addition, there is one feature that takes into 
account any other relationship that is not 
previously considered. 
Finally, as a result we obtain 22 features from 
this alignment method. 
Semeval 2013 test 
set
?
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Run 1
Bagging Classifier (JRip)
Feature Extraction
MinDistance MaxDistance error ?
Training set from 
Semeval 2013
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Feature Extraction
MinDistance MaxDistance error
Supervised Model
Training process (using Weka)
Bagging Classifier (JRip)
Paraphrases Detection
94
Relation Weight (? function) 
Antonym 1000 
Synonym 0 
Hyponym/ Hypernym 
100 if exist an antonym 
before, 30 if exist other 
relation before (except 
synonym, hyponym, 
hypernym), 5 otherwise. 
Meber_Holonym/ 
PartHolonym 
100 if exist an antonym 
before, 20 if exist a 
hyponym or a hypernym,10 
otherwise. 
Cause/ Entailment 
100 if exist an antonym 
before, 2 otherwise. 
Similar_To 
100 if exist an antonym 
before, 3 otherwise. 
Attribute 
100 if exist an antonym 
before, 8 otherwise. 
Also_See 
100 if exist an antonym 
before, 10 otherwise. 
Derivationaly_Related_Form 
100 if exist an antonym 
before, 5 otherwise. 
Domain_Of_Synset_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Domain_Of_Synset_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Member_Of_Domain_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Member_Of_Domain_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Other 100 
Table 1. Most frequents relations with their weight. 
3.2 Semantic Distance 
As aforementioned, our distance depends on 
calculating the similarity between sentences, based 
on the analysis of WordNet relations, and we only 
took into account the most frequent ones. When 
searching the shortest path between two WordNet 
synsets, frequents relations were considered the 
ones extracted according to the analysis made in 
the training corpus, provided by SemEval-2013. 
The distance between two synsets is calculated 
with the relations found; and simply it is the sum 
of the weights assigned to each connection. 
????????(?, ?) =  ????????(??, ??), ? (?, ?) (1) 
????????(?, ?) = ???(?? , ??), ?(?, ?) (2) 
???(??; ??) = ? ?(???(?[?], ?[? + 1]))
?=?
?=0
 (3) 
? = ???(??; ??) (4) 
Where ? and ? represents the i-th and j-th sense of 
the word; P and Q represents words collections; ?? 
is the X-th word of ?; ?? is the Y-th word of ?; 
???????? obtains a value that represents a 
minimal semantic distance across WordNet (Miller 
et al, 2006) resource (this resource is involved into 
the integrator resource, ISR-WN (Guti?rrez et al, 
2011a; 2010a); ????????  the minimal semantic 
distance between two words; ??? represents the 
minimal semantic distance between two senses 
collections; ? is a collection of synsets that 
represents the minimal path between two synsets 
using BFS; ??? obtains semantic relation types 
between two synsets; W is a functions that apply 
the rules described in Table 1. The maximum and 
average distance is calculated in a similar fashion 
but using the maximum and average instead of the 
minimum. 
3.3 Semantic Alignment 
First, the two sentences are pre-processed with 
Freeling 2.2 and the words are classified according 
to their parts-of-speech. Then, all senses of every 
word are taken and treated as a group. Distance 
between two groups will be the minimal distance 
(described in 3.1) between senses of any pair of 
words belonging to the group. 
In the example of Figure 2, Dist=280 is selected 
for the pair ?Balance-Culture? (minimal cost).  
Following the explanation on section 3.1 we 
extract the features guided to measure the level of 
proximity between both sentences. 
 
Figure 2. Distance between ?Balance? and ?Culture?. 
A maximum and average distance is calculated in a 
similar fashion, but using the maximum and 
average instead of the minimum. 
4 Description of the training phase 
For the training process, we used a supervised 
learning framework (based on Weka4), including 
all the training set (positive and negative instances) 
as a training corpus. We conduct several 
experiments in order to select the correct classifier, 
the best result being obtained with a model based 
on bagging (using JRip algorithm). Finally, we 
used 10-fold cross validation technique with the 
selected classifier, obtaining a classification value 
of 73.21%. 
                                                     
4 http://prdownloads.sourceforge.net/weka/ 
L mma: Balance
Sense 1
Sense 2
Lemma: Culture
Sense 1
Sense 2
3350
1030 280
880
Dist=280
95
5 Results and discussion 
EPS task of SemEval-2013 offered many official 
measures to rank the systems. Some of them are 
the following: 
o F-Measure (FM): Correct Response (CR), 
Instances correctly classified, True positives 
(TP), Instances correctly classified as 
positive. False Positives (FP), Instances 
incorrectly classified as positive, True 
Negatives (TN), Instances correctly 
classified as negative, False Negatives (FN), 
Instances incorrectly classified as negative. 
Corpus FM CR TP FP TN FN 
English 0.6892 2826 1198 325 1628 755 
Italian 0.6396 574 245 96 329 180 
Table 2. Official SemEval 2013 results. 
The behavior of our system, for English and 
Italian corpus is shown in Table 2. 
The only thing that changes to process the 
Italian corpus is that Freeling is used as input to 
identify Italian words and it returns the English 
WN synsets. The process continues in the same 
way as English. 
Figure 3: Semantic Distance distribution between 
negative and positive instances.  
As shown in Table 2, our main drawback is to 
classify positive instances. Sometimes, the distance 
between positive phrases is very far. This is due to 
the relations found in the minimum path are very 
similar to the one found in other pairs of negatives 
instances; this can be the cause of our MLS 
classifies them as negatives (see Figure 3). 
Figure 3 shows a distributional graphics that 
take a sample of 200 negative and positive 
instances. The graphics illustrate how close to zero 
value the positive instances are, while the 
negatives are far away from this value. However, 
in the approximate range between 80 and 200, we 
can see values of positive and negative instances 
positioning together. This can be the cause that our 
MLS misclassified some positive instances as 
negative. 
6 Conclusion and future work 
This paper introduced a new framework for EPS, 
which depends on the extraction of several features 
from WordNet relations. We have conducted the 
semantic features extraction in a multidimensional 
context using the resource ISR-WN(Guti?rrez et 
al., 2010a). 
Our semantic distance provides an appealing 
approach for dealing with phrasal detection based 
on WordNet relation. Our team reached the sixth 
position of ten runs for English corpus, with a 
small difference of 0.07 points compared to the 
best results with respect to accuracy parameter. 
Despite the problems caused by poorly selected 
positive instances, our distance (labeled as Our) 
obtained very similar results to those obtained by 
the best team (labeled as First5), which indicates 
that our work is well underway (see Table 3 for 
details). 
Team accuracy recall precision 
First 0.802611 0.751664 0.836944128 
Our 0.723502 0.613415 0.786605384 
Table 3. Comparative results (English corpus). 
It is important to remark that our system has 
been the only competitor to evaluate Italian texts. 
It has been possible due to our system include 
Freeling in the preprocessing stage. 
Our future work will aim to resolve instances 
misclassified by our algorithm. In addition, we will 
introduce lexical substitutions (synonyms) to 
expand the corpus, we will also apply conceptual 
semantic similarity using relevant semantic trees 
(Guti?rrez et al, 2010b; Guti?rrez et al, 2011b). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
                                                     
5 christian_wartena. Team HsH. 
0
500
1000
Semantic Distance Distribution
Positive Instances Negative Instances
96
semantic services in an open-source NLP library. 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC?06), 
2006. 48-55 p.  
Budanitsky, A. and G. Hirst Evaluating wordnet-based 
measures of lexical semantic relatedness 
Computational Linguistics, 2006, 32(1): 13-47. 
Cormen, T. H.; C. E. Leiserson; R. L. Rivest and C. 
Stein. Introduction to algorithms. MIT press, 2001. 
0262032937. 
Dominey, P. F. Aspects of descriptive, referential, and 
information structure in phrasal semantics: A 
construction-based model Interaction Studies, 2005, 
6(2): 287-310. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, 
RANLP 2011 Organising Committee, 2011b. 233--
239 p.  
Jiang, J. J. and D. W. Conrath Semantic similarity based 
on corpus statistics and lexical taxonomy arXiv 
preprint cmp-lg/9709008, 1997. 
Leacock, C. and M. Chodorow Combining local context 
and WordNet similarity for word sense identification 
WordNet: An electronic lexical database, 1998, 
49(2): 265-283. 
Lin, D. An information-theoretic definition of 
similarity. Proceedings of the 15th international 
conference on Machine Learning, San Francisco, 
1998. 296-304 p.  
Mihalcea, R.; C. Corley and C. Strapparava. Corpus-
based and knowledge-based measures of text 
semantic similarity. Proceedings of the national 
conference on artificial intelligence, Menlo Park, 
CA; Cambridge, MA; London; AAAI Press; MIT 
Press; 1999, 2006. 775 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Miller, G. A.; C. Fellbaum; R. Tengi; P. Wakefield; H. 
Langone and B. R. Haskell. WordNet a lexical 
database for the English language. Cognitive Science 
Laboratory Princeton University 2006. 
Pedersen, T.; S. Patwardhan and J. Michelizzi. 
WordNet:: Similarity: measuring the relatedness of 
concepts. Demonstration Papers at HLT-NAACL 
2004, Association for Computational Linguistics, 
2004. 38-41 p.  
Resnik, P. Using information content to evaluate 
semantic similarity in a taxonomy arXiv preprint 
cmp-lg/9511007, 1995. 
Richardson, R.; A. F. Smeaton and J. Murphy. Using 
WordNet as a knowledge base for measuring 
semantic similarity between words, Technical Report 
Working Paper CA-1294, School of Computer 
Applications, Dublin City University, 1994. 
Wu, Z. and M. Palmer. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting on 
Association for Computational Linguistics, 
Association for Computational Linguistics, 1994. 
133-138 p.  
 
 
97
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 241?249, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense 
Frequencies and Multidimensional Semantic Resources to solve 
Multilingual Word Sense Disambiguation 
 
Yoan Guti?rrez, Yenier 
Casta?eda, Andy Gonz?lez, 
Rainel Estrada, Dennys D. Piug, 
Jose I. Abreu, Roger P?rez 
Antonio Fern?ndez Orqu?n, 
Andr?s Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas DLSI, University of Alicante Independent Consultant 
Matanzas, Cuba Alicante, Spain USA 
{yoan.gutierrez, 
yenier.castaneda, 
rainel.estrada, 
dennys.puig, jose.abreu, 
roger.perez}@umcc.cu, 
andy.gonzalez@infonet.umcc
.cu 
antonybr@yahoo.com, 
{montoyo,rafael}@dlsi.ua.
es 
info@franccamara.c
om 
 
Abstract 
This work introduces a new unsupervised 
approach to multilingual word sense 
disambiguation. Its main purpose is to 
automatically choose the intended sense 
(meaning) of a word in a particular context for 
different languages. It does so by selecting the 
correct Babel synset for the word and the 
various Wiki Page titles that mention the 
word. BabelNet contains all the output 
information that our system needs, in its Babel 
synset. Through Babel synset, we find all the 
possible Synsets for the word in WordNet. 
Using these Synsets, we apply the 
disambiguation method Ppr+Freq to find what 
we need. To facilitate the work with WordNet, 
we use the ISR-WN which offers the 
integration of different resources to WordNet. 
Our system, recognized as the best in the 
competition, obtains results around 69% of 
Recall. 
1 Introduction 
Word Sense Disambiguation (WSD) focuses on 
resolving the semantic ambiguity of a given word.  
This is an important task in Natural Language 
Processing (NLP) because in many applications, 
such as Automatic Translation, it is essential to 
know the exact meaning of a word in a given 
context. In order to solve semantic ambiguity, 
different systems have been developed. However, 
we can categorize them in two main groups: 
supervised and unsupervised systems. The 
supervised ones need large quantity of hand-tagged 
data in order to gather enough information to build 
rules, train systems, and so on. Unsupervised 
systems, on the other hand, do not need such a 
large amount of hand-tagged datasets. This means 
that, when there aren?t enough corpora to train the 
systems, an unsupervised system is a good option. 
A sub-task of WSD is Multilingual Word Sense 
Disambiguation (MWSD) (Navigli et al, 2013) 
that aims at resolving ambiguities in different 
languages. 
In a language, there are words that have only one 
sense (or meaning), but in other languages, the 
same words can have different senses. For 
example, ?patient? is a word that in English can be 
either a noun or an adjective, but in German, it 
only has one sense - ?viz? (a person that needs 
treatment). This shows that the information 
obtained by combining two languages can be more 
useful for WSD because the word senses in each 
language can complement each other. For it to be 
useful, MWSD needs a multilingual resource that 
contains different languages, such as BabelNet 
(Navigli and Ponzetto, 2010; 2012) and 
EuroWordNet (Vossen, 1998). 
241
As the preferred disambiguation method, we 
decided to use the Ppr+Freq (Personalized Page 
Rank combined with Frequencies of senses)  
(Guti?rrez, 2012) method because, among 
unsupervised systems, graph-based methods have 
obtained more promising results.  
It is worth mentioning the relevant approaches 
used by the scientific community to achieve 
promising results. One approach used is structural 
interconnections, such as Structural Semantic 
Interconnections (SSI), which create structural 
specifications of the possible senses for each word 
in a context (Navigli and Velardi, 2005). The other 
approaches used are ?Exploring the integration of 
WordNet? (Miller et al, 1990), FrameNet (Laparra 
et al, 2010) and those using Page-Rank such as 
(Sinha and Mihalcea, 2007) and (Agirre and Soroa, 
2009). 
The aforementioned types of graph based 
approaches have achieved relevant results in both 
the SensEval-2 and SensEval-3 competitions (see 
Table 1). 
Algorithm Recall 
TexRank (Mihalcea, 2005)  54.2% 
(Sinha and Mihalcea, 2007) 56.4% 
(Tsatsaronis et al, 2007) 49.2% 
Ppr (Agirre and Soroa, 2009) 58.6% 
Table 1. Relevant WSD approaches. Recall measure is 
calculated recalls using SensEval-2 (English All Word 
task) guidelines over. 
Experiments using SensEval-2 and SensEval-3 
corpora suggest that Ppr+Freq (Guti?rrez, 2012) 
can lead to better results by obtaining over 64% of 
Recall. Therefore we selected Ppr+Freq as the 
WSD method for our system. 
The key proposal for this work is an 
unsupervised algorithm for MWSD, which uses an 
unsupervised method, Ppr+Freq, for semantic 
disambiguation with resources like BabelNet (as 
sense inventory only) (Navigli and Ponzetto, 2010) 
and ISR-WN (as knowledge base) (Guti?rrez et al, 
2011a; 2010a). 
ISR-WN was selected as the default knowledge 
base because of previous NLP research, which 
included: (Fern?ndez et al, 2012; Guti?rrez et al, 
2010b; Guti?rrez et al, 2012; 2011b; 2011c; 
2011d), which achieved relevant results using ISR-
WN as their knowledge base. 
2 System architecture  
By using one of BabelNet (BN) features, our 
technique begins by looking for all the Babel 
synsets (Bs) linked to the lemma of each word in 
the sentence that we need to disambiguate.  
Through the Bs offsets, we can get its 
corresponding WordNet Synset (WNS), which 
would be retrieved from WordNet (WN) using the 
ISR-WN resource. As a result, for each lemma, we 
have a WordNet Synset List (WNSL) from which 
our Word Sense Disambiguation method obtains 
one WNS as the correct meaning. 
Our WSD method consists of applying a 
modification of the Personalizing PageRank (Ppr) 
algorithm (Agirre and Soroa, 2009), which 
involves the senses frequency. More specifically, 
the key proposal is known as Ppr+Freq (see 
Section 2.3).  
Given a set of WNSLs of WNSL, as words 
window, we applied the Synsets ranking method, 
Ppr+Freq, which ranks in a descending order, the 
Synsets of each lemma according to a calculated 
factor of relevance. The first Synset (WNS) of 
each WNSL (the most relevant) is established as 
the correct one and its associated Babel synset (Bs) 
is also tagged as correct. To determine the Wiki 
Page Titles (WK), we examine the WIKI 
(Wikipedia pages) and WIKIRED (Wikipedia 
pages redirections) in the correct Babel synset 
obtained. 
Figure 1 shows a general description of our 
system that is made up of the following steps: 
I. Obtaining lemmas  
II. Obtaing WN Synset of selected lemmas  
III. Applying Ppr+Freq method  
IV. Assigning Synset, Babel synset and Wiki 
page title 
Note that ISR-WN contains WN as its nucleus. 
This allows linking both resources, BabelNet and 
ISR-WN.
242
 
Figure 1. General process description taking as instance a sentence provided by the trial dataset. 
 
2.1 Obtaining lemmas  
For each input sentence, we extract the labeled 
lemmas. As an example, for the sentence, ?The 
struggle against the drug lords in Colombia will be 
a near thing,? the selected lemmas are: ?struggle,? 
?drug_lord,? ?Colombia?, and ?near_thing.? 
 
Figure 2. Obtaining synset of lemmas. 
 
2.2 Obtaing WN Synset of selected lemmas  
For each lemma obtained in the previous section, 
we look through BabelNet to recover the Bs that 
contains the lemma among its labels. When BSs 
are mapped to WN, we use the ISR-WN resource 
to find the corresponding Synset. Since a lemma 
can appear in a different Bs, it can be mapped with 
several WNS. Thus, we get a Synset list for each 
lemma in the sentence. In case the lemma does not 
have an associated Bs, its list would be empty. An 
example of this step is shown on Figure 2. 
2.3 Applying Ppr+Freq method 
In the above case, Ppr+Freq modifies the ?classic? 
Page Rank approach instead of assigning the same 
weight for each sense of WN in the disambiguation 
graph (??). 
The PageRank (Brin and Page, 1998) 
adaptation, Ppr , which was popularized by (Agirre 
IV . Assigning Synset, Babel Synset and Wiki page title
? The struggle against the drug lords in Colombia will be a near thing .?
struggle drug_lord Colombia near_thing
Wikipedia WordNet BabelNet
ISR-WN
WordNet
(WN)
SUMO
WN-Domain
WN-Affect
SemanticClass eXtended WN3.0
eXtended WN1.7
struggle%1:04:01:: drug_lord%1:18:00:: colombia%1:15:00:: near_thing%1:04:00::
bn:00009079n bn:00028876n bn:00020697n bn:00057109n
-- Drug_Lord Colombia --
I. Obtaing lemmas
II. Obtaining Synset of selected lemmas
III. Applying Ppr+Freq method
WN key
BS
WK
struggle
drug_lord Colombia
near_thing
struggle
bn:00074762n wn:00587514n
bn:00009079n wn:00739796n
bn:00009080n wn:00901980n
drug_lord bn:00028876n wn:09394468n
colombia
bn:00020697n wn:08196765n
bn:02051949n
bn:02530766n
near_thing bn:00057109n wn:00193543n
Sentence lemmas 
Babel synset 
WordNet synset 
243
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main idea 
behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ? to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the more 
strength the voter would have. Thus, PageRank is 
generated by applying a random walkthrough from 
the internal interconnection of ?, where the final 
relevance of ??  represents the random walkthrough 
probability over ?, and ending on ??. 
Ppr+Freq includes the existent semantic and 
frequency patterns of each sense of the word to 
disambiguate while finding a way to connect each 
one of these words in a knowledge base. 
The new graph-based approach of WSD 
generates a graph of disambiguated words for each 
input sentence. For that reason, it is necessary to 
classify the word senses according to the other 
words that compose the context. The general 
method is shown in Figure 3. This method is 
divided into three steps: 
I. Creation of a disambiguation graph 
II. Application of Ppr+Freq in the generated 
graph 
III. Selection of the correct answer 
Creation of a disambiguation graph: In the first 
step, a disambiguation graph is built by means of a 
Breath First Search (BFS) over the ?super? graph 
composed by all the resources integrated into ISR-
WN. The components involved in this process are: 
WordNet, SUMO (Zouaq et al, 2009) WordNet 
Domains (Magnini and Cavaglia, 2000) WordNet 
Affects (Strapparava and Valitutti, 2004) Semantic 
Classes (Izquierdo et al, 2007) and eXtended 
WordNet (XWN) relations (Moldovan and Rus, 
2001). This search aims to recover all senses 
(nodes), domain labels (from WordNet Domain 
and WordNet Affects), SUMO categories, and 
Semantic Classes labels through the shortest path 
between every pair of senses in the WNSL set 
associated with the input sentence. Using ISR-WN 
as the KB, through experimentation, we obtained 
the shortest paths with a length of five edges. For a 
better understanding of this process, see (Guti?rrez, 
2012). 
Application of Ppr+Freq in the generated 
graph: In the second step, we use the weighted 
Personalized PageRank. Here, all the vertices from 
vector ? in ?? are initialized with the value  
1
?
 ; 
where ? is the number of nodes in ??. On the 
other hand, the vertices that represent word senses 
in the analyzed sentence are not initialized with 
this value. Instead, they are initialized with values 
in the range [0?1], which are associated to their 
occurrence frequency in SemCor1 (Corpus and 
sense frequencies knowledge). In the last step, 
after applying the Ppr+Freq algorithm over ??, we 
get a representative vector which contains ISR-WN 
nodes in ?? sorted in a descending order by a 
ranking score computed by this algorithm. For a 
better description, see (Guti?rrez, 2012). 
Selection of the correct answer: As the correct 
sense, we take the highest ranked sense of each 
target word involved in this vector. Note that 
domain labels, SUMO categories, semantic class 
labels, and affect labels are ranked too. They could 
be used in the future to determine relevant 
conceptualizations that would be useful for text 
classification and more. 
In our system, we assume the following 
configuration: dumping factor ? = 0.85 and like in 
(Agirre and Soroa, 2009) we used 30 iterations. A 
detailed explanation about PageRank algorithm 
can be found in (Agirre and Soroa, 2009). 
Table 2 shows an example that analyzes the 
Synset for each word in the sentence and also 
shows how the higher ranked Synsets of the target 
words are selected as the correct ones. For a 
detailed explanation of Ppr+Freq, see (Guti?rrez, 
2012). 
2.4 Assigning Synset, Babel synset and Wiki 
Pages 
In this step, English is handled differently from 
other languages because WordNet Synsets are 
available only for English. The following sections 
explain how we proceed in each case. Once the 
Synsets list is obtained for each lemma in section 
2.3, selecting the correct answer for the lemma is 
all that?s left to do. 
                                                     
1 http://www.cse.unt.edu/~rada/downloads.html 
244
 
Figure 3. General process of WSD with Ppr+Freq. 
2.4.1 English 
Given a lemma, we go through its Synset list from 
beginning to end looking for the first Synset that 
contains a key2 for the lemma. If such Synset 
exists, it is designated as the Synset for the lemma. 
Otherwise, no Synset is assigned. 
As already explained, each Synset in the list is 
connected to a Bs. Therefore, the lemma linked 
with the correct WNS selected in the previous step, 
is chosen as the correct lemma. In case no Synsets 
were designated as the correct ones, we take the 
first Bs in BN, which contains the lemma among 
its labels.  
To determine the Wiki pages titles (WK) we 
examine the WIKIRED and WIKI labels in the 
correct Bs selected in the preceding step. This 
search is restricted only to labels corresponding to 
the analyzed language and discriminating upper 
and lower case letters. Table 2 shows some sample 
results of the WSD process. 
Lemma struggle drug_lord 
WNS 00739796n 09394468n 
WN key struggle%1:04:01:: drug_lord%1:18:00:: 
Bs bn:00009079n bn:00028876n 
WK - Drug_Lord 
Lemma colombia near_thing 
WNS 08196765n 00193543n 
WN key colombia%1:15:00:: near_thing%1:04:00:: 
Bs bn:00020697n bn:00057109n 
WK Colombia - 
Table 2 : Example of English Language. 
                                                     
2A sense_key is the best way to represent a sense in 
semantic tagging or other systems that refer to WordNet 
senses. sense_key?s are independent of WordNet sense 
numbers and synset_offset?s, which vary between versions of 
the database. 
2.4.2 Other languages  
For this scenario, we introduce a change in the first 
step discussed in the previous section. The reason 
is that the Synsets do not contain any keys in any 
other language than English. Thus, the correct 
Synset for the lemma is the first in the Synset list 
for the lemma obtained, as described, in section 
2.3. 
3 Results 
We tested three versions (runs) of the proposed 
approach and evaluated them through a trial 
dataset provided by Task123 of Semeval-2013 
using babelnet-1.0.1. Table 3 shows the result for 
each run. Note that the table results were 
calculated with the traditional WSD recall 
measure, being this measure which has ranked 
WSD systems on mostly Semeval competitions. 
On the other hand, note that our precision and 
recall results are different because the coverage is 
not 100%. See Table 5. 
 English French 
Runs WNS Bs WK Bs WK 
Run1 0.70 0.71 0.77 0.59 0.85 
Run2 0.70 0.71 0.78 0.60 0.85 
Run3 0.69 0.70 0.77 - - 
Table 3 : Results of runs with trial recall values. 
As can be noticed on Table 3, results of different 
versions do not have big differences, but in 
general, Run2 achieves the best results; it?s better 
                                                     
3 http://www.cs.york.ac.uk/semeval-2013/task12 
ISR-WN
footballer#1 | cried#9 | winning#3
footballer | cry | winning
Lemmas
?The footballer cried when winning?
Disambiguation
Graph
(0,9)
Footballer#1
(0,3)
cry#7
(0,4)
cry#9
(0,2)
cry#10
(0,2)
cry#11
(0,2)
cry#12
(0,2)
winning#1
(0,3)
winning#3
Creating GD
Ppr+Freq
Selecting senses
245
than Run1 in the WK with a 78% in English and 
Bs with 60% in French. The best results are in the 
WK in French with a value of 85%. 
Since we can choose to include different 
resources into ISR-WN, it is important to analyze 
how doing so would affect the results. Table 4 
shows comparative results for Run 2 of a trial 
dataset with BabelNet version 1.1.1. 
As can be observed in Table 4, the result does not 
have a significant change even though we used the 
ISR-WN with all resources.  
A better analysis of Ppr+Freq in, as it relates to 
the influence of each resource involved in ISR-WN 
(similar to Table 4 description) assessing 
SensEval-2 and SensEval-3 dataset, is shown in 
(Guti?rrez, 2012). There are different resource 
combinations showing that only XWN1.7 and all 
ISR-WN resources obtain the highest performance. 
Other analysis found in (Guti?rrez, 2012) evaluates 
the influence of adding the sense frequency for 
Ppr+Freq.  
By excluding the Factotum Domain, we obtain 
the best result in Bs 54% for French (only 1% 
more than the version used in the competition). 
The other results are equal, with a 69% in WNS, 
66% in Bs, 64% in WK for English, and 69% in 
WK for French. 
        English French 
WN Domains Sumo Affect Factotum 
Domain 
SemanticClass XWN3.0 XWN1.7 WNS Bs WK Bs WK 
X X X X X X X X 0.69 0.66 0.64 0.53 0.69 
X X  X X X X X 0.69 0.66 0.64 0.53 0.69 
X    X X X X 0.68 0.65 0.64 0.52 0.69 
X X X X  X X X 0.69 0.66 0.64 0.54 0.69 
X X X X  X  X 0.68 0.65 0.65 0.53 0.69 
Table 4. Influence of different resources that integrate ISR-WN in our technique. 
    Wikipedia BabelNet WordNet 
System Language Precision Recall F-score Precision Recall F-score Precision Recall F-score 
MFS DE 0.836 0.827 0.831 0.676 0.673 0.686 - - - 
  EN 0.86 0.753 0.803 0.665 0.665 0.656 0.63 0.63 0.63 
  ES 0.83 0.819 0.824 0.645 0.645 0.644 - - - 
  FR 0.698 0.691 0.694 0.455 0.452 0.501 - - - 
  IT 0.833 0.813 0.823 0.576 0.574 0.572 - - - 
Run1 DE 0.758 0.46 0.572 0.619 0.617 0.618 - - - 
  EN 0.619 0.484 0.543 0.677 0.677 0.677 0.639 0.635 0.637 
  ES 0.773 0.493 0.602 0.708 0.703 0.705 - - - 
  FR 0.817 0.48 0.605 0.608 0.603 0.605 - - - 
  IT 0.785 0.458 0.578 0.659 0.656 0.657 - - - 
Run2 DE 0.769 0.467 0.581 0.622 0.62 0.621 - - - 
  EN 0.62 0.487 0.546 0.685 0.685 0.685 0.649 0.645 0.647 
  ES 0.778 0.502 0.61 0.713 0.708 0.71 - - - 
  FR 0.815 0.478 0.603 0.608 0.603 0.605 - - - 
  IT 0.787 0.463 0.583 0.659 0.657 0.658 - - - 
Run3 EN 0.622 0.489 0.548 0.68 0.68 0.68 0.642 0.639 0.64 
Table 5. Results of Runs for Task12 of semeval-2013 using the test dataset. 
 
246
3.1 Run1 
In this Run, WNSLs consist of all the target words 
involved in each sentence. This run is applied at 
the sentence level. The results for the competition 
are shown in Table 5. For this Run, the best result 
was obtained for Spanish with a 70.3% in Bs and 
49.3% in WK of Recall. As we can see, for Run1 
the precision is high for Wikipedia disambiguation, 
obtaining for French the best result of the ranking. The 
low Recall in Wikipedia is due to the exact mismatching 
of labels between our system output and the gold 
standard. This fact, affects the rest of our runs. 
3.2 Run2 
In this Run, WNSLs consist of all the target words 
involved in each domain. We can obtain the target 
words because the training and test dataset contain 
the sentences grouped by topics.  For instance, for 
English, 13 WNSLs are established. This Run is 
applied at the corpora level. The results for the 
competition are shown in Table 5. It is important to 
emphasize that our best results ranked our 
algorithm as first place among all proposed 
approaches for the MWSD task. 
For this run, the best Recall was obtained for 
Spanish with a 70.8% in Bs and 50.2% in WK. 
This Run also has the best result of the three runs. 
For the English competition, it ended up with a 
64.5% in WNS, 68.5% in Bs, and 48.7% in WK. 
This Run obtained promising results, which took 
first place in the competition. It also had better 
results than that of the First Sense (Most Frequent 
Sense) baseline in Bs results for all languages, 
except for German. In Bs, it only obtained lower 
results in German with a 62% of Recall for our 
system and 67.3% for the First Sense baseline. 
3.3 Run3 
In this run, WNSLs consist of all the words 
included in each sentence. This run uses target 
words and non-target words of each sentence, as 
they are applied to the sentence level. The results 
for the competition are shown in Table 5.  
As we can see, the behavior of this run is similar 
to the previous runs. 
4 Conclusions and Future work  
The above results suggest that our proposal is a 
promising approach. It is also important to notice 
that a richer knowledgebase can be built by 
combining different resources such as BabelNet 
and ISR-WN, which can lead to an improvement 
of the results. Notwithstanding, our system has 
been recognized as the best in the competition, 
obtaining results around 70% of Recall. 
According to the Task12 results4, only the 
baseline Most Frequent Sense (MFS) could 
improve our scores in order to achieve better WK 
and German (DE) disambiguation. Therefore, we 
plan to review this point to figure out why we 
obtained better results in other categories, but not 
for this one. At the same time, further work will 
use the internal Babel network to run the Ppr+Freq 
method in an attempt to find a way to enrich the 
semantic network obtained for each target sentence 
to disambiguate. On top of that, we plan to 
compare Ppr (Agirre and Soroa, 2009) with 
Ppr+Freq using the Task12 dataset. 
Availability of our Resource 
In case researchers would like to use our resource, 
it is available at the GPLSI5 home page or by 
contacting us via email. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. 
                                                     
4 http://www.cs.york.ac.uk/semeval-
2013/task12/index.php?id=results 
5 http://gplsi.dlsi.ua.es/ 
247
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; A. 
Gonz?lez; R. Estrada; Y. Casta?eda; S. V?zquez; A. 
Montoyo and R. Mu?oz. UMCC_DLSI: 
Multidimensional Lexical-Semantic Textual 
Similarity. {*SEM 2012}: The First Joint Conference 
on Lexical and Computational Semantics -- Volume 
1: Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Guti?rrez, Y. An?lisis Sem?ntico Multidimensional 
aplicado a la Desambiguaci?n del Lenguaje Natural. 
Departamento de Lenguajes y Sistemas Inform?ticos. 
Alicante, Alicante, 2012. 189. p. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, RANLP 
2011 Organising Committee, 2011b. 233--239 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Sentiment 
Classification Using Semantic Features Extracted 
from WordNet-based Resources. Proceedings of the 
2nd Workshop on Computational Approaches to 
Subjectivity and Sentiment Analysis (WASSA 
2.011), Portland, Oregon., Association for 
Computational Linguistics, 2011c. 139--145 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Word Sense 
Disambiguation: A Graph-Based Approach Using N-
Cliques Partitioning Technique. en:  Natural 
Language Processing and Information Systems. 
MU?OZ, R.;MONTOYO, A.et al Springer Berlin / 
Heidelberg, 2011d. 6716: 112-124.p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. A graph-
Based Approach to WSD Using Relevant Semantic 
Trees and N-Cliques Model. CICLing 2012, New 
Delhi, India, 2012. 225-237 p.  
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Laparra, E.; G. Rigau and M. Cuadros. Exploring the 
integration of WordNet and FrameNet. Proceedings 
of the 5th Global WordNet Conference (GWC'10), 
Mumbai, India, 2010.  
Magnini, B. and G. Cavaglia. Integrating Subject Field 
Codes into WordNet. Proceedings of Third 
International Conference on Language Resources and 
Evaluation (LREC-2000), 2000. 1413--1418 p.  
Mihalcea, R. Unsupervised large-vocabulary word sense 
disambiguation with graph-based algorithms for 
sequence data labeling. Proceedings of HLT05, 
Morristown, NJ, USA., 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Navigli, R.; D. Jurgens and D. Vannella. SemEval-2013 
Task 12: Multilingual Word Sense Disambiguation. . 
Proceedings of the 7th International Workshop on 
Semantic Evaluation (SemEval 2013), in conjunction 
with the Second Joint Conference on Lexical and 
Computational Semantics (*SEM 2013), Atlanta, 
Georgia, 2013.  
Navigli, R. and S. P. Ponzetto. BabelNet: Building a 
Very Large Multilingual Semantic Network. 
Proceedings of the 48th Annual Meeting of the 
Association for Computational Linguistics, Uppsala, 
Sweden, Association for Computational Linguistics, 
2010. 216--225 p.  
Navigli, R. and S. P. Ponzetto BabelNet: The automatic 
construction, evaluation and application of a wide-
coverage multilingual semantic network Artif. Intell., 
2012, 193: 217-250. 
Navigli, R. and P. Velardi Structural Semantic 
Interconnections: A Knowledge-Based Approach to 
Word Sense Disambiguation IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 2005, 
27(7): 1075-1086. 
Sinha, R. and R. Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation Using Measures of 
Word Semantic Similarity. Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC 2007), Irvine, CA, 2007. 
248
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Tsatsaronis, G.; M. Vazirgiannis and I. 
Androutsopoulos. Word sense disambiguation with 
spreading activation networks generated from 
thesauri. IJCAI, 2007.  
Vossen, P. EuroWordNet: A Multilingual Database with 
Lexical Semantic Networks.  Dordrecht, Kluwer 
Academic Publishers, 1998.  
Zouaq, A.; M. Gagnon and B. Ozell. A SUMO-based 
Semantic Analysis for Knowledge Extraction. 
Proceedings of the 4th Language & Technology 
Conference, Pozna?, Poland, 2009.  
 
 
249
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 501?507, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SSA-UO: Unsupervised Twitter Sentiment Analysis
Reynier Ortega, Adrian Fonseca
CERPAMID, University of Oriente
Ave Patricio Lumumba S/N
Santiago de Cuba, Cuba
Yoan Gutie?rrez
DI, University of Matanzas
Autopista a Varadero Km 312
Matanzas, Cuba
Andre?s Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante,Spain
Abstract
This paper describes the specifications and re-
sults of SSA-UO, unsupervised system, pre-
sented in SemEval 2013 for Sentiment Analy-
sis in Twitter (Task 2) (Wilson et al, 2013).
The proposal system includes three phases:
data preprocessing, contextual word polarity
detection and message classification. The
preprocessing phase comprises treatment of
emoticon, slang terms, lemmatization and
POS-tagging. Word polarity detection is car-
ried out taking into account the sentiment as-
sociated with the context in which it appears.
For this, we use a new contextual sentiment
classification method based on coarse-grained
word sense disambiguation, using WordNet
(Miller, 1995) and a coarse-grained sense in-
ventory (sentiment inventory) built up from
SentiWordNet (Baccianella et al, 2010). Fi-
nally, the overall sentiment is determined us-
ing a rule-based classifier. As it may be ob-
served, the results obtained for Twitter and
SMS sentiment classification are good consid-
ering that our proposal is unsupervised.
1 Introduction
The explosion of Web 2.0 has marked a new age
for the human society. The huge use of Social Me-
dia such as Facebook1 , MySpace2 , LinkedIn3 and
Twitter4 , offers a place for people to share informa-
tion in real time. Twitter is one of the most popular
1https://www.facebook.com
2http://www.myspace.com/
3http://www.linkedin.com
4https://www.twitter.com/
social network websites and has been growing at a
very fast pace. The number of active users exceeds
500 million and the number of tweets posted by day
exceeds 500 million (as of May 2012)5. Through the
twitter applications, users shared opinions about per-
sonalities, politicians, products, companies, events,
etc. This has been attracting the attention of dif-
ferent research communities interested in analyz-
ing its content and motivated many natural language
tasks, such as sentiment analysis, emotions detec-
tion, opinions retrieval, product recommendation or
opinion summarization.
One of the most popular sentiment analysis tasks
is polarity classification. This task is a new field
that classifies opinion texts as positive, negative or
neutral (Pang et al, 2002; Turney, 2002; Esuli and
Sebastiani, 2006; Wilson et al, 2006; Wiegand et
al., 2010). Determining polarity might seem an easy
task, as many words have some polarity by them-
selves. However, words do not always express the
same sentiment, and in most cases the polarity of a
word depends on the context in which the word is
used. So, terms that clearly denote negative feel-
ings can be neutral, or even positive, depending
on their context. Hence, sentiment analysis sys-
tems should include semantic-level analysis in order
to solve word ambiguity and correctly capture the
meaning of each word according to its context. Also,
complex linguistic processing is needed to deal with
problems such as the effect of negations and infor-
mal language. Moreover, understanding the senti-
mental meaning of the different textual units is im-
portant to accurately determine the overall polarity
5http://www.statisticbrain.com/twitter-statistics/
501
of a text.
In this paper, we present a system that has as main
objective to analyze the sentiments of tweets and
classify these as positive, negative or neutral. The
proposal system includes three phases: data prepro-
cessing, contextual word polarity detection and mes-
sage classification. The preprocessing phase com-
prises treatment of emoticons, spell-errors, slang
terms, lemmatization and POS-tagging. Word po-
larity detection is carried out taking into account the
sentiment associated with the context within which
it appears. For this, we use a new contextual senti-
ment classification method based on coarse-grained
word sense disambiguation, using WordNet (Miller,
1995) and a coarse-grained sense inventory (senti-
ment inventory) built up from SentiWordNet (Bac-
cianella et al, 2010). Finally, the polarity is deter-
mined using a rule-based classifier. The paper is
organized as follows. Section 2 describes of SSA-
UO system. In Section 3 we evaluate our proposal
and discuss the results obtained in the SemEval 2013
Task No. 2. Finally, section 4 provides concluding
remarks.
2 SSA-UO System
We use an unsupervised strategy consisting in a
coarse-grained clustering-based word sense disam-
biguation (WSD) method that differentiates positive,
negative, highly positive, highly negative and objec-
tive uses of every word on context which it occurs.
The proposal method uses WordNet and a coarse-
grained sense inventory (sentiment inventory) built
up from SentiWordNet. The overall architecture of
our sentiment classifier is shown in Figure 1.
Firstly, data preprocessing is done to eliminate in-
complete, noisy or inconsistent information. A Sen-
timent Word Sense Disambiguation method (Section
2.3) is then applied to content words (nouns, adjec-
tives, verbs and adverbs). Once all content words
are disambiguated, we apply a rule-based classifier
(Section 2.4) to decide whether the tweet is positive,
negative or neutral.
Unsupervised word sense disambiguation method
proposed by (Anaya-Sa?nchez et al, 2006) was
adapted for sentiment word sense disambiguation.
Unlike the authors, who aim to obtain the correct
sense of a word, we use the method to determine
Figure 1: Overall architecture of Sentiment Classifier
when a word is used with highly positive (HP), posi-
tive (P), highly negative (HN), negative (N) or objec-
tive (O) meaning based on a sentiment sense inven-
tory. We make sentiment sense inventory based on
sense-level annotation in SentiWordNet. Finally, we
apply a rule-based classifier to determine the overall
sentiment in tweet.
2.1 Data Preprocessing
The tweets differ from the text in articles, books, or
even spoken language. It is limited to 140 charac-
ters, also includes many idiosyncratic uses, such as
emoticons, slang terms, misspellings, URLs, ?RT?
for re-tweet, ?@? for user mentions, ?#? for hash-
tags, and character repetitions. Therefore it is nec-
essary to preprocess the text, in order to reduce the
noise information. The preprocessing step involve
the following task. The text is tokenized and URL,
re-tweets and author mentions are removed. Hash-
tag tokens frequently contain relevant information
related to the topic of the tweet, this is included as
part of the text but without the ?#? character. We
replace emoticon tokens by emotion words using
an emoticons dictionary, obtained from Wikipedia
502
6. Each emoticon was manually annotated with an
emotion word and polarity value. Emoticons that
suggest positive emotions - ?:-)?, ?:)?, ?X-D? - are
annotated with the emotion word ?happy? and neg-
ative emoticons - ?:-(?, ?:-c?, ?:,(? - are annotated
with the emotion word ?sad?. The presence of ab-
breviations within a tweet is noted, therefore abbre-
viations are replaced by their meaning (e.g., LOL ?
laughing out loud) using a dictionary7. Finally the
text is POS-tagged and lemmatized using TreeTag-
ger (Schmid, 1994) and stopwords are discarded.
2.2 Sentiment Sense Inventory
We considered SentiWordNet for building senti-
ment coarse-grained sense inventory. SentiWordNet
contain positive, negative and objective scores be-
tween 0 and 1 for all senses in WordNet. Based
on this sense level annotation, we define a new
rule (SentiS) for classifying senses in five sentiment
class. The senses are classified in the following man-
ner (Alexandra et al, 2009): senses whose positive
score is greater than or equal to 0.75 are consid-
ered to be highly positive (HP), senses with posi-
tive score greater than or equal to 0.5 and lower than
0.75 are considered positive (P), senses with nega-
tive score greater than or equal 0.75 are considered
highly negative (HN), whereas those whose negative
score is lower than 0.75 and greater than or equal to
0.5 are considered to be negative (N). In the remain-
ing cases, the senses are considered to be objective
(O) (see equation(1)).
sentiS(s)=
?
???????
???????
HP i f ScoreP(s)? 0.75
HN i f ScoreN(s)? 0.75
P i f ScoreP(s) < 0.75 and ScoreP(s)? 0.5
N i f ScoreN(s) < 0.75 and ScoreN(s)? 0.5
O in other case
(1)
Table 1 summarizes the distribution of the five
sentiment classes once classified all senses of Sen-
tiWordNet.
A notable unbalance can be observed between the
number of highly positive, highly negative, positive,
negative and objective senses.
6http://en.wikipedia.org/wiki/List of emoticons
7http://www.noslang.com/dictionary/
Once all senses were classified in a five sentiment
sense class, we create a coarse sense inventory based
on this classification. This inventory is defined in the
following manner: For each word in SentiWordNet
we grouped its senses with the same sentiment class
in a single sense (coarse-sense), in case of objective
senses these are kept separated.
2.3 Contextual Word Polarity Detection
Much work on sentiment analysis have been di-
rected to determine the polarity of opinion using
anotated lexicons with prior polarity (Hatzivas-
siloglou and McKeown, 1997; Kamps and Marx,
2002; Turney, 2002). However a word can mod-
ify your prior polarity in relation to the context
within which it is invoked. For example the word
?earthquake? is used with negative meaning in the
sentence :
?Selling the company caused an earthquake amount
the employees?.
Whereas it is used in an neutral meaning in the
sentence:
?An earthquake is the result of a sudden release of
energy in the Earth?s crust that creates seismic waves?.
For this reason, our system uses a coarse-grained
WSD method for obtaining the contextual polarity
of all words in tweets. The selected disambigua-
tion method (Anaya-Sa?nchez et al, 2006) was de-
veloped for the traditional WSD task. In this WSD
method, the senses are represented as topic signa-
tures (Lin and Hovy, 2000) built from the repository
of concepts of WordNet. The disambiguation pro-
cess starts from a clustering distribution of all pos-
sible senses of the ambiguous words by applying
the Extended Star clustering algorithm (Gil-Garc??a
et al, 2003). Such a clustering tries to identify co-
hesive groups of word senses, which are assumed
to represent different meanings for the set of words.
Resource HP HN P N O
SWN 310 938 2242 2899 109035
Table 1: Senses highly positive, highly negative, positive,
negative and objective distributions.
503
Then, clusters that match the best with the context
are selected. If the selected clusters disambiguate
all words, the process stops and the senses belong-
ing to the selected clusters are interpreted as the dis-
ambiguating ones. Otherwise, the clustering is per-
formed again (regarding the remaining senses) until
a complete disambiguation is achieved. It does not
distinguish between highly positive, positive, nega-
tive, highly negative or objective meaning of a word.
In this paper, we propose a strategy to built a coarse-
grained sense representation. Firstly, a topic signa-
tures for all senses into WordNet is built and the
topic signatures for coarse-grained senses is the sum
of the topic signatures of the corresponding fine-
grained senses that was grouped.
We explain coarse-grained sense representation
using the following example:
Let us consider the adjective ?sad?. This adjec-
tive has three word senses into WordNet 2.0
sad#a#1 ? experiencing or showing sorrow or unhappiness
sad#a#2 ? of things that make you feel sad
sad#a#3 ? bad; unfortunate
Firstly the topic signature are built for each word
sense:
vector1 = topicSignature(sad#a#1)
vector2 = topicSignature(sad#a#2)
vector3 = topicSignature(sad#a#3)
The senses are classified using equation (1)(in
Section 2.2), sense 1 and 3 were considered as
highly negative, whereas the sense 2 is objective.
The topic signature associated to highly negative
coarse-grained sense is computed as:
topicSignature(sad#a#HN) = sum(vector1+ vector3)
and objective coarse-grained sense is kept as
vector2
topicSignature(sad#a#O) = vector2
2.4 Rule-based Sentiment Classifier
We use a rule-based classifier to classify tweets into
positive, negative or neutral. A polarity value is as-
signed to each word, based on equation 2, after these
were disambiguated. It is necessary to clarify that
emotion words that replaced emoticons in the pre-
processing phase, are not disambiguated. Instead,
we give a prior polarity value equal to 4 if emotion
word is ?happy? and -4 in case that emotion word is
?sad?. It is important to mention that the polarity of
a word is forced into the opposite class if it is pre-
ceded by a valence shifter (obtained from the Negate
category in GI (Stone et al, 1966)).
polarity(w) =
?
???????
???????
4
?4
2
?2
0
i f w is disambiguated as HP
i f w is disambiguated as HN
i f w is disambiguated as P
i f w is disambiguated as N
i f w is disambiguated as O
(2)
The polarity of the tweet is determined from the
scores of positive and negative words it contains. To
sum up, for each tweet the overall positive (PosS(t))
value and overall negative value (NegS(t)) , are com-
puted as:
PosS(t) = ?
wi?WP
polarity(wi) (3)
WP: Words disambiguated as highly positive or
positive in tweet t
NegS(t) = ?
wi?WN
polarity(wi) (4)
WN : Words disambiguated as highly negative or
negative in tweet t
If PosS(t) is greater than NegS(t) then the tweet
is considered as positive. On the contrary, if PosS(t)
is less than NegS(t) the tweet is negative. Finally, if
PosS(t) is equal to NegS(t) the tweet is considered
as neutral.
2.5 A Tweet Sentiment Classification Example
The general operation of the algorithm is illustrated
in the following example:
Let us consider the following tweet:
@JoeyMarchant: I really love Jennifer Aniston :-)
#loving, she is very cooooollll and sexy. I?m married to
her... LOL, http://t.co/2RShsRNSDW
504
After applying the preprocessing phase, we
obtain the following normalized text:
I really love Jennifer Aniston ?happy? loving, she
is very cooll and sexy. I?m married to her... lots of laughs.
When the text is lemmatized and stopwords are
removed, we obtain the following bag of words (for
each word we show: lemma and part-of-speech n-
noun, v-verb, a-adjective, r-adverb and u-unknown):
really#r love#v jennifer#a aniston#n ?happy?#a
loving#a cooll#a sexy#a marry#v lot#n laugh#n.
After contextual word polarity detection, we
obtain the following result (for each word we
shown lemma, part-of-speech and sentiment sense,
HP-highly positive, HN-highly negative, P-positive,
N-negative and O-objective).
really#r#P love#v#P jennifer#a#O aniston#n#O
?happy?#a loving#a#HP cooll#a#O sexy#a#P
marry#v#O lot#n#O laugh#n#P
Once that all words were disambiguated we
obtained their polarities using the equation 2 intro-
duced in section 2.4. We show the polarities values
assigned to each word, in Table 2.
Word POS Sentiment Polarity
really r P 2
love v P 2
jennifer a O 0
aniston n O 0
?happy? a - 4
loving a HP 4
cooll a O 0
sexy a P 2
marry a O 0
lot n O 0
laugh n P 2
Table 2: Polarity assigned to each word
Note that the word ?happy? has not been dis-
ambiguated, its polarity is assigned according
to the emoticon associated in the original tweet.
Afterward we compute overall positive and negative
polarity value:
NegS(t) = 0
PosS(t) = 2+2+4+4+2+2 = 16
Therefore, the tweet t is classified as positive.
3 Results
This section presents the evaluation of our system in
the context of SemEval 2013 Task No.2 Subtask B
(Sentiment Analysis in Twitter). For evaluating the
participant?s systems two unlabeled datasets were
provided, one composed of Twitter messages and
another of SMS messages. For each dataset two
runs can be submitted, the first (constrained), the
system can only be used the provided training data
and other resources such as lexicons. In the second
(unconstrained), the system can use additional data
for training. Our runs are considered as constrained
because SSA-UO only use lexical resources for sen-
timent classification.
Runs Dataset F1 all runs Rank
twitter-1 Twitter 50.17 33(48)
sms-1 SMS 44.39 33 (42)
Table 3: SSA-UO results in polarity classification, all
runs summited
Runs Dataset F1 constrained runs Rank
twitter-1 Twitter 50.17 25 (35)
sms-1 SMS 44.39 22 (28)
Table 4: SSA-UO results in polarity classification, con-
strained runs summited
In Table 3 we summarize the results obtained by
SSA-UO system. As may be observed average F1
measure for Twitter dataset is the 50.17 and 44.39
for the SMS dataset. A total of 48 runs were sub-
mitted by all systems participant?s in Twitter and 42
for SMS dataset. Our runs were ranked 33th for both
datasets.
In Table 4 we compare our results with those runs
that can be considered as constrained. A total of 35
runs for Twitter and 28 for SMS were submitted ,
505
ours runs were ranked in 25th and 22th respectively.
It?s worth mentioning that, the results obtained can
be considered satisfactory, considering the complex-
ity of the task and that our system is unsupervised.
4 Conclusion
In this paper, we have described the SSA-UO system
for Twitter Sentiment Analysis Task at SemEval-
2013. This knowledge driven system relies on unsu-
pervised coarse-grained WSD to obtain the contex-
tual word polarity. We used a rule-based classifier
for determining the polarity of a tweet. The experi-
mental results show that our proposal is accurate for
Twitter sentiment analysis considering that our sys-
tem does not use any corpus for training.
Acknowledgments
This research work has been partially funded by
the Spanish Government through the project TEXT-
MESS 2.0 (TIN2009-13391-C04), ?Ana?lisis de Ten-
dencias Mediante Te?cnicas de Opinio?n Sema?ntica?
(TIN2012-38536-C03-03) and ?Te?cnicas de Decon-
struccio?n en la Tecnolog??as del Lenguaje Humano?
(TIN2012-31224); and by the Valencian Govern-
ment through the project PROMETEO (PROME-
TEO/2009/199).
References
Balahur Alexandra, Steinberger Ralf, Goot Erik van der,
Pouliquen Bruno, and Kabadjov Mijail. 2009. Opin-
ion mining on newspaper quotations. In Proceed-
ings of the 2009 IEEE/WIC/ACM International Joint
Conference on Web Intelligence and Intelligent Agent
Technology - Volume 03, WI-IAT ?09, pages 523?526,
Washington, DC, USA. IEEE Computer Society.
Henry Anaya-Sa?nchez, Aurora Pons-Porrata, and Rafael
Berlanga-Llavori. 2006. Word sense disambiguation
based on word sense clustering. In Proceedings of
the 2nd international joint conference, and Proceed-
ings of the 10th Ibero-American Conference on AI 18th
Brazilian conference on Advances in Artificial Intelli-
gence, IBERAMIA-SBIA?06, pages 472?481, Berlin,
Heidelberg. Springer-Verlag.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias, edi-
tors, Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
?10), Valletta, Malta, may. European Language Re-
sources Association (ELRA).
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC?06, pages
417?422.
R. Gil-Garc??a, J. M. Bad??a-Contelles, and A. Pons-
Porrata. 2003. Extended Star Clustering Algorithm.
In CIARP 2003, LNCS, vol. 2905, pages 480?487.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on Eu-
ropean chapter of the Association for Computational
Linguistics, EACL ?97, pages 174?181, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jaap Kamps and Maarten Marx. 2002. Words with atti-
tude. In First International WordNet conference.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics - Volume 1, COLING ?00, pages 495?
501, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceeding of Empirical
Methods in Natural Language Processing, pages 79?
86.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press, Cambridge, MA.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. pages 417?424.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, NeSp-NLP
?10, pages 60?68, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2006.
Recognizing strong and weak opinion clauses. Com-
putational Intelligence, 22:73?99.
506
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
507
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 636?643, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Semantic and Lexical features for detection and 
classification Drugs in biomedical texts 
 
 
Armando Collazo, Alberto 
Ceballo, Dennys D. Puig, Yoan 
Guti?rrez, Jos? I. Abreu, Roger 
P?rez 
Antonio Fern?ndez 
Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
{armando.collazo, dennys.puig, 
yoan.gutierrez, jose.abreu, 
roger.perez}@umcc.cu, 
alberto.ceballo@infonet.umcc.cu 
DLSI, University of Alicante 
Carretera de San Vicente 
S/N Alicante, Spain 
antonybr@yahoo.com, 
{montoyo, 
rafael}@dlsi.ua.es 
Independent Consultant 
USA 
info@franccamara.com 
 
Abstract 
In this paper we describe UMCC_DLSI-
(DDI) system which attempts to detect and 
classify drug entities in biomedical texts. 
We discuss the use of semantic class and 
words relevant domain, extracted with ISR-
WN (Integration of Semantic Resources 
based on WordNet) resource to obtain our 
goal. Following this approach our system 
obtained an F-Measure of 27.5% in the 
DDIExtraction 2013 (SemEval 2013 task 
9). 
1. Introduction 
To understand biological processes, we must 
clarify how some substances interact with our 
body and one to each other. One of these 
important relations is the drug-drug interactions 
(DDIs). They occur when one drug interacts 
with another or when it affects the level, or 
activity of another drug. DDIs can change the 
way medications act in the body, they can cause 
powerful, dangerous and unexpected side 
effects, and also they can make the medications 
less effective. 
As suggested by (Segura-Bedmar et al, 2011), 
?...the detection of DDI is an important research 
area in patient safety since these interactions 
can become very dangerous and increase health 
care costs?. More recent studies (Percha and 
Altman, 2013) reports that ??Recent estimates 
indicate that DDIs cause nearly 74000 
emergency room visits and 195000 
hospitalizations each year in the USA?.  
But, on the other hand, there is an expansion in 
the volume of published biomedical research, 
and therefore the underlying biomedical 
knowledge base (Cohen and Hersh, 2005). 
Unfortunately, as often happens, this 
information is unstructured or in the best case 
scenario semi-structured. 
As we can see in (Tari et al, 2010), ?Clinical 
support tools often provide comprehensive lists 
of DDIs, but they usually lack the supporting 
scientific evidences and different tools can 
return inconsistent results?.  
Although, as mentioned (Segura-Bedmar et al, 
2011) ?there are different databases supporting 
healthcare professionals in the detection of DDI, 
these databases are rarely complete, since their 
update periods can reach up to three years?. In 
addition to these and other difficulties, the great 
amount of drug interactions are frequently 
reported in journals of clinical pharmacology 
and technical reports, due to this fact, medical 
literature becomes most effective source for 
detection of DDI. Thereby, the management of 
DDI is a critical issue due to the overwhelming 
amount of information available on them 
(Segura-Bedmar et al, 2011). 
636
1.1. Task Description 
With the aim of reducing the time the health care 
professionals invest on reviewing the literature, 
we present a feature-based system for drug 
detection and classification in biomedical texts. 
The DDIExtraction2013 task was divided into 
two subtasks: Recognition and classification of 
drug names (Task 9.1) and Extraction of drug-
drug interactions (Task 9.2). Our system was 
developed to be presented in the Task 9.1. In this 
case, participants were to detect and classify the 
drugs that were present in the test data set which 
was a set of sentences related to the biomedical 
domain obtained from a segmented corpus. The 
output consisted of a list mentioning all the 
detected drugs with information concerning the 
sentence it was detected from as well as its 
offset in that sentence (the position of the first 
and the last character of the drug in the sentence, 
0 being the first character of a sentence). Also 
the type of the drug should have been provided. 
As to the type, participants had to classify 
entities in one of these four groups1: 
? Drug: any chemical agent used for 
treatment, cure, prevention or diagnose of 
diseases, which have been approved for 
human usage. 
? Brand: any drug which firstly have been 
developed by a pharmaceutical company. 
? Group: any term in the text designating a 
relation among pharmaceutical substances. 
? No-Human: any chemical agent which 
affects the human organism. An active 
substance non-approved for human usage 
as medication. 
In the next section of the paper, we present 
related works (Section 2). In Section 3, we 
discuss the feature-based system we propose. 
Evaluation results are discussed in Section 4. 
Finally, we conclude and propose future work 
(Section 5). 
2. Related Work 
One of the most important workshops on the 
domain of Bioinformatics has been BioCreAtIve 
(Critical Assessment of Information Extraction 
                                                     
1 http://www.cs.york.ac.uk/semeval-2013/task9 
in Biology) (Hirschman et al, 2005). This 
workshop has improved greatly the Information 
Extraction techniques applied to the biological 
domain. The goal of the first BioCreAtIvE 
challenge was to provide a set of common 
evaluation tasks to assess the state-of-the-art for 
text mining applied to biological problems. The 
workshop was held in Granada, Spain on March 
28-31, 2004. 
According to Hirschman, the first 
BioCreAtIvE assessment achieved a high level 
of international participation (27 groups from 10 
countries). The best system results for a basic 
task (gene name finding and normalization), 
where a balanced 80% precision/recall or better, 
which potentially makes them suitable for real 
applications in biology. The results for the 
advanced task (functional annotation from free 
text) were significantly lower, demonstrating the 
current limitations of text-mining approaches. 
The greatest contribution of BioCreAtIve was 
the creation and release of training and test data 
sets for both tasks (Hirschman et al, 2005). 
One of the seminal works where the issue of 
drug detection was mentioned was (Gr?nroos et 
al., 1995). Authors argue the problem can be 
solved by using a computerized information 
system, which includes medication data of 
individual patients as well as information about 
non-therapeutic drug-effects. Also, they suggest 
a computerized information system to build 
decision support modules that, automatically 
give alarms or alerts of important drug effects 
other than therapeutic effects. If these warnings 
concern laboratory tests, they would be checked 
by a laboratory physician and only those with 
clinical significance would be sent to clinicians. 
Here, it is important to note the appearance of 
the knowledgebase DrugBank 2 . Since its first 
release in 2006 (Wishart et al, 2008) it has been 
widely used to facilitate in silico drug target 
discovery, drug design, drug docking or 
screening, drug metabolism prediction, drug 
interaction prediction and general 
pharmaceutical education. DrugBank has also 
significantly improved the power and simplicity 
of its structure query and text query searches. 
                                                     
2 http://redpoll.pharmacy.ualberta.ca/drugbank/ 
637
Later on, in 2010 Tari propose an approach 
that integrates text mining and automated 
reasoning to derive DDIs (Tari et al, 2010). 
Through the extraction of various facts of drug 
metabolism, they extract, not only the explicitly 
DDIs mentioned in text, but also the potential 
interactions that can be inferred by reasoning. 
This approach was able to find several potential 
DDIs that are not present in DrugBank. This 
analysis revealed that 81.3% of these 
interactions are determined to be correct. 
On the DDIExtraction 2011 (Segura-Bedmar et 
al., 2011) workshop (First Challenge Task on 
Drug-Drug Interaction Extraction) the best 
performance was achieved by the team WBI 
from Humboldt-Universitat, Berlin. This team 
combined several kernels and a case-based 
reasoning (CBR) system, using a voting 
approach. 
In this workshop relation extraction was 
frequently and successfully addressed by 
machine learning methods. Some of the more 
common used features were co-occurrences, 
character n-grams, Maximal Frequent 
Sequences, bag-of-words, keywords, etc. 
Another used technique is distant supervision. 
The first system evaluating distant supervision 
for drug-drug interaction was presented in 
(Bobi? et al, 2012), they have proposed a 
constraint to increase the quality of data used for 
training based on the assumption that no self-
interaction of real-world objects are described in 
sentences. In addition, they merge information 
from IntAct and the University of Kansas 
Proteomics Service (KUPS) database in order to 
detect frequent exceptions from the distant 
supervision assumption and make use of more 
data sources. 
Another important work related to Biomedical 
Natural Language Processing was BioNLP 
(Bj?rne et al, 2011) it is an application of 
natural language processing methods to analyze 
textual data on biology and medicine, often 
research articles. They argue that information 
extraction techniques can be used to mine large 
text datasets for relevant information, such as 
relations between specific types of entities. 
Inspired in the previews works the system we 
propose makes use of machine learning methods 
too, using some of the common features 
described above, such as the n-grams and 
keywords and co-occurrences, but we also add 
some semantic information to enrich those 
features. 
3. System Description  
As it has been mentioned before, the system was 
developed to detect and classify drugs in 
biomedical texts, so the process is performed in 
two main phases:  
? drug detection. 
? drug classification. 
Both phases are determined by the following 
stages, described in Figure 1: 
I. Preprocessing 
II. Feature extraction 
III. Classification 
 
 
 
 
 
 
 
Figure 1. Walkthrough system process. 
Given a biomedical sentence, the system 
obtains the lemmas and POS-tag of every token 
Classification 
 
Pre-Processing (using Freeling 2.2) 
Training set from Semeval 2013 
DDIExtraction2013 task 
Run(3) 
MFSC?s 
CSC MF 2-grams, 3-grams 
UC 
UCA 
MWord 
N 
 
   
I 
II 
III 
Tokenizing 
Run(1) Run(2) 
CSC - All 
EMFS
C 
DRD 
CD 
GC 
InRe 
WNum 
Feature Extraction 
Lemmatizing 
 
POS tagging 
 
 
   
 
   
638
of the sentence, by means of Freeling tool 3 . 
After that, it is able to generate candidates 
according to certain parameters (see section 3.3). 
Then, all the generated candidates are 
processed to extract the features needed for the 
learning methods, in order to determine which 
candidates are drugs. 
After the drugs are detected, the system 
generates a tagged corpus, following the 
provided training corpus structure, containing 
the detected entities, and then it proceeds to 
classify each one of them. To do so, another 
supervised learning algorithm was used (see 
section 3.3). 
3.1. Candidates generation 
Drugs and drug groups, as every entity in 
Natural Language, follow certain grammatical 
patterns. For instance, a drug is usually a noun 
or a set of nouns, or even a combination of verbs 
and nouns, especially verbs in the past participle 
tense and gerunds. But, one thing we noticed is 
that both drugs and drug groups end with a noun 
and as to drug groups that noun is often in the 
plural. 
Based on that idea, we decided to generate 
candidates starting from the end of each 
sentence and going forward. 
Generation starts with the search of a pivot 
word, which in this case is a noun. When the 
pivot is found, it is added to the candidates list, 
and then the algorithm takes the word before the 
pivot to see if it complies with one of the 
patterns i.e. if the word is a noun, an adjective, a 
gerund or past participle verb. If it does, then it 
and the pivot form another candidate.  
After that, the algorithm continues until it finds 
a word that does not comply with a pattern. In 
this case, it goes to the next pivot and stops 
when all the nouns in the sentence have been 
processed, or the first word of the sentence is 
reached. 
3.2. Feature Description 
For the DDIExtraction20134 task 9 three runs of 
the same system were performed with different 
                                                     
3 http://nlp.lsi.upc.edu/freeling/ 
features each time. The next sections describes 
the features we used. 
3.2.1. Most Frequent Semantic Classes 
(MFSC) 
Given a word, its semantic class label (Izquierdo 
et al, 2007) is obtained from WordNet using the 
ISR-WN resource (Guti?rrez et al, 2011; 2010). 
The semantic class is that associated to the most 
probable sense of the word. For each entity in 
the training set we take the words in the same 
sentence and for each word its semantic class is 
determined. This way, we identify the 4005 most 
frequent semantic classes associated to words 
surrounding the entities in the training set.  
For a candidate entity we use 400 features to 
encode information with regard to whether or 
not in its same sentence a word can be found 
belonging to one of the most frequent semantic 
classes. 
Each one of these features takes a value 
representing the distance (measured in words) a 
candidate is from the nearest word with same 
semantic class which represents the attribute.  
If the word is to the left of the candidate, the 
attribute takes a negative value, if it is to the 
right, the value is positive, and zero if no word 
with that semantic class is present in the 
sentence the candidate belongs to. 
To better understand that, consider A1 is the 
attribute which indicates if in the sentence of the 
candidate a word can be found belonging to the 
semantic class 1. Thus, the value of A1 is the 
distance the candidate is from the closest word 
with semantic class 1 in the sentence that is 
being analyzed. 
3.2.2. Candidate Semantic Class (CSC) 
The semantic class of candidates is also included 
in the feature set, if the candidate is a multi-
word, then the semantic class of the last word 
(the pivot word) is taken.  
 
                                                                               
4 http://www.cs.york.ac.uk/semeval-2013/task9/ 
5 This value was extracted from our previous experiment. 
639
3.2.3. Most Frequent Semantic Classes 
from Entities (EMFSC) 
In order to add more semantic information, we 
decided to find the most frequent semantic 
classes among all the entities that were tagged in 
the training data set. We included, in the feature 
set, all the semantic classes with a frequency of 
eight or more, because all the classes we wanted 
to identify were represented in that threshold. In 
total, they make 29 more features. The values of 
every one of them, is the sum of the number of 
times it appears in the candidate.  
3.2.4. Candidate Semantic Class All 
Words (CSC-All) 
This feature is similar to CSC, but in this case 
the candidate is a multi-word, we not only look 
for the semantic class of the pivot, but also the 
whole candidate as one. 
3.2.5. Drug-related domains (DRD) 
Another group of eight attributes describes how 
many times each one of the candidates belongs 
to one of the following drug-related domains 
(DRD) (medicine, anatomy, biology, chemistry, 
physiology, pharmacy, biochemistry, genetics).  
These domains where extracted from WordNet 
Domains. In order to determine the domain that 
a word belongs to, the proposal of DRelevant 
(V?zquez et al, 2007; V?zquez et al, 2004) was 
used. 
To illustrate how the DRD features take their 
values, consider the following sentence: 
??until the lipid response to Accutane is 
established.? 
One of the candidates the system generates 
would be ?lipid response?. It is a two-word 
candidate, so we take the first word and see if it 
belongs to one of the above domains. If it does, 
then we add one to that feature. If the word does 
not belong to any of the domains, then its value 
will be zero. We do the same with the other 
word. In the end, we have a collection where 
every value corresponds to each one of the 
domains. For the example in question the 
collection would be:  
 
 
medicine 1 
anatomy 0 
biology 0 
chemistry 0 
physiology 1 
pharmacy 0 
biochemistry 0 
genetics 0 
Table 1. DRD value assignment example. 
3.2.6. Candidate word number (WNum) 
Because there are candidates that are a multi-
word and others that are not, it may be the case 
that a candidate, which is a multi-word, has an 
EMFSC bigger than others which are not a 
multi-word, just because more than one of the 
words that conform it, have a frequent semantic 
class.  
We decided to add a feature, called WNum, 
which would help us normalize the values of the 
EMFSC. The value of the feature would be the 
number of words the candidate has. Same thing 
happens with DRD. 
3.2.7. Candidate Domain (CD) 
The value of this nominal feature is the domain 
associated to the candidate. If the candidate is a 
multi-word; we get the domain of all the words 
as a whole. In both cases the domain for a single 
word as well as for a multi-word is determined 
using the relevant domains obtained by 
(V?zquez et al, 2007; V?zquez et al, 2004). 
3.2.8. Maximum Frequent 2-grams, 3-
grams 
Drugs usually contain sequences of characters 
that are very frequent in biomedical domain 
texts. These character sequences are called n-
grams, where n is the number of characters in 
the sequence. Because of that, we decided to add 
the ten most frequent n-grams with n between 
two and three. The selected n-grams are the 
following: ?in? (frequency: 8170), ?ne? (4789), 
?ine? (3485), ?ti? (3234), ?id? (2768), ?an? 
(2704), ?ro? (2688), ?nt? (2593), ?et? (2423), 
?en? (2414). 
These features take a value of one if the 
candidate has the corresponding character 
sequence and zero if it does not. For instance: if 
640
we had the candidate ?panobinostat? it will 
generate the following collection:  
?in? 1 
?ne? 0 
?ine? 0 
?ti? 0 
?id? 0 
?an? 1 
?ro? 0 
?nt? 0 
?et? 0 
?en? 0 
Table 2. MF 2-gram, 3-gram. 
3.2.9. Uppercase (UC), Uppercase All 
(UCA). Multi-word (MWord) and 
Number (N) 
Other features say if the first letter of the 
candidate is an uppercase; if all of the letters are 
uppercase (UCA); if it is a multi-word (MWord) 
and also if it is in the singular or in the plural 
(N).  
3.2.10. L1, L2, L3 and R1, R2, R3 
The Part-of-Speech tags of the closest three 
surrounding words of the candidates are also 
included. We named those features L1, L2, and 
L3 for POS tags to the left of the candidate, and 
R1, R2, and R3 for those to the right. 
3.2.11. POS-tagging combination (GC) 
Different values are assigned to candidates, in 
order to identify its POS-tagging combination. 
For instance: to the following entity ?combined 
oral contraceptives? taken from DDI13-train-
TEES-analyses-130304.xml6 training file, which 
was provided for task 9.1, corresponds 5120. 
This number is the result of combining the four 
grammatical categories that really matter to us: 
R for adverb, V for verb, J for adjective, N for 
noun.  
A unique number was given to each 
combination of those four letters. We named this 
feature  GC. 
 
                                                     
6 http://www.cs.york.ac.uk/semeval-2013/task9 
3.2.12. In resource feature (InRe) 
A resource was created which contains all the 
drug entities that were annotated in the training 
corpus, so another attribute tells the system if the 
candidate is in the resource.  
Since all of the entities in the training data set 
were in the resource this attribute could take a 
value of one for all instances. Thus the classifier 
could classify correctly all instances in the 
training data set just looking to this attribute, 
which is not desirable. To avoid that problem, 
we randomly set its value to zero every 9/10 of 
the training instances. 
3.3. Classification 
All the features extracted in the previous stages 
are used in this stage to obtain the two models, 
one for drug detection phase, and the other for 
drug classification phase.  
We accomplished an extensive set of 
experiments in order to select the best classifier. 
All algorithms implemented in WEKA, except 
those that were designed specifically for a 
regression task, were tried. In each case we 
perform a 10-fold cross-validation. In all 
experiments the classifiers were settled with the 
default configuration. From those tests we select 
a decision tree, the C4.5 algorithm (Guti?rrez et 
al., 2011; 2010) implemented as the J48 
classifier in WEKA.  This classifier yields the 
better results for both drug detection and drug 
classification. 
The classifier was trained using a set of 463 
features, extracted from the corpus provided by 
SemEval 2013, the task 9 in question. 
As it was mentioned before, three runs were 
performed for the competition. Run (1) used the 
following features for drug detection: MFSC 
(only 200 frequent semantic classes), MF 2-
grams, 3-grams, UC, UCA, MWord, N, L1, L2, 
L3, R1, R2, R3, CSC, CD, WNum, GC and 
InRe.  
Drug classification in this run used the same 
features except for CD, WNum, and GC. Run 
(2) has all the above features, but we added the 
remaining 200 sematic classes that we left out in 
Run (1) to the detection and the classification 
models. In Run (3), we added EMFSC feature to 
the detection and the classification models. 
641
4. Results 
In the task, the results of the participants were 
compared to a gold-standard and evaluated 
according to various evaluation criteria:  
? Exact evaluation, which demands not only 
boundary match, but also the type of the 
detected drug has to be the same as that of 
the gold-standard. 
? Exact boundary matching (regardless of 
the type). 
? Partial boundary matching (regardless of 
the type) 
? Type matching. 
Precision and recall were calculated using the 
scoring categories proposed by MUC 7: 
? COR: the output of the system and the 
gold-standard annotation agree. 
? INC: the output of the system and the 
gold-standard annotation disagree. 
? PAR: the output of the system and the 
gold-standard annotation are not identical 
but has some overlapping text. 
? MIS: the number of gold-standard entities 
that were not identify by the system. 
? SPU: the number of entities labeled by the 
system that are not in the gold-standard. 
Table 3 , Table 4 and Table 5 show the system 
results in the DDIExtraction2013 competition 
for Run (1).  
Run (2) and Run (3) results are almost the 
same as Run (1). It is an interesting result since 
in those runs 200 additional features were 
supplied to the classifier.  In feature evaluation, 
using CfsSubsetEval and GeneticSearch with 
WEKA we found that all these new features 
were ranked as worthless for the classification. 
On the other hand, the following features were 
the ones that really influenced the classifiers: 
MFSC (215 features only), MF 2-grams, 3-
grams (?ne?, ?ine?, ?ti?, ?ro?, ?et?, ?en?), 
WNum, UC, UCA, L1, R1, CSC, CSC-All, CD, 
DRD (anatomy, physiology, pharmacy, 
biochemistry), InRe, GC and EMFS, specifically 
music.n.01, substance.n.01, herb.n.01, 
artifact.n.01, nutriment.n.01, nonsteroidal_anti-
inflammatory.n.01, causal_agent.n.01 have a 
                                                     
7http://www.itl.nist.gov/iaui/894.02/related_projects/muc/m
uc_sw/muc_sw_manual.html 
frequency of 8, 19, 35, 575, 52, 80, 63 
respectively.  
Measure Strict 
Exact 
Matching 
Partial 
Matching 
Type 
COR 319 354 354 388 
INC 180 145 0 111 
PAR 0 0 145 0 
MIS 187 187 187 187 
SPU 1137 1137 1137 1137 
Precision 0.19 0.22 0.22 0.24 
Recall 0.47 0.52 0.62 0.57 
Table 3. Run (1), all scores. 
Measure Drug Brand Group Drug_n 
COR 197 20 93 9 
INC 23 2 43 1 
PAR 0 0 0 0 
MIS 131 37 19 111 
SPU 754 47 433 14 
Precision 0.2 0.29 016 0.38 
Recall 0.56 0.34 0.6 0.07 
F1 0.3 0.31 0.26 0.12 
Table 4. Scores for entity types, exact matching in 
Run (1). 
 Precision Recall F1 
Macro average 0.26 0.39 0.31 
Strict matching 0.19 0.46 0.27 
Table 5. Macro average and Strict matching measures 
in Run (1). 
5. Conclusion and future works 
In this paper we show the description of 
UMCC_DLSI-(DDI) system, which is able to 
detect and classify drugs in biomedical texts 
with acceptable efficacy. It introduces in this 
thematic the use of semantic information such as 
semantic classes and the relevant domain of the 
words, extracted with ISR-WN resource. With 
this approach we obtained an F-Measure of 
27.5% in the Semeval DDI Extraction2013 task 
9.  
As further work we propose to eliminate some 
detected bugs (i.e. repeated instances, 
multiwords missed) and enrich our knowledge 
base (ISR-WN), using biomedical sources as 
UMLS8, SNOMED9 and OntoFis10. 
                                                     
8 http://www.nlm.nih.gov/research/umls 
9 http://www.ihtsdo.org/snomed-ct/ 
10 http://rua.ua.es/dspace/handle/10045/14216 
642
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
References 
Bj?rne, J.; A. Airola; T. Pahikkala and T. Salakoski 
Drug-Drug Interaction Extraction from 
Biomedical Texts with SVM and RLS Classifiers 
Proceedings of the 1st Challenge Task on Drug-
Drug Interaction Extraction, 2011, 761: 35-42. 
Bobi?, T.; R. Klinger; P. Thomas and M. Hofmann-
Apitius Improving Distantly Supervised Extraction 
of Drug-Drug and Protein-Protein Interactions 
EACL 2012, 2012: 35. 
Cohen, A. M. and W. R. Hersh A survey of current 
work in biomedical text mining Briefings in 
bioinformatics, 2005, 6(1): 57-71. 
Gr?nroos, P.; K. Irjala; J. Heiskanen; K. Torniainen 
and J. Forsstr?m Using computerized individual 
medication data to detect drug effects on clinical 
laboratory tests Scandinavian Journal of Clinical 
& Laboratory Investigation, 1995, 55(S222): 31-
36. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010. 161-168 p. 1135-
5948 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschman, L.; A. Yeh; C. Blaschke and A. Valencia 
Overview of BioCreAtIvE: critical assessment of 
information extraction for biology BMC 
bioinformatics, 2005, 6(Suppl 1): S1. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Percha, B. and R. B. Altman Informatics confronts 
drug?drug interactions Trends in pharmacological 
sciences, 2013. 
Segura-Bedmar, I.; P. Mart?nez and D. S?nchez-
Cisneros The 1st DDIExtraction-2011 challenge 
task: Extraction of Drug-Drug Interactions from 
biomedical texts Challenge Task on Drug-Drug 
Interaction Extraction, 2011, 2011: 1-9. 
Tari, L.; S. Anwar; S. Liang; J. Cai and C. Baral 
Discovering drug?drug interactions: a text-mining 
and reasoning approach based on properties of 
drug metabolism Bioinformatics, 2010, 26(18): 
i547-i553. 
V?zquez, S.; A. Montoyo and Z. Kozareva. 
Extending Relevant Domains for Word Sense 
Disambiguation. IC-AI?07. Proceedings of the 
International Conference on Artificial Intelligence 
USA, 2007.  
V?zquez, S.; A. Montoyo and G. Rigau. Using 
Relevant Domains Resource for Word Sense 
Disambiguation. IC-AI?04. Proceedings of the 
International Conference on Artificial Intelligence, 
Ed: CSREA Press. Las Vegas, E.E.U.U., 2004. 
Wishart, D. S.; C. Knox; A. C. Guo; D. Cheng; S. 
Shrivastava; D. Tzur; B. Gautam and M. Hassanali 
DrugBank: a knowledgebase for drugs, drug 
actions and drug targets Nucleic acids research, 
2008, 36(suppl 1): D901-D906. 
643
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
EmotiBlog: a finer-grained and more precise learning of subjectivity expression models 
Ester Boldrini University of Alicante, Department of Software and Computing Systems eboldrini@dlsi.ua.es 
Alexandra Balahur University of Alicante, Department of Software and Computing Systems abalahur@dlsi.ua.es Patricio Mart?nez-Barco University of Alicante, Department of Software and Computing Systems patricio@dlsi.ua.es 
Andr?s Montoyo University of Alicante, Department of Software and Computing Systems montoyo@dlsi.ua.es  Abstract  
The exponential growth of the subjective in-formation in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog ? a fine-grained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the sys-tems using it for training, through the experi-ments we carried out on opinion mining and emotion detection. We employ corpora of dif-ferent textual genres ?a set of annotated re-ported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life self-expressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detec-tion. 1 Credits This paper has been supported by Ministe-rio de Ciencia e Innovaci?n- Spanish Gov-ernment (grant no. TIN2009-13391-C04-01), and Conselleria d'Educaci?n-Generalitat Valenciana (grant no. PRO-METEO/2009/119 and A-COMP/2010/288).  
2 Introduction The exponential growth of the subjective infor-mation with Web 2.0 created the need to develop new Natural Language Processing (NLP) tools to automatically process and manage the content available on the Internet. Apart from the tradi-tional textual genres, at present we have new ones such as blogs, forums and reviews. The main difference between them is that the latter are predominantly subjective, containing per-sonal judgments. At the moment, NLP tools and methods for analyzing objective information have a better performance than the new ones the research community is creating for managing the subjective content. The survey called ?The State of the Blogosphere 2009?, published by Tech-norati 1 , demonstrates that users are blogging more than ever. Furthermore, in contrast to the general idea about bloggers, each day it is more and more the number of professionals who de-cide to use this means of communication, contra-dicting the common belief about the predomi-nance of an informal editing (Balahur et al, 2009). Due to the growing interest in this text type, the subjective data of the Web is increasing on a daily basis, becoming a reflection of peo-ple?s opinion about a wide range of topics. (Cui, Mittal and Datar, 2006). Blogs represent an im-portant source of real-time, unbiased informa-tion, useful for the development of many applica-tions for concrete purposes. Given the proved importance of automatically processing this data, a new task has appeared in NLP task, dealing with the treatment of subjective data: Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al, 2009), a fine-grained annotation scheme for labeling sub-jectivity in the new textual genres. Subjectivity                                                 1 http://technorati.com/ 
1
can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2  and opinions, generally denomi-nated ?private states? (Uspensky, 1973), not open to verification (Wiebe, 1994). We per-formed a series of experiments focused on dem-onstrating that EmotiBlog represents a step for-ward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, mov-ing a step forward the mere polarity recognition. In particular, the experiments concern expres-sions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification. To that aim, we employ corpora of different textual genres? a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Bala-hur et al, 2010) and the set of news titles anno-tated with polarity and emotion from the SemE-val 2007 Task No. 14 (Strapparava and Mihal-cea, 2007), as well as a corpus of real-life self-expressed emotion entitled ISEAR (Scherer and Walbott, 1999). We subsequently show, through the quality of the results obtained, that Emoti-Blog, through its structure and annotation para-digm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection.  3 Motivation and Contribution The main motivation of this research is the dem-onstrated necessity to work towards the harmoni-zation and interoperability of the increasingly large number of tools and frameworks that sup-port the creation, instantiation, manipulation, querying, and exploitation of annotated resource. This necessity is stressed by the new tools and resources, which have been recently created for processing the subjectivity in the new-textual genres born with the Web 2.0. Such predomi-nantly subjective data is increasing at an expo-nential rate (about 75000 new blogs are reported to be created every day) and contains opinions on the most diverse set of topics. Given its world-wide availability, the subjective data on the Web has become a primary source of information (Balahur et al, 2009). As a consequence, new mechanisms have to be implemented so that this                                                 2 http://dictionary.cambridge.org/ 
data is effectively analyzed and processed. The main challenge of the opinionated content is that, unlike the objective one, which presents facts, the subjective information is most of the times difficult and complex to extract and classify us-ing in grammatically static and fixed rules. Ex-pression of subjectivity is more spontaneous and even if the majority is quite formal, new means of expressivity can be encountered, such as the use of colloquialisms, sayings, collocations or anomalies in the use of punctuation; this is moti-vated by the fact that subjectivity expression is part of our daily life. For example, at the time of taking a decision, people search for information and opinions expressed on the Web on their mat-ter of interest and base their final decision on the information found. At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the perform-ance of that product before purchasing it. There-fore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users. On the other hand, the amount of data to be analyzed requires the development of special-ized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics. (Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Compu-tational Linguistics, which is concerned not with the topic a document is about, but with the opin-ion it expresses. Research in this field has proven the task to be very difficult, due to the high se-mantic variability of affective language. Differ-ent authors have addressed the problem of ex-tracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (?Nokia E65?/?Why do people buy Nokia E65??), type of text (review on forum/blog/dialogue/press arti-cle), and manner of expression of opinion - di-rectly (using opinion statements, e.g. ?I think this product is wonderful!?/?This is a bright initia-tive?), indirectly (using affect vocabulary, e.g. ?I love the pictures this camera takes!?/?Personally, I am shocked one can pro-pose such a law!?) or implicitly (using adjectives and evaluative expressions, e.g. ?It?s light as a feather and fits right into my pocket!?). While determining the overall opinion on a movie is sufficient for taking the decision to watch it or not, when buying a product, people are interested in the individual opinions on the different prod-
2
uct characteristics. When discussing a person, one can judge and give opinion on the person?s actions. Moreover, the approaches taken can vary depending on the manner in which a user asks for the data (general formula such as ?opinions on X? or a specific question ?Why do people like X?? and the text source that needs to be queried). Retrieving opinion information in newspaper articles or blogs posts is more complex, because it involves the detection of different discussion topics, the subjective phrases present and subse-quently their classification according to polarity. Especially in the blog area, determining points of view expressed in dialogues together with the mixture of quotes and pastes from newspapers on a topic can, additionally, involve determining the persons present and whether or not the opinion expressed is on the required topic or on a point previously made by another speaker. This diffi-cult NLP problem requires the use of specialized data for system training and tuning, gathered, annotated and tested within the different text spheres. At the present moment, these specialized resources are scarce and when they exist, they are rather simplistically annotated or highly domain-dependent. Moreover, most of these resources created are for the English. The contribution we describe in this paper intends to propose solutions to the above-mentioned problems, and consists of the following points: first of all, we overcome the problem of corpora scarcity in other languages except English and also improve the English ones; we present the manner in which we compiled a multilingual corpus of blog posts on different topics of interest in three languages-Spanish, Italian and English. The second issue we tried to solve was the coarse-grained annotation schemas employed in other annotation schema. Thus, we describe the new annotation model, EmotiBlog built up in order to capture the different subjectivity/objectivity, emotion/opinion/attitude aspects we are interested in at a finer-grained level. We justify the need for a more detailed annotation model, the sources and the reasons taken into consideration when constructing the corpus and its annotation. Thirdly, we address an aspect strongly related to blogs annotation: due the presence of ?copy and pastes? from news articles or other blogs, the frequent quotes, we include the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research. 
this research. Finally, we conclude on our ap-proach and propose the lines for future work. 4 Related Work In recent years, different researchers have ad-dressed the needs and possible methodologies from the linguistic, theoretical and practical points of view. Thus, the first step involved re-sided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or ?emotion triggers? (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeni-able influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giv-ing a method to annotate a corpus depending on these two aspects ? MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the senti-ment task, as part of Opinion Information Re-trieval  (last three editions of the TREC Blog tracks 3  competitions, the TAC 2008 competi-tion4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al, 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial dis-crimination, but we also add a deeper level of emotion annotation. Since expressions of emo-tion are also highly related to opinions, related work also includes customer review classifica-tion at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations                                                 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 
3
and other attributes with SVM (Mullena and Col-lier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sen-timent classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in ana-lysing sentiment at a sentence level using boot-strapping techniques (Riloff, Wiebe, 2003), con-sidering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then ap-plying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opin-ion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orienta-tion of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, fine-grained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into ac-count modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They are extremely useful and a poll for discussing about any topic with the world. For this reason, the first corpus object of our study is a collection of blog posts extracted from the Web. The texts we selected have distinctive features, extremely different from traditional tex-tual ones. In fact people writing a post can use an informal language colloquialism, emoticons, etc. to express their feelings and it is not rare to find a mix of sources in the same post; people usually mention some facts or discourses and then they give their opinion about them. As we can deduce, 
the source detection represents one of the most complex tasks. As we mentioned above, we car-ried out a multilingual research, collecting texts in three languages: Spanish, Italian, and English about three subjects of interest. The first one contains blog posts commenting upon the signing of the Kyoto Protocol against global warming, the second collection consists of blog entries about the Mugabe government in Zimbabwe, and finally we selected a series of blog posts discuss-ing the issues related to the 2008 USA presiden-tial elections. For each of the abovementioned topics, we have gathered 100 texts, summing up a total of 30.000 words approximately for each language. However in this research we start with English but consider as future work labeling the other languages we have. The second corpus we employed for this research is a collection of 1592 quotes extracted from the news in April 2008. As a consequence they are about many different top-ics and in English (Balahur and Steinberg, 2009). Both of these corpora have been annotated with EmotiBlog that is presented in the next section. 6 EmotiBlog Annotation Model Our annotation schema can be defined as a fine-grained model for labelling subjectivity of the new-textual genres born with the Web 2.0. As mentioned above, it represents a step forward to previous research and it is focused on detecting the linguistic elements, which give subjectivity to the text. The EmotiBlog annotation is divided into different levels (Figure 1).  
 Figure 1: General structure of EmotiBlog.  As we can observe in Figure 1, the first distinc-tion to be made is between objective and subjec-tive speech. If we are labelling an objective sen-tence, we insert the source element, while if we are annotating a subjective discourse, a list of elements with the corresponding attributes have to be added. We select among the list of subjec-tive elements and specify the element?s attrib-
4
utes. Table 1 presents the annotation model in detail.  Elem. Description Obj. speech Confidence, comment, source, target. Subj. speech Confidence, comment, level, emotion, phenomenon, polarity, source and target. Adjectives Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Adverbs Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Verbs Confidence, comment, level, emotion, phenomenon, polarity, mode, source and target. Anaphora Confidence, comment, type, source and target. Capital letter Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Punctuation Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Names Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, and source. Phenomenon Confidence, comment, type: collocation, saying, slang, title, and rhetoric. Reader Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Author Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Emotions Confidence, comment, accept, anger, anticipation, anxiety, appreciation, bad, bewilderment, comfort, ? Table 1: EmotiBlog structure  Each element of the discourse has its own attrib-utes with a series of features, which have to be annotated. Due to space reasons it is impossible to detail each one of them, however we would like to underline the most innovative and rel-evant. For each element we are labelling the an-notator has to insert his level of confidence. In this way we will assign each label a weight that will be computed for future evaluations. More-over, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a com-plete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation pro-cess. In fact emotions of the same subgroup will have less impact when calculating the inter-annotation agreement. In order to make this sub-division proper and effective division, we were inspired by (Scherer, 2005) who created an alter-native dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sen-timents and after that between active and passive. Furthermore emotions are grouped between ob-structive and conductive, and finally between high power and low power control. We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive. Further on, we distributed the sentiments within our list into the Scherer slots creating other smaller categories included in the abovementioned general ones. The result of this division is shown in Table 2: 
Table 2: Alternative dimensional structures of the semantic space for emotions  Following with the description of the model, we said that the first distinction to be made is be-tween objective and subjective speech. Analys-ing the texts we collected, we realised that even if the writer uses an objective speech, sometimes it is just apparently objective and for this reason we added two elements: reader and author inter-pretation. The first one is the impres-sion/feeling/reaction the reader has reading the intervention and what s/he can deduce from the piece of text and the author interpretation is what we can understand from the author (politic orien-tation, preferences). All this information can be deduced form some linguistic elements that ap-parently are not so objective as they may appear. Another innovative element we inserted in the model is the coreference but just at a cross-post level. It is necessary because blogs are composed by posts linked between them and thus cross-
Group Emotions Criticism Sarcasm, irony, incorrect, criticism, objection, opposition, scepticism. Happiness Joy, joke. Support Accept, correct, good, hope, support, trust, rapture, respect, patience, appreciation, excuse. Importance Important, interesting, will, justice, longing, anticipation, revenge. Gratitude Thank. Guilt Guilt, vexation. Fear Fear, fright, troubledness, anxiety. Surprise Surprise, bewilderment, disappoint-ment, consternation. Anger Rage, hatred, enmity, wrath, force, anger, revendication. Envy Envy, rivalry, jealousy. Indifference Unimportant, yield, sluggishness. Pity Compassion, shame, grief. Pain Sadness, lament, remorse, mourning, depression, despondency. Shyness Timidity. Bad Bad, malice, disgust, greed. 
5
document coreference can help the reader to fol-low the conversations. We also label the unusual usage of capital letters and repeated punctuation. In fact, it is very common in blogs to find words written in capital letter or with no conventional usage of punctuation; these features usually mean shouts or a particular mood of the writer. Using EmotiBlog, we annotate the single ele-ments, but we also mark sayings or collocations, representative of each language. A saying is a well-known and wise statement, which often has a meaning, different from the simple meanings of the words it contains5; while a collocation is a word or phrase, which is frequently used with another word or phrase, in a way that sounds cor-rect to native speakers, but might not be expected from the individual words? meanings6. Finally we insert for each element the source and topic. An example of annotation can be:  <phenomenon target="Kyoto Protocol" category="phrase" degree="medium" source="w" polarity="positive" emotion="good">The Onion has a <adjective target="Kyoto Protocol" phenomenon="phrase" de-gree="medium" polarity="positive" emotion="good" source="w" ismodifier="yes">great</adjective> story today titled ?Bush Told to Sign Birthday Treaty for Someone Named Kyoto." </phenomenon> 7 Experiments and Evaluation In order to evaluate the appropriateness of the EmotiBlog annotation scheme and to prove that the fine-grained level it aims at has a positive impact on the performance of the systems em-ploying it as training, we performed several ex-periments. Given that a) EmotiBlog contains an-notations for individual words, as well as for multi-word expressions and at a sentence level, and b) they are labeled with polarity, but also emotion, our experiments show how the anno-tated elements can be used as training for the opinion mining and polarity classification task, as well as for emotion detection. Moreover, tak-ing into consideration the fact that EmotiBlog labels the intensity level of the annotated ele-ments, we performed a brief experiment on de-termining the sentiment intensity, measured on a three-level scale: low, medium and high. In order to perform these three different evaluations, we chose three different corpora. The first one is a collection of quotes (reported speech) from newspaper articles presented in (Balahur et al, 2010), enriched with the manual fine-grained 
                                                5  Definition according to the Cambridge Advanced Learner?s Dictionary 6   Definition according to the Cambridge Advanced Learner?s Dictionary 
annotation of EmotiBlog7; the second one is the collection of newspaper titles in the test set of the SemEval 2007 task number 14 ? Affective Text. Finally, the third one is a corpus of self-reported emotional response ? ISEAR (Scherer and Wal-bott, 1999). The intensity classification task is evaluated only on the second corpus, given that it is the only one in which scores between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles.  6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a ?_? all the tokens pertaining to the NE. All the annotations of punctuation signs that had a specific meaning together were also united un-der a single punctuation sign. Subsequently, we processed the annotated data, using Minipar. We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al, 2005): ? the part of speech (POS)  ? capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) ? opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polar-ity, i.e. 1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) ? syntactic relatedness with other opinion word ? if it is directly dependent of an opin-ion word or modifier (0 or 1), plus the polar-ity/intensity and emotion of this word (0 for all the components otherwise) ?  role in 2-word, 3-word and 4-word annota-tions: opinionatedness, intensity and emo-tion of the other words contained in the an-notation, direct dependency relations with them if they exist and 0 otherwise.  We compute the length of the longest sentence in EmotiBlog. The feature vector for each of the sentences contains the feature vectors of each of its words and 0s for the corresponding feature vectors of the words, which the current sentence has less than the longest annotated sentence. Fi-nally, we add for each sentence as feature binary features for subjectivity and polarity, the value corresponding to the intensity of opinion and the                                                 7 Freely available on request to the authors. 
6
general emotion. These feature vectors are fed into the Weka8 SVM SMO ML algorithm and a model is created (EmotiBlog I). A second model (EmotiBlog II) is created by adding to the collec-tion of single opinion and emotion words anno-tated in EmotiBlog, the Opinion Finder lexicon and the opinion words found in MicroWordNet, the General Inquirer resource and WordNet Af-fect.   6.2 Evaluation of models on test sets  In order to evaluate the performance of the mod-els extracted from the features of the annotations in EmotiBlog, we performed different tests. The first one regarded the evaluation of the polarity and intensity classification task using the Emoit-blog I and II constructed models on two test sets ? the JRC quotes collection and the SemEval 2007 Task Number 14 test set. Since the quotes often contain more than a sentence, we consider the polarity and intensity of the entire quote as the most frequent result in each class, corre-sponding to its constituent sentences. Also, given the fact that the SemEval Affective Text head-lines were given intensity values between -100 and 100, we mapped the values contained in the Gold Standard of the task into three categories: [-100, -67] is high (value 3 in intensity) and nega-tive (value -1 in polarity), [-66, 34] medium negative and [33, 1] is low negative. The values between [1 and 100] are mapped in the same manner to the positive category. 0 was consid-ered objective, so containing the value 0 for in-tensity. The results are presented in Table 3 (the values I and II correspond to the models Emoti-Blog I and EmotiBlog II):   Test  Corpus Evaluation type Precision Recall Polarity 32.13 54.09 JRC quotes I Intensity 36.00 53.2 Polarity 36.4 51.00 JRC quotes II Intensity 38.7 57.81 Polarity 38.57 51.3 SemEval I Intensity 37.39 50.9 Polarity 35.8 58.68 SemEval II Intensity 32.3 50.4 Table 3. Results for polarity and intensity classifi-cation using the models built from the EmotiBlog annotations The results shown in Table 2 show a signifi-cantly high improvement over the results ob-tained in the SemEval task in 2007. This is ex-plainable, on the one hand, by the fact that sys-                                                8 http://www.cs.waikato.ac.nz/ml/weka/ 
tems performing the opinion task did not have at their disposal the lexical resources for opinion employed in the EmotiBlog II model, but also because of the fact that they did not use machine learning on a corpus comparable to EmotiBlog (as seen from the results obtained when using solely the EmotiBlog I corpus). Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in preci-sion, with a recall that is comparable to most of the participating systems. In the second experi-ment, we tested the performance of emotion clas-sification using the two models built using Emo-tiBlog on the three corpora ? JRC quotes, SemE-val 2007 Task No.14 test set and the ISEAR cor-pus. The JRC quotes are labeled using Emoti-Blog; however, the other two are labeled with a small set of emotions ? 6 in the case of the Se-mEval data (joy, surprise, anger, fear, sadness, disgust) and 7 in ISEAR (joy, sadness, anger, fear, guilt, shame, disgust). Moreover, the Se-mEval data contains more than one emotion per title in the Gold Standard, therefore we consider as correct any of the classifications containing one of them. In order to unify the results and ob-tain comparable evaluations, we assessed the performance of the system using the alternative dimensional structures defined in Table 1. The ones not overlapping with the category of any of the 8 different emotions in SemEval and ISEAR are considered as ?Other? and are not included either in the training, nor test set. The results of the evaluation are presented in Table 4. Again, the values I and II correspond to the models EmotiBlog I and II. The ?Emotions? category contains the following emotions: joy, sadness, anger, fear, guilt, shame, disgust, surprise.  Test  corpus Evaluation  type Precision Recall JRC quotes I Emotions   24.7 15.08 
JRC quotes II Emotions  33.65 18.98 
SemEval I Emotions 29.03 18.89 SemEval II Emotions 32.98 18.45 ISEAR I Emotions 22.31 15.01 ISEAR II Emotions 25.62 17.83 Table 4. Results for emotion classification using the models built from the EmotiBlog annotations. The best results for emotion detection were ob-tained for the ?anger? category, where the preci-sion was around 35 percent, for a recall of 19 percent. The worst results obtained were for the ISEAR category of ?shame?, where precision was around 12 percent, with a recall of 15 per-
7
cent. We believe this is due to the fact that the latter emotion is a combination of more complex affective states and it can be easily misclassified to other categories of emotion. Moreover, from the analysis performed on the errors, we realized that many of the affective phenomena presented were more explicit in the case of texts expressing strong emotions such as ?joy? and ?anger?, and were mostly related to common-sense interpreta-tion of the facts presented in the weaker ones. As it can be seen in Table 3, results for the texts per-taining to the news category obtain better results, most of all news titles. This is due to the fact that such texts, although they contain a few words, have a more direct and stronger emotional charge than direct speech (which may be biased by the need to be diplomatic, find the best suited words etc.). Finally, the error analysis showed that emo-tion that is directly reported by the persons expe-riencing is more ?hidden?, in the use of words carrying special signification or related to gen-eral human experience. This fact makes emotion detection in such texts a harder task. Neverthe-less, the results in all corpora are comparable, showing that the approach is robust enough to handle different text types. All in all, the results obtained using the fine and coarse-grained anno-tations in EmotiBlog increased the performance of emotion detection as compared to the systems in the SemEval competition.   6.3 Discussion on the overall results  From the results obtained, we can see that this approach combining the features extracted from the EmotiBlog fine and coarse-grained annota-tions helps to balance between the results ob-tained for precision and recall. The impact of using additional resources that contain opinion words is that of increasing the recall of the sys-tem, at the cost of a slight drop in precision, which proves that the approach is robust enough so that additional knowledge sources can be added. Although the corpus is small, the results obtained show that the phenomena it captures is relevant in the OM task, not only for the blog sphere, but also for other types of text (newspa-per articles, self-reported affect). 8 Conclusions and future work Due to the exponential increase of the subjective information result of the high-level usage of the Internet and the Web 2.0, NLP able to process this data are required. In this paper we presented 
the procedure by which we compiled a multilin-gual corpus of blog posts on different topics of interest in three languages: Spanish, Italian and English. Further on, we explained the need to create a finer-grained annotation schema that can be used to improve the performance of subjectiv-ity mining systems. Thus, we presented the new annotation model, EmotiBlog and justified the benefits of this detailed annotation schema, pre-senting the sources and the reasons taken into consideration when building up the corpus and its labeling. Furthermore, we addressed the pres-ence of ?copy and pastes? from news articles or other blogs, the frequent quotes. For solving this possible ambiguity we included the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We performed several experiments on three dif-ferent corpora, aimed at finding and classifying both the opinion, as well as the expressions of emotion they contained; we showed that the fine and coarse-grained levels of annotation that EmotiBlog contains offers important information on the structure of affective texts, leading to an improvement of the performance of systems trained on it. Although the EmotiBlog corpus is small, the results obtained are promising and show that the phenomena it captures are relevant in the OM task, not only for the blog sphere, but also for other textual-genres. It is well known that OM is an extremely challenging task and a young discipline, thus there is room for im-provement above all to solve linguistic phenom-ena such as the correference resolution at a cross document level, temporal expression recognition. In addition to this, more experiments would need to be done in order to verify the complete ro-bustness of EmotiBlog. Last but not least, our idea is to include the existing tools for a more effective semi-supervised annotation. After the training of the ML system we obtain automati-cally some markables which have to be validated or not by the annotator and the ideal option would be to connect these terms the system de-tects automatically with tools, such as the map-ping with an opinion lexicon based on WordNet (SentiWordNet, WordNet Affect, MicroWord-Net), in order to automatically annotate all the synonyms and antonyms with the same or the opposite polarity respectively and assigning them some other elements contemplated into the Emo-tiBlog annotation schema. This would mean an important step forward for saving time during the annotation process and it will also assure a high quality annotation due to the human supervision. 
8
References Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J. 2010. Sentiment Analysis in the News.  In Proceedings of LREC 2010. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. A Comparative Study of Open Domain and Opinion Question Answering Systems for Fac-tual and Opinionated Queries. In Proceedings of the Recent Advances in Natural Language Proc-essing. Balahur A., Montoyo A. 2008. Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification. In Proceed-ings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scot-land. Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back. In Proceeding of WOMSA 2009. Seville. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. Summarizing Threads in Blogs Using Opinion Polarity. In Proceedings of ETTS work-shop. RANLP. 2009. Boldrini E., Balahur A., Mart?nez-Barco P., Montoyo A. 2009. EmotiBlog: a fine-grained model for emotion detection in non-traditional textual gen-res. In Proceedings of WOMSA. Seville, Spain. Boldrini E., Fern?ndez J., G?mez J.M., Mart?nez-Barco P. 2009. Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres. In Proceedings of WOMSA 2009. Seville, Spain. Chaovalit P, Zhou L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS-05. Carletta J. 1996. Assessing agreement on classification task: the kappa statistic. Computa-tional Linguistics, 22(2): 249?254. Cui H., Mittal V., Datar M. 2006. Comparative Ex-periments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st Na-tional Conference on Artificial Intelligence AAAI. Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007. Language resources and linguistic theory: Typology, second language ac-quisition. English linguistics (Forthcoming), chap-ter Micro-WNOp: A gold standard for the evalua-tion of automatically compiled lexical resources for opinion mining. Franco Angeli Editore, Milano, IT. Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005. Identifying Sources of Opinions with Conditional Random  Fields and Extraction Patterns.  In Pro-ceedings of the HLT/EMNLP.  Dave K., Lawrence S., Pennock, D. ?Mining the Pea-nut Gallery: Opinion Extraction and Semantic Classification of Product Reviews?. In Proceedings of WWW-03. 2003. 
Esuli A., Sebastiani F. 2006. SentiWordNet: A Pub-licly Available Resource for Opinion Mining. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.  Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science. Goldberg A.B., Zhu J. 2006. Seeing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Process-ing. Hu M., Liu B. 2004. Mining Opinion Features in Cus-tomer Reviews. In Proceedings of Nineteenth Na-tional Conference on Artificial Intelligence AAAI. Hatzivassiloglou V., Wiebe J. 2000. Effects of adjec-tive orientation and gradability on sentence subjec-tivity. In Proceedings of COLING.  Kim S.M., Hovy E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING. Mullen T., Collier N. 2006. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proceedings of EMNLP. 2004. Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A. ?Which Side are You On? Identifying Perspectives at the Document and Sentence Levels?. In Proceedings of the Tenth Conference on Natural Language Learn-ing CoNLL.2006.  Ng V., Dasgupta S. and Arifin S. M. 2006. Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Re-views. In the proceedings of the ACL, Sydney. Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Lan-guage Processing. Riloff E., Wiebe J. 2003. Learning Extraction Pat-terns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.  Strapparava C. Valitutti A. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings ofthe 4th International  Conference on Language Resources and Evaluation, LREC. Russell J.A. 1983. Pancultural aspects of the human conceptual organization of emotions. Journal of Personality and Social Psychology 45: 1281?8. Scherer K. R. 2005. What are emotions? And how can they be measured? Social Science Information, 44(4), 693?727. Stoyanov V. and Cardie C. 2006. Toward Opinion Summarization: Linking the Sources. COLING-ACL. Workshop on Sentiment and Subjectivity in Text. Stoyanov V., Cardie C., Litman D., and Wiebe J. 2004. Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and An-
9
swer Corpus. AAAI Spring Symposium on Explor-ing Attitude and Affect in Text.  Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text. In  Proceedings of the ACL.   Turney P. 2002. Thumbs Up or Thumbs Down? Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews. ACL 2002: 417-424. Turney P., Littman M. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems 21. Uspensky B. 1973. A Poetics of Composition. Univer-sity of California Press, Berkeley, California. Wiebe J. M. 1994. Tracking point of view in narra-tive. Computational Linguistics, vol. 20, pp. 233?287. Wiebe J., Wilson T. and Cardie C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation. Wilson T., Wiebe J., Hwa R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI. Wiebe J., Wilson T. and Cardie C. 2005. ?Annotation Expressions of Opinions and Emotions in Lan-guage. Language Resources and Evaluation.  Wiebe J., Riloff E. 2005. Creating Subjective and Objective Sentence Classifiers from Unannotated Texts. In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing).      
10
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,
Uppsala, July 2010.
A Survey on the Role of Negation in Sentiment Analysis
Michael Wiegand
Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Alexandra Balahur
University of Alicante
Alicante, Spain
abalahur@dlsi.ua.es
Benjamin Roth and Dietrich Klakow
Saarland University
Saarbru?cken, Germany
benjamin.roth@lsv.uni-saarland.de
dietrich.klakow@lsv.uni-saarland.de
Andre?s Montoyo
University of Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
This paper presents a survey on the role of
negation in sentiment analysis. Negation
is a very common linguistic construction
that affects polarity and, therefore, needs
to be taken into consideration in sentiment
analysis.
We will present various computational ap-
proaches modeling negation in sentiment
analysis. We will, in particular, focus
on aspects, such as level of representation
used for sentiment analysis, negation word
detection and scope of negation. We will
also discuss limits and challenges of nega-
tion modeling on that task.
1 Introduction
Sentiment analysis is the task dealing with the
automatic detection and classification of opinions
expressed in text written in natural language.
Subjectivity is defined as the linguistic expression
of somebody?s opinions, sentiments, emotions,
evaluations, beliefs and speculations (Wiebe,
1994). Subjectivity is opposed to objectivity,
which is the expression of facts. It is important to
make the distinction between subjectivity detec-
tion and sentiment analysis, as they are two sep-
arate tasks in natural language processing. Sen-
timent analysis can be dependently or indepen-
dently done from subjectivity detection, although
Pang and Lee (2004) state that subjectivity de-
tection performed prior to the sentiment analysis
leads to better results in the latter.
Although research in this area has started only re-
cently, the substantial growth in subjective infor-
mation on the world wide web in the past years
has made sentiment analysis a task on which con-
stantly growing efforts have been concentrated.
The body of research published on sentiment anal-
ysis has shown that the task is difficult, not only
due to the syntactic and semantic variability of
language, but also because it involves the extrac-
tion of indirect or implicit assessments of objects,
by means of emotions or attitudes. Being a part
of subjective language, the expression of opinions
involves the use of nuances and intricate surface
realizations. That is why the automatic study of
opinions requires fine-grained linguistic analysis
techniques and substantial efforts to extract fea-
tures for machine learning or rule-based systems,
in which subtle phenomena as negation can be ap-
propriately incorporated.
Sentiment analysis is considered as a subsequent
task to subjectivity detection, which should ideally
be performed to extract content that is not factual
in nature. Subsequently, sentiment analysis aims
at classifying the sentiment of the opinions into
polarity types (the common types are positive and
negative). This text classification task is also re-
ferred to as polarity classification.
This paper presents a survey on the role of nega-
tion in sentiment analysis. Negation is a very com-
mon linguistic construction that affects polarity
and, therefore, needs to be taken into considera-
tion in sentiment analysis. Before we describe the
computational approaches that have been devised
to account for this phenomenon in sentiment anal-
ysis, we will motivate the problem.
2 Motivation
Since subjectivity and sentiment are related to ex-
pressions of personal attitudes, the way in which
this is realized at the surface level influences the
manner in which an opinion is extracted and its
polarity is computed. As we have seen, sentiment
analysis goes a step beyond subjectivity detection,
60
including polarity classification. So, in this task,
correctly determining the valence of a text span
(whether it conveys a positive or negative opinion)
is equivalent to the success or failure of the auto-
matic processing.
It is easy to see that Sentence 1 expresses a posi-
tive opinion.
1. I like+ this new Nokia model.
The polarity is conveyed by like which is a polar
expression. Polar expressions, such as like or hor-
rible, are words containing a prior polarity. The
negation of Sentence 1, i.e. Sentence 2, using the
negation word not, expresses a negative opinion.
2. I do [not like+]? this new Nokia model.
In this example, it is straightforward to notice the
impact of negation on the polarity of the opinion
expressed. However, it is not always that easy
to spot positive and negative opinions in text. A
negation word can also be used in other expres-
sions without constituting a negation of the propo-
sition expressed as exemplified in Sentence 3.
3. Not only is this phone expensive but it is also heavy and
difficult to use.
In this context, not does not invert the polarity of
the opinion expressed which remains negative.
Moreover, the presence of an actual negation word
in a sentence does not mean that all its polar opin-
ions are inverted. In Sentence 4, for example, the
negation does not modify the second polar expres-
sion intriguing since the negation and intriguing
are in separate clauses.
4. [I do [not like+]? the design of new Nokia model] but
[it contains some intriguing+ new functions].
Therefore, when treating negation, one must be
able to correctly determine the scope that it has
(i.e. determine what part of the meaning expressed
is modified by the presence of the negation).
Finally, the surface realization of a negation is
highly variable, depending on various factors,
such as the impact the author wants to make on
the general text meaning, the context, the textual
genre etc. Most of the times, its expression is far
from being simple (as in the first two examples),
and does not only contain obvious negation words,
such as not, neither or nor. Research in the field
has shown that there are many other words that in-
vert the polarity of an opinion expressed, such as
diminishers/valence shifters (Sentence 5), connec-
tives (Sentence 6), or even modals (Sentence 7).
5. I find the functionality of the new phone less practical.
6. Perhaps it is a great phone, but I fail to see why.
7. In theory, the phone should have worked even under
water.
As can be seen from these examples, modeling
negation is a difficult yet important aspect of sen-
timent analysis.
3 The Survey
In this survey, we focus on work that has presented
novel aspects for negation modeling in sentiment
analysis and we describe them chronologically.
3.1 Negation and Bag of Words in Supervised
Machine Learning
Several research efforts in polarity classification
employ supervised machine-learning algorithms,
like Support Vector Machines, Na??ve Bayes Clas-
sifiers or Maximum Entropy Classifiers. For these
algorithms, already a low-level representation us-
ing bag of words is fairly effective (Pang et al,
2002). Using a bag-of-words representation, the
supervised classifier has to figure out by itself
which words in the dataset, or more precisely fea-
ture set, are polar and which are not. One either
considers all words occurring in a dataset or, as
in the case of Pang et al (2002), one carries out
a simple feature selection, such as removing infre-
quent words. Thus, the standard bag-of-words rep-
resentation does not contain any explicit knowl-
edge of polar expressions. As a consequence of
this simple level of representation, the reversal
of the polarity type of polar expressions as it is
caused by a negation cannot be explicitly modeled.
The usual way to incorporate negation modeling
into this representation is to add artificial words:
i.e. if a word x is preceded by a negation word,
then rather than considering this as an occurrence
of the feature x, a new feature NOT x is created.
The scope of negation cannot be properly modeled
with this representation either. Pang et al (2002),
for example, consider every word until the next
punctuation mark. Sentence 2 would, therefore,
result in the following representation:
8. I do not NOT like NOT this NOT new NOT Nokia
NOT model.
The advantage of this feature design is that a plain
occurrence and a negated occurrence of a word are
61
reflected by two separate features. The disadvan-
tage, however, is that these two contexts treat the
same word as two completely different entities.
Since the words to be considered are unrestricted,
any word ? no matter whether it is an actual po-
lar expression or not ? is subjected to this nega-
tion modification. This is not only linguistically
inaccurate but also increases the feature space with
more sparse features (since the majority of words
will only be negated once or twice in a corpus).
Considering these shortcomings, it comes to no
surprise that the impact of negation modeling on
this level of representation is limited. Pang et al
(2002) report only a negligible improvement by
adding the artificial features compared to plain bag
of words in which negation is not considered.
Despite the lack of linguistic plausibility, super-
vised polarity classifiers using bag of words (in
particular, if training and testing are done on the
same domain) offer fairly good performance. This
is, in particular, the case on coarse-grained clas-
sification, such as on document level. The suc-
cess of these methods can be explained by the
fact that larger texts contain redundant informa-
tion, e.g. it does not matter whether a classifier
cannot model a negation if the text to be classi-
fied contains twenty polar opinions and only one
or two contain a negation. Another advantage
of these machine learning approaches on coarse-
grained classification is their usage of higher order
n-grams. Imagine a labeled training set of docu-
ments contains frequent bigrams, such as not ap-
pealing or less entertaining. Then a feature set us-
ing higher order n-grams implicitly contains nega-
tion modeling. This also partially explains the ef-
fectiveness of bigrams and trigrams for this task as
stated in (Ng et al, 2006).
The dataset used for the experiments in (Pang et
al., 2002; Ng et al, 2006) has been established as
a popular benchmark dataset for sentiment analy-
sis and is publicly available1.
3.2 Incorporating Negation in Models that
Include Knowledge of Polar Expressions
- Early Works
The previous subsection suggested that appropri-
ate negation modeling for sentiment analysis re-
quires the awareness of polar expressions. One
way of obtaining such expressions is by using a
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
polarity lexicon which contains a list of polar ex-
pressions and for each expression the correspond-
ing polarity type. A simple rule-based polarity
classifier derived from this knowledge typically
counts the number of positive and negative polar
expressions in a text and assigns it the polarity
type with the majority of polar expressions. The
counts of polar expressions can also be used as
features in a supervised classifier. Negation is typ-
ically incorporated in those features, e.g. by con-
sidering negated polar expressions as unnegated
polar expressions with the opposite polarity type.
3.2.1 Contextual Valence Shifters
The first computational model that accounts for
negation in a model that includes knowledge of
polar expressions is (Polanyi and Zaenen, 2004).
The different types of negations are modeled via
contextual valence shifting. The model assigns
scores to polar expressions, i.e. positive scores to
positive polar expressions and negative scores to
negative polar expressions, respectively. If a polar
expression is negated, its polarity score is simply
inverted (see Example 1).
clever (+2) ? not clever (?2) (1)
In a similar fashion, diminishers are taken into
consideration. The difference is, however, that
the score is only reduced rather than shifted to the
other polarity type (see Example 2).
efficient (+2)? rather efficient (+1) (2)
Beyond that the model also accounts for modals,
presuppositional items and even discourse-based
valence shifting. Unfortunately, this model is
not implemented and, therefore, one can only
speculate about its real effectiveness.
Kennedy and Inkpen (2005) evaluate a nega-
tion model which is fairly identical to the one pro-
posed by Polanyi and Zaenen (2004) (as far as
simple negation words and diminishers are con-
cerned) in document-level polarity classification.
A simple scope for negation is chosen. A polar
expression is thought to be negated if the negation
word immediately precedes it. In an extension of
this work (Kennedy and Inkpen, 2006) a parser is
considered for scope computation. Unfortunately,
no precise description of how the parse is used
for scope modeling is given in that work. Neither
is there a comparison of these two scope models
measuring their respective impacts.
62
Final results show that modeling negation is im-
portant and relevant, even in the case of such sim-
ple methods. The consideration of negation words
is more important than that of diminishers.
3.2.2 Features for Negation Modeling
Wilson et al (2005) carry out more advanced
negation modeling on expression-level polarity
classification. The work uses supervised machine
learning where negation modeling is mostly en-
coded as features using polar expressions. The
features for negation modeling are organized in
three groups:
? negation features
? shifter features
? polarity modification features
Negation features directly relate to negation ex-
pressions negating a polar expression. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar
expression. The other feature accounts for a polar
predicate having a negated subject. This frequent
long-range relationship is illustrated in Sentence 9.
9. [No politically prudent Israeli]
subject
could
support
polar pred
either of them.
All negation expressions are additionally disam-
biguated as some negation words do not function
as a negation word in certain contexts, e.g. not to
mention or not just.
Shifter features are binary features checking the
presence of different types of polarity shifters. Po-
larity shifters, such as little, are weaker than ordi-
nary negation expressions. They can be grouped
into three categories, general polarity shifters,
positive polarity shifters, and negative polarity
shifters. General polarity shifters reverse polarity
like negations. The latter two types only reverse
a particular polarity type, e.g. the positive shifter
abate only modifies negative polar expressions as
in abate the damage. Thus, the presence of a pos-
itive shifter may indicate positive polarity. The set
of words that are denoted by these three features
can be approximately equated with diminishers.
Finally, polarity modification features describe
polar expressions of a particular type modify-
ing or being modified by other polar expressions.
Though these features do not explicitly contain
negations, language constructions which are sim-
ilar to negation may be captured. In the phrase
[disappointed? hope+]?, for instance, a negative
polar expression modifies a positive polar expres-
sion which results in an overall negative phrase.
Adding these three feature groups to a feature
set comprising bag of words and features count-
ing polar expressions results in a significant im-
provement. In (Wilson et al, 2009), the experi-
ments of Wilson et al (2005) are extended by a
detailed analysis on the individual effectiveness of
the three feature groups mentioned above. The re-
sults averaged over four different supervised learn-
ing algorithms suggest that the actual negation fea-
tures are most effective whereas the binary polar-
ity shifters have the smallest impact. This is con-
sistent with Kennedy and Inkpen (2005) given the
similarity of polarity shifters and diminishers.
Considering the amount of improvement that is
achieved by negation modeling, the improvement
seems to be larger in (Wilson et al, 2005). There
might be two explanations for this. Firstly, the
negation modeling in (Wilson et al, 2005) is con-
siderably more complex and, secondly, Wilson et
al. (2005) evaluate on a more fine-grained level
(i.e. expression level) than Kennedy and Inkpen
(2005) (they evaluate on document level). As al-
ready pointed out in ?3.1, document-level polar-
ity classification contains more redundant infor-
mation than sentence-level or expression-level po-
larity classification, therefore complex negation
modeling on these levels might be more effective
since the correct contextual interpretation of an in-
dividual polar expression is far more important2.
The fine-grained opinion corpus used in (Wilson
et al, 2005; Wilson et al, 2009) and all the re-
sources necessary to replicate the features used in
these experiments are also publicly available3.
3.3 Other Approaches
The approaches presented in the previous sec-
tion (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2005; Wilson et al, 2005) can be consid-
ered as the works pioneering negation modeling
in sentiment analysis. We now present some more
recent work on that topic. All these approaches,
however, are heavily related to these early works.
2This should also explain why most subsequent works
(see ?3.3) have been evaluated on fine-grained levels.
3The corpus is available under:
http://www.cs.pitt.edu/mpqa/
databaserelease and the resources
for the features are part of OpinionFinder:
http://www.cs.pitt.edu/mpqa/
opinionfinderrelease
63
3.3.1 Semantic Composition
In (Moilanen and Pulman, 2007), a method to
compute the polarity of headlines and complex
noun phrases using compositional semantics is
presented. The paper argues that the principles of
this linguistic modeling paradigm can be success-
fully applied to determine the subsentential polar-
ity of the sentiment expressed, demonstrating it
through its application to contexts involving senti-
ment propagation, polarity reversal (e.g. through
the use of negation following Polanyi and Zae-
nen (2004) and Kennedy and Inkpen (2005)) or
polarity conflict resolution. The goal is achieved
through the use of syntactic representations of sen-
tences, on which rules for composition are de-
fined, accounting for negation (incrementally ap-
plied to constituents depending on the scope) us-
ing negation words, shifters and negative polar ex-
pressions. The latter are subdivided into differ-
ent categories, such that special words are defined,
whose negative intensity is strong enough that they
have the power to change the polarity of the entire
text spans or constituents they are part of.
A similar approach is presented by Shaikh et al
(2007). The main difference to Moilanen and
Pulman (2007) lies in the representation format
on which the compositional model is applied.
While Moilanen and Pulman (2007) use syntac-
tic phrase structure trees, Shaikh et al (2007) con-
sider a more abstract level of representation be-
ing verb frames. The advantage of a more abstract
level of representation is that it more accurately
represents the meaning of the text it describes.
Apart from that, Shaikh et al (2007) design a
model for sentence-level classification rather than
for headlines or complex noun phrases.
The approach by Moilanen and Pulman (2007) is
not compared against another established classifi-
cation method whereas the approach by Shaikh et
al. (2007) is evaluated against a non-compositional
rule-based system which it outperforms.
3.3.2 Shallow Semantic Composition
Choi and Cardie (2008) present a more lightweight
approach using compositional semantics towards
classifying the polarity of expressions. Their
working assumption is that the polarity of a phrase
can be computed in two steps:
? the assessment of polarity of the constituents
? the subsequent application of a set of previously-
defined inference rules
An example rule, such as:
Polarity([NP1]? [IN] [NP2]?) = + (3)
may be applied to expressions, such as
[lack]?NP1 [of]IN [crime]?NP2 in rural areas.
The advantage of these rules is that they restrict
the scope of negation to specific constituents
rather than using the scope of the entire target
expression.
Such inference rules are very reminiscent of
polarity modification features (Wilson et al,
2005), as a negative polar expression is modified
by positive polar expression. The rules presented
by Choi and Cardie (2008) are, however, much
more specific, as they define syntactic contexts of
the polar expressions. Moreover, from each con-
text a direct polarity for the entire expression can
be derived. In (Wilson et al, 2005), this decision
is left to the classifier. The rules are also similar
to the syntactic rules from Moilanen and Pulman
(2007). However, they involve less linguistic
processing and are easier to comprehend4 . The
effectiveness of these rules are both evaluated in
rule-based methods and a machine learning based
method where they are anchored as constraints
in the objective function. The results of their
evaluation show that the compositional methods
outperform methods using simpler scopes for
negation, such as considering the scope of the
entire target expression. The learning method
incorporating the rules also slightly outperforms
the (plain) rule-based method.
3.3.3 Scope Modeling
In sentiment analysis, the most prominent work
examining the impact of different scope models
for negation is (Jia et al, 2009). The scope de-
tection method that is proposed considers:
? static delimiters
? dynamic delimiters
? heuristic rules focused on polar expressions
Static delimiters are unambiguous words, such as
because or unless marking the beginning of an-
other clause. Dynamic delimiters are, however,
4It is probably due to the latter, that these rules have
been successfully re-used in subsequent works, most promi-
nently Klenner et al (2009).
64
ambiguous, e.g. like and for, and require disam-
biguation rules, using contextual information such
as their pertaining part-of-speech tag. These de-
limiters suitably account for various complex sen-
tence types so that only the clause containing the
negation is considered.
The heuristic rules focus on cases in which po-
lar expressions in specific syntactic configurations
are directly preceded by negation words which re-
sults in the polar expression becoming a delimiter
itself. Unlike Choi and Cardie (2008), these rules
require a proper parse and reflect grammatical re-
lationships between different constituents.
The complexity of the scope model proposed
by Jia et al (2009) is similar to the ones of
the compositional models (Moilanen and Pulman,
2007; Shaikh et al, 2007; Choi and Cardie, 2008)
where scope modeling is exclusively incorporated
in the compositional rules.
Apart from scope modeling, Jia et al (2009) also
employ a complex negation term disambiguation
considering not only phrases in which potential
negation expressions do not have an actual negat-
ing function (as already used in (Wilson et al,
2005)), but also negative rhetorical questions and
restricted comparative sentences.
On sentence-level polarity classification, their
scope model is compared with
? a simple negation scope using a fixed window size
(similar to the negation feature in (Wilson et al, 2005))
? the text span until the first occurrence of a polar expres-
sion following the negation word
? the entire sentence
The proposed method consistently outperforms
the simpler methods proving that the incorpora-
tion of linguistic insights into negation modeling
is meaningful. Even on polarity document re-
trieval, i.e. a more coarse-grained classification
task where contextual disambiguation usually
results in a less significant improvement, the
proposed method also outperforms the other
scopes examined.
There have only been few research efforts in
sentiment analysis examining the impact of scope
modeling for negation in contrast to other research
areas, such as the biomedical domain (Huang and
Lowe, 2007; Morante et al, 2008; Morante and
Daelemans, 2009). This is presumably due to the
fact that only for the biomedical domain, publicly
available corpora containing annotation for the
scope of negation exist (Szarvas et al, 2008). The
usability of those corpora for sentiment analysis
has not been tested.
3.4 Negation within Words
So far, negation has only be considered as a phe-
nomenon that affects entire words or phrases.
The word expressing a negation and the words
or phrases being negated are disjoint. There are,
however, cases in which both negation and the
negated content which can also be opinionated
are part of the same word. In case, these words
are lexicalized, such as flaw-less, and are conse-
quently to be found a polarity lexicon, this phe-
nomenon does not need to be accounted for in sen-
timent analysis. However, since this process is (at
least theoretically) productive, fairly uncommon
words, such as not-so-nice, anti-war or offensive-
less which are not necessarily contained in lexical
resources, may emerge as a result of this process.
Therefore, a polarity classifier should also be able
to decompose words and carry out negation mod-
eling within words.
There are only few works addressing this particu-
lar aspect (Moilanen and Pulman, 2008; Ku et al,
2009) so it is not clear how much impact this type
of negation has on an overall polarity classification
and what complexity of morphological analysis is
really necessary. We argue, however, that in syn-
thetic languages where negation may regularly be
realized as an affix rather than an individual word,
such an analysis is much more important.
3.5 Negation in Various Languages
Current research in sentiment analysis mainly fo-
cuses on English texts. Since there are signifi-
cant structural differences among the different lan-
guages, some particular methods may only cap-
ture the idiosyncratic properties of the English lan-
guage. This may also affect negation modeling.
The previous section already stated that the need
for morphological analyses may differ across the
different languages.
Moreover, the complexity of scope modeling may
also be language dependent. In English, for ex-
ample, modeling the scope of a negation as a
fixed window size of words following the oc-
currence of a negation expression already yields
a reasonable performance (Kennedy and Inkpen,
2005). However, in other languages, for example
German, more complex processing is required as
the negated expression may either precede (Sen-
65
tence 10) or follow (Sentence 11) the negation ex-
pression. Syntactic properties of the negated noun
phrase (i.e. the fact whether the negated polar ex-
pression is a verb or an adjective) determine the
particular negation construction.
10. Peter mag den Kuchen nicht.
Peter likes the cake not.
?Peter does not like the cake.?
11. Der Kuchen ist nicht ko?stlich.
The cake is not delicious.
?The cake is not delicious.?
These items show that, clearly, some more ex-
tensive cross-lingual examination is required in or-
der to be able to make statements of the general
applicability of specific negation models.
3.6 Bad and Not Good are Not the Same
The standard approach of negation modeling sug-
gests to consider a negated polar expression, such
as not bad, as an unnegated polar expression with
the opposite polarity, such as good. Liu and Seneff
(2009) claim, however, that this is an oversimpli-
fication of language. Not bad and good may have
the same polarity but they differ in their respec-
tive polar strength, i.e. not bad is less positive
than good. That is why, Liu and Seneff (2009)
suggest a compositional model in which for indi-
vidual adjectives and adverbs (the latter include
negations) a prior rating score encoding their in-
tensity and polarity is estimated from pros and
cons of on-line reviews. Moreover, compositional
rules for polar phrases, such as adverb-adjective or
negation-adverb-adjective are defined exclusively
using the scores of the individual words. Thus,
adverbs function like universal quantifiers scaling
either up or down the polar strength of the specific
polar adjectives they modify. The model indepen-
dently learns what negations are, i.e. a subset of
adverbs having stronger negative scores than other
adverbs. In short, the proposed model provides
a unifying account for intensifiers (e.g. very), di-
minishers, polarity shifters and negation words. Its
advantage is that polarity is treated composition-
ally and is interpreted as a continuum rather than
a binary classification. This approach reflects its
meaning in a more suitable manner.
3.7 Using Negations in Lexicon Induction
Many classification approaches illustrated above
depend on the knowledge of which natural lan-
guage expressions are polar. The process of ac-
quiring such lexical resources is called lexicon in-
duction. The observation that negations co-occur
with polar expressions has been used for inducing
polarity lexicons on Chinese in an unsupervised
manner (Zagibalov and Carroll, 2008). One ad-
vantage of negation is that though the induction
starts with just positive polar seeds, the method
also accomplishes to extract negative polar expres-
sions since negated mentions of the positive po-
lar seeds co-occur with negative polar expressions.
Moreover, and more importantly, the distribution
of the co-occurrence between polar expressions
and negations can be exploited for the selection of
those seed lexical items. The model presented by
Zagibalov and Carroll (2008) relies on the obser-
vation that a polar expression can be negated but it
occurs more frequently without the negation. The
distributional behaviour of an expression, i.e. sig-
nificantly often co-occurring with a negation word
but significantly more often occurring without a
negation word makes up a property of a polar ex-
pression. The data used for these experiments are
publicly available5 .
3.8 Irony ? The Big Challenge
Irony is a rhetorical process of intentionally using
words or expressions for uttering meaning that is
different from the one they have when used liter-
ally (Carvalho et al, 2009). Thus, we consider
that the use of irony can reflect an implicit nega-
tion of what is conveyed through the literal use of
the words. Moreover, due to its nature irony is
mostly used to express a polar opinion.
Carvalho et al (2009) confirm the relevance of
(verbal) irony for sentiment analysis by an error
analysis of their present classifier stating that a
large proportion of misclassifications derive from
their system?s inability to account for irony.
They present predictive features for detecting
irony in positive sentences (which are actually
meant to have a negative meaning). Their find-
ings are that the use of emoticons or expressions
of gestures and the use of quotation marks within
a context in which no reported speech is included
are a good signal of irony in written text. Although
the use of these clues in the defined patterns helps
to detect some situations in which irony is present,
they do not fully represent the phenomenon.
5http://www.informatics.sussex.ac.uk/
users/tz21/coling08.zip
66
A data-driven approach for irony detection on
product-reviews is presented in (Tsur et al, 2010).
In the first stage, a considerably large list of simple
surface patterns of ironic expressions are induced
from a small set of labeled seed sentences. A pat-
tern is a generalized word sequence in which con-
tent words are replaced by a generic CW symbol.
In the second stage, the seed sentences are used to
collect more examples from the web, relying on
the assumption that sentences next to ironic ones
are also ironic. In addition to these patterns, some
punctuation-based features are derived from the
labeled sentences. The acquired patterns are used
as features along the punctuation-based features
within a k nearest neighbour classifier. On an in-
domain test set the classifier achieves a reasonable
performance. Unfortunately, these experiments
only elicit few additional insights into the general
nature of irony. As there is no cross-domain eval-
uation of the system, it is unclear in how far this
approach generalizes to other domains.
4 Limits of Negation Modeling in
Sentiment Analysis
So far, this paper has not only outlined the impor-
tance of negation modeling in sentiment analysis
but it has also shown different ways to account for
this linguistic phenomenon. In this section, we
present the limits of negation modeling in senti-
ment analysis.
Earlier in this paper, we stated that negation mod-
eling depends on the knowledge of polar expres-
sions. However, the recognition of genuine polar
expressions is still fairly brittle. Many polar ex-
pressions, such as disease are ambiguous, i.e. they
have a polar meaning in one context (Sentence 12)
but do not have one in another (Sentence 13).
12. He is a disease to every team he has gone to.
13. Early symptoms of the disease are headaches, fevers,
cold chills and body pain.
In a pilot study (Akkaya et al, 2009), it has al-
ready been shown that applying subjectivity word
sense disambiguation in addition to the feature-
based negation modeling approach of Wilson et al
(2005) results in an improvement of performance
in polarity classification.
Another problem is that some polar opinions are
not lexicalized. Sentence 14 is a negative prag-
matic opinion (Somasundaran and Wiebe, 2009)
which can only be detected with the help of exter-
nal world knowledge.
14. The next time I hear this song on the radio, I?ll throw
my radio out of the window.
Moreover, the effectiveness of specific negation
models can only be proven with the help of cor-
pora containing those constructions or the type of
language behaviour that is reflected in the mod-
els to be evaluated. This presumably explains why
rare constructions, such as negations using con-
nectives (Sentence 6 in ?2), modals (Sentence 7
in ?2) or other phenomena presented in the con-
ceptual model of Polanyi and Zaenen (2004), have
not yet been dealt with.
5 Conclusion
In this paper, we have presented a survey on
the role of negation in sentiment analysis. The
plethora of work presented on the topic proves that
this common linguistic construction is highly rel-
evant for sentiment analysis.
An effective negation model for sentiment analy-
sis usually requires the knowledge of polar expres-
sions. Negation is not only conveyed by common
negation words but also other lexical units, such as
diminishers. Negation expressions are ambiguous,
i.e. in some contexts do not function as a nega-
tion and, therefore, need to be disambiguated. A
negation does not negate every word in a sentence,
therefore, using syntactic knowledge to model the
scope of negation expressions is useful.
Despite the existence of several approaches to
negation modeling for sentiment analysis, in or-
der to make general statements about the effective-
ness of specific methods systematic comparative
analyses examining the impact of different nega-
tion models (varying in complexity) with regard to
classification type, text granularity, target domain,
language etc. still need to be carried out.
Finally, negation modeling is only one aspect that
needs to be taken into consideration in sentiment
analysis. In order to fully master this task, other
aspects, such as a more reliable identification of
genuine polar expressions in specific contexts, are
at least as important as negation modeling.
Acknowledgements
Michael Wiegand was funded by the BMBF project NL-
Search under contract number 01IS08020B. Alexandra Bal-
ahur was funded by Ministerio de Ciencia e Innovacio?n -
Spanish Government (grant no. TIN2009-13391-C04-01),
and Conselleria d?Educacio?n-Generalitat Valenciana (grant
no. PROMETEO/2009/119 and ACOMP/2010/286).
67
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjec-
tivity Word Sense Disambiguation. In Proceedings
of EMNLP.
P. Carvalho, L. Sarmento, M. J. Silva, and
E. de Oliveira. 2009. Clues for Detecting
Irony in User-Generated Contents: Oh...!! It?s ?so
easy? ;-). In Proceedings of CIKM-Workshop TSA.
Y. Choi and C. Cardie. 2008. Learning with Compo-
sitional Semantics as Structural Inference for Sub-
sentential Sentiment Analysis. In Proceedings of
EMNLP.
Y. Huang and H. J. Lowe. 2007. A Novel Hybrid Ap-
proach to Automated Negation Detection in Clinical
Radiology Reports. JAMIA, 14.
L. Jia, C. Yu, and W. Meng. 2009. The Effect of Nega-
tion on Sentiment Analysis and Retrieval Effective-
ness. In Proceedings of CIKM.
A. Kennedy and D. Inkpen. 2005. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. In Proceedings of FINEXIN.
A. Kennedy and D. Inkpen. 2006. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. Computational Intelligence, 22.
M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust
Compositional Polarity Classification. In Proceed-
ings of RANLP.
L. Ku, T. Huang, and H. Chen. 2009. Using Morpho-
logical and Syntactic Structures for Chinese Opinion
Analysis. In Proceedings ACL/IJCNLP.
J. Liu and S. Seneff. 2009. Review Sentiment Scoring
via a Parse-and-Paraphrase Paradigm. In Proceed-
ings of EMNLP.
K. Moilanen and S. Pulman. 2007. Sentiment Con-
struction. In Proceedings of RANLP.
K. Moilanen and S. Pulman. 2008. The Good, the Bad,
and the Unknown. In Proceedings of ACL/HLT.
R. Morante and W. Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the Scope of Negation in Biomedical
Texts. In Proceedings of EMNLP.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of COLING/ACL.
B. Pang and L. Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Context Valence
Shifters. In Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
M. A. M. Shaikh, H. Prendinger, and M. Ishizuka.
2007. Assessing Sentiment of Text by Semantic De-
pendency and Contextual Valence Analysis. In Pro-
ceedings of ACII.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing Stances in Online Debates. In Proceedings of
ACL/IJCNLP.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope Corpus: Annotation for Negation,
Uncertainty and Their Scope in Biomedical Texts.
In Proceedings of BioNLP.
O. Tsur, D. Davidov, and A. Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceeding of ICWSM.
J. Wiebe. 1994. Tracking Point of View in Narrative.
Computational Linguistics, 20.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of HLT/EMNLP.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing Contextual Polarity: An Exploration for
Phrase-level Analysis. Computational Linguistics,
35:3.
T. Zagibalov and J. Carroll. 2008. Automatic Seed
Word Selection for Unsupervised Sentiment Classi-
fication of Chinese Text. In Proceedings of COL-
ING.
68
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 53?60,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Detecting Implicit Expressions of Sentiment in Text  
Based on Commonsense Knowledge 
 
 
Alexandra Balahur, Jes?s M. Hermida, Andr?s Montoyo 
Department of Software and Computing Systems  
University of Alicante 
Apartado de correos 99, E-03080 Alicante, Spain 
{abalahur, jhermida, montoyo}@dlsi.ua.es 
 
 
 
 
 
 
Abstract 
Sentiment analysis is one of the recent, 
highly dynamic fields in Natural Language 
Processing. Most existing approaches are 
based on word-level analysis of texts and 
are able to detect only explicit expressions 
of sentiment.  In this paper, we present an 
approach towards automatically detecting 
emotions (as underlying components of 
sentiment) from contexts in which no clues 
of sentiment appear, based on 
commonsense knowledge. The resource we 
built towards this aim ? EmotiNet - is a 
knowledge base of concepts with 
associated affective value. Preliminary 
evaluations show that this approach is 
appropriate for the task of implicit emotion 
detection, thus improving the performance 
of sentiment detection and classification in 
text. 
1 Introduction 
Research in affect has a long established tradition 
in many sciences - linguistics, psychology, socio-
psychology, cognitive science, pragmatics, 
marketing or communication science. Recently, 
many closely related subtasks were developed also 
in the field of Natural Language Proceesing (NLP), 
such as emotion detection, subjectivity analysis, 
opinion mining to sentiment analysis, attitude and 
appraisal analysis or review mining (Pang and Lee, 
2008). 
Among these tasks, sentiment analysis aims at 
detecting the expressions of sentiment in text and 
subsequently classify them, according to their 
polarity (semantic orientation) among different 
categories (usually, among positive and negative). 
The problem is defined by Pang and Lee (2008) as 
?the binary classification task of labeling an 
opinionated document as expressing either an 
overall positive or an overall negative.? (Pang and 
Lee, 2008) 
According to the Webster dictionary 
(http://www.merriam-webster.com/), sentiment suggests 
a settled opinion reflective of one?s feelings, where 
the term feeling is defined as the conscious 
subjective experience of emotion. (Van den Bos, 
2006), ?a single component of emotion, denoting 
the subjective experience process? (Scherer, 2005).  
Most of the research performed in the field of 
sentiment analysis has aimed at detecting explicit 
expressions of sentiment (i.e. situations where 
specific words or word combinations are found in 
texts). Nevertheless, the expression of emotion is 
most of the times not achieved through the use of 
emotion-bearing words (Pennebaker et al, 2003), 
but indirectly, by presenting situations that based 
on commonsense knowledge can be interpreted in 
an affective manner (Balahur and Montoyo, 2008; 
Balahur and Steinberger, 2009).  
In this paper, we present a method to build a 
commonsense knowledge base (EmotiNet) 
representing situations that trigger emotions. We 
demonstrate that by using this resource, we are 
53
able to detect emotion from textual contexts in 
which no explicit mention of affect is present.   
2 State of the Art 
In Artificial Intelligence (AI), the term affective 
computing was first introduced by Picard (1995). 
Previous approaches to spot affect in text include 
the use of models simulating human reactions 
according to their needs and desires (Dyer, 1987), 
fuzzy logic (Subasic and Huettner, 2000), lexical 
affinity based on similarity of contexts ? the basis 
for the construction of WordNet Affect  
(Strapparava and Valitutti, 2004) or SentiWord-
Net (Esuli and Sebastiani, 2005), detection of 
affective keywords (Riloff et al, 2003) and 
machine learning using term frequency (Pang et 
al., 2002; Riloff and Wiebe, 2003), or term 
discrimination (Danisman and Alpkocak, 2008). 
Other proposed methods include the creation of 
syntactic patterns and rules for cause-effect 
modeling (Mei Lee et al, 2009). Significantly 
different proposals for emotion detection in text 
are given in the work by (Liu et al 2003) and the 
recently proposed framework of sentic computing 
(Cambria et al, 2009), whose scope is to model 
affective reaction based on commonsense 
knowledge. For a survey on the affect models and 
their affective computing applications, see (Calvo 
and D?Mello, 2010).  
3 Motivation and Contribution 
The tasks of emotion detection and sentiment 
analysis have been approached by a large volume 
of research in NLP . Nevertheless, most of this 
research has concentrated on developing methods 
for detecting only explicit mentions of sentiment in 
text. Therefore, sentences such as ?I?m going to a 
party?, which express an underlying emotion, 
cannot be classified by most of the existing 
approaches. A method to overcome this issue is 
proposed in by sentic computing (Cambria et al, 
2009) and by (Liu et al 2003), whose main idea is 
acquiring knowledge on the emotional effect of 
different concepts. In this manner, the system 
would know that ?going to a party? is something 
that produces ?joy?. However, more complex 
contexts, such as ?I?m going to a party, although I 
should study for my exam.?, where the emotion 
expressed is most probably ?guilt?, cannot be 
correctly detected and classified by present 
systems. 
In the light of these considerations, our 
contribution relies in proposing and implementing 
a framework for modeling affect based on the 
appraisal theories, which can support the automatic 
processing of texts to extract: 
? The components of the situation presented 
(which we denote by ?action chains?) and 
their relation (temporal, causal etc.) 
? The elements on which the appraisal is 
done in each action of the chain (agent, 
action, object); 
? The appraisal criteria that can 
automatically be determined from the text 
(modifiers of the action, actor, object in 
each action chain); 
4 Modeling Affective Reaction Using 
Commonsense Knowledge  
Our main idea is that emotion can be expressed in 
text by presenting a sequence of actions (situations 
in which different concepts appear), which, based 
on commonsense knowledge and previous 
experiences, trigger an emotional reaction. This 
idea is linked to the Appraisal Theories, which 
claim that emotions are elicited and differentiated 
on the basis of the subjective evaluation of the 
personal significance of a situation, object or event 
(De Rivera, 1977; Frijda, 1986; Johnson-Laird and 
Oatley, 1989 ? among others). Viewed in a simpler 
manner, a situation is presented as a chain of 
actions, each with an actor and an object; the 
appraisal depends on the temporal and causal 
relationship between them, on the characteristics of 
the actors involved in the action and on the object 
of the action.  
Given this insight, the general idea behind our 
approach is to model situations as chains of actions 
and their corresponding emotional effect using an 
ontological representation. According to the 
definition provided by Studer et al (1998), an 
ontology captures knowledge shared by a 
community that can be easily sharable with other 
communities. These two characteristics are 
especially relevant if we want the recall of our 
approach to be increased. Knowledge managed in 
our approach has to be shared by a large 
community and it also needs to be fed by 
heterogeneous sources of common knowledge to 
54
avoid uncertainties. However, specific assertions 
can be introduced to account for the specificities of 
individuals or contexts. In this manner, we can 
model the interaction of different events in the 
context in which they take place. 
5 Building a Knowledge Base for 
Detecting Implicit Expressions of 
Emotion 
In order to build a resource that is capable of 
capturing emotional reaction to real-world 
situations in which commonsense knowledge plays 
a significant role in the affective interpretation, we 
aim at representing chains of actions and their 
corresponding emotional labels from several 
situations in such a way that we will be able to 
extract general patterns of appraisal. Our approach 
defines an action chain as a sequence of action 
links, or simply actions that trigger an emotion on 
an actor. Each specific action link can be described 
with a tuple (actor, action type, patient, emotional 
reaction). 
In order to manage and store action chains, the 
approach we propose defines a new knowledge 
base, called EmotiNet, which aims to be a resource 
for detecting emotions in text, and a 
(semi)automatic, iterative process to build it, which 
is based on existing knowledge from different 
sources. This process extracts the action chains 
from a set of documents and adds them to the KB. 
Specifically, EmotiNet was built by following the 
next steps: 
1. The design of an ontology, which contains 
the definitions of the main concepts of the 
domain.  
2. The extension and population of this 
ontology using the situations stored in the 
ISEAR International Survey of Emotional 
Antecedents and Reactions (ISEAR, 
http://www.unige.ch/fapse/emotion/databanks/isear.
html) ? (Scherer and Wallbott, 1997) 
database. 
3.  The expansion of the ontology using 
existing commonsense knowledge bases ? 
ConceptNet (Liu and Singh, 2004) and 
other resources ? VerbOcean (Chklovski 
and Pantel, 2004). 
5.1 Design of the Ontology 
As mentioned before, the process of building the 
core of the EmotiNet knowledge base (KB) of 
action chains started with the design of the core 
ontology, whose design process was specifically 
divided in three stages:  
1. Establishing the scope and purpose of the 
ontology. The EmotiNet ontology needs to capture 
and manage knowledge from three domains: 
kinship membership, emotions (and their relations) 
and actions (characteristics and relations between 
them).   
2. Reusing knowledge from existing ontologies. 
In a second stage, we searched for other ontologies 
on the Web containing concepts related to the 
knowledge cores we specified. At the end of the 
process, we located two ontologies that are reused 
in our ontological representation: the ReiAction 
ontology (www.cs.umbc.edu/~lkagal1/rei 
/ontologies/ReiAction.owl), which represents actions 
between entities in a general manner, and the 
family ontology (www.dlsi.ua.es/~jesusmhc/emotinet 
/family.owl), which contains knowledge about 
family members and the relations between them.  
anger
fea
r
surprise
joy
sadness
shame
guilt
basicEmotion
basicEmotion
oppositeEmotion
anticipation
disgust
trust
oppositeEmotion
oppositeEmotion
op
po
sit
eE
m
ot
io
n
optimism
hasEmotion
vigilance
hasHigherIntensity
Emotion
CompositeEmotion
hasEmotion
oppositeEmotion
hasHigherIntensity
basicEmotion
rdfs:subClassOf
rdf:type
 
 
Figure 1. Partial RDF graph of the Emotion Ontology. 
 
3. Creating the final knowledge core from the 
ontologies imported. This third stage involved the 
design of the last remaining core, i.e. emotion, and 
the combination of the different knowledge sources 
into a single ontology: EmotiNet. In order to 
describe the emotions and the way they relate and 
compose, we employ Robert Plutchik?s wheel of 
emotion (Plutchik, 2001) and Parrot?s tree-
55
structured list of emotions (Parrot, 2001). These 
models contain an explicit modeling of the 
relations between the different emotions. At the 
end of the design process, the knowledge core 
included different types of relations between 
emotions and a collection of specific instances of 
emotion (e.g. anger, joy). In the last step, these 
three cores were combined using new classes and 
relations between the existing members of these 
ontologies (Fig. 2). 
Emotion
Person
Action
SimpleAction
DomainAction
Feel
Forget
ArgueCrash
emotionFelt
?
Agent
Object
rdfs:subClassOf
rdfs:subClassOf
actor
target
rdfs:subClassOf
rdfs:subClassOf
Modifier
isAffectedBy
rdfs:subClassOf
implyEmotion
 
Figure 2. Main concepts of EmotiNet. 
5.2 Extension and Population of the Ontology 
In order to have a homogenous starting base, we 
selected from the 7667 examples in the ISEAR 
database only the 1081 cases that contained 
descriptions of situations between family members. 
Subsequently, the examples were POS-tagged 
using TreeTagger. Within each emotion class, we 
then computed the similarity of the examples with 
one another, using the implementation of the Lesk 
distance in Ted Pedersen?s Similarity Package. 
This score was used to split the examples in each 
emotion class into six clusters using the Simple K-
Means implementation in Weka. The idea behind 
this approach, confirmed by the output of the 
clusters, was to group examples that are similar, in 
vocabulary and structure. From this collection, we 
manually selected a subset of 175 documents with 
25 expressions related to each of the emotions: 
anger, disgust, guilt, fear, sadness, joy and shame. 
The criteria for choosing this subset were the 
simplicity of the sentences and the variety of 
actions described. 
The next step was to extract the actions chains 
described in each of the examples. For this, we 
employed Semrol, the semantic role labeling (SRL) 
system introduced by Moreda et al (2007). For the 
core of knowledge in the EmotiNet KB, we need 
100% accurate information. Therefore, we 
manually extract the agent, the verb and the patient 
(the surface object of the verb) from the output of 
Semrol. For example, if we use the input sentence 
?I?m going to a family party because my mother 
obliges me to?, the system extracts two triples with 
the main actors of the sentences: (I, go, family 
party) and (mother, oblige, me), related by the 
causal adverb ?because?.  
Further on, we resolve the anaphoric expressions 
automatically, using a heuristic selection of the 
family member mentioned in the text that is closest 
to the anaphoric reference and whose properties 
(gender, number) are compatible with the ones of 
the reference. The replacement of the references to 
the speaker, e.g. ?I?, ?me?, ?myself?, is resolved by 
taking into consideration the entities mentioned in 
the sentence. In case of ambiguity, we choose the 
youngest, female member. Following the last 
example, the subject of the action would be 
assigned to the daughter of the family and the 
triples would be updated: (daughter, go, 
family_party) and (mother, oblige, daughter). 
Finally, the action links (triplets) are grouped and 
sorted in action chains. This process of sorting is 
determined by the adverbial expressions that 
appear within the sentence, which actually specify 
the position of each action on a temporal line (e.g. 
?although? ?because?, ?when?). We defined 
pattern rules according to which the actions 
introduced by these modifiers happen prior to or 
after the current context.   
Using our combined emotion model as a reference, 
we manually assigned one of the seven most basic 
emotions, i.e. anger, fear, disgust, shame, sadness, 
joy or guilt, or the neutral value to all the action 
links obtained, thus generating 4-tuples (subject, 
action, object, emotion), e.g. (daughter, go, family 
party, neutral) or (mother, oblige, daughter, 
disgust).  
Once we carried out these processes on the chosen 
documents, we obtained 175 action chains (ordered 
lists of tuples). In order to be included in the 
EmotiNet knowledge base, all their action links 
needed to be mapped to existing concepts or 
instances within the KB. When these did not exist, 
they were added to it. We would like to highlight 
that in EmotiNet, each tuple (actor, action, patient, 
emotion) extracted has its own representation as an 
instance of the subclasses of Action. Each in-stance 
56
of Action is related to an instance of the class Feel, 
which represents the emotion felt in this action. 
Subsequently, these instances (action links) were 
grouped in sequences of actions (class Sequence) 
ended by an instance of the class Feel, which 
determine the final emotion felt by the main 
actor(s) of the chain.  
In our example, we created two new classes Go 
and Oblige (subclasses of DomainAction) and two 
new instances of them: instance act1 (?Go?, 
?daughter?, ?family_party?, ?Neutral?); and 
instance act2 (?Oblige?, ?mother?, ?daughter?, 
?Angry?). The last action link already existed 
within EmotiNet from another chain so we reused 
it: instance act3 (?Feel?, ?daughter?, ?anger?). The 
next step consisted in grouping these instances into 
sequences by means of instances of the class 
Sequence, which is a subclass of Action that can 
establish the temporal order between two actions 
(which one occurred first). Fig. 3 shows an 
example of a RDF graph with the action chain of 
our example. We used Jena 
(http://jena.sourceforge.net/) and MySQL for the 
management and storage of EmotiNet on a 
database.  
hasChild
feel_anger_1
go_1
oblige_1
sequence_1
sequence_2
emotionFelt
actor actor
actor
Action Chain
target
target
anger
second
second
first
first
mother_f1
daughter_f1
disgust
implies
party_1
 
Figure 3. RDF graph of an action chain. 
5.3 Ontology Expansion 
In order to extend the coverage of the resource, we 
expanded the ontology with the actions and 
relations from VerbOcean. This process is essential 
for EmotiNet, since it adds new types of action and 
relations between actions, which might not have 
been analyzed before, thus reducing the degree of 
dependency between the resource and the initial set 
of examples. In particular, 299 new actions were 
automatically included as subclasses of 
DomainAction, which were directly related to any 
of the actions of our ontology through three new 
relations: can-result-in, happens-before and 
similar. 
6 Experiments and Evaluation 
The evaluation of our approach consists in testing 
if by employing the model we built and the 
knowledge contained in the core of EmotiNet 
(which we denote by ?knowledge sets?), we are 
able to detect the emotion expressed in new 
examples pertaining to the categories in ISEAR. 
Therefore, we use a test set (marked with B) that 
contains 895 examples (ISEAR phrases 
corresponding to the seven emotions modeled, 
from which core examples were removed).  
In order to assess the system performance on the 
two test sets, we followed the same process we 
used for building the core of EmotiNet, with the 
exception that the manual modeling of examples 
into tuples was replaced with the automatic 
extraction of (actor, verb, patient) triples from the 
output given by Semrol. Subsequently, we 
eliminated the stopwords in the phrases contained 
in these three roles and performed a simple corefe-
rence resolution. Next, we ordered the actions 
presented in the phrase, using the adverbs that 
connect the sentences, through the use of patterns 
(temporal, causal etc.). The resulted action chains 
for each of the examples in the two test sets will be 
used in carrying different experiments:  
 (1). In the first approach, for each of the situations 
in the test sets (represented now as action chains), 
we search the EmotiNet KB to encounter the 
sequences in which these actions in the chains are 
involved and their corresponding subjects. As a 
result of the search process, we obtain the emotion 
label corresponding to the new situation and the 
subject of the emotion based on a weighting 
function. This function takes into consideration the 
number of actions and the position in which they 
appear in the sequence contained in EmotiNet. The 
issue in this first approach is that many of the 
examples cannot be classified, as the knowledge 
they contain is not present in the ontology.  
(2). A subsequent approach aimed at surpassing the 
issues raised by the missing knowledge in 
EmotiNet. In a first approximation, we aimed at 
introducing extra knowledge from VerbOcean, by 
adding the verbs that were similar to the ones in 
57
the core examples (represented in VerbOcean 
through the ?similar? relation). Subsequently, each 
of the actions in the examples to be classified that 
was not already contained in EmotiNet, was sought 
in VerbOcean. In case one of the similar actions 
was already contained in the KB, the actions were 
considered equivalent. Further on, each action was 
associated with an emotion, using the ConceptNet 
relations and concepts (HasSubevent, Causes, 
ConceptuallyRelatedTo, HasPrerequisite). Finally, 
new examples were matched against chains of 
actions containing the same emotions, in the same 
order.  While more complete than the first 
approximation, this approach was also affected by 
lack of knowledge about the emotional content of 
actions. To overcome this issue, we proposed two 
heuristics: 
(2a) In the first one, actions on which no affect 
information was available, were sought in within 
the examples already introduced in the EmotiNet 
and were assigned the most frequent class of 
emotion labeling them. The corresponding results 
are marked with A2a and B2a, respectively. 
 (2b) In the second approximation, we used the 
most frequent emotion associated to the known 
links of a chain, whose individual emotions were 
obtained from ConceptNet.  In this case, the core 
of action chains is not involved in the process. The 
corresponding results are marked with A2b and 
B2b. 
We performed the steps described on test set B. 
The results are shown in Table 1 (results on 
classified examples) and Table 2 (results on all 
examples). 
 
Emotio
n 
Correct Total Accuracy 
B1 B2
a 
B2
b 
B1 B 
2a 
B2
b 
B1 B2a B2b 
disgust 16 16 21 44 42 40 
36.3
6 
38.0
9 
52.5
0 
shame 25 25 26 70 78 73 
35.7
1 
32.0
5 
35.6
2 
anger 31 47 57 
10
5 
11
5 121 
29.5
2 
40.8
6 
47.1
1 
fear 35 34 37 58 65 60 
60.3
4 
52.3
0 
61.6
7 
sadness 46 45 41 
11
1 
12
3 125 
41.4
4 
36.5
8 
32.8
0 
joy 13 16 18 25 29 35 52 
55.1
7 
51.4
3 
guilt 59 68 64 
15
8 
16
5 171 
37.3
4 
41.2
1 
37.4
3 
Total 22
5 251 264 
57
1 
61
7 625 
39.4
0 
40.6
8 
42.2
4 
Table 1. Results of the emotion detection using 
EmotiNet on classified examples in test set B 
 
Emotion Correct Total Recall 
B1 B2a B2b B1 B1 B2a B2b 
Disgust 16 16 21 59 27.11 27.11 35.59 
Shame 25 25 26 91 27.47 27.47 28.57 
Anger 31 47 57 145 21.37 32.41 39.31 
Fear 35 34 37 85 60.34 52.30 61.67 
Sadness 46 45 41 267 17.22 16.85 15.36 
Joy 13 16 18 50 26 32 36.00 
Guilt 59 68 64 198 29.79 34.34 32.32 
Total 225 251 264 895 25.13 28.04 29.50 
Baseline 126 126 126 895 14.0.7 14.07 14.07 
Table 2. Results of the emotion detection using 
EmotiNet on all test examples in test set B 
7 Discussion and conclusions 
From the results in Table 1 and 2, we can conclude 
that the approach is valid and represents a method 
that is appropriate for the detection of emotions 
from contexts where no affect-related words are 
present. Nonetheless, much remains to be done to 
fully exploit the capabilities of EmotiNet. We 
showed that the approach has a high degree of 
flexibility, i.e. new information can be easily 
introduced from existing common-sense 
knowledge bases, such as ConceptNet, mainly due 
to its internal structure and degree of granularity.  
The error analysis we performed shed some light 
on the causes of error of the system. The first 
finding is that extracting only the action, verb and 
patient semantic roles is not sufficient. There are 
other roles, such as the modifiers, which change 
the overall emotion in the text. Therefore, such 
modifiers should be included as attributes of the 
concepts identified in the roles. A further source of 
errors was that lack of knowledge on specific 
actions. Thus, the results of our approach can be 
practically limited by the structure, expressivity 
and degree of granularity of the imported 
resources. Therefore, to obtain the final, extended 
version of EmotiNet we should analyze the 
interactions between the core and the imported 
resources and among these re-sources as well. 
Finally, other errors were produced by NLP 
processes and propagated at various steps of the 
processing chain (e.g. SRL, coreference 
resolution). Some of these errors cannot be 
eliminated; however, others can be partially solved 
by using alternative NLP tools.  
Future work aims at extending the model by 
adding affective properties to the concepts 
58
included, so that more of the appraisal criteria can 
be introduced in the model, testing new methods to 
assign affective value to the concepts and adding 
new knowledge from sources such as CYC.  
Acknowledgments 
This paper has been supported by the Spanish 
Ministry of Science and Innovation (grant no. 
TIN2009-13391-C04-01), by the Spanish Ministry 
of Education under the FPU Program (AP2007-
03076), and by the Valencian Ministry of 
Education (grant no. PROMETEO/2009/119 and 
ACOMP/ 2010/288). 
References  
A. Balahur and A. Montoyo. 2008. Applying a Culture 
Dependent Emotion Triggers Database for Text 
Valence and Emotion Classification, proceedings of 
the AISB 2008 Convention ?Communication, 
Interaction and Social Intelligence?. 
A. Balahur and R. Steinberger. 2009. Rethinking 
Opinion Mining in Newspaper Articles: from Theory 
to Practice and Back, proceedings of the first work-
shop on Opinion Mining and Sentiment Analysis 
(WOMSA 2009). 
A. Esuli and F. Sebastiani. 2005. Determining the 
semantic orientation of terms through gloss analysis?, 
proceedings of CIKM 2005. 
B. Pang and L. Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, Vol 2, Nr. 1-2, 2008. 
B. Pang, L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learning 
techniques, proceedings of EMNLP-02.  
C. Strapparava and R. Mihalcea. 2007. Semeval 2007 
task 14: Affective text, proceedings of ACL 2007. 
E. Cambria, A. Hussain, C. Havasi and C. Eckl. 2009. 
Affective Space: Blending Common Sense and 
Affective Knowledge to Perform Emotive 
Reasoning, proceedings of the 1st Workshop on 
Opinion Mining and Sentiment Analysis (WOMSA). 
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions, proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing. 
E. Riloff, J. Wiebe and T. Wilson. 2003. Learning 
subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Conference on 
Natural Language Learning (CoNLL) 2003, pp.25-
32, Edmonton, Canada. 
G. Van den Bos. 2006. APA Dictionary of Psychology. 
Washington, DC: American Psychological 
Association. 
H. Liu and P. Singh. 2004. ConceptNet: A Practical 
Commonsense Reasoning Toolkit, BT Technology 
Journal, Volume 22, Kluwer Academic Publishers. 
H. Liu, H. Lieberman and T. Selker. 2003. A Model of 
Textual Affect Sensing Using Real-World Know-
ledge, proceedings of IUI 2003.  
J. De Rivera. 1977. A structural theory of the emotions, 
Psychological Issues, 10 (4), Monograph 40. 
J. W. Pennebaker, M. R. Mehl and K. Niederhoffer. 
2003. Psychological aspects of natural language use: 
Our words, our selves, Annual Review of Psychology 
54, 547-577. 
K. Scherer and H. Wallbott. 1997. The ISEAR 
Questionnaire and Codebook, Geneva Emotion Re-
search Group. 
K. Scherer, K. 2005. What are emotions? and how can 
they be measured? Social Science Information, 3(44), 
695-729. 
M. Dyer. 1987. Emotions and their computations: three 
computer models, Cognition and Emotion, 1, 323-
347. 
N. Frijda. 1986. The emotions, Cambridge University 
Press. 
P. Moreda, B. Navarro and M. Palomar. 2007. Corpus-
based semantic role approach in information 
retrieval, Data Knowl. Eng. (DKE) 61(3):467-483. 
P. N. Johnson-Laird and K. Oatley. 1989. The language 
of emotions: An analysis of a semantic field, 
Cognition and Emotion, 3, 81-123. 
P. Subasic and A. Huettner. 2000. Affect Analysis of 
text using fuzzy semantic typing, IEEE Trasactions 
on Fuzzy System, 9, 483-496.   
R. A. Calvo and S. D?Mello. 2010. Affect Detection: An 
Interdisciplinary Review of Models, Methods and 
Their Applications, IEEE Transactions on Affective 
Computing, Vol. 1, No. 1, Jan.-Jun.  
R. Picard. 1995. Affective computing, Technical re-
port, MIT Media Laboratory. 
R. Plutchik. 2001. The Nature of Emotions. American 
Scientist. 89, 344. 
R. Studer, R. V. Benjamins and D. Fensel. 1998. 
Knowledge engineering: Principles and methods, 
Data & Knowledge Engineering, 25(1-2):161?197. 
59
S. Y. Mei Lee, Y. Chen and C.-R. Huang. 2009. Cause 
Event Representations of Happiness and Surprise, 
proceedings of PACLIC 2009. 
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining 
the Web for Fine-Grained Semantic Verb Relations?, 
proceedings of EMNLP-04. 
T. Danisman and A. Alpkocak. 2008. Feeler: Emotion 
Classification of Text Using Vector Space Model, 
proceedings of the AISB 2008 Convention, ?Com-
munication, Interaction and Social Intelligence?.  
W. Parrott. 2001. Emotions in Social Psychology, 
Psychology Press, Philadelphia. 
 
60
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 139?145,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Sentiment Classification Using Semantic Features Extracted from 
WordNet-based Resources 
Yoan Guti?rrez 
Department of Informatics 
University of Matanzas, Cuba. 
{yoan.gutierrez}@umcc.cu 
Sonia V?zquez and Andr?s Montoyo 
Department of Software and Computing 
Systems 
 University of Alicante, Spain. 
{svazquez, montoyo}@dlsi.ua.es 
 
Abstract 
In this paper, we concentrate on the 3 of 
the tracks proposed in the NTCIR 8 
MOAT, concerning the classification of 
sentences according to their 
opinionatedness, relevance and polarity. 
We propose a method for the detection of 
opinions, relevance, and polarity 
classification, based on ISR-WN (a 
resource for the multidimensional analysis 
with Relevant Semantic Trees of sentences 
using different WordNet-based information 
sources). Based on the results obtained, we 
can conclude that the resource and methods 
we propose are appropriate for the task, 
reaching the level of state-of-the-art 
approaches. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge to 
extract useful and heterogeneous data. Texts can 
provide from factual information such as 
descriptions, lists of characteristics or instructions 
to opinionated information such as reviews, 
emotions or feelings. This heterogeneity has 
motivated that dealing with the identification and 
extraction of opinions and sentiments in texts 
require special attention. In fact, the development 
of different tools to help government information 
analysts, companies, political parties, economists, 
etc to automatically get feelings from news and 
forums is a challenging task (Wiebe et al, 2005). 
Many researchers such as Balahur et al, (2010), 
Hatzivassiloglou et al(2000), Kim and Hovy 
(2006), Wiebe et al (2005) and many others have 
been working in this way and  related areas. 
Moreover, in the course of years we find a long 
tradition on developing Question Answering (QA) 
systems. However, in recent years, researchers 
have concentrated on the development of Opinion 
Questions Answering (OQA) systems (Balahur et 
al., 2010). This new task has to deal with different 
problems such as Sentiment Analysis where 
documents must be classified according to 
sentiments and subjectivity features. Therefore, a 
new kind of evaluation that takes into account this 
new issue is needed.  
One of the competitions that establishes the 
benchmark for opinion question answering 
systems, in a monolingual and cross-lingual 
setting, is the NTCIR Multilingual Opinion 
Analysis Task (MOAT) 1 . In this competition,  
researchers work hard to achieve better results on 
Opinion Analysis, introducing different 
techniques.  
In this paper, we only concentrate on three 
tracks proposed in the NTCIR 8 MOAT, 
concerning to the classification of sentences 
according to their opinionatedness, relevance and 
polarity. We propose a method for the detection of 
opinions, relevance and polarity classification, 
based on ISR-WN which is a resource for the 
multidimensional analysis with Relevant Semantic 
Trees of sentences using different WordNet-based 
information sources. 
2 Related works 
Related to Opinion Analysis task we can find 
many points of view. Some researchers say that 
adjectives combined with semantic characteristics 
provide vital information to the performance of 
Opinion Analysis (Hatzivassiloglou et al, 2000). 
Others like Zubaryeva and Savoy (2010) assume 
                                                 
1http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
139
that the extraction of relevant terms on the 
documents could define their polarity, designing a 
method capable of selecting terms that clearly 
belong to one type of polarity. Another research 
based on features extraction was conducted by Lai 
et al (2010), they developed a trained system on 
Japanese Opinionated Sentence Identification. And 
Balahur and Montoyo (2009) proposed a method to 
extract, classify and summarize opinions on 
products from web reviews. It was based on the 
prior building of product characteristics taxonomy 
and on the semantic relatedness given by the 
Normalized Google Distance (Cilibrasi and 
Vit?nyi, 2007) and SVM learning. As we can see, 
the usage of features extraction is a suitable mode 
to work on Opinion Analysis task. Apart from that 
other authors have used semantic resources, for 
example, Kim and Hovy (2006, 2005) used 
semantic resources to get an approach on Holder 
Detection and Opinion Extraction tasks. 
In general, using semantic resources is one of 
the most applied procedures over different tasks 
such as Document Indexing, Document 
Classification, Word Sense Disambiguation, etc. In 
Natural Language Processing (NLP), one of the 
most used resources for WSD and other tasks is 
WordNet (WN) (Fellbaum, 1998). WN is a lexical 
dictionary with word senses and descriptions. In 
order to enrich the WN resource, it has been linked 
with different lexical resources such as WordNet 
Domains (WND) (Magnini and Cavaglia, 2000) a 
lexical resource containing the  domains of the 
synsets in WordNet, SUMO (Niles, 2001) an 
ontology relating the concepts in WordNet, 
WordNet Affect (WNA) an extension of WN 
where different synsets are annotated with one of 
the six basic emotions proposed by Ekman (1999), 
SentiWordNet (Esuli and Sebastiani, 2006) a 
lexical resource  where each synset is annotated 
with polarity, Semantic Classes (SC) (Izquierdo et 
al., 2007) a set of Base Level Concepts (BLC) 
based on WN, etc. The usage of these resources 
allows the tackling of NLP tasks from different 
points of view, depending on the resource used.  
Our approach proposes using different semantic 
dimensions according to different resources. In 
order to achieve this, we use the Integration of 
Semantic Resources based on WordNet, which we 
explain in the next section and the Semantic 
Classes (SC). 
2.1 Integration of Semantic Resources based on 
WordNet (ISR-WN) 
ISR-WN (Guti?rrez et al, 2010b) is a new 
resource that allows the integration of several 
semantic resources mapped to WN. In ISR-WN, 
WordNet 1.6 or 2.0 is used as a core to link several 
resources: SUMO, WND and WNA. As Guti?rrez 
et al (2010a) describe, the integrated resource 
allows navigate inside the semantic network. 
2.2 Semantic Classes (SC) 
The Semantic Classes resource (Izquierdo et al, 
2007) consists of a set of Base Level Concepts 
(BLC) from WN obtained before applying a 
bottom-up process using the chain of hypernym 
relations. For each synset in WN, the process 
selects as its Base Level Concept the first local 
maximum, according to the relative number of 
relations. As a result, a resource with a set of BLCs 
linked semantically to several synsets is obtained. 
In order to apply the multidimensionality that 
ISR-WN and SC provide, we have analyzed 
related approaches like (Magnini et al, 2002; 
2008) ,(V?zquez et al, 2004), (Villarejo et al, 
2005), (Zouaq et al, 2009) and others that take 
into account semantic dimensionality. Then, we 
have decided to use Relevant Semantic Trees 
(Guti?rrez et al, 2010a) because it is an approach 
capable of being applied over several dimensions 
(resources) at once. 
2.3 Relevant Semantic Trees (RST) 
RST (Guti?rrez et al, 2010a) is a method able to 
disambiguate the senses of the words contained in 
a sentence by obtaining the Relevant Semantic 
Trees from different resources. In order to measure 
the association between concepts in each sentence 
according to a multidimensional perspective, RST 
uses the Association Ratio (AR) measure (V?zquez 
et al, 2004). Our purpose is to include the 
Multidimensional Semantic Analysis into the 
Opinion Analysis using RSTs. 
In order to evaluate our approach the rules and 
corpus that concern the English monolingual 
subtasks from MOAT were used. 
2.4 English monolingual subtasks 
In these tasks the participants were provided with 
twenty topics. For each one of the topics, a 
question was given with a short and concise query, 
140
the expected polarity of the answer and the period 
of time. For each of the topics, a set of documents 
were assigned and they had to be splitted into 
sentences for the opinionated and relevance 
judgements and into opinion units for the polarity, 
opinion target and source tasks. In this work, we 
describe twelve runs for the opinionated, relevance 
and polarity judgement tasks. 
3 WSD method 
We propose an unsupervised knowledge-based 
method that uses the RST technique combined 
with SentiWordNet 3.0 (Esuli and Sebastiani, 
2006) to tackle 3 of the monolingual English tasks 
proposed in the NTCIR 8 MOAT. In this approach 
WN 2.0 version is used.  
The aim of this method is to obtain a RST of 
each sentence and then associate the RST with 
polarity values. The process involves the following 
resources: WND, WNA, the WN taxonomy, 
SUMO and Semantic Classes (SC). Because of SC 
does not have a tree structure we simply obtain the 
Relevant Semantic Classes. Subsequently, we 
determine the polarities collected for each label of 
each RST obtained according to the analyzed 
sentence. Our proposal involves four steps 
presented on sections 3.1, 3.2, 3.3 and 3.4. 
3.1 Obtaining the Relevant Semantic Trees  
In this section, we use a fragment of the original 
RST method with the aim of obtaining Relevant 
Semantic Trees of the sentences. Notice that this 
step must be applied for each resource. 
Once each sentence is analyzed, the AR value is 
obtained and related to each concept in the trees. 
Equation 1 is used to measure and to obtain the 
values of Relevant Concepts:  
                 
 
   
  (1) 
Where: 
                   
      
    
  (2) 
In both equations C is a concept; f is a sentence 
or set of words (w); fi is the i-th word of the 
sentence f; P (C, w) is the joint probability 
distribution; P (C) is the marginal probability. 
In order to illustrate the processing steps, we 
will consider the following example: ?But it is 
unfair to dump on teachers as distinct from the 
educational establishment?. Using the WND 
resource, we show the manner in which we obtain 
the RST. 
The first stage involves the lemmatization of the 
words in the sentence. For the example considered, 
the obtained lemmas are:  
Lemmas [unfair; dump; teacher, distinct, 
educational; establishment] 
Next, each lemma is looked up in ISR-WN and 
it is correlated with the WND concepts. Table 1 
shows the results after applying Equation 1 over 
the example. 
Vector 
AR Domain AR Domain 
0.90 Pedagogy 0.36 Commerce 
0.90 Administration 0.36 Quality 
0.36 Buildings 0.36 Psychoanalysis 
0.36 Politics 0.36 Economy 
0.36 Environment   
Table 1. Initial Concept Vector of Domains 
After obtaining the Initial Concept Vector of 
Domains we apply Equation 3 in order to obtain 
the Relevant Semantic Tree related to the sentence.  
                              ;(3) 
Where:  
           
         
  
 ;(4) 
Here AR(PC, f) represents the AR value of PC 
related to the sentence f;           is the AR 
value calculated with equation 1 in case of ChC 
was included in the Initial Vector, otherwise is 
calculated with the equation 3; ChC is the Child 
Concept of PC; ND is a Normalized Distance; IC 
is the Initial Concept from we have to add the 
ancestors; PC is Parent Concept; TD is Depth of 
the hierarchic tree of the resource to use; and MP 
is Minimal Path. 
Applying the Equation 3, the algorithm to 
decide which parent concept will be added to the 
vector is shown here: 
if (         value > 0 ){ 
 if ( PC had not been added to vector) 
       PC is added to the vector with AR(PC, f) value;  
else PC value = PC value + AR(PC, f) value; } 
The result after processing is shown in Table 2. 
This vector represents the Domain tree associated 
to the sentence.  After the Relevant Semantic Tree 
is obtained, the Factotum Domain is eliminated 
141
from the tree. Due to the fact that Factotum is a 
generic Domain associated to words that appear in 
general contexts it does not provide useful 
information and experimentally we confirmed that 
it introduced errors; so we eliminate it (Magnini 
and Cavaglia, 2000). 
Vector 
AR Domain AR Domain 
1.63 Social_Science  0.36 Buildings  
0.90 Administration  0.36 Commerce  
0.90 Pedagogy  0.36 Environment  
0.80 Root_Domain  0.11 Factotum 
0.36 Psychoanalysis 0.11 Psychology  
0.36 Economy  0.11 Architecture  
0.36 Quality 0.11 Pure_Science  
0.36 Politics 
  
Table 2. Final Domain Vector 
3.2 Obtaining the Positive Semantic Trees  
In order to obtain the Positive Semantic Trees 
(PST) of the sentence, we will follow the same 
process described in section 3.1. In this case, the 
AR values will be replaced by the polarity value 
pertaining to the analyzed sense. The polarity is 
obtained from the SentiWordNet 3.0 resource, 
where each given sense from ISR-WN for 
WordNet version 2.0 is mapped to WordNet 
version 3.0. Hence, we can find each given sense 
from ISR-WN in SentiWordNet 3.0 and obtain the 
respective polarities. This new value will be called 
Positive Association (PosA). The PosA value is 
calculated using Equation 4 . 
                     
 
   
  (4) 
Where: 
                      
 
   
  (5) 
Where C is a concept; f is a sentence or set of 
words (w); fi is a i-th word of the sentence f; PosA 
(C, wi) is the positive value of the sense (wi) 
related to C. 
The PosA is used to measure the positive value 
associated to the leaves of the Semantic Trees 
where Concepts are placed. Subsequently, using 
the same structure of RST we create new Semantic 
Trees without AR values. Instead, the leaves with 
Concepts of this new Semantic Trees will be 
annotated with the PosA value.  
Later, to assign some Positive value to the 
parent Concepts, each parent Concept will 
accumulate the positive values from child 
Concepts. Equation 6 shows the bottom-up 
process. 
                     
 
   
  (6) 
Where PC is the Parent Concept; ChC is the 
Child Concept of PC; and PosA(ChC) represents 
the positive value of the ChC. 
3.3 Obtaining the Negative Semantic Trees 
(NST)  
In this phase, we repeat the step described in 
Section 3.2, but for negative values. Table 3 shows 
the PST and NST obtained from the example. 
Vectors Pos-Neg 
PosA NegA Domain PosA NegA Domain 
0.00 1.00 Social_Science  0.00 0.00 Buildings  
0. 00 0.00 Administration  0.00 0.50 Commerce  
0.00 0.00 Pedagogy  0.00 0.00 Environment  
0.00 0.00 Root_Domain  0.375 0.375 Factotum 
0.00 0.00 Psychoanalysis 0.00 0.00 Psychology  
0.00 0.50 Economy  0.00 0.00 Architecture  
0.375 0.375 Quality 0.00 0.00 Pure_Science  
0.00 0.00 Politics 
   
Table 3. Final Domain Vectors Pos-Neg 
As we can see, the analyzed sentence is more 
linked to the Social_Science domain and it 
accumulates a negative value of 1 and a positive 
value of 0. This indicates that the sentence is more 
negative than positive. 
3.4 Obtaining polarities of the sentences 
In this step, we concentrate on detecting which 
polarity is more representative according to the 
Semantic Trees obtained for each resource 
(dimension). For that, we combine the RST with 
PST and RST with NST. Depending on the obtained 
results we classify the sentence as Positive, 
Negative or Neutral. Before performing this step, 
we have to normalize the three types of Semantic 
Trees (RST, PST and NST) for each dimension to 
work with values between 0 and1.  
Our main goal is to assign more weight to the 
polarities related to the most relevant Concepts in 
each Relevant Semantic Tree. Equation 7 shows 
the steps followed in order to obtain the positive 
semantic value. 
142
                          
   
  (7) 
Where ACPosA is the Positive Semantic Value 
of the analyzed sentence obtained for one 
Dimension, RST is the Relevant Semantic Tree 
sorted with the format: RST [Concept| AR]; PST is 
the Positive Semantic Tree sorted according RST 
structure with format: PST [Concept|PosA]; RSTi 
     is the i-th AR value of Concept i;      PSTi 
is the i-th PosA value of the concept i. 
In order to measure the negative semantic value 
(ACNegA), we employ a similar equation replacing 
PST with NST. After obtaining the semantic 
opinion requirements, we evaluate our approach 
over three of the tasks proposed in the NTCIR 8 
MOAT, for the monolingual English setting. 
3.5 Judging sentence opinionatedness 
The ?opinionated? subtask requires systems to 
assign the values YES or NO to each of the 
sentences in the document collection provided. 
This value is given depending on whether the 
sentence contains an opinion (Y) or it does not (N). 
In order to tackle this task, we analyze the PST and 
NST of all dimensions (WN, WSD, WNA, SUMO 
and SC). After reviewing the PSTs and NSTs if at 
least one Concept has assigned a value distinct 
from zero the result will be ?YES? in other cases 
will be ?NO?.  
3.6 Determining sentence relevance 
In the sentence relevance judgement task, the 
systems have to decide whether a sentence is 
relevant to the given question or not (Y|N). We 
assume that the given question is related to each 
sentence per topic if it has a RST 50% similar (the 
similarity is obtained by quantity of Concept labels 
that match). The analyzed sentence is relevant only 
if the PST and the NST values of all dimensions 
that are taken into account contain at least a 
positive or a negative value. 
3.7 Polarity and topic-polarity classification  
The polarity judgment task requires the systems to 
assign a value of ?POS?, ?NEG? or ?NEU? 
(positive, negative or neutral) to each of the 
sentences in the documents provided. 
Our proposal consists of accumulating the 
ACPos values and ACNeg values of all Dimensions 
and comparing them. These accumulated values 
will be named ACPosD and ACNegD respectively. 
In case ACPosD > ACNegD the assigned value is 
POS, if ACPosD < ACNegD the assigned value is 
NEG, otherwise, the assigned value is NEU. 
4 Evaluation and analysis  
In this section we concentrated on measuring the 
influence of each Dimension (resource) taken 
separately and jointly in our proposal. Also, we 
have compared our results with the best results 
obtained by the participant systems in the NTCIR 
8 MOAT competition. 
4.1 Influence of each dimension 
In this section, we present the results of the three 
tasks described above using the combination of all 
dimensions and using each of the resources 
separately. Moreover, we describe the experiments 
we have performed. Exp1: Combining all 
Dimensions (WND, WNA, WN taxonomy, SUMO 
and SC). Exp2: Using WNA. Exp3: Using WND. 
Exp4: Using SC. Exp5: Using SUMO. Exp6: 
Using WN taxonomy. The results are presented in 
Table 4. 
Exp 
Opinion Relevance Polarity 
P R F P R F P R F 
1 20.6 87.8 33.3 78.8 86.8 82.6 39.4 34.5 36.8 
2 23.8 57.2 33.6 77.9 55.8 65.1 39.7 22.2 28.5 
3 22.6 69.5 34.1 79.4 69.2 74.0 40.3 27.5 32.7 
4 20.1 88.5 33.3 78.8 87.3 82.3 39.7 34.9 37.2 
5 21.3 86.5 34.2 79.0 85.8 82.3 40.6 33.7 36.8 
6 21.1 87.6 34.1 78.8 86.6 82.5 40.5 34.2 37.1 
Table 4. Results on each task. Precision (P), Recall (R) 
and F-Measure (F). 
As we can see, the best results are obtained in 
Experiment 4 and 6, which use the WN taxonomy 
and SC to obtain the RST, PST and NST. However, 
the other experiments results are similar in 
performance level. This indicates that our proposal 
can be successfully applied to opinion mining 
tasks. 
4.2 Influence of the semantic dimensions 
without normalizing the vector 
In order to prove that the value normalization 
introduces noise, we performed the same 
experiments without normalizing vectors. In Table 
5, we show in bold font the F-Measure obtained 
143
that constitutes an improvement to previous 
results. It is important to remark that not 
normalizing the vectors helps the Polarity 
Classification task. All the experiments presented 
in Table 5 improved the previous results and the 
SC obtained one of the best results for the Polarity 
and the Relevance task. 
 
Exp Opinion Relevance Polarity 
P R F P R F P R F 
7 20.1 88.5 33.3 78.8 87.3 82.8 39.7 34.9 37.2 
8 23.3 61.1 33.7 78.4 60.0 68.0 42.3 25.5 31.8 
9 21.9 77.9 34.2 79.2 77.3 78.2 39.4 30.5 34.4 
10 20.6 87.7 33.4 78.9 86.7 82.6 44.6 38.9 41.6 
11 20.6 85.0 33.2 78.5 83.6 81.0 44.6 37.7 40.9 
12 20.5 85.5 33.1 78.7 84.4 81.5 43.7 37.0 40.1 
Table 5. Results without normalized vectors. Precision 
(P), Recall (R) and F-Measure (F). 
4.3 Comparison with other proposals 
In this section, we present a comparison between 
our proposal and the best participating systems in 
NTCIR 8 MOAT. In the sentence opinionatedness 
judgement task , the only systems that obtained 
better results compared to our proposal are UNINE 
(Zubaryeva and Savoy, 2010) and NECLC 
systems. These systems obtained F-measure values 
of 40.1% and 36.52% respectively. These results 
are not so far from our results, with the simple 
difference of 5.9% and 2.32% respectively.  
In comparison to our proposal, UNINE is based 
on selecting terms that clearly belong to one type 
of polarity compared to the others and the value 
types of polarities are defined summing the count 
number of terms that tend to be overused in 
positive, negative and neutral opinionated 
sentences possibilities (Zubaryeva and Savoy, 
2010). The opinionated score is the sum of Positive 
Scores and Negative Scores for each selected term. 
The score of non-opinionated sentences is 
computed as a sum of Objectivity Score for each 
selected term, divided by the number of words in 
the sentence. Our proposal neither takes into 
account the detection of relevant terms, nor the 
objective scores. UNINE also obtained better 
results than us in the Polarity task; we think that 
the combination of this proposal with ours could 
obtain better results. Taking into account that both 
proposals use Features Extraction we could 
combine not only Lexical Features but also 
Semantic Features. 
In the Polarity task we could obtain similar 
results to the first run of UNINE system around 
37% of F-measure but with results some distance 
of the best system that obtained a 51.03% of F-
measure. For the relevance task, our proposal 
obtained a difference of 3.22% as far as F-measure 
is concerned from the best result of all runs 
submitted by the National Taiwan University 
(NTU). So, our proposal could be located around 
the first places among the three tasks mentioned.  
5 Conclusion and further works 
In this paper our research was focused on solving a 
recent problem stemmed from the availability of 
large volumes of heterogeneous data which 
provides different kind of information. We have 
conducted an analysis of how the scientific 
community confronts the tasks related to Opinion 
Analysis. One of the most used approaches is to 
apply Features Extraction and based on this idea, 
our proposal is to apply Semantic Features 
Extraction based on Relevant Semantic Trees. 
With our proposal we are able to associate the 
polarities presented on the sentences with Concept 
Semantic Trees. Thus, the Semantic Trees allow 
the classification of sentences according to their 
opinionatedness, relevance and polarity, according 
to MOAT competition. The obtained results were 
compared with the best results obtained on this 
competition achieving values very close to the best 
systems. Several experiments were conducted 
applying vector normalization and without 
normalization to know which semantic dimension 
performed better. 
After a comparative analysis with the systems 
which results were not improved, we propose as 
further work to include the lexical features 
extraction in our proposal. We have planned to use 
Latent Semantic Analysis and other techniques to 
do this work. 
Acknowledgements 
This paper has been supported partially by 
Ministerio de Ciencia e Innovaci?n - Spanish 
Government (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat 
Valenciana (grant no. PROMETEO/2009/119, 
ACOMP/2010/288 and ACOMP/2011/001). 
144
 References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo 
and Patricio Mart?nez-Barco. 2010. The OpAL 
System at NTCIR 8 MOAT. In Proceedings of 
NTCIR-8 Workshop Meeting: 241-245. Tokyo, 
Japan. 
Alexandra Balahur and Andr?s Montoyo. 2009. A 
Semantic Relatedness Approach to Classifying 
Opinion from Web Reviews. Procesamiento del 
Lenguaje Natural, 42:47-54. 
Andrea Esuli and Fabrizio Sebastiani. 2006. 
SentiWordNet: A Publicly Available Lexical 
Resource for Opinion Mining. In Fifth international 
conference on Languaje Resources and Evaluation 
417-422.  
Amal Zouaq, Michel Gagnon and Benoit Ozell. 2009. A 
SUMO-based Semantic Analysis for Knowledge 
Extraction. In Proceedings of the 4th Language & 
Technology Conference. Pozna?, Poland. 
Bernardo Magnini and Gabriela Cavaglia. 2000. 
Integrating Subject Field Codes into WordNet. In 
Proceedings of Third International Conference on 
Language Resources and Evaluation (LREC-2000): 
1413--1418.  
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo and Alfio Gliozzo. 2002. Comparing 
Ontology-Based and Corpus-Based Domain 
Annotations in WordNet. In Proceedings of the First 
International WordNet Conference: 21-25 Mysore, 
India. 
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo and Alfio Gliozzo. 2008. Using Domain 
Information for Word Sense Disambiguation. In 
Proceedings of the First International Conference 
on Emerging Trends in Engineering and Technology 
(icetet 2008): 1187-1191. Nagpur, India. 
Christiane Fellbaum. 1998. WordNet. An Electronic 
Lexical Database. The MIT Press.  
Guo-Hau Lai, Jyun-Wei Huang, Chia-Pei Gao and 
Richard Tzong-Han Tsai. 2010. Enhance Japanese 
Opinionated Sentence Identification using Linguistic 
Features: Experiences of the IISR Group at NTCIR-
8 MOAT Task. In Proceedings of NTCIR-8 
Workshop Meeting: 272-275. Tokyo, Japan. 
Hatzivassiloglou, Vasileios and Janyce Wiebe. 2000. 
Effects of Adjective Orientation and Gradability on 
Sentence Subjectivity. In International Conference 
on Computational Linguistics (COLING-2000).  
Ian Niles. 2001. Mapping WordNet to the SUMO 
Ontology. Teknowledge Corporation. 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. 
Annotating Expressions of Opinions and Emotions 
in Language. In Kluwer Academic Publishers: 
Netherlands. 
Luis Villarejo, Llu?s M?rquez and German Rigau. 2005. 
Exploring the construction of semantic class 
classiers for WSD. In Sociedad Espa?ola para el 
Procesamiento del Lenguaje Natural, 35: 195-202.  
Olena Zubaryeva and Jacques Savoy. 2010. Opinion 
Detection by Combining Machine Learning & 
Linguistic Tools In Proceedings of NTCIR-8 
Workshop Meeting: 221-227. Tokyo, Japan. 
Paul Ekman. 1999. Handbook of Cognition and 
Emotion. Handbook of Cognition and Emotion: John 
Wiley & Sons, Ltd. 
Rub?n Izquierdo, Armando Su?rez and German Rigau. 
2007. A Proposal of Automatic Selection of Coarse-
grained Semantic Classes for WSD. Procesamiento 
del Lenguaje Natural, 39:189-196. 
Rudi L. Cilibrasi and Paul M.B. Vit?nyi. 2007. The 
Google Similarity Distance. IEEE Transactions On 
Knowledge And Data Engineering, 19(3). 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In In Proceedings of 
workshop on sentiment and subjectivity in text at 
proceedings of the 21st international conference on 
computational linguistics/the 44th annual meeting of 
the association for computational linguistics 
(COLING/ACL 2006): 1-8. Sydney, Australia. 
Soo-Min Kim and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opinion 
Texts. In Proceedings of AAAI-05 Workshop on 
Question Answering in Restricted Domains. 
Sonia V?zquez, Andr?s Montoyo and German Rigau. 
2004. Using Relevant Domains Resource for Word 
Sense Disambiguation. In IC-AI?04. Proceedings of 
the International Conference on Artificial 
Intelligence: Ed: CSREA Press. Las Vegas, 
E.E.U.U. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010a. UMCC-DLSI: 
Integrative resource for disambiguation task. In 
Proceedings of the 5th International Workshop on 
Semantic Evaluation: 427-432. Uppsala, Sweden. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010b. Integration of semantic 
resources based on WordNet. In XXVI Congreso de 
la Sociedad Espa?ola para el Procesamiento del 
Lenguaje Natural, 45: 161-168. Universidad 
Polit?cnica de Valencia, Valencia, Spain. 
145
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 168?174,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Towards a Unified Approach for Opinion Question Answering and
Summarization
Elena Lloret and Alexandra Balahur and Manuel Palomar and Andre?s Montoyo
Department of Software and Computing Systems
University of Alicante
Alicante 03690, Spain
{elloret,abalahur, mpalomar, montoyo}@dlsi.ua.es
Abstract
The aim of this paper is to present an ap-
proach to tackle the task of opinion question
answering and text summarization. Follow-
ing the guidelines TAC 2008 Opinion Sum-
marization Pilot task, we propose new meth-
ods for each of the major components of the
process. In particular, for the information
retrieval, opinion mining and summarization
stages. The performance obtained improves
with respect to the state of the art by approxi-
mately 12.50%, thus concluding that the sug-
gested approaches for these three components
are adequate.
1 Introduction
Since the birth of the Social Web, users play a cru-
cial role in the content appearing on the Internet.
With this type of content increasing at an exponen-
tial rate, the field of Opinion Mining (OM) becomes
essential for analyzing and classifying the sentiment
found in texts.
Nevertheless, real-world applications of OM of-
ten require more than an opinion mining component.
On the one hand, an application should allow a user
to query about opinions in natural language. There-
fore, Question Answering (QA) techniques must be
applied in order to determine the information re-
quired by the user and subsequently retrieve and
analyze it. On the other hand, opinion mining of-
fers mechanisms to automatically detect and classify
sentiments in texts, overcoming the issue given by
the high volume of such information present on the
Internet. However, in many cases, even the result of
the opinion processing by an automatic system still
contains large quantities of information, which are
still difficult to deal with manually. For example,
for questions such as ?Why do people like George
Clooney?? we can find thousands of answers on the
Web. Therefore, finding the relevant opinions ex-
pressed on George Clooney, classifying them and
filtering only the positive opinions is not helpful
enough for the user. He/she will still have to sift
through thousands of texts snippets, containing rele-
vant, but also much redundant information. For that,
we need to use Text Summarization (TS) techniques.
TS provides a condensed version of one or several
documents (i.e., a summary) which can be used as a
substitute of the original ones (Spa?rck Jones, 2007).
In this paper, we will concentrate on proposing ad-
equate solutions to tackle the issue of opinion ques-
tion answering and summarization. Specifically, we
will propose methods to improve the task of ques-
tion answering and summarization over opinionated
data, as defined in the TAC 2008 ?Opinion Sum-
marization pilot?1. Given the performance improve-
ments obtained, we conclude that the approaches we
proposed for these three components are adequate.
2 Related Work
Research focused on building factoid QA systems
has a long tradition, however, it is only recently that
studies have started to focus on the creation and de-
velopment of opinion QA systems. Example of this
can be (Stoyanov et al, 2004) who took advantage of
opinion summarization to support Multi-Perspective
QA system, aiming at extracting opinion-oriented
information of a question. (Yu and Hatzivassiloglou,
2003) separated opinions from facts and summa-
rized them as answer to opinion questions. Apart
from these studies, specialized competitions for sys-
tems dealing with opinion retrieval and QA have
been organized in the past few years. The TAC
2008 Opinion Summarization Pilot track proposed
a mixed setting of factoid and opinion questions.
1http://www.nist.gov/tac/2008/summarization/
168
It is interesting to note that most of the participat-
ing systems only adapted their factual QA systems
to overcome the newly introduced difficulties re-
lated to opinion mining and polarity classification.
Other relevant competition focused on the treatment
of subjective data is the NTCIR MOAT (Multilin-
gual Opinion Analysis Test Collection). The ap-
proaches taken by the participants in this task are rel-
evant to the process of opinion retrieval, which is the
first step performed by an opinion mining question
answering system. For example, (Taras Zabibalov,
2008) used an almost unsupervised approach ap-
plied to two of the sub-tasks: opinionated sentence
and topic relevance detection.(Qu et al, 2008) ap-
plied a sequential tagging approach at the token level
and used the learned token labels in the sentence
level classification task and their formal run submis-
sion was is trained on MPQA (Wiebe et al, 2005).
3 Text Analysis Conferences
In 2008, the Opinion Summarization Pilot task at
the Text Analysis Conferences2 (TAC) consisted in
generating summaries from blogs, according to spe-
cific opinion questions provided by the TAC orga-
nizers. Given a set of blogs from the Blog06 col-
lection3 and a list of questions, participants had to
produce a summary that answered these questions.
The questions generally required determining opin-
ion expressed on a target, each of which dealt with a
single topic (e.g. George Clooney). Additionally, a
set of text snippets were also provided, which con-
tained the answers to the questions. Table 1 depicts
an example of target, question, and optional snippet.
Target: George Clooney
Questions: Why do people like George Clooney?Why do people dislike George Clooney?
Snippets: 1050 BLOG06-20060209-006-0013539097
he?s a great actor.
Table 1: Example of target, question, and snippet.
Following the results obtained in the evaluation
at TAC 2008 (Balahur et al, 2008), we propose
an opinion question answering and summarization
(OQA&S) approach, which is described in detail in
the following sections.
2www.nist.gov/tac/
3http://ir.dcs.gla.ac.uk/test collections/access to data.html
4 An Opinion Question Answering and
Summarization Approach
In order to improve the results of the OQA&S sys-
tem presented at TAC, we propose new methods for
each of the major components of the system: infor-
mation retrieval, opinion mining and text summa-
rization.
4.1 Opinion Question Answering and
Summarization Components
? Information Retrieval
JAVA Information Retrieval system (JIRS) is
a IR system especially suited for QA tasks
(Go?mez, 2007). Its purpose is to find frag-
ments of text (passages) with more probabil-
ity of containing the answer to a user question
made in natural language instead of finding rel-
evant documents for a query. To that end, JIRS
uses the own question structure and tries to
find an equal or similar expression in the docu-
ments. The more similar the structure between
the question and the passage is, the higher the
passage relevance.
JIRS is able to find question structures in a
large document collection quickly and effi-
ciently using different n-gram models. Subse-
quently, each passage is assessed depending on
the extracted n-grams, the weight of these n-
grams, and the relative distance between them.
Finally, it is worth noting that the number of
passages in JIRS is configurable, and in this
research we are going to experiment with pas-
sages of length 1 and 3.
? Opinion Mining
The first step we took in our approach was
to determine the opinionated sentences, as-
sign each of them a polarity (positive or neg-
ative) and a numerical value corresponding to
the polarity strength (the higher the negative
score, the more negative the sentence and vice
versa). In our first approximation (OMaprox1),
we employed a simple, yet efficient method,
presented in Balahur et al (Balahur et al,
2009). As lexicons for affect detection, we
used WordNet Affect (Strapparava and Vali-
tutti, 2004), SentiWordNet (Esuli and Sebas-
169
tiani, 2006), and MicroWNOp (Cerini et al,
2007). Each of the resources we employed
were mapped to four categories, which were
given different scores: positive (1), negative
(-1), high positive (4) and high negative (-4).
First, the score of each of the blog posts was
computed as the sum of the values of the words
that were identified. Subsequently, we per-
formed sentence splitting4 and classified the
sentences we thus obtained according to their
polarity, by adding the individual scores of the
affective words identified.
In the second approach (OMaprox2), we first
filter out the sentences that are associated to
the topic discussed, using LSA. Further on, we
score the sentences identified as relating to the
topic of the blog post, in the same manner as
in the previous approach. The aim of this ap-
proach is to select for further processing only
the sentences which contain opinions on the
post topic. In order to filter these sentences
in, we first create a small corpus of blog posts
on each of the topics included in our collec-
tion5. For each of the corpora obtained, we
apply LSA, using the Infomap NLP Software6.
Subsequently, we compute the 100 most asso-
ciated words with two of the terms that are most
associated with each of the topics and the 100
most associated words with the topic word. The
approach was proven to be successful in (Bal-
ahur et al, 2010).
? Text Summarization
The text summarization approach used in this
paper was presented in (Lloret and Palomar,
2009). In order to generate a summary, the
suggested approach first carries out a basic pre-
processing stage comprising HTML parsing,
sentence segmentation, tokenization, and stem-
ming. Once the input document or documents
have been pre-processed, a relevance detection
stage, which is the core part of the approach, is
applied. The objective of this step is to identify
4http://alias-i.com/lingpipe/
5These small corpora (30 posts for each of the top-
ics) are gathered using the search on topic words on
http://www.blogniscient.com/ and crawling the resulting pages.
6http://infomap-nlp.sourceforge.net/
potential relevant sentences in the document by
means of three techniques: textual entailment,
term frequency and the code quantity principle
(Givo?n, 1990). Then, each potential relevant
sentence is given a score which is computed
on the basis of the aforementioned techniques.
Finally, all sentences are ordered according
to their scores, and the highest ranked ones
(which mean those sentences contain more im-
portant information) are selected and extracted
up to the desired length, thus building the fi-
nal summary. It is worth stressing upon the fact
that in an attempt to maintain the coherence of
the original documents, sentences are shown in
the same order they appear in the original doc-
uments.
4.2 Experimental Framework
The objective of this section is to describe the corpus
used and the experiments performed with the data
provided in TAC 2008 Opinion Summarization Pi-
lot7 task. The approaches analyzed comprise:
? OQA&S: The three components explained
in the previous section (information retrieval,
opinion mining and summarization) were
bound together in order to produce summaries
that include the answer to opinionated ques-
tions. First, the most relevant passages of
length 1 and 3 are retrieved by the IR module,
as in the aforementioned approach, and then
the subjective information is found and classi-
fied within them using the OM approaches de-
scribed in the previous section. Further on, we
incorporate the TS module, to select and ex-
tract the most relevant opinionated facts from
the pool of subjective information identified
by the OM module. We generate opinion-
oriented summaries of compression rates rang-
ing from 10% to 50%. In the end, four dif-
ferent approaches result from the integration
of the three components: IRp1-OMaprox1-
TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-
TS; and IRp3-OMaprox2-TS.
Moreover, apart from these approaches, two base-
lines were also defined. On the one hand, we sug-
7http://www.nist.gov/tac/data/past-
blog06/2008/OpSummQA08.html#OpSumm
170
gest a baseline using the list of snippets provided by
the TAC organization (QA-snippets). This baseline
produces a summary by joining all the answers in the
snippets that related to the same topic On the other
hand, we took as a second baseline the approach
from our participation in TAC 2008 (DLSIUAES),
without not taking into account any information re-
trieval or question answering system to retrieve the
fragments of information which may be relevant to
the query. In contrast, this was performed by com-
puting the cosine similarity8 between each sentence
in the blog and the query. After all the potential rel-
evant sentences for the query were identified, they
were classified in terms of subjectivity and polarity,
and the most relevant ones were selected for the final
summary.
4.3 Evaluation Methodology
Since we used the corpus provided at the Opinion
Summarization Pilot task, and we followed simi-
lar guidelines, we should evaluate our OQA&S ap-
proach in the same way as participant systems were
assessed. However, the evaluation methodology
proposed differs slightly from the one carried out
in the competition. The reason why we took such
decision was due to the fact that the evaluation car-
ried out in TAC had some limitations, and therefore
was not suitable for our purposes. In this manner,
our evaluation is also based on the gold-standard
nuggets provided by TAC, but in addition we pro-
posed an extended version of them, by adding other
pieces of information that are also relevant to the
topics.
In this section, all the issues concerning the eval-
uation are explained. These comprise the original
evaluation method used in the Opinion Summariza-
tion Pilot task at TAC (Section 4.3.1) , its draw-
backs (Section 4.3.2), and the extended version for
the evaluation method we propose (Section 4.3.3).
Further on, the results obtained together with a wide
discussion, as well as its comparison with the base-
lines and the TAC participants is provided in Section
4.4.
4.3.1 Nugget-based Evaluation at TAC
Within the Opinion Summarization Pilot task,
each summary was evaluated according to its con-
8http://www.d.umn.edu/ tpederse/text-similarity.html
tent using the Pyramid method (Nenkova et al,
2007). A list of nuggets was provided and the asses-
sors used such list of nuggets to count the number
of nuggets a summary contained. Depending on the
number of nuggets the summary included and the
importance of each one given by their weight, the
values for recall, precision and F-measure were ob-
tained. An example of several nuggets correspond-
ing to different topics can be seen in Table 2, where
the weight for each one is also shown in brackets.
Topic Nugget (weight)
Carmax CARMAX prices are firm, the price is
the price (0.9)
Jiffy Lube They should have torque wrenches (0.2)
Talk show hosts Funny (0.78)
Table 2: Example of evaluation nuggets and associated
weights.
4.3.2 Limitations of the Nugget Evaluation
The evaluation method suggested at TAC requires
a lot of human effort when it comes to identify
the relevant fragments of information (nuggets) and
compute how many of them a summary contains, re-
sulting in a very costly and time-consuming task.
This is a general problem associated to the evalua-
tion of summaries, which makes the task of summa-
rization evaluation especially hard and difficult.
But, apart from this, when an exhaustive exam-
ination of the nuggets used in TAC is done, some
other problems arised which are worth mentioning.
The average number of nuggets for each topic is
27, and this would mean, that longer summaries
will be highly penalized, because it will contain
more useless information according to the nuggets.
After analyzing in detail all the provided nuggets,
we mainly classified the possible problems into six
groups, which are:
1. Some of the nuggets were expressed differently
from how they appeared in the original blogs.
Since most of the summarization systems are ex-
tractive, this fact forced that humans had to evaluate
the summaries, otherwise it would be very difficult
to account for the presence of such nugget in the
summary, if they are not using the same vocabulary
as the original blogs.
2. Some nuggets for the same topic express the
171
same idea, despite not being identical. In these
cases, we are counting a single piece of informa-
tion in the summary twice, if the idea that nuggets
expressed is included.
3. Moreover, the meaning of one nugget can be de-
duced from another?s, which is also related to the
problem stated before.
4. Some of the nuggets are not very clear in mean-
ing (e.g. ?hot?, ?fun?). This would mean that a
summary might include such terms in a different
context, thus, obtaining incorrectly that it is reve-
lant when might be out of context.
5. A sentence in the original blog can be covered by
several nuggets. For instance, both nuggets ?it is
an honest book? and ?it is a great book? correspond
to the same sentence ?It was such a great book-
honest and hard to read (content not language dif-
ficulty)?. In this case, it is not clear how to proceed
with the evaluation; whether to count both nuggets
or just one of them.
6. Some information which is also relevant for the
topic is not present in any nugget. For instance:
?I go to Starbucks because they generally provide
me better service?. Although it is relevant with re-
spect to the topic and it appears in a number of sum-
maries, it would be not counted because it has not
been chosen as a nugget.
4.3.3 Extended Nugget-based Evaluation
Since we are interested in testing a wide range of
approaches involving IR, OM and TS, sticking to the
rules to the original TAC evaluation would mean that
a lot of time as well as human effort will be required,
as well as not accounting for important information
that summaries may contain in addition to the one
expressed by the nuggets. Therefore, taking as a ba-
sis the nuggets provided at TAC, we set out a modi-
fied version of them.
The underlying idea behind this is to create an ex-
tended set of nuggets that serve as a reference for
assessing the content of the summaries. In this man-
ner, we will map each original nugget with the set of
sentences in the original blogs that are most similar
to it, thus generating a gold-standard summary for
each topic. For creating this extended gold-standard
nuggets we compute the cosine similarity9 between
9The cosine similarity was computed using Pedersen?s
every nugget and all the sentences in the blog related
to the same topic. We empirically established a sim-
ilarity threshold of 0.5, meaning that if a sentence
was equal or above such similarity value, it will be
considered also relevant. One main disadvantage of
such a lower threshold value is that we can consider
relevant sentences that share the same vocabulary
but in fact they are not relevant to the summary. In
order to avoid this, once we had identified all the
most similar sentences to each nugget, we carried
out a manual analysis to discard cases like this. Hav-
ing created the extended set of nuggets, we grouped
all of them pertaining to the same topic, and consid-
ered it a gold-standard summary. Now, the average
number of nuggets per topic is 53, which we have
increased by twice the number of original nuggets
provided at TAC.
Further on, our summaries are compared against
this new gold-standard using ROUGE (Lin, 2004).
This tool computes the number of different kinds
of overlap n-grams between an automatic summary
and a human-made summary. For our evaluation,
we compute ROUGE-1 (unigrams), ROUGE-2 (bi-
grams), ROUGE-SU4 (it measures the overlap of
skip-bigrams between a candidate summary and a
set of reference summaries with a maximum skip
distance of 4), and ROUGE-L (Longest Common
Subsequence between two texts). The results and
discussion are next provided.
4.4 Results and Discussion
This section contains the results obtained for our
OQA&S approach and all the sub-approaches tested.
IRpN refers to the length of the passage employed
in the information retrieval approach, whereas
OMaproxN indicates the approach used for the opin-
ion mining component. Firstly, we show and ana-
lyze the results of our different approaches, and then
we compared the best performing one with the base-
lines and the average Opinion Summarization Pilot
task participants results in TAC.
Table 3 shows the precision (Pre), recall (Rec) and
F-measure results of ROUGE-1 (R-1) for all the ap-
proaches we experimented with.
Generally speaking, the results obtained show
better figures for precision than for recall, and there-
Text Similarity Package: http://www.d.umn.edu/ tpederse/text-
similarity.html
172
Approach Summary length
Name R-1 10% 20% 30% 40% 50%
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 14.45 18.58 22.32 23.63 26.32
-OMaprox1-TS F?=1 16.53 20.65 24.58 25.75 28.12
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 16.90 20.02 23.36 24.15 26.77
-OMaprox2-TS F?=1 19.45 22.13 25.36 25.94 28.40
Pre 27.27 30.18 30.91 30.05 30.19
IRp3 Rec 20.56 24.76 28.25 31.67 34.47
-OMaprox1-TS F?=1 22.65 26.23 27.98 29.18 29.74
Pre 30.16 32.11 32.35 32.41 32.11
IRp3 Rec 20.64 24.03 27.25 29.78 32.68
-OMaprox2-TS F?=1 23.28 25.64 27.42 28.44 29.21
Table 3: Results of our OQA&S approaches
Approach Performance (ROUGE)
Name % R-1 R-2 R-L R-SU4
Pre 32.11 7.34 29.00 11.37
IRp3-OMaprox2 Rec 32.68 8.31 33.24 12.76
-TS (50%) F?=1 29.21 7.22 28.60 11.13
Pre 17.97 8.76 17.65 9.98
QA-snippets Rec 71.24 31.30 70.10 37.44
F?=1 24.73 11.58 24.29 13.45
Pre 20.54 7.00 19.46 9.29
DLSIUAES Rec 57.66 18.98 54.61 25.77
F?=1 27.04 9.10 25.59 12.22
Pre 23.74 8.35 22.72 10.81
Average TAC Rec 56.65 19.37 54.56 25.40
participants F?=1 27.45 9.64 26.33 12.46
Pre 20.42 6.06 19.55 8.62
Average TAC Rec 56.45 17.3 54.40 24.11
participants? F?=1 24.31 7.25 23.31 10.29
Table 4: Comparison with other systems
fore the F-measure value, which combines both val-
ues, will be affected. Good precision values means
that the information our approaches select is the cor-
rect one, despite not including all the relevant infor-
mation.
Our best performing approach in general is the
one which uses a length passage of 3 and, as far
as OM is concerned, when topic-sentiment analy-
sis is carried out (IRp3-OMaprox2-TS). This shows
that the approach dealing with topic-sentiment anal-
ysis in opinion mining is more suitable than the one
which does not consider topic relevance. Taking a
look at some individual results, we next try to eluci-
date the reasons why our approach performs better
at some approaches and not so good at others. Con-
cerning the IR module, it is important to mention
that a passage length of 1 always obtains poorer re-
sults that when it is increased to 3, meaning that the
longer the passage, the better.
Regarding the best summary length, we observed
that in general terms, the more content we allow
for the summary, the better. In other words, com-
pression rates of 50% get higher results than 20%
or 10%. However, there are cases in which shorter
summaries (10% and 20%) obtains better results
than longer ones (e.g. IRp3-OMaprox2-TS vs. IRp3-
OMaprox1-TS).
Although the results theirselves are not very high
(around 30%), they are in line with the state-of-the-
art, as can be seen in Table 4, where our best per-
forming approach is compared with respect to other
approaches.
Although the compression rate which obtains best
results is not very high (50%), indeed the final sum-
maries have an average length of 2,333 non-white
space characters. This is really low compared to the
length that TAC organization allowed for the Opin-
ion Summarization Pilot task, which was 7,000 non-
white space characters per question, and most of
the times there were two questions for each topic.
Whereas the results of TAC participants are much
better for the recall value than ours, if we take a look
at the precision, our approach outperforms them ac-
cording to this value in all of the cases. The longer
a summary is, the more chances it has to contain in-
formation related to the topic. However, not all this
information may be relevant, as it is shown in the
results for the precision values, which decrease con-
siderably compared to the recall ones. In contrast,
due to the fact that our approach is missing some
relevant information because we use a rather short
passage length (3 sentences), we do not obtain such
high values for the recall, but we obtain good preci-
sion results, which indicate that the information that
we keep is important.
Moreover, comparing those results with the ones
obtained by our approach, it is worth mentioning
that IRp3-OMaprox2-TS outperforms the F-measure
value for all the ROUGE metrics with respect to Av-
erage TAC participants?. More in detail, when the
ROUGE scores are averaged, IRp3-OMaprox2-TS
improves by 12.50% the Average TAC participants?
for the F-measure value.
173
5 Conclusion and Future Work
In this paper, we tackled the process of OQA&S.
In particular, we analyzed specific methods within
each component of this process, i.e., information
retrieval, opinion mining and text summarization.
These components are crucial in this task, since our
final goal was to provide users with the correct infor-
mation containing the answer of a question. How-
ever, contrary to most research work in question an-
swering, we focus on opinionated questions rather
than factual, increasing the difficulty of the task.
Our analysis comprises different configurations
and approaches: i) varying the length for retrieving
the passages of the documents in the retrieval infor-
mation stage; ii) studying a method that take into
consideration topic-sentiment analysis for detecting
and classifying opinions in the retrieved passages
and comparing it to another that does not; and iii)
generating summaries of different compression rates
(10% to 50%). The results obtained showed that
the proposed methods are appropriate to tackle the
OQA&S task, improving state of the art approaches
by 12.50% approximately.
In the future, we plan to continue investigating
suitable approaches for each of the proposed com-
ponents. Our final goal is to build an integrated and
complete approach.
Acknowledgments
This research work has been funded by the Spanish Gov-
ernment through the research program FPI (BES-2007-
16268) associated to the project TEXT-MESS (TIN2006-
1526-C06-01). Moreover, it has been also partially
funded by projects TEXT-MESS 2.0 (TIN2009-13391-
C04), and PROMETEO (PROMETEO/2009/199) from
the Spanish and the Valencian Government, respectively.
References
A. Balahur, E. Lloret, O. Ferra?ndez, A. Montoyo,
M. Palomar, and R. Mun?oz. 2008. The DLSIUAES
team?s participation in the tac 2008 tracks. In Pro-
ceedings of the Text Analysis Conference.
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
Bruno Pouliquen, and Mijai Kabadjov. 2009. Opinion
mining from newspaper quotations. In Proceedings of
the Workshop on Intelligent Analysis and Processing
of Web News Content.
A. Balahur, M. Kabadjov, and J. Steinberger. 2010.
Exploiting higher-level semantic information for the
opinion-oriented summarization of blogs. In Proceed-
ings of CICLing?2010.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Language re-
sources and linguistic theory: Typology, second lan-
guage acquisition, English linguistics.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceedings of LREC.
Talmy Givo?n, 1990. Syntax: A functional-typological in-
troduction, II. John Benjamins.
Jose? M. Go?mez. 2007. Recuperacio?n de Pasajes Multil-
ingu?e para la Bu?squeda de Respuestas. Ph.D. thesis.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. In Proceedings of ACL Text
Summarization Workshop, pages 74?81.
Elena Lloret and Manuel Palomar. 2009. A gradual com-
bination of features for building automatic summarisa-
tion systems. In Proceedings of TSD, pages 16?23.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2):4.
Lizhen Qu, Cigdem Toprak, Niklas jakob, and iryna
Gurevych. 2008. Sentence level subjectivity and sen-
timent analysis experiments in ntcir-7 moat challenge.
In Proceedings of NTCIR-7 Workshop meeting.
Karen Spa?rck Jones. 2007. Automatic summarising: The
State of the Art. Information Processing & Manage-
ment, 43(6):1449?1481.
V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe. 2004.
Evaluating an opinion annotation scheme using a new
multi-perspective question and answer corpus. In
AAAI Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1083?1086.
John Carroll Taras Zabibalov. 2008. Almost-
unsupervised cross-language opinion analysis at ntcis-
7. In Proceedings of NTCIR-7 Workshop meeting.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Language Resources and Evaluation, volume 39.
D. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
174
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 94?99,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
RA-SR: Using a ranking algorithm to automatically building resources 
for subjectivity analysis over annotated corpora 
Yoan Guti?rrez, Andy Gonz?lez 
University of Matanzas, Cuba 
yoan.gutierrez@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, {montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
In this paper we propose a method that 
uses corpora where phrases are annotated 
as Positive, Negative, Objective and 
Neutral, to achieve new sentiment 
resources involving words dictionaries 
with their associated polarity. Our 
method was created to build sentiment 
words inventories based on senti-
semantic evidences obtained after 
exploring text with annotated sentiment 
polarity information. Through this 
process a graph-based algorithm is used 
to obtain auto-balanced values that 
characterize sentiment polarities well 
used on Sentiment Analysis tasks. To 
assessment effectiveness of the obtained 
resource, sentiment classification was 
made, achieving objective instances over 
80%. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge 
to extract useful data. Texts can provide factual 
information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention. Among most widely used terms 
in Natural Language Processing, in concrete in 
Sentiment Analysis (SA) and Opinion Mining, is 
the subjectivity term proposed by (Wiebe, 1994). 
This author defines it as ?linguistic expression of 
somebody?s opinions, sentiments, emotions, 
evaluations, beliefs and speculations?. Another 
important aspect opposed to subjectivity is the 
objectivity, which constitute a fact expression 
(Balahur, 2011). Other interesting terms also 
proposed by (Wiebe et al, 2005) considers, 
private state, theses terms involve opinions, 
beliefs, thoughts, feelings, emotions, goals, 
evaluations and judgments.  
Many researchers such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working in this way and related areas. To 
build systems able to lead SA challenges it is 
necessary to achieve sentiment resources 
previously developed. These resources could be 
annotated corpora, affective semantic structures, 
and sentiment dictionaries.  
In this paper we propose a method that uses 
annotated corpora where phrases are annotated as 
Positive, Negative, Objective and Neutral, to 
achieve new resources for subjectivity analysis 
involving words dictionaries with their 
associated polarity.  
The next section shows different sentiment and 
affective resources and their main characteristics. 
After that, our proposal is developed in section 3. 
Section 4, present a new sentiment resource 
obtained after evaluating RA-SR over many 
corpora. Section 5 described the evaluation and 
analysis of the obtained resource, and also an 
assessment of the obtained resource in Sentiment 
Classification task. Finally, conclusion and 
further works are presented in section 6. 
2 Related work 
It is known that the use of sentiment resources 
has proven to be a necessary step for training and 
evaluation for systems implementing sentiment 
analysis, including also fine-grained opinion 
mining (Balahur, 2011). 
Different techniques have been used into 
product reviews to obtain lexicons of subjective 
words with their associated polarity. We can 
study the relevant research promoted by (Hu and 
Liu, 2004) which start with a set of seed 
adjectives (?good? and ?bad?) and reinforce the 
semantic knowledge applying a expanding the 
lexicon with synonymy and antonymy relations 
provided by WordNet (Miller et al, 1990). As 
result of Hu and Liu researches an Opinion 
Lexicon is obtained with around 6800 positive 
94
 and negative English words (Hu and Liu, 2004; 
Liu et al, 2005). 
A similar approach has been used in building 
WordNet-Affect (Strapparava and Valitutti, 
2004). In this case the building method starting 
from a larger of seed affective words set. These 
words are classified according to the six basic 
categories of emotion (joy, sadness, fear, 
surprise, anger and disgust), are also expanded 
increase the lexicon using paths in WordNet. 
Other widely used in SA has been 
SentiWordNet resource (Esuli and Sebastiani, 
2006)). The main idea that encouraged its 
construction has been that ?terms with similar 
glosses in WordNet tend to have similar 
polarity?. 
Another popular lexicon is MicroWNOp 
(Cerini et al, 2007). It contains opinion words 
with their associated polarity. It has been built on 
the basis of a set of terms extracted from the 
General Inquirer1 (Stone et al, 1996).  
The problem is that these resources do not 
consider the context in which the words appear. 
Some methods tried to overcome this critique 
and built sentiment lexicons using the local 
context of words. 
We can mentioned to (Pang et al, 2002) whom 
built a lexicon with associated polarity value, 
starting with a set of classified seed adjectives 
and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. 
(Turney, 2002) classifies words according to 
their polarity based on the idea that terms with 
similar orientation tend to co-occur in 
documents.  
On the contrary in (Balahur and Montoyo, 
2008b), is computed the polarity of new words 
using ?polarity anchors? (words whose polarity 
is known beforehand) and Normalized Google 
Distance (Cilibrasi and Vit?nyi, 2007) scores 
using as training examples opinion words 
extracted from ?pros and cons reviews? from the 
same domain. This research achieved the lexical 
resource Emotion Triggers (Balahur and 
Montoyo, 2008a). 
Another approach that uses the polarity of the 
local context for computing word polarity is the 
one presented by (Popescu and Etzioni, 2005), 
who use a weighting function of the words 
around the context to be classified. 
All described resources have been obtained 
manually or semi-automatically. Therefore, we 
                                                 
1
 http://www.wjh.harvard.edu/~inquirer/ 
focus our target in archiving automatically new 
sentiment resources supported over some of 
aforementioned resources. In particular, we will 
offer contributions related with methods to build 
sentiment lexicons using the local context of 
words. 
3 Our method 
We propose a method named RA-SR (using 
Ranking Algorithms to build Sentiment 
Resources) to build sentiment words inventories 
based on senti-semantic evidences obtained after 
exploring text with annotated sentiment polarity 
information. Through this process a graph-based 
algorithm is used to obtain auto-balanced values 
that characterize sentiment polarities widely used 
on Sentiment Analysis tasks. This method 
consists of three main stages: (I) Building 
contextual words graphs; (II) Applying ranking 
algorithm; and (III) Adjusting sentiment polarity 
values. 
 
Figure 1. Resource walkthrough development process. 
These stages are represented in the diagram of 
Figure 1, where the development process begins 
introducing two corpuses of annotated sentences 
with positive and negative sentences 
respectively. Initially, a preprocessing of the text 
is made applying Freeling pos-tagger (Atserias et 
al., 2006) version 2.2 to convert all words to 
lemmas2. After that, all lemmas lists obtained are 
introduced in RA-SR, divided in two groups (i.e. 
positive and negative candidates, ????  and ????).  
3.1 Building contextual words graphs 
Giving two sets of sentences (???? and ????) 
annotated as positive and negative respectively, 
where ????	 = [?????, ? , ?????]  and ????	 =[?????, ? , ?????]	  contains list ?  involving 
words lemmatized by Freeling 2.2 Pos-Tagger 
                                                 
2
 Lemma denotes canonic form of the words. 
Corpora
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
Weight =1
Weight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
95
 (Atserias et al, 2006), a process to build two 
lexical contextual graphs, ????  and ????  is 
applied. Those sentences are manually annotated 
as positive and negative respectively. These 
graphs involve lemmas from the positive and 
negative sentences respectively. 
A contextual graph ?  is defined as an 
undirected graph ? =	 (?, ?) , where ?  denotes 
the set of vertices and ? the set of edges. Given 
the list ?	 = [?1 	? ??]  a lemma graph is created 
establishing links among all lemmas of each 
sentence, where words involved allow to 
interconnect sentences ??  in ? . As a result 
word/lemma networks ????  and ????  are 
obtained, where ?	 = 	?	 = [?? 	? ??]	  and for 
every edge (?? , ??)?	? being ??, ???	?. Therefore, ?? and ??	 are the same. 
Then, having two graphs, we proceed to 
initialize weight to apply graph-based ranking 
techniques in order to auto-balance the particular 
importance of each ?? into ???? and ????. 
3.2 Applying ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ???? and ???? take 
the default value 1/N as their weight to define 
the weight of ?  vector, which is used in our 
proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 2) as positive or negative, in relation 
to their respective graph, a weight value of 1 (in 
a range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
Thereafter, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking 
algorithm is able to increase the significance of 
the words related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre 
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and the one that has obtained relevant 
results, was an inspiration to us in this work. The 
main idea behind this algorithm is that, for each 
edge between ?i and ?j in graph ?, a vote is made 
from ?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ????  relevance. The philosophy 
behind it is that, the more important the vertex is, 
the more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
?, where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??. 
In our system, we apply the following equation 
and configuration:  
 
??	 = 	????	 +	(1 ? ?)? (1) 
Where: ?	 is a probabilistic transition matrix 
?	?	? , being ??,?  = ???	  if a link from ? i to ? j 
exist, in other case zero is assigned; ? is a vector ?	?	1	with values previously described in this 
section; ?? is the probabilistic structural vector 
obtained after a random walkthrough to arrive to 
any vertex; ?	  is a dumping factor with value 
0.85, and like in (Agirre and Soroa, 2009) we 
used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009). 
After applying PageRank, in order to obtain 
standardized values for both graphs, we 
normalize the rank values by applying the 
following equation: 
 
??? = ???/???(??) (2) 
Where ???(??)  obtains the maximum rank 
value of ?? vector. 
3.3 Adjusting sentiment polarity values 
After applying the PageRank algorithm on ???? 
and ???? , and having normalized their ranks, 
we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. ??	is represented by ???  lemmas, which would 
have, at that time, two assigned values: Positive, 
and Negative, which correspond to a calculated 
rank obtained by the PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (3) 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (4) 
Where ??? is the Positive value and ??? the 
Negative value related to each lemma in ??. 
In order to standardize the ??? and ??? values 
again and making them more representative in a 
[0?1] scale, we proceed to apply a 
normalization process over the ???  and ??? 
values. 
Following and based on the objective features 
commented by (Baccianella et al, 2010), we 
assume their same premise to establish objective 
values of the lemmas. Equation (5) is used to this 
96
 proceeding, where ???  represent the objective 
value. ??? = 1 ? |??? ? ???| (5) 
4 Sentiment Resource obtained 
At the same time we have obtained a ?? where 
each word is represented by ???, ??? and ??? 
values, acquired automatically from annotated 
sentiment corpora. With our proposal we have 
been able to discover new sentiment words in 
concordance of contexts in which the words 
appear. Note that the new obtained resource 
involves all lemmas identified into the annotated 
corpora. ???, ???, and ??? are nominal values 
between range [0? 	1]. 
5 Evaluation 
In the construction of the sentiment resource we 
used the annotated sentences provided from 
corpora described on Table 1. Note that we only 
used the sentences annotated positively and 
negatively. The resources involved into this table 
were a selection made to prove the functionality 
of the words annotation proposal of subjectivity 
and objectivity. 
The sentiment lexicons used were provided 
from WordNetAffect_Categories 3 and opinion-
words4 files and shown in detail in Table 2. 
Corpus Neg Pos Obj Neu Obj 
or Neu Unknow Total 
computational-
intelligence5 6982 6172 - - - - 13154 
tweeti-b-
sub.dist_out.tsv6 176 368 110 34 - - 688 
b1_tweeti-
objorneu-
b.dist_out.tsv6 
828 1972 788 1114 1045 - 5747 
stno7 1286 660 
 
384 - 10000 12330 
Total 9272 9172 898 1532 1045 10000 31919 
Table 1. Corpora used to apply RA-SR. 
Sources Pos Neg Total 
WordNet-Affects_Categories 
(Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words (Hu and Liu, 2004; Liu 
et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 2. Sentiment Lexicons. 
Some issues were taking into account through 
this process. For example, after obtaining a 
                                                 
3
 http://wndomains.fbk.eu/wnaffect.html 
4
 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
5
 A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
6
 Train dataset of Semeval-2013 (Task 2. Sentiment 
Analysis in Twitter, subtask b.) 
7
 Test dataset of NTCIR Multilingual Opinion Analysis 
Task (MOAT) http://research.nii.ac.jp/ntcir/ntcir-
ws8/meeting/ 
contextual graph ? factotum words are present in 
mostly of the involved sentences (i.e. verb ?to 
be?). This aspect is very dangerous after 
applying PageRank algorithm, because this 
algorithm because this algorithm strengthens the 
nodes possessing many linked elements. For that 
reason, the subtractions ??? ? ???  and ??? ????  are applied, where the most frequently 
words in all contexts obtains high values and 
being the subtraction a damping factor.  
Following an example; when we take the verb 
?to be?, before applying equation (2), verb ?to 
be? archives the highest values into each context 
graph (????  and ???? ), 9.94 and 18.67 rank 
values respectively. These values, applying 
equation (2), are normalized obtaining both ???	 = 	1  and ???	 = 	1  in a range [0...1]. 
Finally, when the next steps are executed 
(Equations (3) and (4)) verb ?to be? 
achieves ???	 = 0 , ??? = 0  and 
therefore 	???	 = 1 . Through this example it 
seems as we subjectively discarded words that 
appear frequently in both contexts (Positive and 
Negative contexts). 
Using the corpora from Table 1 we obtain 
25792 sentimentally annotated lemmas with ???, ??? and ???  features. Of them 12420 positive 
and 11999 negative lemmas were discovered, , 
and 1373 words already derived from existing 
lexical resources. 
Another contribution has been the ??? , ??? 
and ???  scores assigned to words of lexical 
inventory, which were used to reinforce the 
contextual graphs in the building process. Those 
words in concordance to our scenario count 842 
Positives and 383 Negatives.  
5.1 Sentiment Resource Applied on 
Sentiment Analysis 
To know if our method offers resources that 
improve the SA state of the art, we propose a 
baseline supported on the sentiment dictionaries, 
and other method (Ranking Sentiment Resource 
(RSR)) supported over our obtained resource. 
The baseline consists on analyzing sentences 
applying Equation (6) and Equation (7). 
?????????? = ?????????????????	 (6) 
?????????? = ?????????????????	 (7) 
Where: ???????? is the total of positive words 
(aligned with the sentiment dictionaries) in the 
sentence; ????????  is the total of negative 
words (aligned with the sentiment dictionaries) 
97
 in the sentence; ?????????  is the total of 
words in the sentence.  
Using these measures over the analyzed 
sentences, for each sentence, we obtain two 
attributes, ?????????? and	??????????; and 
a third attribute (named Classification) 
corresponding to its classification. 
On the other hand, we propose RSR. This SA 
method uses in a different way the Equation (6) 
and Equation (7), and introduces Equation (8).  
?????????? = ?????????????????	 (8) 
Being ???????? the sum of Positive ranking 
values of the sentence words, aligned with the 
obtained resource (??); ????????  the sum of 
Negative ranking values of the sentence words, 
aligned with the obtained resource (?? ); and ???????  the sum of Objective ranking values 
of the sentence words, aligned with the obtained 
resource (??). 
In RSR method we proved with two approach, 
RSR (1/di) and RSR (1-(1/di)). The first approach 
is based on a resource developed using 
PageRank with  ??,? = 1/??   and the other 
approach is using ??,? = 1 ? (1/??) . Table 3 
shows experimentation results. 
The evaluation has been applied over a corpus 
provided by ?Task 2. Sentiment Analysis in 
Twitter, subtask b?, in particular tweeti-b-
sub.dist_out.tsv file. This corpus contains 597 
annotated phrases, of them Positives (314), 
Negatives (155), Objectives (98) or Neutrals 
(30). For our understanding this quantity of 
instances offers a representative perception of 
RA-SR contribution; however we will think to 
evaluate RA-SR over other corpora in further 
researches. 
 
C I R. Pos (%) 
R. Neg 
(%) 
R. Obj 
(%) 
R. 
Neu 
(%) 
Total 
P. 
(%) 
Total 
R. 
(%) 
Baseline 366 231 91.1 51.6 0.0 0.0 48.2 61.3 
RSR(1/di) 416 181 87.3 39.4 80.6 6.7% 67.8 69.7 
RSR(1-(1/di) 469 128 88.5 70.3 81.6 6.7% 76.8 78.6 
Table 3. Logistic function (Cross-validation 10 folds) 
over tweeti-b-sub.dist_out.tsv8 corpus (597 instances). 
Recall (R), Precision (P), Correct (C), Incorrect (I). 
As we can see the baseline only is able to 
dealing with negative and positive instances. Is 
important to remark that our proposal starting up 
knowing only the words used in baseline and is 
able to growing sentiment information to other 
words related to them. We can see this fact on 
                                                 
8
 Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b.) 
Table 3, RSR is able to classify objective 
instances over 80% of Recall and the baseline 
does not.  
Other relevant element is the recall difference 
between RSR (1/di) and RSR (1 ? (1/??) . 
Traditionally (1/??) result value has been 
assigned to ? in PageRank algorithm. We have 
demonstrated that in lexical contexts RSR (1-
(1/di)) approach offers a better performance of 
PageRank algorithm, showing recall differences 
around 10 perceptual points. 
6 Conclusion and further works 
As a conclusion we can say that our proposal is 
able to automatically increase sentiment 
information, obtaining 25792 sentimentally 
annotated lemmas with ??? , ???  and ??? 
features. Of them 12420 positive and 11999 
negative lemmas were discovered. 
In other hand, The RSR is capable to classify 
objective instances over 80% and negatives over 
70%. We cannot tackle efficiently neutral 
instances, perhaps it is due to the lack of neutral 
information in the sentiment resource we used. 
Also, it could be due to the low quantity of 
neutral instances in the evaluated corpus. 
In further research we will evaluate RA-SR 
over different corpora, and we are also going to 
deal with the number of neutral instances. 
The variant RSR (1 ? (1/??)  performs better 
than RSR(1/??) one. This demonstrates that in 
lexical contexts using PageRank with ??,? = 1 ?(1/??) offers a better performance. Other further 
work consists in exploring Social Medias to 
expand our retrieved sentiment resource 
obtaining real time evidences that occur in Web 
2.0. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
98
 Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. p.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 
p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and 
Computing Systems. Alacant, Univeristy of 
Alacant, 2011. 299. p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. 
Martinez-Barco. The OpAL System at NTCIR 8 
MOAT. Proceedings of NTCIR-8 Workshop 
Meeting, Tokyo, Japan., 2010. 241-245 p.  
Balahur, A. and A. Montoyo. Applying a culture 
dependent emotion trigger database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 2008a. p.  
Balahur, A. and A. Montoyo. Building a 
recommender system using community level social 
filtering. 5th International Workshop on Natural 
Language and Cognitive Science (NLPCS), 2008b. 
32-41 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer 
Networks and ISDN Systems, 1998, 30(1-7): 107-
117. 
Cerini, S.; V. Compagnoni; A. Demontis; M. 
Formentelli and G. Gandini Language resources 
and linguistic theory: Typology, second language 
acquisition, English linguistics (Forthcoming), 
chapter Micro-WNOp: A gold standard for the 
evaluation of automatically compiled lexical 
resources for opinion mining., 2007. 
Cilibrasi, R. L. and P. M. B. Vit?nyi The Google 
Similarity Distance IEEE TRANSACTIONS ON 
KNOWLEDGE AND DATA ENGINEERING, 
2007, VOL. 19, NO 3. 
Esuli, A. and F. Sebastiani. SentiWordNet: A Publicly 
Available Lexical Resource for Opinion Mining. 
Fifth international conference on Languaje 
Resources and Evaluation Genoa - ITaly., 2006. 
417-422 p.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000. 
p.  
Hu, M. and B. Liu. Mining and Summarizing 
Customer Reviews. Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), USA, 
2004. p.  
Kim, S.-M. and E. Hovy. Extracting Opinions, 
Opinion Holders, and Topics Expressed in Online 
News Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the 
association for computational linguistics 
(COLING/ACL 2006), Sydney, Australia, 2006. 1-
8 p.  
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005. p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Pang, B.; L. Lee and S. Vaithyanathan. Thumbs up? 
Sentiment Classification using machine learning 
techniquies. EMNLP -02, the Conference on 
Empirical Methods in Natural Language 
Processing, USA, 2002. 79-86 p.  
Popescu, A. M. and O. Etzioni. Extracting product 
features and opinions from reviews. Proccedings of 
HLT-EMNLP, Canada, 2005. p.  
Stone, P.; D. C.Dumphy; M. S. Smith and D. M. 
Ogilvie The General Inquirer: A Computer 
Approach to Content Analysis The MIT Press, 
1996. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p.  
Turney, P. D. Thumbs up or thumbs down? Semantic 
orientation applied to unsupervised classification 
of reviews. Proceeding 40th Annual Meeting of the 
Association for Computational Linguistic. ACL 
2002, USA, 2002. 417-424 p.  
Wiebe, J. Tracking point of view in narrative 
Computational Linguistic, 1994, 20(2): 233-287. 
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in 
Language. Kluwer Academic Publishers, 
Netherlands, 2005. p.  
99
