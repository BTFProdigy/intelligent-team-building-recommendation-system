Proceedings of NAACL HLT 2007, Companion Volume, pages 5?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic acquisition of grammatical types for nouns 
 
N?ria Bel Sergio Espeja Montserrat Marimon 
IULA 
Universitat Pompeu Fabra 
P. de la Merc?, 10-12 
ES-08002 ? Barcelona 
{nuria.bel,sergio.espeja,montserrat.marimon}@upf.edu 
 
 
 
 
Abstract 
The work1 we present here is concerned 
with the acquisition of deep grammati-
cal information for nouns in Spanish. 
The aim is to build a learner that can 
handle noise, but, more interestingly, 
that is able to overcome the problem of 
sparse data, especially important in the 
case of nouns. We have based our work 
on two main points. Firstly, we have 
used distributional evidences as fea-
tures. Secondly, we made the learner 
deal with all occurrences of a word as a 
single complex unit. The obtained re-
sults show that grammatical features of 
nouns is a level of generalization that 
can be successfully approached with a 
Decision Tree learner. 
1 Introduction 
Our work aims to the acquisition of deep gram-
matical information for nouns, because having in-
formation such as countability and complementa-
tion is necessary for different applications, espe-
cially for deep analysis grammars, but also for 
question answering, topic detection and tracking, 
etc.  
Most successful systems of deep lexical acquisi-
tion are based on the idea that distributional fea-
tures (i.e. the contexts where words occur) are as-
sociated to concrete lexical  types. The difficulties 
                                                          
1 This research was supported by the Spanish Ministerio de Educaci?n y Cien-
cia: project AAILE, HUM2004-05111-C02-01/FILO, Ram?n y Cajal, Juan de la 
Cierva Programs and PTA-CTE/1370/2003 with Fondo Social Europeo,. 
are, on the one hand, that some filtering must be 
applied to get rid of noise, that is, contexts wrongly 
assessed as cues of a given type and, on the other 
hand, that for a pretty large number of words, their  
occurrences in a corpus of any length are very few, 
making statistical treatment very difficult. 
The phenomenon of noise is related to the fact 
that one particular context can be a cue of different 
lexical types. The problem of sparse data is pre-
dicted by the Zipfian distribution of words in texts: 
there is a large number of words likely to occur a 
very reduced number of times in any corpus. Both 
of these typical problems are maximized in the 
case of nouns.  
The aim of the work we present here is to build 
a learner that can handle noise, but, more interest-
ingly, that is able to overcome the problem of 
sparse data. The learner must predict the correct 
type both when there is a large number of occur-
rences as well as when there are only few occur-
rences, by learning on features that maximize gen-
eralization capacities of the learner while control-
ling overfitting phenomena.  
We have based our work on two main points. 
Firstly, we have used morphosyntactic information 
as features. Secondly, we made the learner deal 
with all occurrences of a word as a complex unit. 
In our system, linguistic cues of every occurrence 
are collected in the signature of the word (more 
technically a pair lema + part of speech) in a par-
ticular corpus. In the next sections we give further 
details about the features used, as well as about the 
use of signatures. 
The rest of the paper is as follows. Section 2 
presents an overview of the state of the art in deep 
lexical acquisition. In section 3, we introduce de-
tails about our selection of linguistically motivated 
5
cues to be used as features for training a Decision 
Tree (DT). Section 4 shortly introduces the meth-
odology and data used in the experiments whose 
results are presented in section 5. And in section 6 
we conclude by comparing with the published re-
sults for similar tasks and we sketch future re-
search.  
2 State of the art 
Most of the work on deep lexical information 
acquisition has been devoted to verbs. The existing 
acquisition systems learn very specialized linguis-
tic information such as verb subcategorization 
frame2. The results for verb subcategorization are 
mostly around the 0.8 of precision. Briscoe & Car-
roll (1997) reported a type precision of 0,76 and a 
type recall of 0.43. Their results were improved by 
the work of Korhonen (2002) with a type precision 
of 0.87 and a recall of 0.68 using external re-
sources to filter noise. Shulte im Walde (2002) re-
ports a precision of 0.65 and a recall of 0.58. 
Chesley & Salmon-Alt (2006) report a precision of 
0.86 and a recall of 0.54 for verb subcategorization 
acquisition for French.  
Lexical acquisition for nouns has been con-
cerned mainly with ontological classes and has 
mainly worked on measuring semantic similarity 
on the basis of occurrence contexts. As for gram-
matical information, the work of Baldwin and 
Bond (2003) in acquisition of countability features 
for English nouns also tackles the very important 
problem of feature selection. Other work like Car-
roll and Fang?s (2004) and Baldwin?s (2005) have 
focused on grammatical information acquisition 
for HPSG based computational grammars. The 
latter is the most similar exercises to our work. 
Baldwin (2005) reports his better results in terms 
of type accuracy has been obtained by using syn-
tactic information in a chunked and parsed corpus. 
The type F-scores for the different tested catego-
ries for English were: for verbs 0.47, for nouns 0.6  
and for adjectives 0.832.  
3 Feature selection  
One of the most important tasks in developing 
machine learning applications is the selection of 
                                                          
2 Given the argument-adjunct distinction, subcategorization 
concerns the specification for a predicate of the number and 
type of arguments which it requires for well-formedness. 
the features that leads to the smallest classification 
error. For our system, we have looked at distribu-
tional motivated features that can help in discrimi-
nating the different types that we ultimately use to 
classify words.  
The lexical types used in deep analysis gram-
mars are linguistic generalizations drawn from the 
distributional characteristics of particular sets of 
words. For the research we present here, we have 
taken the lexicon of a HPSG-based grammars de-
veloped in the LKB platform (Copestake, 2002) for 
Spanish, similarly to the work of Baldwin (2005). 
In the LKB grammatical framework, lexical types 
are defined as a combination of features. Lexical 
typology of nouns for Spanish, for instance, can be 
seen as a cross-classification of noun countability 
vs. mass distinctions, and subcategorization frame 
or valence, including prepositional selection.  For 
example nouns as ?temor? (?fear?) and ?adicci?n? 
(?adiction) belong to the type 
n_ppde_pcomp_a_count as they take two com-
plements: one with de and the other with a bound 
preposition a, as in ?El temor de la ni?a a los fan-
tasmas? (?The girl?s fear to ghosts?) vs. ?La adic-
ci?n a la coca?na? (?The addiction to cocaine?).  
We decided to carry out the classification for 
each of the grammatical features that conform the 
cross-classified types as a better level of generali-
zation than the type: mass and countable, on the 
one hand and, on the other hand, for subcategoriza-
tion information three further basic features: trans, 
for nouns with thematic complements introduced 
by the preposition de, intrans, when the noun can 
appear with no complements and pcomp for nouns 
having complements introduced by a bound prepo-
sition. The complete type can be recomposed with 
the assigned features. ?Temor? and ?adicci?n? will 
be examples of trans and pcomp_a. They both 
have also to be assigned the feature countable. The 
combination of features assigned corresponds to 
the final type which is a definition of the complete 
behaviour of the noun with respect, for instance, 
optional complements.  
We have used 23 linguistic cues, that is, the pat-
terns of contexts that can be indicative of a particu-
lar feature. The most frequent cue that can be re-
lated to countable is for the noun to be found with 
plural morphology. A singular noun without de-
terminer after a verb or a preposition is a cue of the 
noun being mass: ?hay barro en el sal?n? (?there is 
mud in the living room?) vs. ?hay hombres en el 
6
sal?n? (?there are men in the living room?). A fur-
ther cue for mass is the presence of particular 
quantifiers, such as ?m?s? (?more?), ?menos? 
(?less?), etc. But these cues, based on a collection 
of lexical items, are less productive than other 
characteristics such as morphological number or 
presence of determiners, as they appear very 
scarcely in texts. Nevertheless, we should mention 
that most of mass nouns in Spanish can also appear 
in the contexts of countables, as in the case of 
?beer? when in constructions such as ?three beers, 
please?.   
More difficult was to find cues for identifying 
the transitive nature of a noun. After some empiri-
cal work, we found a tendency of argumental com-
plements to have a definite article: ?temor de la 
ni?a? (?fear of the girl?), while modifiers tend to 
appear without determiners: ?mesa de juegos? (?ta-
ble of games?). Besides, we have taken as a cue the 
morphological characteristics of deverbal nouns. 
Suffixes such as ?-ci?n?, ?-si?n?, and ?-miento?, 
are very much indicative of transitive nouns. Fi-
nally, to find the bound preposition of comple-
ments, we used a pattern for each possible preposi-
tion found after the noun in question. 
We used Regular Expressions to implement the 
linguistic motivated patterns that check for the in-
formation just mentioned in a part of speech tagged 
corpus. The various patterns determine whether the 
linguistic cues that we have related to syntactic 
features are found in each occurrence of a particu-
lar word in a corpus. The positive or negative re-
sults of the n pattern checking are stored as binary 
values of a n dimensional vector, one for each oc-
currence. All vectors produced, one per occurrence 
of the word in question, are stored then in a kind of 
vector of vectors that we have called its signature.  
The term signature wants to capture the notion that 
the data it embodies is truly representative of a par-
ticular item, and that shows the details of its typical 
behavior. Particularly, we wanted linguistic cues 
appearing in different occurrences of the same 
word to be observed as related information. We 
have not dealt with ambiguity at all, however. One 
of the reasons was our focus on low frequency 
nouns. 
4 Methodology and data 
We have worked with the Corpus T?cnic de 
l?IULA, a multilingual part of speech tagged corpus 
which consists of domain specific texts. The sec-
tion used for our evaluation was the Spanish with 
1,091,314 words in the domain of economy and 
4,301,096 for medicine. A dataset of 289 nouns, 
present in both subcorpora, was selected. It was 
important to compare the behavior of the same 
nouns in both corpus to check whether the learner 
was subject to unwanted overfitting.  
We used the data for building a C4.5 DT clas-
sifier3. DT?s are one well known and successful 
technique for this class of tasks when there is 
enough pre-annotated data available. DT?s have 
the additional benefit that the results can be in-
spected. The signatures of the words in the Gold-
Standard lists were extracted from the corpus of 
medicine and of the economy one. There was a 
further test set of 50 nouns with a single occur-
rence in the corpus of economy for testing pur-
poses. The DT was trained with the signatures of 
the economy corpus, and the medicine ones as well 
as the singles set were used for testing.  
5 Evaluation 
The purpose of the evaluation was to validate our 
system with respect to the two problems men-
tioned: noise filtering and generalization capacity 
by measuring type precision and type recall. We 
understand type precision as a measure of the noise 
filtering success, and recall as a measure of the 
generalization capacity.  
In the following tables we present the results of 
the different experiments. In Table 1, there is a 
view of the results of the experiment after training 
and testing with the signatures got in the smaller 
corpus. The results are for the assignment of the 
grammatical feature for the two values, yes and no. 
And the column named global refers to the total 
percentage of correctly classified instances. 
 
  yes no 
lt global prec. rec.  F prec. rec. F 
MASS 0.67 0.4 0.26 0.31 0.73 0.83 0.78
COUNT 0.96 0.97 0.99 0.98 0 0 0 
TRANS 0.85 0.73 0.45 0.55 0.86 0.95 0.91
INT 0.81 0.84 0.94 0.89 0.64 0.32 0.48
PCOMP 0.9 0.4 0.08 0.13 0.91 0.98 0.95
Table 1. DT results of economy signatures for 
training and test 
                                                          
3 We have used WEKA J48 decision tree classifier (Witten and Frank, 2005). 
7
 
The most difficult task for the learner is to iden-
tify nouns with bound prepositions. Note that there 
are only 20 nouns with prepositional complements 
of the 289 test nouns, and that the occurrence of 
the preposition is not mandatory, and hence the 
signatures are presented to the learner with very 
little information.  
Table 2 shows the results for 50 nouns with only 
one occurrence in the corpus. The performance 
does not change significantly, showing that the 
generalization capacity of the learner can cope 
with low frequency words, and that noise in larger 
signatures has been adequately filtered. 
 
  yes no 
lt global prec. rec.  F prec. rec. F 
MASS 0.71 0.5 0.16 0.25 0.73 0.93 0.82
COUNT 0.97 0.97 1 0.98 0 0 0 
TRANS 0.85 0.75 0.46 0.57 0.87 0.96 0.91
INT 0.83 0.85 0.95 0.89 0.70 0.41 0.52
PCOMP 0.91 0 0 0 0.91 1 0.95
Table 2. DT results for training with signatures of 
the economy corpus and testing 50 unseen nouns 
with a single occurrence as test 
 
Table 3 shows that there is little variation in the 
results of training with signatures of the economy 
corpus and testing with ones of the medicine cor-
pus. As expected, no variation due to domain is 
relevant as the information learnt should be valid 
in all domains.  
 
  yes no 
lt global prec. rec. F prec. rec. F 
MASS 0.65 0.44 0.53 0.48 0.77 0.70 0.73
COUNT 0.97 0.97 1 0.98 0 0 0 
TRANS 0.82 0.62 0.47 0.54 0.86 0.92 0.89
INT 0.78 0.82 0.92 0.86 0.58 0.35 0.43
PCOMP 0.81 0.31 0.28 0.29 0.92 0.93 0.93
Table 3. DT results for training with economy sig-
natures and testing with medicine signatures 
6  Conclusions 
The obtained results show that the learning of 
grammatical features of nouns are learned success-
fully when using distributional linguistic informa-
tion as learning features that allow the learner to 
generalize so as to maintain the performance in 
cases of nouns with just one occurrence.  
There are however issues that should be further 
investigated. Grammatical features with low preci-
sion and recall results (mass and pcomp) show that 
some more research should be carried out for find-
ing relevant linguistic cues to be used as learning 
features. In that respect, the local cues based on 
morphosyntactic tagging have proved to be useful, 
minimizing the text preprocessing requirements for 
getting usable results. 
Acknowledgements 
The authors would like to thank Jordi Porta, 
Daniel Chicharro and the anonymous reviewers for 
helpful comments and suggestions. 
References  
Baldwin, T. 2005. ?Bootstrapping Deep Lexical Re-
sources: Resources for Courses?, ACL-SIGLEX 2005. 
Workshop on Deep Lexical Acquisition. 
Baldwin, T. and F. Bond. 2003. ?Learning the Count-
ability of English Nouns from Corpus Data?. Pro-
ceedings of the 41st. Annual Meeting of the ACL.  
Briscoe, T. and J. Carroll. 1997.  ?Automatic extraction 
of subcategorization from corpora?. In Proceedings 
of the Fifth Conference on Applied Natural Process-
ing, Washington.  
Carroll, J. and A. Fang. 2004. ?The automatic acquisi-
tion of verb subcategorisations and their impact on 
the performance of an HPSG parser?. In Proceedings 
of the 1st International Joint Conference on Natural 
Language Processing (IJCNLP), Sanya City, China.  
Chesley, P and S. Salmon-Alt. 2006. ?Automatic extrac-
tion of subcategorization frames for French?. In 
Proc. of the LREC Conference, Genoa. 
Copestake, A.. 2002. Implementing Typed Feature 
Structure Grammars. CSLI Publications. 
Korhonen, A. 2002. ?Subcategorization acquisition?. As 
Technical Report UCAM-CL-TR-530, University of 
Cambridge, UK. 
Shulte im Walde, S. 2002. ?Evaluating verb subcate-
gorization frames learned by a German statistical 
grammar against manual definitions in the Duden 
Dictionary?. In Proceedings of the 10th EURALEX In-
ternational Congress, 187-197. 
Witten, Ian H. and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques. 2nd 
Edition, Morgan Kaufmann, San Francisco. 
8
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 105?111,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Spanish Resource Grammar: pre-processing strategy and lexical acquisi-
tion 
Montserrat Marimon, N?ria Bel, Sergio Espeja, Natalia Seghezzi 
IULA - Universitat Pompeu Fabra 
Pl. de la Merc?, 10-12 
 08002-Barcelona 
{montserrat.marimon,nuria.bel,sergio.espeja,natalia.seghezzi}@upf.edu
 
 
Abstract 
This paper describes work on the develop-
ment of an open-source HPSG grammar for 
Spanish implemented within the LKB sys-
tem. Following a brief description of the 
main features of the grammar, we present 
our approach for pre-processing and on-
going research on automatic lexical acqui-
sition.1   
1 Introduction 
In this paper we describe the development of the 
Spanish Resource Grammar (SRG), an open-
source 2  medium-coverage grammar for Spanish. 
The grammar is grounded in the theoretical 
framework of HPSG (Head-driven Phrase Struc-
ture Grammar; Pollard and Sag, 1994) and uses 
Minimal Recursion Semantics (MRS) for the se-
mantic representation (Copestake et al 2006). The 
SRG is implemented within the Linguistic Knowl-
edge Building (LKB) system (Copestake, 2002), 
based on the basic components of the grammar 
Matrix, an open?source starter-kit for the devel-
opment of HPSG grammars developed as part of 
the LinGO consortium?s multilingual grammar 
engineering (Bender et al, 2002).  
The SRG is part of the DELPH-IN open-source 
repository of linguistic resources and tools for 
writing (the LKB system), testing (The [incr 
tsbd()]; Oepen and Carroll, 2000) and efficiently 
                                                 
                                                
1 This research was supported by the Spanish Ministerio de 
Educaci?n y Ciencia: Project AAILE (HUM2004-05111-C02-
01), Ramon y Cajal, Juan de la Cierva programmes and PTA-
CTE/1370/2003 with Fondo Social Europeo. 
2 The Spanish Resource Grammar may be downloaded from: 
http://www.upf.edu/pdi/iula/montserrat.marimon/. 
processing HPSG grammars (the PET system; 
Callmeier, 2000). Further linguistic resources that 
are available in the DELPH-IN repository include 
broad-coverage grammars for English, German and 
Japanese as well as  smaller grammars for French, 
Korean, Modern Greek, Norwegian and 
Portuguese .3   
The SRG has a full coverage of closed word 
classes and it contains about 50,000 lexical entries 
for open classes (roughtly: 6,600 verbs, 28,000 
nouns, 11,200 adjectives and 4,000 adverbs).  
These lexical entries are organized into a type 
hierachy of about 400 leaf types (defined by a type 
hierarchy of  around 5,500 types). The grammar 
also has 40 lexical rules to perform valence 
changing operations on lexical items and 84 
structure rules to combine words and phrases into 
larger constituents and to compositionally build up 
the semantic representation.  
We have been developing the SRG since 
January 2005. The range of linguistic phenomena 
that the grammar handles includes almost all types 
of subcategorization structures, valence 
alternations,  subordinate clauses, raising and 
control, determination,  null-subjects and 
impersonal constructions, compound tenses, 
modification,  passive constructions, comparatives 
and superlatives, cliticization, relative and 
interrogative clauses and sentential adjuncts, 
among others. 
Together with the linguistic resources (grammar 
and lexicon) we provide a set of controlled hand-
constructed test suites. The construction of the test 
suites plays a major role in the development of the 
SRG, since test suites provide a fine-grained diag-
 
3 See . http://www.delph-in.net/
 
105
nosis of grammar performance and they allow us to 
compare the SRG with other DELPH-IN gram-
mars. In building the test suites we aimed at (a) 
testing specific phenomena in isolation or in con-
trolled interaction, (b) providing test cases which 
show systematic and exhaustive variations over 
each phenomenon, including infrequent phenom-
ena and variations, (c) avoiding irrelevant variation 
(i.e. different instances of the same lexical type), (d) 
avoiding ambiguity, and (e) including negative or 
ungrammatical data. We have about 500 test cases 
which are distributed by linguistic phenomena (we 
have 17 files). Each test case includes a short lin-
guistic annotation describing the phenomenon and 
the number of expected results when more than 
one analysis cannot be avoided (e.g. testing op-
tionality). 
Test suites are not the only source of data we 
have used for testing the SRG. Hand-constructed 
sentences were complemented by real corpus cases 
from: (a) the Spanish questions from the Question 
Answering track at CLEF (CLEF-2003, CLEF-
2004, CLEF-2005 and CLEF-2006), and (b) the 
general sub-corpus of the Corpus T?cnic de 
l?IULA (IULA?s Technical Corpus; Cabr? and 
Bach, 2004); this sub-corpus includes newspaper 
articles and it has been set up for contrastive 
studies. CLEF cases include short queries showing 
little interaction of phenomena and an average of 
9.2 words; newspaper articles show a high level of 
syntactic complexity and interaction of phenomena, 
sentences are a bit longer, ranging up to 35 words. 
We are currently shifting to much more varied 
corpus data of the Corpus T?cnic de l?IULA, which 
includes specialized corpus of written text in the 
areas of computer science, environment, law, 
medicine and economics, collected from several 
sources, such as legal texts, textbooks, research 
reports, user manuals, ? In these texts sentence 
length may range up to 70 words.  
The rest of the paper describes the pre-
processing strategy we have adopted and on our 
on-going research on lexical acquisition. 
2 Pre-processing in the SRG 
Following previous experiments within the 
Advanced Linguistic Engineering Platform (ALEP) 
platform (Marimon, 2002), we have integrated a 
shallow processing tool, the FreeLing tool, as a 
pre-processing module of the grammar.  
The FreeLing tool is an open-source4 language 
analysis tool suite (Atserias et al, 2006) perfoming 
the following functionalities (though 
disambiguation, named entity classification and the 
last three functionalities have not been integrated):  
 
? Text tokenization (including MWU and 
contraction splitting). 
? Sentence splitting. 
? Morpho-syntactic analysis and 
disambiguation. 
n. 
                                                
? Named entity detection and classification. 
? Date/number/currency/ratios/physical 
magnitude (speed, weight, temperature, 
density, etc.) recognitio
? Chart-based shallow parsing. 
? WordNet-based sense annotation.  
? Dependency parsing.  
FreeLing also includes a guesser to deal with 
words which are not found in the lexicon by 
computing the probability of each possible PoS tag 
given the longest observed termination string for 
that word. Smoothing using probabilities of shorter 
termination strings is also performed. Details can 
be found in Brants (2000) and Samuelson (1993).  
Our system integrates the FreeLing tool by 
means of the LKB Simple PreProcessor Protocol 
(SPPP; http://wiki.delph-in.net/moin/LkbSppp), 
which assumes that a preprocessor runs as an 
external process to the LKB system, and uses the 
LKB inflectional rule component to convert the 
PoS tags delivered by the FreeLing tool into partial 
descriptions of feature structures. 
2.1 The integration of PoS tags 
The integration of the morpho-syntactic analysis in 
the LKB system using the SPPP protocol means 
defining inflectional rules that propagate the mor-
pho-syntactic information associated to full-forms, 
in the form of PoS tags, to the morpho-syntactic 
features of the lexical items. (1) shows the rule 
propagating the tag AQMS (adjective qualitative 
masculine singular) delivered by FreeLing. Note 
 
4 The FreeLing tool may be downloaded from 
http://www.garraf.epsevg.upc.es/freeling/. 
106
that we use the tag as the rule identifier (i.e. the 
name of the inflectional rule in the LKB).  
(1) aqms :=  
 %suffix () 
 [SYNSEM.LOCAL[CAT adj, 
               AGR.PNG[PN 3sg, 
                     GEN masc]]] 
 
In Spanish, when the verb is in non-finite form, 
such as infinitive or gerund, or it is in the impera-
tive, clitics5 take the form of enclitics. That is, they 
are attached to the verb forming a unique word, 
e.g. hacerlo (hacer+lo; to do it), gustarle (gus-
tar+le; to like to him). FreeLing does not split 
verbs and pronouns, but uses complex tags that 
append the tags of each word. Thus, the form ha-
cerlo gets the tag VMN+PP3MSA (verb main in-
finitive + personal pronoun 3rd masculine singular 
accusative). In order to deal with these complex 
tags, the SRG includes a series of rules that build 
up the same type of linguistic structure as that one 
built up with the structure rules attaching affixes to 
the left of verbal heads. Since the application of 
these rules is based on the tag delivered by FreeL-
ing, they are included in the set of inflectional rules 
and they are applied after the set of rules dealing 
with complement cliticization.   
 Apart from avoiding the implementation of in-
flectional rules for such a highly inflected lan-
guage, the integration of the morpho-syntactic 
analysis tags will allow us to implement default 
lexical entries (i.e. lexical entry templates that are 
activated when the system cannot find a particular 
lexical entry to apply) on the basis of the category 
encoded to the lexical tag delivered by FreeLing, 
for virtually unlimited lexical coverage. 6
2.2 The integration of multiword expressions 
All multiword expressions in FreeLing are stored 
in a file. The format of the file is one multiword 
per line, having three fields each: form, lemma and 
PoS.7 (2) shows two examples of multiword fixed 
                                                 
                                                
5 Actually, Spanish weak pronouns are considered pronominal 
affixes rather than pronominal clitics. 
6 The use of underspecified default lexical entries in a 
highly lexicalized grammar, however, may increase 
ambiguity and overgeneration (Marimon and Bel, 
2004). 
7 FreeLing only handles continuous multiword expres-
sions. 
expressions; i.e. the ones that are fully lexicalized 
and never show morpho-syntactic variation, a 
trav?s de (through) and a buenas horas (finally). 
 
(2) a_trav?s_de a_trav?s_de SPS00 
   a_buenas_horas a_buenas_horas RG 
 
The multiword form field may admit lemmas in 
angle brackets, meaning that any form with that 
lemma will be a valid component for the multi-
word. Tags are specified directly or as a reference 
to the tag of some of the multiword components. 
(3) builds a multiword with both singular and plu-
ral forms  (apartado(s) de correos (P.O Box)). The 
tag of the multiform is that of its first form ($1) 
which starts with NC and takes the values for 
number depending on whether the form is singular 
or plural.  
 
(3) <apartado>_de_correos apar-
tado_de _correos \$1:NC 
 
Both fixed expressions and semi-fixed expres-
sions are integrated by means of the inflectional 
rules that we have described in the previous sub-
section and they are treated in the grammar as 
word complex with a single part of speech.  
2.3 The integration of messy details and 
named entities 
FreeLing identifies, classifies and, when appropri-
ate, normalizes special text constructions that may 
be considered peripheral to the lexicon, such as 
dates, numbers, currencies, ratios, physical magni-
tudes, etc.  FreeLing also identifies and classifies 
named entities (i.e. proper names); however, we do 
not activate the classification functionality, since 
high performance of that functionality is only 
achieved with PoS disambiguated contexts.   
To integrate these messy details and named enti-
ties into the grammar, we require special inflec-
tional rules and lexical entry templates for each 
text construction tag delivered by FreeLing. Some 
of these tags are: W for dates, Z for numbers, Zm 
for currencies, ... In order to define one single en-
try for each text construct, we identify the tag and 
the STEM feature. (4) shows the lexical entry for 
dates.8
 
8 Each lexical entry in the SRG consists of a unique identifier, 
a lexical type, an orthography and a semantic relation. 
107
 
(4)  
date := date_le & 
[STEM <?w?>, 
SYNSEM.LKEY.KEYREL.PRED time_n_rel] 
 
The integration of these messy details allows us 
to release the analysis process from certain tasks 
that may be reliably dealt with by shallow external 
components.  
3 Automatic Lexical Acquisition 
We have investigated Machine Learning (ML) 
methods applied to the acquisition of the informa-
tion contained in the lexicon of the SRG. 
ML applied to lexical acquisition is a very active 
area of work linked to deep linguistic analysis due 
to the central role that lexical information has in 
lexicalized grammars and the costs of hand-
crafting them. Korhonen (2002), Carroll and Fang 
(2004), Baldwin (2005), Blunsom and Baldwin 
(2006), and Zhang and Kordoni (2006) are just a 
few examples of reported research work on deep 
lexical acquisition. 
The most successful systems of lexical acquisi-
tion are based on the linguistic idea that the con-
texts where words occur are associated to particu-
lar lexical types. Although the methods are differ-
ent, most of the systems work upon the syntactic 
information on words as collected from a corpus, 
and they develop different techniques to decide 
whether this information is relevant for type as-
signment or it is noise, especially when there are 
just a few examples. In the LKB grammatical 
framework, lexical types are defined as a combina-
tion of grammatical features. For our research, we 
have looked at these morpho-syntactically moti-
vated features that can help in discriminating the 
different types that we will ultimately use to clas-
sify words. Thus, words are assigned a number of 
grammatical features, the ones that define the lexi-
cal types. 
Table 1 and Table 2 show the syntactic features 
that we use to characterize 6 types of adjectives 
and 7 types of nouns in Spanish, respectively.9 As 
can be observed, adjectives are cross-classified 
according to their syntactic position within the NP, 
i.e. (preN(ominal)) vs  postN(ominal), the possibil-
ity of co-occurring in predicative constructions 
                                                 
9 The SRG has 35 types for nouns and 44 types for adjectives. 
(pred) and being modified by degree adverbs (G), 
and their subcategorization frame (pcomp); 
whereas lexical types for nouns are basically de-
fined on the basis of the mass/countable distinction 
and valence information. Thus, an adjective like 
bonito (nice), belonging to the type a_qual_intr, 
may be found both in pre-nominal and post-
nominal position or in predicative constructions, it 
may also be modified by degree adverbs, this type 
of adjectives, however, does not take comple-
ments. Nouns belonging to the type n_intr_count, 
like muchacha (girl), are countable intransitive 
nouns. 
 
TYPE/SF preN postN pred G pcomp 
a_adv_int yes no no no no 
a_adv_event yes yes no no no 
a_rel_nonpred no yes no no no 
a_rel_pred no yes yes no no 
a_qual_intr yes yes yes yes no 
a_qual_trans yes yes yes yes yes 
Table 1. Some adjectival types of the SRG 
 
TYPE/SF mass count intr trans pcomp 
n_intr_mass yes no yes no no 
n_intr_count no yes yes no no 
n_intr_cnt-
mss 
yes yes yes no no 
n_trans_mass yes no no yes no 
n_trans_count no yes no yes no 
n_ppde_pcom
p_count 
no yes no yes yes 
n_ppde_pcom
p_mss 
yes no no yes yes 
Table 2. Some nominal types of the SRG 
 
We have investigated two methods to automati-
cally acquire such linguistic information for Span-
ish nouns and adjectives: a Bayesian model and a 
decision tree. The aim of working with these two 
methods was to compare their performance taking 
into account that while the decision tree gets the 
information from previously annotated data, the 
Bayesian method learns it from the linguistic ty-
pology as defined by the grammar. These methods 
are described in the following subsections.  
3.1 A Bayesian model for lexical acquisition 
We have used a Bayesian model of inductive learn-
ing for assigning grammatical features to words 
occurring in a corpus. Given a hypothesis space 
(the linguistic features of words according to its 
lexical type) and one or more occurrences of the 
108
word to classify, the learner evaluates all hypothe-
ses for word features and values by computing 
their posterior probabilities, proportional to the 
product of prior probabilities and likelihood.  
In order to obtain the likelihood, grammatical 
features are related to the expected contexts where 
their instances might appear. The linguistic typol-
ogy provides likelihood information that is the 
learner?s expectation about which contexts are 
likely to be observed given a particular hypothesis 
of a word type. This likelihood is used as a substi-
tute of the computations made by observing di-
rectly the data, which is what a supervised machine 
learning method does. As said, our aim was to 
compare these two strategies.   
The decision on a particular word is determined 
by averaging the predictions of all hypothesis 
weighted by their posterior probabilities. More 
technically, for each syntactic feature {sf1, sf2, ..., 
sfn} of the set SF (Syntactic Features) represented 
in the lexical typology, we define the goal of our 
system to be the assignment of a value, {no, yes}, 
that maximizes the result of a function f: ?? SF, 
where ? is the collection of its occurrences (? = 
{v1, v2, ..., vz}), each being a n-dimensional vector. 
The decision on value assignment is achieved by 
considering every occurrence as a cumulative evi-
dence in favour or against of having each syntactic 
feature. Thus, our function Z?(SF, ?), shown in (5), 
will assess how much relevant information is got 
from all the vectors. A further function, shown in 
(8), will decide on the maximal value in order to 
assign sfi,x. 
(5)  ?= z
j j
vxisfPxisfZ )|,(),,(' ?
 
To assess P(sfi,x|vj), we use (6), which is the ap-
plication of Bayes Rule for solving the estimation 
of the probability of a vector conditioned to a par-
ticular feature and value.  
(6) 
?
=
k ki
sfPkisfjvP
xisfPxisfjvP
jvxisfP ),(),|(
),(),|(
)|,(
 
 
For solving (6), the prior P(sfi,x) is computed on 
the basis of a lexical typology too, assuming that 
what is more frequent in the typology will corre-
spondingly be more frequent in the data. For com-
puting the likelihood P(vj|sfi,x), as each vector is 
made of m components, that is, the linguistic cues 
vz = {lc1, lc2, ..., lcm}, we proceed as in (7) on the 
basis of P(lcl|sfi,x); i.e. the likelihood of finding the 
word in a particular context given a particular syn-
tactic feature. 
(7)  ?==
m
l xi
sfllcPxisfjvP 1
),|(),|(
 
Finally Z, as in (8), is the function that assigns 
the syntactic features to ? .10
 
(8)  
??
??
?
??
??
?
?=>=
?=>==
no
yesxi
sfZ
noxi
sfZ
yes
noxi
sfZ
yesxi
sfZ
Z
)|
,
(')|
,
('
)|
,
(')|
,
('
??
??
 
For computing the likelihood, we count on the 
conditional probabilities of the correlations be-
tween features as defined in the typology. We use 
these correlations to infer the expectation of ob-
serving the linguistic cues associated to particular 
syntactic features, and to make it to be conditional 
to a particular feature and value. However, linguis-
tic cues and syntactic features are in two different 
dimensions; syntactic features are properties of 
lexical items, while linguistic cues show the char-
acteristics of actual occurrences. As we assume 
that each syntactic feature must have at least one 
corresponding linguistic cue, we must tune the 
probability to acknowledge the factors that affect 
linguistic cues. For such a tuning, we have consid-
ered the following two issues: (i) to include in the 
assessments the known uncertainty of the linguistic 
cues that can be present in the occurrence or not; 
and (ii) to create a dummy variable to deal with the 
fact that, while syntactic features in the typology 
are independent from one another, evidences in 
text are not so. 
We have also observed that the information that 
can be gathered by looking at all word occurrences 
as a complex unit have a conclusive value. Take 
for instance the case of prepositions. The observa-
tion of a given prepositions in different occur-
rences of the same word is a conclusive evidence 
for considering it a bound preposition.  In order to 
take this into account, we have devised a function 
that acts as a dynamic weighting module. The 
function app_lc(sfi, ?) returns the number of con-
texts where the cue is found. In the case that in a 
                                                 
10 In the theoretical case of having the same probability 
for yes and for no, Z is undefined.  
109
particular signature there is no context with such a 
lc, it returns ?1?. Thus, app_lc is used to reinforce 
this conclusive evidence in (5), which is now (9). 
 
(9) 
 
),(_*)|,(),,(' ?? isflcapp
z
j j
vyesxisfPyesxisfZ ??
???
?? ===
 ? ===
z
j j
vnoxisfPnoxisfZ )|,(),,(' ?  
 
3.2 A Decision tree 
Linguistic motivated features have also been 
evaluated using a C4.5 Decision Tree (DT) classi-
fier (Quinlan, 1993) in the Weka implementation 
(Witten and Frank, 2005). These features corre-
spond to the expected contexts for the different 
nominal and adjectival lexical types. 
We have trained the DT with all the vectors of 
the word occurrences that we had in the different 
gold-standards, using their encoding for the super-
vised experiment in a 10-fold cross-validation test-
ing (Bel et al 2007).  
3.3 Evaluation and Results 
For the evaluation, we have applied both methods 
to the lexical acquisition of nouns and adjectives.  
We have worked with a PoS tagged corpus of 
1,091,314 words. Datasets of 496 adjectives and 
289 nouns were selected among the ones that had 
occurrences in the corpus. Some manual selection 
had to be done in order to have all possible types 
represented but still it roughly corresponds to the 
distribution of features in the existing lexicon. 
We evaluated by comparing with Gold-
standard files; i.e. the manually encoded lexicon of 
the SRG. The usual accuracy measures as type 
precision (percentage of feature values correctly 
assigned to all values assigned) and type recall 
(percentage of correct feature values found in the 
dictionary) have been used. F1 is the usual score 
combining precision and recall.  
Table 3 shows the results in terms of F1 score 
for the different methods and PoS for feature as-
signment. From these data, we concluded that the 
probabilistic information inferred from the lexical 
typology defined in our grammar is a good source 
of knowledge for lexical acquisition.  
 
 
PoS noun adj 
Z 0.88 0.87 
DT 0.89 0.9 
Table 3. F1 for different methods and PoS. 
 
Table 4 shows more details of the results compar-
ing between DT and Z for Spanish adjectives. 
 
 SF = no SF = yes 
 Z DT Z DT 
prep_a 0.98 0.97 0.72 0.44 
prep_en 0.98 0.99 0.27 0 
prep_con 0.99 0.99 0.60 0 
prep_para 0.98 0.99 0.51 0.53 
prep_de 0.88 0.97 0.34 0.42 
postN 0 0 0.99 0.99 
preN 0.75 0.83 0.44 0.80 
Pred 0.50 0.41 0.59 0.82 
G 0.85 0.80 0.75 0.72 
Sent 0.97 0.97 0.55 0.44 
Table 4. F1 for Spanish adjectival features. 
 
Finally, Table 5 shows the results for 50 Spanish 
nouns with only one occurrence in the corpus. 
These results show that grammatical features can 
be used for lexical acquisition of low frequency 
lexical items, providing a good hypothesis for en-
suring grammar robustness and adding no over-
generation to parsing results.  
 
 DT Z 
 prec. rec. F prec. rec. F 
MASS 0.50 0.16 0.25 0.66 0.25 0.36 
COUNT 0.97 1.00 0.98 1.00 0.96 0.98 
TRANS 0.75 0.46 0.57 0.68 0.73 0.71 
INTRANS 0.85 0.95 0.89 0.89 0.76 0.82 
PCOMP 0 0 0 0.14 0.20 0.16 
Table 5. Results of 50 unseen nouns with a sin-
gle occurrence. 
4 Future Work 
We have presented work on the development of an 
HPSG grammar for Spanish; in particular, we have 
described our approach for pre-processing and on-
going research on automatic lexical acquisition. 
Besides extending the coverage of the SRG and 
continuing research on lexical acquisition, the spe-
cific aims of our future work on the SRG are: 
? Treebank development. 
110
? To extend the shallow/deep architecture 
and integrate the structures generated by 
partial parsing, to provide robust techniques 
for infrequent structural constructions. The 
coverage of these linguistic structures by 
means of structure rules would increase both 
processing time and ambiguity.  
? To use ML methods for disambiguation; 
i.e. for ranking possible parsings according 
to relevant linguistic features, thus enabling 
the setting of a threshold to select the n-best 
analyses. 
? The development of error mining tech-
niques (van Noord, 2004) to identify errone-
ous and incomplete information in the lin-
guistic resources which cause the grammar 
to fail.  
References 
J. Atserias, B. Casas, E. Comelles, M. Gonz?lez, L. Pa-
dr? and M. Padr?. 2006. FreeLing 1.3: Syntactic and 
semantic services in an open-source NLP library. 5th 
International Conference on Language Resources 
and Evaluation. Genoa, Italy. 
T. Baldwin. 2005. Bootstrapping Deep Lexical Re-
sources: Resources for Courses, ACL-SIGLEX 2005. 
Workshop on Deep Lexical Acquisition. Ann Arbor, 
Michigan.  
N. Bel, S. Espeja, M. Marimon. 2007. Automatic Ac-
quisition of Grammatical Types for Nouns. Human 
Language Technologies: The Annual Conference of 
the North American Chapter of the Association for 
Computational Linguistics. Rochester, NY, USA, 
E.M. Bender, D. Flickinger and S. Oepen. 2002. The 
grammar Matrix. An open-source starter-kit for the 
rapid development of cress-linguistically consistent 
broad-coverage precision grammar. Workshop on 
Grammar Engineering and Evaluation, 19th Interna-
tional Conference on Computational Linguistics. 
Taipei, Taiwan.  
P. Blunsom and T. Baldwin. 2006. Multilingual Deep 
Lexical Acquisition for HPSGs via Supertagging. 
Conference on Empirical Methods in Natural Lan-
guage Processing. Sydney, Australia. 
T. Brants. 2000. TnT: A statistical part-of-speech tag-
ger. 6th Conference on Applied Natural Language 
Processing. Seattle, USA. 
T. Cabr? and C. Bach, 2004. El corpus t?cnic de 
l?IULA: corpus textual especializado pluriling?e. 
Panacea, V. 16, pages 173-176. 
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing. Journal of 
Natural Language Engineering 6(1): Special Issue 
on Efficient Processing with HPSG: Methods, Sys-
tem, Evaluation, pages 99-108. 
A. Copestake, D. Flickinger, C. Pollard and I.A. Sag. 
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation 
3.4:281-332. 
A. Copestake. 2002. Implementing Typed Features 
Structure Grammars. CSLI Publications.  
A. Korhonen. 2002. ?Subcategorization acquisition?. As 
Technical Report UCAM-CL-TR-530, University of 
Cambridge, UK. 
M. Marimon. 2002. Integrating Shallow Linguistic 
Processing into a Unification-based Spanish Gram-
mar. 9th International Conference on Computational 
Linguistics. Taipei, Taiwan.  
M. Marimon and N. Bel. 2004. Lexical Entry Templates 
for Robust Deep Parsing. 4th International Confer-
ence on Language Resources and Evaluation. Lis-
bon, Portugal. 
S. Oepen and J. Carroll. 2000. Performance Profiling for 
Parser Engineering. Journal of Natural Language 
Engineering 6(1): Special Issue on Efficient Process-
ing with HPSG: Methods, System, Evaluation, pages 
81-97. 
C.J. Pollard and I.A. Sag. 1994. Head-driven Phrase 
Structure Grammar. The University of Chicago 
Press, Chicago.  
R.J. Quinlan 1993. C4.5: Programs for Machine Learn-
ing. Series in Machine Learning. Morgan Kaufman, 
San Mateo, CA. 
C. Samuelson. 1993. Morphological tagging based en-
tirely on Bayesian inference. 9th Nordic Conference 
on Computational Linguistics. Stockholm, Sweden.  
I.H. Witten and E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques. Morgan 
Kaufmann, San Francisco. 
G. van Noord. 2004. Error mining for wide-coverage 
grammar engineering. 42th Annual Meeting of the 
ACL. Barcelona, Spain. 
Y. Zhang and V. Kordoni. 2006. Automated deep lexi-
cal acquisition for robust open text processing. 5th 
International Conference on Language Resources 
and Evaluation. Genoa, Italy. 
111
Integrating Shallow Linguistic Processing into a Unicationbased
Spanish Grammar
Montserrat Marimon
gilcUB
Grup dInvestigacio en Lingustica Computacional
Universitat de Barcelona
montsegilcubes
Abstract
This paper describes to what extent deep pro
cessing may benet from shallow processing
techniques and it presents a NLP system which
integrates a linguistic PoS tagger and chunker
as a preprocessing module of a broadcoverage
unicationbased grammar of Spanish Exper
iments show that the eciency of the overall
analysis improves signicantly and that our sys
tem also provides robustness to the linguistic
processing while maintaining both the accuracy
and the precision of the grammar
 Introduction
Deep linguistic processing produces a complete
syntactic and semantic analysis of the sentences
it processes however it fails in producing a re
sult when the linguistic structure being pro
cessed andor words in the input sentences fall
beyond the coverage of the grammatical re
sources Natural Language Processing NLP	
systems with monolithic grammars in addition
have to deal with huge search space due to sev
eral sources of nondeterminism ie ambigu
ity	 This is particularly true of broadcoverage
unicationbased grammars where all dimen
sions of linguistic information are interleaved as
theories such as HPSG propose Lack of robust
ness and inecient processing make such sys
tems inadequate for practical applications eg
Natural Language Interfaces NLI	
This paper presents a NLP system which in
tegrates a linguistic PartofSpeech PoS	 tag
ger and chunker as opposed to datadriven	
as a preprocessing module of a broadcoverage
unicationbased grammar of Spanish
By integrating shallow and deep processing
the eciency of the overall analysis process im
proves signicantly since we can release the
parser from certain tasks that may be e
ciently and reliably dealt with by computation
ally less expensive techniques The integration
of shallow processing in addition provides the
unicationbased grammar with larger coverage
for syntactic structures and allows us to imple
ment default lexical entry templates for virtu
ally unlimited lexical coverage while avoiding in
crease in ambiguity
The system we present is inspired by Abney

	 and it is in accordance with Srinivas et
al 
 Ciravegna and Lavelli 
 Yoon et
al 
 Venkova  Watanabe  Prins
and Noord 
 Grover and Lascarides 

Crysmann et al 	
In the following section we briey present the
unicationbased grammar Section  describes
latch the linguistic tagger and chunker Sec
tion  discusses the extensions required by our
system in order to transfer the information de
livered by the tagger and chunker into the gram
mar In section  we describe the default lexical
entries we have dened Results on the system
performance are provided in section  This pa
pers ends by presenting the general conclusions
 The Unicationbased Grammar
The development of the grammar that served as
the basis of our research work was done in the
framework of the Advanced Language Engineer
ing Platform ALEP	 Simpkins et al 
	
during the project LSGRAM LRE 
	
Schmidt et al 
	 and it was used in
the project MELISSA ESPRIT 	 Bre
denkamp et al 
	 for the rst time in an
industrial context The grammar is currently
being used in the project IMAGINE IST
	 The main goal of the IMAGINE project
is to develop software technology that allows the
interaction with ebusiness applications by us
ing a multilingual NLI from mobile devices and
other appliances

 Coverage of the Grammar
The range of linguistic phenomena that the
grammar handles includes all types of sub
categorization structures determination sim
ple and complex	 a full coverage of agree
ment subjectverb subjectattribute agree
ment within the NP	 nullsubjects prodrop
impersonal sentences	 compound tenses and
periphrastic forms clausal complements com
pletive clauses and indirect questions	 control
and raising structures support verb construc
tions passive constructions with the copula
with or without the byagent complement and
reexive passive	 modiers of verbs nouns ad
jectives and adverbs negation sentential ad
juncts topicalization relative and interroga
tives clauses surface word order variation co
ordination binary enumeration and coordina
tion of unlike categories	 clitics cliticNP al
ternation clitic doubling clitic climbing encl
itics	 NPs with no nounhead nonsentential
input strings and special constructions num
ber dates    	
 The ALEP Architecture
ALEP distinguishes preprocessing operations
and linguistic processing operations The for
mer Text Handling TH	 and orphographemic
analyses account for surface properties of in
put text document formatting delimitation
of textual structural elements orthographemic
aspects of morphology	 while the latter 
parsing and renement deal with its non
surface properties morphosyntactic analysis
constituent structure semantic representa
tion	

A special rulebased operation 
Lifting interfaces the output of the prepro
cessing operation with the parsing operation
 The ALEP Linguistic Formalism
The ALEP linguistic formalism has been devel
oped on the basis of the specications result
ing from the ET design study Alshawi et al

See httpwwwrtdsoftwareagesimagine

A distinctive feature of the ALEP processing archi
tecture is the division of the analysis task into two sub
tasks parsing which builds up a complete but shallow
phrase structure tree and renement which traverses
the structure topdown thus monotonically performing
feature decoration typically with semantic information


	 It is a so called Automatic Selection of HPSG-Parsed Sentences for
Treebank Construction
Montserrat Marimon?
Universitat de Barcelona
Nu?ria Bel??
Universitat Pompeu Fabra
Llu??s Padro??
Universitat Polite`cnica de Catalunya
This article presents an ensemble parse approach to detecting and selecting high-quality lin-
guistic analyses output by a hand-crafted HPSG grammar of Spanish implemented in the LKB
system. The approach uses full agreement (i.e., exact syntactic match) along with a MaxEnt parse
selection model and a statistical dependency parser trained on the same data. The ultimate goal
is to develop a hybrid corpus annotation methodology that combines fully automatic annotation
and manual parse selection, in order to make the annotation task more efficient while maintaining
high accuracy and the high degree of consistency necessary for any foreseen uses of a treebank.
1. Introduction
Treebanks constitute a crucial resource for theoretical linguistic investigations as well
as for NLP applications. Thus, in the past decades, there has been increasing interest in
their construction and both theory-neutral and theory-grounded treebanks have been
developed for a great variety of languages. Descriptions of available annotated corpora
can be found in Abeille? (2003) and in the proceedings from the annual editions of the
International Workshop on Treebanks and Linguistic Theories.
Quantity and quality are two very important objectives when building a treebank,
but speed and low labor costs are also required. In addition, guaranteeing consis-
tency, that is, that the same phenomena receive the same annotation through the corpus,
is crucial for any of the possible uses of the treebank. The first attempts at treebank
projects used manual annotation mainly and devoted many hours of human labor
to their construction. Human annotation is not only slow and expensive, but it also
introduces errors and inconsistencies because of the difficulty and tiring nature of the
? Gran Via de les Corts Catalanes 585, 08007-Barcelona. E-mail: montserrat.marimon@ub.edu.
?? Roc Boronat 138, 08018-Barcelona. E-mail: nuria.bel@upf.edu.
? Jordi Girona 1-3, 08034-Barcelona. E-mail: padro@lsi.upc.edu.
Submission received: 16 October 2012; revised submission received: 20 October 2013; accepted for publication:
5 December 2013.
doi:10.1162/COLI a 00190
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
task.1 Therefore, automating parts of the annotation process aims to leverage effective-
ness, producing a larger number of high-quality and consistent analyses in shorter time
and using fewer resources.
This article presents research that attempts to increase the degree of automation in
the annotation process when constructing a large treebank for Spanish (the IULA Span-
ish LSP Treebank) in the framework of the European project METANET4U (Enhancing
the European Linguistic Infrastructure, GA 270893GA).2
The treebank was developed using the following bootstrapping approach, details
of which are presented in Sections 3 and 4:
 First, we annotated the sentences using the DELPH-IN development
framework, in which the annotation process is effected by manually
selecting the correct parses from among all the analyses produced by a
hand-built symbolic grammar.
 Second, when a number of human-validated parsed sentences were
available, we trained a MaxEnt ranker.
 Third, we trained a dependency parser with the human-validated parsed
sentences converted to the CoNLL format.
 Fourth, we provided a fully automated chain based on an ensemble
method that compared the parse delivered by the dependency parser and
the one delivered by the MaxEnt ranker, and then accepted the
automatically proposed analysis, but only if both were identical.
 Fifth, sentences rejected by the ensemble were given to human annotators
for manual disambiguation.
Obviously, using fully automatic parsing would have been the best solution for
speed and consistency, but no statistical parsers for Spanish are good enough yet, and
when using symbolic parsers, there is no way to separate good parses from incorrect
ones. The ensemble method we propose is a way of avoiding monitoring automatic
parsing; the error is more than acceptable and recall is expected to be augmented by
re-training and the refinement of the different parses.
After this introduction, Section 2 presents an overview of related work on automatic
parse selection, Section 3 summarizes the set-up, Section 4 presents our experiments and
results and, finally, Section 5 concludes.
2. Related Work
In the broadest sense, this work is situated with respect to research into automatic
parse selection. Such projects have had a variety of different goals as well as dif-
ferent approaches, based on (i) semantic filtering techniques (Yates, Schoenmackers,
and Etzioni 2006), (ii) sentence-level features (e.g., length; Kawahara and Uchimoto
1 In order to control errors, a common strategy is to control inter-annotator agreement by making two
annotators work on the same sentences. This makes the task even slower and more expensive.
2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different
domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths
(ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu.
524
Marimon, Bel, and Padro? Automatic Selection of HPSG-Parsed Sentences
2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and
Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007;
Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble
approaches.
Reichart and Rappoport (2007) selected high-quality constituency parses by
using the level of agreement among 20 copies of the same parser, trained on dif-
ferent subsets of a training corpus. Experiments using training and test data for the
same domain and in the parser-adaptation scenario showed improvements over several
baselines.
Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses.
They compared the outputs of two statistical shift-reduce LR models and selected only
identical parses, in their case to retrain the MaxEnt model. Following this procedure,
they achieved the highest score in the domain adaptation track of the CoNLL 2007
shared task.
Finally, Baldridge and Osborne (2003) used an ensemble of parsers in the con-
text of HPSG grammars applied to committee-based active learning, that is, to select
the most informative sentences to be hand-annotated and used as training material
to improve the statistical parser and to minimize the required amount of such sen-
tences. Using the English Resource Grammar (Flickinger 2002) and the Redwoods
treebank (Oepen et al. 2002), they showed that sample selection according to preferred
parse disagreement between two different machine learning algorithms (log-linear
and perceptron), or between the same algorithm trained on two independent feature
sets (configurational and ngram sets, based on the HPSG derivation trees), reduced the
amount of human-annotated material needed to train an HPSG parse selection model
compared with a certainty-based method based on tree entropy and several baseline
selection metrics.
Like Baldridge and Osborne (2003), we investigate ensemble parsing in the context
of HPSG grammars; however, our goal does not involve selecting the most informative
sentences to retrain the parser, but rather to select those sentences most reliably parsed,
in order to enlarge the treebank automatically. Thus, rather than selecting sentences on
which two models disagree, we select those where they agree completely. In addition,
we present two important contributions, going beyond what has been done in previous
work. First, although parsing ensembles have previously been proposed only for closely
related language models (i.e., parsers that use algorithms under the machine-learning
paradigm, varying only the feature set or training data), the presented work is the
first to combine parsers from different paradigms: stochastic dependency parsing and
MaxEnt parse selection over parses produced by a symbolic grammar. Second, the
current work is the first to propose such a methodology for parse selection as a way of
overcoming the seemingly impossible task of automatically selecting good parses from
automatic parsing to speed treebank production and, more importantly, to meet the
requirements of high precision and high consistency that are good for all of the uses of
the treebank.
3. Set-up
We select high-quality HPSG analyses using full agreement among a MaxEnt parse
selection model and a dependency parser. A comparison between the two is performed
on the dependency structures that we obtain converting the parse tree produced by a
symbolic grammar to the CoNLL format.
525
Computational Linguistics Volume 40, Number 3
3.1 HPSG Parsing and Disambiguation
Our investigation uses the Deep Linguistic Processing with HPSG Initiative (DELPH-
IN),3 an open-source processing framework also used in several treebank projects
within this international initiative (Oepen et al. 2002; Flickinger et al. 2012). Using this
framework, the annotation process is divided into two parts: (1) the corpus is parsed
using a hand-built HPSG (Pollard and Sag 1994); (2) the grammar output is ranked by
a MaxEnt-based parse ranker (Toutanova et al. 2005), and the best parse is manually
selected.
The grammar applied in parsing is a broad-coverage, open-source Spanish gram-
mar implemented in the Linguistic Knowledge Builder (LKB) system (Copestake 2002),
the Spanish Resource Grammar (SRG) (Marimon 2013).
The manual selection task is performed with an interface provided as part of the
[incr tstb()] grammar profiling environment (Oepen and Carroll 2000) that allows the
annotator to reduce the set of parses incrementally by choosing so-called discriminants
(Carter 1997); that is, by selecting the features that distinguish between the different
parses, until the appropriate parse is left or, if none of the displayed parses is the correct
one, all parses are rejected.
As always the case with symbolic grammars, the SRG produces several hundreds
of analyses for a sentence. The DELPH-IN framework, however, provides a MaxEnt-
based ranker that sorts the parses produced by the grammar. Although this stochastic
ranker cannot be used to select automatically the correct parse without introducing a
considerable number of errors (as we will show, it only achieves accuracy of about 61%),
it nevertheless allows the annotator to reduce the forest to the n-best trees, typically the
500 top readings. The statistics that form the model of the MaxEnt ranker are gathered
from disambiguated parses and can be updated as the number of annotated sentences
increases.
3.2 Conversion to the CoNLL Format
The linguistic analysis produced by the LKB system for each parsed sentence provides,
together with a constituent structure and a Minimal Recursion Semantics (MRS) seman-
tic representation (Copestake et al. 2005), a derivation tree, obtained from a complete
syntactico-semantic analysis represented in a parse tree with standard HPSG-typed
feature structures at each node.
The derivation tree is encoded in a nested, parenthesized structure whose ele-
ments correspond to the identifiers of grammatical rules and the lexical items used
in parsing. Phrase structure rules?marked by the suffix ? c? (for ?construction?)?
identify the daughter sequence, separated by a hyphen, and, in headed-phrase con-
structions, a basic dependency relation between sentence constituents (e.g., subject-
head (sb-hd) and head-complement (hd-cmp)). Lexical items are annotated with
part-of-speech information according to the EAGLES tag set for Spanish4 and their
lexical entry identifier, and they optionally include a lexical rule identifier. Figure 1
shows an example.
In order to compare the first-best trees selected by the MaxEnt selection model and
the outputs of the dependency parser, we convert the derivation trees to a dependency
3 http://www.delph-in.net/.
4 See http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html.
526
Marimon, Bel, and Padro? Automatic Selection of HPSG-Parsed Sentences
Figure 1
Derivation tree and dependency graph of Conceder licencias, cuando as?? lo dispongan las ordenanzas
[To grant licences, when so stipulated by ordinances].
format, also illustrated in Figure 1. In this target annotation, lexical elements are linked
by asymmetrical dependency relations in which one of the elements is considered
the head of the relation and the other one is its dependant. The conversion is a fully
automatic and unambiguous process that produces the dependency structure in the
CoNLL format (Buchholz and Marsi 2006). A deterministic conversion algorithm
makes use of the identifiers of the phrase structure rules mentioned previously, in
order to identify the heads, dependants, and some dependency types that are directly
transferred onto the dependency structure (e.g., subject, specifier, and modifier). The
identifiers of the lexical entries, which include the syntactic category of the sub-
categorized elements, enable the identification of the argument-related dependency
functions.5
3.3 Dependency Parsing
For dependency parsing, we use MaltParser (Nivre et al. 2007). To train it, we use
manually disambiguated parses among those parses produced by the HPSG grammar,
converted to the dependency format we describe earlier.
5 An alternative proposal for projecting HPSG trees to CoNLL is described in Ivanova et al. (2012).
527
Computational Linguistics Volume 40, Number 3
Table 1
Results of the MaxEnt model and MaltParser as labeled attachment scores, unlabeled
attachment scores, labeled accuracy score, and exact syntactic match.
LAS UAS Label Accur Score Exact Synt Match
MaxEnt model 95.4% 96.8% 97.6% 61.0%
MaltParser 92.0% 95.0% 94.5% 43.1%
4. Experiments and Results
In our experiments, we tested the ability of the ensemble approach to select only correct
parses. The experiment proceeded as follows:
 We divided a set of 15,329 sentences into a training and test set (13,901 and
1,428 sentences, respectively). Sentence length ranged from 4 to 20 words
(longer sentences had not been annotated yet).
 We trained the MaxEnt model and MaltParser and ran each of the models
on the test set. The results we achieved are displayed in Table 1.
 We compared the outputs of the two models and selected those sentences
where both parses produced identical analyses.
The performance of our parser ensemble approach was measured through precision
and recall on the task of selecting those sentences for which the first tree proposed by
the MaxEnt model was the correct one. Table 2 shows the confusion matrix resulting
from the experiment. The row predicted ok counts the number of sentences selected
by our ensemble method (Malt and MaxEnt delivered parses are identical), and the
row predicted nok contains the number of sentences not selected because the parsers
disagreed. Columns gold present the manual evaluation of a MaxEnt model first ranked
parse. From this table, we can compute precision and recall of our sentence selector:
445 sentences were selected out of the 1,428 sentences in the test set (31.2%). Precision
(number of correctly selected sentences among all the selected sentences) stood at 90.6%
(403/445), and recall (number of correctly selected sentences among all the actually
correctly ranked first sentences) was 46.6% (403/864).
We compared the results of our ensemble method with two parse selection methods
based on: (i) a simple probability-based threshold (baseline) and (ii) a parser uncertainty
measure computed as tree entropy as used by Baldridge and Osborne (2003). The baseline
consisted of selecting sentences for which the ratio between the probabilities of the two
highest ranked analyses delivered by the MaxEnt model was over a given threshold.
Table 2
Confusion matrix used to assess the results in terms of precision and recall.
gold
ok nok total
predicted ok 403 42 445nok 461 522 983
total 864 564 1,428
528
Marimon, Bel, and Padro? Automatic Selection of HPSG-Parsed Sentences
The idea was that a very high ratio would indicate that the parse ranked first had a
large advantage over the others, whereas if the ratio was close to 1, both the first and
the second analyses would have similar probabilities, indicating lower confidence of the
model in the decision. Tree entropy takes into account not just the two highest ranked
analyses, but all trees proposed by the parser for that sentence. The rationale is that
high entropy indicates a scattered probability distribution among possible trees (and
thus less certainty of the model in the prediction), whereas low entropy should indicate
that one tree (or a few) gets most of the probability mass.
Results for different thresholds (both for the baseline and tree entropy) are shown
in Table 3 (top). As we can see, setting a high threshold for the baseline, we can select a
small subset of 20% of the sentences with precision similar to that achieved by our parse
ensemble approach. To select 31% of the sentences (i.e., about the same proportion we
obtained with the ensemble approach) we need to set a threshold of 4.5, obtaining a
precision of 84%, which is lower than the 90% obtained with the ensemble method.
Tree entropy exhibits similar behavior, in that a restrictive threshold can select
about 15% of sentences with precision over 90%, while setting a threshold such that
about 31% of sentences are selected, we obtain precision of about 75%.
Note that although the baseline has an F1 score slightly higher than the ensemble,
our goal is a high precision filter that can be utilized to select correctly parsed sentences.
From this point of view, our approach beats both baselines.
The fact that tree entropy yields worse values than the baseline is somehow pre-
dictable: Given a sentence with n possible trees (note that n may be in the order of
dozens or even hundreds), if a small number m of those analyses (1 < m << n) concen-
trate a large portion of probability mass but exhibit small differences between them, the
sentence will be rejected by the baseline (because there is not enough distance between
the first and second analyses) but will be accepted by tree entropy (because entropy will
be relatively low, given the large value of n). Thus, tree entropy is a good measure for
Baldridge and Osborne (2003), whose purpose is to select sentences where the model
is less confident, but our simple baseline seems to be better when the goal is to select
sentences where the first parse is the correct one.
Table 3
Top: Comparative results using different threshold values for the baselines. Bottom: Results per
sentence length when selecting about 31% over all sentences. Thr = threshold; %sel = percentage
of selected sentences; P = precision; R = recall; Len = sentence length.
Baseline Tree entropy Ensemble
Thr. %sel P R Thr. %sel P R %sel P R
2 50.9% 67.6% 70.2% 0.2 59.6% 60.1% 73.2%
3 38.4% 77.5% 60.8% 0.15 38.2% 71.7% 56.0%
4.5 31.0% 84.1% 53.3% 0.133 31.3% 75.7% 48.3% 31.2% 90.6% 46.6%
10 20.8% 91.1% 38.6% 0.1 21.4% 82.0% 35.8%
20 12.1% 97.3% 24.0% 0.075 15.1% 91.5% 28.2%
30 9.9% 98.7% 19.9% 0.05 11.2% 96.6% 22.2%
Len. Baseline Tree entropy Ensemble
%sel P R %sel P R %sel P R
1-10 56.0% 96.7% 70.6% 42.8% 96.6% 53.9% 43.6% 97.7% 70.4%
11-20 19.8% 68.2% 36.9% 26.1% 60.3% 43.0% 10.3% 83.7% 33.8%
All 31.0% 84.1% 53.3% 31.3% 75.7% 48.3% 31.2% 90.6% 46.6%
529
Computational Linguistics Volume 40, Number 3
As shown in Table 3 (bottom), behavior is different for sentences of up to 10 words
than for longer sentences. All three systems have a bias towards selecting short rather
than long sentences (because short sentences are more often correctly analyzed by the
parser). The results for short sentences are similar in all three cases, but the ensemble
approach is clearly more precise for long sentences, with only a moderate loss in recall.
5. Conclusion
We have described research that aims to increase the degree of automation when
building annotated corpora. We propose a parser ensemble approach based on full
agreement between a MaxEnt model and a dependency parser to select correct linguistic
analyses output by an HPSG grammar. This enables a hybrid annotation methodology
that combines fully automatic annotation and manual parse selection, which makes the
annotation task more efficient while maintaining high accuracy and the high degree of
consistency necessary for a useful treebank. Our approach is grammar-independent and
can be used by any DELPH-IN-style treebank. In the future, we plan to investigate the
impact of automatic treebank enlargement on the performance of statistical parsers.
Acknowledgments
This work was supported by grant Ramo?n y
Cajal from Spanish MICINN and the project
METANET4U. We thank the reviewers for
their comments and Carlos Morell for his
support.
References
Abeille?, Anne (editor). 2003. Treebanks:
Building and Using Parsed Corpora. Kluwer,
Amsterdam.
Baldridge, Jason and Miles Osborne.
2003. Active learning for HPSG
parse selection. In Proceedings of the
7th Conference on Computational Natural
Language Learning, pages 17?24,
Edmonton.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
10th Conference on Computational Natural
Language Learning, pages 149?164,
New York, NY.
Carter, David. 1997. The TreeBanker: A tool
for supervised training of parsed corpora.
In Proceedings of the 14th National Conference
on Artificial Intelligence, pages 598?603,
Providence, RI.
Copestake, Ann. 2002. Implementing Typed
Feature Structure Grammars. CSLI
Publications, Stanford, CA.
Copestake, Ann, Dan Flickinger, Carl
Pollard, and Ivan A. Sag. 2005. Minimal
recursion semantics: An introduction.
Research on Language and Computation,
3(4):281?332.
Flickinger, Dan. 2002. On building a more
efficient grammar by exploiting types.
In Natural Language Engineering (6)1?
Special Issue: Efficiency Processing with
HPSG: Methods, Systems, Evaluation,
16(1):1?17.
Flickinger, Dan, Valia Kordoni, Yi Zhang,
Anto?nio Branco, Kiril Simov, Petya
Osenova, Catarina Carvalheiro, Francisco
Costa, and Se?rgio Castro. 2012.
ParDeepBank: Multiple parallel deep
treebanking. In Proceedings of the 11th
Workshop on Treebanks and Linguistic
Theories, pages 97?108, Lisbon.
Ivanova, Angelina, Stephan Oepen, Lilja
Ovrelid, and Dan Flickinger. 2012.
Who did what to whom? A contrastive
study of syntacto-semantic dependencies.
In Proceedings of the 6th Linguistic
Annotation Workshop, pages 2?11,
Jeju Island.
Kawahara, Daisuke and Kiyotaka
Uchimoto. 2008. Learning reliability
of parses for domain adaptation of
dependency parsing. In Proceedings of the
3rd International Joint Conference on Natural
Language Processing, pages 709?714,
Hyderabad.
Marimon, Montserrat. 2013. The Spanish
DELPH-IN grammar. Language Resources
and Evaluation, 47(2):371?397.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?lsen Eryigit, Sandra
Ku?bler, Svetoslav Marinov, and Erwin
Mars. 2007. Maltparser: A language-
independent system for data-driven
dependency parsing. Natural Language
Engineering, 13(2):95?135.
530
Marimon, Bel, and Padro? Automatic Selection of HPSG-Parsed Sentences
Oepen, Stephan and John Carroll. 2000.
Performance profiling for parser
engineering. In Natural Language
Engineering (6)1?Special Issue: Efficiency
Processing with HPSG: Methods, Systems,
and Evaluation, 16(1):81?97.
Oepen, Stephan, Dan Flickinger,
K. Toutanova, and C. D. Manning. 2002.
LinGo Redwoods. A rich and dynamic
treebank for HPSG. In Proceedings
of the 1st Workshop on Treebanks and
Linguistic Theories, pages 139?149,
Sozopol.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
The University of Chicago Press and
CSLI Publications, Chicago.
Reichart, Roi and Ari Rappoport. 2007.
An ensemble method for selection of
high quality parses. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics, pages 408?415,
Prague.
Reichart, Roi and Ari Rappoport. 2009.
Automatic selection of high quality
parses created by a fully unsupervised
parser. In Proceedings of the 13th
Conference on Computational Natural
Language Learning, pages 156?164,
Boulder, CO.
Sagae, Kenji and Jun-Ichi Tsujii. 2007.
Dependency parsing and domain
adaptation with LR models and parser
ensembles. In Proceedings of the Joint
Meeting of the Conference on Empirical
Methods in Natural Language Processing and
Conference on Computational Natural
Language Learning, pages 1,044?1,050,
Prague.
Toutanova, Kristina, Christoper D. Manning,
Dan Flickinger, and Stephan Oepen. 2005.
Stochastic HPSG parse disambiguation
using the Redwoods corpus. Research on
Language and Computation, 3(1):83?105.
Yates, Alexander, Stefan Schoenmackers,
and Oren Etzioni. 2006. Detecting parser
errors using Web-based semantic filters.
In Proceedings of the 11th Conference of
Empirical Methods in Natural Language
Processing, pages 27?34, Sydney.
531

This article has been cited by:
1. Montserrat Marimon, N?ria Bel. 2014. Dependency structure annotation in the IULA Spanish
LSP Treebank. Language Resources and Evaluation . [CrossRef]
