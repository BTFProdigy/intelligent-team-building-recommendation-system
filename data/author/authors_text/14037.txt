Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 1?7, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Computational Linguistics in Brazil: An Overview 
 
Thiago A. S. Pardo1, Caroline V. Gasperin1, Helena M. Caseli2,  
Maria das Gra?as V. Nunes1 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
1 Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Av. Trabalhador S?o-carlense, 400 - Centro 
P.O.Box 668. 13560-970 - S?o Carlos/SP, Brazil 
2
 Departamento de Computa??o, Universidade Federal de S?o Carlos 
Rod. Washington Lu?s, Km 235 
P.O.Box 676. 13565-905 - S?o Carlos/SP, Brazil 
{taspardo,cgasperin}@icmc.usp.br, helenacaseli@dc.ufscar.br, 
gracan@icmc.usp.br 
 
 
 
Abstract 
In this paper we give an overview of 
Computational Linguistics / Natural 
Language Processing in Brazil, describing the 
general research scenario, the main research 
groups, existing events and journals, and the 
perceived challenges, among other relevant 
information. We also identify opportunities 
for collaboration. 
1 Brazilian Research Scenario 
Computational Linguistics (CL) / Natural 
Language Processing (NLP) is an emerging and 
growing area in Brazil. Although there is no 
consensus, it is traditionally understood as a 
research field within Artificial Intelligence, 
gathering researchers mainly from Computer 
Science/Engineering and Linguistics. There is also 
modest interaction with Information Sciences area. 
In general the CL/NLP area in Brazil started 
with researchers that finished their PhD abroad 
and, after coming back, initiated the first CL/NLP 
projects. Since then, but mainly more recently, the 
area has experienced some internationalization due 
to the fact that the number of undergraduate and 
graduate students that undergo internships on 
renowned foreign NLP research centers has 
increased. In Brazil, PhD students have the 
possibility to take their complete PhD course 
abroad or, alternatively, only a part of it. In both 
cases, students may count on Brazilian funding 
agencies. 
The area is more strongly represented and 
promoted by Brazilian Computer Society (SBC)1, 
particularly by its Special Interest Group on NLP 
(CEPLN)2, created in 2007. It is interesting that 
most researchers in Brazil (independent from their 
background area) do not differentiate CL from 
NLP, using both terms interchangeably. 
Research in Brazil is carried out mainly at 
public universities and at a few private universities 
and business companies. Differently from most 
countries, in Brazil public universities are 
generally considered the top ones, although 
exceptions do exist. 
Currently, there are no undergraduate courses on 
CL/NLP in Brazil, therefore researchers in this 
field come mainly from Computer Science and 
Linguistics courses. However, there are a few 
graduate courses on CL/NLP, with both computing 
and language emphases, such as the MSc and PhD 
programs at USP/S?o Carlos3, UFSCar4, 
UNESP/Araraquara5, PUC-RS6, and UFRGS7, 
among others.  
                                                           
1
 http://www.sbc.org.br 
2
 http://www.sbc.org.br/cepln 
3
 http://www.icmc.usp.br/~posgrad/computacao.html 
4
 http://ppgcc.dc.ufscar.br and http://www.ppgl.ufscar.br 
5
 http://www.fclar.unesp.br/poslinpor 
6
 http://www.pucrs.br/inf/pos 
7
 http://www.ufrgs.br 
1
Funding for research comes mainly from 
governmental agencies. Nowadays Brazil has 4 
agencies that significantly support research in the 
country (in this order): CNPq8 (National Council 
for Scientific and Technological Development), 
FAPESP9 (S?o Paulo Research Foundation), 
CAPES10 (Coordena??o de Aperfei?oamento de 
Pessoal de N?vel Superior), and FINEP11 (Research 
and Projects Financing). Private funding is still 
modest, which reflects the limited interaction 
between universities and companies. Some of the 
above agencies have tried to change this scenario 
by providing special joint university-industry 
funding programs. For instance, FAPESP and 
Microsoft Research recently formed a partnership 
to fund socially relevant projects in the state of S?o 
Paulo, e.g., the PorSimples12 text simplification 
project. FAPESP also funds special university-
company programs, where the research to be 
developed must be of interest to a company, which, 
in turn, has to support the research and work 
together with the researchers. 
NLP research in Brazil is varied and deals not 
only with Portuguese processing, but also with 
English and Spanish mainly. Given that Portuguese 
is among the most spoken languages in the world 
(it is estimated that almost 250 million people 
speak some variant of Portuguese in the world13), 
research interests on Portuguese processing is 
shared with other countries, mainly Portugal. In 
this sense, Portugal has launched an initiative to 
create and maintain a unified information storage 
center that indexes resources and publications 
for/on Portuguese processing. The initiative is the 
Linguateca project14, which was officially created 
in 2002, but initial works date back to 1998. Santos 
(2009) presents and evaluates the work carried out 
by Linguateca. 
Brazil and Portugal have a history of partnership 
on Portuguese processing, which formally started 
in 1993 with the first PROPOR conference 
(PROPOR event series is introduced in Section 4). 
                                                           
8
 http://www.cnpq.br 
9
 http://www.fapesp.br 
10
 http://www.capes.gov.br 
11
 http://www.finep.gov.br 
12
 http://caravelas.icmc.usp.br 
13
 Besides Brazil and Portugal, Portuguese is an official 
language in Angola, Cape Verde, East Timor, Equatorial 
Guinea, Guinea-Bissau, Macau, Mozambique, and S?o Tom? 
and Pr?ncipe. 
14
 http://www.linguateca.pt 
We maintain this partnership active by having 
collaborative projects and promoting joint events.  
As far as we know, other Portuguese speaking 
countries do not have a tradition of CL/NLP 
research. However, curiously, there are researchers 
from other non-Portuguese speaking countries that 
develop relevant research on Portuguese language. 
For example, to the best of our knowledge, 
currently the best syntactical parsers for 
Portuguese were developed by researchers from 
Denmark and the USA. These researchers actively 
work with the Brazilian research community. 
In what follows, we briefly present the Brazilian 
research profile (Section 2), the main research 
groups (Section 3), and the Brazilian events and 
journals (Section 4). We also report the main 
challenges for research in Brazil (Section 5) and 
the collaboration opportunities with other 
American researchers that we envision (Section 6). 
2 Research Profile 
In 2009 CEPLN proposed a survey of the status of 
CL/NLP research in Brazil and published the 
results during the 7th Brazilian Symposium in 
Information and Human Language Technology 
(Pardo et al, 2009). The survey aimed at gathering 
information both about researchers (such as their 
location, education level, number of students, etc.) 
and their research (main research topics, number of 
funded projects, main challenges, etc.). 
The survey was carried out mainly on-line. A 
call for participation was sent to all known e-mail 
lists from scientific associations from varied areas. 
Data was also obtained from the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics15. 
148 researchers responded to the survey: 35% of 
these were academic staff with a PhD degree, 16% 
academic staff with a Master's degree, 1% 
academic staff with a Bachelors degree, 9% PhD 
students, 26% Master's students, 14% 
undergraduate students, and 5% others. Table 1 
summarizes the main results of the survey, 
showing the percentage of answers for each issue. 
One may see that CL/NLP research is mainly 
carried out in the south and southeast regions of 
Brazil. 
 
                                                           
15
 http://ww.d.umn.edu/~tpederse/registry/registry.cgi 
2
Table 1. CEPLN survey 
Issues Results 
Geographic distribution 48% S?o Paulo state 
18% Rio Grande do Sul state 
8% Paran? state 
7% Rio de Janeiro state 
19% Other states 
National collaboration 52% Yes, 48% No 
International Collaboration 25% Yes, 75% No 
Background area 62% Computer Science 
29% Linguistics 
9% Other 
Supervision of postgraduate students 28% Yes, 72% No 
Funded projects 28% Yes, 72% No 
Source of funding 43% Federal government agencies 
25% S?o Paulo state government agency 
31% Other state government agencies 
 
. 
  
 
Figure 1. Research topics 
 
The survey also inquired the participants about 
their research topics. Figure 1 shows the 
distribution of topics among researchers who 
responded to the survey. Researchers could mark 
as many research topics as they wanted. Some 
topics subsume others, so these were marked more 
often by respondents. 
Ontologies and semantics were the topics 
marked by most respondents. We believe that there 
is indeed a significant number of researchers 
working on them, but we also believe that they are 
not the main topic of research of most people who 
3
listed them. For example, the statistics for 
?Ontologies? probably also include researchers 
who simply make use of ontologies in their work 
and not necessarily develop ontologies or ontology 
generation methods. Other researchers believe that 
we are in a changing period, moving from syntax-
centered research to semantics-centered research, 
due to the fact that more recently the community 
has produced more robust semantic tools and 
resources, e.g., the first versions of Portuguese 
language wordnets, as TeP 2.016, Wordnet.PT17, 
and MWN.PT18, as well as named entities 
recognizers, e.g., REMBRANDT19. 
Interestingly, corpus linguistics is one of the 
hottest topics but, at the same time, it is not seen as 
a genuine CL/NLP topic: most researchers that 
indicated corpus linguistics as a research topic 
marked it as ?other area of interest?. Some 
researchers have advocated that CL/NLP area and 
corpus linguistics should be considered a unique 
area, while others argue that these areas have 
different purposes and, therefore, different 
scientific methods, what would avoid such 
unification. Text mining is another curious case: 
research on this theme is mostly carried out by 
non-CL/NLP researchers, but instead by 
researchers on general AI and database areas 
Based on the publications on the last Brazilian 
scientific events and on the fact that we personally 
know most of the CL/NLP researchers in Brazil, 
we dare to indicate the following topics as the most 
recurrent ones (in no particular order): text 
summarization, machine translation, text 
simplification, automatic discourse analysis, 
coreference and anaphora resolution, information 
retrieval, text mining, terminology/lexicon 
research, ontologies and semantic tagging, and 
corpus linguistics. 
Based on the survey, we estimate that Brazil has 
about 250 researchers (including students) with 
interest in CL/NLP area. Although only 148 
researchers attended the CEPLN survey, we 
computed other researchers in the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics and in 
the CEPLN e-mail list that did not attend the 
survey. In general, we estimate that about 35-40 of 
                                                           
16
 http://www.nilc.icmc.usp.br/tep2/index.htm 
17
 http://www.clul.ul.pt/clg/wordnetpt 
18
 http://mwnpt.di.fc.ul.pt 
19
 http://xldb.di.fc.ul.pt/Rembrandt 
these are active researchers, whose main topic of 
research is CL/NLP, and who supervise 
undergraduate and graduate students on the 
subject. We also estimate that there are 5-10 
researchers on speech processing that actively 
collaborate with the CL/NLP community. 
3 Main research groups 
The largest CL/NLP research group in Brazil is 
NILC (Interinstitutional Center for Research and 
Development in Computational Linguistics)20, 
which includes researchers mainly from University 
of S?o Paulo (USP; Computer Science and Physics 
departments), Federal University of S?o Carlos 
(UFSCar; Computer Science and Linguistics 
departments) and State University of S?o Paulo 
(UNESP; Linguistics department). The group was 
created at 1993. 
NILC has a long history of research in CL/NLP, 
which has thrived since the ReGra21 project, in 
which the grammar checker for Portuguese that is 
currently used within Microsoft Word since its 
2000 version was built. In fact, ReGra project was 
born from a university-industry collaboration, one 
of the few successful ones in CL/NLP area in 
Brazil. At the moment most of the research at 
NILC is concentrated on the following topics: 
automatic summarization, text simplification, 
coreference resolution, and terminology. NILC has 
hosted STIL 2009 (STIL event series is introduced 
in the next section). NILC also currently holds the 
presidency of CEPLN. 
The NLP group at the Computer Science 
department at the Catholic University of Rio 
Grande do Sul (PUC-RS)22 also has a tradition of 
research on CL/NLP. Their current projects focus 
on information retrieval, ontology engineering and 
anaphora resolution. The group also has research 
on multi-agent systems applied to NLP tasks and, 
more recently, on text categorization. The group 
hosts PROPOR 2010 (PROPOR event series is 
also introduced in the next section). The group has 
held the presidency of CEPLN from its creation 
(2007) until 2009. 
The above research group and NILC form the 
main CL/NLP research vein in Brazil. They have 
joint research projects and have strong 
                                                           
20
 http://www.nilc.icmc.usp.br 
21
 http://www.nilc.icmc.usp.br/nilc/projects/regra.htm 
22
 http://www.inf.pucrs.br/~linatural 
4
collaboration, constantly hosting graduate students 
from each other in internship research periods. 
There are also other very relevant NLP groups 
in Brazil that regularly carry out projects on the 
area. We may cite the Catholic University of Rio 
de Janeiro (PUC-Rio)23, Federal University of Rio 
Grande do Sul (UFRGS), State University of 
Campinas (UNICAMP), University of the Sinos 
River Valley (Unisinos), and State University of 
Maring? (UEM), among others. 
4 Events and Journals 
The Brazilian Symposium on Information and 
Human Language Technology (STIL) is the main 
event on CL/NLP in South America and is in its 
seventh edition. It is promoted by CEPLN and is 
carried out since 1993. It is intended to be a forum 
for gathering everyone with interest in CL/NLP. It 
happens regularly (every one or two years) and 
accepts contributions in Portuguese, Spanish and 
English. Details about the event are available at 
www.nilc.icmc.usp.br/til. 
The International Conference on Computational 
Processing of Portuguese Language (PROPOR) is 
an international conference jointly promoted by 
Brazil and Portugal and is in its ninth edition. It is 
the main conference with focus on Portuguese 
language, giving equal space to research on text 
and speech processing. It is carried out in Brazil 
and in Portugal interchangeably (every two or 
three years) and accepts submissions in English 
only. PROPOR?s proceedings are published as part 
of Springer Lecture Notes series. Details about the 
event are available at 
www.nilc.icmc.usp.br/cgpropor. 
STIL and PROPOR are the most relevant 
conferences for researchers in CL/NLP in Brazil. 
Their last editions received support from NAACL. 
AI events are also recurrent forums for CL/NLP 
researchers. The Brazilian AI events are the 
Brazilian Symposium on Artificial Intelligence 
(SBIA)24 and the National Meeting on Artificial 
Intelligence (ENIA)25, also promoted by SBC. 
They are already in their twentieth and seventh 
editions, respectively. 
Other related events in Brazil are the Corpus 
                                                           
23
 www.letras.puc-rio.br/Clic/ogrupo.htm 
24
 http://www.jointconference.fei.edu.br/ 
25
 http://csbc2009.inf.ufrgs.br/ 
Linguistics Meeting (ELC)26 and Brazilian School 
on Computational Linguistics (EBRALC)27, which 
are in their eighth and third editions, respectively. 
These events are mainly organized by the 
Linguistics research community. EBRALC is 
mainly intended for new students in the area and 
has been held together with ELC. 
Brazilian researchers count mainly on the 
following journals for national periodical 
publications: 
 JBCS28 (Journal of the Brazilian Computer 
Society), which is published by SBC and 
covers all Computer Science areas, including 
CL/NLP; 
 RITA29 (Journal of Theoretical and Applied 
Computing), also of general scope. 
 
It is important to cite Linguam?tica30, which is an 
European initiative to publish CL/NLP research on 
the Iberian languages. 
CEPLN is also organizing a joint journal with 
other SBC AI-related special interest groups. 
5 Challenges 
At STIL 2009, the research community discussed 
challenging issues (raised by respondents of the 
CEPLN survey) that hamper research on CL/PLN 
in Brazil. The main issues raised were: 
 Lack of large and robust language resources 
for Portuguese; 
 Lack of formal models for linguistic 
description and analysis of Portuguese; 
 Difficulty in attracting students and researchers 
to the area; 
 Lack of multidisciplinary collaboration; 
 CL/NLP marginalization in both Computer 
Science and Linguistics. 
 Poor interaction between universities and 
industry; 
 Insufficient funding. 
 
Here we discuss some of these points. Although 
Portuguese has got state of the art tools (as POS 
taggers and syntactic parsers) and comprehensive 
corpora of contemporary written language, there is 
                                                           
26
 http://www.corpuslg.org/elc/Inicial.html 
27
 http://www.corpuslg.org/ebralc/Inicial.html 
28
 http://www.springer.com/computer+science/journal/13173 
29
 http://www.seer.ufrgs.br/index.php/rita 
30
 http://linguamatica.pt 
5
still a need for resources for particular applications 
or domains. Many researchers feel that Portuguese 
syntactic parsers (which are considered basic NLP 
tools) and wordnet-like resources are still too 
limited, not attending their demands. Brazil also 
lacks representative spoken corpora, what may be 
explained by the fact that, in Brazil, written and 
spoken language processing communities have 
modest interaction. While written language 
processing research is reported at SBC events, 
spoken language processing is mainly conducted 
under SBrT (Brazilian Telecommunications 
Society)31. PROPOR series have tried to bring 
together these two communities, fostering joint 
research and mutual awareness of both research 
lines.  
The lack of formal models for Portuguese 
linguistic description and analysis was mainly 
perceived by linguists that work with CL/NLP. In 
fact, they acknowledge that Brazil has no tradition 
in carrying out events on these themes, what would 
eventually harm CL/NLP research. This goes along 
with Sp?rck Jones (2007) opinion paper. One first 
step towards overcoming this lack of formal 
models for Portuguese description was the 
Workshop on Portuguese Description32, carried out 
together with the last edition of STIL. 
Another point that deserves attention is the 
sentiment that CL/NLP research suffers from 
marginalization in both Computer Science and 
Linguistics areas, as it is usually the case for 
multidisciplinary subjects. We believe this might 
be fueled by the way research is assessed in Brazil. 
In Brazil, the quality of research is mainly assessed 
by the publications generated from it, and 
publication vehicles from Linguistics are usually 
rated worse in Computer Science, and vice versa. It 
is expected that different areas may have different 
scientific methods and perspectives, as well as it is 
natural that such differences are mirrored in any 
evaluation instrument. However, such factors lead 
some researchers to feel uncomfortable with the 
multidisciplinary nature of CL/NLP field and the 
way they are recognized in their own major areas. 
Many researchers (not only from Brazil, but also 
from Portugal) have supported that CL/NLP should 
become a new ?major? area, instead of being part 
of Computer Science or Linguistics. 
                                                           
31
 http://www.sbrt.org.br 
32
 http://www.ppgl.ufscar.br/jdp/index.html 
Concerning insufficient funding, we believe that 
the main complaints came from Brazilian regions 
other than south and southeast, which currently 
concentrate CL/NLP research. In fact, during a 
lengthy discussion at STIL 2009 about the raised 
challenges, this issue was dismissed by many 
participants as non-representative. We believe that 
the funding situation in each region of Brazil 
contributes to the status of research on all topics, 
not particularly CL/NLP, in these regions. While 
in most Brazilian states researchers have to 
compete for funding from national agencies, some 
states (mainly in the southeast region) can rely on 
strong state-based funding agencies, such as 
FAPESP, in the state of S?o Paulo. 
6 Opportunities for Collaboration 
We believe that there are many opportunities for 
collaboration on CL/NLP with other researchers in 
the Americas, mainly due to the fact that the 
research community in Brazil works not only with 
Portuguese, but also with English and Spanish. 
One first step towards collaboration in Latin 
America was given in the event CHARLA 2008 
(Grand Challenges in Computer Science Research 
in Latin America Workshop). Organized by several 
scientific societies (including SBC), the event 
aimed at contributing to the definition of a long-
term research agenda in Latin America with the 
potential to significantly advance science and 
motivate the networking of abilities and 
competencies in Latin America. One of the 
recognized challenges was ?multilinguism?, which 
involves several CL/NLP topics. CHARLA 
immediate impact in Brazil was the adaptation of 
Brazilian CL/NLP events to receive contributions 
in Spanish, which has a vast number of speakers in 
Latin America. Contributions in English were 
already traditionally considered in Brazilian 
events. 
We believe that another important source of 
collaboration comes from awareness of the 
ongoing research projects in the Americas. 
Workshops such as this seem to be a channel for 
the exchange of information. We envision that 
initial collaborations may arise within machine 
translation projects, which naturally already deal 
with the representative languages of the Americas. 
Letting aside technical collaboration, we believe 
there is room for higher-level concrete actions that 
6
could foster collaboration in the Americas. These 
are actions that may increase the visibility of the 
research done in Latin America, as well as 
motivate new research. One first action that we 
envisage is the opening of evaluation challenges 
and shared tasks to the languages of the Americas 
other than English. For instance, 
contests/conferences such as TAC33, 
Senseval/SemEval34, and TREC35, among others, 
might make Portuguese/Spanish datasets available, 
as CLEF36 has done in its last editions. This has 
certainly an organizational cost, but it may turn out 
to be a valuable investment.  
Another action that could stimulate the progress 
of CL/NLP research in Latin America consists of 
including the proceedings of other American 
CL/NLP conferences in the ACL Anthology37, for 
example, the proceedings of STIL and PROPOR, 
to mention the Brazilian examples. This could be 
restricted to conferences that received 
ACL/NAACL endorsement and/or sponsorship.  
While the first action we proposed would make 
it feasible for more countries to participate in the 
evaluation contests, the second action would allow 
the works carried out in these countries to be better 
known. 
In a different strategy, we imagine that it must 
be possible for regional scientific associations to 
establish formal partnerships, granting some 
advantages to associated researchers from the 
corresponding countries, such as: registration 
discounts in the CL/NLP conferences from the 
countries (for instance, ACL/NAACL members 
would have discounts for registering in Brazilian 
events, as well as SBC members for ACL/NAACL 
events); and distribution of relevant publications 
for members of the associations (for instance, SBC 
traditionally distributes to its members the JBCS 
journal, which is considered a prestigious 
international publication). 
Our last idea would be to create a fund (possibly 
through the associations? partnership) for funding 
visits for knowledge transfer (1-2 weeks) for 
researchers and mainly students. These could be an 
opportunity for studying/working with researchers 
from other countries that work on topics of 
                                                           
33
 http://www.nist.gov/tac 
34
 http://www.senseval.org 
35
 http://trec.nist.gov 
36
 http://www.clef-campaign.org 
37
 http://aclweb.org/anthology-new 
interest, as well as for renowned researchers to 
visit research groups in order to stimulate work on 
a particular topic. Such opportunities would be 
very positive for Brazilian students. 
We believe that the actions suggested above can 
lead to a more integrated research scenario in the 
Americas. 
Acknowledgments 
The authors are grateful to SBC, CEPLN, 
FAPESP, and CAPES for supporting this work and 
the realization of STIL 2009, where part of the data 
shown in this paper was presented. 
References  
Pardo, T.A.S.; Caseli, H.M.; Nunes, M.G.V. (2009). 
Mapeamento da Comunidade Brasileira de 
Processamento de L?nguas Naturais. In the 
Proceedings of the 7th Brazilian Symposium in 
Information and Human Language Technology - 
STIL, pp. 1-21. September 8-10, S?o Carlos/SP, 
Brazil. 
Santos, D. (2009). Caminhos percorridos no mapa da 
portuguesifica??o: A Linguateca em perspectiva. 
Linguam?tica, N. 1, pp. 25-58. 
Sp?rck Jones, K. (2007). Computational Linguistics: 
What About the Linguistics? Computational 
Linguistics, Last Words Section, Vol. 33, N. 3, pp. 
437-441. 
7
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 83?87,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Distinguishing between Positive and Negative Opinions with Complex
Network Features
Diego R. Amancio, Renato Fabbri, Osvaldo N. Oliveira Jr.,
Maria G. V. Nunes and Luciano da F. Costa
University of Sa?o Paulo, Sa?o Carlos, Sa?o Paulo, Brazil
diego.amancio@usp.br, renato.fabbri@gmail.com, chu@ifsc.usp.br,
gracan@icmc.usp.br, ldfcosta@gmail.com
Abstract
Topological and dynamic features of com-
plex networks have proven to be suitable
for capturing text characteristics in recent
years, with various applications in natu-
ral language processing. In this article we
show that texts with positive and negative
opinions can be distinguished from each
other when represented as complex net-
works. The distinction was possible by
obtaining several metrics of the networks,
including the in-degree, out-degree, short-
est paths, clustering coefficient, between-
ness and global efficiency. For visu-
alization, the obtained multidimensional
dataset was projected into a 2-dimensional
space with the canonical variable analysis.
The distinction was quantified using ma-
chine learning algorithms, which allowed
an recall of 70% in the automatic dis-
crimination for the negative opinions, even
without attempts to optimize the pattern
recognition process.
1 Introduction
The use of statistical methods is well estab-
lished for a number of natural language pro-
cessing tasks (Manning and Schuetze, 2007), in
some cases combined with a deep linguistic treat-
ment in hybrid approaches. Representing text as
graphs (Antiqueira et al, 2007), in particular, has
become popular with the advent of complex net-
works (CN) (Newman, 2003; Albert and Barabasi,
2002), especially after it was shown that large
pieces of text generate scale-free networks (Ferrer
i Cancho and Sole, 2001; Barabasi, 2009). This
scale-free nature of such networks is probably the
main reason why complex networks concepts are
capable of capturing features of text, even in the
absence of any linguistic treatment. Significantly,
the scale-free property has also allowed CN to be
applied in diverse fields (Costa et al, 2008), from
neuroscience (Sporns, 2002) to physics (Gfeller,
2007), from linguistics (Dorogovtsev and Mendes,
2001) to computer science (Moura et al, 2003), to
mention a few areas. Other frequently observed
unifying principles that natural networks exhibit
are short paths between any two nodes and high
clustering coefficients (i.e. the so-called small-
world property), correlations in node degrees, and
a large number of cycles or specific motifs.
The topology and the dynamics of CN can be
exploited in natural language processing, which
has led to several contributions in the literature.
For instance, metrics of CN have been used to as-
sess the quality of written essays by high school
students (Antiqueira et al, 2007). Furthermore,
degrees, shortest paths and other metrics of CN
were used to produce strategies for automatic sum-
marization (Antiqueira et al, 2009), whose results
are among the best for methods that only employ
statistics. The quality of machine translation sys-
tems can be examined using local mappings of lo-
cal measures (Amancio et al, 2008). Other re-
lated applications include lexical resources anal-
ysis (Sigman and Cecchi, 2002), human-induced
words association (Costa, 2004), language evolu-
tion (Dorogovtsev andMendes, 2002), and author-
ship recognition (Antiqueira et al, 2006).
In this paper, we model texts as complex net-
works with each word being represented by a
node and co-occurrences of words defining the
edges (see next section). Unlike traditional meth-
ods of text mining and sentiment detection of re-
views (Tang et al, 2009; Pennebaker et al, 2003),
the method described here only takes into account
the relationships between concepts, regardless of
the semantics related to each word. Specifically,
we analyze the topology of the networks in order
to distinguish between texts with positive and neg-
ative opinions. Using a corpus of 290 pieces of
83
Before pre-processing After pre-processing
The projection of the projection
network data into two network data two
dimensions is crucial dimension be crucial
for big networks big network
Table 1: Adjacency list obtained from the sentence
?The projection of the network data into two di-
mensions is crucial for big networks?.
text with half of positive opinions, we show that
the network features allows one to achieve a rea-
sonable distinction.
2 Methodology
2.1 Representing texts as complex networks
Texts are modeled as complex networks here by
considering each word (concept) as a node and es-
tablishing links by co-occurrence of words, disre-
garding the punctuation. In selecting the nodes,
the stopwords were removed and the remaining
words were lemmatized to combine words with
the same canonical form but different inflections
into a single node. Additionally, the texts were
labeled using the MXPost part-of-speech Tag-
ger based on the Ratnaparki?s model (Ratnaparki,
1996), which helps to resolve problems of am-
biguity. This is useful because the words with
the same canonical form and same meaning are
grouped into a single node, while words that have
the same canonical form but distinct meanings
generate distinct nodes. This pre-processing is
done by accessing a computational lexicon, where
each word has an associated rule for the genera-
tion of the canonical form. For illustrative means,
Table 1 shows the pre-processed form of the sen-
tence ?The projection of the network data into two
dimensions is crucial for big networks? and Figure
1 shows the network obtained for the same sen-
tence.
Several CN metrics have been used to analyze
textual characteristics, the most common of which
are out-degree (kout), in-degree (kin), cluster co-
efficient (C) and shortest paths (l). Here we also
use the betweenness (?) and the global efficiency
(?). The out-degree corresponds to the number
of edges emanating from a given node, where
the weight of each link between any two nodes
may also be considered, being referred to as out-
strength. Analogously, the node?s in-degree is de-
fined as the number of edges arriving at a given
Figure 1: Network obtained from the sentence
?The projection of the network data into two di-
mensions is crucial for big networks?.
node. The network?s kout and kin are evaluated
by calculating the average among all the nodes,
note that such global measures kout and kin are
always equal. Regarding the adjacency matrix to
represent the network, for a given node i, its kout
and kin are calculated by eqs 1 and 2, where N
represents the number of distinct words in the pre-
processed text:
kout(i) =
N
?
j=1
Wji (1)
kin(i) =
N
?
j=1
Wij (2)
The cluster coefficient (C) is defined as follows.
Let S be the set formed by nodes receiving edges
of a given node i, and Nc is the cardinality of this
set. If the nodes of this set form a completely con-
nected set, then there are Nc(Nc-1) edges in this
sub graph. However, if there are only B edges,
then the coefficient is given by eq. (3):
C(i) = B
Nc(Nc ? 1)
(3)
If Nc is less than 1, then C is defined as zero.
Note that this measure quantifies how the nodes
connected to a specific node are linked to each
other, with its value varying between zero and one.
The shortest paths are calculated from all pairs
of nodes within the network. Let dij be the min-
imum distance between any two words i and j in
the network. The shortest path length l of a node i
is given in equation 4.
84
l(i) = 1
N ? 1
?
j ?=i
dij (4)
Another measure often used in network analy-
sis is the global efficiency (?), which is defined in
equation 5, and may be interpreted as the speed
with which information is exchanged between any
two nodes, since a short distance dij contributes
more significantly than a long distance. Note that
the formula below prevents divergence; therefore,
it is especially useful for networks with two or
more components. The inverse of ?, named har-
monic mean of geodesic distances, has also been
used to characterize complex networks.
? = 1
N(N ? 1)
?
i?=j
1
dij
(5)
While l and ? use the length of shortest paths,
the betweenness uses the number of shortest paths.
Formally, the betweenness centrality for a given
vertex v is given in equation 6, where the numera-
tor represents the number of shortest paths passing
through the vertices i, v and j and the denomina-
tor represents the number of shortest paths pass-
ing through the vertices i and j. In other words,
if there are many shortest paths passing through a
given node, this node will receive a high between-
ness centrality.
?(v) =
?
i
?
j
?(i, v, j)
?(i, j)
(6)
2.2 Corpus
The corpus used in the experiments was ob-
tained from the Brazilian newspaper Folha de Sa?o
Paulo1, from which we selected 290 articles over a
10-year period from a special section where a pos-
itive opinion is confronted with a negative opinion
about a given topic. For this study, we selected
the 145 longest texts with positive opinion and the
145 longest text with negative opinions2, in order
to have meaningful statistical data for the CN anal-
ysis.
2.3 Machine Learning Methods
In order to discriminate the topological features
from distinct networks we first applied a technique
for reducing the dimension of the dataset, the
canonical variable analysis (McLachlan, 2004).
1http://www.folha.com.br
2The average size of the selected corpus is 600 words.
The projection of network data into a lower di-
mension is crucial for visualization, in addition
to avoids the so-called ?curse of dimensional-
ity? (Bishop, 2006). To calculate the axes points
for projecting the data, a criterion must be es-
tablished with which the distances between data
points are defined. Let S be the overall disper-
sion of the measurements, as shown in equation 7,
where ? is the number of instances (? = 290),??xc is
the set of metrics for a particular instance and ???x ?
is the average of all ??xc.
S =
?
?
c=1
(
??xc ? ???x ?
)(
??xc ? ???x ?
)T
(7)
Considering that two classes (C1 = positive
opinions and C2 = negative opinions) are used, the
scatter matrix Si is obtained for each class Ci, ac-
cording to equation 8, where ???x ?i is the analo-
gous of ???x ? when only the instances belonging to
class Ci is taken into account.
Si =
?
c?Ci
(
??xc ? ???x ?i
)(
??xc ? ???x ?i
)T
(8)
The intraclass matrix, i.e. the matrix that gives
the dispersion inside C1 and C2, is defined as in
equation 9. Additionally, we define the interclass
matrix, i.e. the matrix that provides the dispersion
between C1 and C2, as shown in equation 10.
Sintra = S1 + S2 (9)
Sinter = S ? Sintra (10)
The principal axes for the projection are then
obtained by computing the eigenvector associ-
ated with the largest eigenvalues of the ma-
trix ? (McLachlan, 2004) defined in equation
11. Since the data were projected in a two-
dimensional space, the two principal axes were se-
lected, corresponding to the two largest eigenval-
ues.
? = S?1intraSinter (11)
Finally, to quantify the efficiency of separa-
tion with the projection using canonical variable
analysis, we implemented three machine learn-
ing algorithms (decision tree, using the C4.5 algo-
rithm (Quinlan, 1993); rules of decision, using the
85
RIP algorithm (Cohen, 1995), and Naive Bayes
algorithm (John and Langley, 1995)) and eval-
uated the accuracy rate using the 10-fold-cross-
validation (Kohavi, 1995).
3 Results and Discussion
The metrics out-degree (kout), in-degree (kin),
shortest paths (l), cluster coefficient (C), between-
ness (?) and global efficiency (?) were computed
for each of the 145 texts for positive and nega-
tive opinions, as described in the Methodology.
The mean values and the standard deviations of
these metrics were used as attributes for each text.
This generated a dataset described in 10 attributes,
since the average kin is equal to the average kout
and the standard deviation of ? is not defined (in
other words, it is always zero). Figure 2 shows the
projection of the dataset obtained with canonical
variable analysis, illustrating that texts with dif-
ferent opinions can be distinguished to a certain
extent. That is to say, the topological features of
networks representing positive opinion tend to dif-
fer from those of texts with negative opinion.
The efficiency of this methodology for charac-
terizing different opinions can be quantified using
machine learning algorithms to process the data
from the projection. The results are illustrated in
Table 2. Again, the distinction between classes is
reasonably good, since the accuracy rate reached
62%. Indeed, this rate seems to be a good result,
since the baseline method3 tested showed an ac-
curacy rate of 53%. One also should highlight
the coverage found for the class of negative re-
views by using the C4.5 algorithm, for which a
value of 82% (result not shown in the Table 2) was
obtained. This means that if an opinion is nega-
tive, the probability of being classified as negative
is only 18%. Thus, our method seems especially
useful when a negative view should be classified
correctly.
Method Correctly classified
C4.5 58%
Rip 60%
Naive Bayes 62%
Table 2: Percentage of correctly classified in-
stances.
3The baseline method used as attributes the frequency of
each word in each text. Then, the algorithm C4.5 was run
with the same parameters used for the methodology based on
complex networks.
-0.120 -0.110 -0.100 -0.090
-0.180
-0.175
-0.170
-0.165
-0.160
-0.155
-0.150
FIRST PRINCIPAL AXIS
S
E
C
O
N
D
 P
R
IN
C
IP
A
L
A
X
IS
POSITIVE
NEGATIVE
PROJECTION OF POSITIVE AND
NEGATIVE OBSERVATIONS
Figure 2: Projection obtained by using the method
of canonical variables. A reasonable distinction
could be achieved between positive and negative
opinions.
4 Conclusion and Further Work
The topological features of complex networks
generated with texts appear to be efficient in dis-
tinguishing between attitudes, as indicated here
where texts conveying positive opinions could be
distinguished from those of negative opinions.
The metrics of the CN combined with a projec-
tion technique allowed a reasonable separation of
the two types of text, and this was confirmed with
machine learning algorithms. An 62% accuracy
was achieved (the baseline reached 53%), even
though there was no attempt to optimize the met-
rics or the methods of analysis. These promis-
ing results are motivation to evaluate other types
of subtleties in texts, including emotional states,
which is presently being performed in our group.
Acknowledgements: Luciano da F. Costa
is grateful to FAPESP (05/00587-5) and CNPq
(301303/06-1 and 573583/2008-0) for the finan-
cial support. Diego R. Amancio is grateful to
FAPESP sponsorship (proc. 09/02941-1) and Re-
nato Fabbri is grateful to CAPES sponsorship. We
also thank Dr. Oto Araujo Vale very much for sup-
plying the corpus.
86
References
C. D. Manning and H. Schuetze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press, First Edition.
L. Antiqueira, M. G. V. Nunes, O. N. Oliveira Jr. and L.
da F. Costa. 2007. Strong correlations between text
quality and complex networks features. Physica A,
373:811?820.
M. E. J. Newman. 2003. The Structure and Function
of Complex Networks. SIAM Review, 45:167?256.
R. Z. Albert and A.L. Barabasi. 2002. Statistical Me-
chanics of Complex Networks. Rev. Modern Phys.,
74:47?97.
R. Ferrer i Cancho and R. V. Sole. 2001. The small
world of human language. Proceedings of the Royal
Society of London B, 268:2261.
A.L. Barabasi. 2009. Scale-Free Networks: a decade
and beyond. Science, 24 325 5939 412?413.
L. F. da Costa, O. N. Oliveira Jr., G. Travieso, F.
A. Rodrigues, P. R. Villas Boas, L. Antiqueira, M.
P. Viana, L. E. C. da Rocha. 2008. Analyzing
and Modeling Real-World Phenomena with Com-
plex Networks: A Survey of Applications. arXiv
0711.3199.
O. Sporns. 2002. Network analysis, complexity, and
brain function. Complexity, 8(1):56?60.
D. Gfeller, P. LosRios, A. Caflisch and F. Rao. 2007.
Complex network analysis of free-energy land-
scapes. Proceedings of the National Academy of
Science USA, 104 (6):1817?1822
S. N. Dorogovtsev and J. F. F.Mendes. 2001. Lan-
guage as an evolving word web. Proceedings of the
Royal Society of London B, 268:2603.
A. P. S. de Moura, Y. C. Lai and A. E. Motter. 2003.
Signatures of small-world and scale-free properties
in large computer programs. Physical Review E,
68(1):017102.
L. Antiqueira, O. N. Oliveira Jr., L. da F. Costa and
M. G. V. Nunes. 2009. A Complex Network Ap-
proach to Text Summarization. Information Sci-
ences, 179:(5) 584?599.
M. Sigman and G.A. Cecchi. 2002. Global Organi-
zation of the Wordnet Lexicon. Proceedings of the
National Academy of Sciences, 99:1742?1747.
L. F. Costa. 2004. What?s in a name ? International
Journal of Modern Physics C, 15:371?379.
S. N. Dorogovtsev and J. F. F. Mendes. 2002. Evo-
lution of networks. Advances in Physics, 51:1079?
1187.
L. Antiqueira, T. A. S. Pardo, M. G. V. Nunes, O. N.
Oliveira Jr. and L. F. Costa. 2006. Some issues on
complex networks for author characterization. Pro-
ceeedings of the Workshop in Information and Hu-
man Language Technology.
H. Tang, S. Tan and X. Cheng. 2009. A survey on
sentiment detection of reviews. Expert Systems with
Applications, 36:7 10760?10773.
J. W. Pennebaker, M. R. Mehl and K. G. Niederhoffer.
2003. Psychological aspects of natural language.
use: our words, our selves. Annual review of psy-
chology, 54 547-77.
D. R. Amancio, L. Antiqueira, T. A. S. Pardo, L.
F. Costa, O. N. Oliveira Jr. and M. G. V. Nunes.
2008. Complex networks analysis of manual and
machine translations. International Journal of Mod-
ern Physics C, 19(4):583-598.
A. Ratnaparki. 1996. A Maximum Entropy Part-Of-
Speech Tagger. Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference,
University of Pennsylvania.
G. J. McLachlan. 2004. Discriminant Analysis and
Statistical Pattern Recognition. Wiley.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer-Verlag New York.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers.
W. W. Cohen. 1995. Fast Effective Rule Induction.
12 International converence on Machine Learning,
115?223.
G. H. John and P. Langley. 1995. Estimating Continu-
ous Distribution in Bayesian Classifiers. 11 Confer-
ence on Uncertainty in Artificial Intelligence, 338?
345.
R. Kohavi. 1995. A study of cross-validation and boot-
strap for accuracy estimation and model selection.
Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence 2, 12:1137-
1143.
87
