The Need for Accurate Alignment in 
Natural Language System Evaluation 
Andrew Kehler* 
UC San Diego 
Douglas Appelt* 
SRI International 
John Bear* 
SRI International 
As evaluations of computational linguistics technology progress toward higher-level interpre- 
tation tasks, the problem o/determining alignments between system responses and answer key 
entries may become l ss straightforward. We present an extensive analysis o/the alignment pro- 
cedure used in the MUC-6 evaluation o/information extraction technology, which reveals effects 
that interfere with the stated goals of the evaluation. These ffects are shown to be pervasive nough 
that they have the potential to adversely impact he technology development process. These results 
argue strongly/or the use o/accurate alignment criteria in natural anguage valuations, and/or 
maintaining the independence o/alignment criteria and mechanisms used to calculate scores. 
1. Introduction 
It would be hard to overestimate he influence of evaluation on current natural an- 
guage processing (NLP) research and development. In contrast o the primarily qual- 
itative methodologies that characterized research in the 1980's, purely quantitative 
evaluation methods now pervade all aspects of the research and development process 
in many areas of NLP. These roles are well summarized by Chinchor and Dungca 
(1995), who refer specifically to the methods used for the U.S.-government-sponsored 
Message Understanding Conference (MUC) evaluations of information extraction (IE) 
technology: 
The resulting scores \[assigned by the MUC evaluation process\] are 
used for decision-making over the entire evaluation cycle, including 
refinement of the task definition based on interannotator comparisons, 
technology development using training data, validating answer keys, 
and benchrnarking both system and human capabilities on the test 
data. (p. 33) 
This passage highlights three major roles an evaluation method can serve. First, 
the method may be used during the process of task definition, to assess interannotator 
agreement on proposed task specifications and revise them accordingly, and to sub- 
sequently validate the final answer keys. Second, the method may be used to drive 
* Department of Linguistics #0108, University of California, San Diego, 9500 Gilman Drive, La Jolla, CA 
92093-0108. E-mail: kehler@ling.ucsd.edu 
j" Artificial Intelligence C nter, 333 Ravenswood Avenue, Menlo Park, CA 94025. E-mail: bear@ai.sri.com. 
~: Artificial Intelligence C nter, 333 Ravenswood Avenue, Menlo Park, CA 94025. E-maih 
appelt@ai.sri.com. 
? 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 2 
the system development process, as system developers and machine learning algo- 
rithms rely heavily on the feedback it provides to determine whether proposed sys- 
tem changes hould be adopted. Third, the results may be used for final cross-system 
comparison, on which judgments concerning the adequacy of competing technologies 
are based. Given their pervasiveness in the entire technology development process, 
the need for adequate valuation methods is of the essence, and has thus become 
a prominent opic for research in itself (Sparck-Jones and Galliers 1996; CSL Special 
Issue 1998, inter alia). 
The need to serve all these roles has necessitated the development of automated 
evaluation methods that are capable of being run repeatedly with little time and cost 
overhead. Automated methods are commonly used for application tasks, including 
speech recognition, information retrieval, and IE, as well as for natural anguage com- 
ponent echnologies, including part-of-speech tagging, syntactic annotation, and coref- 
erence resolution. Such methods generally consist of a two-step rocess--first, system- 
generated responses are aligned with corresponding human-generated responses en- 
coded in an answer key, and then a predetermined scoring procedure is applied to the 
aligned response pairs. 
The second of these tasks (scoring procedure) has been the focus of most previous 
research on evaluation. In this paper, we focus instead on the less well studied problem 
of alignment. The relative inattention to problems in alignment is no doubt a result of 
the fact that alignment is relatively unproblematic n many natural anguage valua- 
tion scenarios. In evaluations of component technologies such as part-of-speech tag- 
ging and treebank-style syntactic annotation systems, for instance, a system-generated 
annotation is simply scored against the human-generated annotation for the same 
word or sentence, regardless of whether matching it to the annotation for a different 
word or sentence would improve the overall score assigned. Alignment is similarly 
trivial in evaluations of applications uch as information retrieval, since the notion of 
document identity is well defined. Alignment in speech recognition evaluations can 
be a bit more complex, but constraints inherent in the methods nonetheless prohibit 
clear misalignments, uch that a system cannot receive credit for recognizing a word 
from an acoustic signal that occurred several utterances later, for instance. 
As the field progresses to address higher-level interpretation tasks, however, the 
problem of determining alignments may become less straightforward. Such tasks may 
require that the output contain information that is synthesized (and perhaps even 
inferred) from disparate parts of the input signal, making the correspondence b tween 
information in the system output and information in the answer key more difficult to 
recover. A case in point is the evaluation IE technology, as most prominently carried 
out by the series of MUCs. For a typical MUC-style IE task, a text may contain several 
extractable events, and thus a method is required for aligning the (often only partially 
correct) event descriptions extracted by a system to the appropriate ones in the answer 
key. It is therefore important o investigate the issues involved in the definition of 
alignment criteria in such tasks, and we can use the MUC experience as a basis for 
such an investigation. 
In this paper, we focus specifically on the criterion used for alignment in MUC-6. In 
light of difficulties in identifying a perfect alignment criterion for the MUC-6 task, the 
MUC-6 community agreed upon on a rather weak and forgiving criterion, leaving the 
resolution of alignment ambiguities to a mechanism that sought o maximize the score 
assigned. While this decision may have been thought o be relatively benign, we report 
on the results of an extensive, post hoc analysis of a 13 1/2-month effort focused on the 
MUC-6 task which reveals several unforeseen and negative consequences a sociated 
with this decision, particularly with respect o its influence on the incremental system 
232 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
development process. While we do not argue that these consequences are so severe 
that they call the integrity of the MUC-6 evaluation into question, they are substantial 
enough that they demonstrate the potential of such an alignment strategy to have a 
significantly adverse impact on the goals of an evaluation. It is therefore important 
that these lessons be brought to bear in the design of future evaluations for IE and 
other high-level language processing tasks. 
We begin with an overview of IE tasks, systems, and evaluation, including the 
alignment procedure used for MUC-6. We then provide an example from the MUC-6 
development corpus that illustrates properties of the alignment process that interfere 
with the stated goals of the evaluation. We assess the pervasiveness of these problems 
based on data compiled from an extended evelopment effort centered on the MUC-6 
task, and conclude that the effect of the alignment criterion is robust enough that it 
could potentially undermine the technology development process. We conclude that 
these results argue strongly for the use of strict and accurate alignment criteria in future 
natural anguage valuations---evaluations in which alignment problems will become 
exacerbated as the natural anguage applications addressed become more complex-- 
and for maintaining the independence of alignment criteria and the mechanisms used 
to calculate scores. 
2. Information Extraction, MUC, and the F-score Metric 
IE systems process streams of natural language input and produce representations 
of the information relevant o a particular task, typically in the form of database 
templates. In accordance with the aforementioned array of roles served by evaluation 
methods, the MUCs have been very influential, being the primary driving force behind 
IE research in the past decade: 
The MUCs have helped to define a program of research and develop- 
ment .. . .  The MUCs are notable ... in that they have substantially 
shaped the research program in information extraction and brought it 
to its current state. (Grishman and Sundheim 1995, 1-2) 
There have been seven MUCs, starting with MUC-1 in 1987, and ending with 
MUC-7 in 1997. The metrics used--precision, recall, and F-score--are probably the 
most exhaustively used metrics for any natural anguage understanding application; 
precision and recall have been in use since MUC-2 in 1989 (Grishman and Sundheim 
1995), and F-score since MUC-4 in 1992 (Hirschman 1998). IE evaluation has thus been 
extensively thought out, revised, and experimented with: 
For the natural anguage processing community in the United States, 
the pre-eminent evaluation activity has been the series of Message Un- 
derstanding Conferences (MUCs) .. . .  the MUC conferences provide us 
with over a decade of experience in evaluating language understand- 
ing. (Hirschman 1998, 282) 
The MUCs therefore provide a rich and established basis for the study of the 
effects of evaluation with respect o its roles noted above. As previously indicated, 
we will focus in this paper on MUC-6, held in 1995, and exclusively on the procedure 
used for aligning system-generated r sponses with those in an answer key. 
233 
Computational Linguistics Volume 27, Number 2 
(TEMPLATE) 
CONTENT (succession_event) 
(SUCCESSION_EVENT} 
SUCCESSION_ORE (organization) 
POST string 
IN_AND_OUT (in_and_out} 
VACANCY_REASON { DEPART_WORKFORCE, 
REASSIGNMENT, 
NEW _POST _CREATED I 
OTH_UNK } 
(ORGANIZATION) 
ORE_NAME string 
ORE_ALIAS string 
ORG_DESCRIPTOR string 
ORG_TYPE { COMPANY~ GOVERNMENT, 
OTHER } 
ORE_LOCALE string 
ORE_COUNTRY string 
(IN_AND_OUT) 
IO_PERSON (person} 
NEW_STATUS { IN, m_ACT~NG, 
OUT~ OUT-ACTING } 
ON_THE_ JOB { YES, NO, UNCLEAR } 
OTHER_ORE (organization) 
REL_OTHER_ORG { SAME_ORE, 
RELATED-ORE t 
OUTSIDE_ORE } 
{PERSON} 
PER_NAME string 
PER_ALIAS string 
PER_TITLE string 
Figure 1 
Output template for MUC-6. 
Marketing & Media: Star TV Chief Steps Down After News Corp. Takeover 
In the wake of a takeover by News Corp., the chief executive officer of Star TV resigned after less 
than six months in that post, industry executives said. 
Last week, News Corp. bought 63.6% of the satellite broadcaster, which serves the entire Asian 
region, for $525 million in cash and stock from Hutchison Whampoa Ltd. and the family of Li Ka-shing. 
At the time of the purchase, News Corp. executives said they would like the executive, Julian 
Mounter, to stay. However, Star's chief balked at the prospect of not reporting directly to Rupert 
Murdoch, News Corp.'s chairman and chief executive officer, people close to Mr. Mounter said. Both 
Mr. Mounter and a Star spokesman declined to comment. 
It is likely that Star's new chief executive will report to either Sam Chisholm, chief executive of 
News Corp.'s U.K.-based satellite broadcaster, British Sky Broadcasting, or Chase Carey, chief operating 
officer of News Corp.'s Fox Inc. film and television unit. 
Mr. Mounter's departure is expected to be formally announced this week. 
Although there are no obvious successors, it is expected that Mr. Murdoch will choose someone 
from either British Sky Broadcasting or Fox to run Star, said a person close to News Corp. 
Figure 2 
Example text from MUC-6 development set (9308040024). 
2.1 Task Definition 
The MUC-6 task was, roughly speaking, to identify information in business news 
that describes executives moving in and out of high-level positions within companies 
(Grishman and Sundheim 1995). The template structure that MUC-6 systems popu- 
lated is shown in Figure 1. There are three types of values used to fill template slots. 
String fills, shown italicized, are simply strings taken from the text, such as CEO for 
the POST slot in a SUCCESSION_EVENT. Set fills are chosen from a fixed set of values, 
such as DEPART_WORKFORCE for the VACANCY_REASON slot in a SUCCESSION_EVENT. 
Finally, Pointer fills, shown in angle brackets, hold the identifier of another template 
structure, e.g., the SUCCESSION_ORE slot in a SUCCESSION_EVENT will contain a pointer 
to an ORGANIZATION template. 
Figure 2 displays a text from the MUC-6 development corpus. When a partici- 
pating system encounters this passage, it should extract he information that Julian 
234 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
cor 
cor 
cor 
cor 
inc 
cor 
mis 
mis 
cor 
spu 
spu 
cor 
inc 
inc 
cor 
mis 
mis 
Figure 3 
TEMPLATE-024-1) 
CONTENT (SUCC_EVENT-024-  i} 
SUCC_EVENT-024-1}  
SUCCESS ION_ORG (ORG-024-1} 
POST "CEO"  
IN_AND_OUT ( IN_AND_OUT-024-  i} 
VACANCY_REASON REASSIGNMENT 
ORG-024-1) 
ORG_NAME "STAR Tv" 
ORG_ALIAS "STAR" 
ORG_DESCRIPTOR "THE SAT BDSTR" 
ORG_TYPE COMPANY 
IN_AND_OUT-024-1} 
IO_PERSON (PERSON-024-1} 
NEW_STATUS OUT 
ON_THE_JOB UNCLEAR 
PERSON-024-1} 
PER_NAME "JULIAN MOUNTER" 
PER_ALIAS "MouNTER" 
PER_TITLE "MR." 
Target and hypothetical outputs for the example text. 
(TEMPLATE-024-1) 
CONTENT (SUCC_EVENT-024- I}  
(SUCC_EVENT-024-1)  
SUCCESS ION_ORG (ORG-024-11 
POST "CEO"  
IN_AND_OUT ( IN_AND_OUT-024-1} 
VACANCY_REASON OTH_UNK 
(ORG-024-1) 
ORG_NAME "STAR TV" 
ORG_TYPE COMPANY 
ORG_LOCALE WHAMPOA 
ORG_COUNTRY UNITED KINGDOM 
(IN_AND_OUT-024-1} 
IO_PERSON (PERSON-024-1) 
NEW_STATUS IN 
ON_THE_JOB No 
(PERSON-024-1) 
PER_NAME "JULIAN MOUNTER" 
Mounter is "out" of the position of CEO of company Star TV, along with other infor- 
mation associated with the event. The correct results for this passage, as encoded in a 
human-annotated answer key, are shown in the middle column of Figure 3. 
2.2 Evaluation: Alignment 
The rightmost column of Figure 3 shows hypothetical output of an IE system. The 
first step of the evaluation algorithm, alignment, determines which templates in the 
system's output correspond to which ones in the key. Generally speaking, there can 
be any number of templates in the key and system response for a given document; 
all, some, or no pairs of which may be descriptions of the same event. The alignment 
algorithm must thus determine the correct emplate pairing. 
This process is not necessarily straightforward, since there may be no slot in a 
template that uniquely identifies the event or object which it describes. In response 
to this problem, the MUC community decided to adopt a relatively lax alignment 
criterion, leaving it to the alignment algorithm to find the alignment hat optimizes 
the resulting score. The procedure has two major steps. First, it determines which 
pairs of templates are possible candidates for alignment; the criterion for candidacy 
was only that the templates hare a common value for at least one slot. (Pointer fills 
share a common value if they point to objects that are aligned by the algorithm.) This 
criterion often results in alignment ambiguities--a key template will often share a 
common slot value with several templates in the system's output, and vice versa--  
and thus a method for selecting among the alternative mappings is necessary. The 
candidate pairs are rank ordered by a mapping score, which simply counts the number 
of slot values the templates have in common. 1 The scoring algorithm then considers 
1 The scoring software provided for MUC-6 allows for alternative scoring configurations based on slot 
content, including the ability to assign different weights to slots, but this was the configuration used 
for development and evaluation i MUC-6. 
235 
Computational Linguistics Volume 27, Number 2 
key and response template pairs according to this order, aligning them when neither 
member has already been mapped to another template. Ties between pairs with the 
same number of common slot values are broken arbitrarily. Because this algorithm is 
heuristic--with many combinations of alignments never being considered--the r sult 
may not be the globally optimal alignment in terms of score assigned. 
In our example, there is only one template of each type in each response, and 
thus there are no mapping ambiguities. The only requirement is that each pair share 
a common slot value, which is the case, and so the algorithm aligns each as shown in 
Figure 3. 
2.3 Evaluation: Scoring 
Once the templates are aligned, the scoring algorithm performs lot-by-slot compar- 
isons to determine rrors. The leftmost column in Figure 3 shows examples of the 
three types of errors that the algorithm will mark. First, while our hypothetical sys- 
tem recognized that the correct PERSON and ORGANIZATION are Julian Mounter and 
Star TV, respectively, it missed the ORG_ALIAS, ORG_DESCRIPTOR, PER_ALIAS, and 
PER_TITLE values that appear later in the passage, resulting in four missing slot fills 
(denoted by mis in the left hand column). Next, it also erroneously assigned a value 
to the ORG_LOCALE and ORG_COUNTRY slots in the ORGANIZATION, resulting in two 
spurious slot fills (denoted by spu). Finally, the system got three of the set fill slots 
wrong--the VACANCY_REASON slot in the SUCCESSION_EVENT, and the NEW_STATUS 
and ON_THE_JOB slots of the IN_AND_OuT template--resulting in three incorrect slot 
fills (denoted by inc). The remainder of the slot fills are correct (denoted by cor). 2 
Again, pointer slots are scored as correct when they point to templates that have been 
aligned. 
The possible fills are those in the key which contribute to the final score, and the 
actual fills are those in the system's response which contribute to the final score: 
POS = COR + INC + MIS 
ACT = COR + INC + SPU 
Three metrics are computed from these results: precision, recall, and F-score. Pre- 
cision is the number of correct fills divided by the total number generated by the 
system, and recall is the number of correct fills divided by the the total number in the 
key: 
COR 
PRE - 
ACT  
COR 
REC - 
POS 
In the example, there are 8 correct fills, 13 generated fills, and 15 possible fills 
in the key, resulting in a precision of 0.615 and a recall of 0.533. Note that the four 
missing slot fills do not figure into the precision computation, and the two spurious 
fills do not figure into the recall computation. F-score is determined by a harmonic 
mean of precision and recall (van Rijsbergen 1979): 
1 (f12 + 1) x PRE x REC 
(1 -- 1 1 1 1 (f12 X PRE) + REC 
2 There is also the possibility of getting partial credit, which we will not discuss further nor include in 
the equations below. 
236 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
In the standard computation of F-score, fl is set to one (indicating equal weight 
for precision and recall), resulting in: 
2 x PRE x REC 
F= 
PRE + REC 
For our example, we get an F-score of 0.571. 
2 x 0.615 x 0.533 = 0.571 
0.615 + 0.533 
2.4 Focus of the Paper 
Before proceeding to the analysis, we take care to note that our aim is neither to provide 
a thorough analysis of all problematic aspects of NLP evaluation, nor to provide a 
criticism of the MUCs in particular. Problematic aspects of the scoring procedures used 
in a variety of NLP evaluations are well attested, including the existence of side effects 
of scoring procedures which reward behavior that may not be perfectly consistent with 
the goals of the evaluations that these scoring procedures erve. For instance, word 
error metrics used in evaluations of speech recognition technology have been criticized 
for the fact that they assign the same degree of credit for the recognition of all words, 
a policy that rewards a focus on recognizing frequent but less important words (e.g., 
urn) over more important but less frequent content words. Similarly, evaluations of 
syntactic annotation systems that use locally oriented crossing brackets and labeled 
precision/recall metrics can assign high degrees of credit to cases in which the assigned 
structures are fairly inaccurate when viewed more globally. Likewise, there are aspects 
of the scoring procedure used for MUC-6 that one could question, such as the choice 
of slots included in each template and their corresponding definitions, the decision 
to weight all slots equally without regard to perceived importance, and the choice to 
define the templates to have hierarchical structure and give credit for slots that merely 
contain pointers to lower-level templates. We believe that any mechanical evaluation 
is likely to have such issues, and while they are very worthy of study and debate, a
more detailed discussion of them would take us too far afield from the main purpose 
of this paper. 
The focus of this paper is purposefully more narrow, being concerned only with 
the effects of alignment criteria on the goals of evaluation. While it is unclear as 
of this writing whether there will be future MUCs, evaluation-driven efforts in IE 
continue to be sponsored, and future evaluations of interpretation tasks of equal or 
greater complexity are not only likely but crucial if the field is to progress while 
maintaining its current focus on quantitative evaluation. Because alignment questions 
will almost certainly become exacerbated as the interpretation problems addressed 
become more complex, and because of the aforementioned pervasiveness of evaluation 
in the entire technology development process, the payoff in avoiding potential pitfalls 
in such evaluations i high. Thus, our aim is to bring lessons learned from the MUC 
experience to the fore so that they can inform future evaluations that, like the MUCs, 
are likely to be principal driving forces for research over extended periods of time. 
3. The Impact of Inaccurate Alignment 
In the introduction, we described several roles that the MUC-6 evaluation has played 
in bringing IE technology to its current state: task definition, system development, and 
cross-system evaluation. We focus here on its role in the system development process, 
where its influence has been substantial, as it has provided system developers the abil- 
ity to obtain rapid feedback with which to iteratively gauge their progress on a training 
237 
Computational Linguistics Volume 27, Number 2 
cor 
(TEMPLATE-024-1} 
CONTENT (SUCC_EVENT-024-1} 
(SUCC_EVENT-024-1) 
cor SUCCESSION_ORG (ORG-024-1} 
cor POST "CEO" 
cor IN_AND_OUT (IN_AND_OUT-024-1) 
inc VACANCY_REASON REASSIGNMENT 
(TEMPLATE-024-1} 
CONTENT (SUCC_EVENT-024-1) 
(SUCC_EVENT-024-1} 
SUCCESSION_ORG {ORG-024-1} 
POST "CEO" 
IN_AND_OUT (IN_AND_OUT-024-1} 
VACANCY_REASON OTH_UNK 
(ORG-024-1)  (ORG-024-1}  
inc ORG_NAME "STAR TV"  ORG_NAME "NEWS CORP."  
mis ORG_ALIAS "STAR" 
mis ORG_DESCRIPTOR "THE SAT BDSTR" 
cor ORG_TYPE COMPANY ORG_TYPE COMPANY 
(IN_AND_OUT-024-1} (IN_AND_OUT-024-1} 
cor IO_PERSON (PERSON-024-1} IO_PERSON (PERSON-024-1} 
inc NEW_STATUS OUT NEW_STATUS IN 
cor ON_THE_JOB UNCLEAR ON_THE_JOB UNCLEAR 
(PERSON-024-1) (PERSON-024-1) 
inc PERd~AME "JULIAN MOUNTER"  PER_NAME "RUPERT MURDOCH"  
inc PER_ALIAS "MOUNTER" PER_ALIAS "MURDOCH" 
cor PER_TITLE "MR." PER_TITLE "MR." 
Figure 4 
Target and actual outputs for the example text. 
corpus.  In essence, a deve loper  can incorporate  a new process ing s t rategy or data  mod-  
if ication, eva luate  the sys tem wi th  the change,  keep it if end- to -end  per fo rmance  (i.e., 
F-score) improves ,  and  w i thdraw it if it does  not. Often changes  have  unant ic ipated  
results,  and  thus a formal  eva luat ion  method is requ i red  to aff i rm or deny  deve lopers '  
(often mis lead ing)  intuit ions.  L ikewise,  the method has an ana logous  role for suppor t -  
ing systems that learn rules automat ica l ly .  In a typica l  learn ing scenario,  an automated  
procedure  i terat ive ly  proposes  rule modi f icat ions  and  adopts  them on ly  in those cases 
in wh ich  an object ive funct ion - - that  is, the eva luat ion  method- - ind icates  that perfor-  
mance  has improved.  Thus, it is abso lute ly  crucial  that both  pos i t ive  and  negat ive  sys- 
tem changes  are ref lected as such in the feedback  prov ided  by  the scor ing mechan ism.  
As we i l lustrate w i th  the passage  shown in F igure 2, the weak  a l ignment  cr i ter ion 
used  in MUC-6  causes the scor ing mechan ism to not  respect  this requ i rement .  3 The 
answer  key  in F igure 4 is as it was  in F igure  3, represent ing  the event  of Ju l ian Mounter  
leav ing the pos i t ion  of CEO at Star TV. The results  of an IE sys tem (in this case, a 
s l ight ly  mod i f ied  vers ion  of what  SRI's FASTUS \ [Appe l t  et al 1995\] extracted for this 
example)  are shown in the r ighthand co lumn.  The IE sys tem made two s igni f icant 
mis takes  on this text: It fa i led to extract any  in format ion  re lat ing to the correct event,  
and  it overgenerated  an incorrect  "event ,"  speci f ical ly  Ruper t  Murdoch 's  becoming  
CEO of News  Corp.  4 Thus, the sys tem's  prec is ion shou ld  be o = 0, s ince it generated  
3 In this section we will be arguing our point primarily on the basis of a single illustrative xample, 
where in fact the MUC development process utilized a 100-text corpus. The arguments extend to this 
broader setting, of course, a topic to which we return in Section 4. 
4 An anonymous reviewer points out that because no slot in the template structure can be guaranteed to
identify an object, intuitions alone are not enough (despite how strong they might be in a case such as 
this) to establish that the system output in Figure 4 actually represents a different event, rather than the 
event described in the output in Figure 3 corrupted by the selection of wrong names for certain slots. 
The FASTUS system produces byte offsets to tie information i  templates to the places in the text from 
which they were created, and from this we can confirm that the event was indeed created solely from 
textual material unrelated to the event described in the key. See also footnote 7. 
238 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
Table 1 
Approximate distribution of values for low-entropy slots. 
Template Type Slot Total Templates Slot Value Distribution 
SUCCESSION_EVENT VACANCY_REASON 214 REASSIGNMENT: 101 
OTH_UNK: 77 
NEW_POST_CREATED: 21 
DEPART_WORKFORCE: 18 
IN_AND_OuT NEW_STATUS 287 IN: 129 
OUT: 148 
IN_ACTING: 7 
OUT_ACTING: 4 
IN_AND_OuT ON_THE_JOB 287 YES: 82 
NO: 137 
UNCLEAR: 96 
IN_AND_OuT REL_OTHER_ORG 287 SAME_ORG: 93 
OUTSIDE_ORG: 66 
RELATED_ORG: 43 
(none): 91 
ORGANIZATION ORG_TYPE 117 COMPANY: 115 
GOVERNMENT: 3 
OTHER:  0 
PERSON PER_TITLE 158 "Mr.": 68 
"Dr.": 3 
"Ms.": 3 
(none): 84 
13 spur ious  slot fills for an i r re levant  event,  and  its recal l  shou ld  be o = 0, since it 
generated  none  of the 15 slots assoc iated wi th  the correct event.  5
In actuality, however ,  the scor ing a lgor i thm a l igned these templates  as shown in 
F igure 4, since each template  pa i r  shares at least one slot value.  With  this a l ignment ,  
the scor ing procedure  ident i f ies 8 correct slot values,  5 incorrect  values,  and  2 miss ing  
values,  resul t ing in a recal l  of 8 = 0.533, a prec is ion of 8 = 0.615, and  an F-score of 
0.571. These are the same scores received for the output  in F igure 3, in wh ich  a part ia l  
but  most ly  accurate descr ip t ion  of the correct event  was  extracted.  
The fact that the who l ly  incorrect output  in F igure 4 and  the largely  correct output  
in F igure 3 receive equa l ly  good  scores demonst ra tes  a ser ious f law wi th  the eva luat ion  
method.  As shown in Table 1, the d is t r ibut ions  of va lues  for several  slots in the MUC-6  
t ra in ing data  have re lat ive ly  low entropy,  and  thus a l ignments  based  on a fortu i tous 
over lap  in these va lues  are not  rare. 6 For  instance, the fact that  the ORG_TYPE slot in 
ORGANIZATION templates  has the va lue COMPANY in 115 out  of 117 instances a lmost  
ensures that any  ORGANIZATION template  produced by  a sys tem can get matched to an 
arb i t rary  one in the key  for a g iven document .  F rom this, in turn, the inaccurate  al ign- 
ments  may then cascade: Two unre la ted  SUCCESSION_EVENT templates  can be a l igned 
5 Its F-score should be undefined, since the denominator in the F-score quation will be zero. For all 
intents and purposes, however, the F-score can be considered to be zero, in the sense that its overall 
contribution to the F-score assigned to the results over a larger corpus will be zero. With this in mind, 
for simplicity we may speak of such cases as having an F-score of zero. 
6 In some cases, the sum of the counts in the rightmost column is greater than the total template count. 
This is because in some cases a key entry allowed for alternative slot values; matching any of the 
alternatives was sufficient for both alignment and scoring. Likewise, instances of optional slot fills were 
also included. 
239 
Computational Linguistics Volume 27, Number 2 
on the basis of sharing pointers to two unrelated (but nonetheless aligned) ORGANIZA- 
TION templates. Likewise, two unrelated IN_AND_OUT templates can be aligned on the 
basis of sharing pointers to two unrelated PERSON templates that share the value Mr. 
for the TITLE slot. Between this prospect and the three set fills for IN_AND_OUT tem- 
plates in Table 1, it is highly likely that an arbitrary IN_AND_OUT template produced by 
a system will overlap in at least one slot value with an arbitrary one in the key, which 
may in turn allow the SUCCESSION_EVENTS that point to them to be incorrectly aligned. 
Although the fact that the scoring algorithm is capable of assigning undeserved 
credit due to overly optimistic alignments i  not unknown to the MUC community (see, 
for instance, Aberdeen et al \[1995, 153, Table 4\] for a reference to "enthusiastic s oring 
mapping" for the Template Element ask of MUC-6), we are unaware of any previous 
acknowledgement of, or similar example which demonstrates, the potential severity 
of the problem to the extent hat Figure 4 does. This notwithstanding, one might still 
be tempted to view this behavior as relatively benign--perhaps there is no real harm 
done by giving systems the benefit of the doubt, along with a little undeserved credit 
that goes with it. While this will perhaps result in somewhat artificially inflated scores, 
there may be no reason to think that it would benefit one system or approach more 
than another, and this concession might seem reasonable considering the fact that there 
is likely to be no completely foolproof way to perform alignments. 
However, the potential harm that this behavior manifests in terms of the tech- 
nology development process--which, to our knowledge, has never been brought o 
light--is that it creates a situation in which uncontroversially positive changes in sys- 
tem output may result in a dramatically worse score, and likewise negative changes 
may result in a dramatically better score. Consider a (common) development scenario 
in which, starting from a state in which the system produces the output in Figure 4 
for the text in Figure 2, it is technically too difficult o modify the system to extract he 
correct (i.e., Julian Mounter) event, but in which a change can nonetheless be made to 
block the overgenerated (i.e., Rupert Murdoch) event. After such a modification, one 
would expect no change in recall, since no correct output is created or removed, and 
an improvement in precision, since an overgenerated vent is removed. 
What actually happens in this example is that recall drops from 0.533 (8) to zero 
(o), and precision goes from 0.615 (8) to undefined (0). To circumvent comparisons 
with undefined values, we can suppose that there was another, independent event 
extracted from a different part of the text that was aligned correctly against its cor- 
responding event in the answer key. For simplicity, we will assume that this event 
receives the same score as the overgenerated vent: a precision of 8 and a recall of 8 ig" 
With the overgenerated vent left in, the same scores as before are obtained: 16 ~___ 0.615 
16 ~--  0.533 recall, resulting in an F-score of 0.571. With the overgenerated precision and ~  
event removed, we obtain 8 = 0.615 precision and 8 = .267 recall, resulting in an 
F-score of 0.372. Instead of no change in recall and an improvement in precision, the 
reward for eliminating the overgenerated vent is the same precision, a 50% reduction 
in recall, and a 20-point reduction in F-score. Our clear "improvement" thus has the 
effect of reducing performance dramatically, implicitly instructing the developer to 
reintroduce the rules responsible for producing the overgenerated vent. 
The converse scenario yields an analogous problem. Consider a situation in which 
a system developer can add a rule to extract at least some of the information in 
the correct event--producing the output shown in Figure 5, for instance--but for 
whatever reason cannot make a change to block the overgenerated vent. We would 
expect his change to result in a marked increase in both recall and precision, since 
unlike before, information for a relevant event is now being produced. Indeed, the 
240 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
<TEMPLATE-024-1> 
CONTENT <SUCC_EVENT-024-1> 
(SUCC_EVENT-024-1> 
SUCCESSION_ORG (ORG-024-1> 
POST "CEO" 
IN_AND_OUT <IN_AND_OUT-024-1} 
VACANCY_REASON OTH_UNK 
(ORG-024-1> 
ORG_NAME "STAR TV" 
ORG-TYPE COMPANY 
Figure 5 
Additional output for example text. 
(IN_AND_OUT-024-1> 
IO_PERSON (PERSON-024-1) 
NEW_STATUS IN 
ON_THE_JOB NO 
<PERSON-024-1> 
PER_NAME "JULIAN MOUNTER" 
PER_ALIAS "MOUNTER" 
alignment algorithm will correctly align this event with the key and leave the Rupert 
Murdoch event unaligned, resulting in a precision of 9 = .600, a recall of 9 = .360, 
and an F-score of 0.450. This is the anticipated result, and would constitute a large 
increase over the zero F-score that the overgenerated vent should have received when 
standing alone. However, this is a substantial reduction from the F-score of 0.571 
that the overgenerated vent actually receives, a change which implicitly instructs the 
developer to remove the rules responsible for extracting the correct event. 
In these two scenarios, positive changes to system output resulted in a dramat- 
ically reduced score. The opposite situation can also occur, in which a change that 
reduces the quality of the system's response nonetheless receives an increased score. 
One can merely reverse the scenarios. For instance, in a situation in which no output 
is being created for the Julian Mounter event and a developer considers adding a rule 
that produces the Rupert Murdoch event, the rise in F-score will indicate that this 
rule should be kept. Likewise, starting with the incorrect output in Figure 4 together 
with the correct output in Figure 5, a developer might consider emoving the rule 
responsible for creating the correct output. This would cause the F-score to rise from 
0.450 to 0.571, implicitly instructing the developer to keep it removed. 
Thus, in all of these scenarios, the feedback provided by the evaluation method 
may steer our system developer off of the path to the optimal system state. Likewise, 
the same effect would occur when employing automatic learning methods that use 
F-score as an objective function. Starting from the state of producing the output in Fig- 
ure 4, for example, suppose the learning procedure could in fact propose ach change 
necessary to get to the desired output, that is, to (i) eliminate the rules producing the 
erroneous output, and (ii) add rules for producing the output shown in Figure 5. These 
changes would result in a precision of 9 = .60, a recall of 9 = .75, and an F-score 
of 0.667, which is an improvement over both the zero result that the current output 
should receive, and the (artificially inflated) score of 0.571 it actually does receive. 
However, the type of incremental search process that efficiency concerns generally 
necessitate--one that can only perform one of steps (i) or (ii) in a single iteration and 
will only adopt the proposed change if it improves on its objective function--will not 
find this system state, since as we have seen, either move taken first would actually 
reduce the F-score. 
To sum, the fortuitous alignments allowed by the MUC-6 evaluation method create 
a situation in which both positive and negative system changes may not be reflected 
as such in the evaluation results. While there are other properties of the evaluation 
that conspire to help produce these anomalous results--including the choice to score 
all slot fills equally without respect o importance or entropy of their distribution of 
values, and to score slots which contain only pointers to other templates--these only 
241 
Computational Linguistics Volume 27, Number 2 
serve to make the effects more or less dramatic than they might otherwise be. The 
root cause of this behavior is the alignment process: None of the foregoing behaviors 
would occur if the alignment criterion was such that the templates in Figure 4 were 
not alignable, thus producing an F-score of zero. 
4. A Case Study 
The foregoing examples demonstrate he pitfalls of not employing strong alignment 
constraints in natural anguage system evaluations. In the case of IE, the constraints 
should come as close as possible to establishing that two template representations 
are meant o describe the same events or objects. Just as it would be nonsensical for 
evaluations of part-of-speech tagging systems to give credit for a correct ag assigned 
to a different word, or evaluations of syntactic annotation systems to give credit for 
bracketings assigned to a different sentence, IE evaluations should not give credit for 
a template structure that represents a different event in the text. While it may be 
tempting to give systems the benefit of the doubt in light of the fact that alignment 
in IE is inherently more difficult than in these other scenarios, we have seen that the 
negative consequences of such a move can subvert he goals and purposes of the 
evaluation, and indeed the technology development process. 
Having said this, a question that naturally arises is how robust his effect actually 
was for the MUC-6 task. Is the example shown in Figure 4 exceptional for MUC-6, and 
thus useful mainly for pedagogical purposes, or is it indicative of a more pervasive 
problem that could impact development using a larger set of training documents? It 
is difficult to answer this question, of course, since one cannot replay past phases of 
technology development. However, we do have a case study with which to investigate 
this question, as we have previously performed a 13 1/2-month effort in which we 
focused on improving performance on the MUC-6 task. Specifically, our goal was to see 
how far the FASTUS paradigm could be pushed by way of making as many incremental 
improvements a possible. We relied heavily on the MUC-6 scoring mechanism during 
this process, using the feedback it provided to drive our development in the manner 
described in Section 3. Throughout this process, we remained ignorant of the problems 
that we are reporting on presently. 
As a result of our effort, we have a record of the output of our system as it ex- 
isted in 53 distinct states of development. We can compare the feedback provided by 
the scoring algorithm used during the development process (henceforth, the standard 
algorithm), with the feedback that would have been received from a more accurate 
and restrictive alignment criterion (henceforth, the restrictive algorithm), which we 
describe in Section 4.1. We report on two types of comparison, each at the level of in- 
dividual documents (Section 4.2) and the entire 100-text development set (Section 4.3). 
We first report on the overall effects that the restrictive algorithm has on scores for 
individual system states. We then report on the extent o which the two algorithms 
disagreed on the direction of the difference in performance between a pair of system 
states--that is,whether the changes implemented between these states had a positive 
or negative ffect--as this is the central factor that developers and learning algorithms 
use to determine whether to adopt proposed system changes. As the difference be- 
tween any pair of states constitutes a set of intermediate system changes that one can 
evaluate, our 53 distinct states provide us with 53?52 = 1,378 pairs to examine. 2 
4.1 A Stricter Alignment Criterion 
As we mentioned in footnote 1, the scoring system provided for MUC-6 allows one to 
customize alignment criteria based on slot content. Because no slot in a template will 
242 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
consistently and uniquely identify the event or object that it describes, it is difficult 
or impossible to design a perfect slot-based criterion. Nonetheless, there are more 
restrictive parameterizations that come much closer to producing only the correct 
alignments. We sought out the criterion that eliminates as many incorrect alignments 
as possible without being so restrictive that a system would be denied partial credit 
for correctly extracted information. 
Table 1 readily suggests a principled manner in which to restrict the mapping 
criterion: Avoid aligning templates olely on the basis of slots with only a small set 
of possible values (and in particular, those which have low entropy distributions), 
since shared values for these provide little evidence that the two templates represent 
the same entity or event. Indeed, our experience confirms that the large majority of 
alignment errors result from a fortuitous match on one of these slots. Each of the slots 
in Table 1 have at most four possible slot values, whereas the set of possible values for 
the remainder of the slots is essentially unbounded (except possibly for the POST slot 
of the SUCCESSION_EVENT template; we will return to this momentarily). Thus, while 
it is unlikely that templates for two unrelated companies will have the same company 
name, it is very likely that they both will have the value COMPANY in the ORG_TYPE 
slot. 
We therefore modified the MUC-6 alignment criterion so that any single shared 
value for a slot not in Table I is sufficient for alignment. (All other aspects of the align- 
ment criterion and procedure remained unchanged.) This criterion is still generous 
in some cases, for instance, the system output shown in Figure 4 receives an F-score 
of 0.143: The system will get credit for the coincidentally identical POST slot values 
and for the pointers to the SUCCESSION_\]~VENTS that will be aligned on the basis of 
those values. However, a criterion that bars such alignments may also disallow cer- 
tain cases in which a system should arguably deserve credit. 7While a small amount 
of undeserved credit may therefore remain, this amount is dramatically reduced from 
that which results from the standard algorithm. All slot values were still counted for 
scoring purposes, as in MUC-6. 
4.2 Effects of Stricter A l ignment  on Individual  Document  Results 
With our more restrictive alignment algorithm in hand, we begin by looking at its 
effect on performance at the document level. Only 53 of the 100 MUC-6 development 
texts were relevant, and because the recall of an irrelevant document is undefined 
regardless of what the system produces, only these 53 have defined F-scores. 
4.2.1 Effect on Scores. The restrictive algorithm tended to assign lower scores than 
the standard algorithm, as one would expect, since the best-scoring alignment found 
by the standard algorithm may be correctly disallowed by the restrictive algorithm. In 
7 As indicated in footnote 4, it is theoretically possible that a template produced by a system and a 
template in the answer key originate from a description ofthe same ntity or event, but in which the 
system's template isso corrupted by inaccurate orincomplete processing that the only correct slots that 
remain are included in Table 1. In our extensive analyses of system results, we found the number of 
such cases to be quite infrequent, and overwhelmed bythe number of cases in which alignments based 
only on these slots were demonstrably incorrect. One could argue about whether any partial credit is 
actually deserved in the former set of cases; in any case, we believe that not assigning the small 
amount of credit a system would receive isa small price to pay for rectifying the much greater negative 
effects of maintaining an overly lax alignment s rategy. Even if such partial credit is deemed eserved, 
however, our analyses suggest that the missed credit is more than made up for by the undeserved 
credit resulting from fortuitous matches on the POST slots of SUGCESSION_EVENT templates that our 
new criterion still allows, as described above. In fact, the overall effect of both appears to be rather 
negligible alone, and even more so when their opposite ffects on scores are taken together. 
243 
Computational Linguistics Volume 27, Number 2 
Madison Group Says Board Has Dismissed Lucas as Its President 
Madison Group Associates Inc. said its board dismissed Kenneth Lucas, president, naming Dean 
J. Trantalis as interim president. 
The company also said two new directors - Roland Breton and Steve Gibboney - had been appointed 
to its board. 
Mr. Lucas became chief executive of the media concern less than two months ago, when William 
T. Craig resigned from his job as a director and chief executive officer. 
The company gave no reason for Mr. Lucas's dismissal. Neither he nor Madison executives could 
be reached for comment. 
The management change is the latest in a series of events that have shaken the company in recent 
months. As previously reported, the Securities and Exchange Commission contacted several individuals 
about their dealings with the company. One of those individuals aid the SEC had asked about how the 
company valued its assets. 
Those assets consist largely of video libraries. According to a recent securities filing, an accountant 
formerly hired by Madison recommended that an independent specialist be hired to evaluate the video 
libraries. 
Figure 6 
Example text from MUC-6 development set (9403100087). 
our system runs, the scores for an average of 21.2 of the 53 documents were reduced. 
Thus, in a typical run, a substantial percentage of the document results--40%--had 
benefited from incorrect alignments from the standard algorithm. The magnitude of 
the reduction in document scores ranged from 0.14 to 36.84 points of F-score, averaging 
7.74. 
Interestingly, there were also cases in which the restrictive algorithm actually as- 
signed a higher score to the results for a document than the standard algorithm. One 
might wonder how this could happen, since the set of possible alignments allowed 
by the restrictive algorithm is a strict subset of those allowed by the standard algo- 
rithm. The reason lies in the fact that the alignment procedure does not perform an 
exhaustive search; instead, it uses the heuristic search method described in Section 2. 
It is therefore possible that an optimal but nonetheless correct solution exists which 
the standard algorithm does not find, but which the restrictive algorithm finds within 
the narrower search-space associated with its more restrictive criterion. 
Figure 6 shows an example from the MUC-6 development corpus, and Figures 7 
and 8 show a fragment of the alignments produced by the standard and restrictive al- 
gorithms, respectively. All of the remaining system output not shown received the 
same alignment by both algorithms. William Craig was represented by templates 
(PERSON-087-5) in the key and (PERSON-087-8) in the system response, and Kenneth 
Lucas was represented by templates (PERSON-087-1) in the key and {PERSON-087-20) 
in the system response; both were aligned correctly by both algorithms. 
The alignment in Figure 7, along with the remainder of the output not shown, 
results in an F-score of 69.44 for the text. Template /IN_AND_OUT-087-1) is aligned 
with (IN_AND_OuT-087-1), and (IN_AND_OuT-087-4) is aligned with (IN_AND_OUT-087- 
3), although in neither case do the IO_PERSON slots point to the (correctly) aligned 
PERSON templates. Nonetheless, each pair yields two correct values, specifically for 
the NEW_STATUS and ON_THE_JOB slots. 
This alignment is not possible with the restrictive algorithm, since it is performed 
solely on the basis of slots listed in Table 1. The restrictive algorithm finds the opposite 
alignment between the IN_AND_OuT templates, hown in Figure 8. This mapping also 
yields two correct slot fills for each IN_AND_OUT template, in this case, the IO_PERSON 
244 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
COR (IN-AND_OUT-087-1) (IN-AND_OUT-087-1) 
inc IO~PERSON:  (PERSON-087-1) (PERSON-087-8) 
cor NEW_STATUS: OUT OUT 
cor ON_THE_JOB: UNCLEAR UNCLEAR 
COR (IN_AND_OUT-087-4) (IN-AND_OUT-087-3) 
inc IO_PERSON:  (PERSON-087-5) (PERSON-087-20) 
cor NEW-STATUS: OUT OUT 
cor ON_THE_JOB: No No 
spu OTHER_ORG: (ORGANIZATION-087-3) 
spu REL_OTHER_ORG: OUTSIDE_ORG 
COR (SUCCESSION_EVENT-087-1) (SUCCESSION_EVENT-087-2) 
cor SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-9) 
cor POST: "PRESIDENT . . . .  PRESIDENT" 
cor IN-AND_OUT: (IN-AND_OUT-087-2) (IN-AND_OUT-087-4) 
inc (IN-AND_OUT-087-1) (IN-AND_OUT-087-3) 
cor VACANCY_REASON: REASSIGNMENT REASSIGNMENT 
COR (SUCCESS1ONfl~VENT-087-2) (SUCCESSION_EVENT-087-1) 
inc SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-3) 
cor POST: "CHIEF EXECUTIVE . . . .  CHIEF EXECUTIVE" 
cor IN-AND_OUT: (IN-AND_OUT-087-3) (1N-AND_OUT-087-2) 
inc (IN-AND_OUT-087-4) (IN-AND_OUT-087-1) 
inc VACANCY_REASON: REASSIGNMENT OTH-UNK 
Figure 7 
Alignment for text 9403100087 with the standard algorithm. 
COR (IN_AND_OUT-087-4) (IN_AND_OUT-087-1) 
cor IO_PERSON:  (PERSON-087-5) (PERSON-087-8) 
cor NEW_STATUS: OUT OUT 
inc ON_THE_JOB: No UNCLEAR 
COR (1NAND_OUT-087-1) (INAND_OUT-087-3) 
cor IOA)ERSON:  (PERSON-087-1) (PERSON-087-20) 
cor NEW_STATUS: OUT OUT 
inc ON_THE_JOB: UNCLEAR NO 
spu OTHER_ORG: (ORGANIZATION-087-3) 
spu REL_OTHER_ORG: OUTSIDE_ORG 
COR (SUCCESSIONfl~VENT-087-1) (SUCCESSION_EVENT-087-2) 
cor SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-9) 
cor POST: "PRESIDENT . . . .  PRESIDENT" 
cor IN.AND_OUT: ( IN_AND_OUT-087-1) (IN-AND_OUT-087-3) 
cor (IN_AND_OUT-087-2) (IN_AND_OUT-087o4) 
cor VACANCY_REASON: REASSIGNMENT REASSIGNMENT 
COR (SUCCESSION~VENT-087-2) (SUCCESSION_EVENT-087-1) 
inc SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-3) 
cor POST: "CHIEF EXECUTIVE" "CHIEF EXECUTIVE" 
cor IN.AND_OUT: (IN-AND_OUT-087-4) (IN-AND_OUT-087-1) 
cor (IN-AND_OUT-087-3} (IN-AND_OUT-087-2) 
inc VACANCY_REASON: REASSIGNMENT OTH_UNK 
Figure 8 
Alignment for text 9403100087 with the restrictive algorithm. 
and  NEW_STATUS slots. This a l ignment - -wh ich  is the correct one- - resu l t s  in an F- 
score of 75.00: Despi te  the fact that  the SUCCESSION_EVENT templates  are a l igned the 
same way in both  cases, the restr ict ive a lgor i thm a l ignment  leads to two add i t iona l  
correct fills for the po in ters  to the proper ly  a l igned IN_AND_OUT templates.  Whi le  this 
a l ignment  was  a poss ib le  so lut ion for the s tandard  a lgor i thm,  it arb i t rar i ly  chose the 
wrong pa i r ing  of IN_AND_OuT templates - -both  possib i l i t ies  resu l ted in two shared slot 
va lues - -and  thus the system was den ied  more  than five po ints  of F-score on the article. 
245 
Computational Linguistics Volume 27, Number 2 
This behavior was not specific to this example in our system runs; 18 of the 53 
relevant texts (34%) displayed this behavior for at least one of the 53 system states. An 
average of 2.7 documents rose in score per system run. The magnitude of the increase 
in document score ranged from 0.25 to 7.27 points of F-score. 
To sum, the more accurate alignment criterion affected approximately 24 out of 
53 relevant documents (45%) for an average system run. In most cases, the effect was 
to reduce the score assigned by the standard algorithm, since the best-scoring (albeit 
incorrect) alignment found by the standard algorithm was often disallowed by the 
restrictive algorithm. However, due to the fact that the heuristic search process used 
for alignment is less likely to find the optimal mapping with the standard criterion, 
there were also cases in which the effect was to increase the score. 
4.2.2 Diverging Indications Between System States. We now ask to what extent hese 
documents exhibit the behaviors een in the four development scenarios described 
in Section 3. In the first two of these scenarios, a system change that should have 
improved F-score decreased it instead. For these, we would expect he restrictive al- 
gorithm to correctly indicate an improvement. In the second two scenarios, a system 
change that should have decreased F-score increased it instead. For these, we would ex- 
pect the restrictive algorithm to correctly indicate a reduction. Thus, we are interested 
in identifying those documents for which one algorithm signaled an improvement and 
the other signaled a reduction in score between a pair of system states. 
It turns out that the output for 42 of the 53 relevant documents (79%) displayed 
this behavior for at least one pair of system states. The magnitude of the difference 
between document scores varied greatly, from 0.10 to 25.00 points of F-score. In the 
case of the 25.00 point difference, the standard algorithm indicated a change from 
59.62 to 51.69, whereas the restrictive algorithm indicated achange from 34.62 to 51.69. 
Thus, while both algorithms assigned the same score to the second system state, the 
standard algorithm had assigned undeserved credit to the first system state, and what 
should have resulted in a 17-point improvement was shown instead as an 8-point 
reduction. 
To sum, the problems we noted in Section 3 are not peculiar to the example shown 
in Figure 2; examples exhibiting this behavior are readily found in practice. 
4.3 Effects of Stricter Alignment on Entire MUC-6 Development Set Results 
In an actual development setting, of course, developers generally do not focus on 
differences in F-score for a single text, but rely instead on the scores assigned to the 
entire 100-text development corpus. Although the previous discussion showed that 
our development scenarios occur in practice, it is quite possible that these document- 
level differences were inconsequential n terms of the feedback obtained for the entire 
development set. We thus look at the difference between the algorithms with respect 
to the scores they assign to this larger set. 
4.3.1 Effect on Scores. Unsurprisingly, the restrictive algorithm assigned a lower over- 
all score than the standard algorithm for all 53 system states. The magnitude of the 
reduction ranged from 2.09 to 4.97 points of F-score, averaging 3.29 points. 
4.3.2 Diverging Indications Between System States. We now ask if the two algorithms 
ever disagree about whether the difference between two system states constitutes a 
positive or negative change. This occurred for 90 of the 1,378 system state pairs (6.5%). 
The magnitude of the difference between the changes of performance measured by 
each algorithm ranged from 0.14 to 2.55 points, averaging 1.38. The magnitude was 
246 
Kehler, Bear, and Appelt Accurate Alignment in Evaluation 
less than I point in 33 cases, between i and 2 points in 42 cases, and was greater than 
2 points in 15 cases. 
These differences, and in particular those in the 2-point range, are large enough 
that they could affect a developer's decision about whether to adopt proposed system 
changes. In the case in which the difference was 2.55 points, for instance, a posi- 
tive change was reported by the standard algorithm as a negative one: It indicated a
decrement in performance of 1.26 points (from 56.74 to 55.48), whereas the restrictive 
algorithm indicated an increment of 1.29 points (from 51.84 to 53.13). In our experi- 
ence, by the time an IE system is in the 50-point performance range on the MUC-6 
task, the majority of further progress results from a series of incremental changes that 
have a relatively small affect on the overall score. Thus, a change that decreases per- 
formance by 1.26 points will almost certainly be removed from the system, whereas a
change that increases performance by 1.29 will almost certainly be kept. 
There were also cases in which a negative change was reported by the standard 
algorithm as a positive one. For one pair of system states, for example, the standard 
algorithm indicated an increase of 2.13 points (from 51.49 to 53.62), whereas the restric- 
tive algorithm indicated a reduction of 0.26 (from 48.91 to 48.65). Again, this difference 
could cause a developer to keep rules in the system that negatively impact he quality 
of its output. 
4.4 Summary 
A study of the results of a 13 1/2-month effort focused on the MUC-6 task suggests that 
the problems described in Section 3 are not merely pedagogical, but can and do occur 
in actual practice. These problems are prevalent enough that they could realistically 
affect he technology development process in an adverse manner. 
While it would be difficult to determine the extent o which these problems may 
have affected evelopment in MUC-6, we take care to note that we do not find that 
their severity was so strong that they, in and of themselves, compromised the integrity 
of the MUC-6 evaluation process. Indeed, the existing evidence suggests that any 
impact he alignment criterion may have had on the manner in which MUC-6 systems 
were developed, as well as how they were ranked with respect o each other in the 
final evaluation, was not likely to have been dramatic. 
The outcome of our study is therefore dually positive, in that the results demon- 
strate the potential of a lax alignment strategy to have a dramatically adverse ffect 
on the technology development process, without this potential having actually been 
fully realized in MUC-6 itself. It should be borne in mind that it would not be dif- 
ficult to construct a scenario in which the ramifications for a MUC-like task would 
have been far more severe--with a different task specification, template structure, set 
of slot definitions, or scoring scheme, for instance--to the extent hat the integrity of 
such an evaluation could be compromised. Thus, these results erve to highlight a po- 
tential pitfall to be avoided in future evaluations of IE and other high-level language 
processing tasks, which, like the MUCs, may be the principle driving forces behind 
technology development for extended periods of time. 
5. Conclusions 
Methods for evaluating NLP systems are essential for tracking progress during the 
technology development process. To properly drive this process in both system build- 
ing and machine learning settings, it is crucial that positive and negative modifications 
be reflected as such in the feedback provided by the scoring mechanism. We have pre- 
sented several scenarios which demonstrate hat the alignment strategy employed in 
247 
Computational Linguistics Volume 27, Number 2 
the MUC-6 evaluation creates a situation in which this requirement is not respected. 
Furthermore, we have shown that the problem is pervasive nough that it could re- 
alistically impact the development process using a larger set of development data. 
These results argue strongly for the use of strict and accurate alignment criteria in 
future natural language evaluations and for maintaining the independence of align- 
ment criteria and the mechanisms used to calculate scores. This lesson is important 
because alignment problems will likely become exacerbated in future evaluations, as 
the natural anguage applications addressed become yet more complex. 
Acknowledgments 
This work was completed while the first 
author was at SRI International. We would 
like to thank Nancy Chinchor, Lynette 
Hirschman, Jerry Hobbs, David Israel, 
Andreas Stolcke, Beth Sundheim, Mabry 
Tyson, Ralph Weischedel, and two 
anonymous reviewers for helpful comments 
on earlier versions of the paper. All 
opinions expressed herein remain our own. 
This work would not have been possible 
without he contributions of the following 
people at SRI: Jerry Hobbs, David Israel 
Megumi Kameyama, David Martin, Karen 
Myers, and Mabry Tyson. 
References 
Aberdeen, John, John Burger, David Day, 
Lynette Hirschman, Patricia Robinson, 
and Marc Vilain. 1995. MITRE: 
Description of the Alembic system used 
for MUC-6. In Proceedings ofthe Sixth 
Message Understanding Conference (MUC-6), 
pages 141-155, Columbia, MD, 
November. Morgan Kaufmann. 
Appelt, Douglas E., Jerry R. Hobbs, John 
Bear, David Israel, Megumi Kameyama, 
Andy Kehler, David Martin, Karen Myers, 
and Mabry Tyson. 1995. SRI International 
FASTUS system MUC-6 test results and 
analysis. In Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 237-248, Columbia, MD, 
November. Morgan Kaufmann. 
Chinchor, Nancy and Gary Dungca. 1995. 
Four scorers and seven years ago: The 
scoring method for MUC-6. In Proceedings 
of the Sixth Message Understanding 
Conference (MUC-6), pages 33-38, 
Columbia, MD, November. Morgan 
Kaufmann. 
CSL Special Issue. 1998. Special Issue on 
Evaluation in Language and Speech 
Technology. Computer Speech and Language, 
12(4). 
Grishman, Ralph and Beth Sundheim. 1995. 
Design of the MUC-6 evaluation. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 1-11, Columbia, MD, November. 
Morgan Kaufmann. 
Hirschrnan, L. 1998. The evolution of 
evaluation: Lessons from the message 
understanding conferences. Computer 
Speech and Language, 12(4):281-305. 
Sparck-Jones, Karen and Julia Rose Galliers. 
1996. Evaluating Natural Language 
Processing Systems: An Analysis and Review. 
Lecture notes in computer science, 1083. 
Springer. 
van Rijsbergen, C. J. 1979. Information 
Retrieval. Butterworths, London. 
248 
The (Non)Utility of Predicate-Argument Frequencies for Pronoun
Interpretation
Andrew Kehler?
UC San Diego
akehler@ucsd.edu
Douglas Appelt
SRI International
appelt@ai.sri.com
Lara Taylor?
UC San Diego
lmtaylor@ucsd.edu
Aleksandr Simma?
UC San Diego
asimma@ucsd.edu
Abstract
State-of-the-art pronoun interpretation sys-
tems rely predominantly on morphosyntac-
tic contextual features. While the use of
deep knowledge and inference to improve
these models would appear technically in-
feasible, previous work has suggested that
predicate-argument statistics mined from
naturally-occurring data could provide a
useful approximation to such knowledge.
We test this idea in several system configu-
rations, and conclude from our results and
subsequent error analysis that such statis-
tics offer little or no predictive information
above that provided by morphosyntax.
1 Introduction
The last several years has seen a number of works
that use weight-based systems (trained either man-
ually or via supervised learning) for pronoun in-
terpretation, in addition to others that have ad-
dressed the broader task of entity-level coreference
(see Mitkov (2002) for a useful survey). These sys-
tems typically rely on a variety of morphosyntactic
factors that have been posited in the literature to
affect the interpretation of pronouns in naturally-
occurring discourse, including gender and number
agreement, the distance between the pronoun and
antecedent, the grammatical positions of the pro-
noun and antecedent, and the linguistic form of the
antecedent, among others. A common refrain is that
the performance of systems that rely on such fea-
tures is plateauing, and that further progress will re-
quire the use of world knowledge and inference (ibid.,
Ch. 9, inter alia). World knowledge, after all, would
seem to play a role in determining that the referent of
it in example (1) is the entity denoted by his industry
rather than Glendening?s initiative or the edge.
?Department of Linguistics.
?Department of Computer Science and Engineering.
(1) He worries that Glendening?s initiative could
push his industry over the edge, forcing it to
shift operations elsewhere.
Of course, no well-suited knowledge base and accom-
panying inference procedure exists that can deliver
such a capability robustly in an open domain.
In lieu of this capability, previous authors have
suggested that what can be viewed as a more su-
perficial form of semantic information ? predicate-
argument statistics mined from naturally-occurring
data ? could be used to capture certain selectional
regularities. For instance, such statistics might re-
veal that forcing industry is a more likely verb-
object combination in naturally-occurring data than
forcing initiative or forcing edge. Assuming that
such statistics imply that industries are more likely
to be forced in the real world than are initiatives or
edges, this information could be taken to establish a
preference for his industry as the antecedent of it in
(1). While there will always be cases that require ar-
bitrarily deep knowledge for their interpretation, the
empirical question of how far one can go by relying
on this sort of selectional information remains.
Our point of departure is the work of Lappin
and Leass (1994, henceforth L&L) and Dagan et
al. (1995). (See also Dagan and Itai (1990).) L&L
demonstrated with a system called RAP that a
(manually-tuned) weight-based scheme for integrat-
ing pronoun interpretation preferences can achieve
high performance on real data, in their case, 86%
accuracy on a corpus of computer training manu-
als.1 Dagan et al (1995) then developed a postpro-
cessor based on predicate-argument statistics that
was used to override RAP?s decision when it failed
to express a clear preference between two or more
antecedents, which resulted in a modest rise in per-
1Kennedy and Boguraev (1996, henceforth, K&B)
adapted L&L?s algorithm to rely on far less syntac-
tic analysis (noun phrase identification and rudimentary
grammatical role marking), with performance in the 75%
range on mixed genres.
formance (2.5%).2 Because RAP is symbolic, the
two systems were necessarily coupled in a black-
box manner. They noted, however, that if one had
a statistically-driven pronoun interpretation system,
co-occurrence information could be modeled along-
side morphosyntactic information:
?A promising direction for future research
is the development of an empirically based
model for salience criteria analogous to the
one that we constructed for lexical prefer-
ence. The integration of these models using
a probabilistic decision procedure will hope-
fully yield an optimized integrated system
for anaphora resolution.? (p. 643)
In this work we set out to evaluate Dagan et al?s
proposal. Indeed, the weight-combination scheme of
L&L is suggestive of a particular approach to super-
vised learning ? maximum entropy (MaxEnt) ? in
which such a system of weights is inferred from max-
imum likelihood counts on annotated data. Using
MaxEnt, we trained a system based on an optimized
set of morphosyntactic features and augmented it
with predicate-argument statistics in two scenarios:
(i) one mimicking the Dagan et al postprocessor,
and (ii) one in which the predicate-argument statis-
tics were represented as features alongside the mor-
phosyntactic features. Our results and subsequent
error analysis suggest, however, that such statistics
offer little or no predictive information above that
provided by morphosyntax.
2 Corpora Used
The training and test data sets came from the news-
paper and newswire segments of the Automatic Con-
tent Extraction (ACE) program corpus. The train-
ing data contained 2773 annotated third-person pro-
nouns, and the test data (the February 2002 evalua-
tion set) contained 762 annotated third-person pro-
nouns. The performance statistics on the test data
reported here are from the only time an evaluation
with this data was performed; progress during devel-
opment was estimated solely via jackknifing on the
training data.
The annotated pronouns included only those that
were ACE ?markables?, i.e., ones that referred to en-
tities of the following types: Persons, Organiza-
tions, GeoPoliticalEntities (politically defined
2The difference amounted to 9 additional correct pre-
dictions in a corpus of 360 examples. They express a be-
lief that the improvement is real, but acknowledge that
they would need twice as many examples in their corpus
to reach statistical significance.
geographical regions, their governments, or their
people), Locations, and Facilities. Thus, there
were pronouns in both the development and (pre-
sumably) test sets for which there were no annota-
tions. As such, certain problems that real-world sys-
tems face, such as non-referential (e.g., ?pleonastic?)
pronouns and pronouns that refer to eventualities,
did not have to be dealt with. (However, these pro-
nouns were possible antecedents to other pronouns,
and thus were sometimes mistakenly selected as the
correct antecedent.) Thus, our results are not nec-
essarily comparable to those of a system that deals
with these difficulties (although previous work varies
a fair bit on how their datasets were filtered in this re-
gard). Our main purpose here is to establish a state-
of-the-art baseline with which to assess the contribu-
tion of predicate-argument frequency information.
3 Learning Algorithms
We implemented three pronoun interpretation sys-
tems: a MaxEnt model, a Naive Bayes model, and
a version of the Hobbs algorithm as a baseline. Our
experimentation was driven predominantly using the
MaxEnt system using an n-fold jackknifing paradigm
(n was typically three). Naive Bayes was imple-
mented toward the end of the project as a machine
learning baseline. Both machine learning algorithms
were trained as binary coreference classifiers, that
is, the examples provided to them consisted of pair-
ings of a pronoun and a possible antecedent phrase,
along with a binary coreference outcome determined
from the annotated keys. Thus, for a given pronoun
there was one example generated for each possible
antecedent phrase. So as to focus learning on only
the coreferential phrase that is most likely to have
been directly responsible for a given pronominaliza-
tion, all coreferential phrases except the closest in
terms of Hobbs distance (discussed later) were elimi-
nated before training. Because we are ultimately in-
terested in identifying the correct antecedent among
a list of possible ones, during testing the antecedent
assigned the highest probability was chosen.
These systems received as input the results of
SRI?s TextPro system, a chunk-style shallow parser
capable of recognizing low-level constituents (noun
groups, verb groups, etc.). No difficult attachments
are attempted, and the results are errorful. There
was no human-annotated linguistic information in
the input. The systems are described further below.
Maximum Entropy Modeling As previously in-
dicated, the weight-based scheme of L&L suggests
MaxEnt modeling (Berger et al, 1996) as a particu-
larly natural choice for a machine learning approach.
In MaxEnt, the parameters of an exponential model
of the following form are estimated:
p(y|x) = e
?
i ?ifi(x,y)
?
y e
?
i ?ifi(x,y)
The variable y represents the outcome (coreference
or not) and x represents the context. There is
one value for each feature that predicts coreference
behavior, represented by the parameters ?1, ..., ?n,
which are Lagrange multipliers that constrain the ex-
pected value of each feature in the model to be the
values found in the distribution of the training data.
(The fi(x, y) are indicator functions which equal 1
when the corresponding feature is present, and 0
otherwise.) The desired values for these parame-
ters are obtained by maximizing the likelihood of
the training data with respect to the model.3 Thus,
whereas L&L?s RAP system uses an additive system
of weights that is trained manually, the MaxEnt sys-
tem learns a multiplicative system of weights auto-
matically. One can view the MaxEnt system as yield-
ing a probabilistic notion of antecedent salience: The
salience value assigned to a potential antecedent of
a given pronoun is just the probability that Maxent
assigns to the outcome of coreference.
Naive Bayes In Naive Bayes modeling, a Bayesian
probability distribution is estimated under a strong
assumption: that all of the features are conditionally
independent given the target value. Thus given n
features xi with respect to the context x, we have:
p(y|x) = p(y)p(x|y)p(x) ?
p(y) ?ni=1 p(xi|y)
p(x)
The context x is constant for each outcome y, so we
only need to find:
argmax
y?{0,1}
p(y)
n?
i=1
p(xi|y)
For most natural language processing scenarios, in-
cluding ours, this independence assumption is almost
certainly false. Nonetheless, Naive Bayes models
seem to work well in practice when used as classifiers.
That is, the choice that receives the highest probabil-
ity (relative to the other choices) is often the correct
one even though the actual probabilities the model
generates may not be very good. These models have
the advantage that they are efficiently trained; only
a single pass through the training data is necessary.
3The results reported here were produced by using the
improved iterative scaling algorithm with binary-
valued features. We also experimented with real-valued
features, with highly similar results on jackknifed data.
Hobbs Algorithm We also implemented a version
of Hobbs?s (1978) well-known pronoun interpretation
algorithm as a baseline, in which no machine learning
is involved. His algorithm takes the syntactic repre-
sentations of the sentences up to and including the
current sentence as input, and performs a search for
an antecedent noun phrase on these trees. Since our
shallow parsing system does not build full syntactic
trees for the input, we developed a version that does
a simple search through the list of noun groups recog-
nized. In accordance with Hobbs?s search procedure,
noun groups are searched in the following order: (i)
in the current sentence from right-to-left, starting
with the first noun group to the left of the pronoun,
(ii) in the previous sentence from left-to-right, (iii) in
two sentences prior from left-to-right, and (iv) in the
current sentence from left-to-right, starting with the
first noun group to the right of the pronoun (for cat-
aphora). The first noun group that agrees with the
pronoun with respect to number, gender, and person
is chosen as the antecedent.
4 Features
Our automatically trained systems employed a set of
hard constraints and soft features. Hard con-
straints are used to weed out potential antecedents
before they are sent to the machine learning algo-
rithm. There are only two such constraints, one
based on number agreement and one based on gen-
der agreement. Both are conservative in their appli-
cation. The soft features are used by the machine
learning algorithm. After considerable experimenta-
tion we settled on a set of forty such features, not
including predicate-argument features that will be
described in Section 5. These features fall into five
categories, listed here with abbreviations that will be
used in the tables given in Section 6:
Gender Agreement (gend): Includes features to
test a strict match of gender (e.g., a male pro-
noun and male antecedent), as well as mere
compatibility (e.g., a male pronoun with an an-
tecedent of unknown gender). These features
are more liberal than the gender-based hard con-
straint mentioned above.
Number Agreement (num): Includes features to
test a strict match of number (e.g., a singu-
lar pronoun and singular antecedent), as well
as mere compatibility (e.g., a singular pro-
noun with an antecedent of unknown number).
These features are likewise more liberal than the
number-based hard constraint mentioned above.
Distance (dist): Includes features pertaining to
the distance between the pronoun and the po-
tential antecedent. Examples include the num-
ber of sentences between them and the ?Hobbs
distance?, that is, the number of noun groups
that Hobbs?s search algorithm has to skip be-
fore the potential antecedent is found (Hobbs,
1978; Ge et al, 1998).
Grammatical Role (pos): Includes features per-
taining to the syntactic position of the potential
antecedent. Examples include whether the po-
tential antecedent appears to be the subject or
object of a verb, and whether the potential an-
tecedent is embedded in a prepositional phrase.
Linguistic Form (lform): Includes features per-
taining to the referential form of the potential
antecedent, e.g., whether it is a proper name,
definite description, indefinite NP, or a pronoun.
The values of these features ? computed from our
system?s errorful shallow constituent parses ? com-
prised the input to the learning algorithms, along
with the outcome as indicated by the annotated key.
5 Predicate-Argument Frequencies
With a trained statistical model for pronoun inter-
pretation in hand, we can now consider the use of
predicate-argument statistics to improve it. Con-
sider sentence (1) again, repeated as (2).
(2) He worries that Glendening?s initiative could
push his industry over the edge, forcing it to
shift operations elsewhere.
Suppose that our system selects the edge as the an-
tecedent of it instead of his industry. It turns out
that in a large corpus of shallowly-parsed data (par-
ticularly the newswire subset of the TDT-2 corpus,
see below), industr(y|ies) appears nine times as the
head of the object noun phrase of force (in its var-
ious number/tense combinations), whereas edge(s)
never does.4 So by collecting predicate-argument co-
occurrence statistics, one could extract the ?knowl-
edge? that industries are (statistically speaking)
more likely to be forced than edges are, and pos-
sibly use this information to change the prediction
of the statistical model.
We utilized three types of predicate-argument
statistics in our experiments: subject-verb, verb-
object, and possessive-noun. We processed the entire
4Likewise, the subject-verb combination industr(y|ies)
shift occurs three times in the corpus whereas edge(s)
shift does not.
newswire subset of the Topic Detection and Tracking
(TDT-2) corpus with TextPro, which resulted in
1,321,072 subject-verb relationships, 1,167,189 verb-
object relationships, and 301,477 possessive-noun re-
lationships. Words were categorized by their lem-
mas when available, and proper names for each of
the ACE entity types were classified into respective
classes (i.e., proper person names all counted as in-
stances of proper person).
While counts were collected for a broad range
of predicate-argument combinations, there were still
many combinations that were only seen once or
twice, and certainly other possible combinations ex-
ist that were not seen at all. The distribution
that these statistics yield therefore needed to be
smoothed. We took two approaches to smooth-
ing. First, because Dagan et al used Good-Turing
smoothing in their experiments, we did likewise so
as to replicate their work as closely as possible. Sec-
ond, we tried an approach based on the distributional
clustering method of Pereira et al (1993). This
method yielded word classes that offered more ro-
bust count approximations for their member words.
However, both methods yielded similar results when
embedded in the larger system, and so we will report
on the results of using Good-Turing so as to remain
more directly comparable to Dagan et al
The smoothed predicate-argument statistics were
employed in two ways. First, we built a postpro-
cessing filter modeled directly on Dagan et al?s sys-
tem. Their implementation made use of two equa-
tions. The first computes the frequency with which
a candidate head noun C is found with the predi-
cate word A, normalized by the number of times C
is found alone, so as to not bias the statistic towards
words that are common in isolation:
stat(C) = P (tuple(C,A) | C) = freq[tuple(C,A)]freq(C)
The second equation then weighs the difference in
statistical co-occurrence against the different salience
values assigned by the pronoun interpretation mod-
ule for two competing candidates C1 and C2:
ln
(stat(C2)
stat(C1)
)
> K ? [salience(C1)? salience(C2)]
The parameter K determines the threshold at which
statistical preferences supersede salience preferences.
In our implementation, the measure of salience is
simply the probability of coreference assigned by the
statistical model. Another parameter max sets a
threshold for the maximum difference between the
salience values for the two candidates; any pair for
which this difference exceeds max will not be con-
sidered. For each combination of feature sets that
we evaluated (see Section 6), we performed addi-
tional experiments to determine the optimal values
of K and max. To keep consistent with Dagan et al,
statistics were not used (here or in the other MaxEnt
system) for potential antecedents that were them-
selves pronominal. To properly use statistics in such
cases, the system would need to have access to an
antecedent for the pronoun that has a lexical head;
neither model was given access to such information.
In our second approach, we simply developed fea-
tures that represent the magnitude of the predicate-
argument statistics and utilized them during Max-
Ent training along with the morphosyntactic features
described earlier. The statistics were normalized by
dividing them by the total counts for the head of
the potential antecedent in the relevant predicate-
argument configuration.5
In certain respects these different system config-
urations mirror questions about pronoun interpre-
tation that linger in the theoretical and psycholin-
guistics literature. A result showing that the post-
processing filter version works best might provide
evidence, as has been suggested, that people pri-
marily use morphosyntactic features to resolve pro-
nouns, relying on semantic information only when
more than one possibility remains active. A result
showing that the integrated version works best might
suggest that semantic information is used in concert
with morphosyntactic information. Finally, a result
showing that neither version improves performance
might suggest that morphosyntactic information is
the dominant determinant of pronoun interpretation,
and/or that any semantic information utilized is not
obtained primarily from superficial cues. The results
are reported in the next section.
6 Results
Our final MaxEnt system used 40 features, which
were categorized into five classes in Section 4. To
get a sense for the relative contributions of each fea-
ture type, we ran evaluations with all 25 (32) possible
combinations of these five groups. We first report re-
sults on the held-out training data, and then provide
the blind test results. Table 1 provides the results
on the held-out sections of the training data dur-
ing 3-fold jackknifing for a sample of five of these 32
combinations. The four rightmost columns represent
the results from: (i) MaxEnt with no frequency fea-
tures (MaxEnt), (ii) MaxEnt with frequency features
5Experiments with unnormalized counts were also run
on jackknifed data with similar results.
included during training (MaxEnt-Features), (iii)
MaxEnt with Dagan et al postprocessing (MaxEnt-
Postprocessing), and (iv) Naive Bayes without fre-
quencies. Experiments with n-fold jackknifing for
other values of n produced similar results. Dagan
et al postprocessing was not attempted with Naive
Bayes since the postprocessor makes crucial use of
the probabilities the model assigns to competing an-
tecedents, and as previously mentioned, the actual
probabilities assigned by Naive Bayes are not neces-
sarily reliable due to the independence assumptions
it makes.
The testing phase breaks ties with respect to the
order imposed by Hobbs?s algorithm. In the case in
which no features were used during ?training? (see
the first row of Table 1, columns 2 and 5), the models
will produce the same probability for each possible
antecedent. Thus, these experiments reduce to using
the Hobbs algorithm, which, performing at 68.23%
accuracy,6 provides a nontrivial baseline. As can be
seen, adding groups of additional features incremen-
tally improves performance, up to a final result of
76.16% for MaxEnt using all morphosyntactic fea-
tures, and a comparable 76.24% for Naive Bayes.
In the end, the predicate-argument statistics pro-
vided little if any value, used either as features dur-
ing MaxEnt training or for Dagan et al postpro-
cessing. In the best-performing MaxEnt system con-
figuration (see bottom row), the statistics improve
performance by less than 0.5%. Interestingly, perfor-
mance was hurt when only statistical features were
used (65.71% in MaxEnt-Features and 66.25% in
MaxEnt-Postprocessing) as compared to none at all
(68.23%). Whereas the Hobbs algorithm ranks all of
the potential antecedents when no features are used,
it only breaks ties in the MaxEnt-Features system
that remain after statistical features order the po-
tential antecedents, and the MaxEnt-Postprocessor
system uses statistics to rerank the Hobbs ordering
between potential antecedents after the fact. This
reranking proved detrimental in both cases.
Table 2 provides the final results of blind test eval-
uation for the same five combinations of feature sets.
The final result of the system without predicate-
argument statistics was 75.72%, which is presumably
reasonable performance considering that the system
does not rely on fully-parsed input and lacks access
6All results are reported here in terms of accuracy,
that is, the number of pronouns correctly resolved divided
by the total number of pronouns read in. Correctness is
defined with respect to anaphor-antecedent relationships:
a chosen antecedent is correct if the ACE keys place the
pronoun and antecedent in the same coreference class.
Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayes
none .6823 .6571 .6625 .6823
num, gend .6870 .6863 .6841 .6859
num, gend, dist .7274 .7386 .7461 .7313
num, gend, dist, pos .7425 .7465 .7505 .7436
num, gend, dist, pos, lform .7616 .7663 .7656 .7624
Table 1: Results from jackknifing on training data
to world knowledge.7 In this case, the integrated fea-
ture system performed identically, whereas the post-
processor system displayed a performance improve-
ment of about 1% (a difference of 8 pronouns).
The MaxEnt results on the test data suffered only
a minimal (and in a few cases, no) loss from those
on the held-out data. Overtraining appears to have
been kept to a minimum; the generality of the fea-
tures was perhaps responsible for this.8 The results
from Naive Bayes generalized less well, exhibiting a
2% decrement on the test evaluation. The Hobbs al-
gorithm, which is not trained, exhibited similar per-
formance on both sets of data.
7 Error Analysis
There are a variety of possible reasons why the
predicate-argument statistics failed to markedly im-
prove performance in each of the system configura-
tions. While it could be that such statistics are sim-
ply not good predictors for pronoun interpretation,
data sparsity in the collected predicate-argument
statistics could also be to blame.
We carried out an error analysis to gain further in-
sight into this question. To address the data-sparsity
issue, we employed the technique used in Keller and
Lapata (2003, K&L) to get a more robust approx-
imation of predicate-argument counts.9 We wrote
7These performance results include 64 ?impossible?
cases in which, due to misparsing, no correct antecedents
were provided to the model; hence 91.6% accuracy is the
best that could be achieved. The results likewise include
errors in which the model selected a bogus antecedent
that resulted from a misparse.
8As such, informal post-hoc experiments with Gaus-
sian smoothing (Chen and Rosenfeld, 2000) failed to im-
prove performance.
9K&L use this technique to obtain frequencies for
predicate-argument bigrams that were unseen in a given
corpus, showing that the massive size of the web out-
weighs the noisy and unbalanced nature of searches per-
formed on it to produce statistics that correlate well with
corpus data. We are admittedly extending this reason-
ing to relations between the heads of predicates and ar-
guments without establishing that K&L?s technique so
generalizes, but we nonetheless feel that it is sufficient
for the purpose of an exploratory error analysis. The re-
a script to collect the number of pages that the
AltaVista search engine found for each predicate-
argument combination and its variants per the fol-
lowing schema, modeled directly after K&L:
Subject-Verb: Search for occurrences of the com-
binations N V where N is the singular or plural
form of the subject head noun and V is the in-
finitive, singular or plural present, past, perfect,
or gerund of the head verb.
Verb-Object: Search for occurrences of the com-
binations V Det N , where V and N are as
above for the verb and object head noun respec-
tively, and Det is the determiner the, a(n), or
the empty string.
Possessive-Noun: Search for occurrences of the
combinations Poss N , where Poss is the sin-
gular or plural form of the possessive and N is
the singular or plural form of the noun.
As in K&L, all searches were done as exact matches.
The results for all of the different form combinations
totaled together comprised the unnormalized counts.
We also computed normalized counts, in which the
unnormalized count was divided by the total num-
ber of pages AltaVista returned for the head of the
candidate antecedent, so that, as before, the counts
would not unduly bias antecedents with head words
that occurred frequently in isolation.
We created a list of those examples for which the
MaxEnt model ? trained with all 5 groups of the
morphosyntactic features activated, but not any sta-
tistical ones ? made incorrect predictions during 3-
fold jackknifing on the training data. (We used held-
out data so that our test data would remain blind.)
We then pared the list down to a reasonable size
for manual analysis in a variety of ways. First, of
course, only those examples that fall into one of the
three predicate-argument configurations with which
we are concerned were included (most were). Second,
we filtered out the cases in which either the most
proximal correct antecedent (with proximity defined
sults we received from this technique held few surprises.
Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayes
none .6877 .6496 .6627 .6877
num, gend .6667 .6745 .6719 .6654
num, gend, dist .7336 .7415 .7428 .7297
num, gend, dist, pos .7441 .7507 .7520 .7441
num, gend, dist, pos, lform .7572 .7572 .7677 .7415
Table 2: Results of final blind test evaluation
with respect to the Hobbs algorithm?s search order)
or the antecedent chosen by the model was a proper
name. Because all proper names of each ACE type
were classed together in our experiments, statistics
would not make different predictions for two such
names. While statistics could differentiate between
a potential proper name antecedent and one headed
by a common noun (and presumably did in our ex-
periments), we could not use K&L?s method on those
cases unless we used the actual proper name instead
of the category in the search query ? this would likely
create an undue bias to the other antecedent; con-
sider comparing counts for lawyer argued with those
for Snodgrass argued. Third, we filtered out cases in
which either the chosen antecedent or most proximal
correct antecedent was itself a pronominal, for the
reasons given in Section 5. Lastly, we eliminated a
small set of cases in which the chosen antecedent was
headless, as no predicate-argument statistics could
be collected for such a case.
These filters pared down the errors to a corpus of
45 examples; in all cases the chosen antecedent and
most proximal correct antecedent were each headed
by common nouns. Upon manual inspection, a fur-
ther subset of the cases were found to be caused by
factors irrelevant to the question at hand: 9 cases in
which the antecedent chosen should have been ruled
out as impossible (e.g., the collocation these days as
the antecedent of they), 5 cases in which either the
annotated keys were incorrect or our mapping system
failed to assign credit for a correct answer where it
was due, and 11 cases in which our shallow parser
misparsed either the chosen antecedent or the cor-
rect antecedent. This left a corpus of 20 cases to
examine using the K&L methodology.
The preferences embodied by the statistics col-
lected split these cases down the middle: in 10 cases
the correct antecedent had a higher normalized prob-
ability than the chosen one, and in the other 10 cases
the opposite was true.10 To get a sense for the data,
we consider two examples, the first being a case in
10The unnormalized counts disagreed with the normal-
ized ones in only one case; the unnormalized one favored
the correct antecedent for that example.
which predicate-argument statistics were definitive:
(3) After the endowment was publicly excoriated for
having the temerity to award some of its money
to art that addressed changing views of gender
and race, many institutions lost the will to show
any art that was rambunctious or edgy.
The MaxEnt model selected the temerity as the an-
tecedent of its (salience value: 0.30), preferring it
to the correct antecedent the endowment (salience
value: 0.10). However, AltaVista found no occur-
rences of temerity?s money or its variants on the web,
and thus the unnormalized and normalized counts
were 0. On the other hand, endowment?s money and
its variants had unnormalized and normalized statis-
tics of 1583 and 1.47? 10?3 respectively.
Example (4), on the other hand, is a case in which
the statistics merely strengthened the bias to the
wrong antecedent:
(4) The dancers were joined by about 70 supporters
as they marched around a fountain not far from
the mayor?s office, chanting: ?Giuliani scared
of sex! Who?s he going to censor next??
The model preferred the supporters as the antecedent
of they (salience value: 0.54) over the correct an-
tecedent the dancers (salience value: 0.45). Statis-
tics support the same conclusion, with unnormalized
and normalized counts of 2283 and 1.18? 10?3 for
supporters marched and its variants, and of 334 and
1.72? 10?4 for dancers marched and its variants.
The analysis of this sample therefore suggests that
predicate-argument statistics are unlikely to be of
much help when used in a model trained with a state-
of-the-art set of morphosyntactic features, even if
robust counts were available. While the statistical
preferences for our data sample were split down the
middle, it is important to understand that the cases
for which statistics hurt are potentially more damn-
ing than those for which they helped. In the cases in
which statistics reinforced a wrong answer, no (rea-
sonable) manipulation of statistical features or filters
can rescue the prediction. On the other hand, for
the cases in which statistics could help, their suc-
cessful use will depend on the existence of a formula
that can capture these cases without changing the
predictions for examples that the model currently
classifies correctly. Although our informal analysis
admittedly has certain limits ? the web counts we
collected are only approximations of true counts, and
the size of our manually-inspected corpus ended up
being fairly small ? our experience leads us to believe
that predicate-argument statistics are a poor substi-
tute for world knowledge, and more to the point, they
do not offer much predictive power to a state-of-the-
art morphosyntactically-driven pronoun interpreta-
tion system. Indeed, crisp ?textbook? examples such
as (3) appear to be empirically rare; the help pro-
vided by statistics for several of the examples seemed
to be more due to fortuity than the capturing of an
actual world knowledge relationship. Consider (5):
(5) Chung, as part of a plea bargain deal with
the department, has claimed that then-DNC fi-
nance director Richard Sullivan personally asked
him for a $125,000 donation in April 1995, the
sources said. Sullivan took the money despite
having previously voiced suspicions that Chung
was acting as a conduit for illegal contributions
from Chinese business executives, they added.
Our system selected Chinese business executives as
the antecedent of they (salience value: 0.34), over the
correct the sources (salience value: 0.10). Predicate-
argument statistics support sources (normalized and
unnormalized values of 29662 and 5.78? 10?4) over
executives (2391 and 1.50? 10?4), and thus this ex-
ample was classified as one of the 10 for which statis-
tics helped. In actuality, however, the correct an-
tecedent is determined by unrelated factors, demon-
strated by the fact that if the head nouns executives
and sources were switched in (5), the preferred an-
tecedent would be the executives, contrary to what
predicate-argument statistics would predict.
8 Conclusion
In conclusion, our experimental results and error
analysis suggest that predicate-argument statistics
offer little predictive power to a pronoun interpre-
tation system trained on a state-of-the-art set of
morphosyntactic features. On the one hand, it ap-
pears that the distribution of pronouns in discourse
allows for a system to correctly resolve a majority
of them using only morphosyntactic cues. On the
other hand, predicate-argument statistics appear to
provide a poor substitute for the world knowledge
that may be necessary to correctly interpret the re-
maining cases.
Acknowledgments
This work was supported by the ACE program
(www.nist.gov/speech/tests/ACE/).
References
Adam Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Stanley F. Chen and Ronard Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Ido Dagan and Alon Itai. 1990. Automatic acquisi-
tion of constraints for the resolution of anaphora
references and syntactic ambiguities. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics (COLING-90), pages 330?
332.
Ido Dagan, John Justenson, Shalom Lappin, Her-
bert Leass, and Amnon Ribak. 1995. Syntax and
lexical statistics in anaphora resolution. Applied
Artificial Intelligence, 9(6):633?644, Nov/Dec.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very Large
Corpora, Montreal, Quebec.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Frank Keller and Mirella Lapata. 2003. Using
the web to obtain frequencies for unseen bigrams.
Computational Linguistics, 29(3).
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora res-
olution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics (COLING-96).
Shalom Lappin and Herbert Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics (ACL-
93), pages 183?190.
Competitive Self-Trained Pronoun Interpretation
Andrew Kehler?
UC San Diego
akehler@ucsd.edu
Douglas Appelt
SRI International
appelt@ai.sri.com
Lara Taylor?
UC San Diego
lmtaylor@ucsd.edu
Aleksandr Simma?
UC San Diego
asimma@ucsd.edu
Abstract
We describe a system for pronoun interpre-
tation that is self-trained from raw data,
that is, using no annotated training data.
The result outperforms a Hobbsian baseline
algorithm and is only marginally inferior to
an essentially identical, state-of-the-art su-
pervised model trained from a substantial
manually-annotated coreference corpus.
1 Introduction
The last several years have seen a number of feature-
based systems for pronoun interpretation in which
the feature weights are determined via manual exper-
imentation or supervised learning (see Mitkov (2002)
for a useful survey). Reliable estimation of the
weights in both paradigms requires a substantial
manually-annotated corpus of examples. In this
short paper we describe a system for (third-person)
pronoun interpretation that is self-trained from raw
data, that is, using no annotated training data what-
soever. The result outperforms a Hobbsian baseline
algorithm and is only marginally inferior (2.3%) to
an essentially identical, state-of-the-art supervised
model trained from a manually-annotated corefer-
ence corpus. This result leaves open the possibil-
ity that systems self-trained on very large datasets
with more finely-grained features could eventually
outperform supervised models that rely on manually-
annotated datasets.
The remainder of the paper is organized as fol-
lows. We first briefly describe the supervised system
(described in more detail in Kehler et al (2004)) to
which we will compare the self-trained system. Both
systems use the same learning algorithm and feature
set; they differ with respect to whether the data they
?Department of Linguistics.
?Department of Computer Science and Engineering.
are trained on is annotated by a human or the algo-
rithm itself. We then describe our Hobbsian baseline
algorithm, and present the results of all three sys-
tems.
2 The Supervised Algorithm
The supervised model was trained using the im-
proved iterative scaling algorithm for Maximum
Entropy (MaxEnt) models described by Berger et
al. (1996) with binary-valued features. As is stan-
dard, the model was trained as a binary coreference
classifier: for each possible antecedent of each pro-
noun, a training instance was created that consisted
of the pronoun, the possible antecedent phrase, and
a binary coreference outcome. (Such a model can
be seen as providing a probabilistic measure of an-
tecedent salience.) Because we are ultimately inter-
ested in identifying the correct antecedent among a
set of possible ones, during testing the antecedent
assigned the highest probability is chosen.
The algorithm receives as input the results of SRI?s
Textpro system, a shallow parser that recognizes
low-level constituents (noun groups, verb groups,
etc.). No difficult syntactic attachments are at-
tempted, and the results are errorful. There was no
human-annotated linguistic information in the input.
The training corpus consists of 2773 annotated
third-person pronouns from the newspaper and
newswire segments of the Automatic Content Ex-
traction (ACE) program training corpus. The an-
notated blind corpus used for evaluation consists of
762 annotated third-person pronouns from the ACE
February 2002 evaluation set. The annotated pro-
nouns in both sets include only those that are ACE
?markables?, i.e., ones that refer to entities of the fol-
lowing types: Persons, Organizations, GeoPo-
liticalEntities (politically defined geographical
regions, their governments, or their people), Loca-
tions, and Facilities.
The system employs a set of hard constraints
and soft features. The hard constraints filter out
those noun groups that fail conservative number and
gender agreement checks before training, whereas the
soft features are used by the MaxEnt algorithm. A
set of forty soft features were developed and opti-
mized manually; they fall into five categories that
have become fairly standard in the literature:
Gender Agreement: Includes features to test a
strict match of gender (e.g., a masculine pro-
noun and a masculine antecedent), as well as
mere compatibility (e.g., a masculine pronoun
with an antecedent of unknown gender). These
features are more liberal than the gender-based
hard constraint mentioned above.
Number Agreement: Includes features to test a
strict match of number (e.g., a singular pronoun
and a singular antecedent), as well as mere com-
patibility (e.g., a singular pronoun with an an-
tecedent of unknown number). These features
are likewise more liberal than the number-based
hard constraint mentioned above.
Distance: Includes features pertaining to the dis-
tance between the pronoun and the potential an-
tecedent. Examples include the number of sen-
tences between them and the ?Hobbs distance?,
that is, the number of noun groups that have
to be skipped before the potential antecedent is
found per the search order used by the Hobbs
algorithm (Hobbs, 1978; Ge et al, 1998).
Grammatical Role: Includes features pertaining
to the syntactic position of the potential an-
tecedent. Examples include whether the poten-
tial antecedent appears to be the subject or ob-
ject of a verb, and whether the potential an-
tecedent is embedded in a prepositional phrase.
Linguistic Form: Includes features pertaining to
the referential form of the potential antecedent,
e.g., whether it is a proper name, definite de-
scription, indefinite NP, or a pronoun.
The values of these features ? computed from
TextPro?s errorful shallow constituent parses ?
comprised the input to the learning algorithm, along
with the outcome as indicated by the annotated key.
3 The Self-Trained Algorithm
The self-trained algorithm likewise uses MaxEnt,
with the same feature set and shallow parser. The
two systems differ in the training data utilized.
Instead of the training corpus of 2773 annotated
pronouns used in the supervised experiments, the
self-trained algorithm creates training data from
pronouns found in a raw corpus, particularly the
newswire segment of the Topic Detection and Track-
ing (TDT-2) corpus. The system was evaluated on
the same annotated set of 762 pronouns as the su-
pervised system; the performance statistics reported
herein are from the only time an evaluation with this
data was carried out.
The self-trained system embeds the MaxEnt algo-
rithm in an iterative loop during which the training
examples are acquired. The first phase of the algo-
rithm builds an initial model as follows:
1. For each third-person pronoun:
(a) Collect possible antecedents, that is, all of
the noun groups found in the previous two
sentences and to the left of the pronoun in
the current sentence.
(b) Filter them by applying the hard con-
straints.
(c) If only one possible antecedent remains,
create a pronoun-antecedent pair and label
the coreference outcome as True.
(d) Otherwise, with some probability (0.2
in our experiments1), create a pronoun-
antecedent pair for each possible antecedent
and label the coreference outcome as False.
2. Train a MaxEnt classifier on this training data.
The simplification assumed above ? that corefer-
ence holds for all and only those pronouns for which
TextPro and hard constraints find a single possi-
ble antecedent ? is obviously false, but it nonetheless
yields a model to seed the iterative part of the algo-
rithm, which goes as follows:
3. For each pronoun in the training data acquired
in step 1:
(a) Apply the current MaxEnt model to each
pronoun-antecedent pair.
(b) Label the pair to which the model assigns
the highest probability the coreference out-
come of True. Label all other pairs (if any)
for that pronoun the outcome of False.
4. Retrain the MaxEnt model with this new train-
ing data.
5. Repeat steps 3 and 4 until the training data
reaches a steady state, that is, there are no
pronouns for which the current model changes
its preference to a different potential antecedent
than it favored during the previous iteration.
1This choice will be explained in Section 5.
The hope is that improved predictions about which
potential antecedents of ambiguous pronouns are cor-
rect will yield iteratively better models (note that the
?unambiguous? pronoun-antecedent pairs collected
in step 1c will be considered to be correct through-
out). This hope is notwithstanding the fact that the
algorithm is based on a simplifying assumption ? that
each pronoun is associated with exactly one correct
antecedent ? that is clearly false for a variety of rea-
sons: (i) there will be cases in which there is more
than one coreferential antecedent in the search win-
dow, all but one of which will get labeled as not coref-
erential during any given iteration, (ii) there will be
cases in which the (perhaps only) correct antecedent
was misparsed or incorrectly weeded out by hard con-
straints, and thus not seen by the learning algorithm
(presumably some of the ?unambiguous? cases iden-
tified in step 1c will be incorrect because of this),
and (iii) some of the pronouns found will not even be
referential, e.g. pleonastic pronouns. The empirical
question remains, however, of how good of a system
can be trained under such an assumption. After all,
the model probabilities need not necessarily be accu-
rate in an absolute sense, but only in a relative one:
that is, good enough so that the antecedent assigned
the highest probability tends to be correct.
4 Hobbs Baseline
For comparison purposes, we also implemented a ver-
sion of Hobbs?s (1978) well-known pronoun interpre-
tation algorithm, in which no machine learning is
involved. This algorithm takes the syntactic repre-
sentations of the sentences up to and including the
current sentence as input, and performs a search for
an antecedent noun phrase on these trees. Since
TextPro does not build full syntactic trees for the
input, we developed a version that does a simple
search through the list of noun groups recognized.
In accordance with Hobbs?s search procedure, noun
groups are searched in the following order: (i) in the
current sentence from right-to-left, starting with the
first noun group to the left of the pronoun, (ii) in the
previous sentence from left-to-right, (iii) in two sen-
tences prior from left-to-right, (iv) in the current sen-
tence from left-to-right, starting with the first noun
group to the right of the pronoun (for cataphora).
The first noun group encountered that agrees with
the pronoun with respect to number, gender, and
person is chosen as the antecedent.
5 Results
Reporting on the results of a self-trained system
means only evaluating the system against annotated
data once, since any system reconfiguration and re-
evaluation based on the feedback received would con-
stitute a form of indirectly supervised training. Thus
we had to select a configuration as representing our
?reportable? system before doing any evaluation. To
allow for the closest comparison with our supervised
system, we opted to train the system with the same
number of pronouns that we had in our supervised
training set (2773), and sought to have approxi-
mately the same ratio of positive to negative training
instances, which meant randomly including one-fifth
of the pronouns in the raw data that had more than
one possible antecedent (see step 1d). Later we re-
port on post-hoc experiments to assess the effect of
training data size on performance.
The self-trained system was trained fourteen
times, once using each of fourteen different segments
of the TDT-2 data that we had arbitrarily appor-
tioned at the inception of the project. The scores
reported below and in Table 1 for the self-trained
system are averages of the fourteen corresponding
evaluations. The final results are as follows:
? Hobbs Baseline: 68.8%
? Self-Trained: 73.4%
? Supervised: 75.7%
The self-trained system beats the competitive Hobbs
baseline system by 4.6% and comes within 2.3% of
the supervised system trained on the same number
of manually-annotated pronouns.2
Convergence for the self-trained system was fairly
rapid, taking between 8 and 14 iterations. The num-
ber of changes in the current model?s predictions
started off relatively high in early iterations (aver-
aging approximately 305 pronouns or 11% of the
dataset) and then steadily declined (usually, but not
always, monotonically) until convergence. Post-hoc
2All results are reported here in terms of accuracy,
that is, the number of pronouns correctly resolved divided
by the total number of pronouns read in from the key. An
antecedent is considered correct if the ACE keys place the
pronoun and antecedent in the same coreference class.
In the case of 64 of the 762 pronouns in the evaluation
set, none of the antecedents input to the learning algo-
rithms were coreferential. Thus, 91.6% accuracy is the
best that these algorithms could have achieved.
In Kehler et al (2004) we describe two ways in which
our supervised system was augmented to use predicate-
argument frequencies, one which used them in a post-
processor and another which modeled them with features
alongside our morphosyntactic ones. In our self-trained
system, the first of these methods improved performance
to 75.1% (compared to 76.8% for the supervised system)
and the second to 74.1% (compared to 75.7% for the su-
pervised system).
Number of Pronouns Blind Test Performance
55 71.4%
138 72.3%
277 72.5%
554 72.6%
1386 73.5%
2773 73.4%
5546 73.5%
Full Segment 73.7%
Table 1: Effect of Training Data Size on Blind Test
Performance
analysis showed that the iterative phase contributed
a gradual (although again not completely monotonic)
improvement in performance during the course of
learning.
We then performed a set of post-hoc experiments
to measure the effect of training data size on perfor-
mance for the self-trained system. The results are
given in Table 1, which show a gradual increase in
performance as the number of pronouns grows. The
final row includes the results when all of the ?un-
ambiguous? pronouns in each TDT segment are uti-
lized (again, along with approximately one-fifth of
the ambiguous pronouns), which amounted to be-
tween 7,212 and 11,245 total pronouns.3 (Note that
since most pronouns have more than one possible
antecedent, the number of pronoun-antecedent train-
ing examples fed to MaxEnt is considerably higher
than the numbers of pronouns shown in the table.)
Perhaps one of the more striking facts is how well
the algorithm performs with relatively few pronouns,
which suggests that the generality of the features
used allow for fairly reliable estimation without much
data.
6 Conclusion
To conclude, a pronoun interpretation system can
be trained solely on raw data using a standard set
of morphosyntactic features to achieve performance
that approaches that of a state-of-the-art supervised
system. Although the self-acquired training data is
no doubt highly noisy, the resulting model is still
accurate enough to perform well at selecting correct
antecedents. As a next step, we will take a closer
look at the training data acquired to try to ascertain
3TDT segment 14, which is smaller than the others,
provided only about 3800 pronouns in the runs corre-
sponding to the last two rows of Table 1. The overall
average performance figures are the same to the first dec-
imal place whether or not the results from this segment
are included.
the underlying reasons for this success.
There are also a number of variants of the algo-
rithm that could be pursued. For instance, whereas
our algorithm uses the current model?s probabilities
in a winner-take-all strategy for positive example se-
lection, these probabilities could instead be used to
dictate the likelihood that examples are assigned a
positive outcome, or they could be thresholded in
various ways to create a more discerning positive out-
come assignment mechanism. Such strategies would
avoid the current simplification of assigning a posi-
tive outcome to exactly one potential antecedent for
each pronoun.
The relative generality of our feature set was ap-
propriate given the size of the data sets used. The
availability of very large raw corpora, however, cre-
ates the prospect of using self-training with consider-
ably more fine-grained features than is possible in a
supervised scenario, due to the relative infrequency
with which they would be found in any corpus of a
size that could be feasibly annotated manually. It
is thus at least conceivable that a self-trained ap-
proach, coupled with a large set of features and a
large corpus of raw data, could eventually overtake
the performance of the best supervised models.
Acknowledgments
This work was supported by the ACE program
(www.nist.gov/speech/tests/ACE/).
References
Adam Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very Large
Corpora, Montreal, Quebec.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of HLT/NAACL-04,
Boston, MA.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 914?923,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Predicting the Presence of Discourse Connectives
Gary Patterson and Andrew Kehler
Department of Linguistics
UC San Diego
9500 Gilman Drive #0108
La Jolla, CA 92093
{gpatterson,akehler}@ucsd.edu
Abstract
We present a classification model that predicts
the presence or omission of a lexical connec-
tive between two clauses, based upon linguis-
tic features of the clauses and the type of dis-
course relation holding between them. The
model is trained on a set of high frequency
relations extracted from the Penn Discourse
Treebank and achieves an accuracy of 86.6%.
Analysis of the results reveals that the most in-
formative features relate to the discourse de-
pendencies between sequences of coherence
relations in the text. We also present results
of an experiment that provides insight into the
nature and difficulty of the task.
1 Introduction
A central goal of natural language generation and
summarization systems is to produce interpretable,
coherent text that rivals material a human would pro-
duce. Doing so requires that systems not only have
the ability to generate clauses that are grammatical
and easy for people to process, but also the ability
to employ the appropriate discourse structuring de-
vices needed to yield fluid transitions between these
clauses. This is a tricky issue in that it requires that
a balance be achieved between the opposing goals
of communicative expressiveness and economy. On
the one hand, insufficient cueing of inter-clausal re-
lationships can lead to a discourse that is at best dif-
ficult to process, and at worst misunderstood. On the
other hand, too much explicit marking can result in
a clunky and even redundant sounding discourse.
Here we consider the question of when to ex-
plicitly mark the COHERENCE RELATIONS in dis-
course, that is, the inter-clausal relationships that
the language producer intends the interpreter to in-
fer between the meanings of clauses (Hobbs, 1979;
Mann and Thompson, 1988; Kehler, 2002; Asher
and Lascarides, 2003). Consider, for example, the
EXPLANATION coherence relation that holds in (1),
in which the second clause provides a cause or rea-
son for the eventuality described in the first:
(1) a. Max will visit Australia this summer be-
cause his father is turning 65.
b. Max will visit Australia this summer. His
father is turning 65.
As example (1) shows, coherence relations can
be marked explicitly?by using lexical connectives
such as coordinating or subordinating conjunctions
(e.g., because in 1a) or certain types of prepositional
or adverbial phrases?or left implicit as in (1b). Ei-
ther way, establishing the relation itself requires the
reader to go through a complex inferential process
necessitating that a variety of assumptions be made,
typically supported by context and/or world knowl-
edge, that are not explicitly asserted by the actual
linguistic material. In (1), for instance, such infer-
ences would include that Max intends to see his fa-
ther when he travels to Australia, that his father re-
sides in that country, and that the birthday will tak-
ing place during the time of the visit. Importantly,
the role of the connective in (1a) is therefore not to
establish that an EXPLANATION relation holds. In-
stead, connectives serve the function of directing the
addressee?s inference processes toward a smaller set
of coherence relations than might otherwise be avail-
able, among other possible roles.
The fact that both (1a) and (1b) are felicitous may
lead us to believe that the choice to insert a connec-
tive between clauses is simply optional. This is not
914
always the case, however. Sometimes the use of a
connective is required, since omitting it would likely
result in incorrect inferences being drawn by the ad-
dressee. For example, the use of when in (2a) im-
plies a backward temporal ordering of events, which
is reversed if the connective is left out, as in (2b).
(2) a. Maggie fell over in shock when Saul of-
fered to help her.
b. Maggie fell over in shock. Saul offered to
help her.
On the other hand, a connective can seem unnec-
essary if the relation between the two clauses is suf-
ficiently implied by other cues in the text. For in-
stance, since the act of throwing a vase against a
concrete wall would normally be expected to cause
the vase to break, the adverbial phrase as a result
in (3a), while felicitous, seems overly verbose and
perhaps even redundant.
(3) a. Susan threw the fragile vase against the
concrete wall. As a result, it broke.
b. Susan threw the fragile vase against the
concrete wall. It broke.
The foregoing examples suggest that the appropri-
ateness of including an explicit connective is inher-
ently gradient, and is in fact correlated with ease of
inference: the more difficult recovering the correct
relation would be without a connective, the more
necessary it is to include one. This characterization
in turn suggests that predicting whether a connec-
tive should be included might be a difficult problem
for an NLP system to address, since current-day sys-
tems lack the requisite world knowledge and capac-
ity for inference that would be necessary to evaluate
the ease with which coherence relations can be es-
tablished on arbitrary examples. However, it is also
possible that the decision to include a connective de-
pends in part on stylistic and other types of factors as
well, such that there might be predictive information
in the kinds of shallow linguistic and textual features
that systems do have access to. This is the question
taken up in this work: Given two adjacent clauses
in a text, the type of coherence relation holding be-
tween them and a candidate connective that could be
used to signal the relation, we ask how well a sys-
tem can predict whether or not that connective was
used by the author of the text. This capability would
be useful to generation systems as a post-cursor to
discourse-level message planning and sentence real-
ization processes, as well as summarization systems
that take existing sentences and have to reconsider
connective placement upon reassembling them.
To our knowledge, there is no work in the lit-
erature that addresses this issue directly. There is
a growing body of research (Sporleder and Las-
carides, 2008; Pitler et al, 2009; Lin et al, 2009;
Zhou et al, 2010) that focuses on building super-
vised models for classifying implicit relations using
a variety of contextual features, such as the polar-
ity of clauses, the semantic class and tense/aspect of
verbs, and information from syntactic parses. With
respect to explicit relations, Elhadad and McKe-
own (1990) sketch a procedure to select an appro-
priate connective to link two propositions as part of
a larger text generation system, using linguistic fea-
tures derived from the sentences. The procedure se-
lects the best connective from a given set of candi-
dates, but does not allow for the option of leaving
the relation implicit. More recently, Asr and Dem-
berg (2012a) look at both explicit and implicit re-
lations, and make the observation that certain rela-
tion types are more likely to be realized explicitly
than others. Relatedly, Asr and Demberg (2012b)
discuss which connectives are the strongest predic-
tors of which relation types. However, there is no
work of which we are aware that specifically pre-
dicts whether connectives should be used or omitted.
2 Classification Model
Our model is a binary classifier trained on data ex-
tracted from the Penn Discourse Treebank (PDTB;
Prasad et al (2008)), a large-scale corpus of an-
notated discourse coherence relations covering the
one-million-word Wall Street Journal corpus of the
Penn Treebank (Marcus et al, 1993).
2.1 Data
For every relation in the PDTB, the following com-
ponents are annotated: (i) the connective used to
signal the relation; (ii) the textual spans of the two
clausal arguments that constitute the relation; (iii)
the semantic sense of the relation, according to a hi-
erarchical tagset of senses; and (iv) the attribution of
the assertions and beliefs expressed in the text to the
915
relevant individuals. Crucially for our purposes, for
the implicit relations the corpus indicates the most
suitable connective if the relation were instead sig-
naled explicitly. For example, the annotators de-
cided that the best connective to signal the REASON
relation in (4) would be because, rather than other
plausible candidates, such as as or since.
(4) It?s a shame their meeting never took place.
[IMPLICIT=because] Mr. Katzenstein certainly
would have learned something. (WSJ0037)
In total, there are 18,459 explicit and 16,053 im-
plicit relations annotated in the PDTB. We excluded
a subset of these cases in training our model based
on two criteria. First, whereas explicit relations in
the PDTB can hold between spans of text that ei-
ther are or are not adjacent, we excluded the non-
adjacent cases. This was done to ensure consistency
in discourse structure between the relations consid-
ered in the model, since only the implicit relations
between adjacent clauses were annotated. Second,
we excluded relations that have lower frequency se-
mantic senses or use low frequency connectives. As
a result, the model considers only the eight most
common semantic senses of relations, which in to-
tal account for just less than 90% of the relations
in the corpus.1 Further, for each relation, we only
consider the connectives that account for more than
5% of the instances of that relation. After applying
these filters, the resulting corpus comprised 10,039
explicit and 11,690 implicit relations.
Table 1 shows the eight relations that were mod-
eled. The majority of these relations exist at the
middle layer of the three-level hierarchy of seman-
tic senses annotated in the PDTB.2 Relations at the
highest level ? representing the four major semantic
categories COMPARISON, CONTINGENCY, EXPAN-
SION and TEMPORAL ? were deemed too broad to
be of practical use in a generation system, whereas
the lowest-level senses were considered either un-
necessarily fine-grained or have too few tokens in
the corpus to allow for meaningful statistical model-
ing. Two exceptions were made for REASON and
RESULT relations, which do appear at the lowest
1The next most common relation type, CONDITION, was ex-
cluded because it is always marked explicitly in the corpus.
2For more details of the PDTB sense hierarchy, see Prasad
et al (2008).
level in the PDTB hierarchy (beneath the CAUSE cat-
egory). These were included because they are both
attested frequently in the corpus and are undeniably
contrastive: with REASON, the second clause pro-
vides an explanation for the proposition expressed
in the first clause, whereas with RESULT, the sec-
ond clause describes a consequence of the first. It
is reasonable to want these relations to be modeled
separately.
Sections 2-22 of the corpus were used as the train-
ing set, and sections 23 and 24 were used as the test
set. Sections 0 and 1 of the corpus were set aside
as a development set for feature design and parame-
ter optimization. The training set comprised 18,218
tokens, distributed as shown in Table 1.
Relation Type Explicit Implicit
Asynchronous 1,120 (70%) 469 (30%)
Conjunction 2,940 (61%) 1,906 (39%)
Contrast 2,044 (66%) 1,054 (34%)
Instantiation 203 (16%) 1,093 (84%)
Reason 771 (28%) 1,938 (72%)
Restatement 76 (4%) 2,081 (96%)
Result 354 (21%) 1,295 (79%)
Synchronous 748 (86%) 126 (14%)
Total 8,256 (45%) 9,962 (55%)
Table 1: Distribution of Training Set
As Table 1 shows, the preference for an overt
connective varies significantly according to the type
of relation. The ASYNCHRONOUS, CONJUNCTION,
CONTRAST and SYNCHRONOUS relations are real-
ized explicitly the majority of the time, whereas IN-
STANTIATION, REASON, RESTATEMENT and RE-
SULT relations are more often left implicit. We can
also see that some relation types (such as RESTATE-
MENT, INSTANTIATION and SYNCHRONOUS) ex-
hibit a strong preference to be realized in a particular
form, whereas other types show more variability in
whether they are realized explicitly or implicitly.
The distribution of tokens in Table 1 can be used
to determine a baseline accuracy against which the
performance of our model is evaluated. A naive
model that uses the semantic type of the coherence
relation as the sole predictive feature makes a bi-
nary classification based simply on the majority cat-
egory for that relation type. A baseline model using
this methodology results in classification accuracy
of 77.0% over the held-out test set.
916
2.2 Model
We built a composite model containing binary logis-
tic regression classifiers for each coherence relation,
trained on a set of linguistic features extracted from
each token in the training set. Logistic regression
was chosen because it produces a model with high
performance and results that are easily interpretable.
The features included in the model fall into the fol-
lowing three broad classes: relation-level, argument-
level, and discourse-level.
Relation-level features
In addition to the semantic type of the relation, we
include as a feature the connective used to signal the
relation in the text (or, for the implicit relations, the
connective indicated by the annotators as most ap-
propriate). This feature (Connect) is included based
upon the observation that connectives vary as to their
rates of being realized explicitly?even for connec-
tives that signal relations with the same semantic
sense. Consequently, given a relation of a partic-
ular semantic type, an indication of the best fitting
connective may be a consistent predictor of whether
or not this relation is realized explicitly.
We also include a feature reflecting the attribution
of the relation. As mentioned above, the PDTB is
annotated to describe the attribution of the proposi-
tions expressed within a relation to individuals or en-
tities in the text. For example, in the relation shown
in (5), the first clause contains a direct quotation,
clearly attributing the proposition expressed to the
individual Rep. Stark. However, the second clause
contains no such indication of attribution to an entity
in the text, and so the proposition is instead assumed
to be asserted by the writer of the article.
(5) ?No magic bullet will be discovered next year,
an election year,? says Rep. Stark. But 1991
could be a window for action. (WSJ0314)
Inspection of the corpus data suggests that when one
argument of a relation contains a proposition that is
attributable to an individual in the text (either by di-
rect or indirect quotation) but the other is assumed
by default to be attributed to the author, this rela-
tion is more likely to be realized explicitly. This
may well have an explanation based on sentence
processing: the intervening attribution phrase ?says
Rep. Stark? may serve as a distraction, with the re-
sult that the intended coherence relation is harder to
infer without a connective. Consequently, we in-
clude a factor (AttMismatch) indicating if the two
arguments are not attributed to the perspective of the
same individual.
Finally, in any particular genre there may be for-
mulaic prose whose systematic features can be ex-
ploited by a system tasked with generating text
within that same genre. In this case, the genre rep-
resented by the corpus data comprises copy-edited
articles from the Wall Street Journal, many of which
refer to company earnings reports or other financial
events, and are written in a highly prescribed style.
Accordingly, we may suspect that there is a greater
prevalence of implicit relations in these cases, since
the reader is assumed to be habituated to the way
in which the information in this type of article is
presented. Consequently, for the domain at hand
we include a binary feature (Financial) indicating
whether the relation pertains to financial informa-
tion. This feature takes the value 1 if the textual
spans of both arguments in the relation contain per-
centage amounts or dollar figures.
Argument-level features
For each relation, the model includes features cap-
turing the size or complexity of each of its two argu-
ments. The arguments were identified by the annota-
tors according to a principle of minimality, whereby
the annotations indicate the shortest text spans nec-
essary for the appropriate coherence relation to be
interpreted. However, the annotators also indicated
other text that is in some way relevant to the interpre-
tation of the arguments. This supplementary mate-
rial can include unrestricted relative clauses, apposi-
tives, or other parenthetical information. Our obser-
vation of the data indicates that relations which have
supplementary material annotated alongside one or
both of their arguments are more often than not re-
alized explicitly with connectives. As a result, we
include binary features (Supp1, Supp2) indicating
whether the first and second arguments of the rela-
tion include such supplementary information. We
also include features (Length1, Length2) reflecting a
simple measure of the length of each argument, cal-
culated as the log transformed count of the number
of words in the arguments? minimal text spans.
One measure of the complexity of an argument
917
is the number of clauses it contains. It might be
thought that the greater the syntactic complexity of
an argument, the more likely it is that the relation
containing it is marked explicitly, so as to give the
reader more help in drawing the correct intended
inference between the arguments. As a proxy for
the number of clauses in each argument, we include
features (NPSbj1, NPSbj2) equal to the total num-
ber of main, subordinate, or complement clause sub-
jects included within the textual spans of the re-
spective arguments, determined using the syntactic
parses available in the corpus data.
We also consider whether the underlying richness
of the informational content expressed by the ar-
gument may influence the presence or omission of
a connective. Considering the way in which read-
ers process text in real time, it would intuitively
be more difficult to infer the intended relation be-
tween two clauses without the aid of a connective if
the arguments themselves had greater processing de-
mands owing to increased lexical retrieval, reference
and anaphor resolution requirements, and so forth.
Given this intuition, we may expect that arguments
with higher density of information are correlated
with the increased use of connectives as a means
of facilitating the inference of the relation type and
thereby easing the overall processing burden. Con-
sequently, our model includes features (ContDen-
sity1, ContDensity2) calculated as the ratio of the
count of words in each argument that are content
words (i.e. ignoring articles, prepositions and pro-
nouns), divided by the total number of words, as
well as features (PronDensity1, PronDensity2) cal-
culated as the ratio of pronouns in each argument to
the number of noun phrases.
Finally, the accessibility of the subject of the sec-
ond argument in a relation may play a role in de-
termining whether the relation is explicitly marked.
Specifically, informal observation of the data sug-
gests that there is a tendency for the second ar-
gument of an implicit relation to begin with a
longer, contentful noun phrase, rather than a pro-
noun. Consequently, our model includes a binary
feature (FirstA2Pron) indicating whether the first
word in the second argument is a pronoun.
Discourse-level features
The final class of features takes account of the way
in which a relation fits into the broader discourse
structure in the text. In their work on implicit re-
lation classification, Pitler et al (2008) identified
various dependencies between bigram sequences of
explicitly- and implicitly-realized relations of differ-
ent semantic types. These results suggest that the
semantic type and the presence of a connective in
one relation may be predictive of whether or not
the following relation in the text is marked with a
connective. Consequently, we include features in-
dicating the semantic type of the relation occurring
immediately prior in the text (PrevSemType), and
whether this relation was marked implicitly or ex-
plicitly (PrevForm).
The other discourse-level features take account of
the dependencies between the relation in question
and its neighboring relations in the text. As part of
a supervised learning model developed to classify
the semantic class of implicit relations in the PDTB,
Lin et al (2009) found features based on the two
main types of discourse dependency pattern in the
corpus (?shared? and ?fully embedded? arguments) to
be highly predictive. We speculatively include simi-
lar features in our model to see if they are helpful in
predicting the presence of connectives.
The first type of dependency between adjacent re-
lations is one where the second argument of one re-
lation is also the first argument of the following re-
lation, as in Figure 1. Accordingly, we include two
binary features indicating whether an argument is
shared with the preceding relation (Arg1isPrevArg2)
or the following relation (Arg2isNextArg1) in the
corpus.
Figure 1: Shared argument
The other main type of discourse dependency, a
?fully embedded? dependency, is one where an entire
relation (including both of its arguments) is com-
pletely embedded within one argument of an adja-
cent relation in the text, as in Figure 2. To capture
918
this type of dependency structure, we include two
binary features (EmbedNext, EmbedPrev) indicating
whether the current relation is embedded within ei-
ther one of its adjacent relations. We also include
two binary features (Arg1Embed, Arg2Embed) to in-
dicate whether either argument of the current rela-
tion completely contains an embedded relation.
Figure 2: Fully embedded argument
The two relations in (6) exemplify a typical in-
stantiation of this embedded dependency structure.
(6) It is an overwhelming job. [IMPLICIT= be-
cause] There are so many possible proportions
when you consider how many things are made
out of eggs and butter and milk. (WSJ0261)
In this example, there is an implicit REASON relation
holding between the two complete sentences, and
an explicit SYNCHRONOUS relation signaled by the
connective when holding between the two clauses
of the second sentence. Since the REASON relation
fully embeds another relation within its second argu-
ment, the feature Arg2Embed for this relation takes
the value 1. For the SYNCHRONOUS relation, the
feature EmbedPrev takes the value 1 since the entire
relation is fully contained within the second argu-
ment of the preceding relation in the text.
3 Results and Evaluation
3.1 Classification accuracy
The model was evaluated by assessing the accuracy
of its predictions against the unseen test set. The
model achieved an overall accuracy of 86.6%, an im-
provement of 9.6% above baseline.3 Table 2 shows
the model accuracy for each relation type, together
with the baseline performance based on the majority
category for that type.
3During the preparation of the final version of this paper,
a model was trained with an SVM using the same set of fea-
tures, which resulted in a modest improvement in performance
(87.3%). The ensuing discussion of results, however, will con-
tinue to pertain to the regression model.
Relation Type Accuracy Baseline
Asynchronous 91.7% 79.7%
Conjunction 84.5% 78.2%
Contrast 81.1% 65.0%
Instantiation 83.3% 82.5%
Reason 88.2% 68.3%
Restatement 95.2% 95.2%
Result 84.4% 76.9%
Synchronous 96.5% 92.9%
Total 86.6% 77.0%
Table 2: Classification Accuracy by Relation Type
The model achieved an improvement in accuracy
across all relation types but one: RESTATEMENT re-
lations, for which the baseline accuracy was already
close to 100%. The greatest improvement in accu-
racy was seen for REASON relations, for which the
model accuracy was 19.9% above baseline. We now
discuss which of the factors in each of the feature
classes turned out to be the most predictive.
3.2 Significant predictors
We trained the model on subsets of the features to
investigate the predictive power of the different fea-
ture classes. The accuracy assessed against the test
set is shown in Table 3.
Feature Class # Features Accuracy
Relation Level 4 80.4%
Argument Level 11 77.2%
Discourse Level 8 80.9%
Rel + Arg Levels 15 82.8%
Rel + Disc Levels 12 85.1%
Arg + Disc Levels 19 82.4%
All Features 23 86.6%
Table 3: Classification Accuracy by Feature Class
The classes of Relation-level and Discourse-level
features each separately yielded significantly better
performance over baseline (one-sided tests of pro-
portion, z=2.50 and z=2.92, respectively; p<0.01
for both), whereas the Argument-level features alone
performed only marginally better than baseline.
However, all three classes of features are needed to
attain the highest model performance.
Across all relation types, we found that the fea-
tures relating to the discourse dependencies between
a relation and its neighbors were the strongest and
919
most consistent predictors of whether that relation
is explicit or implicit. A relation that is fully em-
bedded within a single argument of an adjacent re-
lation in the text (indicated by the features Em-
bedPrev and EmbedNext) has a much higher like-
lihood of being signaled explicitly. Conversely, a
relation that fully contains another relation within
one of its arguments (indicated by Arg1Embed and
Arg2Embed) has a significantly higher likelihood of
being implicit. The result is consistent with the em-
bedded discourse dependency shown in (6), in which
the implicit REASON relation fully contains an ex-
plicit SYNCHRONOUS relation within its second ar-
gument.
The model also found that the features which in-
dicate whether a relation has shared arguments with
either the preceding or following relations in the text
(Arg1isPrevArg2, Arg2isNextArg1) are both predic-
tors of an implicit outcome. In other words, if a
clause in the text serves as the argument for two ad-
jacent relations, then both of these relations are more
likely to be realized implicitly.
The next most predictive feature was the connec-
tive used to signal the relation (Connect). This fea-
ture was a significant predictor for every relation
type. Eliminating this feature from the final model
reduces the overall accuracy by 2.5%. The other
features in the model were less significantly predic-
tive, and generally worked in the expected direction.
Longer arguments (Length1, Length2) and the in-
dication of a financial genre (Financial) were gen-
erally associated with predicted implicit outcomes,
whereas the presence of supplementary material
(Supp1, Supp2), a mismatch of attribution (AttMis-
match), more ?content rich? arguments (ContDen-
sity1, ContDensity2), and a pronoun appearing as
the first word of the second argument (FirstA2Pron)
all tended to increase the odds in favor of a predicted
explicit outcome.
The features indexing syntactic complexity
(NPSbj1 and NPSbj2) were found to be marginally
predictive of an explicit outcome for most relation
types, but the overall effect in the model was rel-
atively small?resulting in only a 0.2% improve-
ment?meaning that the level of performance re-
ported on this task depends very little on the model
having access to full syntactic parses. Somewhat un-
expectedly, the factors indicating the semantic type
of the previous relation in the text (PrevSemType)
and whether or not this relation was explicitly sig-
naled by a connective (PrevForm) were found not to
be significant predictors. Our analysis of the training
data confirmed the findings of Pitler et al (2008) in
that certain bigrams of coherence relation types are
significantly more prevalent than others. However,
the differences in the frequencies were evidently not
sufficiently correlated with the explicit/implicit dis-
tinction as to make the type or form of the previous
relation a significant feature in the model.
3.3 Error analysis
We analyzed a sample of cases incorrectly predicted
by the model to see if there were any consistent
traits. We focus our attention here on the CONTRAST
relations, which is the type with the lowest model
accuracy. The majority of these errors were cases
where the model predicted that the relation would be
explicit?the most likely outcome for a CONTRAST
relation?whereas in the corpus the intended rela-
tion was signaled by linguistic cues other than an
overt connective. For instance, the strong syntactic
parallelism of the two arguments in (7), and the op-
posite polarity of the lexical items delight and detri-
ment, combine to induce a contrastive relationship
without the need for a connective.
(7) To the delight of some doctors, the bill dropped
a plan passed by the Finance Committee.
[IMPLICIT=but] To the detriment of many low-
income people, efforts to boost Medicaid fund-
ing were also stricken. (WSJ2372)
Other ways that the contrast relation is signaled
implicitly include contrasting temporal modifiers (It
wasn?t so long ago X. Now, Y), repetition of the
predicate in the argument (. . . it could only happen
once. . . . it?s happening again), or even by the use
of punctuation such as a semicolon. Previous work
(Sporleder and Lascarides, 2008; Lin et al, 2009)
has sought to make use of such cues to identify and
classify implicit relations in the text. The results
of this brief error analysis suggest that such indi-
rect cues could also be useful factors in determining
whether to choose to use a connective for a given
relation type when generating text.
920
4 Judgment Study
The system described in the last section outper-
formed a baseline majority-category classifier on the
task of deciding whether a relation should be made
explicit or left implicit. This result might be con-
sidered surprising, for two reasons that we have
previously discussed. First, the system was able
to make this improvement using relatively shallow
features extracted from the text, without access to
the richer types of contextual information and world
knowledge required for establishing coherence rela-
tions during actual discourse comprehension. Sec-
ond, the data suggest that the appropriateness of in-
cluding a connective is not as cut-and-dried as a bi-
nary classification task may suggest, but is instead
gradient, with many cases for which the inclusion
of a connective appears to be optional. Obviously,
the PDTB does not avail us of the opportunity to
evaluate this gradience directly (or even use a 3-
way required/optional/redundant distinction), since
the producer of the actual text samples in the corpus
had to ultimately decide whether or not to use a con-
nective. The apparent optionality of many examples
thus puts limits on how well we can expect a system
to perform, since there is no way to reliably predict
cases in which the decision is made arbitrarily.
This observation leads us to ask how well humans
perform on this same task. Do they make highly
accurate predictions, or does optionality limit their
performance? In order to shed light on this question,
we carried out an experiment to see how consistently
humans choose to use lexical connectives to signal
intended coherence relations between clauses.
4.1 Methodology
We selected a balanced sample of 100 clause-pair
tokens from the test set, reflecting the distribution of
the different major relation types (six relations were
represented in the sample). This sample comprised
44 explicit and 56 implicit tokens, consistent with
the distribution in the overall corpus. The experi-
mental stimulus for each item consisted of two ver-
sions of the same clause pair, one including a con-
nective between the clauses, and the other without.
For relations that were realized explicitly in the cor-
pus, as in (8a), the alternative implicit stimulus omit-
ted the connective and showed the second argument
as a separate sentence, as in (8b).
(8) a. Mr. Nesbit also said the FDA has asked
Bolar Pharmaceutical Co. to recall at the
retail level its urinary tract antibiotic, but
so far the company hasn?t complied with
that request.
b. Mr. Nesbit also said the FDA has asked
Bolar Pharmaceutical Co. to recall at the
retail level its urinary tract antibiotic. So
far the company hasn?t complied with that
request.
For the implicit relations, the alternative explicit
stimulus for the experiment used the connective an-
notated in the PDTB as the one being most appro-
priate. For each item, a short passage was created
including the preceding and following sentences in
the text to serve as context. The relative ordering
of the presentation of the explicit and implicit forms
was randomized, without regard to the actual corpus
outcome for that stimulus.
Using Amazon?s Mechanical Turk, judges were
presented with the two passages for each item. They
were told to assume that the passages had the same
intended meaning, and were asked to judge which
of the two sounded more natural. We collected 30
responses for each item.4
4.2 Results
We classified each experimental item as either ex-
plicit or implicit, based on the majority response of
the judges. Using this classification, the judges? re-
sponses matched the actual outcomes in 68 of the
100 cases.5 The distribution of correctly-judged
items across relation types is shown in Table 4.
The judgments for REASON relations most closely
matched the corpus outcomes, with 9 out of the 12
explicit tokens and all 6 implicit tokens in the cor-
4The data from a small number of judges were discarded
due to an unreasonably fast response time or because their judg-
ments showed a unanimous preference across every experimen-
tal item. This left a total of 2,925 judgments over the 100 ex-
perimental items, from 113 different judges.
5Using the majority response of judges for each item to
measure classification accuracy is consistent with the statisti-
cal model, whereby probabilities are rounded up or down to ar-
rive at a binary classification. If accuracy is instead calculated
in terms of average correctness over the individual responses,
performance drops to 60.4%.
921
pus correctly identified by the judges. The lowest
scoring relation type was CONTRAST, for which 9
of the 10 explicit tokens were judged correctly but
only 4 out of the 11 implicit tokens were correctly
identified.
Relation Type Items Correct Accuracy
Conjunction 22 15 68.2%
Contrast 21 13 61.9%
Instantiation 9 6 66.7%
Reason 18 15 83.3%
Restatement 16 9 62.5%
Result 14 10 64.2%
Total 100 68 68.0%
Table 4: Results of Mechanical Turk study
There were hence 32 experimental items for
which the majority response by the judges did not
match the actual corpus outcome. In two-thirds
(21) of these cases, the judges indicated a prefer-
ence for a connective when the relation in the corpus
was implicit. These mismatches occurred across the
range of relation types. This suggests that the judges
tended to err on the side on inserting a connective,
even when it may not have been strictly necessary.
While the reason for this is not clear, one possibility
is that the texts reflected the genre and the highly-
prescribed editing guidelines for the newspaper arti-
cles that comprise the corpus, under which unneces-
sary or redundant words are excised. Without such
pressures to edit the copy down to a minimal form,
the judges may have preferred to see the relations
signaled explicitly in cases in which either decision
would result in a felicitous passage.
In the remaining 11 cases, for which the relations
in the corpus were explicitly signaled with a connec-
tive, the judges on average indicated a preference to
leave the relation implicit. Interestingly, all of these
cases were either CONJUNCTION or CONTRAST re-
lations, semantic types which are usually signaled
explicitly with a connective. We inspected these
cases to ascertain why judges may have preferred
an outcome opposite to that actually seen in the text.
We found that all 7 of the CONTRAST mismatches
were instances where the second argument of the re-
lation in the corpus was a sentence beginning with
the coordinating conjunction but, as in (9).
Similarly, three of the mismatched CONJUNCTION
(9) At those levels stocks are set up to be ham-
mered by index arbitragers. But nobody knows
at what level the futures and stocks will open
today. (WSJ2300)
relations had a sentential second argument begin-
ning with the conjunction and. The responses of
the judges to these cases may simply reflect a dis-
preference for sentence-initial conjunctions, a prac-
tice which is frowned upon in prescriptive grammar
books, but apparently allowed by the Wall Street
Journal style sheet.
For this sample of 100 relations, the model
achieves a classification accuracy of 84%. This may
seem at first blush to be an odd result, since it ap-
pears that the model is surpassing human perfor-
mance. As we have suggested, however, this could
be the result of our experimental judges having dif-
ferent preferences than the writers and editors at the
Wall Street Journal for cases in which connective
placement is truly optional. We therefore sought to
evaluate the effect of optionality on these results.
If inaccurate predictions are associated with op-
tionality of connective use, we might expect that
both human judges and the classification model
would be less certain about their categorizations of
these examples than for the cases that were cor-
rectly classified. This was indeed the case. First,
there was a significant difference in the variability
of judges? responses between items that were incor-
rectly classified and those that were correct (66% vs.
73%, respectively; two-sample t test: t=2.60, df=73,
p<0.02). Thus, as a group the judges were less sure
of themselves in those cases in which they incor-
rectly decided to use or omit the connective, sug-
gesting that either option may have been acceptable.
Second, we analyzed the levels of confidence our
model had for its judgments on correctly and incor-
rectly categorized cases, measured in terms of the
probability of the predicted outcome assigned by
the model. The analysis revealed that the average
model confidence for the relations that were incor-
rectly classified was significantly lower than the av-
erage model confidence for the correctly-classified
items (71% vs. 88%, respectively; t=5.65, df=25,
p<0.001). Taken together, these results are consis-
tent with the idea that, at least for a significant por-
tion of the data, the incorrect judgments made by
922
both the judges and the model may have occurred
on passages for which either including or omitting
the connective would have been acceptable.
5 Conclusion
We have presented a model that predicts whether the
coherence relation holding between two clauses is
marked explicitly with a lexical connective or left
implicit. Whereas there is reason to think that an
author?s decision to use a connective is in part in-
fluenced by properties of the extra-linguistic con-
text that are inaccessible to NLP systems (such as
semantics and world knowledge), we find that rel-
atively simple linguistic features derivable from the
clauses and from local discourse dependencies can
be exploited to reach a level of performance signifi-
cantly greater than that achieved by a baseline. The
variability in the judgments of native speakers when
presented with these data suggests that the use of a
connective is in many cases simply optional; in such
cases the decision may reflect lower-level stylistic
choices on the part of the author. This in turn in-
dicates that there may be an inherent upper bound
to the performance of computational systems on this
task.
Acknowledgments
We thank Roger Levy for useful discussions about
this work and three anonymous reviewers for their
helpful feedback.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
conversation. Cambridge University Press.
Fatemeh Torabi Asr and Vera Demberg. 2012a. Implicit-
ness of discourse relations. In Proceedings of the 24th
International Conference on Computational Linguis-
tics, pages 2669?2684.
Fatemeh Torabi Asr and Vera Demberg. 2012b. Mea-
suring the strength of linguistic cues for discourse re-
lations. In Proceedings of the Workshop on Advances
in Discourse Analysis and its Computational Aspects
(ADACA), pages 33?42.
Michael Elhadad and Kathleen R McKeown. 1990. Gen-
erating connectives. In Proceedings of the 13th Con-
ference on Computational Linguistics-Volume 3, pages
97?101.
Jerry R Hobbs. 1979. Coherence and coreference. Cog-
nitive science, 3(1):67?90.
Andrew Kehler. 2002. Coherence, reference, and the
theory of grammar. CSLI Publications, Stanford.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1, pages 343?351.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings of
the 22nd International Conference on Computational
Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
tions in text. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetorical
relations: An assessment. Natural Language Engi-
neering, 14(3):369?416.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recognition.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1507?
1514.
923
