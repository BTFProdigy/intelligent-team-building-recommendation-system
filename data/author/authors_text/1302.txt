Proceedings of SPEECHGRAM 2007, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Handling Out-of-Grammar Commands in Mobile Speech Interaction  
Using Backoff Filler Models 
Tim Paek1, Sudeep Gandhe2, David Maxwell Chickering1, Yun Cheng Ju1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA 
2
 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292, USA 
{timpaek|dmax|yuncj}@microsoft.com, gandhe@usc.edu 
 
 
Abstract 
In command and control (C&C) speech in-
teraction, users interact by speaking com-
mands or asking questions typically speci-
fied in a context-free grammar (CFG). Un-
fortunately, users often produce out-of-
grammar (OOG) commands, which can re-
sult in misunderstanding or non-
understanding.  We explore a simple ap-
proach to handling OOG commands that 
involves generating a backoff grammar 
from any CFG using filler models, and util-
izing that grammar for recognition when-
ever the CFG fails.  Working within the 
memory footprint requirements of a mobile 
C&C product, applying the approach 
yielded a 35% relative reduction in seman-
tic error rate for OOG commands.  It also 
improved partial recognitions for enabling 
clarification dialogue. 
1 Introduction 
In command and control (C&C) speech interaction, 
users interact with a system by speaking com-
mands or asking questions.  By defining a rigid 
syntax of possible phrases, C&C reduces the com-
plexity of having to recognize unconstrained natu-
ral language.  As such, it generally affords higher 
recognition accuracy, though at the cost of requir-
ing users to learn the syntax of the interaction 
(Rosenfeld et al, 2001).  To lessen the burden on 
users, C&C grammars are authored in an iterative 
fashion so as to broaden the coverage of likely ex-
pressions for commands, while remaining rela-
tively simple for faster performance.  Nevertheless, 
users can, and often still do, produce OOG com-
mands.  They may neglect to read the instructions, 
or forget the valid expressions.  They may mistak-
enly believe that recognition is more robust than it 
really is, or take too long to articulate the right 
words.  Whatever the reason, OOG commands can 
engender misunderstanding (i.e., recognition of the 
wrong command) or non-understanding (i.e., no 
recognition), and aggravate users who otherwise 
might not realize that their commands were OOG.  
In this paper, we explore a simple approach to 
handling OOG commands, designed specifically to 
meet the memory footprint requirements of a C&C 
product for mobile devices.  This paper is divided 
into three sections.  First, we provide background 
on the C&C product and discuss the different types 
of OOG commands that occur with personal mo-
bile devices. Second, we explain the details of the 
approach and how we applied it to the product do-
main. Finally, we evaluate the approach on data 
collected from real users, and discuss possible 
drawbacks. 
2 Mobile C&C 
With the introduction of voice dialing on mobile 
devices, C&C speech interaction hit the wider 
consumer market, albeit with rudimentary pattern 
recognition. Although C&C has been 
commonplace in telephony and accessibility for 
many years, only recently have mobile devices 
have the memory and processing capacity to 
support not only automatic speech recognition 
(ASR), but a whole range of multimedia 
functionalities that can be controlled with speech.  
Leveraging this newfound computational capacity 
is Voice Command, a C&C application for high-
end mobile devices that allows users to look up 
contacts, place phone calls, retrieve appointments, 
obtain device status information, control 
multimedia and launch applications.  It uses an 
embedded, speaker-independent recognizer and 
operates on 16 bit, 16 kHz, Mono audio. 
33
OOG commands pose a serious threat to the us-
ability of Voice Command. Many mobile users ex-
pect the product to ?just work? without having to 
read the manual.  So, if they should say ?Dial Bob?, 
when the proper syntax for making a phone call is 
Call {Name}, the utterance will likely be mis-
recognized or dropped as a false recognition.  If 
this happens enough, users may abandon the prod-
uct, concluding that it or ASR in general, does not 
work. 
2.1 OOG frequency 
Given that C&C speech interaction is typically 
geared towards a relatively small number of words 
per utterance, an important question is, how often 
do OOG commands really occur in C&C?  In Pro-
ject54 (Kun & Turner, 2005), a C&C application 
for retrieving police information in patrol cars, 
voice commands failed on average 15% of the time, 
roughly 63% of which were due to human error.  
Of that amount, roughly 54% were from extrane-
ous words not found in the grammar, 12% from 
segmentation errors, and the rest from speaking 
commands that were not active. 
To examine whether OOG commands might be 
as frequent on personal mobile devices, we col-
lected over 9700 commands of roughly 1 to 3 sec-
onds each from 204 real users of Voice Command, 
which were recorded as sound (.wav) files.  We 
also logged all device data such as contact entries 
and media items.  All sound files were transcribed 
by a paid professional transcription service.  We 
ignored all transcriptions that did not have an asso-
ciated command; the majority of such cases came 
from accidental pressing of the push-to-talk button.  
Furthermore, we focused on user-initiated com-
mands, during which time the active grammar had 
the highest perplexity, instead of yes-no responses 
and clarification dialogue.  This left 5061 tran-
scribed utterances. 
2.2 Emulation method 
With the data transcribed, we first needed a me-
thod to distinguish between In-Grammar (ING) 
and OOG utterances.  We developed a simulation 
environment built around a desktop version of the 
embedded recognizer which could load the same 
Voice Command grammars and update them with 
user device data, such as contact entries, for each 
sound file.  It is important to note the desktop ver-
sion was not the engine that is commercially 
shipped and optimized for particular devices, but 
rather one that serves testing and research purposes. 
The environment could not only recognize sound 
files, but also parse string input using the dynami-
cally updated grammars as if that were the recog-
nized result.  We utilized the latter to emulate rec-
ognition of all transcribed utterances for Voice 
Command.  If the parse succeeded, we labeled the 
utterance ING, otherwise it was labeled OOG. 
Overall, we found that slightly more than one 
out of every four (1361 or 26.9%) transcribed ut-
terances were OOG.  We provide a complete 
breakdown of OOG types, including extraneous 
words and segmentation errors similar to Project54, 
in the next section.  It is important to keep in mind 
that being OOG by emulation does not necessarily 
entail that the recognizer will fail on the actual 
sound file.  For example, if a user states ?Call Bob 
at mobile phone? when the word ?phone? is OOG, 
the recognizer will still perform well.  The OOG 
percentage for Voice Command may also reflect 
the high perplexity of the name-dialing task.  Users 
had anywhere from 5 to over 2000 contacts, each 
of which could be expressed in multiple ways (e.g., 
first name, first name + last name, prefix + last 
name, etc.).  In summary, our empirical analysis of 
the data suggests that OOG utterances for mobile 
C&C on personal devices can indeed occur on a 
frequent basis, and as such, are worth handling. 
2.3 OOG type 
In order to explore how we might handle different 
types of OOG commands, we classified them ac-
cording to functional anatomy and basic edit op-
erations.  With respect to the former, a C&C utter-
ance consists of three functional components: 
 
1. Slot: A dynamically adjustable list repre-
senting a semantic argument, such as {Con-
tact} or {Date}, where the value of the ar-
gument is typically one of the list members. 
2. Keyword: A word or phrase that uniquely 
identifies a semantic predicate, such as Call 
or Battery, where the predicate corresponds 
in a one-to-one mapping to a type of com-
mand.  
3. Carrier Text: A word or phrase that is de-
signed to facilitate naturalistic expression of 
commands and carries no attached semantic 
content, such as ?What is? or ?Tell me?. 
 
34
For example, in the command ?Call Bob at mo-
bile?, the word ?Call? is the keyword, ?Bob? and 
?mobile? are slots, and ?at? is a carrier word.  
If we were to convert an ING command to 
match an OOG command, we could perform a se-
ries of edit operations: substitution, deletion, and 
insertion.  For classifying OOG commands, substi-
tution implies the use of an unexpected word, dele-
tion implies the absence of an expected word, and 
insertion implies the addition of a superfluous 
word. 
Starting with both functional anatomy and edit 
operations for classification, Table 1 displays the 
different types of OOG commands we labeled 
along with their relative frequencies.  Because 
more than one label might apply to an utterance, 
we first looked to the slot for an OOG type label, 
then keyword, then everything else.  
The most frequent OOG type, at about 60%, was 
OOG Slot, which referred to slot values that did 
not exist in the grammar.  The majority of these 
cases came from two sources: 1) contact entries 
that users thought existed but did not ? sometimes 
they did exist, but not in any normalized form (e.g., 
?Rich? for ?Richard?), and 2) mislabeling of most-
ly foreign names by transcribers.  Although we 
tried to correct as many names as we could, given 
the large contact lists that many users had, this 
proved to be quite challenging.  
The second most frequent OOG type was Inser-
tion at about 14%.  The majority of these insertions 
were single words.  Note that similar to Project54, 
segmentation errors occurred quite often at about 
9%, when the different segmentation types are 
added together. 
3 Backoff Approach 
Having identified the different types of OOG 
commands, we needed to devise an approach for 
handling them that satisfied the requirements of 
Voice Command for supporting C&C on mobile 
devices.  
3.1 Mobile requirements 
For memory footprint, the Voice Command team 
specified that our approach should operate with 
less than 100KB of ROM and 1MB of RAM.  Fur-
thermore, the approach could not require changes 
OOGType % Total Description Examples 
Insertion 14.2% adding a non-keyword, non-slot word call britney porter on mobile phone [?phone? is 
superfluous] 
Deletion 3.1% deleting a non-keyword, non-slot 
word my next appointments [?what are? missing] 
Substitution 2.5% replacing a non-keyword, non-slot 
word 
where is my next appointment  
[?where? is not supported] 
Segmentation 8.2% incomplete utterance show, call, start 
Keyword 
Substitution 4.6% replacing a keyword 
call 8 8 2 8 0 8 0 [?dial? is keyword] ,  
dial john horton [?call? is keyword] 
Keyword 
Segmentation 0.1% incomplete keyword what are my appoint 
Keyword  
Deletion 2.2% deleting the keyword marsha porter at home [?call? missing] 
Slot  
Substitution 0.4% replacing slot words 
call executive 5 on desk  
[?desk? is not slot value] 
Slot  
Segmentation 0.9% incomplete slot call alexander woods on mob 
Slot Deletion 1.0% deleted slot call tracy morey at 
Disfluencies 1.8% disfluencies - mostly repetitions start expense start microsoft excel 
Order  
Rearrangement 0.6% 
changing the order of words within a 
keyword 
what meeting is next [Should be ?what is my 
next meeting?] 
Noise 0.7% non primary speaker oregon state home coming call brandon jones 
on mobile phone 
OOG Slot 59.8% The slot associated with this utterance is out of domain 
Show Rich Lowry [?Richard? is contact entry] , 
dial 0 2 1 6 [Needs > 7 digits] 
 
Table 1. Different OOG command types and their relative frequencies for the Voice Command product. The brack-
eted text in the ?Examples? column explicates the cause of the error 
35
to the existing embedded Speech API (SAPI).  
Because the team also wanted to extend the func-
tionality of Voice Command to new domains, we 
could not assume that we would have any data for 
training models.  Although statistical language 
models (SLM) offer greater robustness to varia-
tions in phrasing than fixed grammars (Rosenfeld, 
2000), the above requirements essentially prohib-
ited them.  So, we instead focused on extending the 
use of the base grammar, which for Voice Com-
mand was a context-free grammar (CFG): a formal 
specification of rules allowing for embedded recur-
sion that defines the set of possible phrases (Man-
ning & Sch?tze, 1999).  
Despite the manual effort that CFGs often re-
quire, they are widely prevalent in industry (Knight 
et al, 2001) for several reasons. First, they are easy 
for designers to understand and author.  Second, 
they are easy to modify; new phrases can be added 
and immediately recognized with little effort.  And 
third, they produce transparent semantics without 
requiring a separate natural language understand-
ing component; semantic properties can be at-
tached to CFG rules and assigned during recogni-
tion.  By focusing on CFGs, our approach allows 
industry designers who are more accustomed to 
fixed grammars to continue using their skill set, 
while hopefully improving the handling of utter-
ances that fall outside of their grammar. 
3.2 Leveraging a backoff grammar 
As long as utterances remain ING, a CFG affords 
fast and accurate recognition, especially because 
engines are often tuned to optimize C&C recogni-
tion.  For example, in comparing recognition per-
formance in a statistical and a CFG-based recog-
nizer for the same domain, Knight et al (2001) 
found that the CFG outperformed the SLM.  In 
order to exploit the optimization of the engine for 
C&C utterances that are ING, we decided to utilize 
a two-pass approach where each command is ini-
tially submitted to the base CFG.  If the confidence 
score of the top recognition C1 falls below a rejec-
tion threshold RCFG, or if the recognizer declares a 
false recognition (based on internal engine fea-
tures), then the audio stream is passed to a backoff 
grammar which then attempts to recognize the 
command.  If the backoff grammar fails to recog-
nize the command, or the top recognition falls 
again below a rejection threshold RBG, then users 
experience the same outcome as they normally 
would otherwise, except with a longer delay.  Fig-
ure 1(a) summarizes the approach. 
In order to generate the backoff grammar and 
still stay within the required memory bounds of 
Voice Command, we explored the use of the built-
in filler or garbage model, which is a context-
independent, acoustic phone loop.  Expressed in 
the syntax as ?...?, filler models capture phones in 
whatever context they are placed.  The functional 
anatomy of a C&C utterance, as explained in Sec-
tion 2.3, sheds light on where to place them: before 
and/or after keywords and/or slots.  As shown Fig-
ure 1(b), to construct a backoff grammar from a 
CFG during design time, we simply parse each 
CFG rule for keywords and slots, remove all car-
rier phrases, and insert filler models before and/or 
after the keywords and/or slots.  Although it is 
straightforward to automatically identify keywords 
(words that uniquely map to a CFG rule) and slots 
(lists with semantic properties), developers may 
want to edit the generated backoff grammar for any 
keywords and slots they wish to exclude; for ex-
ample, in cases where more than one keyword is 
found for a CFG rule. 
For both slots and keywords, we could employ 
any number of different patterns for placing the 
filler models, if any.  Table 2 displays some of the 
patterns in SAPI 5 format, which is an XML for-
mat where question marks indicate optional use.  
Although the Table is for keywords, the same pat-
terns apply for slots.  As shown in k4, even the 
functional constituent itself can be optional.  Fur-
thermore, alternate lists of patterns can be com-
posed, as in kn.  Depending on the number and type 
 
 
Figure 1. (a) A two-pass approach which leverages a 
base CFG for ING recognition and a backoff grammar 
for failed utterances. (b) Design time procedure for 
generating a backoff grammar 
36
of functional constituents for a CFG rule, backoff 
rules can be constructed by adjoining patterns for 
each constituent.  We address the situation when a 
backoff rule corresponds to multiple CFG rules in 
Section 3.4. 
3.3 Domain feasibility 
Because every C&C utterance can be characterized 
by its functional constituents, the backoff filler ap-
proach generically applies to C&C domains, re-
gardless of the actual keywords and slots.  But the 
question remains, is this generic approach feasible 
for handling the different OOG types for Voice 
Command discussed in Section 2.3? 
The filler model is clearly suited for Insertions, 
which are the second most frequent OOG type, 
because it would capture the additional phones.  
However, the most frequent OOG type, OOG Slot, 
cannot be handled by the backoff approach.  That 
requires the developer to write better code for 
proper name normalization (e.g, ?Rich? from ?Ri-
chard?) as well as breaking down the slot value 
into further components for better partial matching 
of names.  Because new C&C domains may not 
utilize name slots, we decided to treat improving 
name recognition as separate research.  Fortu-
nately, opportunity for applying the backoff filler 
approach to OOG Slot types still exists. 
3.4 Clarification of partial recognitions 
As researchers have observed, OOG words con-
tribute to increased word-error rates (Bazzi & 
Glass, 2000) and degrade the recognition perform-
ance of surrounding ING words (Gorrell, 2003).  
Hence, even if a keyword surrounding an OOG 
slot is recognized, its confidence score and the 
overall phrase confidence score will often be de-
graded.  This is in some ways an unfortunate by-
product of confidence annotation, which might be 
circumvented if SAPI exposed word lattice prob-
abilities.  Because SAPI does not, we can instead 
generate partial backoff rules that comprise only a 
subset of the functional constituents of a CFG rule.  
For example, if a CFG rule contains both a key-
word and slot, then we can generate a partial back-
off rule with just one or the other surrounded by 
filler models.  Using partial backoff rules prevents 
degradation of confidence scores for ING constitu-
ents and improves partial recognitions, as we show 
in Section 4.  Partial backoff rules not only handle 
OOG Slot commands where, for example, the 
name slot is not recognized, but also many types of 
segmentation, deletion and substitution commands 
as well. 
Following prior research (Gorrell et al, 2002; 
Hockey et al, 2003), we sought to improve partial 
recognitions so that the system could provide feed-
back to users on what was recognized, and to en-
courage them to stay within the C&C syntax.  Cla-
rification dialogue with implicit instruction of the 
syntax might proceed as follows: If a partial recog-
nition only corresponded to one CFG rule, then the 
system could assume the semantics of that rule and 
remind the user of the proper syntax.  On the other 
hand, if a partial recognition corresponded to more 
than one rule, then a disambiguation dialogue 
could relate the proper syntax for the choices.  For 
example, suppose a user says ?Telephone Bob?, 
using the OOG word ?Telephone?.  Although the 
original CFG would most likely misrecognize or 
even drop this command, our approach would ob-
tain a partial recognition with higher confidence 
score for the contact slot.  If only one CFG rule 
contained the slot, then the system could engage in 
the confirmation, ?Did you mean to say, call 
Bob?? On the other hand, if more than one CFG 
rule contained the slot, then the system could en-
gage in a disambiguation dialogue, such as ?I 
heard 'Bob'. You can either call or show Bob?.  
Either way, the user is exposed to and implicitly 
taught the proper C&C syntax. 
3.5 Related research 
In related research, several researchers have inves-
tigated using both a CFG and a domain-trained 
SLM simultaneously for recognition (Gorrell et al, 
2002; Hockey et al, 2003).  To finesse the per-
formance of a CFG, Gorrell (2003) advocated a 
two-pass approach where an SLM trained on CFG 
Scheme Keyword Pattern 
k1 <keyword/> 
k2 (?)?  <keyword> 
k3 (?)?  <keyword/>  (?)? 
k4 (?)?  <keyword/>? (?)? 
kn 
<list> 
(?)?  <keyword/>? (?)? 
(?) 
</list> 
 
Table 2. Possible patterns in SAPI 5 XML format for 
placing the filler model, which appears as 
?...?.Question marks indicate optional use. 
37
data (and slightly augmented) is utilized as a back-
off grammar.  However, only the performance of 
the SLM on a binary OOG classification task was 
evaluated and not the two-pass approach itself.  In 
designing a multimodal language acquisition sys-
tem, Dusan & Flanagan (2002) developed a two-
pass approach where they utilized a dictation n-
gram as a backoff grammar and added words rec-
ognized in the second pass into the base CFG.  Un-
fortunately, they only evaluated the general usabil-
ity of their architecture. 
Because of the requirements outlined in Section 
3.1, we have focused our efforts on generating a 
backoff grammar from the original CFG, taking 
advantage of functional anatomy and filler models.  
The approach is agnostic about what the actual fil-
ler model is, and as such, the built-in phone loop 
can easily be replaced by word-level (e.g., Yu et 
al., 2006) and sub-word level filler models (e.g., 
Liu et al, 2005).  In fact, we did explore the word-
level filler model, though so far we have not been 
able to meet the footprint requirements.  We are 
currently investigating phone-based filler models. 
Outside of recognition with a CFG, researchers 
have pursued methods that directly model OOG 
words as sub-word units in the recognition search 
space of a finite state transducer (FST) (Bazzi & 
Glass, 2000).  OOG words can also be dynamically 
incorporated into the FST (Chung et al, 2004).  
Because this line of research depends on entirely 
different engine architecture, we could not apply 
the techniques. 
4 Evaluation 
In C&C speech interaction, what matters most is 
not word-error rate, but semantic accuracy and task 
completion.  Because task completion is difficult to 
evaluate without collecting new data, we evaluated 
the semantic accuracy of the two-pass approach 
against the baseline of using just the CFG on the 
data we collected from real users, as discussed in 
Section 2.1.  Furthermore, because partial 
recognitions can ultimately result in a successful 
dialogue, we carried out separate evaluations for 
the functional constituents of a command (i.e., 
keyword and slot) as well as the complete 
command (keyword + slot).  For Voice Command, 
no command contained more than one slot, and 
because the vast majority of single slot commands 
were commands to either call or show a contact 
entry, we focused on those two commands as a 
proof of concept. 
For any utterance, the recognizer can either ac-
cept or reject it.  If it is accepted, then the seman-
tics of the utterance can either be correct (i.e., it 
matches what the user intended) or incorrect.  The 
following metrics can now be defined: 
 
precision = CA / (CA + IA)   (1) 
recall = CA / (CA + R)    (2) 
accuracy = CA / (CA + IA + R)   (3) 
 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejected 
commands.  Although R could be decomposed into 
correct and incorrect rejections, for C&C, 
recognition failure is essentially perceived the 
same way by users: that is, as a non-understanding. 
4.1 Results 
For every C&C command in Voice Command, the 
embedded recognizer returns either a false 
recognition (based on internal engine parameters) 
or a recognition event with a confidence score.  As 
described in Section 3.2, if the confidence score 
falls below a rejection threshold RCFG, then the 
audio stream is processed by the backoff grammar 
which also enforces its own threshold RBG.  The 
RCFG for Voice Command was set to 45% by a 
proprietary tuning procedure for optimizing 
acoustic word-error rate.  For utterances that 
exceeded RCFG, 84.2% of them were ING and 
15.8% OOG.  For utterances below RCFG, 48.5% 
 
 
Figure 2. The semantic accuracies comparing the 
baseline CFG against both the BG (backoff grammar 
alone) and the two-pass approach (CFG + Backoff) 
separated into functional constituent groups and fur-
ther separated by ING and OOG commands. 
38
were ING and 51.5% OOG.  Because a 
considerable number of utterances may be ING in 
the second pass, as it was in our case, RBG requires 
tuning as well.  Instead of using a development 
dataset to tune RBG, we decided to evaluate our 
approach on the entire data with RBG set to the 
same proprietary threshold as RCFG.  In post-hoc 
analyses, this policy of setting the two thresholds 
equal and reverting to the CFG recognition if the 
backoff confidence score falls below RBG achieved 
results comparable to optimizing the thresholds. 
Figure 2 displays semantic accuracies separated 
by ING and OOG commands.  Keyword evalua-
tions comprised 3700 ING and 1361 OOG com-
mands.  Slot and keyword + slot evaluations com-
prised 2111 ING and 138 OOG commands.  Over-
all, the two-pass approach was significantly higher 
in semantic accuracy than the baseline CFG, using 
McNemar's test (p<0.001).  Not surprisingly, the 
largest gains were with OOG commands.  Notice 
that for partial recognitions (i.e., keyword or slot 
only), the approach was able to improve accuracy, 
which with further clarification dialogue, could 
result in task completions.  Interestingly, the ap-
proach performed the same for keyword + slot as it 
did for slot, which suggests that getting the slot 
correct is crucial to recognizing surrounding key-
words.  Despite the high percentage of OOG Slots, 
slot accuracy still increased due to better handling 
of other OOG types such as deletions, insertions 
and substitutions.   
Finally, as a comparison, for the keyword + slot 
task, an upper bound of 74.3% ? 1.1% (10-fold 
cross-validated standard error) overall semantic 
accuracy was achieved using a small footprint sta-
tistical language modeling technique that re-ranked 
CFG results (Paek & Chickering, 2007), though 
the comparison is not completely fair given that the 
technique was focused on predictive language 
modeling and not on explicitly handling OOG ut-
terances.  Also note that in all cases, the backoff 
grammar alone performed worse than either the 
CFG or the two-pass approach.   
Table 3 provides a more detailed view of the re-
sults for the just OOG commands as well as the 
relative reductions in semantic error rate (RER).  
Notice that the approach increases recall, which 
signifies less non-understandings. However, this 
comes at the price of a small increase in misunder-
standings, as seen in the decrease in precision.  
Overall, the best reduction in semantic error rate 
achieved by the approach was about 35%. 
Decomposing RER by OOG types, we found 
that for keyword evaluations, the biggest im-
provement (52% RER), came about for Deletion 
types, or commands with missing carrier words.  
This makes sense because the backoff grammar 
only cares about the keyword.  For slot and key-
word + slot evaluations, Insertion types maintained 
the biggest improvement at 38% RER. 
Note that the results presented are those ob-
tained without tuning.  If application developers 
wanted to find an optimal operating point, they 
would need to decide what is more important for 
their application: precision or recall, and adjust the 
thresholds until they reach acceptable levels of per-
formance.  Ideally, these levels should accord with 
what real users of the application would accept. 
4.2 Efficiency 
Given that the approach was aimed at satisfying 
the mobile requirements stated in Section 3.1, 
which it did, we also compared the processing time 
it takes to arrive at a recognition or false 
recognition between the CFG alone and the two-
pass approach.  Because of the filler models, the 
backoff grammar is a more relaxed version of CFG 
with a larger search space, and as such, takes 
slightly more processing time. The average 
processing time for the CFG in our simulation 
environment was about 395 milliseconds, whereas 
the average processing time for the two passes was 
about 986 milliseconds.  Hence, when the backoff 
grammar is used, the total computation time is 
approximately 2.5 times that of a single pass alone.  
In our experiments, a total of 1570 commands (i.e. 
31%) required the two passes, while 3491 of them 
were accepted after a single CFG pass. 
 CFG 2-PASS RER 
Prec 85.0% 79.0% -39.7% 
Recall 36.8% 58.6% 34.5% Keyword 
Acc 34.5% 50.7% 24.7% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Slot 
Acc 54.4% 70.3% 34.9% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Keyword 
+ Slot 
Acc 54.4% 70.3% 34.9% 
 
Table 3. Relative reductions in semantic error rate, or 
Relative Error Reduction (RER) for OOG commands 
grouped by keyword, slot and keyword + slot evalua-
tions. ?2-PASS? denotes the two-pass approach. 
39
4.3 Drawbacks 
In exploring the backoff filler approach, we 
encountered a few drawbacks that are worth 
considering when applying this approach to other 
domains.  The first issue dealt with false positives.  
In the data collection for Voice Command, a total 
of 288 utterances contained no discernable speech.  
If these were included in the data set, they would 
amount to about 5% of all utterances.  As 
mentioned previously, these were mostly cases 
when the push-to-talk button was accidentally 
triggered.  When we evaluated the approach on 
these utterances, we found that the CFG accepted 
36 or roughly 13% of them, while the proposed 
approach accepted 115 or roughly 40% of them.  
For our domain, this problem can be avoided by 
instructing users to lock their devices when not in 
use to prevent spurious initiations.  For other C&C 
domains where unintentional command initiations 
occur frequently, this may be a serious concern, 
though we suspect that users will be more 
forgiving of accidental errors than real errors. 
Another drawback dealt with generating the 
backoff grammar.  As we discussed in Section 3.2, 
various patterns for placing filler models can be 
utilized.  Although we did explore the possibility 
that perhaps certain patterns might generalize 
across domains, we found that it was better to 
hand-craft patterns to the application.  For Voice 
Command, we used the kn pattern specified in Ta-
ble 2 for keywords, and the identical sn pattern for 
slots because they proved to be best suited to the 
product grammars in pre-trial experiments. 
5 Conclusion & Future Direction 
In this paper, we classified the different types of 
OOG commands that might occur in a mobile 
C&C application, and presented a simple two-pass 
approach for handling them that leverages the base 
CFG for ING recognition and a backoff grammar 
OOG recognition.  The backoff grammar is gener-
ated from the original CFG by surrounding key-
words and/or slots with filler models.  Operating 
within the memory footprint requirements of a 
mobile C&C product, the approach yielded a 35% 
relative reduction in semantic error rate for OOG 
commands, and improved partial recognitions, 
which can facilitate clarification dialogue. 
We are now exploring small footprint, phone-
based filler models.  Another avenue for future 
research is to further investigate optimal policies 
for deciding when to pass to the backoff grammar 
and when to use the backoff grammar recognition. 
References 
I. Bazzi & J. Glass. 2000. Modeling out-of-vocabulary 
words for robust speech recognition. In Proc. ICSLP. 
G. Chung, S. Seneff, C.Wang, & I. Hetherington. 2004. 
A dynamic vocabulary spoken dialogue interface. In 
Proc. ICSLP. 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based 
upon multimodal language acquisition. In Proc. IC-
MI. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intel-
ligent help to mixed initiative spoken dialogue sys-
tems. In Proc. ICSLP. 
G. Gorrell. 2003. Using statistical language modeling to 
identify new vocabulary in a grammar-based speech 
recognition system. In Proc. Eurospeech. 
B. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist, J. 
Hieronymus, A. Gruenstein, & J. Dowding. 2003. 
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance. In 
Proc. EACL, pp. 147?154. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, & I. Lewin. 2001. Comparing grammar-based 
and robust approaches to speech understanding: A 
case study. In Proc. Eurospeech. 
A. Kun & L. Turner. 2005. Evaluating the project54 
speech user interface. In Proc. Pervasive. 
P. Liu, Y. Tian, J. Zhou, & F. Soong. 2005. Background 
model based posterior probability for measuring 
confidence. In Proc. Interspeech. 
C.D. Manning & H. Sch?utze. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge,Massachusetts. 
Paek, T. & Chickering, D. 2007. Improving command 
and control speech recognition: Using predictive us-
er models for language modeling. UMUAI, 17(1):93-
117. 
Rosenfeld, R. 2000. Two decades of statistical language 
modeling: Where do we go from here? In Proc. of the 
IEEE, 88(8): 1270?1278. 
R. Rosenfeld, D. Olsen, & A. Rudnicky. 2001. Univer-
sal speech interfaces. Interactions, 8(6):34?44. 
D. Yu, Y.C. Ju, Y. Wang, & A. Acero. 2006. N-gram 
based filler model for robust grammar authoring. In 
Proc. ICASSP. 
40
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64?67,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Rapidly Deploying Grammar-Based Speech Applications 
with Active Learning and Back-off Grammars 
Tim Paek1, Sudeep Gandhe2, David Maxwel Chickering1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 
2 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292 
{timpaek|dmax}@microsoft.com, gandhe@usc.edu 
                                                          
2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opi-
nions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred. 
Abstract 
Grammar-based approaches to spoken lan-
guage understanding are utilized to a great ex-
tent in industry, particularly when developers 
are confronted with data sparsity. In order to 
ensure wide grammar coverage, developers 
typically modify their grammars in an itera-
tive process of deploying the application, col-
lecting and transcribing user utterances, and 
adjusting the grammar. In this paper, we ex-
plore enhancing this iterative process by leve-
raging active learning with back-off 
grammars. Because the back-off grammars 
expand coverage of user utterances, develop-
ers have a safety net for deploying applica-
tions earlier. Furthermore, the statistics related 
to the back-off can be used for active learning, 
thus reducing the effort and cost of data tran-
scription. In experiments conducted on a 
commercially deployed application, the ap-
proach achieved levels of semantic accuracy 
comparable to transcribing all failed utter-
ances with 87% less transcriptions. 
1 Introduction 
Although research in spoken language understand-
ing is typically pursued from a statistical perspec-
tive, grammar-based approaches are utilized to a 
great extent in industry (Knight et al, 2001). 
Speech recognition grammars are often manually 
authored and iteratively modified as follows: Typi-
cally, context-free grammars (CFG) are written in 
a format such as Speech Recognition Grammar 
Specification (SRGS) (W3C, 2004) and deployed. 
Once user utterances are collected and transcribed, 
the grammars are then adjusted to improve their 
coverage. This process continues until minimal 
OOG utterances are observed. In this paper, we 
explore enhancing this iterative process of gram-
mar modification by combining back-off gram-
mars, which expand coverage of user utterances, 
with active learning, which reduces ?the number of 
training examples to be labeled by automatically 
processing unlabeled examples, and then selecting 
the most informative ones with respect to a speci-
fied cost function for a human to label? (Hakkani-
Tur et al, 2002). This paper comprises three sec-
tions. In Section 2, we describe our overall ap-
proach to rapid application development (RAD). In 
Section 3, we explain how data transcription can 
be reduced by leveraging active learning based on 
statistics related to the usage of back-off gram-
mars. Finally, in Section 4, we evaluate the active 
learning approach with simulation experiments 
conducted on data collected from a commercial 
grammar-based speech application. 
2 RAD Approach & Related Work 
Working under the assumption that developers in 
industry will continue to use CFGs for rapid appli-
cation development, our approach to grammar 
modification is as follows: 
1. Create a CFG (either manually or automatically). 
1.1 Generate a back-off grammar from the CFG. 
2. Deploy the application. 
2.1 Use the back-off grammar for OOG utterances. 
3. Gather data from users. 
4. Selectively transcribe data by using statistics re-
lated to the back-off for active learning; i.e., transcribe 
only those utterances that satisfy the active learning 
criterion. 
5. Modify CFG either manually or automatically and 
go to step 1.1. 
To begin with, developers start with a CFG in Step 
1. If they had access to a grammatical platform 
64
such as Regulus (Rayner et al, 2006), they could 
in principle construct a CFG automatically for any 
new domain, though most developers will probably 
manually author the grammar. Two steps are added 
to the typical iterative process. In Step 1.1, we 
generate a back-off grammar from the CFG. One 
way to accomplish this is by constructing a back-
off CFG using filler models (Paek et al, 2007), 
which when applied to the same command-and-
control task in Section 4 can result in a 35% rela-
tive reduction in semantic error rate for OOG ut-
terances. However, the back-off grammar could 
also be a SLM trained on artificial data created 
from the CFG (Galescu et al, 1998). Whatever 
back-off mechanism is employed, its coverage 
should be wider than the original CFG so that ut-
terances that fail to be recognized by the CFG, or 
fall below an acceptable confidence threshold, can 
be handled by the back-off in a second or simulta-
neous pass. That is the gist of Step 2.1, the second 
additional step. It is not only important to generate 
a back-off grammar, but it must be utilized for 
handling possible OOG utterances. 
Our approach attempts to reduce the usual cost 
associated with grammar modification after the 
application has been deployed and data collected in 
Step 4. The idea is simple: Exploit the fast and ac-
curate CFG recognition of in-grammar (ING) ut-
terances by making OOG utterances handled by 
the back-off grammar ING. In other words, expand 
CFG coverage to include whatever gets handled by 
the back-off grammar. This idea is very comple-
mentary with a two-pass recognition approach 
where the goal is to get utterances correctly recog-
nized by a CFG on the first pass so as to minimize 
computational expenses (Paek et al, 2007).  
All of this can be accomplished with reduced 
transcription effort by keeping track of and leve-
raging back-off statistics for active learning. If the 
back-off is a CFG, we keep track of statistics re-
lated to which CFG rules were utilized the most, 
whether they allowed the task to be successfully 
completed, etc. If the back-off is a SLM, we keep 
track of similar statistics related to the semantic 
alignment and mapping in spoken language under-
standing. Given an active learning criterion, these 
statistics can be used to selectively transcribe ut-
terances which can then be used to modify the 
CFG in Step 5 so that OOG utterances become 
ING. Section 3 covers this in more detail. 
Finally, in Step 5, the CFG grammar is mod-
ified using the selectively transcribed utterances. 
Although developers will probably want to do this 
manually, it is possible to automate much of this 
step by making grammar changes with minimal 
edit distance or Levenshtein distance. 
Leveraging a wider coverage back-off grammar 
is of course not new. For grammar-based applica-
tions, several researchers have investigated using a 
CFG along with a back-off grammar either simul-
taneously via a domain-trained SLM (Gorrell et 
a1., 2002), or in two-pass recognition using either 
an SLM trained on CFG data (Gorrell, 2003) or a 
dictation n-gram (Dusan & Flanagan, 2002). To 
our knowledge however, no prior research has con-
sidered leveraging statistics related to the back-off 
grammar for active learning, especially as part of a 
RAD approach. 
3 Active Learning 
Our overall approach utilizes back-off grammars to 
provide developers with a safety net for deploying 
applications earlier, and active learning to reduce 
transcription effort and cost. We now elaborate on 
active learning, demonstrate the concept with re-
spect to a CFG back-off. 
Active learning aims at reducing transcription 
of training examples by selecting utterances that 
are most likely to be informative according to a 
specified cost function (Hakkani-Tur et al, 2002). 
In the speech community, active learning has been 
successfully applied to reducing the transcription 
effort for ASR (Hakkani-Tur et al, 2002), SLU 
(Tur et al, 2003b), as well as finding labeling er-
rors (Tur et al, 2003). In our case, the examples 
are user utterances that need to be transcribed, and 
the learning involves modifying a CFG to achieve 
wider coverage of user expressions. Instead of pas-
sively transcribing everything and modifying the 
CFG as such, the grammar can ?actively? partici-
pate in which utterances are transcribed. 
The usual procedure for selecting utterances for 
grammar modification is to transcribe at least all 
failed utterances, such as those that fall below a 
rejection threshold. By leveraging a back-off 
grammar, developers have more information with 
which to select utterances for transcription. For a 
CFG back-off, how frequently a back-off rule fired 
can serve as an active learning criterion because 
that is where OOG utterances are handled. Given 
65
this active learning criterion, the algorithm would 
proceed as follows (where i denotes iteration, St 
denotes the set of transcribed utterances, and Su 
denotes the set of all utterances): 
[1] Modify CFGi using St and generate corresponding 
back-offi from the CFGi. 
[2] Recognize utterances in set Su using CFGi + back-
offi. 
[3] Compute statistics on what back-off rules fired 
when and how frequently. 
[4] Select the k utterances that were handled by the 
most frequently occurring back-off rule and tran-
scribe them. Call the new transcribed set as Si. 
[5] ;t t i u u iS S S S S S? ? ??  
[6] Stop when CFGi achieves a desired level of seman-
tic accuracy, or alternatively when back-off rules 
only handle a desired percentage of Su, otherwise 
go to Step 1. 
Note that the set Su grows with each iteration and 
follows as a result of deploying an application with 
a CFGi + back-offi. Step [1] corresponds to Step 5, 
1.1, and 2.1 of our approach. Steps [2-4] above 
constitute the active learning criterion and can be 
adjusted depending on what developers want to 
optimize. This algorithm currently assumes that 
runtime efficiency is the main objective (e.g., on a 
mobile device); hence, it is critical to move utter-
ances recognized in the second pass to the first 
pass. If developers are more interested in learning 
new semantics, in Step [4] above they could tran-
scribe utterances that failed in the back-off. With 
an active learning criterion in place, Step [6] pro-
vides a stopping criterion. This too can be adjusted, 
and may even target budgetary objectives. 
4 Evaluation 
For evaluation, we used utterances collected from 
204 users of Microsoft Voice Command, a gram-
mar-based command-and-control (C&C) applica-
tion for high-end mobile devices (see Paek et al, 
2007 for details). We partitioned 5061 transcribed 
utterances into five sets, one of which was used 
exclusively for testing. The remaining four were 
used for iterative CFG modification. For the first 
iteration, we started with a CFG which was a de-
graded version of the grammar currently shipped 
with the Voice Command product. It was obtained 
by using the mode, or the most frequent user utter-
ance, for each CFG rule. We compared two ap-
proaches: CFG_Full, where each iterative CFG 
was modified using the full set of transcribed utter-
ances that resulted in a failure state (i.e., when a 
false recognition event occurred or the phrase con-
fidence score fell below 45%, which was set by a 
proprietary tuning procedure for optimizing word-
error rate), and CFG_Active, where each iterative 
CFG was modified using only those transcribed 
utterances corresponding to the most frequently 
occurring CFG back-off rules. For both CFG_Full 
and CFG_Active, CFGi was modified using the 
same set of heuristics akin to minimal edit dis-
tance. In order to assess the value of using the 
back-off grammar as a safety net, we also com-
pared CFG_Full+Back-off, where a derived CFG 
back-off was utilized whenever a failure state oc-
curred with CFG_Full, and CFG_Active+Back-off, 
where again a CFG back-off was utilized, this time 
with the back-off derived from the CFG trained on 
selective utterances. 
As our metric, we evaluated semantic accuracy 
since that is what matters most in C&C settings. 
Furthermore, because recognition of part of an ut-
terance can increase the odds of ultimately achiev-
ing task completion (Paek et al, 2007), we carried 
out separate evaluations for the functional consti-
tuents of a C&C utterance (i.e., keyword and slot) 
as well as the complete phrase (keyword + slot). 
We computed accuracy as follows: For any single 
utterance, the recognizer can either accept or reject 
it. If it is accepted, then the semantics of the utter-
ance can either be correct (i.e., it matches what the 
user intended) or incorrect, hence: 
accuracy = CA / (CA + IA + R)   (1) 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejections. 
Table 2 displays semantic accuracies for both 
CFG_Full and CFG_Active. Standard errors about 
the mean were computed using the jacknife proce-
dure with 10 re-samples. Notice that both 
CFG_Full and CFG_Active initially have the same 
accuracy levels because they start off with the 
same degraded CFG. The highest accuracies ob-
tained almost always occurred in the second itera-
tion after modifying the CFG with the first batch of 
transcriptions. Thereafter, all accuracies seem to 
decrease. In order to understand why this would be 
case, we computed the coverage of the ith CFG on 
the holdout set. This is reported in the ?OOG%? 
column. Comparing CFG_Full to CFG_Active on 
66
keyword + slot accuracy, CFG_Full decreases in 
accuracy after the second iteration as does 
CFG_Active. However, the OOG% of CFG_Full is 
much lower than CFG_Active. In fact, it seems to 
level off after the second iteration, suggesting that 
perhaps the decrease in accuracies reflects the in-
crease in grammar perplexity; that is, as the gram-
mar covers more of the utterances, it has more 
hypotheses to consider, and as a result, performs 
slightly worse. Interestingly, after the last iteration, 
CFG_Active for keyword + slot and slot accuracies 
was slightly higher (69.06%) than CFG_Full 
(66.88%) (p = .05). Furthermore, this was done 
with 193 utterances as opposed to 1393, or 87% 
less transcriptions. For keyword accuracy, 
CFG_Active (64.09%) was slightly worse than 
CFG_Full (66.10%) (p < .05). 
With respect to the value of having a back-off 
grammar as a safety net, we found that both 
CFG_Full and CFG_Active achieved much higher 
accuracies with the back-off for keyword, slot, and 
keyword + slot accuracies. Notice also that the dif-
ferences between CFG_Full and CFG_Active after 
the last iteration were much closer to each other 
than without the back-off, suggesting applications 
should always be deployed with a back-off. 
5 Conclusion 
In this paper, we explored enhancing the usual 
iterative process of grammar modification by leve-
raging active learning with back-off grammars. 
Because the back-off grammars expand coverage 
of user utterances to handle OOG occurrences, de-
velopers have a safety net for deploying applica-
tions earlier. Furthermore, because statistics related 
to the back-off can be used for active learning, de-
velopers can reduce the effort and cost of data 
transcription. In our simulation experiments, leve-
raging active learning achieved levels of semantic 
accuracy comparable to transcribing all failed ut-
terances with 87% less transcriptions. 
References 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based upon multimod-
al language acquisition. In Proc. of ICMI. 
L. Galescu, E. Ringger, & J. Allen. 1998. Rapid language model de-
velopment for new task domains. In Proc. of LREC. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intelligent help to 
mixed initiative spoken dialogue systems. In Proc. of ICSLP. 
G.. Gorrell. 2003. Using statistical language modeling to identify new 
vocabulary in a grammar-based speech recognition system. In 
Proc. of Eurospeech. 
D. Hakkani-Tur, G. Riccardi & A. Gorin. 2002. Active learning for 
automatic speech recognition. In Proc. of ICASSP. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, & I. Le-
win. 2001. Comparing grammar-based and robust approaches to 
speech understanding: A case study. In Proc. of Eurospeech. 
T. Paek, S. Gandhe, D. Chickering & Y. Ju. 2007. Handling out-of-
grammar commands in mobile speech interaction using back-off 
filler models. In Proc. of ACL Workshop on Grammar-Based Ap-
proaches to Spoken Language Processing (SPEECHGRAM). 
M. Rayner, B.A. Hockey, & P. Bouillon. 2006. Putting Linguistics 
into Speech Recognition: The Regulus Grammar Compiler. CSLI. 
G. Tur, M. Rahim & D. Hakkani-Tur. 2003. Active labeling for spo-
ken language understanding. In Proc. of Eurospeech. 
G. Tur, R. Schapire, & D. Hakkani-Tur. 2003b. Active learning for 
spoken language understanding. In Proc. of ICASSP. 
W3C. 2004. Speech Recognition Grammar Specification Version 1.0. 
http://www.w3.org/TR/speech-grammar  
Approach i 
Utterances 
Transcribed 
Keyword  
Accuracy 
Slot  
Accuracy 
Keyword + Slot 
Accuracy 
Processing 
Time (ms) 
OOG% 
CFG_Full 
 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 590 66.20% (0.12%) 71.02% (0.23%) 70.59% (0.23%) 401 (4.0586) 31.92% 
3 1000 65.80% (0.15%) 69.72% (0.19%) 69.06% (0.19%) 422 (4.5804) 31.30% 
4 1393 66.10% (0.13%) 67.54% (0.22%) 66.88% (0.21%) 433 (4.7061) 30.95% 
CFG_Full + 
Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 590 73.32% (0.11%) 72.11% (0.22%) 71.68% (0.23%) 562 (10.4696) 31.92% 
3 1000 72.52% (0.12%) 72.11% (0.21%) 71.46% (0.22%) 584 (10.4985) 31.30% 
4 1393 73.02% (0.10%) 71.02% (0.23%) 70.37% (0.23%) 592 (10.6805) 30.95% 
CFG_Active 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 87 64.09% (0.13%) 74.29% (0.21%) 74.07% (0.22%) 395 (4.1469) 42.09% 
3 138 64.29% (0.15%) 70.15% (0.22%) 69.50% (0.24%) 409 (4.3375) 38.02% 
4 193 64.09% (0.15%) 69.72% (0.23%) 69.06% (0.24%) 413 (4.4015) 37.93% 
CFG_Active 
+ Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 87 72.52% (0.10%) 76.91% (0.19%) 76.47% (0.21%) 568 (10.3494) 42.09% 
3 138 71.72% (0.14%) 71.90% (0.24%) 71.24% (0.27%) 581 (10.6330) 38.02% 
4 193 71.21% (0.15%) 71.90% (0.25%) 71.24% (0.26%) 580 (10.5266) 37.93% 
Table 2. Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either 
?Full? or ?Active? transcriptions (i.e., selective transcriptions based on active learning). Parentheses indicate standard error about the mean.  
The ?i? column represents iteration.  The ?Utterances Transcribed? column is cumulative.  The ?OOG%? column represents coverage of the 
ith CFG on the hold-out set. Rows containing ?Back-off? evaluate 2-pass recognition using both the CFG and a derived CFG back-off. 
 
 
67
