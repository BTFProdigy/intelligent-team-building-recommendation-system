Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 407?411,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards the Orwellian Nightmare 
Separation of Business and Personal Emails 
 
Sanaz Jabbari, Ben Allison, David Guthrie, Louise Guthrie 
Department of Computer Science 
University of Sheffield 
211 Portobello St. 
Sheffield 
S1 4DP 
{s.jabbari, b.allison, d.guthrie, l.guthrie}@dcs.shef.ac.uk 
 
Abstract 
This paper describes the largest scale annotation pro-
ject involving the Enron email corpus to date. Over 
12,500 emails were classified, by humans, into the 
categories ?Business? and ?Personal?, and then sub-
categorised by type within these categories. The paper 
quantifies how well humans perform on this task 
(evaluated by inter-annotator agreement). It presents 
the problems experienced with the separation of these 
language types. As a final section, the paper presents 
preliminary results using a machine to perform this 
classification task. 
 
1 Introduction 
Almost since it became a global phenomenon, com-
puters have been examining and reasoning about our 
email. For the most part, this intervention has been 
well natured and helpful ? computers have been try-
ing to protect us from attacks of unscrupulous blanket 
advertising mail shots. However, the use of computers 
for more nefarious surveillance of email has so far 
been limited. The sheer volume of email sent means 
even government agencies (who can legally intercept 
all mail) must either filter email by some pre-
conceived notion of what is interesting, or they must 
employ teams of people to manually sift through the 
volumes of data. For example, the NSA has had mas-
sive parallel machines filtering e-mail traffic for at 
least ten years. 
 
The task of developing such automatic filters at re-
search institutions has been almost impossible, but for 
the opposite reason. There is no shortage of willing 
researchers, but progress has been hampered by the 
lack of any data ? one?s email is often hugely private, 
and the prospect of surrendering it, in its entirety, for 
research purposes is somewhat unsavoury. 
 
Recently, a data resource has become available where 
exactly this condition (several hundred people?s entire 
email archive) has been satisfied ? the Enron dataset. 
During the legal investigation of the collapse of En-
ron, the FERC (Federal Energy Regulatory Commis-
sion) seized the emails of every employee in that 
company. As part of the process, the collection of 
emails was made public and subsequently prepared 
for research use by researchers at Carnegie Melon 
University (Klimt and Yang, 2004).Such a corpus of 
authentic data, on such a large scale, is unique, and an 
invaluable research tool. It then falls to the prospec-
tive researcher to decide which divisions in the lan-
guage of email are interesting, which are possible, and 
how the new resource might best be used.  
 
Businesses which offer employees an email system at 
work (and there are few who do not) have always 
known that they possess an invaluable resource for 
monitoring their employees? work habits. During the 
1990s, UK courts decided that that an employee?s 
email is not private ? in fact, companies can read 
them at will. However, for exactly the reasons de-
scribed above, automatic monitoring has been impos-
sible, and few businesses have ever considered it suf-
ficiently important to employ staff to monitor the 
email use of other staff. However, in monitoring staff 
productivity, few companies would decline the use of 
a system which could analyse the email habits of its 
employees, and report the percentage of time which 
each employee was spending engaged in non-work 
related email activities. 
 
The first step in understanding how this problem 
might be tackled by a computer, and if it is even fea-
sible for this to happen, is to have humans perform the 
task. This paper describes the process of having hu-
mans annotate a corpus of emails, classifying each as 
to whether they are business or personal, and then 
attempting to classify the type of business or personal 
mail being considered. 
 
A resource has been created to develop a system able 
to make these distinctions automatically. Furthermore, 
the process of subcategorising types of business and 
personal has allowed invaluable insights into the areas 
407
where confusion can occur, and how these confusions 
might be overcome. 
 
The paper presents an evolution of appropriate sub-
categories, combined with analysis of performance 
(measured by inter-annotator agreement) and reasons 
for any alterations. It addresses previous work done 
with the Enron dataset, focusing particularly on the 
work of Marti Hearst at Berkeley who attempted a 
smaller-scale annotation project of the Enron corpus, 
albeit with a different focus. It concludes by suggest-
ing that in the main part (with a few exceptions) the 
task is possible for human annotators. The project has 
produced a set of labeled messages (around 14,000, 
plus double annotations for approximately 2,500) with 
arguably sufficiently high business-personal agree-
ment that machine learning algorithms will have suf-
ficient material to attempt the task automatically. 
2 Introduction to the Corpus 
Enron?s email was made public on the Web by FERC 
(Federal Energy Regulatory Commission), during a 
legal investigation on Enron Corporation. The emails 
cover 92 percent of the staff?s emails, because some 
messages have been deleted "as part of a redaction 
effort due to requests from affected employees". The 
dataset was comprised of 619,446 messages from 158 
users in 3,500 folders. However, it turned out that the 
raw data set was suffering from various data integrity 
problems. Various attempts were made to clean and 
prepare the dataset for research purposes. The dataset 
used in this project was the March 2, 2004 version 
prepared at Carnegie Mellon University, acquired 
from http://www.cs.cmu.edu/~enron/. This version of 
the dataset was reduced to 200,399 emails by remov-
ing some folders from each user. Folders like ?discus-
sion threads? and ?all documents?, which were ma-
chine generated and contained duplicate emails, were 
removed in this version.  
 
There were on average 757 emails per each of the 158 
users. However, there are between one and 100,000 
emails per user. There are 30,091 threads present in 
123,091 emails. The dataset does not include attach-
ments. Invalid email addresses were replaced with 
?user@enron.com?.  When no recipient was specified 
the address was replaced with 
?no_address@enron.com? (Klimt and Yang, 2005). 
 
3 Previous Work with the Dataset 
The most relevant piece of work to this paper was 
performed at Berkeley. Marti Hearst ran a small-scale 
annotation project to classify emails in the corpus by 
their type and purpose (Email annotation at Berkely). 
In total, approximately 1,700 messages were anno-
tated by two distinct annotators. Annotation catego-
ries captured four dimensions, but broadly speaking 
they reflected the following qualities of the email: 
coarse genre, the topic of the email if business was 
selected, information about any forwarded or included 
text and the emotional tone of the email. However, the 
categories used at the Berkeley project were incom-
patible with our requirements for several reasons: that 
project allowed multiple labels to be assigned to each 
email; the categories were not designed to facilitate 
discrimination between business and personal emails; 
distinctions between topic, genre, source and purpose 
were present in each of the dimensions; and no effort 
was made to analyse the inter-annotator agreement 
(Email annotation at Berkely). 
 
User-defined folders are preserved in the Enron data, 
and some research efforts have used these folders to 
develop and evaluate machine-learning algorithms for 
automatically sorting emails (Klimt and Yang, 2004). 
However, as users are often inconsistent in organising 
their emails, so the training and testing data in these 
cases are questionable.  For example, many users 
have folders marked ?Personal?, and one might think 
these could be used as a basis for the characterisation 
of personal emails. However, upon closer inspection it 
becomes clear that only a tiny percentage of an indi-
vidual?s personal emails are in these folders. Simi-
larly, many users have folders containing exclusively 
personal content, but without any obvious folder 
name to reveal this. All of these problems dictate that 
for an effective system to be produced, large-scale 
manual annotation will be necessary. 
 
Researchers at Queen?s University, Canada (Keila, 
2005) recently attempted to categorise and identify 
deceptive messages in the Enron corpus. Their 
method used a hypothesis from deception theory (e.g., 
deceptive writing contains cues such as reduced fre-
quency of first-person pronouns and increased fre-
quency of ?negative emotion? words) and as to what 
constitutes deceptive language. Single value decom-
position (SVD) was applied to separate the emails, 
and a manual survey of the results allowed them to 
conclude that this classification method for detecting 
deception in email was promising. 
 
Other researchers have attempted to analyse the Enron 
emails from a network analytic perspective (Deisner, 
2005).  Their goal was to analyse the flow of commu-
nication between employees at times of crisis, and 
develop a characterisation for the state of a communi-
cation network in such difficult times, in order to 
identify looming crises in other companies from the 
state of their communication networks. They com-
pared the network flow of email in October 2000 and 
October 2001.    
 
4 Annotation Categories for this Project 
Because in many cases there is no definite line be-
tween business emails and personal emails, it was 
decided to mark emails with finer categories than 
408
Business and Personal. This subcategorising not only 
helped us to analyse the different types of email 
within business and personal emails, but it helped us 
to find the nature of the disagreements that  occurred 
later on, in inter-annotation.  In other words, this 
process allowed us to observe patterns in disagree-
ment.  
 
Obviously, the process of deciding categories in any 
annotation project is a fraught and contentious one. 
The process necessarily involves repeated cycles of 
category design, annotation, inter-annotation, analysis 
of disagreement, category refinement. While the proc-
ess described above could continue ad infinitum, the 
sensible project manager must identify were this 
process is beginning to converge on a set of well-
defined but nonetheless intuitive categories, and final-
ise them. 
 
Likewise, the annotation project described here went 
through several evolutions of categories, mediated by 
input from annotators and other researchers. The final 
categories chosen were: 
 
Business: Core Business, Routine Admin, Inter-
Employee Relations, Solicited/soliciting mailing, Im-
age. 
 
Personal: Close Personal, Forwarded, Auto generated 
emails. 
 
5 Annotation and Inter-Annotation 
Based on the categories above, approximately 12,500 
emails were single-annotated by a total of four anno-
tators. 
 
The results showed that around 83% of the emails 
were business related, while 17% were personal. The 
company received one personal email for every five 
business emails. 
 
Fig 1: Distribution of Emails in the Corpus
BUSINESS
83%
PERSONAL
17%
BUSINESS
PERSONAL
 
 
A third of the received emails were ?Core Business? 
and a third were ?Routine Admin?. All other catego-
ries comprised the remaining third of the emails. One 
could conclude that approximately one third of emails 
received at Enron were discussions of policy, strategy, 
legislation, regulations, trading, and other high-level 
business matters. The next third of received emails 
were about the peripheral, routine matters of the com-
pany. These are emails related to HR, IT administra-
tion, meeting scheduling, etc. which can be regarded 
as part of the common infrastructure of any large 
scale corporation. 
 
The rest of the emails were distributed among per-
sonal emails, emails to colleagues, company news 
letters, and emails received due to subscription. The 
biggest portion of the last third, are emails received 
due to subscription, whether the subscription be busi-
ness or personal in nature. 
 
In any annotation project consistency should be 
measured. To this end 2,200 emails were double an-
notated between four annotators. As Figure 2 below 
shows, for 82% of the emails both annotators agreed 
that the email was business email and in 12% of the 
emails, both agreed on them being personal. Six per-
cent of the emails were disagreed upon. 
 
Fig 2: Agreements and Disagreements in Inter-Annotation
Disagreement
6%
Personal 
Agreement
12%
Business 
Agreement
82%
Business Agreement
Personal Agreement
Disagreement
 
 
By analysing the disagreed categories, some patterns 
of confusion were found.  
 
Around one fourth of the confusions were solicited 
emails where it was not clear whether the employee 
was subscribed to a particular newsletter group for his 
personal interest, private business, or Enron?s busi-
ness. While some subscriptions were clearly personal 
(e.g. subscription to latest celebrity news) and some 
were clearly business related (e.g. Daily Energy re-
ports), for some it was hard to identify the intention of 
the subscription (e.g. New York Times). 
 
Eighteen percent of the confusions were due to emails 
about travel arrangements, flight and hotel booking 
confirmations, where it was not clear whether the per-
sonal was acting in a business or personal role. 
 
409
Thirteen percent of the disagreements were upon 
whether an email is written between two Enron em-
ployees as business colleagues or friends. The emails 
such as ?shall we meet for a coffee at 2:00?? If insuf-
ficient information exists in the email, it can be hard 
to draw the line between a personal relationship and a 
relationship between colleagues. The annotators were 
advised to pick the category based on the formality of 
the language used in such emails, and reading be-
tween the lines wherever possible. 
 
About eight percent of the disagreements were on 
emails which were about services that Enron provides 
for its employees. For example, the Enron?s running 
club is seeking for runners, and sending an ad to En-
ron?s employers. Or Enron?s employee?s assistance 
Program (EAP), sending an email to all employees, 
letting them know that in case of finding themselves 
in stressful situations they can use some of the ser-
vices that Enron provides for them or their families.  
 
One theme was encountered in many types of confu-
sions: namely, whether to decide an e-mail?s category 
based upon its topic or its form. For example, should 
an email be categorised because it is scheduling a 
meeting or because of the subject of the meeting be-
ing scheduled? One might consider this a distinction 
by topic or by genre. 
 
As the result, final categories were created to reflect 
topic as the only dimension to be considered in the 
annotation. ?Solicited/Soliciting mailing?, ?Solic-
ited/Auto generated mailing? and ?Forwarded? were 
removed and ?Keeping Current?, ?Soliciting? were 
added as business categories and ?Personal Mainte-
nance? and ?Personal Circulation? were added as per-
sonal categories. The inter-annotation agreement was 
measured for one hundred and fifty emails, annotated 
by five annotators. The results confirmed that these 
changes had a positive effect on the accuracy of anno-
tation. 
 
6 Preliminary Results of Automatic 
Classification 
Some preliminary experiments were performed with 
an automatic classifier to determine the feasibility of 
separating business and personal emails by machine. 
The classifier used was a probabilistic classifier based 
upon the distribution of distinguishing words. More 
information can be found in (Guthrie and Walker, 
1994). 
 
Two categories from the annotation were chosen 
which were considered to typify the broad categories 
? these were Core Business (representing business) 
and Close Personal (representing personal). The Core 
Business class contains 4,000 messages (approx 
900,000 words), while Close Personal contains ap-
proximately 1,000 messages (220,000 words). 
 
The following table summarises the performance of 
this classifier in terms of Recall, Precision and F-
Measure and accuracy: 
 
Class Recall Precision F-
Measure 
Accuracy 
Business 0.99 0.92 0.95 0.99 
Personal 0.69 0.95 0.80 0.69 
AVERAGE 0.84 0.94 0.88 0.93 
 
Based upon the results of this experiment, one can 
conclude that automatic methods are also suitable for 
classifying emails as to whether they are business or 
personal. The results indicate that the business cate-
gory is well represented by the classifier, and given 
the disproportionate distribution of emails, the classi-
fier?s tendency towards the business category is un-
derstandable. 
 
Given that our inter-annotator agreement statistic tells 
us that humans only agree on this task 94% of the 
time, preliminary results with 93% accuracy (the sta-
tistic which correlates exactly to agreement) of the 
automatic method are encouraging. While more work 
is necessary to fully evaluate the suitability of this 
task for application to a machine, the seeds of a fully 
automated system are sown. 
 
7 Conclusion 
This paper describes the process of creating an email 
corpus annotated with business or personal labels. By 
measuring inter-annotator agreement it shows that this 
process was successful. Furthermore, by analysing the 
disagreements in the fine categories, it has allowed us 
to characterise the areas where the business/personal 
decisions are difficult.  
 
In general, the separation of business and personal 
mails is a task that humans can perform. Part of the 
project has allowed the identification of the areas 
where humans cannot make this distinction (as dem-
onstrated by inter-annotator agreement scores) and 
one would not expect machines to perform the task 
under these conditions either. In all other cases, where 
the language is not ambiguous as judged by human 
annotators, the challenge has been made to automatic 
classifiers to match this performance. 
 
Some initial results were reported where machines 
attempted exactly this task. They showed that accu-
racy almost as high as human agreement was 
achieved by the system. Further work, using much 
larger sets and incorporating all types of business and 
personal emails, is the next logical step. 
 
410
Any annotation project will encounter its problems in 
deciding appropriate categories. This paper described 
the various stages of evolving these categories to a 
stage where they are both intuitive and logical and 
also, produce respectable inter-annotator agreement 
scores. The work is still in progress in ensuring 
maximal consistency within the data set and refining 
the precise definitions of the categories to avoid pos-
sible overlaps. 
References 
Brian Klimt and Yiming Yang. 2004. Introducing the 
Enron Email Corpus, Carnegie Mellon University.  
 
Brian Klimt and Yiming Yang. 2004. The Enron Cor-
pus: A New Data Set for Email Classification Re-
search. Carnegie Mellon University. 
 
Email Annotation at Berkely   
http://bailando.sims.berkeley.edu/enron_email.html
 
Jana Diesner and Kathleen M. Karley. 2005. Explora-
tion of Communication Networks from the Enron 
Email Corpus, Carnegie Mellon University  
 
Louise Guthrie,  Elbert Walker and Joe Guthrie. 1994 
Document classification by machine: Theory and 
practice. Proc. of COLING'94 
 
Parambir S. Keila and David B. Skillcorn. 2005.  De-
tecting Unusual and Deceptive Communication in 
Email. Queen?s University, CA 
 
 
 
 
 
411
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 289?292,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation Metrics for the Lexical Substitution Task
Sanaz Jabbari Mark Hepple Louise Guthrie
Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield, S1 4DP, UK
{S.Jabbari,M.Hepple,L.Guthrie}@dcs.shef.ac.uk
Abstract
We identify some problems of the evaluation
metrics used for the English Lexical Substitu-
tion Task of SemEval-2007, and propose al-
ternative metrics that avoid these problems,
which we hope will better guide the future de-
velopment of lexical substitution systems.
1 Introduction
The English Lexical Substitution task at SemEval-
2007 (here called ELS07) requires systems to find
substitutes for target words in a given sentence (Mc-
Carthy & Navigli, 2007: M&N). For example, we
might replace the target word match with game in
the sentence they lost the match. System outputs are
evaluated against a set of candidate substitutes pro-
posed by human subjects for test items. Targets are
typically sense ambiguous (e.g. match in the above
example), and so task performance requires a com-
bination of word sense disambiguation (by exploit-
ing the given sentential context) and (near) synonym
generation. In this paper, we discuss some problems
of the evaluation metrics used in ELS07, and then
propose some alternative measures that avoid these
problems, and which we believe will better serve to
guide the development of lexical substitution sys-
tems in future work.1 The subtasks within ELS07
divide into two groups, in terms of whether they fo-
cus on a system?s ?best? answer for a test item, or ad-
dress the broader set of answer candidates a system
1We consider here only the case of substituting for single
word targets. Subtasks of ELS07 involving multi-word substi-
tutions are not addressed.
can produce. In what follows, we address these two
cases in separate sections, and then present some re-
sults for applying our new metrics for the second
case. We begin by briefly introducing the test ma-
terials that were created for the ELS07 evaluation.
2 Evaluation Materials
Briefly stated, the ELS07 dataset comprises around
2000 sentences, providing 10 test sentences each
for some 201 preselected target words, which were
required to be sense ambiguous and have at least
one synonym, and which include nouns, verbs, ad-
jectives and adverbs. Five human annotators were
asked to suggest up to three substitutes for the tar-
get word of each test sentence, and their collected
suggestions serve as the gold standard against which
system outputs are compared. Around 300 sentences
were distributed as development data, and the re-
mainder retained for the final evaluation.
To assist defining our metrics, we formally de-
scribe this data as follows.2 For each sentence t
i
in the test data (1 ? i ? N , N the number of test
items), let H
i
denote the set of human proposed sub-
stitutes. A key aspect of the data is the count of hu-
man annotators that proposed each candidate (since
a term appears a stronger candidate if proposed by
annotators). For each t
i
, there is a function freq
i
which returns this count for each term within H
i
(and 0 for any other term), and a value maxfreq
i
corresponding to the maximal count for any term in
H
i
. The pairing of H
i
and freq
i
in effect provides a
multiset representation of the human answer set. We
2For consistency, we also restate the original ELS07 metrics
in these terms, whilst preserving their essential content.
289
use |S|i in what follows to denote the multiset car-
dinality of S according to freq
i
, i.e. ?
a?S
freq
i
(a).
Some of the ELS07 metrics use a notion of mode
answer m
i
, which exists only for test items that
have a single most-frequent human response, i.e.
a unique a ? H
i
such that freq
i
(a) = maxfreq
i
.
To adapt an example from M&N, an item with tar-
get word happy (adj) might have human answers
{glad ,merry , sunny , jovial , cheerful } with counts
(3,3,2,1,1) respectively. We will abbreviate this an-
swer set as H
i
= {G:3,M:3,S:2,J:1,Ch:1} where it
is used later in the paper.
3 Best Answer Measures
Two of the ELS07 tasks address how well systems
are able to find a ?best? substitute for a test item, for
which individual test items are scored as follows:
best(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
? |A
i
|
mode(i) =
{
1 if bg
i
= m
i
0 otherwise
For the first task, a system can return a set of an-
swers A
i
(the answer set for item i), but since the
score achieved is divided by |A
i
|, returning multiple
answers only serves to allow a system to ?hedge its
bets? if it is uncertain which candidate is really the
best. The optimal score on a test item is achieved by
returning a single answer whose count is maxfreq
i
,
with proportionately lesser credit being received for
any answer in H
i
with a lesser count. For the sec-
ond task, which uses the mode metric, only a single
system answer ? its ?best guess? bg
i
? is allowed,
and the score is simply 0 or 1 depending on whether
the best guess is the mode. Overall performance is
computed by averaging across a broader set of test
items (which for the second task includes only items
having a mode value). M&N distinguish two over-
all performance measures: Recall, which averages
over all relevant items, and Precision, which aver-
ages only over those items for which the system gave
a non-empty response.
We next discuss these measures and make an al-
ternative proposal. The task for the first measure
seems a reasonable one, i.e. assessing the ability of
systems to provide a ?best? answer for a test item,
but allowing them to offer multiple candidates (to
?hedge their bets?). However, the metric is unsatis-
factory in that a system that performs optimally in
terms of this task (i.e. which, for every test item, re-
turns a single correct ?most frequent? response) will
get a score that is well below 1, because the score is
also divided by |H
i
|
i, the multiset cardinality of H
i
,
whose size varies between test items (being a con-
sequence of the number of alternatives suggested by
the human annotators), but which is typically larger
than the numerator value maxfreq
i
of an optimal an-
swer (unless H
i
is singleton). This problem is fixed
in the following modified metric definition, by di-
viding instead by maxfreq
i
, as then a response con-
taining a single optimal answer will score 1.
best(i) =
?
a?A
i
freq
i
(a)
maxfreq
i
? |A
i
|
best
1
(i) =
freq
i
(bg
i
)
maxfreq
i
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
an optimal response A
i
= {M} receives score 1,
where the original metric gives score 0.3. Singleton
responses containing a correct but non-optimal an-
swer receive proportionately lower credit, e.g. for
A
i
= {S} we score 0.66 (vs. 0.2 for the origi-
nal metric). For a non-singleton answer set includ-
ing, say, a correct answer and an incorrect one, the
credit for the correct answer will be halved, e.g. for
A
i
= {S,X} we score 0.33.
Regarding the second task, we think it reasonable
to have a task where systems may offer only a single
?best guess? response, but argue that the mode met-
ric used has two key failings: it is too brittle in being
applicable only to items that have a mode answer,
and it loses information valuable to system rank-
ing, in assigning no credit to a response that might
be good but not optimal. We propose instead the
best
1
metric above, which assigns score 1 to a best
guess answer with count maxfreq
i
, but applies to all
test items irrespective of whether or not they have
a unique mode. For answers having lesser counts,
proportionately less credit is assigned. This metric
is equivalent to the new best metric shown beside it
for the case where |A
i
| = 1.
For assessing overall performance, we suggest
just taking the average of scores across all test items,
c.f. M&N?s Recall measure. Their Precision met-
ric is presumably intended to favour a system that
can tell whether it does or does not have any good
answers to return. However, the ability to draw a
290
boundary between good vs. poor candidates will be
reflected widely in a system?s performance and cap-
tured elsewhere (not least by the coverage metrics
discussed later) and so, we argue, does not need to
be separately assessed in this way. Furthermore, the
fact that a system does not return any answers may
have other causes, e.g. that its lexical resources have
failed to yield any substitution candidates for a term.
4 Measures of Coverage
A third task of ELS07 assesses the ability of systems
to field a wider set of good substitution candidates
for a target, rather than just a ?best? candidate. This
?out of ten? (oot) task allows systems to offer a set
A
i
of upto 10 guesses per item i, and is scored as:
oot(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
Since the score is not divided by the answer set
size |A
i
|, no benefit derives from offering less than
10 candidates.3 When systems are asked to field a
broader set of candidates, we suggest that evalua-
tion should assess if the response set is good in con-
taining as many correct answers as possible, whilst
containing as few incorrect answers as possible. In
general, systems will tackle this problem by com-
bining a means of ranking candidates (drawn from
lexical resources) with a means of drawing a bound-
ary between good and bad candidates, e.g. thresh-
old setting.4 Since the oot metric does not penalise
incorrect answers, it does not encourage systems to
develop such boundary methods, even though this is
important to their ultimate practical utility.
The view of a ?good? answer set described above
suggests a comparison of A
i
to H
i
using versions
of ?recall? and ?precision? metrics, that incorporate
the ?weighting? of human answers via freq
i
. Let us
begin by noting the obvious definitions for recall and
3We do not consider here a related task which assesses
whether the mode answer m
i
is found within an answer set of
up to 10 guesses. We do not favour the use of this metric for
reasons parallel to those discussed for the mode metric of the
previous section, i.e. brittleness and information loss.
4In Jabbari et al (2010), we define a metric that directly
addresses the ability of systems to achieve good ranking of sub-
stitution candidates. This is not itself a measure of lexical sub-
stitution task performance, but addresses a component ability
that is key to the achievement of lexical substitution tasks.
precision metrics without count-weighting:
R(i) =
|H
i
?A
i
|
|H
i
|
P (i) =
|H
i
?A
i
|
|A
i
|
Our definitions of these metrics, given below, do
include count-weighting, and require some explana-
tion. The numerator of our recall definition is |A
i
|
i
not |H
i
? A
i
|
i as |A
i
|
i
= |H
i
? A
i
|
i (as freq
i
as-
signs 0 to any term not in H
i
), an observation which
also affects the numerator of our P definition. Re-
garding the latter?s denominator, merely dividing by
|A
i
|
i would not penalise incorrect terms (as, again,
freq
i
(a) = 0 for any a /? H
i
), so this is done di-
rectly by adding k|A
i
?H
i
|, where |A
i
?H
i
| is the
number of incorrect answers, and k some penalty
factor, which might be k = 1 in the simplest case.
(Note that our weighted R metric is in fact equiv-
alent to the oot definition above.) As usual, an F-
score can be computed as the harmonic mean of
these values (i.e. F = 2PR/(P + R)). For as-
sessing overall performance, we might average P ,
R and F scores across all test items.
R(i) =
|A
i
|
i
|H
i
|
i
P (i) =
|A
i
|
i
|A
i
|
i
+ k|A
i
?H
i
|
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
the perfect response set A
i
= {G,M,S, J,Ch}
gives P and R scores of 1. The response
A
i
= {G,M,S, J,Ch,X, Y, Z, V,W}, containing
all correct answers plus 5 incorrect ones, gets R =
1, but only P = 0.66 (assuming k = 1, giving
10/(10 + 5)). The response A
i
= {G,S, J,X, Y },
with 3 out of 5 correct answers, plus 2 incorrect
ones, gets R = 0.6 (6/10) and P = 0.75 (6/6 + 2))
5 Applying the Coverage measure
Although the ?best guess? task is a valuable indicator
of the likely utility of a lexical substitution system
within various broader applications, we would argue
that the core task for lexical substitution is coverage,
i.e. the ability to field a broad set of correct substi-
tution candidates. This task requires systems both to
field and rank promising candidates, and to have a
means of drawing a boundary between the good and
bad candidates, i.e. a boundary strategy.
In this section, we apply the coverage metrics to
the outputs of some lexical substitution systems, and
291
Model 1 2 3 4 5 6 7 8 9 10
bow .067 .114 .151 .173 .191 .201 .212 .219 .222 .225
lm .119 .192 .228 .246 .256 .267 .271 .272 .271 .271
cmlc .139 .205 .251 .271 .284 .288 .291 .290 .289 .286
KU .173 .244 .287 .307 .318 .321 .320 .318 .314 .311
Table 3: Coverage F-scores (macro-avgd), for simple boundary strategies (with penalty factor k = 1).
All By part-of-speech
Model words nouns adj verb adv
bow .326 .343 .334 .205 .461
lm .393 .372 .442 .252 .562
cmlc .414 .404 .447 .311 .534
KU .462 .408 .511 .398 .567
Table 1: Out-of-ten recall scores for all the systems (with
a subdivision by pos of target item).
All By part-of-speech
Model words nouns adj verb adv
bow .298 .315 .302 .189 .422
lm .371 .35 .408 .24 .539
cmlc .395 .383 .419 .31 .506
KU .435 .379 .477 .385 .536
Table 2: Optimal F-scores (macro-avgd) for coverage,
computed over the (oot) ranked outputs of the systems
(with penalty factor k = 1).
compare the indication it provides of relative sys-
tem performance to that of the oot metric. We con-
sider three systems described in Jabbari (2010), de-
veloped as part of an investigation into the means
and benefits of combining models of lexical context:
(i) bow: a system using a bag-of-words model to
rank candidates, (ii) lm: using a (simple) n-gram lan-
guage model, and (iii) cmlc: using a model that com-
bines bow and lm models into one. We also consider
the system KU, which uses a very large language
model and an advanced treatment of smoothing, and
which performed well at ELS07 (Yuret, 2007).5 Ta-
ble 1 shows the oot scores for these systems, includ-
ing a breakdown by part-of-speech, which indicate a
performance ranking: bow < lm < cmlc < KU
Our first problem is that these systems are devel-
oped for the oot task, not coverage, so after rank-
5We thank Deniz Yuret for allowing us to use his system?s
outputs in this analysis.
ing their candidates, they do not attempt to draw
a boundary between the candidates worth returning
and those not. Instead, we here use the oot out-
puts to compute an optimal performance for each
system, i.e. we find, for the ranked candidates of
each question, the cut-off position giving the high-
est F-score, and then average these scores across
questions, which tells us the F-score the system
could achieve if it had an optimal boundary strategy.
These scores, shown in Table 2, indicate a ranking of
systems in line with that in Table 1, which is not sur-
prising as both will ultimately reflect the quality of
candidate ranking achieved by the systems.
Table 3 shows the coverage results achieved by
applying a naive boundary strategy to the system
outputs. The strategy is just to always return the
top n candidates for each question, for a fixed value
n. Again, performance correlates straightforwardly
with the underlying quality of ranking. Comparing
tables, we see, for example, that by always returning
6 candidates, the system KU could achieve a cover-
age of .32 as compared to the .435 optimal score.
References
D. McCarthy and R. Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task.
Proc. of the 4th Int. Workshop on Semantic Eval-
uations (SemEval-2007), Prague.
S. Jabbari. 2010. A Statistical Model of Lexical Con-
text, PhD Thesis, University of Sheffield.
S. Jabbari, M. Hepple and L.Guthrie. 2010. Evaluat-
ing Lexical Substitution: Analysis and NewMea-
sures. Proc. of the 7th Int. Conf. on Language
Resources and Evaluation (LREC-2010). Malta.
D. Yuret. 2007. KU: Word Sense Disambiguation by
Substitution. In Proc. of the 4th Int. Workshop on
Semantic Evaluations (SemEval-2007), Prague.
292
