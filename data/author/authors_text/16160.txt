Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 344?348,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Latent Information to Predict Diffusions of Novel Topics on 
Social Networks 
Tsung-Ting Kuo1*, San-Chuan Hung1, Wei-Shih Lin1, Nanyun Peng1, Shou-De Lin1, 
Wei-Fen Lin2 
1Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan 
2MobiApps Corporation, Taiwan 
*d97944007@csie.ntu.edu.tw 
 
Abstract 
This paper brings a marriage of two seemly 
unrelated topics, natural language 
processing (NLP) and social network 
analysis (SNA). We propose a new task in 
SNA which is to predict the diffusion of a 
new topic, and design a learning-based 
framework to solve this problem. We 
exploit the latent semantic information 
among users, topics, and social connections 
as features for prediction. Our framework is 
evaluated on real data collected from public 
domain. The experiments show 16% AUC 
improvement over baseline methods. The 
source code and dataset are available at 
http://www.csie.ntu.edu.tw/~d97944007/dif
fusion/ 
1 Background 
The diffusion of information on social networks 
has been studied for decades. Generally, the 
proposed strategies can be categorized into two 
categories, model-driven and data-driven. The 
model-driven strategies, such as independent 
cascade model (Kempe et al, 2003), rely on 
certain manually crafted, usually intuitive, models 
to fit the diffusion data without using diffusion 
history. The data-driven strategies usually utilize 
learning-based approaches to predict the future 
propagation given historical records of prediction 
(Fei et al, 2011; Galuba et al, 2010; Petrovic et al, 
2011).  Data-driven strategies usually perform 
better than model-driven approaches because the 
past diffusion behavior is used during learning 
(Galuba et al, 2010). 
Recently, researchers started to exploit content 
information in data-driven diffusion models (Fei et 
al., 2011; Petrovic et al, 2011; Zhu et al, 2011). 
However, most of the data-driven approaches 
assume that in order to train a model and predict 
the future diffusion of a topic, it is required to 
obtain historical records about how this topic has 
propagated in a social network (Petrovic et al, 
2011; Zhu et al, 2011). We argue that such 
assumption does not always hold in the real-world 
scenario, and being able to forecast the propagation 
of novel or unseen topics is more valuable in 
practice. For example, a company would like to 
know which users are more likely to be the source 
of ?viva voce? of a newly released product for 
advertising purpose. A political party might want 
to estimate the potential degree of responses of a 
half-baked policy before deciding to bring it up to 
public. To achieve such goal, it is required to 
predict the future propagation behavior of a topic 
even before any actual diffusion happens on this 
topic (i.e., no historical propagation data of this 
topic are available). Lin et al also propose an idea 
aiming at predicting the inference of implicit 
diffusions for novel topics (Lin et al, 2011). The 
main difference between their work and ours is that 
they focus on implicit diffusions, whose data are 
usually not available. Consequently, they need to 
rely on a model-driven approach instead of a data-
driven approach. On the other hand, our work 
focuses on the prediction of explicit diffusion 
behaviors. Despite the fact that no diffusion data of 
novel topics is available, we can still design a data-
driven approach taking advantage of some explicit 
diffusion data of known topics. Our experiments 
show that being able to utilize such information is 
critical for diffusion prediction. 
2 The Novel-Topic Diffusion Model 
We start by assuming an existing social network G 
= (V, E), where V is the set of nodes (or user) v, 
and E is the set of link e. The set of topics is 
344
denoted as T. Among them, some are considered as 
novel topics (denoted as N), while the rest (R) are 
used as the training records.  We are also given a 
set of diffusion records D = {d | d = (src, dest, t)}, 
where src is the source node (or diffusion source), 
dest is the destination node, and t is the topic of the 
diffusion that belongs to R but not N. We assume 
that diffusions cannot occur between nodes without 
direct social connection; any diffusion pair implies 
the existence of a link e = (src, dest ?)  E. Finally, 
we assume there are sets of keywords or tags that 
relevant to each topic (including existing and novel 
topics). Note that the set of keywords for novel 
topics should be seen in that of existing topics. 
From these sets of keywords, we construct a topic-
word matrix TW = (P(wordj | topici))i,j of which the 
elements stand for the conditional probabilities that 
a word appears in the text of a certain topic. 
Similarly, we also construct a user-word matrix 
UW= (P(wordj | useri))i,j from these sets of 
keywords. Given the above information, the goal is 
to predict whether a given link is active (i.e., 
belongs to a diffusion link) for topics in N. 
2.1 The Framework 
The main challenge of this problem lays in that the 
past diffusion behaviors of new topics are missing. 
To address this challenge, we propose a supervised 
diffusion discovery framework that exploits the 
latent semantic information among users, topics, 
and their explicit / implicit interactions. Intuitively, 
four kinds of information are useful for prediction: 
? Topic information: Intuitively, knowing the 
signatures of a topic (e.g., is it about politics?) 
is critical to the success of the prediction. 
? User information: The information of a user 
such as the personality (e.g., whether this user 
is aggressive or passive) is generally useful. 
? User-topic interaction: Understanding the users' 
preference on certain topics can improve the 
quality of prediction. 
? Global information: We include some global 
features (e.g., topology info) of social network. 
Below we will describe how these four kinds of 
information can be modeled in our framework. 
2.2 Topic Information 
We extract hidden topic category information to 
model topic signature. In particular, we exploit the 
Latent Dirichlet Allocation (LDA) method (Blei et 
al., 2003), which is a widely used topic modeling 
technique, to decompose the topic-word matrix TW 
into hidden topic categories:  
                        TW = TH * HW 
, where TH is a topic-hidden matrix, HW is hidden-
word matrix, and h is the manually-chosen 
parameter to determine the size of hidden topic 
categories. TH indicates the distribution of each 
topic to hidden topic categories, and HW indicates 
the distribution of each lexical term to hidden topic 
categories. Note that TW and TH include both 
existing and novel topics.  We utilize THt,*, the row 
vector of the topic-hidden matrix TH for a topic t, 
as a feature set. In brief, we apply LDA to extract 
the topic-hidden vector THt,* to model topic 
signature (TG) for both existing and novel topics. 
Topic information can be further exploited. To 
predict whether a novel topic will be propagated 
through a link, we can first enumerate the existing 
topics that have been propagated through this link. 
For each such topic, we can calculate its similarity 
with the new topic based on the hidden vectors 
generated above (e.g., using cosine similarity 
between feature vectors). Then, we sum up the 
similarity values as a new feature: topic similarity 
(TS). For example, a link has previously 
propagated two topics for a total of three times 
{ACL, KDD, ACL}, and we would like to know 
whether a new topic, EMNLP, will propagate 
through this link. We can use the topic-hidden 
vector to generate the similarity values between 
EMNLP and the other topics (e.g., {0.6, 0.4, 0.6}), 
and then sum them up (1.6) as the value of TS. 
2.3 User Information 
Similar to topic information, we extract latent 
personal information to model user signature (the 
users are anonymized already). We apply LDA on 
the user-word matrix UW: 
UW = UM * MW 
, where UM is the user-hidden matrix, MW is the 
hidden-word matrix, and m is the manually-chosen 
size of hidden user categories. UM indicates the 
distribution of each user to the hidden user 
categories (e.g., age). We then use UMu,*, the row 
vector of UM for the user u, as a feature set. In 
brief, we apply LDA to extract the user-hidden 
vector UMu,* for both source and destination nodes 
of a link to model user signature (UG). 
345
2.4 User-Topic Interaction 
Modeling user-topic interaction turns out to be 
non-trivial. It is not useful to exploit latent 
semantic analysis directly on the user-topic matrix 
UR = UQ * QR , where UR represents how many 
times each user is diffused for existing topic R (R 
?
 T), because UR does not contain information of 
novel topics, and neither do UQ and QR. Given no 
propagation record about novel topics, we propose 
a method that allows us to still extract implicit 
user-topic information. First, we extract from the 
matrix TH (described in Section 2.2) a subset RH 
that contains only information about existing topics. 
Next we apply left division to derive another user-
hidden matrix UH: 
UH = (RH \ URT)T = ((RHT RH
 
)-1 RHT URT)T 
Using left division, we generate the UH matrix 
using existing topic information. Finally, we 
exploit UHu,*, the row vector of the user-hidden 
matrix UH for the user u, as a feature set. 
Note that novel topics were included in the 
process of learning the hidden topic categories on 
RH; therefore the features learned here do 
implicitly utilize some latent information of novel 
topics, which is not the case for UM. Experiments 
confirm the superiority of our approach. 
Furthermore, our approach ensures that the hidden 
categories in topic-hidden and user-hidden 
matrices are identical. Intuitively, our method 
directly models the user?s preference to topics? 
signature (e.g., how capable is this user to 
propagate topics in politics category?). In contrast, 
the UM mentioned in Section 2.3 represents the 
users? signature (e.g., aggressiveness) and has 
nothing to do with their opinions on a topic. In 
short, we obtain the user-hidden probability vector 
UHu,* as a feature set, which models user 
preferences to latent categories (UPLC). 
2.5 Global Features 
Given a candidate link, we can extract global 
social features such as in-degree (ID) and out-
degree (OD). We tried other features such as 
PageRank values but found them not useful. 
Moreover, we extract the number of distinct topics 
(NDT) for a link as a feature. The intuition behind 
this is that the more distinct topics a user has 
diffused to another, the more likely the diffusion 
will happen for novel topics. 
2.6 Complexity Analysis 
The complexity to produce each feature is as below: 
(1) Topic information: O(I * |T| * h * Bt) for LDA 
using Gibbs sampling, where I is # of the 
iterations in sampling, |T| is # of topics, and Bt 
is the average # of tokens in a topic. 
(2) User information: O(I * |V| * m * Bu) , where |V| is # of users, and Bu is the average # of 
tokens for a user. 
(3) User-topic interaction: the time complexity is 
O(h3 + h2 * |T| + h * |T| * |V|). 
(4) Global features: O(|D|), where |D| is # of 
diffusions. 
3 Experiments 
For evaluation, we try to use the diffusion records 
of old topics to predict whether a diffusion link 
exists between two nodes given a new topic.  
3.1 Dataset and Evaluation Metric 
We first identify 100 most popular topic (e.g., 
earthquake) from the Plurk micro-blog site 
between 01/2011 and 05/2011. Plurk is a popular 
micro-blog service in Asia with more than 5 
million users (Kuo et al, 2011). We manually 
separate the 100 topics into 7 groups. We use 
topic-wise 4-fold cross validation to evaluate our 
method, because there are only 100 available 
topics. For each group, we select 3/4 of the topics 
as training and 1/4 as validation. 
The positive diffusion records are generated 
based on the post-response behavior. That is, if a 
person x posts a message containing one of the 
selected topic t, and later there is a person y 
responding to this message, we consider a 
diffusion of t has occurred from x to y (i.e., (x, y, t) 
is a positive instance). Our dataset contains a total 
of 1,642,894 positive instances out of 100 distinct 
topics; the largest and smallest topic contains 
303,424 and 2,166 diffusions, respectively. Also, 
the same amount of negative instances for each 
topic (totally 1,642,894) is sampled for binary 
classification (similar to the setup in KDD Cup 
2011 Track 2). The negative links of a topic t are 
sampled randomly based on the absence of 
responses for that given topic. 
The underlying social network is created using 
the post-response behavior as well. We assume 
there is an acquaintance link between x and y if and 
346
only if x has responded to y (or vice versa) on at 
least one topic. Eventually we generated a social 
network of 163,034 nodes and 382,878 links. 
Furthermore, the sets of keywords for each topic 
are required to create the TW and UW matrices for 
latent topic analysis; we simply extract the content 
of posts and responses for each topic to create both 
matrices. We set the hidden category number h = m 
= 7, which is equal to the number of topic groups. 
We use area under ROC curve (AUC) to 
evaluate our proposed framework (Davis and 
Goadrich, 2006); we rank the testing instances 
based on their likelihood of being positive, and 
compare it with the ground truth to compute AUC. 
3.2 Implementation and Baseline 
After trying many classifiers and obtaining similar 
results for all of them, we report only results from 
LIBLINEAR with c=0.0001 (Fan et al, 2008) due 
to space limitation. We remove stop-words, use 
SCWS (Hightman, 2012) for tokenization, and  
MALLET (McCallum, 2002) and GibbsLDA++ 
(Phan and Nguyen, 2007) for LDA. 
There are three baseline models we compare the 
result with. First, we simply use the total number 
of existing diffusions among all topics between 
two nodes as the single feature for prediction. 
Second, we exploit the independent cascading 
model (Kempe et al, 2003), and utilize the 
normalized total number of diffusions as the 
propagation probability of each link. Third, we try 
the heat diffusion model (Ma et al, 2008), set 
initial heat proportional to out-degree, and tune the 
diffusion time parameter until the best results are 
obtained. Note that we did not compare with any 
data-driven approaches, as we have not identified 
one that can predict diffusion of novel topics.  
3.3 Results 
The result of each model is shown in Table 1. All 
except two features outperform the baseline. The 
best single feature is TS. Note that UPLC performs 
better than UG, which verifies our hypothesis that 
maintaining the same hidden features across 
different LDA models is better. We further conduct 
experiments to evaluate different combinations of 
features (Table 2), and found that the best one (TS 
+ ID + NDT) results in about 16% improvement 
over the baseline, and outperforms the combination 
of all features. As stated in (Witten et al, 2011), 
adding useless features may cause the performance 
of classifiers to deteriorate. Intuitively, TS captures 
both latent topic and historical diffusion 
information, while ID and NDT provide 
complementary social characteristics of users. 
 
Table 1: Single-feature results. 
 
Table 2: Feature combination results. 
4 Conclusions 
The main contributions of this paper are as below: 
1. We propose a novel task of predicting the 
diffusion of unseen topics, which has wide 
applications in real-world.  
2. Compared to the traditional model-driven or 
content-independent data-driven works on 
diffusion analysis, our solution demonstrates 
how one can bring together ideas from two 
different but promising areas, NLP and SNA, 
to solve a challenging problem. 
3. Promising experiment result (74% in AUC) 
not only demonstrates the usefulness of the 
proposed models, but also indicates that 
predicting diffusion of unseen topics without 
historical diffusion data is feasible. 
Acknowledgments 
This work was also supported by National Science 
Council, National Taiwan University and Intel 
Corporation under Grants NSC 100-2911-I-002-001, 
and 101R7501. 
Method Feature AUC
Baseline
Existing Diffusion 58.25%
Independent Cascade 51.53%
Heat Diffusion 56.08%
Learning
Topic Signature (TG) 50.80%
Topic Similarity (TS) 69.93%
User Signature (UG) 56.59%
User Preferences to
Latent Categories (UPLC) 61.33%
In-degree (ID) 65.55%
Out-degree (OD) 59.73%
Number of Distinct Topics (NDT) 55.42%
Method Feature AUC
Baseline Existing Diffusion 58.25%
Learning
ALL 65.06%
TS + UPLC + ID + NDT 67.67%
TS + UPLC + ID 64.80%
TS + UPLC + NDT 66.01%
TS + ID + NDT 73.95%
UPLC + ID + NDT 67.24%
347
References  
David M. Blei, Andrew Y. Ng & Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3.993-1022. 
Jesse Davis & Mark Goadrich. 2006. The relationship 
between Precision-Recall and ROC curves. 
Proceedings of the 23rd international conference on 
Machine learning, Pittsburgh, Pennsylvania. 
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang & Chih-Jen Lin. 2008. LIBLINEAR: A 
Library for Large Linear Classification. J. Mach. 
Learn. Res., 9.1871-74. 
Hongliang Fei, Ruoyi Jiang, Yuhao Yang, Bo Luo & 
Jun Huan. 2011. Content based social behavior 
prediction: a multi-task learning approach. 
Proceedings of the 20th ACM international 
conference on Information and knowledge 
management, Glasgow, Scotland, UK. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic & Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. Proceedings of the 3rd 
conference on Online social networks, Boston, MA. 
Hightman. 2012. Simple Chinese Words Segmentation 
(SCWS). 
David Kempe, Jon Kleinberg & Eva Tardos. 2003. 
Maximizing the spread of influence through a social 
network. Proceedings of the ninth ACM SIGKDD 
international conference on Knowledge discovery 
and data mining, Washington, D.C. 
Tsung-Ting Kuo, San-Chuan Hung, Wei-Shih Lin, 
Shou-De Lin, Ting-Chun Peng & Chia-Chun Shih. 
2011. Assessing the Quality of Diffusion Models 
Using Real-World Social Network Data. Conference 
on Technologies and Applications of Artificial 
Intelligence, 2011. 
C.X. Lin, Q.Z. Mei, Y.L. Jiang, J.W. Han & S.X. Qi. 
2011. Inferring the Diffusion and Evolution of 
Topics in Social Communities. Proceedings of the 
IEEE International Conference on Data Mining, 
2011. 
Hao Ma, Haixuan Yang, Michael R. Lyu & Irwin King. 
2008. Mining social networks using heat diffusion 
processes for marketing candidates selection. 
Proceeding of the 17th ACM conference on 
Information and knowledge management, Napa 
Valley, California, USA. 
Andrew Kachites McCallum. 2002. MALLET: A 
Machine Learning for Language Toolkit. 
Sasa Petrovic, Miles Osborne & Victor Lavrenko. 2011. 
RT to Win! Predicting Message Propagation in 
Twitter. International AAAI Conference on Weblogs 
and Social Media, 2011. 
Xuan-Hieu Phan & Cam-Tu Nguyen. 2007. 
GibbsLDA++: A C/C++ implementation of latent 
Dirichlet alocation (LDA). 
Ian H. Witten, Eibe Frank & Mark A. Hall. 2011. Data 
Mining: Practical machine learning tools and 
techniques. San Francisco: Morgan Kaufmann 
Publishers Inc. 
Jiang Zhu, Fei Xiong, Dongzhen Piao, Yun Liu & Ying 
Zhang. 2011. Statistically Modeling the 
Effectiveness of Disaster Information in Social 
Media. Proceedings of the 2011 IEEE Global 
Humanitarian Technology Conference. 
 
 
348
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611?617,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Enriching Cold Start Personalized Language Model 
Using Social Network Information 
  Yu-Yang Huang
?
, Rui Yan*, Tsung-Ting Kuo
?
, Shou-De Lin
??
 
?
Graduate Institute of Computer Science and Information Engineering,  
National Taiwan University, Taipei, Taiwan 
?
Graduate Institute of Network and Multimedia,  
National Taiwan University, Taipei, Taiwan 
*Computer and Information Science Department,  
University of Pennsylvania, Philadelphia, PA 19104, U.S.A. 
{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.edu 
 
Abstract 
We introduce a generalized framework to enrich 
the personalized language models for cold start 
users. The cold start problem is solved with 
content written by friends on social network 
services. Our framework consists of a mixture 
language model, whose mixture weights are es-
timated with a factor graph. The factor graph is 
used to incorporate prior knowledge and heuris-
tics to identify the most appropriate weights. 
The intrinsic and extrinsic experiments show 
significant improvement on cold start users. 
1 Introduction 
Personalized language models (PLM) on social 
network services are useful in many aspects (Xue 
et al, 2009; Wen et al, 2012; Clements, 2007), 
For instance, if the authorship of a document is 
in doubt, a PLM may be used as a generative 
model to identify it. In this sense, a PLM serves 
as a proxy of one?s writing style. Furthermore, 
PLMs can improve the quality of information 
retrieval and content-based recommendation sys-
tems, where documents or topics can be recom-
mended based on the generative probabilities. 
However, it is challenging to build a PLM for 
users who just entered the system, and whose 
content is thus insufficient to characterize them. 
These are called ?cold start? users. Producing 
better recommendations is even more critical for 
cold start users to make them continue to use the 
system. Therefore, this paper focuses on how to 
overcome the cold start problem and obtain a 
better PLM for cold start users. 
The content written by friends on a social 
network service, such as Facebook or Twitter, is 
exploited. It can be either a reply to an original 
post or posts by friends. Here the hypothesis is 
that friends, who usually share common interests, 
tend to discuss similar topics and use similar 
words than non-friends. In other words, we be-
lieve that a cold start user?s language model can 
be enriched and better personalized by incorpo-
rating content written by friends. 
Intuitively, a linear combination of document-
level language models can be used to incorporate 
content written by friends. However, it should be 
noticed that some documents are more relevant 
than others, and should be weighted higher. To 
obtain better weights, some simple heuristics 
could be exploited. For example, we can measure 
the similarity or distance between a user lan-
guage model and a document language model. In 
addition, documents that are shared frequently in 
a social network are usually considered to be 
more influential, and could contribute more to 
the language model. More complex heuristics 
can also be derived. For instance, if two docu-
ments are posted by the same person, their 
weights should be more similar. The main chal-
lenge lies in how such heuristics can be utilized 
in a systematic manner to infer the weights of 
each document-level language model. 
In this paper, we exploit the information on 
social network services in two ways. First, we 
impose the social dependency assumption via a 
finite mixture model. We model the true, albeit 
unknown, personalized language model as a 
combination of a biased user language model and 
a set of relevant document language models. Due 
to the noise inevitably contained in social media 
content, instead of using all available documents, 
we argue that by properly specifying the set of 
relevant documents, a better personalized lan-
guage model can be learnt. In other words, each 
user language model is enriched by a personal-
ized collection of background documents. 
Second, we propose a factor graph model 
(FGM) to incorporate prior knowledge (e.g. the 
heuristics described above) into our model. Each 
611
mixture weight is represented by a random vari-
able in the factor graph, and an efficient algo-
rithm is proposed to optimize the model and infer 
the marginal distribution of these variables. Use-
ful information about these variables is encoded 
by a set of potential functions. 
The main contributions of this work are sum-
marized below: 
? To solve the cold start problem encountered 
when estimating PLMs, a generalized frame-
work based on FGM is proposed. We incorpo-
rate social network information into user lan-
guage models through the use of FGM. An it-
erative optimization procedure utilizing per-
plexity is presented to learn the parameters. 
To our knowledge, this is the first proposal to 
use FGM to enrich language models. 
? Perplexity is selected as an intrinsic evalua-
tion, and experiment on authorship attribution 
is used as an extrinsic evaluation. The results 
show that our model yields significant im-
provements for cold start users. 
2 Methodology 
2.1 Social-Driven Personalized Language 
Model 
The language model of a collection of documents 
can be estimated by normalizing the counts of 
words in the entire collection (Zhai, 2008). To 
build a user language model, one na?ve way is to 
first normalize word frequency ?(?, ?)  within 
each document, and then average over all the 
documents in a user?s document collection. The 
resulting unigram user language model is: 
??(?) =
1
|??|
?
?(?, ?)
|?|????
 
=
1
|??|
? ??(?)
????
 
(1) 
where ??(?) is the language model of a particu-
lar document, and ?? is the user?s document col-
lection. This formulation is basically an equal-
weighted finite mixture model. 
A simple yet effective way to smooth a lan-
guage model is to linearly interpolate with a 
background language model (Chen and Good-
man, 1996; Zhai and Lafferty, 2001). In the line-
ar interpolation method, all background docu-
ments are treated equally. The entire document 
collection is added to the user language model 
??(?) with the same interpolation coefficient. 
Our main idea is to specify a set of relevant 
documents for the target user using information 
embedded in a social network, and enrich the 
smoothing procedure with these documents. Let 
????  denote the content from relevant persons 
(e.g. social neighbors) of u1, our idea can be con-
cisely expressed as: 
??1
? (?) = ??1??1(?) + ? ??????(?)
???????
 (2) 
where ??? is the mixture weight of the language 
model of document di, and ??1 + ???? = 1 . 
Documents posted by irrelevant users are not 
included as we believe the user language model 
can be better personalized by exploiting the so-
cial relationship in a more structured way. In our 
experiment, we choose the first degree neighbor 
documents as ????. 
Also note that we have made no assumption 
about how the ?base? user language model 
??1(?) is built. In practice, it need not be models 
following maximum likelihood estimation, but 
any language model can be integrated into our 
framework to achieve a better refined model. 
Furthermore, any smoothing method can be ap-
plied to the language model without degrading 
the effectiveness. 
2.2 Factor Graph Model (FGM) 
Now we discuss how the mixture weights can be 
estimated. We introduce a factor graph model 
(FGM) to make use of the diverse information on 
a social network. Factor graph (Kschischang et 
al., 2006) is a bipartite graph consisting of a set 
of random variables and a set of factors which 
signifies the relationships among the variables. It 
is best suited in situations where the data is clear-
ly of a relational nature (Wang et al, 2012). The 
joint distribution of the variables is factored ac-
cording to the graph structure. Using FGM, one 
can incorporate the knowledge into the potential 
function for optimization and perform joint in-
ference over documents. As shown in Figure 1, 
the variables included in the model are described 
as follows: 
Candidate variables ?? = ??, ??? . The ran-
dom variables in the top layer stand for the de-
grees of belief that a document di should be in-
cluded in the PLM of the target user ?. 
Figure 1: A two-layered factor graph (FGM) 
proposed to estimate the mixture weights. 
612
Attribute variables xi. Local information is 
stored as the random variables in the bottom lay-
er. For example, x1 might represent the number 
of common friends between the author of a doc-
ument di and our target user. 
The potential functions in the FGM are: 
Attribute-to-candidate function. This poten-
tial function captures the local dependencies of a 
candidate variable to the relevant attributes. Let 
the candidate variable yi correspond to a docu-
ment di, the attribute-to-candidate function of yi 
is defined in a log-linear form: 
?(?? , ?) =
1
??
???{???(??, ?)} (3) 
where A is the set of attributes of either the doc-
ument di or target user u; f is a vector of feature 
functions which locally model the value of yi 
with attributes in A; ??  is the local partition 
function and ? is the weight vector to be learnt. 
In our experiment, we define the vector of 
functions as ? = ?????, ???? , ????, ????, ????
? as: 
? Similarity function ???? . The similarity be-
tween language models of the target user and 
a document should play an important role. We 
use cosine similarity between two unigram 
models in our experiments. 
? Document quality function ????. The out-of-
vocabulary (OOV) ratio is used to measure the 
quality of a document. It is defined as 
???? = 1 ?
|{?:? ? ?? ? ? ? ?}|
|??|
 (4) 
where ?  is the vocabulary set of the entire 
corpus, with stop words excluded. 
? Document popularity function ???? . This 
function is defined as the number of times di is 
shared to model the popularity of documents. 
? Common friend function ????. It is defined 
as the number of common friends between the 
target user u1 and the author of di. 
? Author friendship function ??? . Assuming 
that documents posted by a user with more 
friends are more influential, this function is 
defined as the number of friends of di?s author. 
Candidate-to-candidate function. This po-
tential function defines the correlation of a can-
didate variable yi with another candidate variable 
yj in the factor graph. The function is defined as 
?(?? , ??) =
1
???,?
???{???(??, ??)} (5) 
where g is a vector of feature functions indicat-
ing whether two variables are correlated. If we 
further denote the set of all related variables as 
?(??) , then for any candidate variable yi, we 
have the following brief expression: 
?(?? , ?(??)) = ? ?(?? , ??)
????(??)
 (6) 
For two candidate variables, let the corre-
sponding document be di and dj, respectively, we 
define the vector ? = ????? , ?????
? as: 
? User relationship function ????. We assume 
that two candidate variables have higher de-
pendency if they represent documents of the 
same author or the two authors are friends. 
The dependency should be even greater if two 
documents are similar. Let ?(?)  denote the 
author of a document d and ?[?] denote the 
closed neighborhood of a user u, we define 
???? = ?{?(??) ? ?[?(??)]} ? ???(?? , ??) (7) 
? Co-category function ????. For any two can-
didate variables, it is intuitive that the two var-
iables would have a higher correlation if di 
and dj are of the same category. Let ?(?) de-
note the category of document d, we define 
???? = ?{?(??) = ?(??)} ? ???(?? , ??) (8) 
2.3 Model Inference and Optimization 
Let Y and X be the set of all candidate variables 
and attribute variables, respectively. The joint 
distribution encoded by the FGM is given by 
multiplying all potential functions. 
?(?, ?) =??(??, ?)?(?? , ?(??))
?
 (9) 
The desired marginal distribution can be ob-
tained by marginalizing all other variables. Since 
under most circumstances, however, the factor 
graph is densely connected, the exact inference is 
intractable and approximate inference is required. 
After obtaining the marginal probabilities, the 
mixture weights ???  in Eq. 2 are estimated by 
normalizing the corresponding marginal proba-
bilities ?(??) over all candidate variables, which 
can be written as 
??? = (1 ? ??1)
?(??)
? ?(??)?:???????
 (10) 
where the constraint ??1 + ???? = 1 leads to a 
valid probability distribution for our mixture 
model. 
A factor graph is normally optimized by gra-
dient-based methods. Unfortunately, since the 
ground truth values of the mixture weights are 
not available, we are prohibited from using su-
pervised approaches. Here we propose a two-step 
iterative procedure to optimize our model. At 
613
first, all the model parameters (i.e. ?, ?, ??) are 
randomly initialized. Then, we infer the marginal 
probabilities of candidate variables. Given these 
marginal probabilities, we can evaluate the per-
plexity of the user language model on a held-out 
dataset, and search for better parameters. This 
procedure is repeated until convergence. Also, 
notice that by using FGM, we reduce the number 
of parameters from 1 + |????| to 1 + |?| + |?|, 
lowering the risk of overfitting. 
3 Experiments 
3.1 Dataset and Experiment Setup 
We perform experiments on the Twitter dataset 
collected by Galuba et al (2010). Twitter data 
have been used to verify models with different 
purposes (Lin et al, 2011; Tan et al, 2011). To 
emphasize on the cold start scenario, we random-
ly selected 15 users with about 35 tweets and 70 
friends as candidates for an authorship attribution 
task. Our experiment corpus consists of 4322 
tweets. All words with less than 5 occurrences 
are removed. Stop words and URLs are also re-
moved and all tweets are stemmed. We identify 
the 100 most frequent terms as categories. The 
size of the vocabulary set is 1377. 
We randomly partitioned the tweets of each 
user into training, validation and testing sets. The 
reported result is the average of 10 random splits. 
In all experiments, we vary the size of training 
data from 1% to 15%, and hold out the same 
number of tweets from each user as validation 
and testing data. The statistics of our dataset, 
given 15% training data, are shown in Table 1. 
 Loopy belief propagation (LBP) is used to ob-
tain the marginal probabilities of the variables 
(Murphy et al, 1999). Parameters are searched 
with the pattern search algorithm (Audet and 
Dennis, 2002). To not lose generality, we use the 
default configuration in all experiments. 
# of Max. Min. Avg. 
Tweets 70 19 35.4 
Friends 139 24 68.9 
Variables 467 97 252.7 
Edges 9216 231 3427.1 
Table 1: Dataset statistics 
3.2 Baseline Methods 
We compare our framework with two baseline 
methods. The first (?Cosine?) is a straightfor-
ward implementation that sets all mixture 
weights ??? to the cosine similarity between the 
probability mass vectors of the document and 
user unigram language models. The second 
(?PS?) uses the pattern search algorithm to per-
form constrained optimization over the mixture 
weights. As mentioned in section 2.3, the main 
difference between this method and ours 
(?FGM?) is that we reduce the search space of 
the parameters by FGM. Furthermore, social 
network information is exploited in our frame-
work, while the PS method performs a direct 
search over mixture weights, discarding valuable 
knowledge. 
Different from other smoothing methods that 
are usually mutually exclusive, any other 
smoothing methods can be easily merged into 
our framework. In Eq. 2, the base language 
model ??1(?) can be already smoothed by any 
techniques before being plugged into our frame-
work. Our framework then enriches the user lan-
guage model with social network information. 
We select four popular smoothing methods to 
demonstrate such effect, namely additive 
smoothing, absolute smoothing (Ney et al, 1995), 
Jelinek-Mercer smoothing (Jelinek and Mercer, 
1980) and Dirichlet smoothing (MacKay and 
Peto, 1994). The results of using only the base 
model (i.e. set ??? = 0 in Eq. 2) are denoted as 
?Base? in the following tables. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 
5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 
10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 
15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 
5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** 
10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0** 
15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4 
Table 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet-
ter than the next highest score, by t-test at a significance level of 0.05. 
614
3.3 Perplexity 
As an intrinsic evaluation, we first compute the 
perplexity of unseen sentences under each user 
language model. The result is shown in Table 2. 
Our method significantly outperforms all of 
the methods in almost all settings. We observe 
that the ?PS? method takes a long time to con-
verge and is prone to overfitting, likely because 
it has to search about a few hundred parameters 
on average. As expected, the advantage of our 
model is more apparent when the data is sparse. 
3.4 Authorship Attribution (AA) 
The authorship attribution (AA) task is chosen as 
the extrinsic evaluation metric. Here the goal is 
not about comparing with the state-of-the-art ap-
proaches in AA, but showing that LM-based ap-
proaches can benefit from our framework. 
To apply PLM on this task, a na?ve Bayes 
classifier is implemented (Peng et al, 2004). The 
most probable author of a document d is the one 
whose PLM yields the highest probability, and is 
determined by ?? = argmax?{? ??(?)??? }. 
The result is shown in Table 3. Our model im-
proves personalization and outperforms the base-
lines under cold start settings. When data is 
sparse, the ?PS? method tends to overfit the 
noise, while the ?Cosine? method contains too 
few information and is severely biased. Our 
method strikes a balance between model com-
plexity and the amount of information included, 
and hence performs better than the others. 
4 Related Work 
Personalization has long been studied in various 
textual related tasks. Personalized search is es-
tablished by modeling user behavior when using 
search engines (Shen et al, 2005; Xue et al, 
2009). Query language model could be also ex-
panded based on personalized user modeling 
(Chirita et al, 2007). Personalization has also 
been modeled in many NLP tasks such as sum-
marization (Yan et al, 2011) and recommenda-
tion (Yan et al, 2012). Different from our pur-
pose, these models do not aim at exploiting so-
cial media content to enrich a language model. 
Wen et al (2012) combines user-level language 
models from a social network, but instead of fo-
cusing on the cold start problem, they try to im-
prove the speech recognition performance using 
a mass amount of texts on social network. On the 
other hand, our work explicitly models the more 
sophisticated document-level relationships using 
a probabilistic graphical model. 
5 Conclusion 
The advantage of our model is threefold. First, 
prior knowledge and heuristics about the social 
network can be adapted in a structured way 
through the use of FGM. Second, by exploiting a 
well-studied graphical model, mature inference 
techniques, such as LBP, can be applied in the 
optimization procedure, making it much more 
effective and efficient. Finally, different from 
most smoothing methods that are mutually ex-
clusive, any other smoothing method can be in-
corporated into our framework to be further en-
hanced. Using only 1% of the training corpus, 
our model can improve the perplexity of base 
models by as much as 40% and the accuracy of 
authorship attribution by at most 15%. 
6 Acknowledgement 
This work was sponsored by AOARD grant 
number No. FA2386-13-1-4045 and National 
Science Council, National Taiwan University 
and Intel Corporation under Grants NSC102-
2911-I-002-001 and NTU103R7501 and grant 
102-2923-E-002-007-MY2, 102-2221-E-002-170, 
101-2628-E-002-028-MY2. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27** 
5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53** 
10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87** 
15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67** 
5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93 
10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53 
15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40 
Table 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is 
significantly better than the next highest score, by t-test at a significance level of 0.05. 
615
Reference 
Charles Audet and J. E. Dennis, Jr. 2002. Analysis of 
generalized pattern searches. SIAM J. on Optimiza-
tion, 13(3):889?903, August. 
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language 
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics, 
ACL ?96, pages 310?318, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-
gang Nejdl. 2007. Personalized query expansion 
for the web. In Proceedings of the 30th Annual In-
ternational ACM SIGIR Conference on Research 
and Development in Information Retrieval, 
SIGIR ?07, pages 7?14, New York, NY, USA. 
ACM. 
Maarten Clements. 2007. Personalization of social 
media. In Proceedings of the 1st BCS IRSG Con-
ference on Future Directions in Information Access, 
FDIA?07, pages 14?14, Swinton, UK, UK. British 
Computer Society. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic, and Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. In Proceedings of the 3rd 
Conference on Online Social Networks, WOSN?10, 
pages 3?3, Berkeley, CA, USA. USENIX Associa-
tion. 
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters 
from sparse data. In In Proceedings of the Work-
shop on Pattern Recognition in Practice, pages 
381?397, Amsterdam, The Netherlands: North-
Holland, May. 
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 
2006. Factor graphs and the sum-product algorithm. 
IEEE Trans. Inf. Theor., 47(2):498?519, Septem-
ber. 
Jimmy Lin, Rion Snow, and William Morgan. 2011. 
Smoothing techniques for adaptive online language 
models: Topic tracking in tweet streams. In Pro-
ceedings of the 17th ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining, KDD ?11, pages 422?429, New York, NY, 
USA. ACM. 
David J.C. MacKay and Linda C. Bauman Peto. 1994. 
A hierarchical dirichlet language model. Natural 
Language Engineering, 1:1?19. 
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the 
Fifteenth Conference on Uncertainty in Artificial 
Intelligence, UAI?99, pages 467?475, San Francis-
co, CA, USA. Morgan Kaufmann Publishers Inc. 
Hermann Ney, Ute Essen, and Reinhard Kneser. 1995. 
On the estimation of ?small? probabilities by leav-
ing-one-out. IEEE Trans. Pattern Anal. Mach. In-
tell., 17(12):1202?1212, December. 
Fuchun Peng, Dale Schuurmans, and Shaojun Wang. 
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Inf. Retr., 7(3-4):317?345, 
September. 
Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. 
Implicit user modeling for personalized search. In 
Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Manage-
ment, CIKM ?05, pages 824?831, New York, NY, 
USA. ACM. 
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, 
Ming Zhou, and Ping Li. 2011. User-level senti-
ment analysis incorporating social networks. In 
Proceedings of the 17th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and 
Data Mining, KDD ?11, pages 1397?1405, New 
York, NY, USA. ACM. 
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie 
Tang. 2012. Cross-lingual knowledge linking 
across wiki knowledge bases. In Proceedings of the 
21st International Conference on World Wide Web, 
WWW ?12, pages 459?468, New York, NY, USA. 
ACM. 
Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and 
Lin-Shan Lee. 2012. Personalized language model-
ing by crowd sourcing with social network data for 
voice access of cloud applications. In Spoken Lan-
guage Technology Workshop (SLT), 2012 IEEE, 
pages 188?193. 
Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 
2009. User language model for collaborative per-
sonalized search. ACM Trans. Inf. Syst., 
27(2):11:1?11:28, March. 
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. 
Summarize what you are interested in: An optimi-
zation framework for interactive personalized 
summarization. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1342?1351, Strouds-
burg, PA, USA. Association for Computational 
Linguistics. 
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. 
Tweet recommendation with graph co-ranking. In 
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long Pa-
pers - Volume 1, ACL ?12, pages 516?525, 
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics. 
ChengXiang Zhai. 2008. Statistical Language Models 
for Information Retrieval. Now Publishers Inc., 
Hanover, MA, USA. 
616
Chengxiang Zhai and John Lafferty. 2001. A study of 
smoothing methods for language models applied to 
ad hoc information retrieval. In Proceedings of the 
24th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval, SIGIR ?01, pages 334?342, New York, NY, 
USA. ACM. 
 
617
