Proceedings of NAACL HLT 2007, pages 548?555,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Can Semantic Roles Generalize Across Genres?
Szu-ting Yi
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Edward Loper
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Martha Palmer
Dept of Computer Science
University of Colorado at Boulder
Boulder, CO 80309
Abstract
PropBank has been widely used as train-
ing data for Semantic Role Labeling.
However, because this training data is
taken from the WSJ, the resulting machine
learning models tend to overfit on idiosyn-
crasies of that text?s style, and do not port
well to other genres. In addition, since
PropBank was designed on a verb-by-verb
basis, the argument labels Arg2 - Arg5 get
used for very diverse argument roles with
inconsistent training instances. For exam-
ple, the verb ?make? uses Arg2 for the
?Material? argument; but the verb ?multi-
ply? uses Arg2 for the ?Extent? argument.
As a result, it can be difficult for auto-
matic classifiers to learn to distinguish ar-
guments Arg2-Arg5. We have created a
mapping between PropBank and VerbNet
that provides a VerbNet thematic role la-
bel for each verb-specific PropBank label.
Since VerbNet uses argument labels that
are more consistent across verbs, we are
able to demonstrate that these new labels
are easier to learn.
1 Introduction
Correctly identifying semantic entities and success-
fully disambiguating the relations between them and
their predicates is an important and necessary step
for successful natural language processing applica-
tions, such as text summarization, question answer-
ing, and machine translation. For example, in or-
der to determine that question (1a) is answered by
sentence (1b), but not by sentence (1c), we must de-
termine the relationships between the relevant verbs
(eat and feed) and their arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
An important part of this task is Semantic Role
Labeling (SRL), where the goal is to locate the con-
stituents which are arguments of a given verb, and to
assign them appropriate semantic roles that describe
how they relate to the verb. Many researchers have
investigated applying machine learning to corpus
specifically annotated with this task in mind, Prop-
Bank, since 2000 (Chen and Rambow, 2003; Gildea
and Hockenmaier, 2003; Hacioglu et al, 2003; Mos-
chitti, 2004; Yi and Palmer, 2004; Pradhan et al,
2005b; Punyakanok et al, 2005; Toutanova et al,
2005). For two years, the CoNLL workshop has
made this problem the shared task (Carreras and
Ma?rquez, 2005). However, there is still little con-
sensus in the linguistic and NLP communities about
what set of role labels are most appropriate. The
Proposition Bank (PropBank) corpus (Palmer et al,
2005) avoids this issue by using theory-agnostic la-
bels (Arg0, Arg1, . . . , Arg5), and by defining those
labels to have verb-specific meanings. Under this
scheme, PropBank can avoid making any claims
548
about how any one verb?s arguments relate to other
verbs? arguments, or about general distinctions be-
tween verb arguments and adjuncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such as
thematic roles, would facilitate both inferencing and
generalization.
The second issue with PropBank?s verb-specific
approach is that it can make training automatic se-
mantic role labeling (SRL) systems more difficult.
A vast amount of data would be needed to train the
verb-specific models that are theoretically mandated
by PropBank?s design. Instead, researchers typically
build a single model for the numbered arguments
(Arg0, Arg1, . . . , Arg5). This approach works sur-
prisingly well, mainly because an explicit effort was
made to use arguments Arg0 and Arg1 consistently
across different verbs; and because those two argu-
ment labels account for 85% of all arguments. How-
ever, this approach causes the system to conflate
different argument types, especially with the highly
overloaded arguments Arg2-Arg5. As a result, these
argument labels are quite difficult to learn.
A final difficulty with PropBank?s current ap-
proach is that it limits SRL system robustness in
the face of verb senses, verbs or verb constructions
that were not included in the training data, and the
training data is all Wall Street Journal corpora. If
a PropBank-trained SRL system encounters a novel
verb or verb usage, then there is no way for it to
know which role labels are used for which argument
types, since role labels are defined so specifically.
This is especially problematic for Arg2-5. Similarly,
PropBank-trained SRL systems can have difficulty
generalizing when a known verb is encountered in
a novel construction. These problems can happen
quite frequently if the training data comes from a
different genre than the test data. This issue is re-
flected in the relatively poor performance of most
state-of-the-art SRL systems when tested on a novel
genre, the Brown corpus, during CoNLL 2005. For
example, the SRL system described in (Pradhan et
al., 2005b; Pradhan et al, 2005a) achieves an F-
score of 81% when tested on the same genre as it
is trained on (WSJ); but that score drops to 68.5%
when the same system is tested on a different genre
(the Brown corpus). DARPA-GALE is funding an
ongoing effort to PropBank additional genres, but
better techniques for generalizing the semantic role
labeling task are still needed.
In this paper, we demonstrate an increase in the
generality of our semantic role labeling based on a
mapping that has been developed between PropBank
and another lexical resource, VerbNet. By taking ad-
vantage of VerbNet?s more consistent set of labels,
we can generate more useful role label annotations
with a resulting improvement in SRL performance
on novel genres.
2 Background
2.1 PropBank
PropBank (Palmer et al, 2005) is an annotation of
one million words of the Wall Street Journal por-
tion of the Penn Treebank II (Marcus et al, 1994)
with predicate-argument structures for verbs, using
semantic role labels for each verb argument. In or-
der to remain theory neutral, and to increase anno-
tation speed, role labels were defined on a per-verb-
sense basis. Although the same tags were used for
all verbs, (namely Arg0, Arg1, ..., Arg5), these tags
are meant to have a verb-specific meaning.
Thus, the use of a given argument label should
be consistent across different uses of that verb, in-
cluding syntactic alternations. For example, the
Arg1 (underlined) in ?John broke the window? is the
same window that is annotated as the Arg1 in ?The
window broke?, even though it is the syntactic sub-
ject in one sentence and the syntactic object in the
other. However, there is no guarantee that an argu-
ment label will be used consistently across different
verbs. For example, the Arg2 label is used to des-
ignate the destination of the verb ?bring;? but the
extent of the verb ?rise.? Generally, the arguments
are simply listed in the order of their prominence
for each verb. However, an explicit effort was made
when PropBank was created to use Arg0 for argu-
ments that fulfill Dowty?s criteria for ?prototypical
549
agent,? and Arg1 for arguments that fulfill the cri-
teria for ?prototypical patient.? (Dowty, 1991) As
a result, these two argument labels are significantly
more consistent across verbs than the other three.
But nevertheless, there are still some inter-verb in-
consistencies for even Arg0 and Arg1.
2.2 VerbNet
VerbNet (Schuler, 2005) consists of hierarchically
arranged verb classes, inspired by and extended
from classes of Levin 1993 (Levin, 1993). Each
class and subclass is characterized extensionally by
its set of verbs, and intensionally by a list of the
arguments of those verbs and syntactic and seman-
tic information about the verbs. The argument list
consists of thematic roles (23 in total) and pos-
sible selectional restrictions on the arguments ex-
pressed using binary predicates. The syntactic infor-
mation maps the list of thematic arguments to deep-
syntactic arguments (i.e., normalized for voice alter-
nations, and transformations). The semantic predi-
cates describe the participants during various stages
of the event described by the syntactic frame.
The same thematic role can occur in different
classes, where it will appear in different predicates,
providing a class-specific interpretation of the role.
VerbNet has been extended from the original Levin
classes, and now covers 4526 senses for 3769 verbs.
A primary emphasis for VerbNet is the grouping of
verbs into classes that have a coherent syntactic and
semantic characterization, that will eventually facil-
itate the acquisition of new class members based on
observable syntactic and semantic behavior. The hi-
erarchical structure and small number of thematic
roles is aimed at supporting generalizations.
2.3 Mapping PropBank to VerbNet
Because PropBank includes a large corpus of man-
ually annotated predicate-argument data, it can be
used to train supervised machine learning algo-
rithms, which can in turn provide PropBank-style
annotations for novel or unseen text. However, as
we discussed in the introduction, PropBank?s verb-
specific role labels are somewhat problematic. Fur-
thermore, PropBank lacks much of the information
that is contained in VerbNet, including information
about selectional restrictions, verb semantics, and
inter-verb relationships.
We have therefore created a mapping between
VerbNet and PropBank (Loper et al, 2007), which
will allow us to use the machine learning tech-
niques that have been developed for PropBank anno-
tations to generate more semantically abstract Verb-
Net representations. Additionally, the mapping can
be used to translate PropBank-style numbered ar-
guments (Arg0. . .Arg5) to VerbNet thematic roles
(Agent, Patient, Theme, etc.), which should allow us
to overcome the verb-specific nature of PropBank.
The mapping between VerbNet and PropBank
consists of two parts: a lexical mapping and an in-
stance classifier. The lexical mapping is responsible
for specifying the potential mappings between Prop-
Bank and VerbNet for a given word; but it does not
specify which of those mappings should be used for
any given occurrence of the word. That is the job
of the instance classifier, which looks at the word
in context, and decides which of the mappings is
most appropriate. In essence, the instance classi-
fier is performing word sense disambiguation, de-
ciding which lexeme from each database is correct
for a given occurrence of a word. In order to train
the instance classifier, we semi-automatically anno-
tated each verb in the PropBank corpus with Verb-
Net class information.1 This mapped corpus was
then used to build the instance classifier. More de-
tails about the mapping, and how it was created, can
be found in (Loper et al, 2007).
3 Analysis of the Mapping
In order to confirm our belief that PropBank roles
Arg0 and Arg1 are relatively coherent, while roles
Arg2-5 are much more overloaded, we performed
a preliminary analysis of how argument roles were
mapped. Figure 1 shows how often each PropBank
role was mapped to each VerbNet thematic role, cal-
culated as a fraction of instances in the mapped cor-
pus. From this figure, we can see that Arg0 maps to
agent-like roles, such as ?agent? and ?experiencer,?
over 94% of the time; and Arg1 maps to patient-
like roles, including ?theme,? ?topic,? and ?patient,?
over 82% of the time. In contrast, arguments Arg2-5
get mapped to a much broader variety of roles. It is
also worth noting that the sample size for arguments
1Excepting verbs whose senses are not present in VerbNet
(24.5% of instances).
550
Arg3-5 is quite small in comparison with arguments
Arg0-2, suggesting that any automatically built clas-
sifier for arguments Arg3-5 will suffer severe sparse
data problems for those arguments.
4 Training a SRL system with VerbNet
Roles to Achieve Robustness
An important issue for state-of-the-art automatic
SRL systems is robustness: although they receive
high performance scores when tested on the Wall
Street Journal (WSJ) corpus, that performance drops
significantly when the same systems are tested on a
corpus from another genre. This performance drop
reflects the fact that the WSJ corpus is highly spe-
cialized, and tends to use genre-specific word senses
for many verbs. The 2005 CoNLL shared task has
addressed this issue of robustness by evaluating par-
ticipating systems on a test set extracted from the
Brown corpus, which is very different from the WSJ
corpus that was used for training. The results sug-
gest that there is much work to be done in order to
improve system robustness.
One of the reasons that current SRL systems have
difficulty deciding which role label to assign to a
given argument is that role labels are defined on a
per-verb basis. This is less problematic for Arg0
and Arg1, where a conscious effort was made to be
consistent across verbs; but is a significant problem
for Args[2-5], which tend to have very verb-specific
meanings. This problem is exacerbated even fur-
ther on novel genres, where SRL systems are more
likely to encounter unseen verbs and uses of argu-
ments that were not encountered in the training data.
4.1 Addressing Current SRL Problems via
Lexical Mappings
By exploiting the mapping between PropBank and
VerbNet, we can transform the data to make it more
consistent, and to expand the size and variety of the
training data. In particular, we can use the map-
ping to transform the verb-specific PropBank role
labels into the more general thematic role labels that
are used by VerbNet. Unlike the PropBank labels,
the VerbNet labels are defined consistently across
verbs; and therefore it should be easier for statisti-
cal SRL systems to model them. Furthermore, since
the VerbNet role labels are significantly less verb-
Arg0 (45,579)
Agent 85.4%
Experiencer 7.2%
Theme 2.1%
Cause 1.9%
Actor1 1.8%
Theme1 0.8%
Patient1 0.2%
Location 0.2%
Theme2 0.2%
Product 0.1%
Patient 0.0%
Attribute 0.0%
Arg1 (59,884)
Theme 47.0%
Topic 23.0%
Patient 10.8%
Product 2.9%
Predicate 2.5%
Patient1 2.4%
Stimulus 2.0%
Experiencer 1.9%
Cause 1.8%
Destination 0.9%
Theme2 0.7%
Location 0.7%
Source 0.7%
Theme1 0.6%
Actor2 0.6%
Recipient 0.5%
Agent 0.4%
Attribute 0.2%
Asset 0.2%
Patient2 0.2%
Material 0.2%
Beneficiary 0.0%
Arg2 (11,077)
Recipient 22.3%
Extent 14.7%
Predicate 13.4%
Destination 8.6%
Attribute 7.6%
Location 6.5%
Theme 5.5%
Patient2 5.3%
Source 5.2%
Topic 3.1%
Theme2 2.5%
Product 1.5%
Cause 1.2%
Material 0.8%
Instrument 0.6%
Beneficiary 0.5%
Experiencer 0.3%
Actor2 0.2%
Asset 0.0%
Theme1 0.0%
Arg3 (609)
Asset 38.6%
Source 25.1%
Beneficiary 10.7%
Cause 9.7%
Predicate 9.0%
Location 2.0%
Material 1.8%
Theme1 1.6%
Theme 0.8%
Destination 0.3%
Instrument 0.3%
Arg4 (18)
Beneficiary 61.1%
Product 33.3%
Location 5.6%
Arg5 (17)
Location 100.0%
Figure 1: The frequency with which each PropBank
numbered argument is mapped to each VerbNet the-
matic role in the mapped corpus. The numbers
next to each PropBank argument reflects the num-
ber of occurrences of that numbered argument in the
mapped corpus.
551
dependent than the PropBank roles, the SRL?s mod-
els should generalize better to novel verbs, and to
novel uses of known verbs.
5 SRL Experiments on Linked Lexical
Resources
In order to verify the feasibility of performing se-
mantic role labeling with VerbNet thematic roles, we
re-trained our existing SRL system, which originally
used PropBank role labels, with a new label set that
makes use of VerbNet thematic role information.
5.1 The SRL System
Our SRL system is a Maximum Entropy based
pipelined system which consists of four compo-
nents: Pre-processing, Argument Identification, Ar-
gument Classification, and Post Processing. The
Pre-processing component pipes a sentence through
a syntactic parser and filters out constituents which
are unlikely to be semantic arguments based on a
constituents location in the parse tree. The Argu-
ment Identification component is a binary MaxEnt
classifier, which tags candidate constituents as ar-
guments or non-arguments. The Argument Classifi-
cation component is a multi-class MaxEnt classifier
which assigns a semantic role to each constituent.
The Post Processing component further selects the
final arguments based on global constraints. Our ex-
periments mainly focused on changes to the Argu-
ment Classification stage of the SRL pipeline, and
in particular, on changes to the set of output tags.
For more information on our SRL system, see (Yi
and Palmer, 2004; Yi and Palmer, 2005).
The evaluation of SRL systems is typically ex-
pressed by precision, recall and the F1-measure.
Precision is the number of correct arguments pre-
dicted by a system divided by the total number of
arguments proposed. Recall is the number of cor-
rect arguments divided by the number of the total
number of arguments in the Gold Standard Data. F1
computes the harmonic mean of precision and recall.
5.2 SRL Experiments on Mapped VerbNet
Thematic Roles
Since PropBank arguments Arg0 and Arg1 are al-
ready quite coherent, we left them as-is in the new
label set. But since arguments Arg2-Arg5 are highly
Group 1 Group 2 Group 3 Group 4 Group 5
Recipient Extent Predicate Patient2 Instrument
Destination Asset Attribute Product Cause
Location Theme Experiencer
Source Theme1 Actor2
Material Theme2
Beneficiary Topic
Figure 2: Thematic Role Groupings for the exper-
iments on linked lexical resources; and for Arg2 in
the experiments on arguments with different verb in-
dependency.
overloaded, we replaced them by mapping them
to their corresponding VerbNet thematic role. We
found that mapping directly to individual role labels
created a significant sparse data problem, since the
number of output tags was increased from 6 to 23.
We therefore grouped the VerbNet thematic roles
into five coherent groups of similar thematic roles,
shown in Figure 2.2 Our new tag set therefore in-
cluded the following tags: Arg0 (agent); Arg1 (pa-
tient); Group1 (goal); Group2 (extent); Group3
(predicate/attrib); Group4 (product); and Group5
(instrument/cause).
Training our SRL system using these thematic
role groups, we obtained performance similar to the
original SRL system. However, it is important to
note that these performance figures are not directly
comparable, since the two systems are performing
different tasks: The Original system labels Arg0-
5,ArgA and ArgM and the Mapped system labels
Arg0, Arg1, ArgA, ArgM and Group1-5. In partic-
ular, the role labels generated by the original system
are verb-specific, while the role labels generated by
the new system are less verb-dependent.
5.2.1 Results
For our testing and training, we used the portion
of Penn Treebank II that is covered by the mapping,
and where at least one of Arg2-5 is used. Training
was performed using sections 2-21 of the Treebank
(10,783 instances of argument); and testing was per-
formed on section 23 (859 instances). Table 1 dis-
plays the performance score for the SRL system us-
ing the augmented tag set (?Mapped?). The per-
formance score of the original system (?Original?)
is also listed, for reference; however, as was dis-
2Karin Kipper assisted in creating the groupings.
552
System Precision Recall F1
Original 90.65 85.43 87.97
Mapped 88.85 84.56 86.65
Table 1: Overall SRL System performance using the
PropBank tag set (?Original?) and the augmented
tag set (?Mapped?)
System Precision Recall F1
Original 97.60 83.67 90.10
Mapped 91.70 82.86 87.06
Table 2: SRL System performance evaluated on only
Arg2-5 (Original) or Group1-5 (Mapped).
cussed above, these results are not directly compara-
ble because the two systems are performing different
tasks.
The results indicate that the performance drops
when we train on the new argument labels, espe-
cially on precision when we evaluate the systems
on only Arg2-5/Group1-5 (see Table 2). However,
it is premature to conclude that there is no benefit
from the VerbNet thematic role labels. Firstly, we
have very few mapped Arg3-5 instances (less than
1,000 instances); secondly, we lack test data gen-
erated from a genre other than WSJ to allow us to
evaluate the robustness (generality) of SRL trained
on the new argument labels.
We therefore redesigned our experiments by lim-
iting the scope to mapped instances of Arg1 and
Arg2. By doing this, we should be able to accom-
plish the following: 1) we can map new argument la-
bels back to the original PropBank labels; therefore
we can directly compare results; 2) With the ability
of testing our systems on other test data, we can eval-
uate the influence of the mapping on SRL robust-
ness; 3) We can validate our original hypothesis that
the behavior of Arg1 is primarily verb-independent
while Arg2 is more verb-specific.
5.3 SRL Experiments on Arguments with
Different Verb Independency
We conducted two further sets of experiments: one
to test the effect of the mapping on learning Arg2;
and one to test the effect on learning Arg1. Since
Arg2 is used in very verb-dependent ways, we ex-
pect that mapping it to VerbNet role labels will in-
Group 1 Group 2 Group 3 Group 4 Group 5
Theme Source Patient Agent Topic
Theme1 Location Product Actor2
Theme2 Destination Patient1 Experiencer Group 6
Predicate Recipient Patient2 Cause Asset
Stimulus Beneficiary
Attribute Material
Figure 3: Thematic Role Groupings for Arg1 in the
experiments on arguments with different verb inde-
pendency.
crease our performance. However, since a conscious
effort was made to keep the meaning of Arg1 consis-
tent across verbs, we expect that mapping it to Verb-
Net labels will provide less of an improvement.
Each experiment compares two SRL systems: one
trained using the original PropBank role labels; the
other trained with the argument role under consid-
eration (Arg1 or Arg2) subdivided based on which
VerbNet role label it maps to. In order to prevent
the training data from these subdivided labels from
becoming too sparse (which would impair system
performance) we grouped similar thematic roles to-
gether. For Arg2, we used the same groupings as the
previous experiment, shown in Figure 2. The argu-
ment role groupings we used for Arg1 are shown in
Figure 3.
The training data for both experiments is the por-
tion of Penn Treebank II (sections 02-21) that is cov-
ered by the mapping. We evaluated each experi-
mental system using two test sets: section 23 of the
Penn Treebank II, which represents the same genre
as the training data; and the PropBank-ed portion of
the Brown corpus, which represents a very different
genre.
5.3.1 Results and Discussion
Table 3 describes the results of SRL overall per-
formance tested on the WSJ corpus Section 23; Ta-
ble 4 demonstrates the SRL overall system perfor-
mance tested on the Brown corpus. Systems Arg1-
Original and Arg2-Original are trained using the
original PropBank labels, and show the baseline
performance of our SRL system. Systems Arg1-
Mapped and Arg2-Mapped are trained using Prop-
Bank labels augmented with VerbNet thematic role
groups. In order to allow comparison between the
system using the original PropBank labels and the
systems that augmented those labels with VerbNet
553
System Precision Recall F1
Arg1-Original 89.24 77.32 82.85
Arg1-Mapped 90.00 76.35 82.61
Arg2-Original 73.04 57.44 64.31
Arg2-Mapped 84.11 60.55 70.41
Table 3: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the WSJ cor-
pus (section 23). This represents performance on the
same genre as the training corpus.
System Precision Recall F1
Arg1-Original 86.01 71.46 78.07
Arg1-Mapped 88.24 71.15 78.78
Arg2-Original 66.74 52.22 58.59
Arg2-Mapped 81.45 58.45 68.06
Table 4: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the PropBank-
ed Brown corpus. This represents performance on a
different genre from the training corpus.
thematic role groups, system performance was eval-
uated based solely on the PropBank role label that
was assigned.
We had hypothesized that with the use of thematic
roles, we would be able to create a more consis-
tent training data set which would result in an im-
provement in system performance. In addition, the
thematic roles would behave more consistently than
the overloaded Args[2-5] across verbs, which should
enhance robustness. However, since in practice we
are also increasing the number of argument labels
an SRL system needs to tag, the system might suf-
fer from data sparseness. Our hope is that the en-
hancement gained from the mapping will outweigh
the loss due to data sparseness.
From Table 3 and Table 4 we see the F1 scores of
Arg1-Original and Arg1-Mapped are statistically in-
different both on the WSJ corpus and the Brown cor-
pus. These results confirm the observation that Arg1
in the PropBank behaves fairly verb-independently
so that the VerbNet mapping does not provide much
benefit. The increase of precision due to a more co-
herent training data set is compensated for by the
loss of recall due to data sparseness.
The results of the Arg2 experiments tell a differ-
Confusion ARG2-Original
Matrix ARG1 ARG2 ARGM
ARG2- ARG0 53 50 -
Mapped ARG1 - 716 -
ARG2 1 - 2
ARG3 - 1 -
ARGM 1 482 -
233 ARG2-Mapped arguments are not labeled by ARG2-
Original
Table 5: Confusion matrix on the 1,539 instances
which ARG2-Mapped tags correctly and ARG2-
Original fails to predict.
ent story. Both precision and recall are improved
significantly, which demonstrates that the Arg2 label
in the PropBank is quite overloaded. The Arg2 map-
ping improves the overall results (F1) on the WSJ
by 6% and on the Brown corpus by almost 10%. As
a more diverse corpus, the Brown corpus provides
many more opportunities for generalizing to new us-
ages. Our new SRL system handles these cases more
robustly, demonstrating the consistency and useful-
ness of the thematic role categories.
5.4 Improved Argument Distinction via
Mapping
The ARG2-Mapped system generalizes well both
on the WSJ corpus and the Brown corpus. In or-
der to explore the improved robustness brought by
the mapping, we extracted and observed the 1,539
instances to which the system ARG2-Mapped as-
signed the correct semantic role label, but which the
system ARG2-Original failed to predict. From the
confusion matrix depicted in Table 5, we discover
the following:
The mapping makes ARG2 more clearly defined,
and as a result there is a better distinction be-
tween ARG2 and other argument labels: Among
the 1,539 instances that ARG2-Original didn?t tag
correctly, 233 instances are not assigned an argu-
ment label, and 1,252 instances ARG2-Original con-
fuse the ARG2 label with another argument label:
the system ARG2-Original assigned the ARG2 la-
bel to 50 ARG0?s, 716 ARG1?s, 1 ARG3 and 482
ARGM?s, and assigned other argument labels to 3
ARG2?s.
554
6 Conclusions
In conclusion, we have described a mapping from
the annotated PropBank corpus to VerbNet verb
classes with associated thematic role labels. We hy-
pothesized that these labels would be more verb-
independent and less overloaded than the PropBank
Args2-5, and would therefore provide more consis-
tent training instances which would generalize better
to new genres. Our preliminary experiments confirm
this hypothesis, with a 6% performance improve-
ment on the WSJ and a 10% performance improve-
ment on the Brown corpus for Arg2.
In future work, we will map the PropBank-ed
Brown corpus to VerbNet as well, which will allow
much more thorough testing of our hypothesis. We
will also examine back-off to verb class membership
as a technique for improving performance on out of
vocabulary verbs. Finally, we plan to explore the ef-
fect of different thematic role groupings on system
performance.
References
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:574?619.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using Combinatory Categorial Gram-
mar. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 57?64,
Sapporo, Japan.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Shallow semantic
parsing using support vector machines. Technical re-
port, The Center for Spoken Language Research at the
University of Colorado (CSLR).
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Empirical evidence for useful semantic role categories.
In Proceedings of the International Workshop on Com-
putational Linguistics.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate argu-
ment structure.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42-th Conference on Association for Computa-
tional Linguistic (ACL-2004), Barcelona, Spain.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Mar-
tin, James, and Daniel Jurafsky. 2005a. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd
annual meeting (ACL-2005), Ann Arbor, MI.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
In Proceedings of the 19th International Joint Confer-
ence on Artificial Intelligence (IJCAI-05).
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Kristina Toutanova, Aria Haghighi, and Christopher D.
2005. Joint learning improves semantic role labeling.
In Proceedings of the Association for Computational
Linguistics 43rd annual meeting (ACL-2005), Ann Ar-
bor, MI.
Szu-ting Yi and Martha Palmer. 2004. Pushing the
boundaries of semantic role labeling with svm. In Pro-
ceedings of the International Conference on Natural
Language Processing.
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
555
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 237?240, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Integration of Syntactic Parsing and Semantic Role Labeling
Szu-ting Yi
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104 USA
szuting@linc.cis.upenn.edu
Martha Palmer
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104 USA
mpalmer@linc.cis.upenn.edu
Abstract
This paper describes a system for the
CoNLL-2005 Shared Task on Semantic
Role Labeling. We trained two parsers
with the training corpus in which the se-
mantic argument information is attached
to the constituent labels, we then used the
resulting parse trees as the input of the
pipelined SRL system. We present our re-
sults of combining the output of various
SRL systems using different parsers.
1 Introduction
Semantic parsing, identifying and classifying the se-
mantic entities in context and the relations between
them, potentially has great impact on its downstream
applications, such as text summarization, question
answering, and machine translation. As a result, se-
mantic parsing could be an important intermediate
step for natural language comprehension. In this pa-
per, we investigate the task of Semantic Role Label-
ing (SRL): Given a verb in a sentence, the goal is to
locate the constituents which are arguments of the
verb, and assign them appropriate semantic roles,
such as, Agent, Patient, and Theme.
Previous SRL systems have explored the effects
of using different lexical features, and experimented
on different machine learning algorithms. (Gildea
and Palmer, 2002; Pradhan et al, 2005; Punyakanok
et al, 2004) However, these SRL systems generally
extract features from sentences processed by a syn-
tactic parser or other shallow parsing components,
such as a chunker and a clause identifier. As a result,
the performance of the SRL systems relies heavily
on those syntax-analysis tools.
In order to improve the fundamental performance
of an SRL system, we trained parsers with training
data containing not only syntactic constituent infor-
mation but also semantic argument information. The
new parsers generate more correct constituents than
that trained on pure syntactic information. Because
the new parser generate different constituents than a
pure syntactic parser, we also explore the possibility
of combining the output of several parsers with the
help of a voting post-processing component.
This paper is organized as follows: Section 2
demonstrates the components of our SRL system.
We elaborate the importance of training a new parser
and outline our approach in Section 3 and Section 4.
Finally, Section 5 reports and discusses the results.
2 Semantic Role Labeling: the
Architecture
Our SRL system has 5 phases: Parsing, Pruning, Ar-
gument Identification, Argument Classification, and
Post Processing. The Argument Identification and
Classification components are trained with Sec 02-
21 of the Penn Treebank corpus.
2.1 Parsing
Previous SRL systems usually use a pure syntactic
parser, such as (Charniak, 2000; Collins, 1999), to
retrieve possible constituents. Once the boundary of
a constituent is defined, there is no way to change
it in later phases. Therefore the quality of the syn-
tactic parser has a major impact on the final per-
237
formance of an SRL system, and the percentage of
correct constituents that is generated by the syntac-
tic parser also defines the recall upper bound of an
SRL system. In order to attack this problem, in addi-
tion to Charniak?s parser (Charniak, 2000), our sys-
tem combine two parser which are trained on both
syntactic constituent information and semantic argu-
ment information. (See Section 3)
2.2 Pruning
Given a parse tree, a pruning component filters out
the constituents which are unlikely to be semantic
arguments in order to facilitate the training of the Ar-
gument Identification component. Our system uses
the heuristic rules introduced by (Xue and Palmer,
2004). The heuristics first spot the verb and then ex-
tract all the sister nodes along the verb spine of the
parse tree. We expand the coverage by also extract-
ing all the immediate children of an S, ADVP, PP
and NP node. This stage generally prunes off about
80% of the constituents given by a parser. For our
newly trained parsers, we also extract constituents
which have a secondary constituent label indicating
the constituent in question is an argument.
2.3 Argument Identification and Classification
We have as our Argument Identification component
a binary maximum-entropy classifier to determine
whether a constituent is an argument or not. If
a constituent is tagged as an argument, the Argu-
ment Classification component, which is a multi-
class maximum-entropy classifier, would assign it
a semantic role. The implementation of both the
Argument Identification and Classification compo-
nents makes use of the Mallet package1.
The lexical features we use to train these two
components are taken from (Xue and Palmer, 2004).
We trained the Argument Identification compo-
nent with the following single features: the path
from the constituent to the verb, the head word of
the constituent and its POS tag, and the distance
between the verb and the constituent, and feature
combinations: the verb and the phrasal type of the
constituent, the verb and the head word of the con-
stituent. If the parent node of the constituent is a PP
node, then we also include the head word of the PP
1http://mallet.cs.umass.edu
node and the feature combination of the verb and the
head word of the PP node.
In addition to the features listed above, the Ar-
gument Classification component also contains the
following features: the verb, the first and the last
content word of the constituent, the phrasal type
of the left sibling and the parent node, voice (pas-
sive or active), position of the constituent relative to
the verb, the subcategorization frame, and the syn-
tactic frame which describes the sequential pattern
of the noun phrases and the verb in the sentence.
2.4 Post Processing
The post processing component merges adjacent dis-
continuous arguments and marks the R-arguments
based on the content word and phrase type of the ar-
gument. Also it filters out arguments according to
the following constraints:
1. There are no overlapping arguments.
2. There are no repeating core arguments.
In order to combine the different systems, we also
include a voting scheme. The algorithm is straight-
forward: Suppose there are N participating systems,
we pick arguments with N votes, N-1 votes ..., and
finally 1 vote. The way to break a tie is based on
the confidence level of the argument given by the
system. Whenever we pick an argument, we need
to check whether this argument conflicts with pre-
viously selected arguments based on the constraints
described above.
3 Training a Parser with Semantic
Argument Information
A good start is always important, especially for a
successful SRL system. Instead of passively accept-
ing candidate constituents from the upstream syn-
tactic parser, an SRL system needs to interact with
the parser in order to obtain improved performance.
This motivated our first attempt which is to integrate
syntactic parsing and semantic parsing as a single
step, and hopefully as a result we would be able to
discard the SRL pipeline. The idea is to augment
the Penn Treebank (Marcus et al, 1994) constituent
labels with the semantic role labels from the Prop-
Bank (Palmer et al, 2005), and generate a rich train-
ing corpus. For example, if an NP is also an ar-
238
gument ARG0 of a verb in the given sentence, we
change the constituent label NP into NP-ARG0. A
parser therefore is trained on this new corpus and
should be able to serve as an SRL system at the same
time as predicting a parse.
However, this ideal approach is not feasible.
Given the fact that there are many different semantic
role labels and the same constituent can be different
arguments of different verbs in the same sentence,
the number of constituent labels will soon grow out
of control and make the parser training computation-
ally infeasible. Not to mention that anchor verb in-
formation has not yet been added to the constituent
label, and general data sparseness. As a compro-
mise, we decided to integrate only Argument Iden-
tification with syntactic parsing. We generated the
training corpus by simply marking the constituents
which are also semantic arguments.
4 Parsing Experiments
We trained a maximum-entropy parser based
on (Ratnaparkhi, 1999) using the OpenNLP pack-
age 2. We started our experiments with this specific
parsing implementation because of its excellent flex-
ibility that allows us to test different features. Be-
sides, this parser contains four clear parse tree build-
ing stages: TAG, CHUNK, BUILD, and CHECK.
This parsing structure offers us an isolated working
environment for each stage that helps us confine nec-
essary implementation modifications and trace down
implementation errors.
4.1 Data Preparation
Following standard practice, we use Sec 02-21 of
the Penn Treebank and the PropBank as our training
corpus. The constituent labels defined in the Penn
Treebank consist of a primary label and several sec-
ondary labels. A primary label represents the major
syntactic function carried by the constituent, for in-
stance, NP indicates a noun phrase and PP indicates
a prepositional phrase. A secondary label, starting
with ?-?, represents either a grammatical function of
a constituent or a semantic function of an adjunct.
For example, NP-SBJ means the noun phrase is a
surface subject of the sentence; PP-LOC means the
prepositional phrase is a location. Although the sec-
2http://sourceforge.net/projects/opennlp/
ondary labels give us much to encourage informa-
tion, because of data sparseness problem and train-
ing efficiency, we stripped off all the secondary la-
bels from the Penn Treebank.
After stripping off the secondary labels from the
Penn Treebank, we augment the constituent labels
with the semantic argument information from the
PropBank. We adopted four different labels, -AN,
-ANC, -AM, and -AMC. If the constituent in the
Penn Treebank is a core argument, which means
the constituent has one of the labels of ARG0-5 and
ARGA in the PropBank, we attach -AN to the con-
stituent label. The label -ANC means the constituent
is a discontinuous core argument. Similarly, -AM
indicates an adjunct-like argument, ARGM, and -
AMC indicates a discontinuous ARM.
For example, the sentence from Sec 02, [ARG0
The luxury auto maker] [ARGM-TMP last year]
sold [ARG1 1,214 cars] [ARGM-LOC in the U.S.],
would appear in the following format in our train-
ing corpus: (S (NP-AN (DT The) (NN luxury) (NN
auto) (NN maker) ) (NP-AM (JJ last) (NN year) )
(VP (VBD sold) (NP-AN (CD 1,214) (NNS cars) )
(PP -AM (IN in) (NP (DT the) (NNP U.S.) ) ) ) )
4.2 The 2 Different Parsers
Since the core arguments and the ARGMs in the
PropBank loosely correspond to the complements
and adjuncts in the linguistics literature, we are in-
terested in investigating their individual effect on
parsing performance. We trained two parsers. An
AN-parser was trained on the Penn Treebank cor-
pus augmented with two semantic argument labels:
-AN, and -ANC. Another AM-parser was trained on
labels -AM, and -AMC.
5 Results and Discussion
Table 1 shows the results after combining various
SRL systems using different parsers. In order to ex-
plore the effects of combining, we include the over-
all performance on the development dataset of indi-
vidual SRL systems in Table 2.
The performance of Semantic Role Labeling
(SRL) is determined by the quality of the syntactic
information provided to the system. In this paper,
we investigate that for the SRL task whether it is
more suitable to use a parser trained with data con-
239
Precision Recall F  
Development 75.70% 69.99% 72.73
Test WSJ 77.51% 72.97% 75.17
Test Brown 67.88% 59.03% 63.14
Test WSJ+Brown 76.31% 71.10% 73.61
Test WSJ Precision Recall F  
Overall 77.51% 72.97% 75.17
A0 85.14% 77.32% 81.04
A1 77.61% 75.16% 76.37
A2 68.18% 62.16% 65.03
A3 66.91% 52.60% 58.90
A4 77.08% 72.55% 74.75
A5 100.00% 40.00% 57.14
AM-ADV 59.73% 51.58% 55.36
AM-CAU 67.86% 52.05% 58.91
AM-DIR 65.67% 51.76% 57.89
AM-DIS 80.39% 76.88% 78.59
AM-EXT 78.95% 46.88% 58.82
AM-LOC 57.43% 55.37% 56.38
AM-MNR 54.37% 56.10% 55.22
AM-MOD 96.64% 94.01% 95.31
AM-NEG 96.88% 94.35% 95.59
AM-PNC 41.38% 41.74% 41.56
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.13% 74.15% 75.61
R-A0 86.82% 85.27% 86.04
R-A1 67.72% 82.05% 74.20
R-A2 46.15% 37.50% 41.38
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 100.00% 42.86% 60.00
R-AM-MNR 33.33% 33.33% 33.33
R-AM-TMP 78.57% 63.46% 70.21
R-C-A1 0.00% 0.00% 0.00
V 97.35% 95.54% 96.44
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
taining both syntactic bracketing and semantic ar-
gument boundary information than a pure syntactic
one.
The results of the SRL systems using the AM-
or AN- parsers are not significantly better than that
using the Charniak?s parser. This might due to the
simple training mechanism of the base parsing al-
gorithm which the AM- and AN- parsers exploit. It
also suggests our future work to apply the approach
to more sophisticated parsing frameworks. By then,
We show that we can boost the final performance
by combining different SRL systems using different
parsers, given that the combination algorithm is ca-
Precision Recall F  
AN-parser 71.31% 63.68% 67.28
AM-parser 74.09% 65.11% 69.31
Charniak 76.31% 64.62% 69.98
All 3 combined 75.70% 69.99% 72.73
Table 2: Overall results on the development set of
individual SRL systems.
pable of maintaining the quality of the final argu-
ments.
6 Acknowledgments
We thank Tom Morton for providing detailed expla-
nation for any of our parsing related inquiries.
References
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL-2000.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD Dissertation, Uni-
versity of Pennsylvania.
Daniel Gildea and Martha Palmer. 2002. The Neces-
sity of Parsing for Predicate Argument Recognition.
In Proceedings of ACL 2002, Philadelphia, USA.
Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz,
et al 1994. The Penn Treebank: Annotating Predicate
Argument Structure. In Proceedings of ARPA Speech
and Natural Language Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles. Computational Linguistics, 31(1).
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin,
J., and Jurafsky, D. 2005. Support Vector Learning for
Semantic Argument Classification. To appear in Ma-
chine Learning journal, Special issue on Speech and
Natural Language Processing.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic Role Labeling via Integer Linear Program-
ming Inference. In Proceedings of COLING.
Adwait Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Machine
Learning, 34, 151?175.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceedings
of EMNLP.
240
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 70?77,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Issues in Synchronizing the English Treebank and PropBank 
 
Olga Babko-Malayaa, Ann Biesa, Ann Taylorb, Szuting Yia, Martha Palmerc,  
Mitch Marcusa, Seth Kulicka and Libin Shena 
aUniversity of Pennsylvania, bUniversity of York, cUniversity of Colorado 
{malayao,bies}@ldc.upenn.edu, {szuting,mitch,skulick,libin}@linc.cis.upenn.edu,
at9@york.ac.uk, Martha.Palmer@colorado.edu
 
Abstract 
The PropBank primarily adds semantic 
role labels to the syntactic constituents in 
the parsed trees of the Treebank. The 
goal is for automatic semantic role label-
ing to be able to use the domain of local-
ity of a predicate in order to find its ar-
guments. In principle, this is exactly what 
is wanted, but in practice the PropBank 
annotators often make choices that do not 
actually conform to the Treebank parses. 
As a result, the syntactic features ex-
tracted by automatic semantic role label-
ing systems are often inconsistent and 
contradictory. This paper discusses in de-
tail the types of mismatches between the 
syntactic bracketing and the semantic 
role labeling that can be found, and our 
plans for reconciling them. 
1 Introduction 
The PropBank corpus annotates the entire Penn 
Treebank with predicate argument structures by 
adding semantic role labels to the syntactic 
constituents of the Penn Treebank.  
Theoretically, it is straightforward for PropBank 
annotators to locate possible arguments based on 
the syntactic structure given by a parse tree, and 
mark the located constituent with its argument 
label. We would expect a one-to-one mapping 
between syntactic constituents and semantic 
arguments. However, in practice, PropBank 
annotators often make choices that do not 
actually conform to the Penn Treebank parses. 
The discrepancies between the PropBank and 
the Penn Treebank obstruct the study of the syn-
tax and semantics interface and pose an immedi-
ate problem to an automatic semantic role label-
ing system. A semantic role labeling system is 
trained on many syntactic features extracted from 
the parse trees, and the discrepancies make the 
training data inconsistent and contradictory. In 
this paper we discuss in detail the types of mis-
matches between the syntactic bracketing and the 
semantic role labeling that can be found, and our 
plans for reconciling them. We also investigate 
the sources of the disagreements, which types of 
disagreements can be resolved automatically, 
which types require manual adjudication, and for 
which types an agreement between syntactic and 
semantic representations cannot be reached. 
1.1 Treebank  
The Penn Treebank annotates text for syntactic 
structure, including syntactic argument structure 
and rough semantic information. Treebank anno-
tation involves two tasks: part-of-speech tagging 
and syntactic annotation. 
The first task is to provide a part-of-speech tag 
for every token. Particularly relevant for Prop-
Bank work, verbs in any form (active, passive, 
gerund, infinitive, etc.) are marked with a verbal 
part of speech (VBP, VBN, VBG, VB, etc.). 
(Marcus, et al 1993; Santorini 1990) 
The syntactic annotation task consists of 
marking constituent boundaries, inserting empty 
categories (traces of movement, PRO, pro), 
showing the relationships between constituents 
(argument/adjunct structures), and specifying a 
particular subset of adverbial roles. (Marcus, et 
al. 1994; Bies, et al 1995) 
Constituent boundaries are shown through 
syntactic node labels in the trees. In the simplest 
case, a node will contain an entire constituent, 
complete with any associated arguments or 
modifiers. However, in structures involving syn-
tactic movement, sub-constituents may be dis-
placed. In these cases, Treebank annotation 
represents the original position with a trace and 
shows the relationship as co-indexing. In (1) be-
low, for example, the direct object of entail is 
shown with the trace *T*, which is coindexed to 
the WHNP node of the question word what. 
 
(1) (SBARQ (WHNP-1 (WP What ))
(SQ (VBZ does )
(NP-SBJ (JJ industrial )
(NN emigration ))
(VP (VB entail)
(NP *T*-1)))
(. ?))
70
In (2), the relative clause modifying a journal-
ist has been separated from that NP by the prepo-
sitional phrase to al Riyadh, which is an argu-
ment of the verb sent. The position where the 
relative clause originated or ?belongs? is shown 
by the trace *ICH*, which is coindexed to the 
SBAR node containing the relative clause con-
stituent. 
 
(2)(S (NP-SBJ You)  
(VP sent
(NP (NP a journalist)
(SBAR *ICH*-2))
(PP-DIR to
(NP al Riyadh))
(SBAR-2
(WHNP-3 who)
(S (NP-SBJ *T*-3)
(VP served
(NP (NP the name)
(PP of
(NP Lebanon)))
(ADVP-MNR
magnificently))))))
 
Empty subjects which are not traces of move-
ment, such as PRO and pro, are shown as * (see 
the null subject of the infinite clause in (4) be-
low). These null subjects are coindexed with a 
governing NP if the syntax allows. The null sub-
ject of an infinitive clause complement to a noun 
is, however, not coindexed with another node in 
the tree in the syntax. This coindexing is shown 
as a semantic coindexing in the PropBank anno-
tation. 
The distinction between syntactic arguments 
and adjuncts of the verb or verb phrase is made 
through the use of functional dashtags rather than 
with a structural difference. Both arguments and 
adjuncts are children of the VP node. No distinc-
tion is made between VP-level modification and 
S-level modification. All constituents that appear 
before the verb are children of S and sisters of 
VP; all constituents that appear after the verb are 
children of VP.  
Syntactic arguments of the verb are NP-SBJ, 
NP (no dashtag), SBAR (either ?NOM-SBJ or no 
dashtag), S (either ?NOM-SBJ or no dashtag),  
-DTV, -CLR (closely/clearly related), -DIR with 
directional verbs. 
Adjuncts or modifiers of the verb or sentence 
are any constituent with any other adverbial 
dashtag, PP (no dashtag), ADVP (no dashtag). 
Adverbial constituents are marked with a more 
specific functional dashtag if they belong to one 
of the more specific types in the annotation sys-
tem (temporal ?TMP, locative ?LOC, manner  
?MNR, purpose ?PRP, etc.). 
Inside NPs, the argument/adjunct distinction is 
shown structurally. Argument constituents (S and 
SBAR only) are children of NP, sister to the head 
noun. Adjunct constituents are sister to the NP 
that contains the head noun, child of the NP that 
contains both:  
 
(NP (NP head)
(PP adjunct)) 
1.2 PropBank   
PropBank is an annotation of predicate-argument 
structures on top of syntactically parsed, or Tree-
banked, structures. (Palmer, et al 2005; Babko-
Malaya, 2005). More specifically, PropBank 
annotation involves three tasks: argument 
labeling, annotation of modifiers, and creating 
co-reference chains for empty categories.  
The first goal is to provide consistent argu-
ment labels across different syntactic realizations 
of the same verb, as in   
 
(3) [ARG0 John] broke [ARG1 the window]   
 [ARG1 The window] broke.  
 
As this example shows, semantic arguments 
are tagged with numbered argument labels, such 
as Arg0, Arg1, Arg2, where these labels are de-
fined on a verb by verb basis.  
The second task of the PropBank annotation 
involves assigning functional tags to all modifi-
ers of the verb, such as MNR (manner), LOC 
(locative), TMP (temporal), DIS (discourse con-
nectives), PRP (purpose) or DIR (direction) and 
others. 
And, finally, PropBank annotation involves 
finding antecedents for ?empty? arguments of the 
verbs, as in (4). The subject of the verb leave in 
this example is represented as an empty category 
[*] in Treebank. In PropBank, all empty catego-
ries which could be co-referred with a NP within 
the same sentence are linked in ?co-reference? 
chains:  
 
(4) I made a decision [*] to leave 
 
Rel:    leave,   
Arg0: [*] -> I 
 
As the following sections show, all three tasks 
of PropBank annotation result in structures 
which differ in certain respects from the corre-
sponding Treebank structures. Section 2 presents 
71
our approach to reconciling the differences be-
tween Treebank and PropBank with respect to 
the third task, which links empty categories with 
their antecedents. Section 3 introduces mis-
matches between syntactic constituency in Tree-
bank and PropBank. Mismatches between modi-
fier labels are not addressed in this paper and are 
left for future work. 
2 Coreference and syntactic chains  
PropBank chains include all syntactic chains 
(represented in the Treebank) plus other cases of 
nominal semantic coreference, including those  
in which the coreferring NP is not a syntactic 
antecedent. For example, according to PropBank 
guidelines, if a trace is coindexed with a NP in 
Treebank, then the chain should be reconstructed: 
 
(5) What-1 do you like [*T*-1]? 
 
Original PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*] -> What 
 
Such chains usually include traces of A and A? 
movement and PRO for subject and object con-
trol. On the other hand, not all instances of PROs 
have syntactic antecedents. As the following ex-
ample illustrates, subjects of infinitival verbs and 
gerunds might have antecedents within the same 
sentence, which cannot be linked as a syntactic 
chain. 
 
(6) On the issue of abortion , Marshall Coleman 
wants  to take away your right  [*] to choose 
and give it to the politicians .  
 
ARG0:          [*] -> your 
REL:           choose 
 
Given that the goal of PropBank is to find all 
semantic arguments of the verbs, the links be-
tween empty categories and their coreferring NPs 
are important, independent of whether they are 
syntactically coindexed or not. In order to recon-
cile the differences between Treebank and Prop-
Bank annotations, we decided to revise Prop-
Bank annotation and view it as a 3 stage process. 
First, PropBank annotators should not recon-
struct syntactic chains, but rather tag empty cate-
gories as arguments. For example, under the new 
approach annotators would simply tag the trace 
as the Arg1 argument in (7): 
(7) What-1 do you like [*T*-1]? 
 
Revised PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*]  
  
As the second stage, syntactic chains will be re-
constructed automatically, based on the 
coindexation provided by Treebank (note that the 
trace is coindexed with the NP What in (7)). And, 
finally, coreference annotation will be done on 
top of the resulting resource, with the goal of 
finding antecedents for the remaining empty 
categories, including empty subjects of infinitival 
verbs and gerunds.   
One of the advantages of this approach is that 
it allows us to distinguish different types of 
chains, such as syntactic chains (i.e., chains 
which are derived as the result of syntactic 
movement, or control coreference), direct 
coreference chains (as illustrated by the example 
in (6)), and semantic type links for other ?indi-
rect? types of links between an empty category 
and its antecedent.  
Syntactic chains are annotated in Treebank, 
and are reconstructed automatically in PropBank. 
The annotation of direct coreference chains is 
done manually on top of Treebank, and is re-
stricted to empty categories that are not 
coindexed with any NP in Treebank. And, finally, 
as we show next, a semantic type link is used for 
relative clauses and a coindex link for verbs of 
saying. 
A semantic type link is used when the antece-
dent and the empty category do not refer to the 
same entity, but do have a certain kind of rela-
tionship. For example, consider the relative 
clause in (8):  
 
(8) Answers that we?d like to have 
 
Treebank annotation: 
(NP (NP answers)
(SBAR (WHNP-6 which)
(S (NP-SBJ-3 we)
(VP 'd
(VP like
(S (NP-SBJ *-3)
(VP to
(VP have
(NP *T*-6)
))))))))
 
In Treebank, the object of the verb have is a trace, 
which is coindexed with the relative pronoun. In 
72
the original PropBank annotation, a further link 
is provided, which specifies the relative pronoun 
as being of ?semantic type? answers.  
 
(9) Original PropBank annotation: 
Arg1:    [NP *T*-6] -> which -> answers 
 rel:         have 
 Arg0:     [NP-SBJ *-3] -> we 
 
This additional link between which and answers 
is important for many applications that make use 
of preferences for semantic types of verb argu-
ments, such as Word Sense Disambiguation 
(Chen & Palmer 2005). In the new annotation 
scheme, annotators will first label traces as ar-
guments: 
 
(10) Revised PropBank annotation (stage 1): 
Rel:  have 
Arg1: [*T*-6]  
Arg0: [NP-SBJ *-3] 
 
As the next stage, the trace [*T*-6] will be 
linked to the relative pronoun automatically (in 
addition to the chain [NP-SBJ *-3] -> we being 
automatically reconstructed). As the third stage, 
PropBank annotators will link which to answers. 
However, this chain will be labeled as a ?seman-
tic type? to distinguish it from direct coreference 
chains and to indicate that there is no identity 
relation between the coindexed elements. 
Verbs of saying illustrate another case of links 
rather than coreference chains. In many sen-
tences with direct speech, the clause which intro-
duces a verb of saying is ?embedded? into the 
utterance. Syntactically this presents a problem 
for both Treebank and Propbank annotation. In 
Treebank, the original annotation style required a 
trace coindexed to the highest S node as the ar-
gument of the verb of saying, indicating syntactic 
movement. 
 
(11) Among other things, they said  [*T*-1] , Mr. 
Azoff would develop musical acts for a new 
record label . 
 
Treebank annotation: 
(S-1 (PP Among
(NP other things))
(PRN ,
(S (NP-SBJ they)
(VP said
(SBAR 0
(S *T*-1))))
,)
(NP-SBJ Mr. Azoff)
(VP would
(VP develop
(NP (NP musical acts)
(PP for
(NP a new record
label)))))
.)
In PropBank, the different pieces of the utterance, 
including the trace under the verb said, were 
concatenated 
 
(12) Original PropBank annotation: 
ARG1:      [ Among other things] [ Mr. 
Azoff] [ would develop musical acts for a 
new record label] [ [*T*-1]] 
ARG0:       they 
rel:        said 
 
Under the new approach, in stage one, Tree-
bank annotation will introduce not a trace of the 
S clause, but rather *?*, an empty category indi-
cating ellipsis. In stage three, PropBank annota-
tors will link this null element to the S node, but 
the resulting chain will not be viewed as  ?direct? 
coreference. A special tag will be used for this 
link, in order to distinguish it from other types of 
chains. 
 
(13) Revised PropBank  annotation: 
ARG1:      [*?*] (-> S) 
ARG0:       they 
rel:        said 
3 Differences in syntactic constituency  
3.1 Extractions of mismatches between 
PropBank and Treebank 
In order to make the necessary changes to both 
the Treebank and the PropBank, we have to first 
find all instances of mismatches. We have used 
two methods to do this: 1) examining the argu-
ment locations; 2) examining the discontinuous 
arguments. 
 
Argument Locations  In a parse tree which ex-
presses the syntactic structure of a sentence, a 
semantic argument occupies specific syntactic 
locations: it appears in a subject position, a verb 
complement location or an adjunct location. 
Relative to the predicate, its argument is either a 
sister node, or a sister node of the predicate?s 
ancestor. We extracted cases of PropBank argu-
ments which do not attach to the predicate spine, 
and filtered out VP coordination cases. For ex-
ample, the following case is a problematic one 
because the argument PP node is embedded too 
73
deeply in an NP node and hence it cannot find a 
connection with the main predicate verb lifted. 
This is an example of a PropBank annotation 
error. 
 
(14) (VP (VBD[rel] lifted) 
(NP us) )
(NP-EXT
(NP a good 12-inches)
(PP-LOC[ARGM-LOC] above
(NP the water level))))
 
However, the following case is not problem-
atic because we consider the ArgM PP to be a 
sister node of the predicate verb given the VP 
coordination structure:  
 
(15) (VP (VP (VB[rel] buy)  
(NP the basket of ? )
(PP in whichever market ?))
(CC and)
(VP (VBP sell)
(NP them)
(PP[ARGM] in the more
expensive market)))
 
Discontinuous Arguments happen when Prop-
Bank annotators need to concatenate several 
Treebank constituents to form an argument.  Dis-
continuous arguments often represent different 
opinions between PropBank and Treebank anno-
tators regarding the interpretations of the sen-
tence structure. 
For example, in the following case, the Prop-
Bank concatenates the NP and the PP to be the 
Arg1. In this case, the disagreement on PP at-
tachment is simply a Treebank annotation error. 
 
(16) The region lacks necessary mechanisms for 
handling the aid and accounting items. 
 
Treebank annotation: 
(VP lacks
(NP necessary mechanisms)
(PP for
(NP handing the aid?)))
 
PropBank annotation: 
REL: lacks 
Arg1: [NP necessary mechanisms][PP for 
handling the aid and accounting items] 
 
All of these examples have been classified into 
the following categories: (1) attachment ambi-
guities, (2) different policy decisions, and (3) 
cases where one-to-one mapping cannot be pre-
served. 
3.2 Attachment ambiguities  
Many cases of mismatches between Treebank 
and PropBank constituents are the result of am-
biguous interpretations. The most common ex-
amples are cases of modifier attachment ambi-
guities, including PP attachment. In cases of am-
biguous interpretations, we are trying to separate 
cases which can be resolved automatically from 
those which require manual adjudication. 
 
PP-Attachment  The most typical case of PP 
attachment annotation disagreement is shown in 
(17).  
 
(17) She wrote a letter for Mary. 
 
Treebank annotation: 
(VP wrote
(NP (NP a letter)
(PP for
(NP Mary))))
 
PropBank annotation: 
REL: write 
Arg1: a letter 
Arg2: for Mary 
 
In (17), the PP ?for Mary? is attached to the 
verb in PropBank and to the NP in Treebank. 
This disagreement may have been influenced by 
the set of roles of the verb ?write?, which in-
cludes a beneficiary as its argument.  
 
(18) Frameset write:  Arg0: writer 
   Arg1: thing written 
   Arg2: beneficiary 
 
Examples of this type cannot be automatically 
resolved and require manual adjudication. 
Adverb Attachment  Some cases of modifier 
attachment ambiguities, on the other hand, could 
be automatically resolved. Many cases of mis-
matches are of the type shown in (19), where a 
directional adverbial follows the verb. In Tree-
bank, this adverbial is analyzed as part of an 
ADVP which is the argument of the verb in 
question. However, in PropBank, it is annotated 
as a separate ArgM-DIR.  
(19) Everything is going back to Korea or Japan. 
 
 
74
Treebank annotation:  
(S (NP-SBJ (NN Everything) )
(VP (VBZ is)
(VP (VBG[rel] going)
(ADVP-DIR
(RB[ARGM-DIR] back)
(PP[ARG2] (TO to)
(NP (NNP Korea)
(CC and)
(NNP Japan)
))))) (. .))
 
Original PropBank annotation: 
Rel: going 
ArgM-DIR: back 
Arg2: to Korea or Japan 
 
For examples of this type, we have decided to 
automatically reconcile PropBank annotations to 
be consistent with Treebank, as shown in (20). 
 
(20) Revised PropBank annotation: 
Rel:  going 
Arg2: back to Korea or Japan 
3.3 Sentential complements 
Another area of significant mismatch between 
Treebank and PropBank annotation involves sen-
tential complements, both infinitival clauses and 
small clauses. In general, Treebank annotation 
allows many more verbs to take sentential com-
plements than PropBank annotation. 
For example, the Treebank annotation of the 
sentence in (21) gives the verb keep a sentential 
complement which has their markets active un-
der the S as the subject of the complement clause. 
PropBank annotation, on the other hand, does not 
mark the clause but rather labels each subcon-
stituent as a separate argument. 
 
(21)  ?keep their markets active 
 
Treebank annotation: 
(VP keep
(S (NP-SBJ their markets)
(ADJP-PRD active)))
 
PropBank annotation: 
REL: keep 
Arg1: their markets 
Arg2: active 
 
In Propbank, an important criterion for decid-
ing whether a verb takes an S argument, or de-
composes it into two arguments (usually tagged 
as Arg1 and Arg2) is based on the semantic in-
terpretation of the argument, e.g. whether the 
argument can be interpreted as an event or pro-
position. 
For example, causative verbs (e.g. make, get), 
verbs of perception (see, hear), and intensional 
verbs (want, need, believe), among others, are 
analyzed as taking an S clause, which is inter-
preted as an event in the case of causative verbs 
and verbs of perception, and as a proposition in 
the case of intensional verbs. On the other hand, 
?label? verbs (name, call, entitle, label, etc.), do 
not select for an event or proposition and are 
analyzed as having 3 arguments: Arg0, Arg1, 
and Arg2. 
Treebank criteria for distinguishing arguments, 
on the other hand, were based on syntactic 
considerations, which did not always match with 
Propbank. For example, in Treebank, evidence of 
the syntactic category of argument that a verb 
can take is used as part of the decision process 
about whether to allow the verb to take a small 
clause. Verbs that take finite or non-finite (verbal) 
clausal arguments, are also treated as taking 
small clauses. The verb find takes a finite clausal 
complement as in We found that the book was 
important and also a non-finite clausal comple-
ment as in We found the book to be important. 
Therefore, find is also treated as taking a small 
clause complement as in We found the book 
important.  
 
(22) (S (NP-SBJ We) 
(VP found
(S (NP-SBJ the book)
(ADJP-PRD important))))
 
The obligatory nature of the secondary predi-
cate in this construction also informed the deci-
sion to use a small clause with a verb like find. In 
(22), for example, important is an obligatory part 
of the sentence, and removing it makes the sen-
tence ungrammatical with this sense of find (?We 
found the book? can only be grammatical with a 
different sense of find, essentially ?We located 
the book?). 
With verbs that take infinitival clausal com-
plements, however, the distinction between a 
single S argument and an NP object together 
with an S argument is more difficult to make. 
The original Treebank policy was to follow the 
criteria and the list of verbs taking both an NP 
object and an infinitival S argument given in 
Quirk, et al (1985).  
Resultative constructions are frequently a 
source of mismatch between Treebank annota-
75
tion as a small clause and PropBank annotation 
with Arg1 and Arg2. Treebank treated a number 
of resultative as small clauses, although certain 
verbs received resultative structure annotation, 
such as the one in (23). 
 
(23) (S (NP-SBJ They) 
(VP painted
(NP-1 the apartment)
(S-CLR (NP-SBJ *-1)
(ADJP-PRD orange))))
 
In all the mismatches in the area of sentential 
complementation, Treebank policy tends to 
overgeneralize S-clauses, whereas Propbank 
leans toward breaking down clauses into separate 
arguments.  
This type of mismatch is being resolved on a 
verb-by-verb basis. Propbank will reanalyze 
some of the verbs (like consider and find), which 
have been analyzed as having 3 arguments, as 
taking an S argument. Treebank, on the other 
hand, will change the analysis of label verbs like 
call, from a small clause analysis to a structure 
with two complements. 
Our proposed structure for label verbs, for ex-
ample, is in (24). 
 
(24) (S (NP-SBJ[Arg0] his parents) 
(VP (VBD called)
(NP-1[Arg1] him)
(S-CLR[Arg2]
(NP-SBJ *-1)
(NP-PRD John))))
 
This structure will accommodate both Treebank 
and PropBank requirements for label verbs. 
4 Where Syntax and Semantics do not 
match  
Finally, there are some examples where the dif-
ferences seem to be impossible to resolve with-
out sacrificing some important features of Prop-
Bank or Treebank annotation. 
4.1 Phrasal verbs   
PropBank has around 550 phrasal verbs like 
keep up, touch on, used to and others, which are 
analyzed as separate predicates in PropBank. 
These verbs have their own set of semantic roles, 
which is different from the set of roles of the cor-
responding ?non-phrasal? verbs, and therefore 
they require a separate PropBank entry. In Tree-
bank, on the other hand, phrasal verbs are not 
distinguished. If the second part of the phrasal 
verb is labeled as a verb+particle combination in 
the Treebank, the PropBank annotators concate-
nate it with the verb as the REL. If Treebank la-
bels the second part of the ?phrasal verb? as part 
of a prepositional phrase, there is no way to re-
solve the inconsistency.   
 
(25) But Japanese institutional investors are used 
to quarterly or semiannual payments on their in-
vestments, so ?  
 
Treebank annotation: 
(VBN used)
(PP (TO to)
(NP quarterly or ?
on their investments))
 
PropBank annotation: 
      Arg1: quarterly or ? on their investments 
 Rel: used to (?used to? is a separate predi-
cate in PropBank) 
4.2 Conjunction  
In PropBank, conjoined NPs and clauses are 
usually analyzed as one argument, parallel to 
Treebank. For example, in John and Mary came, 
the NP John and Mary is a constituent in Tree-
bank and it is also marked as Arg0 in PropBank. 
However, there are a few cases where one of the 
conjuncts is modified, and PropBank policy is to 
mark these modifiers as ArgMs. For example, in 
the following NP, the temporal ArgM now modi-
fies a verb, but it only applies to the second con-
junct.  
 
(26) 
(NP (NNP Richard)
(NNP Thornburgh) )
(, ,)
(SBAR
(WHNP-164 (WP who))
(S
(NP-SBJ-1 (-NONE- *T*-164))
(VP
(VBD went)
(PRT (RP on) )
(S
(NP-SBJ (-NONE- *-1))
(VP (TO to)
(VP (VB[rel] become)
(NP-PRD
(NP[ARG2]
(NP (NN governor))
(PP (IN of)
(NP
(NNP
Pennsylvania))))
76
(CC and)
(PRN (, ,)
(ADVP-TMP (RB now))
(, ,) )
(NP[ARG2] (NNP U.S.)
(NNP Attorney)
(NNP General))
)))))))
 
In PropBank, cases like this can be decom-
posed into two propositions: 
 
(27) Prop1:      rel: become    
                Arg1: attorney general         
                Arg0: [-NONE- *-1]                       
         
   Prop2: rel:  become    
  ArgM-TMP: now   
  Arg0: [-NONE- *-1] 
Arg1: a governor               
 
In Treebank, the conjoined NP is necessarily 
analyzed as one constituent. In order to maintain 
the one-to-one mapping between PropBank and 
Treebank, PropBank annotation would have to 
be revised in order to allow the sentence to have 
one proposition with a conjoined phrase as an 
argument. Fortunately, these types of cases do 
not occur frequently in the corpus. 
4.3 Gapping 
Another place where the one-to-one mapping 
is difficult to preserve is with gapping construc-
tions. Treebank annotation does not annotate the 
gap, given that gaps might correspond to differ-
ent syntactic categories or may not even be a 
constituent. The policy of Treebank, therefore, is 
simply to provide a coindexation link between 
the corresponding constituents:  
 
(28) Mary-1 likes chocolates-2 and  
 Jane=1 ? flowers=2 
 
This policy obviously presents a problem for 
one-to-one mapping, since Propbank annotators 
tag Jane and flowers as the arguments of an im-
plied second likes relation, which is not present 
in the sentence. 
5 Summary 
In this paper we have considered several types 
of mismatches between the annotations of the 
English Treebank and the PropBank: coreference 
and syntactic chains, differences in syntactic 
constituency, and cases in which syntax and se-
mantics do not match. We have found that for the 
most part, such mismatches arise because Tree-
bank decisions are based primarily  on syntactic 
considerations while PropBank decisions give 
more weight  to semantic representation.. 
In order to reconcile these differences we have 
revised the annotation policies of both the Prop-
Bank and Treebank in appropriate ways. A 
fourth source of mismatches is simply annotation 
error in either the Treebank or PropBank. Look-
ing at the mismatches in general has allowed us 
to find these errors, and will facilitate their cor-
rection.  
References 
Olga Babko-Malaya. 2005. PropBank Annotation 
Guidelines. http://www.cis.upenn.edu/~mpalmer/ 
project_pages/PBguidelines.pdf 
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre. 1995. Bracketing Guidelines for Treebank 
II Style. Penn Treebank Project, University of 
Pennsylvania, Department of Computer and Infor-
mation Science Technical Report MS-CIS-95-06. 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambigua-
tion of English Verbs Using Rich Linguistic Fea-
tures. In Proceedings of the 2nd International Joint 
Conference on Natural Language Processing, 
IJCNLP2005, pp. 933-944. Oct. 11-13, Jeju Island, 
Republic of Korea. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz & B. Schas-
berger, 1994. The Penn Treebank: Annotating 
predicate argument structure. Proceedings of the 
Human Language Technology Workshop, San 
Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics, 
31(1). 
R. Quirk, S. Greenbaum, G. Leech and J. Svartvik. 
1985. A Comprehensive Grammar of the English 
Language. Longman, London. 
B. Santorini. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Penn-
sylvania, Department of Computer and Information 
Science Technical Report MS-CIS-90-47. 
77
