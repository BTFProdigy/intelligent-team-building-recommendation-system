Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660?664,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Does Size Matter ? How Much Data is Required to Train a REG Algorithm?
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Abstract
In this paper we investigate how much data
is required to train an algorithm for attribute
selection, a subtask of Referring Expressions
Generation (REG). To enable comparison be-
tween different-sized training sets, a system-
atic training method was developed. The re-
sults show that depending on the complexity
of the domain, training on 10 to 20 items may
already lead to a good performance.
1 Introduction
There are many ways in which we can refer to ob-
jects and people in the real world. A chair, for ex-
ample, can be referred to as red, large, or seen from
the front, while men may be singled out in terms
of their pogonotrophy (facial hairstyle), clothing and
many other attributes. This poses a problem for al-
gorithms that automatically generate referring ex-
pressions: how to determine which attributes to use?
One solution is to assume that some attributes
are preferred over others, and this is indeed what
many Referring Expressions Generation (REG) al-
gorithms do. A classic example is the Incremental
Algorithm (IA), which postulates the existence of
a complete ranking of relevant attributes (Dale and
Reiter, 1995). The IA essentially iterates through
this list of preferred attributes, selecting an attribute
for inclusion in a referring expression if it helps sin-
gling out the target from the other objects in the
scene (the distractors). Crucially, Dale and Reiter do
not specify how the ranking of attributes should be
determined. They refer to psycholinguistic research
suggesting that, in general, absolute attributes (such
as color) are preferred over relative ones (such as
size), but stress that constructing a preference order
is essentially an empirical question, which will dif-
fer from one domain to another.
Many other REG algorithms similarly rely on
preferences. The graph-based based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, with cheaper properties be-
ing more preferred. Various ways to compute costs
are possible; they can be defined, for instance, in
terms of log probabilities, which makes frequently
encountered properties cheap, and infrequent ones
more expensive. Krahmer et al (2008) argue that
a less fine-grained cost function might generalize
better, and propose to use frequency information
to, somewhat ad hoc, define three costs: 0 (free),
1 (cheap) and 2 (expensive). This approach was
shown to work well: the graph-based algorithm was
the best performing system in the most recent REG
Challenge (Gatt et al, 2009).
Many other attribute selection algorithms also
rely on training data to determine preferences in one
form or another (Fabbrizio et al, 2008; Gerva?s et
al., 2008; Kelleher, 2007; Spanger et al, 2008; Vi-
ethen and Dale, 2010). Unfortunately, suitable data
is hard to come by. It has been argued that determin-
ing which properties to include in a referring expres-
sion requires a ?semantically transparent? corpus
(van Deemter et al, 2006): a corpus that contains
the actual properties of all domain objects as well
as the properties that were selected for inclusion in
a given reference to the target. Obviously, text cor-
pora tend not to meet this requirement, which is why
660
semantically transparent corpora are often collected
using human participants who are asked to produce
referring expressions for targets in controlled visual
scenes for a given domain. Since this is a time con-
suming exercise, it will not be surprising that such
corpora are thin on the ground (and are often only
available for English). An important question there-
fore is how many human-produced references are
needed to achieve a certain level of performance. Do
we really need hundreds of instances, or can we al-
ready make informed decisions about preferences on
a few or even one training instance?
In this paper, we address this question by sys-
tematically training the graph-based REG algorithm
on a number of ?semantically transparent? data sets
of various sizes and evaluating on a held-out test
set. The graph-based algorithm seems a good can-
didate for this exercise, in view of its performance
in the REG challenges. For the sake of compari-
son, we also follow the evaluation methodology of
the REG challenges, training and testing on two do-
mains (a furniture and a people domain), and using
two automatic metrics (Dice and accuracy) to mea-
sure human-likeness. One hurdle needs to be taken
beforehand. Krahmer et al (2008) manually as-
signed one of three costs to properties, loosely based
on corpus frequencies. For our current evaluation
experiments, this would hamper comparison across
data sets, because it is difficult to do it in a manner
that is both consistent and meaningful. Therefore we
first experiment with a more systematic way of as-
signing a limited number of frequency-based costs
to properties using k-means clustering.
2 Experiment I: k-means clustering costs
In this section we describe our experiment with k-
means clustering to derive property costs from En-
glish and Dutch corpus data. For this experiment we
looked at both English and Dutch, to make sure the
chosen method does not only work well for English.
2.1 Materials
Our English training and test data were taken from
the TUNA corpus (Gatt et al, 2007). This semanti-
cally transparent corpus contains referring expres-
sions in two domains (furniture and people), col-
lected in one of two conditions: in the -LOC con-
dition, participants were discouraged from mention-
ing the location of the target in the visual scene,
whereas in the +LOC condition they could mention
any properties they wanted. The TUNA corpus was
used for comparative evaluation in the REG Chal-
lenges (2007-2009). For training in our current ex-
periment, we used the -LOC data from the training
set of the REG Challenge 2009 (Gatt et al, 2009):
165 furniture descriptions and 136 people descrip-
tions. For testing, we used the -LOC data from the
TUNA 2009 development set: 38 furniture descrip-
tions and 38 people descriptions.
Dutch data were taken from the D-TUNA corpus
(Koolen and Krahmer, 2010). This corpus uses the
same visual scenes and annotation scheme as the
TUNA corpus, but with Dutch instead of English
descriptions. D-TUNA does not include locations as
object properties at all, hence our restriction to -LOC
data for English (to make the Dutch and English data
more comparable). As Dutch test data, we used 40
furniture items and 40 people items, randomly se-
lected from the textual descriptions in the D-TUNA
corpus. The remaining furniture and people descrip-
tions (160 items each) were used for training.
2.2 Method
We first determined the frequency with which each
property was mentioned in our training data, relative
to the number of target objects with this property.
Then we created different cost functions (mapping
properties to costs) by means of k-means clustering,
using the Weka toolkit. The k-means clustering al-
gorithm assigns n points in a vector space to k clus-
ters (S1 to Sk) by assigning each point to the clus-
ter with the nearest centroid. The total intra-cluster
variance V is minimized by the function
V =
k
?
i=1
?
xj?Si
(xj ? ?i)2
where ?i is the centroid of all the points xj ? Si.
In our case, the points n are properties, the vector
space is one-dimensional (frequency being the only
dimension) and ?i is the average frequency of the
properties in Si. The cluster-based costs are defined
as follows:
?xj ? Si, cost(xj) = i? 1
661
where S1 is the cluster with the most frequent
properties, S2 is the cluster with the next most fre-
quent properties, and so on. Using this approach,
properties from cluster S1 get cost 0 and thus can be
added ?for free? to a description. Free properties are
always included, provided they help distinguish the
target. This may lead to overspecified descriptions,
mimicking the human tendency to mention redun-
dant properties (Dale and Reiter, 1995).
We ran the clustering algorithm on our English
and Dutch training data for up to six clusters (k = 2
to k = 6). Then we evaluated the performance of
the resulting cost functions on the test data from
the same language, using Dice (overlap between at-
tribute sets) and Accuracy (perfect match between
sets) as evaluation metrics. For comparison, we also
evaluated the best scoring cost functions from Theu-
ne et al (2010) on our test data. These ?Free-Na??ve?
(FN) functions were created using the manual ap-
proach sketched in the introduction.
The order in which the graph-based algorithm
tries to add attributes to a description is explicitly
controlled to ensure that ?free? distinguishing prop-
erties are included (Viethen et al, 2008). In our
tests, we used an order of decreasing frequency; i.e.,
always examining more frequent properties first.1
2.3 Results
For the cluster-based cost functions, the best perfor-
mance was achieved with k = 2, for both domains
and both languages. Interestingly, this is the coarsest
possible k-means function: with only two costs (0
and 1) it is even less fine-grained than the FN func-
tions advocated by Krahmer et al (2008). The re-
sults for the k-means costs with k = 2 and the FN
costs of Theune et al (2010) are shown in Table 1.
No significant differences were found, which sug-
gests that k-means clustering, with k = 2, can be
used as a more systematic alternative for the manual
assignment of frequency-based costs. We therefore
applied this method in the next experiment.
3 Experiment II: varying training set size
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
1We used slightly different property orders than Theune et
al. (2010), leading to minor differences in our FN results.
Furniture People
Language Costs Dice Acc. Dice Acc.
English k-means 0.810 0.50 0.733 0.29
FN 0.829 0.55 0.733 0.29
Dutch k-means 0.929 0.68 0.812 0.33
FN 0.929 0.68 0.812 0.33
Table 1: Results for k-means costs with k = 2 and the
FN costs of Theune et al (2010) on Dutch and English.
mance, in the second experiment we derived cost
functions and property orders from different sized
training sets, and evaluated them on our test data.
For this experiment, we only used English data.
3.1 Materials
As training sets, we used randomly selected subsets
of the full English training set from Experiment I,
with set sizes of 1, 5, 10, 20 and 30 items. Be-
cause the accidental composition of a training set
may strongly influence the results, we created 5 dif-
ferent sets of each size. The training sets were built
up in a cumulative fashion: we started with five sets
of size 1, then added 4 items to each of them to cre-
ate five sets of size 5, etc. This resulted in five series
of increasingly sized training sets. As test data, we
used the same English test set as in Experiment I.
3.2 Method
We derived cost functions (using k-means clustering
with k = 2) and orders from each of the training
sets, following the method described in Section 2.2.
In doing so, we had to deal with missing data: not all
properties were present in all data sets.2 For the cost
functions, we simply assigned the highest cost (1)
to the missing properties. For the order, we listed
properties with the same frequency (0 for missing
properties) in alphabetical order. This was done for
the sake of comparability between training sets.
3.3 Results
To determine significance, we calculated the means
of the scores of the five training sets for each set
size, so that we could compare them with the scores
of the entire set. We applied repeated measures of
2This problem mostly affected the smaller training sets. By
set size 10 only a few properties were missing, while by set size
20, all properties were present in all sets.
662
variance (ANOVA) to the Dice and Accuracy scores,
using set size (1, 5, 10, 20, 30, entire set) as a within
variable. The mean results for each training set size
are shown in Table 2.3 The general pattern is that
the scores increase with the size of the training set,
but the increase gets smaller as the set sizes become
larger.
Furniture People
Set size Dice Acc. Dice Acc.
1 0.693 0.25 0.560 0.13
5 0.756 0.34 0.620 0.15
10 0.777 0.40 0.686 0.20
20 0.788 0.41 0.719 0.25
30 0.782 0.41 0.718 0.27
Entire set 0.810 0.50 0.733 0.29
Table 2: Mean results for the different set sizes.
In the furniture domain, we found a main effect
of set size (Dice: F(5,185) = 7.209, p < .001; Ac-
curacy: F(5,185) = 6.140, p < .001). To see which
set sizes performed significantly different as com-
pared to the entire set, we conducted Tukey?s HSD
post hoc comparisons. For Dice, the scores of set
size 10 (p = .141), set size 20 (p = .353), and set
size 30 (p = .197) did not significantly differ from
the scores of the entire set of 165 items. The Accu-
racy scores in the furniture domain show a slightly
different pattern: the scores of the entire training set
were still significantly higher than those of set size
30 (p < .05). This better performance when trained
on the entire set may be caused by the fact that not
all of the five training sets that were used for set sizes
1, 5, 10, 20 and 30 performed equally well.
In the people domain we also found a main effect
of set size (Dice: F(5,185) = 21.359, p < .001; Accu-
racy: F(5,185) = 8.074, p < .001). Post hoc pairwise
comparisons showed that the scores of set size 20
(Dice: p = .416; Accuracy: p = .146) and set size
30 (Dice: p = .238; Accuracy: p = .324) did not
significantly differ from those of the full set of 136
items.
3For comparison: in the REG Challenge 2008, (which in-
volved a different test set, but the same type of data), the best
systems obtained overall Dice and accuracy scores of around
0.80 and 0.55 respectively (Gatt et al, 2008). These scores may
well represent the performance ceiling for speaker and context
independent algorithms on this task.
4 Discussion
Experiment II has shown that when using small data
sets to train an attribute selection algorithm, results
can be achieved that are not significantly different
from those obtained using a much larger training
set. Domain complexity appears to be a factor in
how much training data is needed: using Dice as an
evaluation metric, training sets of 10 sufficed in the
simple furniture domain, while in the more complex
people domain it took a set size of 20 to achieve re-
sults that do not significantly differ from those ob-
tained using the full training set.
The accidental composition of the training sets
may strongly influence the attribute selection per-
formance. In the furniture domain, we found clear
differences between the results of specific training
sets, with ?bad sets? pulling the overall performance
down. This affected Accuracy but not Dice, possibly
because the latter is a less strict metric.
Whether the encouraging results found for the
graph-based algorithm generalize to other REG ap-
proaches is still an open question. We also need
to investigate how the use of small training sets af-
fects effectiveness and efficiency of target identifica-
tion by human subjects; as shown by Belz and Gatt
(2008), task-performance measures do not necessar-
ily correlate with similarity measures such as Dice.
Finally, it will be interesting to repeat Experiment II
with Dutch data. The D-TUNA data are cleaner than
the TUNA data (Theune et al, 2010), so the risk of
?bad? training data will be smaller, which may lead
to more consistent results across training sets.
5 Conclusion
Our experiment has shown that with 20 or less train-
ing instances, acceptable attribute selection results
can be achieved; that is, results that do not signif-
icantly differ from those obtained using the entire
training set. This is good news, because collecting
such small amounts of training data should not take
too much time and effort, making it relatively easy
to do REG for new domains and languages.
Acknowledgments
Krahmer and Koolen received financial support from
The Netherlands Organization for Scientific Re-
search (Vici grant 27770007).
663
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation re-
sults. In Proceedings of the 5th International Natural
Language Generation Conference (INLG 2008), pages
198?206.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Emiel Krahmer, Marie?t Theune, Jette Viethen, and Iris
Hendrickx. 2008. GRAPH: The costs of redundancy
in referring expressions. In Proceedings of the 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 227?229.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, and Emiel Krahmer. 2010.
Cross-linguistic attribute selection for REG: Compar-
ing Dutch and English. In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG 2010), pages 174?182.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
664
Cross-Linguistic Attribute Selection for REG:
Comparing Dutch and English
Marie?t Theune
University of Twente
The Netherlands
M.Theune@utwente.nl
Ruud Koolen
Tilburg University
The Netherlands
R.M.F.Koolen@uvt.nl
Emiel Krahmer
Tilburg University
The Netherlands
E.J.Krahmer@uvt.nl
Abstract
In this paper we describe a cross-linguistic
experiment in attribute selection for refer-
ring expression generation. We used a
graph-based attribute selection algorithm
that was trained and cross-evaluated on
English and Dutch data. The results indi-
cate that attribute selection can be done in
a largely language independent way.
1 Introduction
A key task in natural language generation is refer-
ring expression generation (REG). Most work on
REG is aimed at producing distinguishing descrip-
tions: descriptions that uniquely characterize a tar-
get object in a visual scene (e.g., ?the red sofa?),
and do not apply to any of the other objects in the
scene (the distractors). The first step in generating
such descriptions is attribute selection: choosing a
number of attributes that uniquely characterize the
target object. In the next step, realization, the se-
lected attributes are expressed in natural language.
Here we focus on the attribute selection step. We
investigate to which extent attribute selection can
be done in a language independent way; that is,
we aim to find out if attribute selection algorithms
trained on data from one language can be success-
fully applied to another language. The languages
we investigate are English and Dutch.
Many REG algorithms require training data, be-
fore they can successfully be applied to generate
references in a particular domain. The Incremen-
tal Algorithm (Dale and Reiter, 1995), for exam-
ple, assumes that certain attributes are more pre-
ferred than others, and it is assumed that determin-
ing the preference order of attributes is an empir-
ical matter that needs to be settled for each new
domain. The graph-based algorithm (Krahmer et
al., 2003), to give a second example, similarly
assumes that certain attributes are preferred (are
?cheaper?) than others, and that data are required
to compute the attribute-cost functions.
Traditional text corpora have been argued to be
of restricted value for REG, since these typically
are not ?semantically transparent? (van Deemter
et al, 2006). Rather what seems to be needed is
data collected from human participants, who pro-
duce referring expressions for specific targets in
settings where all properties of the target and its
distractors are known. Needless to say, collecting
and annotating such data takes a lot of time and ef-
fort. So what to do if one wants to develop a REG
algorithm for a new language? Would this require
a new data collection, or could existing data col-
lected for a different language be used? Clearly,
linguistic realization is language dependent, but to
what extent is attribute selection language depen-
dent? This is the question addressed in this paper.
Below we describe the English and Dutch cor-
pora used in our experiments (Section 2), the
graph-based algorithm we used for attribute se-
lection (Section 3), and the corpus-based attribute
costs and orders used by the algorithm (Section 4).
We present the results of our cross-linguistic at-
tribute selection experiments (Section 5) and end
with a discussion and conclusions (Section 6).
2 Corpora
2.1 English: the TUNA Corpus
For English data, we used the TUNA corpus of
object descriptions (Gatt et al, 2007). This cor-
pus was created by presenting the participants in
an on-line experiment with a visual scene consist-
ing of seven objects and asking them to describe
one of the objects, the target, in such a way that it
could be uniquely identified. There were two ex-
perimental conditions: in the +LOC condition, the
participants were free to describe the target object
using any of its properties, including its location
on the screen, whereas in the -LOC condition they
were discouraged (but not prevented) from men-
tioning object locations. The resulting object de-
scriptions were annotated using XML and com-
bined with an XML representation of the visual
scene, listing all objects and their properties in
terms of attribute-value pairs. The TUNA corpus
is split into two domains: one with descriptions of
furniture and one with descriptions of people.
The TUNA corpus was used for the comparative
evaluation of REG systems in the TUNA Chal-
lenges (2007-2009). For our current experiments,
we used the TUNA 2008 Challenge training and
development sets (Gatt et al, 2008) to train and
evaluate the graph-based algorithm on.
2.2 Dutch: the D-TUNA Corpus
For Dutch, we used the D(utch)-TUNA corpus of
object descriptions (Koolen and Krahmer, 2010).
The collection of this corpus was inspired by the
TUNA experiment described above, and was done
using the same visual scenes. There were three
conditions: text, speech and face-to-face. The
text condition was a replication (in Dutch) of the
TUNA experiment: participants typed identify-
ing descriptions of target referents, distinguishing
them from distractor objects in the scene. In the
other two conditions participants produced spo-
ken descriptions for an addressee, who was either
visible to the speaker (face-to-face condition) or
not (speech condition). The resulting descriptions
were annotated semantically using the XML anno-
tation scheme of the English TUNA corpus.
The procedure in the D-TUNA experiment dif-
fered from that used in the original TUNA exper-
iment in two ways. First, the D-TUNA experi-
ment used a laboratory-based set-up, whereas the
TUNA study was conducted on-line in a relatively
uncontrolled setting. Second, participants in the
D-TUNA experiment were completely prevented
from mentioning object locations.
3 Graph-Based Attribute Selection
For attribute selection, we use the graph-based al-
gorithm of Krahmer et al (2003), one of the
highest scoring attribute selection methods in the
TUNA 2008 Challenge (Gatt et al (2008), table
11). In this approach, a visual scene with tar-
get and distractor objects is represented as a la-
belled directed graph, in which the objects are
modelled as nodes and their properties as looping
edges on the corresponding nodes. To select the
attributes for a distinguishing description, the al-
gorithm searches for a subgraph of the scene graph
that uniquely refers to the target referent. Starting
from the node representing the target, it performs a
depth-first search over the edges connected to the
subgraph found so far. The algorithm?s output is
the cheapest distinguishing subgraph, given a par-
ticular cost function that assigns costs to attributes.
By assigning zero costs to some attributes, e.g.,
the type of an object, the human tendency to men-
tion redundant attributes can be mimicked. How-
ever, as shown by Viethen et al (2008), merely
assigning zero costs to an attribute is not a suffi-
cient condition for inclusion; if the graph search
terminates before the free attributes are tried, they
will not be included. Therefore, the order in which
attributes are tried must be explicitly controlled.
Thus, when using the graph-based algorithm for
attribute selection, two things must be specified:
(1) the cost function, and (2) the order in which the
attributes should be searched. Both can be based
on corpus data, as described in the next section.
4 Costs and Orders
For our experiments, we used the graph-based at-
tribute selection algorithm with two types of cost
functions: Stochastic costs and Free-Na??ve costs.
Both reflect (to a different extent) the relative at-
tribute frequencies found in a training corpus: the
more frequently an attribute occurs in the training
data, the cheaper it is in the cost functions.
Stochastic costs are directly based on the at-
tribute frequencies in the training corpus. They
are derived by rounding ?log2(P (v)) to the first
decimal and multiplying by 10, where P (v) is the
probability that attribute v occurs in a description,
given that the target object actually has this prop-
erty. The probability P (v) is estimated by deter-
mining the frequency of each attribute in the train-
ing corpus, relative to the number of target ob-
jects that possess this attribute. Free-Na??ve costs
more coarsely reflect the corpus frequencies: very
frequent attributes are ?free? (cost 0), somewhat
frequent attributes have cost 1 and infrequent at-
tributes have cost 2. Both types of cost functions
are used in combination with a stochastic ordering,
where attributes are tried in the order of increasing
stochastic costs.
In total, four cost functions were derived from
the English corpus data and four cost functions de-
rived from the Dutch corpus data. For each lan-
guage, we had two Stochastic cost functions (one
for the furniture domain and one for the people do-
main), and two Free-Na??ve cost functions (idem),
giving eight different cost functions in total. For
each language we determined two attribute orders
to be used with the cost functions: one for the fur-
niture domain and one for the people domain.
4.1 English Costs and Order
For English, we used the Stochastic and Free-
Na??ve cost functions and the stochastic order from
Krahmer et al (2008). The Stochastic costs
and order were derived from the attribute frequen-
cies in the combined training and development
sets of the TUNA 2008 Challenge (Gatt et al,
2008), containing 399 items in the furniture do-
main and 342 items in the people domain. The
Free-Na??ve costs are simplified versions of the
stochastic costs. ?Free? attributes are TYPE in
both domains, COLOUR for the furniture domain
and HASBEARD and HASGLASSES for the people
domain. Expensive attributes (cost 2) are X- and
Y-DIMENSION in the furniture domain and HAS-
SUIT, HASSHIRT and HASTIE in the people do-
main. All other attributes have cost 1.
4.2 Dutch Costs and Order
The Dutch Stochastic costs and order were de-
rived from the attribute frequencies in a set of 160
items (for both furniture and people) randomly se-
lected from the text condition in the D-TUNA cor-
pus. Interestingly, our Stochastic cost computa-
tion method led to an assignment of 0 costs to
the COLOUR attribute in the furniture domain, thus
enabling the Dutch Stochastic cost function to in-
clude colour as a redundant property in the gener-
ated descriptions. In the English stochastic costs,
none of the attributes are free. Another difference
is that in the furniture domain, the Dutch stochas-
tic costs for ORIENTATION attributes are much
lower than the English costs (except with value
FRONT); in the people domain, the same holds for
attributes such as HASSUIT and HASTIE. These
cost differences, which are largely reflected in the
Dutch Free-Na??ve costs, do not seem to be caused
by differences in expressibility, i.e., the ease with
which the attributes can be expressed in the two
languages (Koolen et al, 2010); rather, they may
be due to the fact that the human descriptions in D-
TUNA do not include any DIMENSION attributes.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.92 0.63 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.87 0.58 0.75 0.25
English 0.67 0.29 0.67 0.24
Table 1: Evaluation results for stochastic costs.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
Table 2: Evaluation results for Free-Na??ve costs.
5 Results
All cost functions were applied to both Dutch and
English test data. As Dutch test data, we used a set
of 40 furniture items and a set of 40 people items,
randomly selected from the text condition in the
D-TUNA corpus. These items had not been used
for training the Dutch cost functions. As English
test data, we used a subset of the TUNA 2008 de-
velopment set (Gatt et al, 2008). To make the En-
glish test data comparable to the Dutch ones, we
only included items from the -LOC condition (see
Section 2.1). This resulted in 38 test items for the
furniture domain, and 38 for the people domain.
Tables 1 and 2 show the results of applying the
Dutch and English cost functions (with Dutch and
English attribute orders respectively) to the Dutch
and English test data. The evaluation metrics used,
Dice and Accuracy (Acc.), both evaluate human-
likeness by comparing the automatically selected
attribute sets to those in the human test data. Dice
is a set-comparison metric ranging between 0 and
1, where 1 indicates a perfect match between sets.
Accuracy is the proportion of system outputs that
exactly match the corresponding human data. The
results were computed using the ?teval? evaluation
tool provided to participants in the TUNA 2008
Challenge (Gatt et al, 2008).
To determine significance, we applied repeated
measures analyses of variance (ANOVA) to the
evaluation results, with three within factors: train-
ing language (Dutch or English), cost function
(Stochastic or Free-Na??ve), and domain (furniture
or people), and one between factor representing
test language (Dutch or English).
An overall effect of cost function shows that the
Free-Na??ve cost functions generally perform better
than the Stochastic cost functions (Dice: F(1,76) =
34.853, p < .001; Accuracy: F(1,76) = 13.052, p =
.001). Therefore, in the remainder of this section
we mainly focus on the results for the Free-Na??ve
cost functions (Table 2).
As can be clearly seen in Table 2, Dutch and
English Free-Na??ve cost functions give almost the
same scores in both the furniture and the people
domain, when applied to the same test language.
The English Free-Na??ve cost function performs
slightly better than the Dutch one on the Dutch
people data, but this difference is not significant.
An overall effect of test language shows that the
cost functions (both Stochastic and Free-Na??ve)
generally give better Dice results on the Dutch
data than for the English data (Dice: F(1,76) =
7.797, p = .007). In line with this, a two-way in-
teraction between test language and training lan-
guage (Dice: F(1,76) = 6.870, p = .011) shows that
both the Dutch and the English cost functions per-
form better on the Dutch data than on the English
data. However, the overall effect of test language
did not reach significance for Accuracy, presum-
ably due to the fact that the Accuracy scores on the
English people data are slightly higher than those
on the Dutch people data.
Finally, the cost functions generally perform
better in the furniture domain than in the people
domain (Dice: F(1,76) = 10.877, p = .001; Accu-
racy: F(1,76) = 16.629, p < .001).
6 Discussion
The results of our cross-linguistic attribute selec-
tion experiments show that Free-Na??ve cost func-
tions, which only roughly reflect the attribute fre-
quencies in the training corpus, have an overall
better performance than Stochastic cost functions,
which are directly based on the attribute frequen-
cies. This holds across the two languages we in-
vestigated, and corresponds with the findings of
Krahmer et al (2008), who compared Stochas-
tic and Free-Na??ve functions that were trained and
evaluated on English data only. The difference in
performance is probably due to the fact that Free-
Na??ve costs are less sensitive to the specifics of
the training data (and are therefore more generally
applicable) and do a better job of mimicking the
human tendency towards redundancy.
Moreover, we found that Free-Na??ve cost func-
tions trained on different languages (English or
Dutch) performed equally well when tested on the
same data (English or Dutch), in both the furniture
and people domain. This suggests that attribute
selection can in fact be done in a language inde-
pendent way, using cost functions that have been
derived from corpus data in one language to per-
form attribute selection for another language.
Our results did show an effect of test language
on performance: both English and Dutch cost
functions performed better when tested on the
Dutch D-TUNA data than on the English TUNA
data. However, this difference does not seem to
be caused by language-specific factors but rather
by the quality of the respective test sets. Although
the English test data were restricted to the -LOC
condition, in which using DIMENSION attributes
was discouraged, still more than 25% of the En-
glish test data (both furniture and people) included
one or more DIMENSION attributes, which were
never selected for inclusion by either the English
or the Dutch Free-Na??ve cost functions. The Dutch
test data, on the other hand, did not include any
DIMENSION attributes. In addition, the English
test data contained more non-unique descriptions
of target objects than the Dutch data, in particu-
lar in the furniture domain. These differences may
be due to the fact that data collection was done
in a more controlled setting for D-TUNA than for
TUNA. In other words, the seeming effect of test
language does not contradict our main conclusion
that attribute selection is largely language inde-
pendent, at least for English and Dutch.
The success of our cross-linguistic experiments
may have to do with the fact that English and
Dutch hardly differ in the expressibility of object
attributes (Koolen et al, 2010). To determine the
full extent to which attribute selection can be done
in a language-dependent way, additional experi-
ments with less similar languages are necessary.
Acknowledgements
We thank the TUNA Challenge organizers for the
English data and the evaluation tool used in our
experiments; Martijn Goudbeek for helping with
the statistical analysis; and Pascal Touset, Ivo
Brugman, Jette Viethen, and Iris Hendrickx for
their contributions to the graph-based algorithm.
This research is part of the VICI project ?Bridg-
ing the gap between psycholinguistics and com-
putational linguistics: the case of referring expres-
sions?, funded by the Netherlands Organization for
Scientific Research (NWO Grant 277-70-007).
References
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of refer-
ring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA Chal-
lenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Natural Lan-
guage Generation Conference (INLG 2008), pages
198?206.
R. Koolen and E. Krahmer. 2010. The D-TUNA cor-
pus: A Dutch dataset for the evaluation of referring
expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
R. Koolen, A. Gatt, M. Goudbeek, and E. Krahmer.
2010. Overspecification in referring expressions:
Causal factors and language differences. Submitted.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
E. Krahmer, M. Theune, J. Viethen, and I. Hendrickx.
2008. Graph: The costs of redundancy in refer-
ring expressions. In Proceedings of the 5th Inter-
national Natural Language Generation Conference
(INLG 2008), pages 227?229.
K. van Deemter, I. I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Natural Language Genera-
tion Conference (INLG 2006), pages 130?132.
J. Viethen, R. Dale, E. Krahmer, M. Theune, and
P. Touset. 2008. Controlling redundancy in refer-
ring expressions. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 3?11,
Utica, May 2012. c?2012 Association for Computational Linguistics
Learning Preferences for Referring Expression Generation:
Effects of Domain, Language and Algorithm
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Abstract
One important subtask of Referring Expres-
sion Generation (REG) algorithms is to se-
lect the attributes in a definite description for
a given object. In this paper, we study how
much training data is required for algorithms
to do this properly. We compare two REG al-
gorithms in terms of their performance: the
classic Incremental Algorithm and the more
recent Graph algorithm. Both rely on a notion
of preferred attributes that can be learned from
human descriptions. In our experiments, pref-
erences are learned from training sets that vary
in size, in two domains and languages. The
results show that depending on the algorithm
and the complexity of the domain, training on
a handful of descriptions can already lead to a
performance that is not significantly different
from training on a much larger data set.
1 Introduction
Most practical NLG systems include a dedicated
module for Referring Expression Generation (REG)
in one form or another (Mellish et al, 2006). One
central problem a REG module needs to address is
deciding on the contents of a description. Jordan
and Walker (2005), for example, studied human-
produced descriptions in a furniture scenario, and
found that speakers can refer to a target in many dif-
ferent ways (?the yellow rug?, ?the $150 rug?, etc.).
The question, then, is how speakers decide which at-
tributes to include in a description, and how this de-
cision process can be modeled in a REG algorithm.
When we focus on the generation of distinguish-
ing descriptions (which is often done in REG), it is
usually assumed that some attributes are more pre-
ferred than others: when trying to identify a chair,
for example, its colour is probably more helpful than
its size. It is precisely this intuition of preferred at-
tributes which is incorporated in the Incremental Al-
gorithm (Dale and Reiter, 1995), arguably one of the
most influential REG algorithms to date. The Incre-
mental Algorithm (IA) assumes the existence of a
complete, ordered list of preferred attributes. The
algorithm basically iterates through this list, adding
an attribute (e.g., COLOUR) to the description under
construction if its value (e.g., yellow) helps ruling
out one or more of the remaining distractors.
Even though the IA is exceptional in that it re-
lies on a complete ordering of attributes, most cur-
rent REG algorithms make use of preferences in
some way (Fabbrizio et al, 2008; Gerva?s et al,
2008; Kelleher, 2007; Spanger et al, 2008; Viethen
and Dale, 2010). The graph-based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, where cheaper is more pre-
ferred. Contrary to the IA, the graph-based algo-
rithm assumes that preferences operate at the level
of attribute-value pairs (or properties) rather than at
the level of attributes; in this way it becomes pos-
sible to prefer a straightforward size (large) over a
subtle colour (mauve, taupe). Moreover, the graph-
based algorithm looks for the cheapest overall de-
scription, and may opt for a description with a sin-
gle, relatively dispreferred property (?the man with
the blue eyes?) when the alternative would be to
combine many, relatively preferred properties (?the
large, balding man with the bow tie and the striped
tuxedo?). This flexibility is arguably one of the
3
reasons why the graph-based REG approach works
well: it was the best performing system in the most
recent REG Challenge (Gatt et al, 2009).
But where do the preferences used in the algo-
rithms come from? Dale and Reiter point out that
preferences are domain dependent, and that deter-
mining them for a given domain is essentially an
empirical question. Unfortunately, they do not spec-
ify how this particular empirical question should be
answered. The general preference for colour over
size is experimentally well-established (Pechmann,
1989), but for most other cases experimental data
are not readily available. An alternative would be
to look at human data, preferably in a ?semantically
transparent? corpus (van Deemter et al, 2006), that
is: a corpus that contains the attributes and values of
all domain objects, together with the attribute-value
pairs actually included in a target reference. Such
corpora are typically collected using human partic-
ipants, who are asked to produce referring expres-
sions for targets in controlled visual scenes. One
example is the TUNA corpus, which is a publicly
available data set containing 2280 human-produced
descriptions in total, and which formed the basis of
various REG Challenges. Clearly, building a corpus
such as TUNA is a time consuming and labour in-
tensive exercise, so it will not be surprising that only
a handful of such corpora exists (and often only for
English).
This raises an important question: how many
human-produced references are needed to make a
good estimate of which attributes and properties are
preferred? Do we really need hundreds of instances,
or is it conceivable that a few of them (collected in a
semantically transparent way) will do? This is not an
easy matter, since various factors might play a role:
from which data set are example references sampled,
what are the domains of interest, and, perhaps most
importantly, which REG algorithm is considered? In
this paper, we address these questions by systemati-
cally training two REG algorithms (the Incremental
Algorithm and the graph-based REG algorithm) on
sets of human-produced descriptions of increasing
size and evaluating them on a held-out test set; we
do this for two different domains (people and furni-
ture descriptions) and two data sets in two different
languages (TUNA and D-TUNA, the Dutch version
of TUNA).
That size of the training set may have an impact
on the performance of a REG algorithm was already
suggested by Theune et al (2011), who used the En-
glish TUNA corpus to determine preferences (costs)
for the graph-based algorithm using a similar learn-
ing curve set-up as we use here. However, the cur-
rent paper expands on Theune et al (2011) in three
major ways. Firstly, and most importantly, where
Theune et al reported results for only one algorithm
(the graph-based one), we directly compare the per-
formance of the graph-based algorithm and the In-
cremental Algorithm (something which, somewhat
surprisingly, has not been done before). Secondly,
we test whether these algorithms perform differently
in two different languages (English and Dutch), and
thirdly, we use eight training set sizes, which is more
than the six set sizes that were used by Theune et al
Below we first explain in more detail which algo-
rithms (Section 2) and corpora (Section 3) we used
for our experiments. Then we describe how we de-
rived costs and orders from subsets of these corpora
(Section 4), and report the results of our experiments
focusing on effects of domain, language and size
of the training set (Section 5). We end with a dis-
cussion and conclusion (Section 6), where we also
compare the performance of the IA trained on small
set sizes with that of the classical Full Brevity and
Greedy algorithms (Dale and Reiter, 1995).
2 The Algorithms
In this section we briefly describe the two algo-
rithms, and their settings, used in our experiment.
For details about these algorithms we refer to the
original publications.
The Incremental Algorithm (IA) The basic
assumption underlying the Incremental Algorithm
(Dale and Reiter, 1995) is that speakers ?prefer?
certain attributes over others when referring to
objects. This intuition is formalized in the notion
of a list of attributes, ranked in order of preference.
When generating a description for a target, the al-
gorithm iterates through this list, adding an attribute
to the description under construction if its value
helps rule out any of the distractors not previously
ruled out. There is no backtracking in the IA, which
means that a selected attribute is always realized in
4
the final description, even if the inclusion of later
attributes renders it redundant. In this way, the IA is
capable of generating overspecified descriptions, in
accordance with the human tendency to mention re-
dundant information (Pechmann, 1989; Engelhardt
et al, 2006; Arts et al, 2011). The TYPE attribute
(typically realized as the head noun) has a special
status in the IA. After running the algorithm it is
checked whether TYPE is in the description; if not,
it is added, so that TYPE is always included even if
it does not rule out any distractors.
To derive preference orders from human-
produced descriptions we proceeded as follows:
given a set of n descriptions sampled from a
larger corpus (where n is the set size, a variable
we systematically control in our experiment), we
counted the number of times a certain attribute
occurred in the n descriptions. The most frequently
occurring attribute was placed at the first position of
the preferred attributes list, followed by the second
most frequent attribute, etc. In the case of a tie (i.e.,
when two attributes occurred equally often, which
typically is more likely to happen in small training
sets), the attributes were ordered alphabetically. In
this way, we made sure that all ties were treated in
the same, comparable manner, which resulted in a
complete ranking of attributes, as required by the IA.
The Graph-based Algorithm (Graph) In the
graph-based algorithm (Krahmer et al, 2003),
which we refer to as Graph, information about
domain objects is represented as a labelled directed
graph, and REG is modeled as a graph-search
problem. The output of the algorithm is the
cheapest distinguishing subgraph, given a particular
cost function assigning costs to properties (i.e.,
attribute-value pairs). By assigning zero costs to
some properties Graph is also capable of generating
overspecified descriptions, including redundant
properties. To ensure that the graph search does not
terminate before the free properties are added, the
search order must be explicitly controlled (Viethen
et al, 2008). To ensure a fair comparison with the
IA, we make sure that if the target?s TYPE property
was not originally selected by the algorithm, it is
added afterwards.
In this study, both the costs and orders required
by Graph are derived from corpus data. We base
the property order on the frequency with which each
attribute-value pair is mentioned in a training cor-
pus, relative to the number of target objects with
this property. The properties are then listed in or-
der of decreasing frequency. Costs can be derived
from the same corpus frequencies; here, following
Theune et al (2011), we adopt a systematic way of
deriving costs from frequencies based on k-means
clustering. Theune and colleagues achieved the best
performance with k = 2, meaning that the prop-
erties are divided in two groups based on their fre-
quency. The properties in the group with the high-
est frequency get cost 0. These ?free? properties are
always included in the description if they help dis-
tinguish the target. The properties in the less fre-
quent group get cost 1; of these properties, the al-
gorithm only adds the minimum number necessary
to achieve a distinguishing description. Ties due to
properties occurring with the same frequency need
not be resolved when determining the cost function,
since Graph does not assume the existence of a com-
plete ordering. Properties that did not occur in a
training corpus were automatically assigned cost 1.
Like we did for the IA, we listed attribute-value pairs
with the same frequency in alphabetical order.
3 Corpora
Training and test data for our experiment were
taken from two corpora of referring expressions,
one English (TUNA) and one Dutch (D-TUNA).
TUNA The TUNA corpus (Gatt et al, 2007)
is a semantically transparent corpus consisting of
object descriptions in two domains (furniture and
people). The corpus was collected in an on-line
production experiment, in which participants were
presented with visual scenes containing one target
object and six distractor objects. These objects were
ordered in a 5 ? 3 grid, and the participants were
asked to describe the target in such a way that it
could be uniquely distinguished from its distractors.
Table 1 shows the attributes and values that were
annotated for the descriptions in the two domains.
There were two experimental conditions: in
the +LOC condition, the participants were free
to describe the target using any of its properties,
including its location on the screen (represented
5
Furniture
Attribute Possible values
TYPE chair, desk, sofa, fan
COLOUR green, red, blue, gray
ORIENTATION front, back, left, right
SIZE large, small
X-DIMENSION 1, 2, 3, 4, 5
Y-DIMENSION 1, 2, 3
People
Attribute Possible values
TYPE person
AGE old, young
HAIRCOLOUR light, dark
ORIENTATION front, left, right
HASBEARD true, false
HASGLASSES true, false
HASSHIRT true, false
HASSUIT true, false
HASTIE true, false
X-DIMENSION 1, 2, 3, 4, 5
Y-DIMENSION 1, 2, 3
Table 1: Attributes and values in the furniture and people
domains. X- and Y-DIMENSION refer to an object?s hori-
zontal and vertical position in a scene grid and only occur
in the English TUNA corpus.
in Table 1 as the X- and Y-DIMENSION), whereas
in the -LOC condition they were discouraged (but
not prevented) from mentioning object locations.
However, some descriptions in the -LOC condition
contained location information anyway.
D-TUNA For Dutch, we used the D-TUNA
corpus (Koolen and Krahmer, 2010). This corpus
uses the same visual scenes and annotation scheme
as the TUNA corpus, but consists of Dutch instead
of English target descriptions. Since the D-TUNA
experiment was performed in laboratory conditions,
its data is relatively ?cleaner? than the TUNA data,
which means that it contains fewer descriptions that
are not fully distinguishing and that its descriptions
do not contain X- and Y-DIMENSION attributes. Al-
though the descriptions in D-TUNA were collected
in three different conditions (written, spoken, and
face-to-face), we only use the written descriptions
in this paper, as this condition is most similar to the
data collection in TUNA.
4 Method
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
mance for the IA and Graph, we derived orders and
costs from different sized training sets. We then
evaluated the algorithms, using the derived orders
and costs, on a test set. Training and test sets were
taken from TUNA and D-TUNA.
As Dutch training data, we used 160 furniture and
160 people items, randomly selected from the tex-
tual descriptions in the D-TUNA corpus. The re-
maining furniture and people descriptions (40 items
each) were used for testing. As English training
data, we took all -LOC data from the training set
of the REG Challenge 2009 (Gatt et al, 2009): 165
furniture and 136 people descriptions. As English
test data we used all -LOC data from the REG 2009
development set: 38 furniture and 38 people descrip-
tions. We only used -LOC data to increase compa-
rability to the Dutch data.
From the Dutch and English furniture and people
training data, we selected random subsets of 1, 5,
10, 20, 30, 40 and 50 descriptions. Five different
sets of each size were created, since the accidental
composition of a training set could strongly influ-
ence the results. All training sets were built up in a
cumulative fashion, starting with five randomly se-
lected sets of size 1, then adding 4 items to each of
them to create five sets of size 5, and so on, for each
combination of language and domain. We used these
different training sets to derive preference orders of
attributes for the IA, and costs and property orders
for Graph, as outlined above.
We evaluated the performance of the derived pref-
erence orders and cost functions on the test data for
the corresponding domain and language, using the
standard Dice and Accuracy metrics for evaluation.
Dice measures the overlap between attribute sets,
producing a value between 1 and 0, where 1 stands
for a perfect match and 0 for no overlap at all. Ac-
curacy is the percentage of perfect matches between
the generated attribute sets and the human descrip-
tions in the test set. Both metrics were used in the
REG Generation Challenges.
6
English furniture
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.764 36.8 0.693 24.7
5 0.829 55.3 0.756 33.7
10 0.829 55.3 0.777 39.5
20 0.829 55.3 0.788 40.5
30 0.829 55.3 0.782 40.5
40 0.829 55.3 0.793 45.3
50 0.829 55.3 0.797 45.8
All 0.829 55.3 0.810 50.0
Dutch furniture
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.925 63.0 0.876 44.5
5 0.935 67.5 0.917 62.0
10 0.929 68.5 0.923 66.0
20 0.930 65.5 0.923 64.0
30 0.931 67.0 0.924 65.5
40 0.931 67.0 0.931 67.5
50 0.929 66.0 0.929 67.0
All 0.926 65.0 0.929 67.5
Table 2: Performance for each set size in the furniture
domain. For sizes 1 to 50, means over five sets are given.
The full sets are 165 English and 160 Dutch descriptions.
Note that the scores of the IA for the English sets of sizes
1 to 30 were also reported in Theune et al (2011).
5 Results
5.1 Overall analysis
To determine the effect of domain and language on
the performance of REG algorithms, we applied re-
peated measures analyses of variance (ANOVA) to
the Dice and Accuracy scores, using set size (1, 5,
10, 20, 30, 40, 50, all) and domain (furniture, peo-
ple) as within variables, and algorithm (IA, Graph)
and language (English, Dutch) as between variables.
The results show main effects of domain (Dice:
F(1,152) = 56.10, p < .001; Acc.: F(1,152) = 76.36,
p < .001) and language (Dice: F(1,152) = 30.30,
p < .001; Acc.: F(1,152) = 3.380, p = .07). Regard-
ing the two domains, these results indicate that both
the IA and the Graph algorithm generally performed
better in the furniture domain (Dice: M = .86, SD =
.01; Acc.: M = .56, SD = .03) than in the people do-
main (Dice: M = .72, SD = .01; Acc.: M = .20, SD =
.02). Regarding the two languages, the results show
that both algorithms generally performed better on
English people
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.519 7.4 0.558 12.6
5 0.605 15.8 0.617 14.5
10 0.682 21.1 0.683 20.0
20 0.710 22.1 0.716 24.7
30 0.682 15.3 0.716 26.8
40 0.716 26.3 0.723 26.3
50 0.718 27.9 0.727 26.3
All 0.724 31.6 0.730 28.9
Dutch people
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.626 4.5 0.682 17.5
5 0.737 16.0 0.738 21.0
10 0.738 12.5 0.741 19.5
20 0.765 12.5 0.778 25.5
30 0.762 14.5 0.789 25.0
40 0.763 11.5 0.792 25.0
50 0.764 10.5 0.798 26.0
All 0.775 12.5 0.812 32.5
Table 3: Performance for each set size in the people do-
main. For sizes 1 to 50, means over five sets are given.
The full sets are 136 English and 160 Dutch descriptions.
Note that the scores of the IA for the English sets of sizes
1 to 30 were also reported in Theune et al (2011).
the Dutch data (Dice: M = .84, SD = .01; Acc.: M
= .41, SD = .03) than on the English data (Dice: M
= .74, SD = .01; Acc.: M = .34, SD = .03). There
is no main effect of algorithm, meaning that over-
all, the two algorithms had an equal performance.
However, this is different when we look separately
at each domain and language, as we do below.
5.2 Learning curves per domain and language
Given the main effects of domain and language de-
scribed above, we ran separate ANOVAs for the dif-
ferent domains and languages. For these four analy-
ses, we used set size as a within variable, and algo-
rithm as a between variable. To determine the effects
of set size, we calculated the means of the scores
of the five training sets for each set size, so that we
could compare them with the scores of the entire set.
The results are shown in Tables 2 and 3.
We made planned post hoc comparisons to test
which is the smallest set size that does not perform
significantly different from the entire training set in
7
terms of Dice and Accuracy scores (we call this the
?ceiling?). We report results both for the standard
Bonferroni method, which corrects for multiple
comparisons, and for the less strict LSD method
from Fisher, which does not. Note that with the
Bonferroni method we are inherently less likely to
find statistically significant differences between the
set sizes, which implies that we can expect to reach
a ceiling earlier than with the LSD method. Table 4
shows the ceilings we found for the algorithms, per
domain and language.
The furniture domain Table 2 shows the Dice
and Accuracy scores in the furniture domain. We
found significant effects of set size for both the
English data (Dice: F(7,518) = 15.59, p < .001;
Acc.: F(7,518) = 17.42, p < .001) and the Dutch data
(Dice: F(7,546) = 5.322, p < .001; Acc.: F(7,546)
= 5.872, p < .001), indicating that for both lan-
guages, the number of descriptions used for training
influenced the performance of both algorithms in
terms of both Dice and Accuracy. Although we
did not find a main effect of algorithm, suggesting
that the two algorithms performed equally well, we
did find several interactions between set size and
algorithm for both the English data (Dice: F(7,518) =
1.604, ns; Acc.: F(7,518) = 2.282, p < .05) and the
Dutch data (Dice: F(7,546) = 3.970, p < .001; Acc.:
F(7,546) = 3.225, p < .01). For the English furniture
data, this interaction implies that small set sizes
have a bigger impact for the IA than for Graph.
For example, moving from set size 1 to 5 yielded a
Dice improvement of .18 for the IA, while this was
only .09 for Graph. For the Dutch furniture data,
however, a reverse pattern was observed; moving
from set size 1 to 5 yielded an improvement of .01
(Dice) and .05 (Acc.) for the IA, while this was .11
(Dice) and .18 (Acc.) for Graph.
Post hoc tests showed that small set sizes were
generally sufficient to reach ceiling performance:
the general pattern for both algorithms and both
languages was that the scores increased with the size
of the training set, but that the increase got smaller
as the set sizes became larger. For the English
furniture data, Graph reached the ceiling at set size
10 for Dice (5 with the Bonferroni test), and at set
size 40 for Accuracy (again 5 with Bonferroni),
while this was the case for the IA at set size 5 for
English furniture Dutch furniture
Dice Accuracy Dice Accuracy
IA 5 (5) 5 (5) 1 (1) 1 (1)
Graph 10 (5) 40 (5) 5 (1) 5 (1)
English people Dutch people
Dice Accuracy Dice Accuracy
IA 10 (10) 40 (1) 20 (5) 1 (1)
Graph 20 (10) 20 (1) 30 (20) 5 (1)
Table 4: Ceiling set sizes computed using LSD, with
Bonferroni between brackets.
both Dice and Accuracy (also 5 with Bonferroni).
For the Dutch furniture data, Graph reached the
ceiling at set size 5 for both Dice and Accuracy
(and even at 1 with the Bonferroni test), while this
was at set size 1 for the IA (again 1 with Bonferroni).
The people domain Table 3 shows the Dice
and Accuracy scores in the people domain. Again,
we found significant effects of set size for both the
English data (Dice: F(7,518) = 39.46, p < .001;
Acc.: F(7,518) = 11.77, p < .001) and the Dutch data
(Dice: F(7,546) = 33.90, p < .001; Acc.: F(7,546)
= 3.235, p < .01). Again, this implies that for
both languages, the number of descriptions used
for training influenced the performance of both
algorithms in terms of both Dice and Accuracy.
Unlike we did in the furniture domain, we found
no interactions between set size and algorithm, but
we did find a main effect of algorithm for the Dutch
people data (Dice: F(1,78) = .751, ns; Acc.: F(1,78)
= 5.099, p < .05), showing that Graph generated
Dutch descriptions that were more accurate than
those generated by the IA.
As in the furniture domain, post hoc tests showed
that small set sizes were generally sufficient to reach
ceiling performance. For the English data, Graph
reached the ceiling at set size 20 for both Dice and
Accuracy (with Bonferroni: 10 for Dice, 1 for Accu-
racy), while this was the case for the IA at set size 10
for Dice (also 10 with Bonferroni), and at set size 40
for Accuracy (and even at 1 with Bonferroni). For
the Dutch data, Graph reached the ceiling at set size
30 for Dice (20 with Bonferroni), and at set size 5
for Accuracy (1 with Bonferroni). For the IA, ceil-
ing was reached at set size 20 for Dice (Bonferroni:
5), and already at 1 for Accuracy (Bonferroni: 1).
8
6 Discussion and Conclusion
Our main goal was to investigate how many human-
produced references are required by REG algo-
rithms such as the Incremental Algorithm and the
graph-based algorithm to determine preferences (or
costs) for a new domain, and to generate ?human-
like? descriptions for new objects in these domains.
Our results show that small data sets can be used
to train these algorithms, achieving results that are
not significantly different from those derived from
a much larger training set. In the simple furniture
domain even one training item can already be suffi-
cient, at least for the IA. As shown in Table 4, on the
whole the IA needed fewer training data than Graph
(except in the English people domain, where Graph
only needed a set size of 10 to hit the ceiling for
Dice, while the IA needed a set size of 20).
Given that the IA ranks attributes, while the
graph-based REG algorithm ranks attribute-value
pairs, the difference in required training data is
not surprising. In any domain, there will be more
attribute-value pairs than attributes, so determining
an attribute ranking is an easier task than determin-
ing a ranking of attribute-value pairs. Another ad-
vantage of ranking attributes rather than attribute-
value pairs is that it is less vulnerable to the problem
of ?missing data?. More specifically, the chance that
a specific attribute does not occur in a small train-
ing set is much smaller than the chance that a spe-
cific attribute-value pair does not occur. As a conse-
quence, the IA needs fewer data to obtain complete
attribute orderings than Graph needs to obtain costs
for all attribute-value pairs.
Interestingly, we only found interactions between
training set size and algorithm in the furniture do-
main. In the people domain, there was no signifi-
cant difference between the size of the training sets
required by the algorithms. This could be explained
by the fact that the people domain has about twice as
many attributes as the furniture domain, and fewer
values per attribute (see Table 1). This means that
for people the difference between the number of at-
tributes (IA) and the number of attribute-value pairs
(Graph) is not as big as for furniture, so the two al-
gorithms are on more equal grounds.
Both algorithms performed better on furniture
than on people. Arguably, the people pictures in the
TUNA experiment can be described in many more
different ways than the furniture pictures can, so it
stands to reason that ranking potential attributes and
values is more difficult in the people than in the fur-
niture domain. In a similar vein, we might expect
Graph?s flexible generation strategy to be more use-
ful in the people domain, where more can be gained
by the use of costs, than in the furniture domain,
where there are relatively few options anyway, and a
simple linear ordering may be quite sufficient.
This expectation was at least partially confirmed
by the results: although in most cases the differences
are not significant, Graph tends to perform numeri-
cally better than the IA in the people domain. Here
we see the pay-off of Graph?s more fine-grained
preference ranking, which allows it to distinguish
between more and less salient attribute values. In the
furniture domain, most attribute values appear to be
more or less equally salient (e.g., none of the colours
gets notably mentioned more often), but in the peo-
ple domain certain values are clearly more salient
than others. In particular, the attributes HASBEARD
and HASGLASSES are among the most frequent at-
tributes in the people domain when their value is
true (i.e., the target object can be distinguished by
his beard or glasses), but they hardly get mentioned
when their value is false. Graph quickly learns this
distinction, assigning low costs and a high ranking
to <HASBEARD, true> and <HASGLASSES, true>
while assigning high costs and a low ranking to
<HASBEARD, false> and <HASGLASSES, false>.
The IA, on the other hand, does not distinguish be-
tween the values of these attributes.
Moreover, the graph-based algorithm is arguably
more generic than the Incremental Algorithm, as it
can straightforwardly deal with relational properties
and lends itself to various extensions (Krahmer et
al., 2003). In short, the larger training investment
required for Graph in simple domains may be com-
pensated by its versatility and better performance
on more complex domains. To test this assump-
tion, our experiment should be repeated using data
from a more realistic and complex domain, e.g., ge-
ographic descriptions (Turner et al, 2008). Unfortu-
nately, currently no such data sets are available.
Finally, we found that the results of both algo-
rithms were better for the Dutch data than for the
English ones. We think that this is not so much an ef-
9
fect of the language (as English and Dutch are highly
comparable) but rather of the way the TUNA and D-
TUNA corpora were constructed. The D-TUNA cor-
pus was collected in more controlled conditions than
TUNA and as a result, arguably, it contains training
data of a higher quality. Also, because the D-TUNA
corpus does not contain any location properties (X-
and Y-DIMENSION) its furniture and people domains
are slightly less complex than their TUNA counter-
parts, making the attribute selection task a bit easier.
One caveat of our study is that so far we have
only used the standard automatic metrics on REG
evaluation (albeit in accordance with many other
studies in this area). However, it has been found
that these do not always correspond to the results of
human-based evaluations, so it would be interesting
to see whether the same learning curve effects
are obtained for extrinsic, task based evaluations
involving human subjects. Following Belz and
Gatt (2008), this could be done by measuring
reading times, identification times or error rates as a
function of training set size.
Comparing IA with FB and GR We have shown
that small set sizes are sufficient to reach ceiling for
the IA. But which preference orders (PO?s) do we
find with these small set sizes? And how does the
IA?s performance with these orders compare to the
results obtained by alternative algorithms such as
Dale and Reiter?s (1995) classic Full Brevity (FB)
and Greedy Algorithm (GR)? ? a question explicitly
asked by van Deemter et al (2012). In the furniture
domain, all five English training sets of size 5 yield
a PO for which van Deemter et al showed that it
causes the IA to significantly outperform FB and
GR (i.e., either C(olor)O(rientation)S(ize) or CSO;
note that here we abstract over TYPE which van
Deemter and colleagues do not consider). When
we look at the English people domain and consider
set size 10 (ceiling for Dice), we find that four
out of five sets have a preference order where
HAIRCOLOUR, HASBEARD and HASGLASSES are
in the top three (again disregarding TYPE); one of
these is the best performing preference order found
by van Deemter and colleagues (GBH), another
performs slightly less well but still significantly
better than FB and GR (BGH); the other two score
statistically comparable to the classical algorithms.
The fifth people PO includes X- and Y-DIMENSION
in the top three, which van Deemter et al ignore. In
sum: in almost all cases, small set sizes (5 and 10
respectively) yield POs with which the IA performs
at least as well as the FB and GR algorithms, and in
most cases significantly better.
Conclusion We have shown that with few training
instances, acceptable attribute selection results can
be achieved; that is, results that do not significantly
differ from those obtained using a much larger
training set. Given the scarcity of resources in
this field, we feel that this is an important result
for researchers working on REG and Natural
Language Generation in general. We found that less
training data is needed in simple domains with few
attributes, such as the furniture domain, and more in
relatively more complex domains such as the people
domain. The data set being used is also of influence:
better results were achieved with D-TUNA than
with the TUNA corpus, which probably not so much
reflects a language difference, but a difference in
the way the corpora were collected.
We found some interesting differences between
the IA and Graph algorithms, which can be largely
explained by the fact that the former ranks attributes,
and the latter attribute-value pairs. The advantage
of the former (coarser) approach is that overall,
fewer training items are required, while the latter
(more fine-grained) approach is better equipped to
deal with more complex domains. In the furniture
domain both algorithms had a similar performance,
while in the people domain Graph did slightly better
than the IA. It has to be kept in mind that these
results are based on the relatively simple furniture
and people domains, and evaluated in terms of a
limited (though standard) set of evaluation met-
rics. We hope that in the near future semantically
transparent corpora for more complex domains will
become available, so that these kinds of learning
curve experiments can be replicated.
Acknowledgments Krahmer and Koolen re-
ceived financial support from The Netherlands
Organization for Scientific Research (NWO Vici
grant 27770007). We thank Albert Gatt for allowing
us to use his implementation of the IA, and Sander
Wubben for help with k-means clustering.
10
References
Anja Arts, Alfons Maes, Leo Noordman, and Carel
Jansen. 2011. Overspecification facilitates object
identification. Journal of Pragmatics, 43(1):361?374.
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL?08),
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Paul E. Engelhardt, Karl G.D Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
Gricean Maxim of Quantity? Journal of Memory and
Language, 54:554?573.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Chris Mellish, Donia Scott, Lynn Cahill, Daniel Paiva,
Roger Evans, and Mike Reape. 2006. A reference
architecture for natural language generation systems.
Natural Language Engineering, 12:1?34.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:98?110.
Ehud Reiter and Anja Belz. 2009. An investigation
into the validity of some metrics for automatically
evaluating NLG systems. Computational Linguistics,
35(4):529?558.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? How much
data is required to train a REG algorithm? In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 660?664, Portland, Oregon, USA.
Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian P.
Davy. 2008. Using spatial reference frames to gener-
ate grounded textual summaries of georeferenced data.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference (INLG), pages 16?24.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Kees van Deemter, Albert Gatt, Ielka van der Sluis, and
Richard Power. 2012. Generation of referring expres-
sions: Assessing the Incremental Algorithm. Cogni-
tive Science, to appear.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
11
