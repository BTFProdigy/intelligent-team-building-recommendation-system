Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371?380,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for
Data-Oriented Translation
Daniel Galron
CIMS
New York University
galron@cs.nyu.edu
Sergio Penkale, Andy Way
CNGL
Dublin City University
{spenkale,away}
@computing.dcu.ie
I. Dan Melamed
AT&T Shannon Laboratory
{lastname}
@research.att.com
Abstract
In this work we present a novel technique
to rescore fragments in the Data-Oriented
Translation model based on their contri-
bution to translation accuracy. We de-
scribe three new rescoring methods, and
present the initial results of a pilot experi-
ment on a small subset of the Europarl cor-
pus. This work is a proof-of-concept, and
is the first step in directly optimizing trans-
lation decisions solely on the hypothesized
accuracy of potential translations resulting
from those decisions.
1 Introduction
The Data-Oriented Translation (DOT) (Poutsma,
2000) model is a tree-structured translation model,
in which linked subtree fragments extracted from
a parsed bitext are composed to cover a source-
language sentence to be translated. Each linked
fragment pair consists of a source-language side
and a target-language side, similar to (Wu, 1997).
Translating a new sentence involves composing
the linked fragments into derivations so that a
new source-language sentence is covered by the
source tree fragments of the linked pairs, where
the yields of the target-side derivations are the can-
didate translations. Derivations are scored accord-
ing to their likelihood, and the translation is se-
lected from the derivation pair with the highest
score. However, we have no reason to believe that
maximizing likelihood is the best way to maxi-
mize translation accuracy ? likelihood and accu-
racy do not necessarily correlate well.
We can frame the problem as a search problem,
where we are searching a space of derivations for
the one that yields the highest scoring translation.
By putting weights on the derivations in the search
space, we wish to point the decoder in the direc-
tion of the optimal translation. Since we want
the decoder to find the translation with the high-
est evaluation score, we would want to score the
derivations with weights that correlate well with
the particular evaluation measure in mind.
Much of the work in the MT literature has
focused on the scoring of translation decisions
made. (Yamada and Knight, 2001) follow (Brown
et al, 1993) in using the noisy channel model,
by decomposing the translation decisions mod-
eled by the translation model into different types,
and inducing probability distributions via max-
imum likelihood estimation over each decision
type. This model is then decoded as described
in (Yamada and Knight, 2002). This type of ap-
proach is also followed in (Galley et al, 2006).
There has been some previous work on
accuracy-driven training techniques for SMT, such
as MERT (Och, 2003) and the Simplex Armijo
Downhill method (Zhao and Chen, 2009), which
tune the parameters in a linear combination of var-
ious phrase scores according to a held-out tun-
ing set. While this does tune the relative weights
of the scores to maximize the accuracy of candi-
dates in the tuning set, the scores themselves in the
linear combination are not necessarily correlated
with the accuracy of the translation. Tillmann and
Zhang (2006) present a procedure to directly opti-
mize the global scoring function used by a phrase-
based decoder on the accuracy of the translations.
Similarly to MERT, Tillmann and Zhang estimate
the parameters of a weight vector on a linear com-
bination of (binary) features using a global objec-
tive function correlated with BLEU (Papineni et
al., 2002).
In this work, we prototype some methods for
moving directly towards incorporating a measure
of the translation quality of each fragment used,
bringing DOT more into the mainstream of cur-
rent SMT research. In Section 2 we describe
probability-based DOT fragment scoring. In Sec-
tion 3 we describe our rescoring setup and the
371
(a)
S
NP VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
(b)
NP
John
NP
John
(c)
S
NP
John
VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
John
(d)
NP
Mary
NP
Mary
Figure 1: Example DOT Fragments.
three rescoring methods. In Section 4, we describe
our experiments. In Section 5 we compare the
results of rescoring the fragments with the three
methods. In Section 6 we discuss some of the
decisions that are affected by our rescoring meth-
ods. Finally, we discuss the next steps in training
the DOT system by optimizing over a translation
accuracy-based objective function in Section 7.
2 DOT Scoring
As described in previous work (Poutsma, 2000;
Hearne and Way, 2003), DOT scores translations
according to the probabilities of the derivations,
which are in turn computed from the relative fre-
quencies of linked tree fragments in a parallel tree-
bank. Linked fragment pairs are conditionally in-
dependent, so the score of a derivation is the prod-
uct of the probabilities of all the linked fragments
used. To find the probability of a translation,
DOT marginalizes over the scores of all deriva-
tions yielding the translation.
From a parallel treebank aligned at the sub-
sentential level, we extract all possible linked frag-
ment pairs by first selecting all linked pairs of
nodes in the treebank to be the roots of a new sub-
tree pair, and then selecting a (possibly empty) set
of linked node pairs that are descendants of the
newly selected fragment roots and deleting all sub-
tree pairs dominated by these nodes. Leaves of
fragments can either be terminals, or non-terminal
frontier nodes where we can compose other frag-
ments (c.f. (Eisner, 2003)). We give example DOT
fragment pairs in Figure 1.
Given two subtree pairs ?s
1
, t
1
? and ?s
2
, t
2
?,
we can compose them using the DOT composi-
tion operator ? if the leftmost non-terminal fron-
tier node of s
1
is equal to the root node of s
2
,
and the leftmost non-terminal frontier node of s
1
?s
linked counterpart in t
1
is equal to the root node
of t
2
. The resulting tree pair consists of a copy
of s
1
where s
2
has been inserted at the leftmost
frontier node, and a copy of t
1
where t
2
has been
inserted at the node linked to s
1
?s leftmost frontier
node (Hearne and Way, 2003).
In Figure 1, fragment pair (a) is a fragment with
two open substitution sites. If we compose this
fragment pair with fragment pair (b), the source
side composition must take place on the leftmost
non-terminal frontier node (the leftmost NP). On
the target side we compose on the frontier linked
to the leftmost source side non-terminal frontier.
The result is fragment pair (c). If we now com-
pose the resulting fragment pair with fragment pair
(d), we obtain a fragment pair with no open sub-
stitution sites whose source-side yield is John likes
Mary and whose target-side yield is Mary pla??t a`
John. Note that there are two different derivations
using the fragment pairs in Figure 1 that result in
the same fragment pair, namely (a) ? (b) ? (d), and
(c) ? (d).
For a given linked fragment pair ?d
s
, d
t
?, the
probability assigned to it is
P (?d
s
, d
t
?) =
|?d
s
, d
t
?|
?
r(u
s
)=r(d
s
)?r(u
t
)=r(d
t
)
|?u
s
, u
t
?|
(1)
where |?d
s
, d
t
?| is the number of times the frag-
ment pair ?d
s
, d
t
? is found in the bitext, and r(d)
is the root nonterminal of d. Essentially, the prob-
ability assigned to the fragment pair is the relative
frequency of the fragment pair to the pair of non-
terminals that root the fragments.
Then, with the assumption that DOT fragments
are conditionally independent, the probability of a
derivation is
P (d) = P (?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
P (?d
s
, d
t
?
i
) (2)
In the original DOT formulation, DOT disam-
biguated translations according to their probabil-
ities. Since a translation can have many possible
derivations, to obtain the probability of a transla-
tion it is necessary to marginalize over the distinct
derivations yielding a translation. The probabil-
ity of a translation w
t
of a source sentence w
s
, is
372
given by (3):
P (w
s
, w
t
) =
?
d?D
P (d
?w
s
,w
t
?
) (3)
and the translation is chosen so as to maximize (4):
w?
t
= argmax
w
t
P (w
s
, w
t
) (4)
Hearne and Way (2006) examined alternative dis-
ambiguation strategies. They found that rather
than disambiguating on the translation probability,
the translation quality would improve by disam-
biguating on the derivation probability, as in (5):
w?
t
= argmax
d
P (d) (5)
Our analysis suggest that this is because many
derivations with very low probabilities generate
the same, poor translation. When applying Equa-
tion (3) to marginalize over those derivations, the
resulting score is higher for the poor translation
than a better translation with fewer derivations but
where the derivations had higher likelihood.
Using the DOT model directly is difficult ?
the number of fragments extracted from a paral-
lel treebank is exponential in the size of the tree-
bank. Therefore we use the Goodman reduction
of DOT (Hearne, 2005) to create an isomorphic
PCFG representation of the DOT model that is lin-
ear in the size of the treebank. The idea behind the
Goodman reduction is that rather than storing frag-
ments in the grammar and translating via compo-
sition, we simultaneously build up the fragments
using the PCFG reduction and compose them to-
gether. To perform the reduction, we first relabel
the two linked nodes (X, Y) with the new label
X=Y. We then label each node in the parallel tree-
bank with a unique Goodman index. Each binary-
branching node and its two children can be inter-
nal or root/frontier. We add rules to the grammar
reflecting the role that each node can take, keeping
unaligned nodes as fragment-internal nodes. So in
the case where a node and both of its children are
aligned, we commit 8 rules into the grammar, as
follows:
LHS ? RHS1 RHS2 LHS+a ? RHS1 RHS2
LHS ? RHS1+b RHS2 LHS+a ? RHS1+b RHS2
LHS ? RHS1 RHS2+c LHS+a ? RHS1 RHS2+c
LHS ? RHS1+b RHS+c LHS+a ? RHS1+b RHS2+c
A category label which ends in a ?+? symbol fol-
lowed by a Goodman index is fragment-internal
and all other nodes are either fragment roots or
S=S
1
N=N
3
John
VP
2
V
4
likes
N=N
5
Mary
S=S
1
N=N
4
Mary
VP
2
V
5
pla??t
PP
3
P
6
a`
N=N
7
John
Source PCFG Target PCFG
S=S? N=N VP+2 0.5 S=S? N=N VP+2 0.5
S=S? N=N+3 VP+2 0.5 S=S? N=N+4 VP+2 0.5
S=S+1? N=N VP+2 0.5 S=S+1? N=N VP+2 0.5
S=S+1? N=N+3 VP+2 0.5 S=S+1? N=N+4 VP+2 0.5
N=N? John 0.5 N=N?Mary 0.5
N=N+3? John 1 N=N+4?Mary 1
VP+2? V+4 N=N 0.5 VP+2? V+5 PP+3 1
VP+2? V+4 N=N+5 0.5 V+5? pla??t 1
V+4? likes 1 PP+3? P+6 N=N 0.5
N=N?Mary 0.5 PP+3? P+6 N=N+7 0.5
N=N+5?Mary 1 P+6? a` 1
N=N? John 0.5
N=N+7? John 1
Figure 2: A parallel tree and its corresponding Goodman re-
duction.
frontier nodes. A fragment pair, then, is a pair of
subtrees in which the root does not have an index,
all internal nodes have indices, and all the leaves
are either terminals or un-indexed nodes. We give
an example Goodman reduction in Figure 2.
While we store the source grammar and the tar-
get grammar separately, we also keep track of the
correspondence between source and target Good-
man indices and can easily identify the alignments
according to the Goodman indices. Probabilities
for the PCFG rules are computed monolingually
as in the standard Goodman reduction for DOP
(Goodman, 1996). In decoding with the Goodman
reduction, we first find the n-best parses on the
source side, and for each source fragment, we con-
struct the k-best fragments on the target side. We
finally compute the bilingual derivation probabil-
ities by multiplying the source and target deriva-
tion probabilities by the target fragment relative
frequencies conditioned on the source fragment.
There are a few problems with a likelihood-
based scoring scheme. First, it is not clear that
if a fragment is more likely to be seen in training
data then it is more likely to be used in a correct
translation of an unseen sentence. In our analysis
of the candidate translations of the DOT system,
we observed that frequently, the highest-likelihood
candidate translation output by the system was not
the highest-accuracy candidate inferred. An addi-
tional problem is that, as described in (Johnson,
2002), the relative frequency estimator for DOP
373
(and by extension, DOT) is known to be biased
and inconsistent.
3 Accuracy-Based Fragment Scoring
In our work, we wish to incorporate a measure
of fragment accuracy into the scoring. To do so,
we reformulate the scoring of DOT as log-linear
rather than probabilistic, in order to incorporate
non-likelihood features into the derivation scores.
For all tree fragment pairs ?d
s
, d
t
?, let
l(?d
s
, d
t
?) = log(p(?d
s
, d
t
?)) (6)
The general form of a rescored tree fragment will
be
s(?d
s
, d
t
?) = ?
0
l(?d
s
, d
t
?) +
k
?
i=1
?
i
f
i
(?d
s
, d
t
?)
(7)
where each ?
i
is the weight of that term in the fi-
nal score, and each f
i
(d) is a feature. In this work,
we only consider f
1
(d), an accuracy-based score,
although in future work we will consider a wide
variety of features in the scoring function, includ-
ing combinations of the different scoring schemes
described below, binary lexical features, binary
source-side syntactic features, and local target side
features. The score of a derivation is now given by
(8):
s(d) = s(?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
s(?d
s
, d
t
?
i
) (8)
In order to disambiguate between candidate
translations, we follow (Hearne and Way, 2006)
by using Equation (5).
3.1 Structured Fragment Rescoring
In all our approaches, we rescore fragments ac-
cording to their contribution to the accuracy of
a translation. We would like to give fragments
that contribute to good translations relatively high
scores, and give fragments that contribute to bad
translations relatively low scores, so that during
decoding fragments that are known to contribute to
good translations would be chosen over those that
are known to contribute to bad translations. Fur-
thermore, we would like to score each fragment in
a derivation independently, since bad translations
may contain good fragments, and vice-versa.
In practice, it is infeasible to rescore only those
fragments seen during the rescoring process, due
to the Goodman reduction for DOT. If we were to
properly rescore each fragment, a new rule would
need to be added to the grammar for each rule ap-
pearing in the fragment. Since the number of frag-
ments is exponential, this would lead to a substan-
tial increase in grammar size. Instead, we rescore
the individual rules in the fragments, by evenly di-
viding the total amount of scoring mass among the
rules of the particular fragment, and then assigning
them the average of the rule scores over all frag-
ments in which they appear. That is for each rule
r in a fragment f consisting of c
f
(r) rules with
score ?(f), the score of the rule is given as:
s(r) =
?
f :r?f
?(f)/c
f
(r)
|f |
(11)
This has the further advantage that we are al-
lowing fragments that were unseen during tuning
to be rescored according to previously seen frag-
ment substructures.
To implement this scheme, we select a set of or-
acle translations for each sentence in the tuning
data by evaluating all the candidate translations
against the gold standard translation using the F-
score (Turian et al, 2003), and selecting those
with the highest F
1
-measure, with exponent 1. We
use GTM, rather than BLEU, because BLEU is
not known to work well on a per-sentence level
(Lavie et al, 2004) as needed for oracle selection.
We then compare all the target-side fragments in-
ferred in the translation process for each candidate
translation against the fragments that yielded the
oracles. There are two relevant parts of the frag-
ments ? the internal yields (i.e. the terminal leaves
of the fragment) and the substitution sites (i.e. the
frontiers where other fragments attach). We score
the fragments rooted at the substitution sites sepa-
rately from the parent fragment. We can uniquely
identify the set of fragments that can be rooted at
substitution sites by determining the span of the
linked source-side derivation.
To compare two fragments, we define an edit
distance between them. For a given fragment d,
let r(d) be the root of the fragment, let r(d) ?
rhs1 be the left subtree of r(d), and let r(d) ?
rhs2 be the right subtree. The difference between
a candidate fragment d
c
and an oracle fragment
d
gs
is given by the equations in Table 1.
These equations define a minimum edit dis-
tance between two fragment trees, allowing sub-
fragment order inversion, insertion, and deletion
374
?(d
c
, d
gs
) =
(
0 if d
c
= d
gs
1 if d
c
6= d
gs
Base case: d
c
and d
gs
are unary subtrees or substitution sites (9)
?(d
c
, d
gs
) = min
8
>
>
>
>
<
>
>
>
>
:
?(d
c
? rhs1, d
gs
? rhs1) + ?(d
c
? rhs2, d
gs
? rhs2),
?(d
c
? rhs2, d
gs
? rhs1) + ?(d
c
? rhs1, d
gs
? rhs2) + 1,
?(d
c
, d
gs
? rhs1) + |y(d
gs
? rhs2)|,
?(d
c
, d
gs
? rhs2) + |y(d
gs
? rhs1)|,
?(d
c
? rhs1, d
gs
) + |y(d
c
? rhs2)|,
?(d
c
? rhs2, d
gs
) + |y(d
c
? rhs1)|
(10)
Table 1: The recursive relation defining the fragment difference between two fragments.
(a) A
B
b
C
c
(b) A
C
c
B
b
(c) D
A
B
b
F
f
E
e
Figure 3: Comparing trees (a) and (b) with our distance met-
ric yields a value of 1. The difference between trees (a) and
(c) is 2, and for trees (b) and (c) the distance is 3.
as edit operations. For example, the only dif-
ference between trees (a) and (b) in Figure 3 is
that their children have been inverted. To com-
pare these trees using our distance metric, we first
compute the first argument of the min function in
Equation (10), directly comparing the structure of
each immediate subtree. We then compute the sec-
ond argument, obtaining the cost of performing an
inversion, and finally compute the remaining argu-
ments, assessing the cost of allowing each tree to
be a direct subtree of the other. The result of this
computation is 1, representing the inversion oper-
ation required to transform tree (a) into tree (b).
If we compare trees (a) and (c) in Figure 3, we
obtain a value of 2, given that the minimum opera-
tions required to transform tree (a) into tree (c) are
inserting an additional subtree at the top level and
then substituting the subtree rooted by C for the
subtree rooted by F. If we compare tree (b) with
tree (c) then the distance is 3, since we are now
required to also replace the subtree rooted by C by
the one rooted by B.
Since it is not efficient to compute the differ-
ences directly, we utilize common substructures
and derive a dynamic programming implementa-
tion of the recursion. We compare each fragment
against the set of oracle fragments for the same
source span, and select the lowest cost as the score,
assigning the candidate the negative difference be-
tween it and the oracle fragment it is most similar
to, as in (12):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??(d
t
, d
o
t
) (12)
In practice, given the Goodman reduction for
DOT, we divide the fragment score by the number
of rules in the fragment, and assign the average of
those scores for each rule instance across all frag-
ments rescored.
3.2 Normalized Structured Fragment
Rescoring
In the structured fragment rescoring scheme, the
scores that the fragments are assigned are the un-
normalized edit distances between the two frag-
ments. It may be better to normalize the fragment
scores, rather than using the minimum number of
tree transformations to convert one fragment into
the other. We would expect that when compar-
ing larger fragments, on average there would be
more transformations needed to change one into
the other than when comparing small fragments.
However in the previous scheme, small fragments
would have higher scores than large fragments,
since fewer differences would be observed. The
normalized score is given in (13):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
log(1 ? ?(d
t
, d
o
t
)/
max(|d
t
|, |d
o
t
|))
(13)
Essentially, we are normalizing the edit distance
by the maximum edit distance possible, namely
the size of the largest fragment of the two being
compared.
3.3 Fragment Surface Rescoring
The disadvantage of the minimum tree fragment
edit approach is that it explicitly takes the internal
375
syntactic structure of the fragment into account.
In comparing two fragments, they may have the
same (or very similar) surface yields, but differ-
ent internal structures. The previous approach
would penalize the candidate fragment, even if its
yield is quite close to the oracle. In this rescor-
ing method, we extract the leaves of the candi-
date and oracle fragments, representing the substi-
tution sites by the source span which their frag-
ments cover. We then compare them using the
Damerau-Levenshtein distance ?
dl
(d
c
, d
gs
) (Dam-
erau, 1964) between the two fragment yields, and
score them as in (14):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??
dl
(d
t
, d
o
t
) (14)
In Equation (14) we are selecting the maximal
score for ?d
s
, d
t
? from its comparison to all the
possible corresponding oracle fragments. In this
way, we are choosing to score ?d
s
, d
t
? against the
oracle fragment it is closest to.
4 Experiments
For our pilot experiments, we tested all the rescor-
ing methods in the previous section on Spanish-to-
English translation against the relative-frequency
baseline. We randomly selected 10,000 sentences
from the Europarl corpus (Koehn, 2005), and
parsed and aligned the bitext as described in (Tins-
ley et al, 2009). From the parallel treebank, we
extracted a Goodman reduction DOT grammar, as
described in (Hearne, 2005), although on an order
of magnitude greater amount of training data. Un-
like (Bod, 2007), we did not use the unsupervised
version of DOT, and did not attempt to scale up
our amount of training data to his levels, although
in ongoing work we are optimizing our system to
be able to handle that amount of training data. To
perform the rescoring, we randomly chose an ad-
ditional 30K sentence pairs from the Spanish-to-
English bitext. We rescored the grammar by trans-
lating the source side of the 10K training sentence
pairs and 10K of the additional sentences, and us-
ing the methods in Section 3 to score the frag-
ments derived in the translation process. We then
performed the same experiment translating the full
40K-sentence set. Rules in the grammar that were
not used during tuning were rescored using a de-
fault score defined to be the median of all scores
observed.
Our system performs translation by first obtain-
ing the n-best parses for the source sentences and
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.30 10.31 10.32 10.27 10.08
NSFR 8.31 9.37 9.53 9.66 9.90
FSR 10.19 10.25 10.18 10.19 9.93
NIST SFR 3.792 3.805 3.808 3.800 3.781
NSFR 3.431 3.638 3.661 3.693 3.722
FSR 3.784 3.799 3.792 3.795 3.764
F-SCORE SFR 40.92 40.82 40.86 40.84 40.78
NSFR 37.53 39.50 39.93 40.38 40.78
FSR 40.83 40.85 40.87 40.91 40.67
Table 2: Results on test set. Rescoring on 20K sentences.
SFR stands for Structured Fragment Rescoring, NSFR for
Normalized SFR and FSR for Fragment Surface Rescoring.
system-i-j represents the corresponding system with ?
0
= i
and ?
1
= j. Underlined results are statistically significantly
better than the baseline at p = 0.01.
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.59 10.58 10.41 10.38 10.08
NSFR 8.61 9.71 9.90 9.96 9.93
FSR 10.49 10.48 10.35 10.38 10.06
NIST SFR 3.841 3.835 3.810 3.807 3.785
NSFR 3.515 3.694 3.713 3.734 3.727
FSR 3.834 3.833 3.820 3.816 3.784
F-SCORE SFR 41.12 40.99 40.86 40.88 40.75
NSFR 38.16 40.39 40.69 40.90 40.75
FSR 41.03 41.02 41.01 40.98 40.72
Table 3: Results on test set. Rescoring on 40K sentences. Un-
derlined are statistically significantly better than the baseline
at p = 0.01.
then computing the k-best bilingual derivations for
each source parse. In our experiments we used
beams of n = 10, 000 and k = 5. We also ex-
perimented with different values of ?
0
and ?
1
in
Equation (7). We set these parameters manually,
although in future work we will automatically tune
them, perhaps using a MERT-like algorithm.
We tested our rescored grammars on a set of
2,000 randomly chosen Europarl sentences, and
used a set of 200 randomly chosen sentences as
a development test set. 1
5 Results
Translation quality results can be found in Tables
2 and 3. In these tables, columns labeled i-j in-
dicate that the corresponding system was trained
using parameters ?
0
= i and ?
1
= j in Equa-
tion 7. Statistical significance tests for NIST and
BLEU were performed using Bootstrap Resam-
pling (Koehn, 2004).
1All sentences, including the ones used for training, were
limited to a length of at most 20 words.
376
BLEU NIST F-SCORE
Baseline 10.82 3.493 42.31
2-8 4-6 5-5 6-4 8-2
BLEU SFR 11.34 12.12 11.94 11.97 11.78
NSFR 9.68 10.99 11.38 11.63 11.30
FSR 11.40 11.49 11.72 11.91 11.72
NIST SFR 3.653 3.727 3.723 3.708 3.694
NSFR 3.376 3.530 3.554 3.616 3.572
FSR 3.655 3.675 3.698 3.701 3.675
F-SCORE SFR 44.84 45.47 45.36 45.33 45.08
NSFR 41.44 43.38 44.18 44.79 44.26
FSR 44.68 44.91 45.15 45.19 44.82
Table 4: Results on development test set. Rescoring on 40K
sentences.
As Table 2 indicates, all three rescoring meth-
ods significantly outperform the relative frequency
baseline. The unnormalized structured fragment
rescoring method performed the best, with the
largest improvement of 1.5 BLEU points, a 17.5%
relative improvement. We note that the BLEU
scores for both the baseline and the experiments
are low. This is to be expected, because the gram-
mar is extracted from a very small bitext espe-
cially when the heterogeneity of the Europarl cor-
pus is considered. In our analysis, only 32.5 per-
cent of the test sentences had a complete source-
side parse, meaning that a lot of structural infor-
mation is lost contributing to arbitrary target-side
ordering. In these experiments we did not use an
additional language model. DOT (and many other
syntax-based SMT systems) essentially have the
target language model encoded within the trans-
lation model, since the inferences derived dur-
ing translations link source structures to target
structures, so in principle, no additional language
model should be necessary. Furthermore, we only
evaluate against a single reference, which also
contributes to the lowering of absolute scores. To
provide a sanity check against a state-of-the-art
system, we trained the Moses phrase-based MT
system (Koehn et al, 2007) using our training
corpus, using no language model and using uni-
form feature weights, to provide a fair comparison
against our baseline. We used this system to de-
code our development test set, and as a result we
obtained a BLEU score of 10.72, which is compa-
rable to the score obtained by our baseline on the
same set.
When we scale up to tuning on 40,000 sen-
tences we see an improvement in BLEU scores as
well, as shown in Table 3. When tuning on 40K
sentences, we observe an increase of 1.81 BLEU
points on the best-performing system, which is a
20.6% improvement over the baseline. We note
that rescoring on 20K sentences rescores approxi-
mately 275,000 rules out of 655,000 in the gram-
mar, whereas rescoring on 40K sentences rescores
approximately 280,000.
To analyze the benefits of the rescored gram-
mar, we set aside a separate development set that
we decoded with the grammar trained on 40K sen-
tences. The results are presented in Table 4. The
analysis is presented in Section 6.
Interestingly, there is a large difference between
the normalized and unnormalized versions of the
SFR scoring scheme. Our analysis suggests that
the differences are mostly due to numerical issues,
namely the difference in magnitude between the
NSFR scores and the likelihood scores in the linear
combination, and the default value assigned when
the NSFR score was zero. In ongoing work, we
are working to address these issues.
For most configurations the difference between
SFR and FSR was not statistically significant at
p = 0.05. Our analysis indicated that surface dif-
ferences tended to co-occur with structural differ-
ences. We hypothesize that as we scale up to larger
and more ambiguous grammars, the system will
infer more derivations with the same yields, ren-
dering a larger difference between the quality of
the two scoring mechanisms.
6 Discussion
To analyze the advantages and disadvantages of
our approach over the baseline, we closely ex-
amined and compared the derivations made on
the devset translation by the SFR-scored gram-
mar and the likelihood-scored grammar. Although
the BLEU scores are rather low, there were sev-
eral sentences in which the SFR-scored grammar
showed a marked improvement over the baseline.
We observed two types of improvements.
The first is where the rescored grammar gave
us translations that, while still generally bad, were
closer to the gold standard than the baseline trans-
lation. For example, the Spanish sentence ?Y en
tercer lugar , esta? el problema de la aplicacio?n uni-
forme del Derecho comunitario .? translates into
the gold standard ?Thirdly , we have the problem
of the uniform application of Community law .?
The baseline grammar translates the sentence as
?on third place , Transport and Tourism . I are
the problems of the implementation standardised
is the EU law .? with a GTM F-Score of 0.378,
377
sn=NP+67600 ?1.97/?5.66
NP+67608
the rapporteur
sp=PP+67601
s=IN ?0.48/?0.37
in
sn=SBAR+165198 ?1.39/?1.90
nc=TO+165203
to
dn=VP 0/?0.49
make
sn=NP+36950 ?5.89/?5.09
NP+36952
the rapporteur
sp=PP+36951 ?4.28/?3.81
s=IN ?0.48/?0.37
in
sn=NP+36953
dn=DT 0/?0.58
both
nc=NNS ?1.03/?0.81
questions
Figure 4: Target side of the highest-scoring translations for a sentence, according to the baseline system (left) and the SFR
system (right). Boxed nodes are substitution sites. Scores in superscripts denote the score of the sub-derivation according to
the baseline and to the SFR system.
and the rescored grammar outputs the translation
?to there in the third place , I are the problem of
the implementation standardised is the Commu-
nity law .?, with an F-Score of 0.5. While many of
the fragments in the derivations that yielded these
two translations differ, the ones we would like to
focus on are the fragments that yield the transla-
tion of ?comunitario?. The grammar contains sev-
eral competing unary fragment pairs for ?comuni-
taro?. In the baseline grammar, the pair (aq=NNP
? comunitario, aq=NNP ? EU) has a score
of ?0.693147, whereas the pair (aq=NNP ?
comunitario, aq=NNP? Community) has a
score of ?1.38629. In the rescored grammar how-
ever, (aq=NNP ? comunitario, aq=NNP ?
EU) has a score of -0.762973, whereas (aq=NNP
? comunitario, aq=NNP ? Community)
has a score of -0.74399. In effect, the rescoring
scheme rescored the word alignment itself. This
suggests that in future work, it may be possible
to integrate a word aligner or fragment aligner di-
rectly into the MT training method.
The other improvement was where the baseline
and the SFR-scored grammar output translations
of roughly the same quality according to the eval-
uation measure, yet in terms of human evaluation,
the SFR translation was much better than the base-
line translation. For instance, our devset contained
the Spanish sentence ?Estoy de acuerdo con el po-
nente en dos cuestiones .? The baseline transla-
tion given is ?I agree with the rapporteur in to
make .?, and the SFR-scored translation given is
?I agree with the rapporteur in both questions .?.
While both translations have the same GTM score
against the gold standard ?I agree with the rap-
porteur on two issues .?, clearly, the second one
is of far higher quality than the first. As we can
see in Figure 4, the derivation over the substring
?in both questions? gets a higher score than ?in
to make? when translated with the rescored gram-
mar. In the baseline, ?en dos cuestiones? is not
translated as a whole unit ? rather, the derivation of
?el ponente en dos cuestiones? is decomposed into
four subderivations, yielding ?el? ?ponente? ?en?
?dos cuestiones?, where each of those is translated
separately, into ??? ?the rapporteur? ?in? and ?to
make?. The SFR-scored grammar, however, out-
puts a different bilingual derivation. The source
is decomposed into five sub-derivations, one for
each word, and each word is translated separately.
Then, the rescored target fragments set the proper
target-side word order and select the target-side
words that maximize the score of the subderiva-
tion covering the source span. We note that in this
example, the score of translating ?dos? to ?make?
was higher than the score of translating ?dos? to
?both?. However, the higher level target frag-
ment that composed the translation of ?dos? to-
gether with the translation of ?cuestiones? yielded
a higher score when composing ?both questions?
rather than ?to make?.
7 Conclusions and Future Work
The results presented above indicate that aug-
menting the scoring mechanism with an accuracy-
based measure is a promising direction for transla-
tion quality improvement. It gives us a statistically
significant improvement over the baseline, and our
analysis has indicated that the system is indeed
making better decisions, moving us a step closer
towards the goal of making translation decisions
based on the hypothesis of the resulting transla-
378
tion?s accuracy.
Now that we have demonstrated that translation
quality can be improved by incorporating a mea-
sure of fragment quality into the scoring scheme,
our immediate next step is to optimize our sys-
tem so that we can scale up to significantly larger
training and tuning sets, and determine whether
the improvements we have noted carry over when
the likelihood is computed from more data. Af-
terwards, we will implement a training scheme
to maximize an accuracy-based objective func-
tion, for instance, by minimizing the difference
between the scores of the highest-scoring deriva-
tion and the oracle derivations, in effect maximiz-
ing the score of the highest-scoring translation.
The rescoring method presented in this paper
need not be limited to DOT. Fragments can be
thought of as analogous to phrases in Phrase-
Based SMT systems ? we could implement a sim-
ilar rescoring system for phrase-based systems,
where we generate several candidate translations
for source sentences in a tuning set, and score each
phrase used against the phrases used in a set of or-
acles. More broadly, we could potentially take any
statistical MT system, and compare the features
of all candidates generated against those of oracle
translations, and score those that are closer to the
oracle higher than those further away.
Finally, by explicitly framing the translation
problem as a search problem, where we are di-
vorcing the inferences in the search space (i.e.
the model) from the path we take to find the op-
timal inference according to some criterion (i.e.
the scoring scheme), we can remove some of the
variability when comparing two models or scoring
mechanisms (Lopez, 2009).
Acknowledgements
This work is supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142). We would like to
thank the anonymous reviewers for their helpful
comments and suggestions.
References
R. Bod. 2007. Unsupervised syntax-based ma-
chine translation: The contribution of discontiguous
phrases. In Proceedings of the 11th Machine Trans-
lation Summit, pages 51?57, Copenhagen, Den-
mark.
P. F. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
F. J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7(3):171?176.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Companion Volume,
pages 205?208, Sapporo.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 961?968, Sydney, Aus-
tralia.
J. Goodman. 1996. Efficient algorithms for parsing the
DOP model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 143?152, Philadelphia, PA.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation. In Proceedings of
the Ninth Machine Translation Summit, pages 165?
172, New Orleans, LA.
M. Hearne and A. Way. 2006. Disambiguation strate-
gies for data-oriented translation. In Proceedings of
the 11th Conference of the European Association for
Machine Translation, pages 59?68, Oslo, Norway.
M. Hearne. 2005. Data-Oriented Models of Parsing
and Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71?76, March.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics, demonstation session, pages
177?180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
379
the Conference on Empirical Methods in Natural
Language Processing, pages 388?395, Barcelona,
Spain.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
A. Lavie, K. Sagae, and S. Jayaraman. 2004. The sig-
nificance of recall in automatic metrics for MT eval-
uation. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Ameri-
cas, pages 134?143, Washington, DC.
A. Lopez. 2009. Translation as weighted deduction. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532?540,
Athens, Greece.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 311?318, Philadelphia, PA.
A. Poutsma. 2000. Data-oriented translation. In The
18th International Conference on Computational
Linguistics, pages 635?641, Saarbru?cken, Germany.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical MT. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 721?728, Sydney, Australia.
J. Tinsley, M. Hearne, and A. Way. 2009. Parallel tree-
banks in phrase-based statistical machine transla-
tion. In Proceedings of the Tenth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), pages 318?331, Mex-
ico City, Mexico.
J. Turian, L. Shen, and I. D. Melamed. 2003. Eval-
uation of machine translation and its evaluation. In
Proceedings of the Ninth Machine Translation Sum-
mit, pages 386?393, New Orleans, LA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France.
K. Yamada and K. Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 303?310, Philadelphia, PA.
B. Zhao and S. Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical ma-
chine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 21?
24, Boulder, Colorado.
380
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 95?99,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2009
Jinhua Du, Yifan He, Sergio Penkale, Andy Way
Centre for Next Generation Localisation
Dublin City University
Dublin 9, Ireland
{jdu,yhe,spenkale,away}@computing.dcu.ie
Abstract
In this paper, we describe the machine
translation system in the evaluation cam-
paign of the Fourth Workshop on Statisti-
cal Machine Translation at EACL 2009.
We describe the modular design of our
multi-engine MT system with particular
focus on the components used in this par-
ticipation.
We participated in the translation task
for the following translation directions:
French?English and English?French, in
which we employed our multi-engine ar-
chitecture to translate. We also partic-
ipated in the system combination task
which was carried out by the MBR de-
coder and Confusion Network decoder.
We report results on the provided devel-
opment and test sets.
1 Introduction
In this paper, we present a multi-engine MT
system developed at DCU, MATREX (Machine
Translation using Examples). This system exploits
EBMT, SMT and system combination techniques
to build a cascaded translation framework.
We participated in both the French?English and
English-French News tasks. In these two tasks,
we employ three individual MT system which are
1) Baseline: phrase-based system (PB); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chun-
ker (Gough and Way, 2004). 3) HPB: a typical
hierarchical phrase-based system (Chiang, 2005).
Meanwhile, we also use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final result.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypothe-
sis as the alignment reference for the Confusion
Network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search and generate the translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide results on the development and
test sets. Section 4 is our conclusion.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits aspects
of both the EBMT and SMT paradigms.
This architecture includes three individual sys-
tems which are phrase-based, example-based and
hierarchical phrase-based.
The combination structure is the MBR decoder
and CN decoder, which is based on the word-level
combination strategy.
In the final stage, we use a new rescoring mod-
ule to process the N -best list generated by the
combination module. See Figure 1 as a detailed
illustration.
2.2 Example-Based Machine Translation
EBMT obtains resources using the Marker Hy-
pothesis (Green, 1979), a psycholinguistic con-
straint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Given
a set of closed-class words we segment each sen-
tence into chunks, creating a chunk at each new
occurrence of a marker word, with the restriction
that each segment must contain at least one non-
marker word (Gough and Way, 2004).
95
Mutiple 1-best
MBR Decoder
CN/MERT
System 
Combination
HPB Baseline EBMT
Dev/MERT
Decoding
Rescore/MERT
Rescore/MERT
TestSet
Recaser
Rescore
Mutiple 1-best
MBR Decoder
CN Decoder
Rescore
Recaser
Figure 1: System Framework
We then align these segments using an edit-
distance-style algorithm, in which the insertion
and deletion probabilities depend on word-to-
word translation probabilities and word-to-word
cognates (Stroppa and Way, 2006).
We extracted phrases of at most 7 words on
each side. We then merged these phrases with the
phrases extracted by the baseline system adding
word alignment information, and used this system
seeded with this additional information.
2.3 Hierarchical Machine Translation
HPB translation system is a re-implementation of
the hierarchical phrase translation model which is
based on PSCFG (Chiang, 2005). We generate re-
cursively PSCFG rules from the initial rules as
N ? f1 . . . fm/e1 . . . en
where N is a rule which is initial or includes non-
terminals.
M ? fi . . . fj/eu . . . ev
where 1 ? i ? j ? m and 1 ? u ? v ? n, at
which point a new rule can be obtained, named,
N ? f i?11 Xkfmj+1/eu?11 Xkenv+1
where k is an index for the nonterminal X . The
number of nonterminals permitted in a rule is no
more than two.
When extracting hierarchical rules,we set some
limitations that initial rules are of no more than
7 words in length and other rules should have
no more than 5 terminals and nonterminals, and
we disallow rules with adjacent source-side and
target-side nonterminals.
The decoder is an enhanced CYK-style chart
parser that maximizes the derivation probability
and spans up to 12 source words. A 4-gram lan-
guage model generated by SRI Language Model-
ing toolkit (SRILM) (Stolcke, 2002) is used in the
cube-pruning process. The search space is pruned
with a chart cell size limit of 50.
2.4 System Combination
For multiple system combination, we implement
an MBR-CN framework as shown in Figure 1. In-
stead of using a single system output as the skele-
ton, we employ a minimum Bayes-risk decoder
to select the best single system output from the
merged N -best list by minimizing the BLEU (Pa-
pineni et al, 2002) loss.
The confusion network is built by the output of
MBR as the backbone which determines the word
order of the combination. The other hypotheses
are aligned against the backbone based on the TER
metric. NULL words are allowed in the alignment.
Each arc in the CN represents an alternative word
at that position in the sentence and the number of
votes for each word is counted when constructing
the network. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
Also, we use MERT (Och, 2003) to tune the
weights of confusion network.
2.5 Rescore
Rescore is a very important part in post-processing
which can select a better hypothesis from the N -
best list. We add some new global features in
rescore model. The features we used are as fol-
lows:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram POS language model (Ratna-
parkhi, 1996; Schmid, 1994);
96
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT algorithm.
3 Experimental Setup
The following section describes the system and
experimental setup for the French-English and
English-French translation tasks.
3.1 Statistics of Data
Parallel Corpus
We used Europarl and Giga data for this evalua-
tion. The statistics of parallel data are shown in
Table 1.
Corpra Sen Token-En Token-Fr Len
Europarl 1.46M 39,240,672 42,252,067 80
Giga 2M 48,648,104 57,869,002 65
Table 1: Statistics of Parallel Data
In this table, Sen indicates the number of sentence
pairs; Len denotes the maximum sentence length
of each corpus. This year the translation task is
only evaluated on News Domain. Experimental re-
sults showed that giga data is more correlated than
Europarl and the BLEU score is significantly im-
proved(See Table 4).
Monolingual Corpus
In this evaluation, we trained a small 4-gram lan-
guage model using data in Table 1 and a large 4-
gram language model using data in Table 2. We
configured these two LMs for Baseline and EBMT
systems while HPB only used the large one.
Language Sen Token Source
English 9,966,838 240,849,221 E/N/NC
French 9,966,838 260,520,313 E/N/NC
Table 2: Statistics of Monolingual Data
In the above table, E/N/NC refers to Eu-
roparl/News/New Commentary corpus.
3.2 Pre-Processing
We preprocessed both Europarl and Giga Release
1 corpus. For the Europarl corpus, we removed
the reserved characters in GIZA++ and tokenized
and lowercased the corpus with tools provided by
WMT09. The Giga corpus was too large for our
resource, so we performed sentence selection be-
fore cleaning, in the following steps.
? We split the Giga corpus into even segments,
each segment consisting of 20 lines.
? We trained an SVM classifier on English side
with positive examples from the monolin-
gual news data and negative examples from
noisy sentences (numbers, meaningless word
combinations, and random segments) from
the Giga corpus. We used ?-ly? and ?-ing?
to approximate adverbs and present partici-
ples and did not use other POS-induced fea-
tures, as in (Ferizis and Bailey, 2006). We
added these features to remove noise: aver-
age length of sentences, frequency of capital-
ized characters, frequency of numerical char-
acters and short word penalty (equals to 1
when average length of words < 4, and 0
otherwise). We used the classifier to remove
20% segments of lowest scores.
? We selected 1, 600 words having the highest
mutual information scores with monolingual
training data against the Giga corpus.
? We selected 100, 000 segments where these
words occurred most frequently. However
the sentence was dropped if the length ratio
between English and French was larger than
1.5 or less than 0.67.
3.3 System Configuration
The two language models were done using the
SRILM employing linear interpolation and modi-
fied K-N discounting (Chen and Goodman, 1996).
The configuration for the three systems is listed
in Table 3.
System P-Table Length LM Features
Baseline-E 55.9M 7 2 15
Baseline-G 58.4M 7 2 15
EBMT 59.4M 7 2 15
HPB 122M 5 1 8
Table 3: Statistics of MT Systems
In this table, E indicates the Europarl corpus
97
which is used for all three systems, and G stands
for the Giga corpus which is only used for the
Baseline system. We can see from Table 3 that
the size of the HPB phrase-table is more than 2
times as large as the other phrase tables. How to
filter and process such a huge hierarchical table is
a challenging problem.
We tuned our systems on the development set
devset2009-a and devset2009-b, and performed
the crossover experiment by these two devsets.
3.4 Experimental Results
The system output is evaluated with respect to
BLEU score. In Table 4, we used devset2009-b
to tune the various parameters in our three single
systems and devset2009-a for testing. In terms of
the Europarl data, we can see that the three sys-
tems we used achieved similar performance on the
test set for both translation directions, with the
Baseline-E system yielding slightly better results
than the other two.
System Fr-En En-Fr
Baseline-E 22.24 22.68
Baseline-G 24.90 ?1
EBMT 22.04 22.12
HPB 21.69 21.12
MBR 25.11 22.68
CN 25.24 22.76
Rescore 25.40 22.97
Table 4: Experimental Results on Devset2009-a
We then used the translations of the devset2009-
a produced by each system to tune the parame-
ters of our system combination module. From Ta-
ble 4, we can see that using MBR and confusion
network decoding leads to a slight improvement
over the strongest single system, i.e. the baseline
Phrase-Based SMT system. Rescoring the N -best
lists yielded an increase of 0.5 (2.0 relative) ab-
solute BLEU points over the baseline for French?
English Translation and 0.29 (1.28 relative) abso-
lute BLEU points for English?French Translation.
Table 5 is the results on 2009 Test Data. The
scores with a slash in the last two rows are low-
ercased and cased respectively. From the table we
1Not much time to do the experiments on English-French
direction. EBMT and HPB just used the Europarl corpus.
2The official automatic result is scored on 2525 sentences
out of the whole 3007 sentences in test set. The other 502
sentences are used as the development set for combination
evaluation task.
System Fr-En En-Fr
Baseline-E 25.64 24.47
Baseline-G 26.75 ?
EBMT 25.67 24.43
HPB 25.20 24.19
Combination 27.20/25.14 25.26/22.28
Official-Auto2 26.86/24.93 23.78/22.14
Table 5: Summary of Results on 2009 Test Data
can see that combination yielded 0.45 and 0.79 ab-
solute BLEU points over the best single system for
Fr-En and En-Fr direction respectively. However,
1.93 (7.2 relative) and 1.64 (6.58 relative) BLEU
points are dropped between cased and lowercased
results of both directions. Accordingly, training an
effective recasing model is very important for our
future work.
4 Conclusion
This paper presents our machine translation sys-
tem in WMT2009 shared task campaign. We de-
veloped a multi-engine framework which com-
bined the output results of the three MT sys-
tems and generated a new N -best list after CN
decoding. Then by using some global features
the rescoring model generated the final translation
output. The experimental result proved that the
combination module and rescoring module are ef-
fective in our framework.
We also applied simple yet effective methods
of genre and topical classification to remove noise
and out-of-domain sentences in the Giga corpus,
from which we built better translation models than
from Europarl.
In future work, we will refine our system frame-
work to investigate its effect on the tasks pre-
sented here, and we will develop more powerful
post-processing tools such as recaser to reduce the
BLEU loss.
Acknowledgments
This work is supported by Science Foundation Ireland (Grant
No. 07/CE/I1142). Thanks also to the reviewers for their
insightful comments and suggestions.
References
Chen, S. F. and Goodman, J. (1996). An Empirical Study of
Smoothing Techniques for Language Modeling. In Pro-
ceedings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, pages 310?318,
San Francisco, CA.
Chiang, D. (2005). A Hierarchical Phrase-Based Model for
Statistical Machine Translation. In Proceedings of the
98
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 263?270, Ann Arbor,
MI.
Ferizis, G. and Bailey, P. (2006). Towards practical genre
classification of web documents. In Proceedings of the
15th international conference on World Wide Web (WWW
?06), pages 1013?1014, New York, USA.
Fiscus, J. G. (1997). A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (ROVER). In Proceedings 1997 IEEE Work-
shop on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?352, Santa Barbara, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale EBMT
with Marker-Based Segmentation. In Proceedings of
the 10th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-04),
pages 95?104, Baltimore, MD.
Green, T. (1979). The Necessity of Syntax Markers. Two
experiments with artificial languages. Journal of Verbal
Learning and Behavior, 18:481?496.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the Joint Meeting of the Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2004), pages 169?176, Boston, MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Finding con-
sensus in speech recognition: Word error minimization
and other applications of confusion networks. Computer
Speech and Language, 14(4):373?400.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference
(EMNLP), pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S., Schwartz, R., Ayan,
N. F., and Dorr, B. J. (2007). Combining outputs from
multiple machine translation systems. In Proceedings
of the Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2007), pages 228?235, Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate with
targeted human annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Translation in the
Americas (AMTA 2006), pages 223?231, Cambridge, MA.
Stolcke, A. (2002). SRILM - An Extensible Language Mod-
eling Toolkit. In Proceedings of the International Confer-
ence Spoken Language Processing, pages 901?904, Den-
ver, CO.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU machine
translation system for IWSLT 2006. In Proceedings of the
International Workshop on Spoken Language Translation,
pages 31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
Joint Meeting of the Human Language Technology Con-
ference and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL 2006),
pages 72?77, New York, USA.
99
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the 4th International Workshop on Computational Terminology, pages 42?51,
Dublin, Ireland, August 23 2014.
Bilingual Termbank Creation via Log-Likelihood Comparison and
Phrase-Based Statistical Machine Translation
Rejwanul Haque, Sergio Penkale, Andy Way
?
Lingo24, Edinburgh, UK
{rejwanul.haque, sergio.penkale}@lingo24.com
?
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Bilingual termbanks are important for many natural language processing (NLP) applications, es-
pecially in translation workflows in industrial settings. In this paper, we apply a log-likelihood
comparison method to extract monolingual terminology from the source and target sides of a
parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a
bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel
terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe
excellent performance for all domains. Furthermore, we report the performance of our monolin-
gual terminology extraction model comparing with a number of the state-of-the-art terminology
extraction models on the English-to-Hindi datasets.
1 Introduction
Terminology plays an important role in various NLP tasks including Machine Translation (MT) and
Information Retrieval. It is also exploited in human translation workflows, where it plays a key role
in ensuring translation consistency and reducing ambiguity across large translation projects involving
multiple files and translators over a long period of time. The creation of monolingual and bilingual
terminological resources using human experts are, however, expensive and time-consuming tasks. In
contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed
to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or
discard anomalous items from an automatically extracted terminology list.
The automatic terminology extraction task starts with selecting candidate terms from the input domain
corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are
regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word
sequences are regarded as candidate terms (Deane, 2005).
Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et
al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and
TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which
is composed of two consecutive and independent processes:
1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences)
independently from the source and target sides of a parallel corpus,
2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based
Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003).
We then evaluate our novel bilingual terminology extraction model on various domain corpora consid-
ering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see
excellent performance for all data sets.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
42
The remainder of the paper is organized as follows. In Section 2, we discuss related work. In Section
3, we describe our two-stage terminology extraction model. Section 4 presents the results and analyses
of our experiments, while Section 5 concludes, and provides avenues for further work.
2 Related Work
Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can
be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches
dominate this field, with some of the leading work including the use of frequency-based filtering (Daille
et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Gar-
side, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm
(Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007).
In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006)
demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, sim-
ilarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both
sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the
most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998),
Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignment-
based terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike
our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might
be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied
statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the lat-
ter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In
contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use
to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al.
(2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms.
3 Methodology
In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we
extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel
corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms
using a PB-SMT model.
3.1 Monolingual Terminology Extraction
The monolingual term extraction task involves the identification of terms from a list of candidate terms
formed from all n-gram word sequences from the monolingual domain corpus (i.e. in our case, each side
of the domain parallel corpus, cf. Section 4.1). On both source and target sides, we used lists of language-
specific stop-words and punctuation marks in order to filter out anomalous items from the candidate
termlists. In order to rank the candidate terms in those lists, we used a log-likelihood comparison method
that compares the frequencies of each candidate term in both the domain corpus and the large general
corpus used as a reference.
1
The log-likelihood (LL) value of a candidate term (C
n
) is calculated using equation (1) from Gelbukh
et al. (2010).
LL = 2 ? ((F
d
? log(F
d
/E
d
)) + (F
g
? log(F
g
/E
g
))) (1)
where F
d
and F
g
are the frequencies of C
n
in the domain corpus and the generic reference corpus,
respectively. E
d
and E
g
are the expected frequencies of C
n
, which are calculated using (2) and (3).
E
d
= N
n
d
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (2)
E
g
= N
n
g
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (3)
1
Before the term-extraction process begins, we apply a number of preprocessing methods including tokenisation to the input
domain corpus and the generic reference corpus.
43
where N
n
d
and N
n
g
are the numbers of n-grams in the domain corpus and reference corpus, respectively.
Thus, each candidate term is associated with a weight (LL value) which is used to sort the candidate
terms: those candidates with the highest weights have the most significant differences in frequency in the
two corpora. However, we are interested in those candidate terms that are likely to be terms in the domain
corpus. Gelbukh et al. (2010) used the condition in (4) in order to filter out those candidate terms whose
relative frequencies are bigger in the domain corpus than in the reference corpus, and we do likewise.
F
d
/N
n
d
> F
g
/N
n
g
(4)
In contrast with Gelbukh et al. (2010), we extract multi-word terms up to 3-grams, whereas they focused
solely on extracting single word terms.
3.2 Creating a Bilingual Termbank
We obtained source and target termlists from the bilingual domain corpus using the approach described
in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the
extracted source and target termlists.
This section provides a mathematical derivation of the PB-SMT model to show how we scored can-
didate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilin-
gual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability
P(e
I
1
|f
J
1
) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually
comprise M translational features, and the language model, as in (5):
log P(e
I
1
|f
J
1
) =
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) + ?
LM
log P(e
I
1
) (5)
where e
I
1
= e
1
, ..., e
I
is the probable candidate translation for the given input sentence f
J
1
= f
1
, ..., f
J
and s
K
1
= s
1
, ..., s
k
denotes a segmentation of the source and target sentences respectively into the se-
quences of phrases (
?
f
1
, ...,
?
f
k
) and (e?
1
, ..., e?
k
) such that (we set i
0
:= 0):
?k ? [1,K] s
k
:= (i
k
; b
k
, j
k
), (b
k
corresponds to starting index of f
k
)
e?
k
:= e?
i
k?1
+1
, ..., e?
i
k
,
?
f
k
:=
?
f
b
k
, ...,
?
f
j
k
Each feature h
m
in (5) can be rewritten as in (6):
h
m
(f
J
1
, e
I
1
, s
K
1
) =
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (6)
Therefore, the translational features in (5) can be rewritten as in (7):
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) =
M
?
m=1
?
m
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (7)
In equation (7),
?
h
m
is a feature defined on phrase-pairs (
?
f
k
, e?
k
), and ?
m
is the feature weight of
?
h
m
.
These weights (?
m
) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out
500 sentence-pair development set for each of the experiments.
We create a list of probable source?target term-pairs by taking each source and target term from the
source and target termlists, respectively, provided that those source?target term-pairs are present in the
PB-SMT phrase-table. We calculate a weight (w) for each source?target term-pair (essentially, a phrase-
pair, i.e. (e?
k
,
?
f
k
)) using (8):
2
w(e?
k
,
?
f
k
) =
M
?
m=1
?
m
?
h
m
(
?
f
k
, e?
k
) (8)
2
Equation (8) is derived from the right-hand side of equation (7) for a single source?target phrase-pair.
44
In order to calculate w, we used the four standard PB-SMT translational features (
?
h
m
), namely forward
phrase translation log-probability (log P(e?
k
|
?
f
k
)), its inverse (log P(
?
f
k
|e?
k
)), the lexical log-probability
(log P
lex
(e?
k
|
?
f
k
)), and its inverse (log P
lex
(
?
f
k
|e?
k
)). We considered a higher threshold value for weights
and considered those term-pairs whose weights exceeded this threshold. For each source term, we con-
sidered a maximum of the four highest-weighted target terms.
Domain Parallel Corpus
Domain Sentences Words (English)
English-to-Spanish
Banking, Finance and Economics 50,112 548,594
Engineering 91,896 1,165,384
IT 33,148 367,046
Tourism and Travel 50,042 723,088
Science 79,858 1,910,482
Arts and Culture 9,124 100,620
English-to-Hindi
EILMT 7,096 173,770
EMILLE 9,907 159,024
Launchpad 67,663 380,546
KDE4 84,089 324,289
Reference Corpus
Language Sentences Words
English 4,000,000 82,048,154
Spanish 4,132,386 128,005,190
Hindi 10,000,000 182,066,982
Table 1: Corpus Statistics.
4 Experiments and Discussion
4.1 Data Used
We conducted experiments on several data domains for two different language-pairs, English-to-Spanish
and English-to-Hindi. For English-to-Spanish, we worked with client-provided data taken from six dif-
ferent domains in the form of translation memories. For English-to-Hindi, we used three parallel corpora
from three different sources (EILMT, EMILLE and Launchpad) taken from HindEnCorp
3
(Bojar et al.,
2014) released for the WMT14 shared translation task,
4
and a parallel corpus of KDE4 localization files
5
(Tiedemann, 2009). The EMILLE corpus contains leaflets from the UK Government and various local
authorities. The domain of the EILMT
6
corpus is tourism.
We used data from a collection of translated documents from the United Nations (MultiUN)
7
(Tiede-
mann, 2009) and the European Parliament (Koehn et al., 2005) as the monolingual English and Spanish
reference corpora. We used the HindEnCorp monolingual corpus (Bojar et al., 2014) as the monolingual
Hindi reference corpus. The statistics of the data used in our experiments are shown in Table 1.
4.2 Runtime Performance
Our terminology extraction model is composed of two main processes: (i) Moses training and tuning
(restricting the number of iterations of MERT to a maximum of 6), and (ii) terminology extraction. In
Table 2, we report the actual runtimes of these two processes on the six domain corpora. As Table
3
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4
http://www.statmt.org/wmt14/
5
http://opus.lingfil.uu.se/KDE4.php
6
English-to-Indian Language Machine Translation (EILMT) is a Ministry of IT, Govt. of India sponsored project.
7
http://opus.lingfil.uu.se/MultiUN.php
45
2 demonstrates, both MT system-building (training and tuning combined) and terminology extraction
processes are very short on each corpus. Given the crucial influence of bilingual terminology on quality
in translation workflows, we believe that the creation of such assets from scratch in less than 30 minutes
may prove to be a significant breakthrough for translators.
MT System Terminology
Building Extraction
English-to-Spanish
Banking, Finance and Economics 05:49 04:23
Engineering 06:47 04:33
IT 04:10 04:31
Tourism and Travel 05:34 04:24
Science 15:26 04:52
Arts and Culture 03:20 04:16
English-to-Hindi
EILMT 12:41 15:47
EMILLE 05:41 17.18
Launchpad 04:37 24.11
KDE4 04:05 16:50
Table 2: Runtimes (minutes:seconds) for MT system-building and bilingual terminology extraction on
the different domain data sets.
4.3 Human Evaluation
Of course, it is one thing to rapidly create translation assets such as bilingual termbanks, and another en-
tirely to ensure the quality of such resources. Accordingly, we evaluated the performance of our bilingual
terminology extraction model on each English-to-Spanish and English-to-Hindi domain corpus reported
in Table 1, with the evaluation goals being twofold: (i) measuring the accuracy of the monolingual ter-
minology extraction process, and (ii) measuring the accuracy of our novel bilingual terminology creation
model.
As mentioned in Section 3.2, a source term may be aligned with up to four target terms. For evaluation
purposes, we considered the top-100 source terms based on the LL values (cf. (1)) and their target coun-
terparts (i.e. one to four target terms). The quality of the extracted terms was judged by native Spanish
and Hindi speakers, both with excellent English skills, and the evaluation results are reported in Table
3. Note that we were not able to measure recall of the term extraction model on the domain corpora due
to the unavailability of a reference terminology set. The evaluator counted the number of valid terms in
the source term list for the domain in question, and the percentage of valid terms with respect to the total
number of terms (i.e. 100) is reported in the second column in Table 3. We refer to this as VST (Valid
Source Terms). For each valid source term there are one to four target terms that are ranked according to
the weights in (8). In theory, therefore, the top-ranked target term is the most suitable target translation of
the aligned source term. The evaluator counted the number of instances where the top-ranked target term
was a suitable target translation of the source term; the percentage with respect to the number of valid
source terms is shown in the third column in Table 3, and denoted as VTT (Valid Target Terms). The
evaluator also reported the number of cases where any of the four target terms was a suitable translation
of the source term; the percentage with respect to the number of valid source terms is given in the fourth
column in Table 3. Furthermore, the evaluator counted the number of instances where any of the four
target terms with minor editing can be regarded as suitable target translation; the percentage with respect
to the number of valid source terms is reported in the last column of Table 3. In Table 4, we show three
English?Spanish term-pairs extracted by our automatic term extractor where the target terms (Spanish)
are slightly incorrect. In all these examples the edit distance between the correct term and the one pro-
posed by our automatic extraction method is quite low, meaning that just a few keystrokes can transform
46
the candidate term into the correct one. In these cases editing the candidate term is much cheaper (in
terms of time) than creating the translations from scratch.
VST VTT1 VTT4 VTTME4
(%) (%) (%) (%)
English-to-Spanish
Banking, Finance and Economics 76 92.1 93.4 94.7
Engineering 84 90.5 91.7 94.1
IT 89 90.0 97.8 97.8
Tourism and Travel 72 86.1 93.1 93.1
Science 94 93.6 93.6 93.6
Arts and Culture 89 91.9 96.5 96.5
English-to-Hindi
EILMT 91 81.3 83.5 96.7
EMILLE 79 62.1 83.5 98.7
Launchpad 88 95.4 98.8 98.8
KDE4 79 88.6 89.8 94.9
Table 3: Manual evaluation results obtained on the top-100 term pairs. VST: Valid Source Terms, VTT1:
Valid Target Terms (1-best), VTT4: Valid Target Terms (4-best), VTTME4: Valid Target Terms with
Minor Editing (4-best).
Source Terms Target Terms Target Terms Edit
(using Bilingual Term Extractor) corrected with Minor Editing Distance
Shutter Obturaci?on Obturador 5
comment: wrong choice of inflection is likely caused by the term being most frequently used as
?shutter speed?
Lenses Objetivos EF Objetivos 3
comment: The qualifier ?EF? should not be present in the target, as it is not in the source
Leave Cancel Cancelaci?on Vacaciones Cancelaci?on de Vacaciones 3
comment: The preposition ?de? is missing in the target term
Table 4: Slightly wrong target terms corrected with minor editing.
In Table 3, we see that the accuracy of the monolingual term extraction model varies from 72% to 94%
for both English-to-Spanish and English-to-Hindi. For English-to-Spanish, the accuracy of our bilingual
terminology creation model ranges from 86.1% to 93.6%, 91.7% to 97.8% and 93.1% to 97.8% when
the 1-best, 4-best and 4-best with slightly edited target terms are considered, respectively. For English-
to-Hindi, the accuracy of our bilingual terminology creation model ranges from 62.1% to 95.4%, 83.5%
to 98.8% and 94.9% to 98.8% when the 1-best, 4-best and 4-best with slightly edited target terms are
considered, respectively.
We are greatly encouraged by these results, as they demonstrate that our novel bilingual termbank
creation method is robust in the face of the somewhat noisy monolingual term-extraction results; as a
consequence, if better methods for suggesting monolingual term candidates are proposed, we expect the
performance of our bilingual term-creation model to improve accordingly.
We calculated the distributions of unigram, bigram and trigram in the valid source terms (cf. Table 3)
and reported in Table 5. We also calculated the percentages of their distributions in the valid source terms
averaged over all 10 data sets. As can be seen from Table 3, the percentage of the average distribution of
the trigram terms is quite low (i.e. 2.5%). This result justifies our decision for extraction of up to 3-gram
terms.
47
Unigram Bigram Trigram
English-to-Spanish
Banking, Finance and Economics 55 20 1
Engineering 64 18 2
IT 75 12 2
Tourism and Travel 49 18 5
Science 91 3 0
Arts and Culture 76 10 3
English-to-Hindi
EILMT 73 17 1
EMILLE 35 37 7
Launchpad 85 3 0
KDE4 74 5 0
Average 80.4% 17.0 % 2.5%
Table 5: Distributions of unigram, bigram and trigram in the valid source term pairs (cf. second column
in Table 3).
4.4 Comparison: Monolingual Terminology Extraction
In this section we report the performance of our monolingual terminology extraction model (cf. Section
3.1) comparing with the performance of several state-of-the-art terminology extraction algorithms capa-
ble of recognising multiword terms. In order to extract monolingual multiword terms we used the JATE
toolkit
8
(Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic
tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The
JATE toolkit is currently available only for the English language. For evaluation purposes, we considered
the source-side of the English-to-Hindi domain corpora.
Algorithm Reference EILMT EMILLE Launchpad KDE4
LLC (Bilingual) cf. VST in Table 3 91 79 88 79
LLC 77 53 80 71
STF 46 04 54 44
ACTF 42 15 62 48
TF-IDF 50 36 45 17
Glossex Kozakov et al. (2004) 76 43 76 71
JK Justeson & Katz (1995) 42 13 58 42
NC-Value Frantzi et al. (2000) 46 34 52 25
RIDF Church & Gale (1995) 27 16 23 21
TermEx Sclano et al. (2007) 42 08 46 41
C-Value Ananiadou (1994) 49 44 62 40
Weirdness Ahmed et al. (1999) 77 57 82 63
Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Fre-
quency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz
For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)).
The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the
candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality
of the extracted terms (top-100 highest weighted) was judged by an evaluator with excellent English
skills, and the evaluation results are reported in Table 6. The evaluator counted the number of valid terms
8
https://code.google.com/p/jatetoolkit/
48
in the highest weighted 100 terms that were extracted using different state-of-the-art term extraction
algorithms.
The third row of Table 6 represents the percentage of the valid source terms extracted by our log-
likelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows rep-
resent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: aver-
age corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent
seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the best-
performing algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004)
algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best per-
forming algorithm on the EMILLE and the Lauchpad corpora.
We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus.
This might be caused by it containing information leaflets in a variety of domains (consumer, education,
housing, health, legal, social), which might bring down the percentage of valid source terms on this
corpus.
Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the
top-100 source terms from the bilingual term-pair list that were extracted using the method described in
Section 3.2. For comparison purposes we again report this percentage (VST in Table 3) in the second row
in Table 6. Our bilingual term extraction method discards any anomalous pairs from the initial candidate
term-pair list (cf. Section 3.2). This essentially removes some of the source entries that are not pertinent
to the domain. As a result, the percentage of the valid source terms extracted applying our bilingual
terminology extraction method (Table 3) is higher than the percentage of the valid source terms extracted
applying our monolingual terminology extraction algorithm (LLC) (Table 6). We clearly see from Tables
3 and 6 that this bilingual approach to term extraction not only achieves remarkable performance on the
bilingual task, but that when used in a monolingual context it outperforms most state-of-the-art extraction
algorithms, and is comparable with the best ones. We should also note that JATE?s implementation of
these algorithms (including Weirdness) uses language-dependent modules such as a lemmatizer, unlike
our implementation of LLC which is language-independent.
5 Conclusions and Future Work
In this paper we presented a bilingual multi-word terminology extraction model based on two inde-
pendent consecutive processes. Firstly, we employed a log-likelihood comparison method to extract
source and target terms independently from both sides of a parallel domain corpus. Secondly, we used
a PB-SMT model to align source terms to one or more target terms. The manual evaluation results
on ten different domain corpora of two syntactically divergent language-pairs showed the accuracy of
our bilingual terminology extraction model to be very high, especially in the light of the rather noisier
monolingual candidate terms presented to it. Given the reported high levels of performance ? minimum
levels of 93.1% and 94.9% in the 4-best set-up across all six domains for English-to-Spanish and all four
domains for English-to-Hindi, respectively ? we are convinced that the extracted bilingual multiword
termbanks are useful ?as is?, and with a small amount of post-processing from domain experts would be
completely error-free.
The proposed bilingual terminology extraction model has been tested on a highly investigated
language-pair, English-to-Spanish, and a less-explored and low-resourced English-to-Indic language-
pair, English-to-Hindi. Interestingly, the performance of the bilingual terminology extraction model
is excellent for the both language-pairs. We also tested several state-of-the-art monolingual terminol-
ogy extraction algorithms including our own (log-likelihood comparison) on the source-side of the four
English-to-Hindi domain data sets. According to the manual evaluation results, our monolingual multi-
word term extraction model proves to be the best-performing algorithm on two domain data sets and the
second best-performing algorithm on the remaining two domain data sets. Our monolingual multiword
terminology extraction method is clearly comparable to the state-of-the-art monolingual terminology
extraction algorithms.
In this work, we considered all n-gram word sequences from the domain corpus as candidate terms.
49
In future work, we would like to incorporate the candidate phrasal term identification model of Deane
(2005), which would omit irrelevant multiword units, and help us extend our evaluation beyond the top-
100 terms. We also plan to demonstrate the impact of the created termbanks on translator productivity in
a number of workflows ? different language pairs, domains, and levels of post-editing ? in an industrial
setting.
Acknowledgements
This work was partially supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of
CNGL at Dublin City University, and by Grant 610879 for the Falcon project funded by the European
Commission.
References
S. Ananiadou. 1994. A methodology for automatic term recognition. In COLING: 15th International Conference
on Computational Linguistics, pages 1034?1038.
K. Ahmad, L. Gillam and L. Tostevin. 1999. University of Surrey Participation in TREC8: Weirdness Indexing for
Logical Document Extrapolation and Retrieval (WILDER). In the Eighth Text REtrieval Conference (TREC-8).
National Institute of Standards and Technology, Gaithersburg, MD., pp.717?724.
R. Basili, A. Moschitti, M. Pazienza and F. Zanzotto. 2001. A contrastive approach to term extraction. In Pro-
ceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001). Nancy, France, 10pp.
K. Church and W. Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In
Proceedings of the 3rd Workshop on Very Large Corpora, pages 121?130. Cambridge, MA.
B. Daille, E. Gaussier and J-M. Lang?e. 1994. Towards automatic extraction of monolingual and bilingual termi-
nology. In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings. Kyoto,
Japan, pp.515?521.
P. Deane. 2007. A nonparametric method for extraction of candidate phrasal terms. In ACL-05: 43rd Annual
Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, USA, pp.605?613.
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic Recognition of Multi-word Terms: the C-value/NC-value
Method. International Journal of Digital Libraries. 3(2): 115?130.
E. Gaussier. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Cor-
pora. In COLING-ACL ?98, 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics, Proceedings of the Conference, Volume II. Montreal,
Quebec, Canada, pp.444?450.
A. Gelbukh, G. Sidorov, E. Lavin-Villa and L. Chanona-Hernandez. 2010. Automatic Term Extraction Using
Log-Likelihood Based Comparison with General Reference Corpus. In 15th International Conference on Ap-
plications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS vol. 6177. Berlin:
Springer. pp.248?255.
L. Ha, G. Fernandez, R. Mitkov and G. Corpas. 2008. Mutual bilingual terminology extraction. In LREC 2008:
6th Language Resources and Evaluation Conference. Marrakech, Morocco, pp.1818?1824.
T. He, T., X. Zhang and Y. Xinghuo. 2006. An Approach to Automatically Constructing Domain Ontology. In
Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC 2006.
Wuhan, China, pp.150?157.
J. S. Justeson, and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural language engineering, 1(1) 9?27.
S. Kim, T. Baldwin and M-Y. Kan. 2009. An Unsupervised Approach to Domain-Specific Term Extraction. In
Proceedings of the Australasian Language Technology Association Workshop 2009. Sydney, Australia, pp.94?
98.
P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X: The Tenth
Machine Translation Summit. Phuket, Thailand, pp.79?86.
50
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical
Machine Translation. In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions. Prague,
Czech Republic, pp.177?180.
P. Koehn, F. Och and H. Ney. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003: conference
combining Human Language Technology conference series and the North American Chapter of the Association
for Computational Linguistics conference series. Edmonton, Canada, pp. 48?54.
L. Kozakov, Y. Park, T. H. Fin, Y. Drissi, Y. N. Doganata, and T. Cofino. 2004. Glossary extraction and knowledge
in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web
Conference and the 2nd Asian Semantic Web Conference.
J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In 31st Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Columbus, Ohio,
USA, pp.17?22.
E. Lefever, L. Macken and V. Hoste. 2009. Language-Independent Bilingual Terminology Extraction from a
Multilingual Parallel Corpus. In EACL ?09 Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics. Athens, Greece, pp.496?504.
F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the
Association for Computational Linguistics, Proceedings of the Conference. Sapporo, Japan, pp.160?167.
F. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Transla-
tion. In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.
Philadelphia, PA, USA, pp.295?302.
O. Bojar, V. Diatka, P. Rychl?y, P. Stra?n?ak, A. Tamchyna, and D. Zeman. 2014. Hindi-English and Hindi-only
Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation
Conference (LREC?14). Reykjavik, Iceland.
P. Pantel and D. Lin. 2001. A Statistical Corpus-Based Term Extractor. In E. Stroulia and S. Matwin (eds.)
Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies
of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS vol. 2056. Berlin: Springer, pp.36?46.
P. Rayson and R. Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop
on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics (ACL 2000). Hong Kong, pp.1?6.
F. Sclano and P. Velardi. 2007. TermExtractor: a web application to learn the shared terminology of emergent web
communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software
and Applications (I-ESA 2007). Funchal, Madeira Island, Portugal, pp.287?290.
J. Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.
In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language
Processing (vol V), pages 237?248, John Benjamins, Amsterdam/Philadelphia.
Z. Zhang, J. Iria, C. Brewster and F. Ciravegna. 2008. A Comparative Evaluation of Term Recognition Algorithms.
In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC 2008),
pages 2108?2113, Marrakech, Morocco.
51
