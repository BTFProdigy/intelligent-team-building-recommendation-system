Disti l l ing dialogues - A method using natural dialogue 
dialogue systems development 
Arne  JSnsson  and  N i l s  Dah lb~ick  
Depar tment  of Computer  and  In format ion  Sc ience 
L inkSp ing  Un ivers i ty  
S-581 83, L INKOPING 
SWEDEN 
nilda@ida.liu.se, arnjo@ida.liu.se 
corpora for 
Abst ract  
We report on a method for utilising corpora col- 
lected in natural settings. It is based on distilling 
(re-writing) natural dialogues to elicit the type of 
dialogue that would occur if one the dialogue par- 
ticipants was a computer instead of a human. The 
method is a complement toother means uch as Wiz- 
ard of Oz-studies and un-distilled natural dialogues. 
We present he distilling method and guidelines for 
distillation. We also illustrate how the method af- 
fects a corpus of dialogues and discuss the pros and 
cons of three approaches in different phases of dia- 
logue systems development. 
1 In t roduct ion  
It has been known for quite some time now, that 
the language used when interacting with a comput- 
er is different from the one used in dialogues between 
people, (c.f. JSnsson and Dahlb~ick (1988)). Given 
that we know that the language will be different, 
but not how it will be different, we need to base 
our development of natural language dialogue sys- 
tems on a relevant set of dialogue corpora. It is our 
belief that we need to clarify a number of different 
issues regarding the collection and use of corpora in 
the development of speech-only and multimodal dia- 
logue systems. Exchanging experiences and develop- 
ing guidelines in this area are as important as, and in 
some sense a necessary pre-requisite to, the develop- 
ment of computational models of speech, language, 
and dialogue/discourse. It is interesting to note the 
difference in the state of art in the field of natu- 
ral language dialogue systems with that of corpus 
linguistics, where issues of the usefulness of different 
samples, the necessary sampling size, representative- 
ness in corpus design and other have been discussed 
for quite some time (e.g. (Garside t al., 1997; Atkins 
et al, 1992; Crowdy, 1993; Biber, 1993)). Also the 
neighboring area of evaluation of NLP systems (for 
an overview, see Sparck Jones and Galliers (1996)) 
seems to have advanced further. 
Some work have been done in the area of natu- 
ral language dialogue systems, e.g. on the design 
of Wizard of Oz-studies (Dahlb~ck et al, 1998), 
on measures for inter-rater eliability (Carletta, 
1996), on frameworks for evaluating spoken dialogue 
agents (Walker et al, 1998) and on the use of differ- 
ent corpora in the development of a particular sys- 
tem (The Carnegie-Mellon Communicator, Eskenazi 
et al (1999)). 
The question we are addressing in this paper is 
how to collect and analyse relevant corpora. We be- 
gin by describing what we consider to be the main 
advantages and disadvantages of the two currently 
used methods; studies of human dialogues and Wiz- 
ard of Oz-dialogues, especially focusing on the eco- 
logical validity of the methods. We then describe a 
method called 'distilling dialogues', which can serve 
as a supplement to the other two. 
2 Natural and Wizard of 
Oz-Dialogues 
The advantage of using real dialogues between peo- 
ple is that they will illustrate which tasks and needs 
that people actually bring to a particular service 
provider. Thus, on the level of the users' general 
goals, such dialogues have a high validity. But there 
are two drawbacks here. First; it is not self-evident 
that users will have the same task expectations from 
a computer system as they have with a person. Sec- 
ond, the language used will differ from the language 
used when interacting with a computer. 
These two disadvantages have been the major 
force behind the development of Wizard of Oz- 
methods. The advantage here is that the setting will 
be human-computer interaction. But there are im- 
portant disadvantages, too. First, on the practical 
side, the task of setting up a high quality simulation 
environment and training the operators ('wizards') 
to use this is a resource consuming task (Dahlb~ck et 
al., 1998). Second, and probably even more impor- 
tant, is that we cannot hen observe real users using 
a system for real life tasks, where they bring their 
own needs, motivations, resources, and constraints 
to bear. To some extent this problem can be over- 
come using well-designed so called 'scenarios'. As 
pointed out in Dahlb~ck (1991), on many levels of 
analysis the artificiality of the situation will not af- 
44 
fect the language used. An example of this is the 
pattern of pronoun-antecedent relations. But since 
the tasks given to the users are often pre-described 
by the researchers, this means that this is not a good 
way of finding out which tasks the users actually 
want to perform. Nor does it provide a clear enough 
picture on how the users will act to find something 
that satisfies their requirements. If e.g. the task is 
one of finding a charter holiday trip or buying a TV- 
set within a specified set of constraints (economical 
and other), it is conceivable that people will stay 
with the first item that matches the specification, 
whereas in real life they would probably look for 
alternatives. In our experience, this is primarily a 
concern if the focus is on the users' goals and plans, 
but is less a problem when the interest is on lower- 
level aspects, such as, syntax or patterns of pronoun- 
antecedent relationship (c.f. Dahlb~ick (1991)). 
To summarize; real life dialogues will provide a 
reasonably correct picture of the way users' ap- 
proach their tasks, and what tasks they bring to 
the service provider, but the language used will not 
give a good approximation of what the system un- 
der construction will need to handle. Wizard of Oz- 
dialogues, on the other hand, will give a reasonable 
approximation of some aspects of the language used, 
but in an artificial context. 
The usual approach has been to work in three 
steps. First analyse real human dialogues, and based 
on these, in the second phase, design one or more 
Wizard of Oz-studies. The final step is to fine-tune 
the system's performance on real users. A good ex- 
ample of this method is presented in Eskenazi et al 
(1999). But there are also possible problems with 
this approach (though we are not claiming that this 
was the case in their particular project). Eskenazi et 
al. (1999) asked a human operator to act 'computer- 
like' in their Wizard of Oz-phase. The advantage 
is of course that the human operator will be able 
to perform all the tasks that is usually provided by 
this service. The disadvantage is that it puts a heavy 
burden on the human operator to act as a comput- 
er. Since we know that lay-persons' ideas of what 
computers can and cannot do are in many respects 
far removed from what is actually the case, we risk 
introducing some systematic distortion here. And 
since it is difficult to perform consistently in similar 
situations, we also risk introducing non-systematic 
distortion here, even in those cases when the 'wiz- 
ard' is an NLP-professional. 
Our suggestion is therefore to supplement he 
above mentioned methods, and bridge the gap be- 
tween them, by post-processing human dialogues to 
give them a computer-like quality. The advantage, 
compared to having people do the simulation on the 
fly, is both that it can be done with more consis- 
tency, and also that it can be done by researchers 
that actually know what human-computer natural 
language dialogues can look like. A possible dis- 
advantage with using both Wizard of Oz-and real 
computer dialogues, is that users will quickly adapt 
to what the system can provide them with, and will 
therefore not try to use it for tasks they know it 
cannot perform. Consequently, we will not get a full 
picture of the different services they would like the 
system to provide. 
A disadvantage with this method is, of course, 
that post-processing takes some time compared to 
using the natural dialogues as they are. There is al- 
so a concern on the ecological validity of the results, 
as discussed later. 
3 Distilling dialogues 
Distilling dialogues, i.e. re-writing human interac- 
tions in order to have them reflect what a human- 
computer interaction could look like involves a num- 
ber of considerations. The main issue is that in cor- 
pora of natural dialogues one of the interlocutors i
not a dialogue system. The system's task is instead 
performed by a human and the problem is how to 
anticipate the behaviour of a system that does not 
exist based on the performance of an agent with dif- 
ferent performance characteristics. One important 
aspect is how to deal with human features that are 
not part of what the system is supposed to be able  
to handle, for instance if the user talks about things 
outside of the domain, such as discussing an episode 
of a recent TV show. It also involves issues on how 
to handle situations where one of the interlocuters 
discusses with someone lse on a different opic, e.g. 
discussing the up-coming Friday party with a friend 
in the middle of an information providing dialogue 
with a customer. 
It is important for the distilling process to have at 
least an outline of the dialogue system that is under 
development: Will it for instance have the capacity 
to recognise users' goals, even if not explicitly stat- 
ed? Will it be able to reason about the discourse 
domain? What services will it provide, and what 
will be outside its capacity to handle? 
In our case, we assume that the planned dialogue 
system has the ability to reason on various aspects 
of dialogue and properties of the application. In our 
current work, and in the examples used for illustra- 
tion in this paper, we assume a dialogue model that 
can handle any relevant dialogue phenomenon and 
also an interpreter and speech recogniser being able 
to understand any user input that is relevant o the 
task. There is is also a powerful domain reason- 
ing module allowing for more or less any knowledge 
reasoning on issues that can be accomplished with- 
in the domain (Flycht-Eriksson, 1999). Our current 
system does, however, not have an explicit user task 
model, as opposed to a system task model (Dahlb~ick 
45 
and JSnsson, 1999), which is included, and thus, we 
can not assume that the 'system' remembers utter- 
ances where the user explains its task. Furthermore, 
as our aim is system development we will not con- 
sider interaction outside the systems capabilities as 
relevant o include in the distilled dialogues. 
The context of our work is the development a 
multi-modal dialogue system. However, in our cur- 
rent work with distilling dialogues, the abilities of 
a multi-modal system were not fully accounted for. 
The reason for this is that the dialogues would be 
significantly affected, e.g. a telephone conversation 
where the user always likes to have the next con- 
nection, please will result in a table if multi-modal 
output is possible and hence a fair amount of the di- 
alogne is removed. We have therefore in this paper 
analysed the corpus assuming a speech-only system, 
since this is closer to the original telephone conversa- 
tions, and hence needs fewer assumptions on system 
performance when distilling the dialogues. 
4 Dis t i l l a t ion  gu ide l ines  
Distilling dialogues requires guidelines for how to 
handle various types of utterances. In this section 
we will present our guidelines for distilling a corpus 
of telephone conversations between a human infor- 
mation provider on local buses 1to be used for devel- 
oping a multimodal dialogue system (Qvarfordt and 
JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; 
Dahlb~ick et al, 1999; Qvarfordt, 1998). Similar 
guidelines are used within another project on devel- 
oping Swedish Dialogue Systems where the domain 
is travel bureau information. 
We can distinguish three types of contributors: 
'System' (i.e. a future systems) utterances, User ut- 
terances, and other types, such as moves by other 
speakers, and noise. 
4.1 Modifying system utterances 
The problem of modifying 'system' utterances can 
be divided into two parts: how to change and when 
to change. They are in some respects intertwined, 
but as the how-part affects the when-part more we 
will take this as a starting point. 
? The 'system' provides as much relevant infor- 
mation as possible at once. This depends on 
the capabilities of the systems output modal- 
ities. If we have a screen or similar output 
device we present as much as possible which 
normally is all relevant information. If we, on 
the other hand, only have spoken output the 
amount of information that the hearer can inter- 
pret in one utterance must be considered when 
1The bus time table dialogues are collected at 
LinkSping University and are available (in Swedish) on 
http://www.ida.l iu.se/~arnjo/kfb/dialoger.html 
distilling. The system might in such cases pro- 
vide less information. The principle of provid- 
ing all relevant information is based on the as- 
sumption that a computer system often has ac- 
cess to all relevant information when querying 
the background system and can also present it 
more conveniently, especially in a multimodal 
system (Ahrenberg et al, 1996). A typical ex- 
ample is the dialogue fragment in figure 1. In 
this fragment he system provides information 
on what train to take and how to change to a 
bus. The result of distilling this fragment pro- 
vides the revised fragment of figure 2. As seen in 
the fragment of figure 2 we also remove a num- 
ber of utterances typical for human interaction, 
as discussed below. 
* System utterances are made more computer-l ike 
and do not include irrelevant information. The 
latter is seen in $9 in the dialogue in figure 3 
where the provided information is not relevant. 
It could also be possible to remove $5 and re- 
spond with $7 at once. This, however, depends 
on if the information grounded in $5-U6 is need- 
ed for the 'system' in order to know the arrival 
time or if that could be concluded from U4. 
This in turn depends on the system's capabili- 
ties. If we assume that the dialogue system has 
a model of user tasks, the information in $5-U6 
could have been concluded from that. We will, 
in this case, retain $5-U6 as we do not assume a
user task model (Dahlb/ick and JSnsson, 1999) 
and in order to stay as close to the original di- 
alogue as possible. 
The next problem concerns the case when 'system' 
utterances are changed or removed. 
? Dialogue contributions provided by something or 
someone other than the user or the 'system' are 
removed. These are regarded as not being part 
of the interaction. This means that if some- 
one interrupts the current interaction, say that 
the telephone rings during a face-to-face inter- 
action, the interrupting interaction is normally 
removed from the corpus. 
Furthermore, 'system' interruptions are re- 
moved. A human can very well interrupt anoth- 
er human interlocuter, but a computer system 
will not do that. 
However, this guideline could lead to problems, 
for instance, when users follow up such interrup- 
tions. If no information is provided or the in- 
terrupted sequence does not affect the dialogue, 
we have no problems removing the interruption. 
The problem is what to do when information 
from the 'system' is used in the continuing dia- 
logue. For such cases we have no fixed strategy, 
46 
U4: 
$5: 
U6: 
$7: 
U8: 
$9: 
U10: 
$11: 
U12: 
S13: 
U14: 
$15: 
yes I wonder if you have any mm buses or (.) like express buses leaving from LinkSping 
to Vadstena (.) on sunday 
ja ville undra om ni hade ndgra 5h bussar eUer (.) typ expressbussar sore dkte frdn LinkSping 
till Vadstena (.) pd sSnda 
no the bus does not run on sundays 
nej bussen g~r inte pd sSndagar 
how can you (.) can you take the train and then change some way (.) because (.) 
to MjSlby 'n' so 
hur kan man (.) kan man ta tdg d sen byta p~ ndtt sStt (.) fSr de (.) 
till mjSlby ~ sd 
that you can do too yes 
de kan du gSra ocksd ja 
how (.) do you have any such suggestions 
hut (.) har du n~ra n~gra s~na fSrslag 
yes let's see (4s) a moment (15s) now let us see here (.) was it on the sunday you should travel 
ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr (.) va de p~ sSndagen du skulle dka pd 
yes right afternoon preferably 
ja just de eftermidda ggirna 
afternoon preferable (.) you have train from LinkSping fourteen twenty nine 
eftermidda gSrna (.) du hat t~g frdn LinkSping fjorton d tjugonie 
mm 
mm 
and then you will change from MjSlby station six hundred sixty 
sd byter du frdn MjSlby station sexhundrasexti 
sixhundred sixty 
sexhundrasexti 
fifteen and ten 
femton ~ tie 
Figure 1: Dialogue fragment from a real interaction on bus time-table information 
U4: I wonder if you have any buses or (.) like express buses going from LinkSping 
to Vadstena (.) on sunday 
S5: no the bus does not run on sundays 
U6: how can you (.) can you take the train and then change some way (.) because (.) 
to MjSlby and so 
$7: you can take the train from LinkSping fourteen and twenty nine and then you will 
change at MjSlby station to bus six hundred sixty at fifteen and ten 
Figure 2: A distilled version of the dialogue in figure 1 
the dialogue needs to be rearranged epending 
on how the information is to be used (c.f. the 
discussion in the final section of this paper). 
? 'System' utterances which are no longer valid 
are removed. Typical examples of this are the 
utterances $7, $9, $11 and $13 in the dialogue 
fragment of figure 1. 
* Remove sequences of utterances where the 'sys- 
tem' behaves in a way a computer would not do. 
For instance jokes, irony, humor, commenting 
on the other dialogue participant, or dropping 
the telephone (or whatever is going on in $7 
in figure 4). A common case of this is when 
the 'system' is talking while looking for infor- 
mation, $5 in the dialogue fragment of figure 4 
is an example of this. Related to this is when 
the system provides its own comments. If we 
can assume that it has such capabilities they 
are included, otherwise we remove them. 
The system does not repeat information that has 
already been provided unless explicitly asked to 
do so. In human interaction it is not uncommon 
to repeat what has been uttered for purposes 
other than to provide grounding information or 
feedback. This is for instance common during 
47  
U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the 
interstate buses 
ja ska va p~ rececentrum innan \]jorton ~ trettifem (.) f5 vi ska till 
l~ngf~irdsbussarna 
$5: aha (.) 'n' then you must be there around twenty past two something then 
jaha (.) ~ dd behhver du va here strax e~ter tjuge 5vet tvd n~nting d~ 
U6: yes around that 
ja ungefgir 
$7: let's see here ( l ls)  two hundred and fourteen Ryd end station leaves forty six (.) thirteen 'n' 
forty six then you will be down fourteen oh seven (.) 
d~ ska vise hSr (11s) tv~hundrafjorton Ryd 5ndh~llplatsen gdr ~5rtisex (.) tretton d 
\]Srtisex d~ dr du nere ~jorton noll sju 5) 
U8: aha 
jaha 
$9: 'n' (.) the next one takes you there (.) fourteen thirty seven (.) but that is too late 
(.) ndsta dr du nere 5) ~jorton d trettisju (.) men de 5 ju ~Sr sent 
Figure 3: Dialogue fragment from a real interaction on bus time-table information 
U2: Well, hi (.) I am going to Ugglegatan eighth 
ja hej (.) ja ska till Ugglegatan dtta 
$3: Yes 
ja 
U4: and (.) I wonder (.) it is somewhere in Tannefors 
och (.) jag undrar (.) det ligger ndnstans i Tannefors 
$5: Yes (.) I will see here one one I will look exactly where it is one moment please 
ja (.) jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn 
U6: Oh Yeah 
jar~ 
$7: (operator disconnects) (25s) mm (.) okey (hs) what the hell (2s) 
(operator connects again) hello yes 
((Telefonisten kopplar ur sig)) (25s) iihh (.) okey (hs) de va sore \]aan (2s) 
((Telefonisten kopplar in sig igen)) halld ja 
U8: Yes hello 
ja hej 
$9: It is bus two hundred ten which runs on old tannefors road that you have to take and get off at 
the bus stop at that bus stop named vetegatan 
det ~i buss tv~hundratio sore g~r gamla tanne~orsvSgen som du ~r  ~ka ~ g~ av rid 
den hdllplatsen rid den hdllplatsen sore heter vetegatan. 
Figure 4: Dialogue fragment from a natural bus timetable interaction 
search procedures as discussed above. 
? The system does not ask for information it has 
already achieved. For instance asking again if it 
is on Sunday as in $9 in figure 1. This is not un- 
common in human interaction and such utter- 
ances from the user are not removed. However, 
we can assume that the dialogue system does 
not forget what has been talked about before. 
4.2 Mod i fy ing  user  u t te rances  
The general rule is to change user utterances as lit- 
tle as possible. The reason for this is that we do not 
want to develop systems where the user needs to 
restrict his/her behaviour to the capabilities of the 
dialogue system. However, there are certain changes 
made to user utterances, in most cases as a conse- 
quence of changes of system utterances. 
Utterances that are no longer valid are removed. 
The most common cases are utterances whose 
request has already been answered, as seen in 
the distilled dialogue in figure 2 of the dialogue 
in figure 1. 
48 
Sl1: sixteen fifty five 
sexton \]emti/em 
U12: sixteen fifty five (.) aha 
sexton femti/em (.) jaha 
S13: bus line four hundred thirty five 
linje \]yrahundra tretti/em 
Figure 5: Dialogue fragment from a natural bus 
timetable interaction 
? Utterances are removed where the user discuss- 
es things that are in the environment. For 
instance commenting the 'systems' clothes or 
hair. This also includes other types of commu- 
nicative signals such as laughter based on things 
outside the interaction, for instance, in the en- 
vironment of the interlocuters. 
? User utterances can also be added in order to 
make the dialogue continue. In the dialogue in 
figure 5 there is nothing in the dialogue xplain- 
ing why the system utters S13. In such cases 
we need to add a user utterance, e.g. Which 
bus is that?. However, it might turn out that 
there are cues, such as intonation, found when 
listening to the tapes. If  such detailed analyses 
are carried out, we will, of course, not need to 
add utterances. Furthermore, it is sometimes 
the case that the telephone operator deliberate- 
ly splits the information into chunks that can 
be comprehended by the user, which then must 
be considered in the distillation. 
5 App ly ing  the  method 
To illustrate the method we will in this section try to 
characterise the results from our distillations. The 
illustration is based on 39 distilled dialogues from 
the previously mentioned corpus collected with a 
telephone operator having information on local bus 
time-tables and persons calling the information ser- 
vice. 
The distillation took about three hours for all 39 
dialogues, i.e. it is reasonably fast. The distilled 
dialogues are on the average 27% shorter. However, 
this varies between the dialogues, at most 73% was 
removed but there were also seven dialogues that 
were not changed at all. 
At the most 34 utterances where removed from 
one single dialogue and that was from a dialogue 
with discussions on where to find a parking lot, i.e. 
discussions outside the capabilities of the applica- 
tion. There was one more dialogue where more than 
30 utterances were removed and that dialogue is a 
typical example of dialogues where distillation actu- 
ally is very useful and also indicates what is normal- 
ly removed from the dialogues. This particular dia- 
logue begins with the user asking for the telephone 
number to 'the Lost property office' for a specific bus 
operator. However, the operator starts a discussion 
on what bus the traveller traveled on before provid- 
ing the requested telephone number. The reason for 
this discussion is probably that the operator knows 
that different bus companies are utilised and would 
like to make sure that the user really understands 
his/her request. The interaction that follows can, 
thus, in that respect be relevant, but for our pur- 
pose of developing systems based on an overall goal 
of providing information, not to understand human 
interaction, our dialogue system will not able to han- 
dle such phenomenon (JSnsson, 1996). 
The dialogues can roughly be divided into five dif- 
ferent categories based on the users task. The dis- 
cussion in twenty five dialogues were on bus times 
between various places, often one departure and one 
arrival but five dialogues involved more places. In 
five dialogues the discussion was one price and var- 
ious types of discounts. Five users wanted to know 
the telephone number to 'the Lost property office', 
two discussed only bus stops and two discussed how 
they could utilise their season ticket to travel out- 
side the trafficking area of the bus company. It is 
interesting to note that there is no correspondence 
between the task being performed uring the inter- 
action and the amount of changes made to the dia-  
logue. Thus, if we can assume that the amount of 
distillation indicates omething about a user's inter- 
action style, other factors than the task are impor- 
tant when characterising user behaviour. 
Looking at what is altered we find that the most 
important distilling principle is that the 'system' 
provides all relevant information at once, c.f. fig- 
ures 1 and 2. This in turn removes utterances pro- 
vided by both 'system' and user. 
Most added utterances, both from the user and 
the 'system', provide explicit requests for informa- 
tion that is later provided in the dialogue, e.g. ut- 
terance $3 in figure 6. We have added ten utterances 
in all 39 dialogues, five 'system' utterances and five 
user utterances. Note, however, that we utilised the 
transcribed ialogues, without information on into- 
nation. We would probably not have needed to add 
this many utterances if we had utilised the tapes. 
Our reason for not using information on intonation 
is that we do not assume that our system's peech 
recogniser can recognise intonation. 
Finally, as discussed above, we did not utilise the 
full potential of multi-modality when distilling the 
dialogues. For instance, some dialogues could be 
further distilled if we had assumed that the system 
had presented a time-table. One reason for this is 
that we wanted to capture as many interesting as- 
pects intact as possible. The advantage is, thus, that 
we have a better corpus for understanding human- 
49 
U2: Yees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum 
in LinkSping 
jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum 
i LinkSping. 
$3: mm When do you  want  to  leave? 
mm N~ir r i l l  du  ?ka? 
U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the 
interstate buses 
ja ska va p~ rececentrum innan fjorton d trettifem (.) f5 vi ska till 
l~ngfiirdsbussarna 
Figure 6: Distilled dialogue fragment with added utterance 
computer interaction and can from that corpus do 
a second distillation where we focus more on multi- 
modal interaction. 
6 Discuss ion 
We have been presenting a method for distilling hu- 
man dialogues to make them resemble human com- 
puter interaction, in order to utilise such dialogues 
as a knowledge source when developing dialogue sys- 
tems. Our own main purpose has been to use them 
for developing multimodal systems, however, as dis- 
cussed above, we have in this paper rather assumed 
a speech-only system. But we believe that the basic 
approach can be used also for multi-modal systems 
and other kinds of natural language dialogue sys- 
tems. 
It is important o be aware of the limitations of 
the method, and how 'realistic' the produced result 
will be, compared to a dialogue with the final sys- 
tem. Since we are changing the dialogue moves, by 
for instance providing all required information in one 
move, or never asking to be reminded of what the us- 
er has previously requested, it is obvious that what 
follows after the changed sequence would probably 
be affected one way or another. A consequence of 
this is that the resulting dialogue is less accurate as 
a model of the entire dialogue. It is therefore not an 
ideal candidate for trying out the systems over-all 
performance during system development. But for 
the smaller sub-segments or sub-dialogues, we be- 
lieve that it creates a good approximation of what 
will take place once the system is up and running. 
Furthermore, we believe distilled dialogues in some 
respects to be more realistic than Wizard of Oz- 
dialogues collected with a wizard acting as a com- 
puter. 
Another issue, that has been discussed previously 
in the description of the method, is that the distilling 
is made based on a particular view of what a dialogue 
with a computer will look like. While not necessari- 
ly being a detailed and specific model, it is at least 
an instance of a class of computer dialogue models. 
One example of this is whether the system is meant 
to acquire information on the user's underlying mo- 
tivations or goals or not. In the examples presented, 
we have not assumed such capabilities, but this as- 
sumption is not an absolute necessity. We believe, 
however, that the distilling process should be based 
on one such model, not the least to ensure a con- 
sistent treatment of similar recurring phenomena t 
different places in the corpora. 
The validity of the results based on analysing dis- 
tilled dialogues depends part ly on how the distilla- 
tion has been carried out. Even when using natural 
dialogues we can have situations where the interac- 
tion is somewhat mysterious, for instance, if some of 
the dialogue participants behaves irrational such as 
not providing feedback or being too elliptical. How- 
ever, if careful considerations have been made to stay 
as close to the original dialogues as possible, we be- 
lieve that distilled dialogues will reflect what a hu- 
man would consider to be a natural interaction. 
Acknowledgments  
This work results from a number of projects on de- 
velopment of natural language interfaces upported 
by The Swedish Transport & Communications Re- 
search Board (KFB) and the joint Research Program 
for Language Technology (HSFR/NUTEK) .  We are 
indebted to the participants of the Swedish Dialogue 
Systems project, especially to Staffan Larsson, Lena 
Santamarta, and Annika Flycht-Eriksson for inter- 
esting discussions on this topic. 
Re ferences  
Lars Ahrenberg, Nils Dahlb~ck, Arne JSnsson, 
and /~ke Thur~e. 1996. Customizing interac- 
tion for natural language interfaces. LinkSpin9 
Electronic articles in Computer and Informa- 
tion Science, also in Notes from Workshop on 
Pragmatics in Dialogue, The XIV:th Scandi- 
navian Conference of Linguistics and the VI- 
II:th Conference of Nordic and General Linguis- 
50 
tics, GSteborg, Sweden, 1993, 1(1), October, 1. 
http :/ / www.ep.liu.se / ea /cis /1996 / O01/. 
Sue Atkins, Jeremy Clear, and Nicholas Ostler. 
1992. Corpus design criteria. Literary and Lin- 
guistic Computing, 7(1):1-16. 
Douglas Biber. 1993. Representativeness in cor- 
pus design. Literary and Linguistic Computing, 
8(4):244-257. 
Jean Carletta. 1996. Assessing agreement on classi- 
fication tasks: The kappa statistic. Computation- 
al Linguistics, 22(2):249-254. 
Steve Crowdy. 1993. Spoken corpus design. Literary 
and Linguistic Computing, 8(4):259-265. 
Nils Dahlb/ick and Arne JSnsson. 1999. Knowledge 
sources in spoken dialogue systems. In Proceed- 
ings of Eurospeech'99, Budapest, Hungary. 
Nils Dahlb/ick, Arne JSnsson, and Lars Ahrenberg. 
1998. Wizard of oz studies - why and how. 
In Mark Maybury & Wolfgang Wahlster, editor, 
Readings in Intelligent User Interfaces. Morgan 
Kaufmann. 
Ntis Dahlb/ick, Annika Flycht-Eriksson, Arne 
JSnsson, and Pernilla Qvarfordt. 1999. An ar- 
chitecture for multi-modal natural dialogue sys- 
tems. In Proceedings of ESCA Tutorial and Re- 
search Workshop (ETRW) on Interactive Dialogue 
in Multi-Modal Systems, Germany. 
Nils Dahlb/ick. 1991. Representations ofDiscourse, 
Cognitive and Computational Aspects. Ph.D. the- 
sis, LinkSping University. 
Maxine Eskenazi, Alexander Rudnicki, Karin Grego- 
ry, Paul Constantinides, Robert Brennan, Christi- 
na Bennett, and Jwan Allen. 1999. Data collec- 
tion and processing in the carnegie mellon com- 
municator. In Proceedings of Eurospeech'99, Bu- 
dapest, Hungary. 
Annika Flycht-Eriksson and Arne JSnsson. 1998. A 
spoken dialogue system utilizing spatial informa- 
tion. In Proceedings of ICSLP'98, Sydney, Aus- 
tralia. 
Annika Flycht-Eriksson. 1999. A survey of knowl- 
edge sources in dialogue systems. In Proceedings 
of lJCAI-99 Workshop on Knowledge and Reason- 
ing in Practical Dialogue Systems, August, Stock- 
holm. 
Roger Garside, Geoffrey Leech, and Anthony 
MeEnery. 1997. Corpus Annotation. Longman. 
Arne JSnsson and Nils Dahlb/ick. 1988. Talking to a 
computer is not like talking to your best friend. In 
Proceedings of the First Scandinavian Conference 
on Artificial InterUigence, Tvoms?. 
Arne JSnsson. 1996. Natural language generation 
without intentions. In Proceedings of ECAI'96 
Workshop Gaps and Bridges: New Directions 
in Planning and Natural Language Generation, 
pages 102-104. 
Pernilla Qvarfordt and Arne JSnsson. 1998. Effects 
of using speech in timetable information systems 
for www. In Proceedings of ICSLP'98, Sydney, 
Australia. 
Pernilla Qvarfordt. 1998. Usability of multimodal 
timetables: Effects of different levels of do- 
main knowledge on usability. Master's thesis, 
LinkSping University. 
Karen Sparck Jones and Julia R. Galliers. 1996. 
Evaluating Natural Language Processing Systems. 
Springer Verlag. 
Marilyn A. Walker, Diane J. Litman, Candace A. 
Kamm, and Alicia Abella. 1998. Paradise: A 
framework for evaluating spoken dialogue agents. 
In Mark Maybury & Wolfgang Wahlster, editor, 
Readings in Intelligent User Interfaces. Morgan 
Kaufmann. 
51 
Dialogue and Domain Knowledge Management 
Systems 
in Dialogue 
Annika Flycht-Eriksson and Arne JSnsson 
Department ofComputer and Information Science 
LinkSping University, SE-581 83, LINKOPING, SWEDEN 
annfl@ida.liu.se arnjo@ida.liu.se 
Abst ract  
Intelligent dialogue systems must be able 
to respond properly to a variety of re- 
quests involving knowledge of the dia- 
logue, the task at hand, and the domain. 
This requires advanced knowledge rea- 
soning performed by various processing 
modules. We argue that it is impor- 
tant to understand the nature of the var- 
ious reasoning mechanisms involved and 
to separate not only, for instance, inter- 
pretation, generation, and dialogue man- 
agement but also domain knowledge and 
task reasoning. This facilitates portabili- 
ty of the dialogue system to new domains 
and makes it easier to enhance its capa- 
bilities. In this paper we will focus on the 
dialogue and domain knowledge reason- 
ing components and show how they can 
cooperate to achieve natural interaction. 
1 In t roduct ion  
As information services and domains grow more 
complex the complexity of dialogue systems in- 
creases. They tend to need more and more domain 
knowledge and the domain reasoning mechanisms 
also have to become more sophisticated. Utilis- 
ing domain knowledge reasoning is in many cases 
necessary for a dialogue system to interpret and 
respond to a request in an intelligent manner, es- 
pecially as requests can be vague and sometimes 
ambiguous. This involves not only requests for 
information from application specific knowledge 
sources, but also requests related to the properties 
and structures of the application and requests that 
are outside the scope of the application. Thus, 
dialogue systems must be able to access, gath- 
er and integrate knowledge from various domain 
knowledge sources and application systems in or- 
der to determine the precise meaning of a request 
and produce an appropriate r sponse. However, 
although the dialogue system gather information 
from various sources it differs from the informa- 
tion retrieval problem discussed for instance in 
Stein et al (1999). We assume that the tasks are 
well-defined and that the users have articulated 
information needs that they can express in specif- 
ic terms. 
In this paper we will discuss how these different 
tasks can be performed in dialogue systems of sim- 
ple service character, i.e. dialogue systems that 
can provide information given a set of parameters 
collected from the user (Hayes and Reddy, 1983). 
2 Types  o f  requests  and  
c la r i f i ca t ions  
Users interacting with a dialogue system utilise 
various communicative acts. Bunt (1989) makes a 
distinction between factual information acts and 
dialogue control acts. The latter is used to control 
the dialogue and the former involves any transfer 
of factual information. Factual information re- 
quests can be further divided into two basic types 
of requests: 
? Task related requests. Requests where the 
response from the dialogue system includes 
domain and task specific information 
? System related requests. Requests where the 
response includes information on what can be 
done with the system or pointers to other in- 
formation sources 
To be able to respond to questions on the sys- 
tem's capabilities and how to interpret he pro- 
vided information, the dialogue system needs to 
represent knowledge about itself, here called sys- 
tem information. Also, if an answer can not be 
found in the application system(s) the dialogue 
system should give as helpful information as pos- 
sible, for example suggesting other resources the 
user can consult. For this purpose knowledge is 
needed on where such information can be found. 
The requests for task related information can be 
divided into simple and complex requests. Sim- 
ple requests are basically requests for information 
121 
~ Interpreter 
I i Generator 
i Dialogue 1 
history 
~ .  I .... I 
"-~ Dialogue I_ 
M agor r 
Knowledge Knowledge 
Module 1 Module 2 
Knowledge 
Module 3 
Knowledge 
Module n 
Figure 1: A dialogue system architecture. The picture shows different processing modules: Interpreter, 
Generator, Dialogue Manager and Domain Knowledge Manager. Some of the knowledge sources: dia- 
logue model, domain task model, system task model, and various knowledge modules, are also depicted, 
but not the grammar and lexicon. 
concerning properties of and relations between 
simple objects, for which the answers can be val- 
ues of properties or names of objects. A simple 
object is typically an entity that can be identified 
by a name or a set of distinguishing features. Sim- 
ple requests can be specified by an arbitrary set 
of parameters. The parameters describe certain 
properties which constraints he search for an ob- 
ject, or the requested properties of an object or 
set of objects. A typical example of a simple re- 
quest is How fast is a Volvo 850?, which can be 
directly mapped onto a structure specifying that 
the requested object is 'Volvo 850' and the prop- 
erty requested is its 'speed', which in turn can be 
converted to an application system request. 
Complex requests on the other hand are con- 
cerned with the specification and construction of
compound objects. The specification of such an 
object requires that the user provides information 
on a specific set of parameters, which often in- 
volves several dialogue turns. The specification is 
used to construct a matching object by retriev- 
ing, and sometimes integrating, knowledge from 
one or several domain and application knowledge 
sources. Examples of complex requests are found 
in timetable information applications, uch as the 
ATIS dialogues. To answer requests on a trip, 
the system needs to have a number of parame- 
ters specified, such as departure and arrival time 
and place, before it is able to access the time- 
tables. However, for such systems there are al- 
so simple requests that can be directly mapped 
to a request from the background system, for in- 
stance, requests regarding meals on a flight that 
can be identified by a flight number, e.g. Is break- 
fast served on flight SK2818f. 
Since requests are specified by a set of entities 
the system needs capabilities to identify entities 
from descriptions (Hayes and Reddy, 1983). An 
attempt to map a description to an entity can have 
three different outcomes, a unique entity is found, 
the description is ambiguous and corresponds to 
several objects, or the description is unsatisfiable 
and no matching object can be found. There exist 
several strategies to deal with these problems, but 
all of them include some clarification from the user 
or domain reasoning. In dealing with ambiguous 
descriptions the system should be able to provide 
options or find a distinguishing feature that can 
be used to ask the user for clarification. Unsatisfi- 
able descriptions can be dealt with in three differ- 
ent ways: inform the user of the problem giving 
as helpful information as possible, find near misses 
by relaxing some of the features in thedescription, 
or find and inform the user of faulty presupposi- 
tions. 
3 D ia logue  sys tem arch i tec tures  
Dialogue systems often have a modular archio 
tecture with processing modules for interpreta- 
tion, dialogue management, background system 
access, and generation, see figure 1. The pro- 
cessing modules utilise a number of knowledge 
sources, such as, grammar, lexicon, dialogue mod- 
122 
el, domain model, and task model (for an overview 
of some systems, see Flycht-Eriksson (1999)). In 
this paper focus is on dialogue management and 
domain knowledge management, which includes 
background system access. 
3.1 Dialogue management 
The role of the Dialogue Manager differs slightly 
between different dialogue system architectures, 
but it's primary responsibility is to control the 
flow of the dialogue by deciding how the system 
should respond to a user utterance. This is done 
by inspecting and contextually specifying the in- 
formation structure produced by an interpreta- 
tion module. If some information is missing or 
a request is ambiguous, clarification questions are 
specified by the Dialogue Manager and posed to 
the user. Should a request be fully specified and 
unambiguous the background system can be ac- 
cessed and an answer be produced. As a basis 
for these tasks the Dialogue Manager can utilise 
a dialogue model, a task model, and a dialogue 
history. 
The Dialogue model holds a generic description 
of how the dialogue is to be constructed, i.e. to 
decide what action to take in a certain situation. 
It is used to control the interaction, which in- 
volves determining: 1) what the system should 
do next (and what module is responsible for car- 
rying out the task) and 2) deciding what com- 
municative action is appropriate at a given dia- 
logue state. There are various proposals on dia- 
logue models which can be divided in two groups: 
intention-based and structurally based. They dif- 
fer in how they model the dialogue, especially 
if the user's goals and intentions behind the ut- 
terance need to be captured or not. Structural- 
ly based models are often controlled using a di- 
alogue grammar whereas intention-based utilise 
plan operators. Furthermore, plan-based sys- 
tems use plan operators to model not only dia- 
logue knowledge but also task, domain and meta 
knowledge (c.f. Lambert and Carberry (1991), 
Ramshaw (1991), Ferguson et al (1996)). This 
allows for plan recognition to be the only process- 
ing mechanism needed. 
The System Task model represents how the sys- 
tem's tasks are performed, cf. Application De- 
scription (Hagen, 1999). However, the terms task 
and task model can refer to very different phe- 
nomena. It is important to make a clear distinc- 
tion between the system's task(s) and the user's 
task(s) (van Loo and Bego, 1993; Dahlb~ck and 
JSnsson, 1999). A user task is non-linguistic and 
takes place in the real world. Models of such 
tasks involve the user's goals and how they can be 
achieved (cf. Wahlster and Kobsa (1989)). Mod- 
els of system tasks describe how the system's com- 
municative and other tasks, e.g. database access, 
are carried out. 
A typical example of the difference between 
the two types of task models can be found in a 
time-table system where the user states that (s)he 
needs to be at the train station to catch a cer- 
tain train and requests information on buses go- 
ing there. The information that the user is going 
to the train station is user task model informa- 
tion, indicating that buses arriving after the de- 
parture thne of the train are not relevant. The 
system task model on the other hand models the 
information required for complex requests, uch as 
date and departure place in a time-table system 
(cf. Bennacef et al (1996)). It is used by the Di- 
alogue Manager when collecting user information 
in order to perform a background system access. 
In plan-based systems the domain models takes a 
similar role, but wider as they often also involves 
advanced problem solving. We will in this paper 
not consider user task models, only system task 
models. 
The Dialogue history records the focus of atten- 
tion (Grosz and Sidner, 1986) and contains infor- 
mation about objects, properties, and relations as 
well as other dialogue information such as speech 
act information and system task information. 
3.2 Domain Knowledge Management 
If a request is fully specified it can be used to re- 
trieve the desired information from a background 
system. This task is seldom discussed in litera- 
ture on dialogue systems, perhaps because it is 
considered a rather straight forward task. There 
are, however, several problems related to this. For 
example, in cases where the background system is 
distributed and consists of several domain and ap- 
plication system knowledge sources the dialogue 
system must know which of them to access, in 
what order, and how the results should be inte- 
grated into one answer. This type of knowledge 
can be represented in a domain task model. 
Other problems related to domain knowledge 
reasoning and application access where mentioned 
in section 2. Although fully specified, requests can 
contain vague or ambiguous information or even 
some errors that can not be detected and han- 
died without extensive domain knowledge. This 
type of domain knowledge is stored in domain 
knowledge sources, called knowledge modules in 
figure 1. They contain knowledge of the world 
that is talked about and can vary much in form 
and content. Information from a domain knowl- 
edge source is primarily used to find the relevant 
123 
Interpreter 
\[ Generator 
Timetable 
System 
and Help 
Information 
Figure 2: The MALIN dialogue system architecture in an application for local bus traffic time-table 
information. The dialogue model used is a dialogue gr~.mrnar, the dialogue history is modelled as a 
dialogue tree, and Information Specification Forms correspond to the system task model. The domain 
and application knowledge modules perform spatial and temporal reasoning, and provide time-table and 
system information controlled by recipes and integration rules. 
items and relations that are discussed, to supply 
default values, etc. The knowledge represented 
in a domain knowledge source is often coupled to 
the application system, e.g. a database system. 
In such cases it is often used to map information 
from a Dialogue Manager to concepts uitable for 
database search. It is for example common that 
user's give vague temporal descriptions that has to 
be mapped to more precise time intervals before 
the information can be used to access an applica- 
tion system. 
To develop a Dialogue Manager that easily can 
be cnstomi~ed to new domains and in which dif- 
ferent dialogue strategies can be explored, the Di- 
alogue Manager should only be concerned with 
phenomena related to the dialogue with the user. 
It should not be involved in the process of access- 
ing the background system or performing domain 
reasoning. These tasks should instead be carried 
out by a separate module, a Domain Knowledge 
Manager. 
The Domain Knowledge Manager is responsible 
for retrieving and coordinating knowledge from 
the different domain knowledge sources and ap- 
plication systems that constitutes the background 
system. The Dialogue Manager can deliver a re- 
quest to the Domain Knowledge Manager and in 
return expects an answer etrieved from the back- 
ground system. If a request is under-specified or 
contains inconsistencies from the Domain Knowl- 
edge Manager's point of view, a specification of 
what clarifying information is needed will instead 
be returned to the Dialogue Manager. 
4 MAL IN  
In what follows we describe and exemplify a di- 
alogue system with separate modules for dia- 
logue management and domain knowledge man- 
agement. The presentation will be based on the 
MALIN dialogue system architecture:, figure 2, 
which has been used to implement an application 
for time-table information for local bus traffic in 
ostergStland. 
One issue in the design of a dialogue system is 
how to control the various modules and the user 
interaction. In some systems there is no module 
responsible for the communication, i stead a sep- 
arate module, called hub (Aberdeen et al, 1999) 
or facilitator (Martin et al, 1999), is used for co- 
ordinating the modules and the internal informa- 
tion flow. Alternatively, the Dialogue Manager is 
the central unit of the system where the overall 
system behaviour is determined. 
The approach taken in MALIN is a combina- 
tion where a Dialogue Manager is the central con- 
troller of the interaction and the Domain Knowl- 
edge Manager is based on an agent architecture. 
XMALIN (Multi-modal Application of LINLIN) is a re- 
finement of the LINLINsystem (Ahrenberg et al, 1990; 
JSnsson, 1997) to handle also multi-modal interaction 
and more advanced applications. 
124 
4.1 The D ia logue  Manager  
In the MALIN dialogue model the dialogue is struc- 
tured in terms of discourse segments, and a dis- 
course segment in terms of moves and embed- 
ded segments. Utterances are analysed as linguis- 
tic objects which function as vehicles for atom- 
ic move segments. An initiative-response (IR) 
structure determines the compound iscourse seg- 
ments, where an initiative opens the IR-segment 
by introducing a new goal and the response clos- 
es the IR-segment (Dahlb~ck, 1991). The dis- 
course segments are classified by general speech 
act categories, such as question (Q) and an- 
swer (A) (JSnsson, 1997), rather than specialised 
(cf. (Hagen, 1999)), or domain related (Alexander- 
sson and Reithinger, 1995). The action to carry 
out for the Dialogue Manager, as modeled in a di- 
alogue grammar, depends on how domain entities 
are specified and their relation to other entities in 
the domain and the dialogue history. 
In the MALIN dialogue system architecture there 
is only One dialogue history maintained by the Di- 
alogue Manager. Thus, the other modules in the 
system have no memory of the previous interac- 
tion since this could cause conflicts. The dialogue 
history records focal information, that is, what 
has been talked about and what is being talked 
about at the moment. It is used for dialogue con- 
trol, disambiguation of context dependent utter- 
ances, and context sensitive interpretation. The 
dialogue history is represented as a dialogue tree. 
The nodes in the dialogue tree record information 
utilising various information structures depending 
on the application. 
For simple information requests we have identi- 
fied two important concepts, termed Objects and 
Properties (JSnsson, 1997) where Objects models 
the set of objects in the database and Proper- 
ties denotes a complex predicate ascribed to this 
set. The parameters Objects and Properties axe 
application dependent. We also utilise Markers for 
various purposes (J5nsson and StrSmb~ck, 1998), 
but they will not be further discussed in this pa- 
per. Structures that represent information about 
objects and properties (and markers) are termed 
OPMs. Figure 3 shows an example OPM which 
represents the request Which bus lines passes the 
North gate ?. 
For complex requests the Dialogue Manager 
needs an information structure that holds the pa- 
rameters needed before successful access of the 
background system can be performed. We call 
such structures Information Specification Forms 
(ISFs) (Dahlb~ck and JSnsson, 1999). Just like 
OPMs the ISFs are application dependent and be- 
Obj : #1 \[ BusIine : ? \] 
#2\ [  Stop: North gate \] 
Prop : PassesBy : Stop ~2 
Figure 3: An OPM for the utterance Which bus 
lines passes the North gate?. 
sides holding information they are also used as sys- 
tem task models, i.e. to inform the Dialogue Man- 
ager which parameters that has to be provided by 
the user. We have identified a number of differ- 
ent user information eeds (Qvarfordt, 1998) for 
which ISFs are needed. The most common, called 
trip information, occurs when the user needs to 
know how and when on a particular day, most of- 
ten the present day, one can travel from one point 
to another in town by bus. An ISF for such re- 
quests model information on departure and arrival 
destinations and information on arrival and/or de- 
parture time, which is required information. The 
user can also give information about the travel 
type, but this is optional. Figure 4 shows an emp- 
ty Trip ISF. 
Type : Trip 
Art  : req. 
Dep : req. 
TT ime : req. 
TType : opt. 
Figure 4: An empty trip ISF. 
Another common information eed, called route 
information, is when the caller wants information 
on which busses or trains that go from one point 
to another. This ISF is similar to the Trip ISF 
but time information is no longer required. 
For the time-table information application both 
structures, ISF and OPM, are needed. This is not 
the case for all types of applications but we believe 
that if an ISF is needed an OPM can also often 
be useful. 
4.2 The  Dom~;~ Knowledge Manager  
The domain knowledge sources and application 
systems in MALIN are implemented as agents and 
will from now on be called domain agents. Do- 
main agents provide different services, typically to 
retrieve and reason about some information giv- 
en some parameters, and can also request services 
from each other. Communication and cooperation 
among the agents are achieved by passing mes- 
sages. 
125 
Agent Service 
Spatial Reasoning Agent getBusStops(From.BusStop, From.Place, From.Street, From.Area, 
From.Town, FromBusStops) 
Spatial Reasoning Agent getBusStops(To.BusStop, To.Place, To.Street, To.Area, To.Town, 
ToBusStops) 
Temporal Reasoning Agent getTime(TTime.Time, TravelTime) 
Timetable Agent getTrips(FromBusStops, ToBusStops, TravelTime) 
Figure 5: An ex~nple of an uninstantiated recipe for trip information. 
UI: I want to go to the city cem;er. 
$2: The city center is a big area. Can you point in the map or give more specific information like 
a landmark or a street? 
U3: Are there any bus stops near the Garden square? 
$4: There are several bus stops near the Garden square. 
< Shows the bus stops in ti~e map > 
U5: Then I want to go there from the University. 
$6: When do you want to go? 
UT: On the 31st of April before lunch. 
$8: The 31st is not a valid date:, there are only 30 days in April. Give a new date please. 
U9: The 30th of April. 
S10: The alternative trips are shown in the table. 
< Shows a table of  trips > 
Figure 6: A hypothetical dialogue with the MALIN dialogue system for a local bus time-table information 
application. The dialogue is constructed based on a corpus of 43 dialogues collected with users of the 
current information service in order to illustrate some of the features of the dialogue and domain 
knowledge managers and our multi-modal system. 
In the application of MALIN "tO time-table in- 
formation, four different domain agents are used, 
see figure 2. The Temporal Reasoning Agent con- 
tain~ a calendar and reasons about temporal ex- 
pressions. The Spatial Reasoning Agent utilises 
a Geographical Information System and reason- 
ing mechanism used to deduce the relations be- 
tween geographical objects (Flycht-Eriksson and 
JSnsson, 1998). The Timetable Agent retrieves 
time-table information for local bus and train traf- 
fic from an Internet source. There is also a Sys- 
tem Information Agent which provides ystem in- 
formation like references to human operators for 
questions outside the scope of thne-table informa- 
tion. 
The processing of a request performed by the 
Domain Knowledge Manager is based on a knowl- 
edge structure called recipe. A recipe is applica- 
tion specific and consists of a series of service calls 
from different agents, which are executed in order 
to construct an answer to a specific request, see 
figure 5 for an example. Domain Knowledge Man- 
agement in general involves three steps. First the 
Domain Knowledge Manager has to decide how 
to treat the request, i.e. to produce one or more 
recipes. In most cases one recipe is enough, but 
sometimes the user has provided ambiguous infor- 
mation that cannot be resolved by the interpreter 
or the Dialogue Manager, in which cases several 
recipes are needed. The next step is to process 
the recipe(s). The processing must be carefully 
monitored and aborted if an error occurs. Final- 
ly, alternatives must be inspected and integrated 
into one answer that can be sent back to the Di- 
alogue Manager. For more details on the Domain 
Knowledge Manager, see Flycht-Eriksson (2000). 
4.3 Communicat ion  between DM and 
DKM 
To illustrate how the Dialogue Manager (DM) and 
the Domain Knowledge Manager (DKM) coop- 
erates in processing of requests and handling of 
clarifications, consider the hypothetical dialogue 
shown in figure 6. The dialogue tree in figure 7 
shows the resulting structure of the dialogue. 
The first utterance, U1, initiates a trip ISF. In- 
formation about the arrival location provided by 
the user is inserted in the ISF in the field Art, 
126 
D 
IR1 
U1 IR2 
IR3 
U3 $4 
IR4 IR5 S10 
S6 U7 S8 U9 
Figure 7: The dialogue tree resulting from the dialogue in figure 6. 
which results in the structure presented in figure 8 
included in IR1 in the dialogue tree. The ISF indi- 
cates that information about the departure place 
and time has to be further specified by the user 
by the marker req in the fields Dep and TTime 
(TravelThne). 
Type : Trip 
Art : \[ Area : 
Dep : req. 
TTime : req. 
TType : opt. 
City center \] 
Figure 8: The ISF in IR1 after processing of U1. 
However, before continuing the dialogue and 
asking the user for the information that is miss- 
ing in the ISF, the DM asks the DKM to validate 
the provided values. This validation is performed 
in order to detect vague or erroneous information 
that might have been given by the user. 
The arrival ocation in a trip ISF will be used to 
find suitable bus stops that can be used to search 
the time-table database. The validation of the 
arrival ocation therefore means that the Spatial 
Reasoning Agent tries to map the location to a 
small set of bus stops. In this case it discovers 
that Area: City Centre is a too vague description 
since it corresponds totoo many stops, in our case 
more than 5 stops. The DM is informed of this and 
is also given the information that more specific 
information like a point, a landmark or a street is 
required, figure 9. Thus, the user will not be asked 
to provide the value of another parameter since it 
would be an implicit confirmation that the arrival 
place is correct, instead a new IR-unit, IR2 in the 
dialogue tree, is created and a clarification, $2, is 
initiated based on the information from the DKM 
that indicates the problematic tem, the type of 
problem, and a possible solution to the problem. 
Status :
Item : 
Type : 
Solution :
Error 1 Area : City center \] 
TooMany : BusStops\[ U : 5\ ] \ ] \ ]  
SpecInfo : (Point, 
Landmark, 
Street) 
Figure 9: The response from the DKM to the do- 
main validation of the arrival ocation. 
Instead of answering the system's question the 
user takes the initiative by requesting new infor- 
mation, U3. This request results in a new m-unit, 
IR3, to be inserted in the dialogue tree as a clar- 
ification of the system's clarification in IR2, as 
shown in figure 7. The utterance is a simple re- 
quest and the DM utilises an OPM to model this, 
figure 10. 
Oh j :  
Prop : 
#l iS t?p :  ? \] \] #2 Landmark : Garden \] 
square J 
Near : Place2 : 
Figure 10: The OPM in IR3 after processing of 
U3. 
To answer this request means reasoning about 
spatial relations between geographical objects. 
The request is therefore sent to the DKM which 
asks the Spatial Reasoning Agent for information. 
The request is successfully processed and some 
nearby bus stops are found and sent back to the 
DM utilising the structure in figure 11. The DM 
can then ask the generator to present them to the 
user, $4. 
127 
Status : 
Stops: 
Su~es8 
Name:  
Id : 
Name:  
Id : 
Name:  
Id : 
Cen~.~rum \] " 
Snickareg. 30 
1268 
Linnegatan \] 
1220 J~ 
Stora forget \[ 
450 J 
Figure 11: The response from the DKM to the 
OPM in IR3. 
The user responds to this answer by confirming 
his departure location, U5, and thereby responds 
to the request $2 of IR2. He also provides an 
arrival location. This new information is repre- 
sented in the OPM of IR2, figure 12. 
Oh j :  
Prop : 
 a mark  ?r enl \] sqr  
#2 Landmark : University \] 
A r t  : #1 \] 
Dep : #2 \] 
Figure 12: The OPM in II:t2 after processing of 
U5. 
The DM resumes processing of the ISF in IR1 
and updates it with the arrival and departure loca- 
tion based on the information i  the OPM of IR2. 
Information about the arrival ocation is added to 
the previously provided information in the field 
Art. The new information about the departure 
location is inserted in the field Dep, yielding the 
structure in figure 13. 
Type : Tr ip 
t Area : A r t  : Landmark :
Dep : Landmark :
TT ime : req. 
TType : opt. 
City center \] 
Garden square 
University \] 
Figure 13: The ISF in IR1 after updates with in- 
formation from the subtree in IR2. 
Again the DM asks the DKM for domain val- 
idation of the partially specified ISF. Since both 
locations can be mapped to a limited number of 
bus stops the ISF is approved by the DKM. The 
DM now needs to have a time to complete the 
ISF, and consequently a new IR-unit, IR4 in the 
dialogue tree, is created and the user is, in utter- 
ance $6, asked for this. The answer U7 is a valid 
response to $6 and produces a new OPM, see fig- 
ure 14. 
Oh j: 
Prop : 
Figure 14: 
U7. 
Day : 
Date : Month : 
#1 
POD : 
T ime : Mod : 
\[ TTime: #I \] 
31 
April 
lunch 
before 
The OPM in IR4 after processing of 
The new information from IR4 is then inserted 
as TTime in the ISF of IR1. This results in a fully 
specified Trip ISF, figure 15. 
Type : 
Art : 
Dep : 
TT ime : 
TType : 
Trip 
Area : City center 
Landmark : Garden square 
Landmark : University \] 
. \[ Day : 31 "/ 
~a~e : L Month:  April 1 
~.  | POD:  lunch I 
1,me: \[ Mod : before \] 
opt. 
Figure 15: The ISF of 1R1 after updates with in- 
formation from IR4. 
The ISF is again sent to the DKM for valida- 
tion. When the Temporal Reasoning Agent tries 
to map the temporal description in TTime to a 
format suitable for time-table database search it 
discovers the erroneous date. The DKM then re- 
turns a response, figure 16, to the DM informing it 
of the error. The DM initiates a new clarification 
IR-unit, IR5, and a clarification is formulated, $8. 
Status : Error 
I tem : Date : Month : April 
Type : NotVal id : Up : 30 
Solution : Spec lnfo  : {Date} \] 
Figure 16: The response from the DKM to the 
domain validation of the time description. 
The user responds to the system's clarification 
request and provides a new date, ug. The re- 
sponse is modelled in an OPM in IR5, figure 17. 
\[ I I oo : 30 Obj : #1  Date  : Month  : Apr i l  
P rop :  \[ TT ime : ~1 \] 
Figure 17: The OPM of ItL5 after U9. 
128 
The information i the clarification request IR- 
unit, IR5, is propagated tothe ISF of IR1 which is 
updated. This time the new information replaces 
the old in -VTime since it was erroneous. The re- 
sulting ISF is presented in figure 18. 
Type : 
Art  : 
Dep : 
TTimc : 
Time : 
TType : 
Tr/p 
Area : Citycenter \] 
Landmark : Gardensquare J 
Landmark : University \] 
. r Day  : 30 " / 1 
lJa~e: \[ Month: April J J 
POD: lunch \] 
Mod : before J opt. 
Figure 18: The ISF of IR1 after integration with 
the information i  IR5. 
Once more a validation of the ISF is performed 
by the DKM. This time no problems are detected 
and a search for suitable trips can fmaUy be done. 
The DKM does this by first asking the Spatial 
Reasoning Agent to map the departure and arrival 
locations to two sets of bus stops, then asking the 
Temporal Reasoning Agent o map the vague tem- 
poral description to a precise time interval. Given 
this information the DKM then searches the time- 
table database to find one or more trips that ful- 
fill the requirements. The resulting trips are sent 
back to the DM and displayed to the user, S10. 
4.4 Implementat ion 
The MALIN dialogue system customised for the 
traffic information application is currently un- 
der development. The Dialogue Manager from 
the LINLIN dialogue system architecture has been 
adapted to allow also ISFs and we are currently 
specifying the dialogue grammar and how to han- 
dle focus tracking utilising ISFs and OPMs at the 
same time. 
The Domain Knowledge Manager is function- 
al utilising a Spatial Reasoner for one sub-area 
of OstergStland and a Temporal Reasoner. The 
Timetable Agent retrieves trip information ~om 
the current Internet based timetables. Recipes 
are developed for accessing these modules, but the 
System and Help Information knowledge source is 
not yet implemented. 
5 Conclusions and future work 
In this paper we have presented an architecture 
for dialogue systems where a Domain Knowledge 
Manager and a Dialogue Manager cooperate to 
achieve natural interaction. Information provid- 
ing dialogue systems based on this architecture 
can handle a variety of requests; imple and com- 
plex concerning the domain, and requests for sys- 
tem related information. 
Separating domain knowledge reasoning from 
dialogue and task knowledge reasoning has a num- 
ber of advantages. First of all, it is clearer what 
the responsibilities and possibilities of the differ- 
ent modules are, e.g. the dialogue manager han- 
dles the dialogue and not domain reasoning. Fur- 
thermore, it facilitates customisation to new ap- 
plication domains. Another important feature is 
that domain knowledge sources can easily be re- 
placed, added, removed, and reused. This implies 
that a system can be made more intelligent by 
adding new domain agents without changing the 
dialogue and task models. 
Future challenges are to apply the proposed ar- 
chitecture, utilising a Domain Knowledge Manag- 
er, to other domains and types of dialogue sys- 
tems, such as advisory or tutoring systems. For 
such systems other knowledge sources like user 
models and argumentation models are relevant 
and have to be incorporated in the system archi- 
tecture. 
6 Acknowledgments  
This work is supported by The Swedish Transport 
& Communications Research Board (KFB) and 
the Center for Industrial Information Technology 
(CENIIT). We are indebted to Lars Degerstedt, 
H~tk~n Johansson and Lena Santamarta for fruit- 
ful discussions. 
Re ferences  
John Aberdeen, Sam Bayer, Sasha Caskey, 
Lauire Damianos, Alan Goldschen, Lynette 
Hirschman, Dan Loehr, and Hugo Trappe. 
1999. Implementing practical dialogue sys- 
tems with the DARPA communicator architec- 
ture. In Proceedings of IJCAI'g9 Workshop on 
Knowledge and Reasoning in Practical Dialogue 
Systems, August, Stockholm. 
Lars Ahrenberg, Arne J5nsson, and Ntis 
Dahlbiick. 1990. Discourse representation 
and discourse management for natural lan- 
guage interfaces. In Proceedings of the Second 
Nordic Conference on Text Comprehension i
Man and Machine, T~by, Sweden. 
Jan Alexandersson a d Norbert Reithinger. 1995. 
Designing the dialogue component in a speech 
translation system. In Proceedings of the 
Ninth Twente Workshop on Language Technol- 
ogy (TWLT-9), pages 35--43. 
129 
S. Bennacef, L. Devillers, S. Rosset, and L. Lamel. 
1996. Dialog in the RAILTEL telephone-based 
system. In Proceedings of Inliernational Con- 
ference on Spoken Language Processing, IC- 
SLP'g6, volume 1, pages 550-553, Philadelphia, 
USA, October. 
Harry C. Bunt. 1989. Information dialogues 
as communicative action in relation to part- 
ner modelling and information processing. In 
M. M. Taylor, F. N~el, and D. G. Bouwhuis, 
editors, The Structure of Multimodal Dialogue, 
pages 47-73. Elsevier Science Publishers B.V. 
(North-Holland). 
Nils Dahlb$ck and Arue JSusson. 1999. Knowl- 
edge sources in spoken dialogue systems. In 
Proceedings of Eurospeeeh'99, Budapest, Hun- 
gary. 
Nils Dahlb~ck. 1991. Representations of Dis- 
course, Cognitive and Computational Aspects. 
Ph.D. thesis, LinkSping University. 
George Ferguson, James Allen, and Brad Miller. 
1996. TRAINS-95: Towards a mixed-initiative 
planning assistant. In Proceedings of the Third 
Conference on Artificial Intelligence Planning 
Systems, AIPS-96, pages 70-77. 
Armika Flycht-Eriksson and Arne JSnsson. 1998. 
A spoken dialogue system utilizing spatial infor- 
mation. In Proceedings of International Con- 
ference on Spoken Language Processing, IC- 
SLP'98, page 1207, Sydney, Australia. 
Annika Flycht-Eriksson. 1999. A survey of knowl- 
edge sources in dialogue systems. In Proceed- 
ings of IJCAI'g9 workshop on Knowledge and 
Reasoning in Practical Dialogue Systems, Au- 
gust, Stockholm, pages 41--48. 
Annika Flycht-Eriksson. 2000. A domain knowl- 
edge manager for dialogue systems. In Proceed- 
ings of the 1,~th European Conference on Arti- 
ficial Intelligence, ECAI 2000. IOS Press, Am- 
sterdam. 
Barbara J. Grosz and Candace L. Sidner. 1986. 
Attention, intention and the structure of dis- 
course. Computational Linguistics, 12(3):175- 
204. 
Eli Hagen. 1999. An approach to mi<ed initia- 
tive spoken information retrieval dialogue. Us- 
er modeling and User-Adapted Interaction, 9(1- 
2):167-213. 
Philip J. Hayes and D. Raj Reddy. 1983. Steps 
toward graceful interaction i  spoken and writ- 
ten man-machine communication. I ternation- 
al Journal of Man-Machine Studies, 19:231- 
284. 
Arne JSnsson and Lena Str5mb~ick. 1998. Ro- 
bust interaction through partial interpretation 
and dialogue management. In Proceedings of 
Coling/A CL '98, Montrdal. 
Arue J5nsson. 1997. A model for habitable and 
efficient dialogue management for natural an- 
guage interaction. Natural Language Engineer- 
ing, 3(2/3):103-122. 
Lynn Lambert and Sandra Carberry. 1991. A 
tripartite plan-based model of dialogue. In Pro- 
ceedings of the 29th Annual Meeting of the A CL, 
Berkeley, pages 193-200. 
David L. Martin, Adam J. Cheyer, and Douglas B. 
Moran. 1999. The open agent architecture: 
A framework for building distributed software 
systems. Applied Artificial Intelligence, 13(1- 
2):91-128, January-March. 
Peruilla Qvaffordt. 1998. Usability of multi- 
modal timetables: Effects of different levels of 
domain knowledge on usability. Master's thesis, 
LinkSping University. 
Lance A. Ramshaw. 1991. A three-level model for 
plan exploration. In Proceedings of the 29th An- 
nual Meeting of the A CL, Berkeley, pages 39- 
46. 
Adelheit Stein, Jon Atle Gulla, and Ulrich Thiel. 
1999. User-tailored planning of mixed initiative 
information-seeking dialogues. User Modeling 
and User-Adapted Interaction, (9):133-166. 
Wire van Loo and Harry Bego. 1993. Agent tasks 
and dialogue management. In Workshop on 
Pragmaties in Dialogue, The XIV:th Scandina- 
vian Conference of Linguistics and the VIII:th 
Conference of Nordic and General Linguistics, 
GSteborg, Sweden. 
Wolfgang Wahlster and Alfred Kobsa. 1989. User 
models in dialog systems. In User Models in 
Dialog Systems. Springer-Verlag. 
130 
Some empirical findings on dialogue management and domain ontologies in
dialogue systems ? Implications from an evaluation of BIRDQUEST
Annika Flycht-Eriksson
Department of Computer and
Information Science
Linko?ping University, Sweden
annfl@ida.liu.se
Arne Jo?nsson
Department of Computer and
Information Science
Linko?ping University, Sweden
arnjo@ida.liu.se
Abstract
In this paper we present implications
for development of dialogue systems,
based on an evaluation of the system
BIRDQUEST which combine dialogue in-
teraction with information extraction. A
number of issues detected during the
evaluation concerning primarily dialogue
management, and domain knowledge rep-
resentation and use are presented and dis-
cussed.
1 Introduction
In the field of Question Answering (Q&A), Infor-
mation extraction (IE) techniques have been used
successfully when it comes to handling simple fac-
toid questions, but the Q&A approach has yet not
reached the level of sophistication for handling con-
nected dialogue as is present in dialogue systems tai-
lored to background systems with structured data.
Dialogue capabilities allow for more precise formu-
lation of information requests and more natural in-
teraction. The challenge is to combine the IE tech-
niques and some of the features of Q& approaches
with dialogue systems (Burger et al, 2001). By a
successful combination of these techniques, users
would be allowed to access information derived
from a large set of, initially unstructured, docu-
ments, using dialogue functionalities, such as a di-
alogue history and clarification requests.
We have developed a first version of such a com-
bined system, BIRDQUEST (Jo?nsson and Merkel,
2003), which supports dialogue interaction to access
textual data in a bird encyclopaedia. The source data
is initially provided as unstructured text but refined
with IE techniques to be used within a dialogue sys-
tem framework. As a basis for many of the tasks
in the system domain knowledge represented in an
ontology is utilised.
To assess the approach and get insights into what
areas need further improvement an evaluation of the
system has been carried out. In this paper the results
of this evaluation are presented together with a dis-
cussion of implications for development of dialogue
systems with focus on dialogue management and the
use of domain ontologies.
2 Combining IE with dialogue interaction
in a system
Combining dialogue interaction with information
extraction has several benefits; dialogue is a natural
and efficient means of interaction and with IE tech-
niques information can be retrieved from unstruc-
tured information sources that are otherwise hard to
manage and search for a user. A possible way of
merging these two in a practical system is to have
two components, an information processing compo-
nent and an interaction component that, as a basis
for their tasks, use a set of shared knowledge sources
that define the scope of the language and domain.
2.1 The Information Processing Component
The Information Processing Component takes col-
lections of unstructured or semistructured docu-
ments and transforms them into structured informa-
tion that can be used by the Interaction Component
in the interaction with the user. The transforma-
tion utilise IE techniques, and the documents are
analysed in several ways going through lexical and
morphological, syntactical, and semantical analy-
sis (Sullivan, 2001).
A wide variety of pattern extraction rules are
used to identify the relevant information as slots and
fillers. The objective is to fill the database with rel-
evant information and ignore text segments that do
not meet the needs of the users. Figure 1 illustrates
how unstructured text is transformed into slot and
filler type information in the database.
Original text
Black-throated diver
Gavia arctica
58-73 cm, wingspan 110-130 cm.
In breeding plumage the head is gray
and the throat is black, the sides
of the throat striped in black and
white. [...]
Extracted information
NAME: Black-throated diver
LATIN NAME: Gavia arctica
MAX WING: 130
MIN WING: 110
MAX HEIGHT: 73
MIN HEIGHT: 58
BR PLUMAGE: ?the head is gray and the
throat is black, the sides
of the throat striped in
black and white.?
Figure 1: Original text passage from the text book
and the corresponding entry in the database (trans-
lated from Swedish).
2.2 The Interaction Component
The Interaction Component is responsible for the di-
alogue with the user. It collaborates with the user
to produce a query and access the structured infor-
mation sources to retrieve an answer to the query.
The interaction component in BIRDQUEST is based
on the MALIN framework (Dahlba?ck et al, 1999).
MALIN is a modularised dialogue system and it
separates dialogue management (DM) from domain
knowledge management (DKM) (Flycht-Eriksson
and Jo?nsson, 2000). The former handles the dia-
logue whereas the latter handles access to various
background information sources.
The Dialogue Manager is responsible for control-
ling the flow of the dialogue by deciding how the
system should respond to a user utterance. This
is done by inspecting and contextually specifying
the information structure produced by an interpreta-
tion module. The MALIN dialogue model classifies
the discourse segments by general speech act cate-
gories, such as question (Q) and answer (A), rather
than specialised (cf. (Hagen, 1999)), or domain re-
lated (Alexandersson and Reithinger, 1995). The di-
alogue manager instead utilise the focal parameters
to control interaction (cf. (Jokinen et al, 1998; De-
necke, 1997; Jo?nsson, 1995)). In MALIN dialogue
history is represented in dialogue objects with a pa-
rameter termed Objects, which identify a set of pri-
mary referents, and the parameter Properties which
denote a complex predicate ascribed to this set. In
BIRDQUEST Objects are normally birds and Proper-
ties model information about the birds, such as ap-
pearance, number of eggs and feed.
The Domain knowledge manager receives re-
quests from the dialogue manager and process them
further using domain knowledge, for example, dis-
ambiguation and mapping of vague concepts to ones
more suitable for database access. It then retrieves
and coordinates information from available informa-
tion sources, such as data and knowledge bases. If
a request is under-specified or contains inconsisten-
cies from the domain knowledge manager?s point of
view, a specification of what clarifying information
is needed will be returned to the dialogue manager
to help the formulation of a clarification question to
the user.
2.3 Knowledge sources
As a basis for the processing of documents and user
queries a number of knowledge sources are utilised.
Some are highly specialised and only used by one or
a few submodules of a component, for example the
dialogue model in the Interaction Component, while
others are more general and used for several tasks in
both components. These shared knowledge sources
comprise lexicon, grammar, and domain ontologies.
Building lexicon and grammars to be used for dif-
ferent tasks also involves several challenges but will
not be further discussed in this paper.
The term ontology is used very differently in var-
ious areas of computer science, ranging from sim-
ple taxonomies, meta data schemes, to logical the-
ories. A general and commonly used definition
given by Gruber (1993) is that ?An ontology is a
formal, explicit specification of a shared conceptu-
alisation?. A more practical view is to consider
an ontology as ?a world model used as a com-
putational resource for solving a particular set of
problems? (Mahesh and Nirenburg, 1995), i.e. a
database with information about what categories (or
concepts) exist in the world/domain, what properties
they have, and how they are related to one another.
An ontology provides a common vocabulary that
can be used to state facts and formulate questions
about the domain. Constructing an ontology that can
be shared by the Information Processing Component
and the Interaction Component then gives us a pos-
sible way to bridge users? expression and queries to
the information contained in the unstructured docu-
ments.
3 Constructing the domain ontology
A challenge when constructing a shared domain on-
tology lies in capturing and including two different
conceptualisations of the domain, the one present in
the information sources and the one users have. The
shared ontology for the BIRDQUEST system was de-
veloped based on the analysis of two different types
of empirical material, a bird encyclopaedia and a
question corpus. The corpus consists of more than
250 questions about birds. It was collected by The
Swedish Public Service Television Company on a
web site for one of their nature programs, where the
public could send in questions, i.e. it is not a dia-
logue corpus.
The analysis of the empirical material focused
on identifying objects and properties, which in turn
were organised using hyponym relations. From
the encyclopaedia a conceptualisation underlying
the structure and presentation of information that
were to be extracted by the Information Process-
ing Component was constructed. The result was a
system-oriented domain ontology representing ex-
perts? view of the domain. The question corpus
yielded a user-oriented conceptualisation of the do-
main, thus providing a non-expert view of the do-
main useful for the interaction component. These
two conceptualisations were then merged to form a
shared domain ontology for all components of the
system.
The users? view of the domain as reflected in the
questions seemed to correspond to the one found in
the reference book, most objects and properties were
the same, but there were two aspects that deviated.
The first concerned the classification of birds and the
second the granularity of the properties of birds.
  Users sometimes utilised another way of cat-
egorising birds from the biologically oriented
taxonomy in the reference book, talking about
?Spring birds?, ?Small birds?, ?Migratory
birds?, and ?Birds of prey? instead of orders,
families, kins etc.
  In many cases the properties of the birds were
more general than the terms used in the book,
for example questions about a bird?s appear-
ance, e.g. What does a European Robin look
like? which includes plumage, size, body
shape, description of beak and feet, etc.
Since the two conceptualisations had many ob-
jects and properties in common and these were re-
lated in similar ways they could be integrated in the
following way (cf. figure 2). Taking the system-
oriented ontology as a starting point the new cate-
gories of birds found in the question corpora were
added. Allowing multiple inheritance new links be-
tween existing categories and new categories were
added. Note, for example, how the new category
?Small bird? is introduced and a new link is added to
?Finches? in figure 2. In a similar manner the vague
properties were introduced and linked to the exist-
ing properties. This is illustrated in figure 2 where
two new levels are introduced, ?Wingspan? and
?Length? are sub-properties of the property ?Size?,
which in turn is a sub-property of the property ?Ap-
pearance?.
4 Evaluating BIRDQUEST
As stated above BIRDQUEST was developed based
on a corpus of questions. For further development
of BIRDQUEST, we needed to assess its strengths
and limitations during dialogues with real users. An
evaluation of the system was thus performed with
Hyponym/Meronym
Instance Of
Object instance
System concept
User concept
FamilySmall bird
Bird
Species
Order
Cardinality: 0..1
Range:Number Range: String
Bird
Geographic location
Domain:
Range:
Migratory birdDomain:
OBJECTS
PROPERTIES BirdDomain:Range: Value
Cardinality: 0..N
WingspanLength Eclipse
plumage
Summer
plumage
Winter
plumage
Plumage
Migratory
bird
Cardinality: 0..N
DistributionRELATIONS
Breeding Winter
distributiondistribution
Range
Pine
Grosbeak
Finches
Songbirds
Appearance
Size
Figure 2: A part of the integrated ontology representing the conceptualisations of both bird encyclopaedia
and users.
the goal of detecting problems concerning inter-
pretation, dialogue management, and representation
and use of domain knowledge.
4.1 Data collection
BIRDQUEST is intended to be used by casual users
without previous experience of dialogue systems or
extensive knowledge of birds. It was therefore eval-
uated in a walk-up and use situation similar to a real
use situation during a day when the public was in-
vited to the university. In that respect the situation
resembles that of Gustafson and Bell (2000), though
slightly more controlled.
We had six machines running BIRDQUEST during
2 hours and 30 minutes and collected dialogues from
27 users. They received minimal instructions in ad-
vance, they were only told that the system can an-
swer questions on Nordic birds, that it understands
Swedish, and that the dialogue would be recorded.
The resulting corpus consisting of 27 dialogues
have a total number of 518 user utterances, with a
mean of 19 for each user. However, with individ-
ual differences, for instance, three users posing more
than 40 utterances to the system and three users pos-
ing less than 5.
Personal data about age, gender, interest in birds,
and knowledge of birds were collected together with
each dialogue. The users where of varying age, 5
female and 22 male. Most of them had no inter-
est in birds, nor any knowledge of birds. Thus, de-
spite having no interest in birds, they were fairly
representative of the intended users. Besides the
logged dialogue, the users were also asked to fill
out a small questionnaire on how they liked to use
the system. Most users thought the system was fun
to use, on a 10-graded scale we had a mean of 7.1.
The users also though that it was fairly easy to use
BIRDQUEST, mean 6.1. On the question how they
liked the system we had a score of 4.7, i.e. the users
neither disliked nor liked BIRDQUEST.
4.2 Corpus annotation and initial analysis
As we had no predefined tasks we did not have
a situation that allowed for a controlled evalua-
tion, as e.g. PARADISE (Walker et al, 1998) or
PROMISE (Beringer et al, 2002). Instead we used
a combination of quantitative and qualitative ap-
proaches to analyse the collected dialogue corpus.
The dialogues were tagged in order to provide statis-
tics over successful and problematic information ex-
changes.
The user utterances were categorised as in Ta-
ble 1 and the categorisation of responses from
BIRDQUEST is presented in Table 2.
Table 1 shows that approximately half of the users
utterances (48%) were involved in successful infor-
mation exchanges where the user initiated an infor-
Table 1: User utterances
No of Percentage of
utterances user utterances
Interpretable
Requests 189 37%
Cooperative CR
Responses 55 11%
Uncooperative
CR responses 11 2%
Out of scope 121 23%
Mis-interpreted 141 27%
Table 2: System utterances
No of Percentage of
utterances system utterances
Successful resp. 180 35%
Clarification req. 70 13%
Incorrect resp. 15 3%
Incorrect focus 16 3%
Error message 240 46%
mation request or answered a clarification request
from the system. We also see that 25% of the
user?s utterances are erroneous in some way and that
BIRDQUEST failed on 141 utterances, as will be fur-
ther discussed in section 5.
From Table 2 we see that BIRDQUEST presented
180 successful responses. A successful response
is a response where BIRDQUEST presents infor-
mation found in the database. A response where
the bird encyclopaedia does not include the infor-
mation and BIRDQUEST responds e.g. Information
on wing span is missing for magpie. is, however,
also considered successful. The reason being that
BIRDQUEST successfully accessed the database and
presented whatever information was there, includ-
ing cases where there was no information in the
database. Among the 180 there are 55 such re-
sponses, so they are not rare, and shows one of the
many interesting problems we encountered in the
development of a dialogue system based on infor-
mation extraction from a text book.
4.2.1 Clarifications
Notable is that a fair amount of the dialogue
moves involve clarifications. The system initiates
70 clarification sub-dialogues in order to transform a
vague information request to a specific question, as
exemplified by the excerpt below 1.
U17: What do water fowls look like?
S18: Choose one of the following Water fowls: Barnacle
Goose, Tufted Duck, Bean Goose, Whooper Swan,
Gadwall,...
U19: gadwall
S20: You can receive information about the size and
plumage of a Gadwall. Please specify what you are
interested in.
U21: size
S22: A Gadwall has a length of 56 cm from beak tip to
tail.
Information about wingspan is missing for Gadwall
The basis for this type of clarification is domain
knowledge collected from the domain ontology. Ut-
terance U17 is under specified since the object, the
bird family ?Water fowls?, can refer to a number of
different species of birds, and the property ?Appear-
ance?, is vague. To pose clarification question S18,
information about which species belong to the given
family is gathered from the ontology and the user is
asked to chose one of them. Next, in S20, the ontol-
ogy is accessed to retrieve the sub-properties of ap-
pearance. When the user has chosen a specific prop-
erty (U21) the request is sufficiently specified. The
ontology is used to find the sub-properties of ?Size?
and these are then used to access the database and
the result is presented to the user (S22).
The users responded cooperatively to 55 clarifi-
cation requests from the system and incorrectly 11
times. A typical example of the latter is seen below.
S22: You can receive information about size and
plumage of a Blue Tit. Please specify what you are
interested in.
U23: blue tit
Dialogue management, such as clarification sub-
dialogues, thus plays an important role for the per-
formance of BIRDQUEST.
Contextual interpretation and dialogue history
management are other important dialogue phenom-
ena from MALIN that are frequently utilised in the
dialogues. Managing dialogue history is, however,
not trivial. There are 16 cases in the corpus, termed
Incorrect focus in Table 2, when BIRDQUEST
presents doubtful responses because of how dia-
logue history is handled, as will be further discussed
in section 5.1.
1All examples are translations of excerpts from the Swedish
dialogue corpus.
4.2.2 Utterances out of scope for BIRDQUEST
Approximately half of the non-successful user ut-
terances (23% of all user utterances) were ques-
tions that BIRDQUEST will never be able to an-
swer. Beringer et al (2002) use the term incooper-
ative user for users who ?fall out of the role or pur-
posely misuse the system.?, and propose to exclude
them in evaluations. We include such users in our
corpus, but group them together in a wider category
called Out of scope.
Out of Scope utterances include user requests for
information that is outside the scope of the applica-
tion, such as How do you kill crows?, or socialisation
utterances (Gustafson and Bell, 2000) such as How
are you?. Utterances can also be out of the database?
scope, e.g. How high does a magpie fly? is such an
utterance since there is no information on how high
birds fly in the Bird encyclopaedia. These type of
requests are further discussed in section 5.5
The reason for grouping such utterances together
is that BIRDQUEST can never present information
to them. Instead, we need to add a number of
well-designed responses informing the user on the
system?s abilities. Utterances that are out of the
system?s scope require different types of responses
from the system, and the corpus gave us valuable in-
sights on the importance of system help messages
describing what BIRDQUEST can and cannot do.
4.2.3 Utterances where BIRDQUEST fails
Finally, there are those utterances where the sys-
tem failed, i.e. those where an answer can be found
in the encyclopaedia, but where BIRDQUEST fails
to present a successful response for various reasons.
Such utterances comprise 27% of the users? input.
We have further analysed these and categorised
them as being 1) spelling mistakes, 2) lexical gaps,
or 3) grammatically out of scope, as seen in Table 3.
Table 3 includes only utterances that can be success-
fully responded to, not, for instance, misspellings in
utterances that are out of the systems? scope.
Table 3 only gives a very brief indication on the
nature of non-interpretable utterances in the corpus.
For instance, each utterance is tagged as being of one
type only, with misspellings having highest priority
and missing grammar rules the lowest. Furthermore,
there could be several misspellings in one utterance.
It is also the case that the categories overlap, i.e.
Table 3: User utterances not interpreted by
BIRDQUEST
No of Percentage of
utterances system utterances
Misspelled 28 5%
Not in lexicon 64 12%
Not in grammar 50 10%
utterances can belong to more than one category, e.g.
a misspelled word can also be missed in the lexi-
con. There are three such utterances tagged mis-
spelled that also contain words not in the lexicon. 11
of the utterances tagged misspelled cannot be han-
dled because of missing grammar rules. Thus, with
a spelling checker, BIRDQUEST can handle another
14 utterances, i.e. misspelling on its own only ac-
counts for 2,5% of the utterances that BIRDQUEST
cannot handle.
5 Implications for development
Our main goal was to collect data for further de-
velopment. Utterances in the categories involving
unsuccessful information exchanges, such as incor-
rect responses and mis-interpretations, were there-
fore analysed more closely.
5.1 Problematic focus management
Focus management and methods to determine topic
shifts play an important role in many dialogue sys-
tems. Our corpus includes a variety of problematic
cases, that need further consideration, both on how
focus is handled in the dialogue manager and how
the grammar rules are constructed.
When a user poses a question that gives rise to
a partially interpreted request the dialogue manager
seeks the dialogue history to see if it can be contex-
tually specified. This works well in most cases when
utterances are elliptic but in some cases, especially
when the interpreter has failed to produce a complete
interpretation, it can cause faulty responses, see the
example below.
U35: How old does a common gull get?
S36: Information about the greatest age of a Common
Gull:
Greatest age: 29 years and 7 months
U37: What migratory birds exist?
S38: Information about range is missing for Common
Gull.
The reason for not being able to correctly inter-
preting U37 is that migratory birds is not a well de-
fined concept in the bird encyclopaedia 2. Thus, only
a partial interpretation with the property ?Range?,
based on interpreting ?exist? as connected to a loca-
tion, is produced. The partial interpretation is then
combined with the object ?Common gull? from U35
and an incorrect response is produced in S38.
A more difficult problem to handle is when there
has been a long segment of uninterpreted user ut-
terances often after sequences of misspellings or re-
quests outside the scope of the system. Normally,
such sequences of utterances are contextually inter-
preted utilising the dialogue history and a successful
response can be presented to the user. There are,
however, also ?faulty? cases, as seen in the dialogue
fragment below3. Here, the user is trying to shift
focus and thus the property ?Feed? should not have
been inherited from the previous question (U11). A
more appropriate response in S18 would have been
You can receive information about a song thrush?s
flight, feed, breeding location, appearance,... Please
specify what you are interested in.
U11: What do the honey buzzard eat
S12: Information about feed for a Honey Buzzard:
Main feed consists of Honeycomb,[...]
U13: What birds can imitate speech
S14: Failed to interpret the question. Please try again
U15: Talking birds
S16: Failed to interpret the question. Please try again
U17: Song thrush
S18: Information about feed for a Song Thrush:
Worms, larvas and mollusc etc[...]
The problem of dialogue history and fo-
cus shifts does not have a simple solution.
Leceuche et al (2000) use focus rules based on re-
lations between the various things in the domain
to track focus shifts. A similar approach is to en-
code the domain in a type hierarchy and use under-
specified feature structures to determine clarification
requests (Denecke, 1997). Jokinen et al (1998) use
a topic model based on a tree organisation of domain
knowledge to detect topic shifts. Such approaches
would correctly handle the dialogue above and start
a new focus space after U13, as there is no relation
2Many bird species do not migrate every year and often
some exemplars stay when most birds of that specie migrate
etc. cf. section 5.4.
3The Swedish name of Song thrush, (Sw. taltrast), can be
literally translated as ?Talking thrush?
between the items in U11-S12 and U13. However,
as we assume that it is a good idea to keep items
in focus even after user utterances with no previous
relation to the current focus (Jo?nsson, 1995), such
mechanisms are not applicable here. For instance, if
the user instead typed ?U13b: magpi?, we would not
have a relation between the items in the utterances,
and can then not be able to correctly respond if the
user corrected that to ?U15b: magpie?.
Another approach is proposed by
Chu-Carroll (2000) who distinguish between task
initiatives and discourse initiatives that are identified
based on cues. These are then utilised to adopt
different interaction strategies. In BIRDQUEST
one solution could be to keep focus for discourse
initiatives and drop focus for new task initiatives.
In the dialogue above, U13, is to be considered a
new task initiative whereas U13b is not a new task
initiative and focus is maintained.
5.2 Unnecessary clarifications
Since information extraction is used to populate the
database with information there can be holes in
it, i.e. for some properties values are missing due
to shortcomings in the information extraction com-
ponent or lack of information in the original text
source. This causes unnecessary clarifications. In
the example below a more appropriate answer to
U19 would have been to give information about size
instead of making a clarification since there is no in-
formation about plumage in the database.
U19: What does a mute swan look like?
S20: You can receive information about the size and
plumage of a Mute Swan. Please specify what you
are interested in.
U21: What do the plumage of a mute swan look like?
S22: Information about eclipse plumage is missing for
Mute Swan.
Information about winter plumage is missing for
Mute Swan.
Information about summer plumage is missing for
Mute Swan.
Information about breeding plumage is missing for
Mute Swan.
The database has many such missing values as
very often there was no information in the informa-
tion source. It is, thus, not possible to adopt general
dialogue strategies, for instance, for clarification re-
quests. Instead, the domain knowledge base must be
consulted to decide on a proper dialogue strategy for
each property.
In BIRDQUEST the unnecessary clarifications can
be dealt with through extended co-operation be-
tween the dialogue manager and the domain knowl-
edge manager. When a vague property is encoun-
tered the dialogue manager can send the request to
the domain knowledge manager for information on
suitable clarifications. By traversal of the ontology
the DKM can produce a set of sub-properties which
can be used to access the database. Should there
be only a few pieces of information these can be
returned directly as an answer to the request. Oth-
erwise the proper clarification is decided based on
the relation between the given property and the sub-
properties which produced the values.
Note, however, that such a strategy can violate the
learning aspect discussed below.
5.3 Partial and empty answers
A problem related to unnecessary clarifications are
how partial and empty answers should be presented
to the user when a vague property has been mapped
to several sub-properties, for example ?Plumage? in
S22 in the example above, or ?Size? in the example
below.
S2: You can receive information about the size and
plumage of a Magpie. Please specify what you are
interested in.
U3: size
S4: Information about wingspan is missing for Magpie.
A Magpie has a length of 46 cm from beak tip to
tail.
In the case of empty answers, S22 above, an alter-
native could be to just state that Information about
plumage is missing for mute swan, and for partial
answers the sub-properties with missing information
could be omitted. However, including all the sub-
properties have a point in so far as it helps the user
learn what type of information the system can pro-
vide. A solution is to make a compromise and make
the system adaptive. This means that the first time a
vague property is mapped to a set of sub-properties
all of them are presented regardless of whether they
have values or not, thereafter empty answers can be
omitted.
5.4 Ontological interpretation failures
Some seemingly simple interpretation failures that
have been tagged as missing information in the lexi-
con are actually more complicated and relates to the
representation of domain knowledge in the ontology.
For example, users tend to ask what colour a spe-
cific bird have, see U1 below4. However, in the bird
encyclopaedia colour is linked to the body parts of
a bird. The complex relation between the concept
?Bird? and the property ?Colour? via the body parts
thus have to be captured in the ontology so that this
knowledge can be used for interpretation and dia-
logue management.
U1: What colour is a blue tit?
S2: Failed to interpret the question. Please try again
With more information about how concepts and
properties are related, for example, through a chain
of hypernym and meronym relations, requests like
U1 could be dealt with. Since a bird consists of body
parts and body parts have colours the missing link in
the request is a body part and based on this a clarifi-
cation could be produced A bird has several colours
for different body parts. What part of the blue tit are
you interested in?
Migratory birds, as discussed in section 5.1, is
another example when the ontology can be used to
stipulate a certain interpretation. With the help of
an ornithologist, we can define what migrating be-
haviour constitutes a migratory bird and add that to
the ontology. Used by the information extraction
component, we can easily pick those birds that are
migratory birds, and as we utilise the same ontology
for the interaction component, we can interpret such
requests. Note, however, that this goes beyond our
initial goal of only utilising the bird encyclopaedia.
5.5 Questions outside database coverage
Users often posed questions that were outside the
database coverage. A successful interpretation of
such a request and an appropriate response is seen
in the dialogue below:
U1: What is the wingspan of an Albatross?
S2: There is no information on non-Nordic birds. The
available birds are listed in the menu ?Birds a-o??.
Please try again
The same strategy could be used to handle similar
questions concerning non-Nordic birds like canaries
and ostriches. However, that would require addition
of information of non-Nordic birds to the ontology
4Many of the requests for appearance can be handled by pre-
senting a picture of the bird. However, the pictures in our bird
encyclopaedia are copyrighted and can therefore not be pre-
sented.
collected from other sources than the bird encyclo-
pedia.
However, in most cases the requests concerned
properties that are not covered by the database but
users often ask about, for example ?Weight? and
?Flight-speed? as in How much does a Sea Gull
weigh? or How fast can an Eagle fly?. The common
response to these types of questions were Failed to
interpret the question. Please try again or in some
cases a partial interpretation was made which led to
inappropriate responses. A more desirable response
would be to give more informative error messages
and explain to the user that it cannot answer ques-
tions about these topics.
Extending the ontology could help give informa-
tive answers when the questions are outside database
coverage. The properties similar to those in the
database, such as ?Weight?, ?Flight-speed?, could
be added to the ontology as user-oriented proper-
ties. Since the DKM always have to map this type
of properties to the system-oriented sub-properties
before database access it could conclude that, if a
user-oriented property do not have any user-oriented
sub-properties, it is outside database coverage and
an appropriate answer can be given. If these prop-
erties were related to others, for example, ?Weight?
is a sub-property of ?Appearance?, the system could
even suggest some of the sibling properties, in this
case ?Size? and ?Plumage?.
Another strategy is to have BIRDQUEST respond
with help phrases explaining how to pose valid re-
quests, as is done in Targeted Help (Gorrell et al,
2002). Targeted help is used for improving user be-
haviour in speech interfaces. It utilises the SLM-
based recognition and categorised help message
templates to present targeted help when the gram-
mar based recogniser fails. Thus, a system must
learn the most common types of mistakes which in
turn must be classified to provide a targeted help.
Unfortunately, we do not yet have a large enough
BIRDQUEST corpus for such classification.
6 Summary
In this paper we have presented an evaluation of
a dialogue system that was developed to access a
database built from information automatically ex-
tracted from a text book. The results from our eval-
uation show that it is possible to develop such a sys-
tem and that users staying within the boundaries of
the application will get useful information.
Dialogue is important for the interaction as well
as a shared ontology for both information extraction
and interaction. The evaluation also revealed a num-
ber of challenging issues, especially regarding, sys-
tem help messages, dialogue management, problems
with gaps in the database due to incomplete informa-
tion and how to utilise a domain ontology.
7 Acknowledgement
Many thanks to Frida Ande?n, Lars Degerstedt and
Sara Norberg for interesting discussions and work
on the development and implementation of the
BIRDQUEST system. This research is financed by
Vinnova, Swedish Agency for Innovation Systems
References
Jan Alexandersson and Norbert Reithinger. 1995. De-
signing the dialogue component in a speech translation
system. In Proceedings of the Ninth Twente Workshop
on Language Technology (TWLT-9), pages 35?43.
Nicole Beringer, Ute Kartal, Katerina Louka, Florian
Schiel, and Uli Tu?rk. 2002. Promise - a procedure
for multimodal interactive system evaluation. In Pro-
ceedings of the Workshop ?Multimodal Resources and
Multimodal Systems Evaluation?. Las Palmas, Gran
Canaria, Spain.
J. Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C. Y. Lin,
S. Maiorano, G. Miller, D. Moldovan, B. Og-
den, J. Prager, E. Riloff, A. Singhal, R. Shrihari,
T. Strzalkowski, E. Voorhees, and R. Weishedel.
2001. Issues, tasks and program structures to
roadmap research in question & answering (Q&A).
http://wwwnlpir.nist.gov/projects/duc/papers/qa.
Roadmap-paper v2.doc.
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proceedings of 6th Applied Natural
Language Processing Conference, pages 97?104.
Nils Dahlba?ck, Annika Flycht-Eriksson, Arne Jo?nsson,
and Pernilla Qvarfordt. 1999. An architecture for
multi-modal natural dialogue systems. In Proceedings
of ESCA Tutorial and Research Workshop (ETRW)
on Interactive Dialogue in Multi-Modal Systems, Ger-
many.
Matthias Denecke. 1997. An information-based ap-
proach for guiding multi-modal human-computer-
interaction. In IJCAI?97, Nagoya, Japan, pages 1036?
1041.
Annika Flycht-Eriksson and Arne Jo?nsson. 2000. Dia-
logue and domain knowledge management in dialogue
systems. In 1st SIGdial Workshop on Discourse and
Dialogue, Hong Kong.
Genevieve Gorrell, Ian Lewin, and Manny Rainer. 2002.
Adding intelligent help to mixed initiative spoken dia-
logue systems. In Proceedings of ICSLP 2002.
Tom R. Gruber. 1993. A translation approach to
portable ontology specification. Knowledge Acquisi-
tion, 5:199?220.
Joakim Gustafson and Linda Bell. 2000. Speech tech-
nology on trial: Experiences from the august system.
Natural Language Engineering, 6(3-4):273?286.
Eli Hagen. 1999. An approach to mixed initiative spo-
ken information retrieval dialogue. User modeling and
User-Adapted Interaction, 9(1-2):167?213.
Arne Jo?nsson and Magnus Merkel. 2003. Some issues in
dialogue-based question-answering. In Working Notes
from AAAI Spring Symposium, Stanford.
Kristiina Jokinen, Hideki Tanaka, and Akio Yokoo.
1998. Context management with topics for spoken di-
alogue systems. In Proceedings of the 36th Annual
Meeting of the Association of Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, COLING-ACL?98, Montreal, pages
631?637.
Arne Jo?nsson. 1995. Dialogue actions for natural
language interfaces. In Proceedings of IJCAI-95,
Montre?al, Canada.
Renaud Leceuche, Dave Robertson, Catherine Barry,
and Chris Mellish. 2000. Evaluating focus theories
for dialogue management. International Journal on
Human-Computer Studies, 52:23?76.
Kavi Mahesh and Sergei Nirenburg. 1995. A situated on-
tology for practical NLP. In Proceedings of IJCA?95
Workshop on Basic Ontological Issues in Knowledge
Sharing, Montreal, Canada.
Dan Sullivan. 2001. Document Warehousing and Text
Mining. John Wiley & Sons.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1998. Paradise: A framework for
evaluating spoken dialogue agents. In Mark Maybury
& Wolfgang Wahlster, editor, Readings in Intelligent
User Interfaces. Morgan Kaufmann.
