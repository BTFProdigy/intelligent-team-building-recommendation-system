Chinese Syntactic Parsing Based on Extended GLR Parsing 
Algorithm with PCFG* 
 
Yan Zhang, Bo Xu and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of sciences, Beijing 100080, P. R. China 
E-mail: {yzhang, xubo, cqzong}@nlpr.ia.ac.cn 
 
Abstract  
This paper presents an extended GLR 
parsing algorithm with grammar PCFG* that 
is based on Tomita?s GLR parsing algorithm 
and extends it further. We also define a new 
grammar?PCFG* that is based on PCFG 
and assigns not only probability but also 
frequency associated with each rule. So our 
syntactic parsing system is implemented 
based on rule-based approach and statistics 
approach. Furthermore our experiments are 
executed in two fields: Chinese base noun 
phrase identification and full syntactic 
parsing. And the results of these two fields 
are compared from three ways. The 
experiments prove that the extended GLR 
parsing algorithm with PCFG* is an 
efficient parsing method and a 
straightforward way to combine statistical 
property with rules. The experiment results 
of these two fields are presented in this 
paper. 
1. Introduction 
Recently the syntactic parsing system is one of 
significant components in natural language 
processing. Many parsing methods have been 
developed as the development of corpus 
linguistics and applications of linguistics. 
Tomita? GLR parsing (Tomita M., 1986, 1987) 
is the most general shift-reduce method of 
bottom-up parsing and widely used in syntactic 
parsing. Several methods are based on it. Lavie 
(Lavie A., 1996) used the GLR* parsing 
algorithm for spoken language system. It uses a 
finite-state probabilistic model to compute the 
action probabilities. Inui (Inui K. et al, 1997, 
1998) presented a formalization of probabilistic 
GLR (PGLR) parsing model which assigns a 
probability to each LR parsing action. To 
shallow parsing, many researchers have made 
experiments with identification of noun phrases. 
Abney (Abney S., 1991) used two level 
grammar rules to implement the noun phrase 
parsing through pure LR parsing algorithm.  
Some new methods based on GLR algorithm 
aim to capture action probabilities by statistics 
distribution and context relations. This paper 
combines rule approach and statistics approach 
simultaneously. Furthermore, based on GLR and 
PCFG, we present an extended GLR parsing and 
a new grammar PCFG* that provides the action 
probabilities to prune the meaningless branches 
in the parsing table. Our experiments are also 
made in two parts: Chinese base noun phrase 
parsing and Chinese full parsing. The former is a 
simplified formalization of full parsing and is 
relatively simpler than the latter. 
This paper includes four sections. Section 2 
presents a brief description of rule structure 
system-PCFG*. Section 3 gives our extended 
GLR parsing algorithm and the parsing 
processing. Section 4 shows the experiment 
results of our parser including Chinese base 
noun phrases (baseNP) identification and 
Chinese full syntactic parser. The conclusions 
are drawn in section 5. 
2. A New Grammar (PCFG*) and the 
Rule Structure 
Grammar system is one of the important pars of 
a parsing system. We explain it in detail in the 
following section.  
2.1 Structure of Rules 
The definition of symbols in our system inherits 
the classifications of Penn Chinese tree-bank 
(Xia F., 2000). There are totally 33 
part-of-speech tags, 23 syntactic tags and 26 
functional tags in the Chinese tree-bank tag set. 
The POS tags belong to terminal symbols, while 
others belong to non-terminal symbols.  
In the final rule base there are about 2000 rules 
and 400 rules learned from corpus for full 
parsing and base noun phrases identification 
respectively. The rules have the following 
format showed in table 1. 
num rule probability frequency
1 VCD?VV  
+VV 
0.754491 126 
2 VCP?
VV+VC 
0.545455 6 
3 VCP?
VV+VV 
0.454545 5 
    Table 1: the format of grammar rules 
In order to denote each rule explicitly, the mark 
?+? is used as the junction mark. In above 
examples, symbols VP, VCD and VCP are verb 
phrase and verb compounds. Symbols VV and 
VC stand for common verbs and copula ??? 
respectively. 
2.2  A New Grammar (PCFG*) 
Context-free grammars (CFGs) are widely used 
to describe the grammar structures in natural 
language processing. And probabilistic 
context-free grammars (PCFGs) directly add the 
probabilities to the rules. But it is sometimes not 
sufficient to only associate probability with each 
rule. So we define a new grammar 
system-PCFG*: each rule is assigned probability 
distribution and frequency distribution 
simultaneously. The probability number is the 
relative value since it is the percentage value in 
the rule group that have the same left sides. 
While the frequency number is the absolute 
value because it is the total numbers occurred in 
whole corpus. The probability property is the 
key value to full parsing. The probability 
attribute is superior to frequency attribute. 
A sample is presented to show how to use 
probability and frequency of a rule.  
Suppose there are three rules showed in table 2 
and the relations is displayed in figure 1. 
Rule F(r) P(r) 
X?A+C f1 p1=f1/(f1+f2)
X?A+B+C f2 >f1 p2=f2/(f1+f2)
Y?A+C f3 <f1 p3 =1>p1 
Table 2: the examples of rule 
A B
X Y
C  
Figure 1: structure of rules 
Suppose the input symbols contain A, B and C. 
When rule 1 and rule 3 simultaneously satisfy 
the reduce condition, rule 3 is executed and the 
left side item ?Y? is pushed to the stack because 
p3 is bigger than p1. To complete parsing, 
probability always has the priority to frequency. 
But to baseNP parsing, frequency is superior to 
probability attribution. Since f1>f3, rule1 is 
executed first. If f1 is equal to f3, then go on to 
compare probability.  
3. Parsing Algorithm 
The parsing algorithm is very significant as well 
as the grammar rules to the parsing system. We 
produce an extended GLR parsing algorithm 
based on the Tomita?s GLR parsing algorithm in 
our system.  
3.1 the Extended GLR Parsing Algorithm 
The GLR method augments the LR parser and 
overcomes the drawback of the LR parser. In 
fact, from the point of parsing algorithm, there 
are no clear differences between LR and GLR 
algorithm. In parsing processing, there are also 
four actions in GLR algorithm that are similar to 
the LR parsing. But GLR parsing algorithm 
admits multiple entries in the parsing table. Our 
extended GLR algorithm also permits that 
several shift and reduce actions exist in one 
branch in the parsing table simultaneously. So 
there are mainly two types of conflicts: 
shift-reduce conflict and reduce-reduce conflict. 
These conflicts are the most difficult problems 
of GLR algorithm. In the parsing process, when 
the conflicts between shift and reduce occur, the 
principle of our parsing method is that the 
reduce action is superior to the shift action.  
If only grammar rules are used to describe the 
context relations, they may produce many 
conflicts when several rules satisfy the 
conditions. So we use the grammar 
system--PCFG* to add statistical information. 
The probabilities distributions are associated 
with the rules to each parsing action and decide 
which step to continue.  
Therefore the extended GLR algorithm handles 
the conflicts with two steps: (1). The reduce 
action is always executed first, then the shift 
action. (2). When more than one reduce actions 
satisfy the conditions, probability and frequency 
decide the order of these reduce actions.  
 
3.2 Parsing Actions and Parsing Process 
3.2.1 Parsing Table and Actions 
The parsing table consists of two sub-tables: 
ACTION table and GOTO table that are 
constructed by the grammar rules. The GOTO 
table is not different from GLR table. Just 
ACTION table is modified a little. Figure 2 
shows the structure of the parsing table. 
ACTION GOTO State 
X1, X2, ?, Xi ,       # Y1, ?, 
Yj  
S0 Sh1  
S1  Re1  
?  Re-Sh  
Sn  Accept 
 
Figure 2: the parsing table 
 
The ACTION table contains four action 
sub-tables: Sh1, Re1, Re-Sh and Accept. They 
stand for shift part, reduce part, reduce-shift part 
and accept part respectively. Because the error 
action is similar to accept action, it is not 
explained here. The Re-Sh part is the key part in 
the table. It contains multiple entries while the 
others have no conflicts. In the Re-Sh part, the 
rules are firstly arranged according to the 
probabilities and then compared based on the 
frequencies. The maximum probability is put on 
the top. This sequence continues until the last 
rule with minimum probability. According to the 
order of Re-Sh sub-table, the parsing program is 
transformed to the corresponding state of the 
stack. This order suits for the full parsing. But to 
the base noun phrases identification, frequency 
is firstly compared. 
Since the ambiguities and conflicts existed in the 
Re-Sh sub-table, we give a limit that no more 
than 20 entries in the Re-Sh part. From the 
experiment results, it is better to select 20 rules 
as the branch limit in the parsing process 
because it not only permits the multiple entries, 
but also fits for the performance efficiency of 
our program.  
Since the parser uses PCFG*, it has strong 
control to handle action conflicts and rule 
ambiguities. The parsing process need to prune 
the meaningless parsing branches. Excessive 
pruning may cause the loss of some grammar 
rules and add the error opportunities. Reasonable 
pruning can improve efficiency. 
 
3.2.2 the Parsing Process 
We give the following the symbols definition 
and interpretation to explain the parsing process.  
Let ?#? denotes the start and the end of the input 
Chinese sentence. The system contains a list of 
stacks simultaneously. The parsing table 
contains two elements: state nodes and symbol 
nodes. The parsing stack includes state stack 
(StateStack, name in the program), symbol state 
(SymbolStack) and input stack (InputStack) 
whose pointers are ps, pb and pi respectively.  
Following algorithm is established for the 
shift-reduce parsing process. 
Input:  
An input Chinese words sequence W in which 
each word has its part-of-speech and a parsing 
table produced by grammar rules; 
Output:  
If the input word sequence W satisfies the 
grammar rules and is accepted according to the 
parsing table, then output the parsing result of W, 
otherwise give error result; 
 
Main Loop:  
It mainly consists of four parts: shift, reduce, 
accept and error in the parsing process.  
Repeat  
Begin 
s := *ps++;  //s is current state 
b := *pb++; //to the next symbol 
c := *pi++; //to the next input word 
if Action[reduce rule 
VtVnVnAA ?????? ,, ] = reduce(), 
then begin 
1) Pop |?| symbols from top of the symbol 
stack, and push the left side symbol A to 
the symbol state; 
2) Pop |?| symbols from top of the state 
stack, and push s* 
3) ps -= |?|; *ps := s*;  
 end reduce(); //reduce part 
  
else if Action[] = shift(input s*),  
then begin 
 pi++; *pi := s*; pb++; *pb := s*; 
end shift(); //shift part 
 
else if Action[] = accept() 
then Success and Output; //the parsing 
succeeds 
else 
     error(); // parsing is error here 
End 
Until: The input symbol is the end of the 
sentence. Or accept function occurs or error 
function occurs. 
 
(1) Reduce Action 
When the reduce action is performed, the rule 
candidates are selected in the list from the first 
rule to the last one that are arranged according to 
the probabilities and frequencies. If one of these 
rules satisfies the condition, then the flag of this 
rule is changed from FALSE to TRUE and stop 
here, and continue to read input word. Otherwise 
trace back.  
(2) Shift Action 
Shift action is executed under two conditions. 
One is based on the action table. The other is 
that when error action occurs, the base noun 
phrase identification continues to perform shift 
action while the full parsing enters trace part. 
(3) Error Action 
When error action occurs, trace back to the 
previous branch and perform another rule 
candidate listed in the entry. If there is no path 
can be searched in the current branch point or all 
routes are not passed through, the parsing fails 
and output the final error symbol. This situation 
is only used to the full parsing.   
3.2.3 the Comparison with GLR 
In order to explain explicitly our extended GLR 
parsing algorithm, we compare it with GLR 
algorithm. Table 3 gives the comparison results. 
    methods 
aspects 
GLR algorithm Our 
algorithm 
Grammar 
System 
CFG PCFG* 
Statistical 
Information 
no Probability, 
Frequency 
Data Structure Graph-Structured 
Stack 
Stack List 
Parsing 
Process 
Not simplified Pruning  
Other 
Attributes 
Augmentation to 
each rule 
no 
Table 3: Comparison with GLR  
4. Experiment and Results 
Our experiments include two parts: Chinese base 
noun phrase parsing and Chinese full syntactic 
parsing. 
The obvious difference of Chinese baseNP 
parsing and full parsing is that the former must 
give the parsing results while the latter 
sometimes need to trace back and output the 
error symbols. Because baseNP identification 
belongs to the shallow parsing, it only need to 
gives the recognized noun phrase structures. If 
there are no phrases found, then output the 
original sentence. Obviously Chinese baseNP 
parsing is much simpler and more efficient than 
the full parsing from the point of the method and 
the runtime.  
Our experiments are performed based on 
Chinese tree-bank corpus. There are totally 
10,000 Chinese sentences whose grammar 
structures are described by brackets. Table 4 
shows the characteristic of the corpus in the 
parsing process. 
Corpus 
 
Style 
Of Parsing 
Number of the 
Sentences. 
Average 
length of 
each 
sentence 
Training: 97% 22 words BaseNP 
Identification Testing: 3% 15 words 
Training: 98% 22 words Full Parsing 
Test: 2% 15 words 
Table 4: characteristic of corpus 
 
To two styles of parsing presented above, we 
give two types of results respectively.  
(1). Chinese BaseNP identification 
In our system, base noun phrases are defined to 
include not only pure noun phrase (NP) but also 
quantifier phrase (QP), such as QP ( ???/CD 
?/M ). 
To each Chinese sentence, baseNP identification 
always gives the final parsing results in which 
the base noun phrases are distinguished by 
brackets. Some samples are listed. 
1. ??/VV ?/AS  NP (??/NR ??/NN) 
?/DEG  NP(??/JJ ??/NN)  
2. (?/DT ?/M ??/NN ) ??/VV ?/AS 
(???/NN ???/NN) ?/DEG (?/JJ 
?/NN) 
There are two and three base noun phrases in 
sentence 1 and sentence 2 respectively.  
 
(2). Chinese full parsing 
Following sentences are the results of Chinese 
full parsing.  
1. VP (VP (??/VV ?/AS)  NP ( NP ( ??
/NR ??/NN )  ?/DEG  NP ( ??/JJ ?
?/NN ) ) ) 
2.IP ( NP (?/DT ?/M ??/NN )  VP( ??
/VV ?/AS)   NP ( NP (???/NN ???
/NN) ?/DEG  NP (?/JJ ?/NN)))) 
In order to display the parsing result clearly, 
sentence 2 is showed in the tree bank format. 
IP (NP ( DT   ? 
M   ? 
NN  ??) 
VP (VV ?? 
       AS  ?? 
NP ( NP ( NN ??? 
NN ???) 
        DEG ? 
NP ( JJ ? 
NN ?))) 
Type Precision 
(%) 
Recall  
(%) 
Num 
of 
Rules 
BaseNP 87.42 81.4 400 
Full 
parsing 
70.56 67.77 2000 
Table 5 is the results of these types of 
parsing. 
The experimental results show that our parsing 
algorithm, extended GLR parsing algorithm, is 
efficient to both Chinese baseNP parsing and 
full parsing. 
5. Conclusions 
In our system, we present the extended GLR 
parsing algorithm that is based on the Tomita?s 
GLR algorithm. A new grammar system PCFG* 
based on PCFG is proposed to describe the 
grammatical rules that are added probability and 
frequency attributes. So our parsing system 
combines Chinese grammar phenomena with 
statistics distribution. This is feasible and 
efficient to implement Chinese shallow parsing 
and full parsing. In the future task, we further 
improve the efficiency and robust of our parsing 
algorithm and expand Chinese grammatical rules 
with both statistical attributions and language 
information. It is important to utilize the results 
of base noun phrases identification and to 
improve the precision of Chinese full parsing.  
Acknowledgements 
The research work described in this paper is 
supported by the National Nature Science 
Foundation of China under grant number 
9835003 and the National Science Foundation of 
China under grand number 60175012 and the 
National Key Basic Research Program of China 
under grand number G1998030504. 
References  
Masaru Tomita, Efficient Parsing for Natural 
Language ? A Fast Algorithm for Practical 
Systems, Kluwer Academic Publishers, 1986 
Tomita M., an Efficient Augmented-Context-Free 
Algorithm, Computational Linguistics, Volume 13, 
Numbers 1-2, 1987 
Inui K., Sornlertlamvanich V., Tanaka H. and 
Tokunaga T., Probabilistic GLR Parsing: a New 
Formalization and Its Impact on parsing 
Performance, Journal of Natural Language 
Processing, Vol.5, No.3, pp.33-52, 1998 
Sornlertlamvanich V., Inui K., Tanaka H. and 
Tokunaga, T., A New Probabilistic LR Parsing, 
Proceedings of Annual Meeting of the Japan 
Association for Natural Language Processing,  
1997 
Lavie A., GLR*: A Robust Grammar-Focus Parser 
for Spontaneously Spoken Language, Ph.D. thesis, 
Carnegie Mellon University, USA, 1996 
Abney S., Parsing by Chunks, Kluwer Academic 
Publishers, 1991 
Xia F., the Segmentation Guidelines for the Penn 
Chinese Treebank (3.0), 2000 
 
Corpus-oriented Acquisition of Chinese Grammar
Yan Zhang 
ATR Spoken language 
Communication Research 
Laboratories
2-2-2 Keihanna Science City, 
Kyoto, 619-0288 
yan.zhang@atr.jp
Hideki Kashioka  
ATR Spoken language 
Communication Research 
Laboratories
2-2-2 Keihanna Science City, 
Kyoto, 619-0288 
Hideki.kashioka@atr.jp
Abstract
The acquisition of grammar from a 
corpus is a challenging task in the 
preparation of a knowledge bank. In 
this paper, we discuss the extraction of 
Chinese grammar oriented to a re-
stricted corpus. First, probabilistic con-
text-free grammars (PCFG) are 
extracted automatically from the Penn 
Chinese Treebank and are regarded as 
the baseline rules. Then a corpus-
oriented grammar is developed by add-
ing specific information including head 
information from the restricted corpus. 
Then, we describe the peculiarities and 
ambiguities, particularly between the 
phrases ?PP? and ?VP? in the extracted 
grammar. Finally, the parsing results of 
the utterances are used to evaluate the 
extracted grammar.  
1 Introduction 
Research and development work on spoken lan-
guage systems for special domains has been 
gaining more attention in recent years. Many 
approaches to spoken language processing re-
quire a grammar system for parsing the input 
utterances in order to obtain the structures, espe-
cially for rule-based approaches.  
Manually developing grammars based on lin-
guistics theories is a very difficult task. Lan-
guage phenomena are usually described as being 
symbolic systems such as lexical, syntactic, se-
mantic and common sense. Grammar develop-
ment has to depend on linguistic knowledge and 
the characteristics of the corpus to explicate a 
system of linguistic entities. However, it is ex-
pensive and time-consuming to maintain a ro-
bust grammar system by manual writing.  
Recently some researchers (H. Meng et al, 
2002; S. Dipper, 2004 and Y. Ding, 2004) have 
presented a methodology to semi-automatically 
capture different grammar inductions from an-
notated corpora within restricted domains. A 
corpus-oriented approach (Y. Miyao, 2004) pro-
vides a way to extract grammars automatically 
from an annotated corpus. The specific language 
knowledge and knowledge relations need to be 
constructed and oriented to different corpora and 
tasks (K. Chen, 2004).
The Chinese treebank is a useful resource for 
acquiring grammar rules and the context rela-
tions. Currently there are several Chinese tree-
banks on a scale of size. In the Penn Chinese 
Treebank (F. Xia, 2000), each structural tree is 
annotated with words, parts-of-speech and syn-
tactic structure brackets. In the Sinica Treebank 
(CKIP), thematic roles are also labeled in the 
CKIP to provide deeper information.
In this paper, we discuss the extraction of Chi-
nese grammar oriented to a restricted corpus. 
First, probabilistic context-free grammars 
(PCFG) are extracted automatically from the 
Penn Chinese Treebank and are regarded as the 
baseline rules. Then a corpus-oriented grammar 
is developed by adding specific information in-
cluding head information from the restricted 
corpus. We then describe the peculiarities and 
ambiguities, especially between the phrases 
?PP? and ?VP? in the extracted grammar. Fi-
17
nally, the parsing results of the utterances are
used to evaluate the extracted grammar.
The outline of this paper is as follows: Section 2 
gives the process of acquiring the baseline Chi-
nese grammar and the extension of the current 
grammar oriented to the corpus. Section 3 ex-
plains the grammar properties in our corpus and
our approach to disambiguating some special 
phrase rules, such as ?PP? and ?VP? and the 
word ??(ZAI)? in different categories. Section 
4 describes the evaluation results of the ex-
tracted Chinese grammar. Finally section 5 of-
fers some concluding remarks and outlines our
future work. 
2 Grammar Acquisition
There are two parts to acquiring grammar in our 
system. The baseline grammar is extracted
automatically from the Penn Chinese Treebank. 
We define a suitable parts-of-speech and phrase
categories and extend them by introducing spe-
cific information from our corpus.
2.1 Grammar Extraction from Penn Chinese
Treebank
The University of Pennsylvania (Upenn) has 
released a scale of Chinese treebanks as a kind 
of resource since 2000 (Xia Fei et al, 2000).
Each structural tree includes parts-of-speech and
syntactic structure brackets, which provides a 
good way to extract Chinese probabilistic con-
text-free grammars (PCFG). There are a total of 
325 files collected from the Xinhua newswire in
this treebank. The majority of these documents
focus on economic development and are organ-
ized in written formats as opposed to spoken 
utterances, so the grammars extracted from it are
seen as the baseline bank.
The probabilistic context-free grammars have 
proven to be very effective for parsing natural 
language. The produced rules are learnt by
matching the bracketed structures automatically
from the trees, and the rule probabilities are cal-
culated based on the maximum likelihood esti-
mation (MLE), presented in the following 
formula (Charniak, 1996):
? o
o o
k
ki
ji
ji
NC
NCNP
)(
)()( ]
]]   (1)
The baseline grammar includes about 400 PCFG 
rules after cleaning and merging some rules with 
low probabilities (Imamula et al, 2003).
2.2 Extension of the Extracted Grammar
Different corpora produce different grammars 
that have some specific information. In baseline 
grammars, many grammars are not suitable for 
spoken corpora. Therefore, we need to build an 
appropriate grammar by using specific informa-
tion in our corpus to improve the parsing results 
and machine translation systems that operates in
a restricted field. The data we used in this sys-
tem is from the ATR Basic Travel Expression 
Corpus (BTEC) in which the format of utter-
ances is different from the sentences in Upenn.
Consequently, an appropriate phrase category is
required to be constructed by analyzing the 
knowledge characteristics in BTEC. We define 
it by comparing three Chinese structure category
systems: Sinica, University of Pennsylvania, and 
HIT (Harbin Institute of Technology). A phrase 
category should be not too complicated as but 
cover language phenomenon in the corpus. Our
phrase category is defined and explained in table
1.
Categories Explanation
NNP Noun Phrase
TNP Temporal Noun Phrase 
LP Localizer Phrase
NSP Location Phrase
VP Verb Phrase
AP Adjective Phrase
DP Adverbial Phrase
QP Quantifier Phrase
PP Preposition Phrase
VBAP Phrase with ?? (BA)? 
DENP Nominal Phrase Ending 
by ?? (DE)? 
DEP Attributive Phrase formed
by ?? (DE)? 
       Table 1 Phrase Categories
In BTEC, Chinese utterances are segmented and 
labeled as parts-of-speech. We not only con-
struct corpus-oriented grammar rules differently
from the baseline grammars but also add head 
information for each rule.
In the above Table 1, the phrase category 
?VBAP? is a phrase name including the preposi-
tion ??(BA)? and its following noun or verb
phrase. The phrase ?DENP? is a special nominal
phrase which has no word after the auxiliary 
18
word ?? (DE)?, and it is usually put at the end 
of the utterance. Following are some examples 
of our grammars.  
1. PP ? p(sem"?") (head)n  
2. DENP ? (head)a y(sem"?")
3. PP ? p(sem"?") (head)r  
4. DEP ? (head)DP de
In above rules, the mark ?sem? means its fol-
lowing word is a terminal node.   
3 Grammar Annotation and Disam-
biguation
Above constructed Chinese grammars some-
times bring out conflicts when parsing utter-
ances because of the ambiguity phenomenon. 
Grammar annotation is done to make the gram-
matical relations of an utterance more explicit. 
Thus, some ideas are proposed to deal with these 
ambiguities that are tightly related to Chinese 
language.
3.1 Annotation and Analysis of Grammar 
Plenty of prepositions are rooted in verbs in 
Chinese language, and most of them still keep 
the function of verbs. This phenomenon pro-
duces ambiguous problems not only between 
categories preposition ?p? and verb ?v? but be-
tween the phrases ?VP? and ?PP? in the struc-
tures of the utterances. PP-attachment ambiguity 
is a big problem related to the construction of 
grammar (S. Zhao. 2004).  
Firstly, we extract a lexicon of Chinese preposi-
tions, which have other categories at the same 
time, such as the category ?v?, adjective ?a?, and 
so on. The following table shows the colloca-
tions of these words and their frequencies.  
Word Category Frequency  
p 226?
v 85
p 2423?
vt 4857
p 579?
a 1058
p 6422?
v 4309
p 1270?
v 1226
p 11115 
v 2381
?
d 39
 Table 2 Some Examples in the Preposition 
Lexicon
We construct some particular grammar rules for 
these preposition words showed in Table 2 in 
order to deal with the conflicts among these 
words. For example, following rules are related 
to the word ???.
PP? p(sem"?") (head)n
VP? p(sem"?") (head)V
VP? v(sem"?") NNP (head)VP 
In order to represent the function of the ex-
tracted grammar, we compare the coverage of 
the grammar in different layers between a termi-
nal node and a phrase layer. The different struc-
tural trees of the same utterance in Figure 1 are 
listed as follows.
1.Sentence (??/r ?/de ?/n ?/r ?/q ?/v1
??/n??/n?/w ) 
|__ NNP__NNP(head)__DEP__r(head) ??
|     |              |                       | ___ de ?
|     |              | _________NNP __ n(head) ?
|     | 
|     | ___ QP __ r(sem"?")
|               | ____q(head) ?
|
| __ v1(sem"?")
|
| __ NNP(head) __ NNP _____n(head) ??
|         | _________NNP(head)__n(head) ??
| __ w ?
 2. Sentence (??/r ?/de ?/n ?/r ?/q ?/v1
??/n??/n?/w ) 
|__NNP__NNP(head)*__ r ??
|     |                       | ___ de ?
|     |                       | ____  n(head) ?
|     | 
|     | ___ QP __ r(sem"?")
|                | __ q(head) ?
|
| __ v1(sem"?")
|
| __ NNP(head)** __ n ??
|           | __________ n(head) ??
| __ w ?
Figure 1 Annotation of Different trees in the 
same sentence  
The same utterance obtains different structural 
trees from different levels of grammar rules by 
parsing results, although these two trees are cor-
19
rect and acceptable. The grammar plays an im-
portant role in the machine translation system 
when we build the mapping relations with the 
goal languages by transform rules. This problem 
is also called Granularity (K. Chen, 2004). 
Symbol ?**? in Figure 1 denotes that the phrase 
?NNP? is produced by the rule ?NNP ? n 
(head)n? rather than ?NNP ? NNP (head)NNP?.  
3.2 Grammar Disambiguation 
A grammar inevitably includes ambiguities 
among its rules. To some extent, certain kinds of 
ambiguities are produced by the same ambigu-
ous problems found among part-of-speech tags. 
As with the expression in Section 2, the ambigu-
ity between the phrases ?PP? and ?VP? is partly 
produced by the multiple categories ?p? and ?v? 
of the words. This is a common case where the 
phrases ?PP? and ?VP? are nested against each 
other. For example, the rule ?PP ? p (head)v? 
and ?VP ? PP (head)VP?. This situation is de-
scribed in the following two utterances in Figure 
2.
1. Sentence (???/n ?/d ?/p ??/v ?/de 
??/n?/v?)
|__NNP__ n???
|
|__VP__d?
|      |___VP__PP*__p?
|               |        |____NNP__v??
|               |                    |____de ?
|               |                    |_____n??
|               | 
|               |___VP__v?
|
| ___ w ?
2. sentence (?/vw?/p?/r?/v?/q??/n?)
|__VP__vw?
|      | 
|      |___VP**__PP__p?
|                |          |___r?
|                | 
|                |___VP__VP__v?
|                         | 
|                         |___NNP__q?
|                                   |____n??
|
|__w?
Figure 2 The Correct Trees of Utterances In-
cluding phrases ?PP? and ?VP? 
In sentence 1 of figure 2, the phrase ?PP? (?/p
??/v ?/de ??/n) contains the verb ????,
and is produced by the rule ?DEP ? v de? and 
?NNP? DEP n? firstly. Likewise, in sentence 2, 
the phrase ?VP(????)? is produced firstly 
rather than phrase ????? is got by rule ?VP 
? PP v?. That is to say, the phrase ?VP? has 
higher priority to be produced than ?PP?.  
The Chinese word ??? is a special individual 
word in our corpus. Its correlative disambigua-
tion rules are constructed by the knowledge rela-
tions listed in the following table:
Category 
of ? ?
(ZAI)?
The order of 
rules
Ambiguity parts 
bracketed in  ut-
terance
P
(preposi-
tion)
1. VP ?
V(sem? ? ?)
(head)r
2. VP ? PP 
(head)VP
?/r ??/d [[?/p 
?? /r] [? /v ?
?/n] ]?
V (verb) 1. VP ?
V(sem? ? ?)
(head)r
2. VP D  
(head)VP
?/r [??/d ?/v
??/n] 
D (ad-
verb)
1. VP ?
D(sem? ? ?)
(head)VP
2. DP ? D 
(head)d
?/r [??/d ?/d
?/v?/u?/n] 
      Table 3 The characteristics of word ???
The following steps are used to identify the am-
biguities between the phrases ?PP? and ?VP?:  
1. The first step is to look up the preposition 
lexicon based on the category of the word and 
find the relative rules from the extracted gram-
mar.
2. When the ?PP? rules conflict with the ?VP? 
rules, we firstly consider the verb and then select 
an appropriate rule by comparing the relation-
ship to neighboring preposition words.  
3. Long distance rules have priority. For in-
stance, rule ?PP ? p v nd? is preferred to rule 
?PP? p  v?.  
4. It is clear that the fine-grained rules have less 
representational ambiguity than the coarse-
20
grained grammar rules in relation to the tree 
presentations.
5. The head information in the rules is viewed as
being types of reference knowledge because of 
their own ambiguities.
4 Evaluation for Grammar 
We use the extracted grammar described in sec-
tion 3 to parse Chinese utterances in BTEC and
to evaluate the roles of the grammar.
4.1 Parsing with Grammar
The parser adopts a bottom-up parsing algorithm
in order to obtain the phrase structures of utter-
ances. There are 200 Chinese utterances selected
in our experiment. The number of rules totals 
682 that are constructed manually except base-
line rules from Upenn Chinese treebank. Table 4 
lists the number of PCFG rules which have dif-
ferent left-side phrases.
Left-side
phrase
fre-
quency
Proportion as 
head information
AP 42 15
DENP 20 2
DEP 15 2
DP 9 5
LP 10 3
NNP 240 114
NSP 18 2
PP 39 1
QP 50 17
TNP 28 15
VBAP 7 0
VP 162 106
sentence 40 --
Table 4 The number of rules with different left-
side phrases 
In our current experiment, the evaluation is lim-
ited to obtaining several special phrase struc-
tures including ?NNP?, ?VP?, ?PP?, and 
?DENP? by using the extracted grammar. There-
fore, the parsing results are calculated using the 
precision of these phrases in the following for-
mula (2) and are listed in Table 5. We give the 
evaluation results of the word ??? separately in 
Table 6.
%100)(Pr u 
t
c
N
Nphraseec (2)
where denotes the number of correct
phrases in the parsing results, and is the total 
number of the phrases in the utterances.
cN
tN
Phrase Precision
without dis-
ambiguation
Precision
with disam-
biguation
Prec(NNP) 70.03 70.43
Prec(PP) 81.51 84.17
Prec(VP) 69.01 70.13
Prec(DENP) 82.61 82.81
      Table 5 The evaluation results
Phrase
with ???
Precision with-
out disam-
biguation
Precision
with disam-
biguation
Prec(PP) 79.12 83.67
Prec(VP) 89.34 91.72
Prec(DP) 87.71 88.02
    Table 6 The evaluation results of ???
From the evaluation results, we found that the 
precisions of the phrases ?NNP? and ?VP? were
not high due to the diversity and complexity. We
only processed the ambiguity between ?VP? and
?PP? and improve the precision of phrase ?PP?.
From the condition of the word ???, it is very
useful for the grammar extraction to construct
information on high-frequency words and word-
to-word collocation relations.
4.2 Discussion
The Chinese language is one of the most diffi-
cult languages to process. There is still no uni-
form standard for acquiring Chinese grammar
that covers all domains. Hence, a grammar
should be constructed from the view of point of
real research requirements in real corpora. It is 
the most important to maintain consistency and 
satisfy the actual requirements of a real corpus.
One of the main purposes in constructing a Chi-
nese grammar is to improve its validity and ro-
bustness to machine translation in a restricted
corpus. The development of a robust grammar
based on linguistics is difficult because of the
complexity of deep linguistic analysis. For ex-
ample, how many annotated grammars are suit-
able for the parsing system and a real machine
translation? What is the balance between the 
granularity of grammar structures and grammar
21
coverage including the ambiguities? In general, 
the coarse-grained grammar rules have a higher 
coverage rate compared with fine-grained rules, 
which contain more terminal nodes. There is 
also the major problem of determining which 
Treebank size is required to acquire the gram-
mar rules.
5 Conclusion and Future Work 
Corpus-oriented grammar extraction is con-
ducted for the purpose of constructing more ex-
plicit grammar knowledge and improving the 
machine translation system in a restricted corpus. 
Treebanks provide a useful resource for acquir-
ing grammar rules. However, it is time consum-
ing to construct a much larger size Treebank, 
which is better for grammar extraction. It would 
be better if the knowledge extraction process 
could be carried out iteratively. The parser could 
use the initial grammar to produce a large 
amount of structural trees. These new trees 
would provide more information on the gram-
mar to improve the robustness of the grammar 
and the power of the parsing system. This whole 
process can be regarded as an automatic knowl-
edge learning system.  
The principal idea in this paper was to acquire 
Chinese grammar from a restricted corpus for a 
machine translation system. The extracted 
grammar was not only from the Penn Chinese 
treebank but also from new information added to 
our experimental corpus. The corpus-oriented 
Chinese grammar was evaluated by parsing the 
phrase structures that includes ?NNP?, ?VP?, 
?PP?, ?DENP?, and the phrases relative to the 
word ???.
Currently, we only focus on a few limited 
phrases, and the disambiguation process has 
been explored with specific rules manually. 
Therefore, to improve grammar extraction in the 
future, we will aim at increasing the robustness 
and coverage of the rules and try to automati-
cally reduce the ambiguity rate by constructing 
more knowledge relations. The word-to-word 
collocation relations provided useful informa-
tion on grammar extraction for the detailed 
processing.
Acknowledgment  
This research was supported by a contract with 
the National Institute of Information and Com-
munication Technology (NICT) of Japan.  
References
Helen M. Meng and Kai-Chung Siu. 2002. Semi-
Automatic Acquisition of Domain-Specific Se-
mantic Structures, IEEE Transactions on Knowl-
edge and Data Engineering, vol 14, n 1, 
January/February, pp. 172-180 
Stefanie Dipper. Grammar Modularity and its Impact 
on Grammar Documentation. In Proceedings of 
the 20th International Conference on Computa-
tional Linguistics (COLING), pp. 1-7, Geneva, 
Switzerland, 2004  
Claire Gardent, Marilisa Amoia and Evelyne Jacquey. 
Paraphrastic Grammars. ACL Workshop on text 
meaning, Barcelona, July 2004  
Yuan Ding and Martha Palmer. Automatic Learning 
of Parallel Dependency Treelet Pairs. In  Proceed-
ings of the First International Joint Conference on 
Natural Language Processing (IJCNLP2004). 
March, Sanya, pp. 30-37, 2004 
Shaojun Zhao and Dekang Lin. A nearest-neighbor 
method for resolving pp-attachment ambiguity. In  
Proceedings of the First International Joint Con-
ference on Natural Language Processing 
(IJCNLP2004). March, Sanya, pp. 428-434, 2004 
Kenji Imamura, Eiichiro Sumita and Yuji Matsumoto. 
2003. Feedback Cleaning of Machine Translation 
Rules Using Automatic Evaluation. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL 2003), pp. 
447-454.  
Keh-Jian Chen and Yu-Ming Hsieh. Chinese Tree-
bank and Grammar Extraction. In  Proceedings of 
the First International Joint Conference on Natural 
Language Processing (IJCNLP2004). March, 
Sanya, pp. 560-565, 2004 
CKIP (Chinese Knowledge Information Processing). 
1993. The Categorical Analysis of Chinese. [In 
Chinese]. CKIP Technical Report 93-05. 
Nankang: Academic Sinica.  
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Fudong Chiou, Shizhe 
Huang, Tony Kroch, and Mitch Marcus. 2000. 
Developing Guidelines and Ensuring Consistency 
for Chinese Text Annotation. Proceeding of the 
second International Conference on Language Re-
22
sources and Evaluation (LREC-2000), Athens, 
Greece.  
Rashmi Prasad, Elini Miltsahaki, Aravind Joshi and 
Bonnie Webber. Annotation and Data Mining of 
the Penn Discourse Treebank. In Proceedings of 
the ACL 2004 Workshop on Discourse Annotation, 
Barcelona. 2004. 
Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-
jii. Corpus-oriented Grammar Development for 
Acquiring a Head-driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of 
the First International Joint Conference on Natural 
Language Processing (IJCNLP2004). March, 
Sanya, pp. 390-397, 2004 
E. Charniak. 1996. Treebank Grammars. Technical 
Report CS-96-02, Department of Computer Sci-
ence, Brown University. 
23
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 433?443,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Timeline Generation through Evolutionary Trans-Temporal Summarization
Rui Yan?, Liang Kong? , Congrui Huang?, Xiaojun Wan?, Xiaoming Li\, Yan Zhang??
?School of Electronics Engineering and Computer Science, Peking University, China
?Institute of Computer Science and Technology, Peking University, China
\State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China
{r.yan,kongliang,hcr,lxm}@pku.edu.cn,
wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn
Abstract
We investigate an important and challeng-
ing problem in summary generation, i.e.,
Evolutionary Trans-Temporal Summarization
(ETTS), which generates news timelines from
massive data on the Internet. ETTS greatly
facilitates fast news browsing and knowl-
edge comprehension, and hence is a neces-
sity. Given the collection of time-stamped web
documents related to the evolving news, ETTS
aims to return news evolution along the time-
line, consisting of individual but correlated
summaries on each date. Existing summariza-
tion algorithms fail to utilize trans-temporal
characteristics among these component sum-
maries. We propose to model trans-temporal
correlations among component summaries for
timelines, using inter-date and intra-date sen-
tence dependencies, and present a novel com-
bination. We develop experimental systems to
compare 5 rival algorithms on 6 instinctively
different datasets which amount to 10251 doc-
uments. Evaluation results in ROUGE metrics
indicate the effectiveness of the proposed ap-
proach based on trans-temporal information.
1 Introduction
Along with the rapid growth of the World Wide
Web, document floods spread throughout the Inter-
net. Given a large document collection related to
a news subject (for example, BP Oil Spill), readers
get lost in the sea of articles, feeling confused and
powerless. General search engines can rank these
?Corresponding author.
news webpages by relevance to a user specified as-
pect, i.e., a query such as ?first relief effort for BP
Oil Spill?, but search engines are not quite capable
of ranking documents given the whole news subject
without particular aspects. Faced with thousands of
news documents, people usually have a myriad of in-
terest aspects about the beginning, the development
or the latest situation. However, traditional infor-
mation retrieval techniques can only rank webpages
according to their understanding of relevance, which
is obviously insufficient (Jin et al, 2010).
Even if the ranked documents could be in a satis-
fying order to help users understand news evolution,
readers prefer to monitor the evolutionary trajecto-
ries by simply browsing rather than navigate every
document in the overwhelming collection. Summa-
rization is an ideal solution to provide an abbrevi-
ated, informative reorganization for faster and bet-
ter representation of news documents. Particularly,
a timeline (see Table 1) can summarize evolutionary
news as a series of individual but correlated com-
ponent summaries (items in Table 1) and offer an
option to understand the big picture of evolution.
With unique characteristics, summarizing time-
lines is significantly different from traditional sum-
marization methods which are awkward in such sce-
narios. We first study a manual timeline of BP Oil
Spill in Mexico Gulf in Table 1 from Reuters News1
to understand why timelines generation is observ-
ably different from traditional summarization. No
traditional method has considered to partition corpus
into subsets by timestamps for trans-temporal cor-
relations. However, we discover two unique trans-
1http://www.reuters.com
433
Table 1: Part of human generated timeline about BP Oil
Spill in 2010 from Reuters News website.
April 22, 2010
The Deepwater Horizon rig, valued at more than $560 million,
sinks and a five mile long (8 km) oil slick is seen.
April 25, 2010
The Coast Guard approves a plan to have remote underwater vehi-
cles activate a blowout preventer and stop leak. Efforts to activate
the blowout preventer fail.
April 28, 2010
The Coast Guard says the flow of oil is 5,000 barrels per day (bpd)
(210,000 gallons/795,000 litres) ? five times greater than first esti-
mated. A controlled burn is held on the giant oil slick.
April 29, 2010
U.S. President Barack Obama pledges ?every single available re-
source,? including the U.S. military, to contain the spreading spill.
Obama also says BP is responsible for the cleanup. Louisiana de-
clares state of emergency due to the threat to the state?s natural
resources.
April 30, 2010
An Obama aide says no drilling will be allowed in new areas, as the
president had recently proposed, until the cause of the Deepwater
Horizon accident is known.
temporal characteristics of component summaries
from the handcrafted timeline. Individuality. The
component summaries are summarized locally: the
component item on date t is constituted by sentences
with timestamp t. Correlativeness. The compo-
nent summaries are correlative across dates, based
on the global collection. To the best of our knowl-
edge, no traditional method has examined the rela-
tionships among these timeline items.
Although it is profitable, summarizing timeline
faces with new challenges:
? The first challenge for timeline generation is
to deliver important contents and avoid information
overlaps among component summaries under the
trans-temporal scenario based on global/local source
collection. Component items are individual but not
completely isolated due to the dynamic evolution.
? As we have individuality and correlativeness
to evaluate the qualities of component summaries,
both locally and globally, the second challenge is to
formulate the combination task into a balanced op-
timization problem to generate the timelines which
satisfy both standards with maximum utilities.
We introduce a novel approach for the web min-
ing problem Evolutionary Trans-Temporal Summa-
rization (ETTS). Taking a collection relevant to a
news subject as input, the system automatically out-
puts a timeline with items of component summaries
which represent evolutionary trajectories on specific
dates. We classify sentence relationships as inter-
date and intra-date dependencies. Particularly, the
inter-date dependency calculation includes temporal
decays to project sentences from all dates onto the
same time horizon (Figure 1 (a)). Based on intra-
/inter-date sentence dependencies, we then model
affinity and diversity to compute the saliency score
of each sentence and merge local and global rank-
ings into one unified ranking framework. Finally we
select top ranked sentences. We build an experimen-
tal system on 6 real datasets to verify the effective-
ness of our methods compared with other 4 rivals.
2 Related Work
Multi-document summarization (MDS) aims to pro-
duce a summary delivering the majority of informa-
tion content from a set of documents and has drawn
much attention in recent years. Conferences such as
ACL, SIGIR, EMNLP, etc., have advanced the tech-
nology and produced several experimental systems.
Generally speaking, MDS methods can be either
extractive or abstractive summarization. Abstractive
summarization (e.g. NewsBlaster2) usually needs
information fusion, sentence compression and refor-
mulation. We focus on extraction-based methods,
which usually involve assigning saliency scores to
some units (e.g. sentences, paragraphs) of the docu-
ments and extracting the units with highest scores.
To date, various extraction-based methods have
been proposed for generic multi-document summa-
rization. The centroid-based method MEAD (Radev
et al, 2004) is an implementation of the centroid-
based method that scores sentences based on fea-
tures such as cluster centroids, position, and TF.IDF,
etc. NeATS (Lin and Hovy, 2002) adds new features
such as topic signature and term clustering to select
important content, and use MMR (Goldstein et al,
1999) to remove redundancy.
Graph-based ranking methods have been pro-
posed to rank sentences/passages based on ?votes?
or ?recommendations? between each other. Tex-
tRank (Mihalcea and Tarau, 2005) and LexPageR-
ank (Erkan and Radev, 2004) use algorithms similar
to PageRank and HITS to compute sentence impor-
tance. Wan et al have improved the graph-ranking
2http://www1.cs.columbia.edu/nlp/newsblaster/
434
algorithm by differentiating intra-document and
inter-document links between sentences (2007b),
and have proposed a manifold-ranking method to
utilize sentence-to-sentence and sentence-to-topic
relationships (Wan et al, 2007a).
ETTS seems to be related to a very recent task of
?update summarization? started in DUC 2007 and
continuing with TAC. However, update summariza-
tion only dealt with a single update and we make a
novel contribution with multi-step evolutionary up-
dates. Further related work includes similar timeline
systems proposed by (Swan and Allan, 2000) us-
ing named entities, by (Allan et al, 2001) measured
in usefulness and novelty, and by (Chieu and Lee,
2004) measured in interest and burstiness. We have
proposed a timeline algorithm named ?Evolution-
ary Timeline Summarization (ETS)? in (Yan et al,
2011b) but the refining process based on generated
component summaries is time consuming. We aim
to seek for more efficient summarizing approach.
To the best of our knowledge, neither update sum-
marization nor traditional systems have considered
the relationship among ?component summaries?, or
have utilized trans-temporal properties. ETTS ap-
proach can also naturally and simultaneously take
into account global/local summarization with biased
information richness and information novelty, and
combine both summarization in optimization.
3 Trans-temporal Summarization
We conduct trans-temporal summarization based on
the global biased graph using inter-date dependency
and local biased graph using intra-date dependency.
Each graph is the complementary graph to the other.
3.1 Global Biased Summarization
The intuition for global biased summarization is that
the selected summary should be correlative with sen-
tences from neighboring dates, especially with those
informative ones. To generate the component sum-
mary on date t, we project all sentences in the collec-
tion onto the time horizon of t to construct a global
affinity graph, using temporal decaying kernels.
3.1.1 Temporal Proximity Based Projection
Clearly, a major technical challenge in ETTS is
how to define the temporal biased projection func-
tion ?(?t), where ?t is the distance between the
Figure 1: Construct global/local biased graphs. Solid cir-
cles denote intra-date sentences on the pending date t and
dash ones represent inter-date sentences from other dates.
Figure 2: Proximity-based kernel functions, where ?=10.
pending date t and neighboring date t?, i.e., ?t =
|t? ? t|. As in (Lv and Zhai, 2009), we present 5
representative kernel functions: Gaussian, Triangle,
Cosine, Circle, and Window, shown in Figure 2. Dif-
ferent kernels lead to different projections.
1. Gaussian kernel
?(?t) = exp[??t
2
2?2 ]
2. Triangle kernel
?(?t) =
{
1? ?t? if ?t ? ?
0 otherwise
3. Cosine (Hamming) kernel
?(?t) =
{
1
2 [1 + cos(?t?pi? )] if ?t ? ?
0 otherwise
4. Circle kernel
?(?t) =
{?
1? (?t? )2 if ?t ? ?
0 otherwise
435
5. Window kernel
?(?t) =
{
1 if ?t ? ?
0 otherwise
All kernels have one parameter ? to tune, which
controls the spread of kernel curves, i.e., it restricts
the projection scope of each sentence. In general,
the optimal setting of ? may vary according to the
news set because sentences presumably would have
wider semantic scope in certain news subjects, thus
requiring a higher value of ? and vice versa.
3.1.2 Modeling Global Affinity
Given the sentence collectionC partitioned by the
timestamp set T , C = {C1, C2, . . . , C |T |}, we ob-
tain Ct = {sti|1 ? i ? |Ct|} where si is a sentence
with the timestamp t = tsi . When we generate com-
ponent summary on t, we project all sentences onto
time horizon t. After projection, all sentences are
weighted by their influence on t. We use an affinity
matrix M t with the entry of the inter-date transition
probability on date t. The sum of each row equals to
1. Note that for the global biased matrix, we mea-
sure the affinity between local sentences from t and
global sentences from other dates. Therefore, intra-
date transition probability between sentences with
the timestamp t is set to 0 for local summarization.
M ti,j is the transition probability of si to sj based
on the perspective of date t, i.e., p(si ? sj |t):
p(si ? sj |t) =
{ f(si?sj |t)?
|C| f(si?sk|t)
if ? f 6= 0
0 if tsi = tsj = t
(1)
f(si ? sj |t) is defined as the temporal weighted
cosine similarity between two sentences:
f(si ? sj |t) =
?
w?si?sj
pi(w, si|t) ? pi(w, sj |t) (2)
where the weight pi associated with term w is calcu-
lated with the temporal weighted tf.isf formula:
pi(w, s|t) =
?|t? ts| ? tf(w, s)(1 + log( |C|Nw ))??
|s|(tf(w, s)(1 + log(
|C|
Nw )))2
.
(3)
where ts is the timestamp of sentence s, and
tf(w, s) is the term frequency of w in s. ts can be
any date from T . |C| is the sentences set size and
Nw is the number of sentences containing term w.
We let p(si ? si|t)=0 to avoid self transition.
Note that although f(.) is a symmetric function,
p(si ? sj |t) is usually not equal to p(sj ? si|t),
depending on the degrees of nodes si and sj .
Now we establish the affinity matrix M ti,j and by
using the general form of PageRank, we obtain:
~? = ?M?1~?+ 1? ?|C| ~e (4)
where ~? is the selective probability of all sentence
nodes and ~e is a column vector with all elements
equaling to 1. ? is the damping factor set as 0.85.
Usually the convergence of the iteration algorithm is
achieved when difference between the scores com-
puted at two successive iterations for any sentences
falls below a given threshold (0.0001 in this study).
3.1.3 Modeling Diversity
Diversity is to reflect both biased information
richness and sentence novelty, which aims to reduce
information redundancy. However, using standard
PageRank of Equation (4) will not result in diver-
sity. The aggregational effect of PageRank assigns
high salient scores to closely connected node com-
munities (Figure 3 (b)). A greedy vertex selection
algorithm may achieve diversity by iteratively se-
lecting the most prestigious vertex and then penal-
izing the vertices ?covered? by the already selected
ones, such as Maximum Marginal Relevance and its
applications in Wan et al (2007b; 2007a). Most re-
cently diversity rank DivRank is another solution
to diversity penalization in (Mei et al, 2010).
We incorporate DivRank in our general ranking
framework, which creates a dynamicM during each
iteration, rather than a static one. After z times of
iteration, the matrix M becomes:
M (z) = ?M (z?1) ? ~?(z?1) + 1? ?|C| ~e (5)
Equation (5) raises the probability for nodes with
higher centrality and nodes already having high
weights are likely to ?absorb? the weights of its
neighbors directly, and the weights of neighbors?
neighbors indirectly. The process is to iteratively ad-
just matrix M according to ~? and then to update ~?
according to the changed M . As iteration increases
436
there emerges a rich-gets-richer phenomenon (Fig-
ure 3 (c) and (d)). By incorporating DivRank, we
obtain rank r?i and the global biased ranking score
Gi for sentence si from date t to summarize Ct.
3.2 Local Biased Summarization
Naturally, the component summary for date t should
be informative within Ct. Given the sentence col-
lection Ct = {sti|1 ? i ? |Ct|}, we build an affin-
ity matrix for Figure 1 (b), with the entry of intra-
date transition probability calculated from standard
cosine similarity. We incorporate DivRank within
local summarization and we obtain the local biased
rank and ranking score for si, denoted as r?i and Li.
3.3 Optimization of Global/Local Combination
We do not directly add the global biased ranking
score and local biased ranking score, as many previ-
ous works did (Wan et al, 2007b; Wan et al, 2007a),
because even the same ranking score gap may indi-
cate different rank gaps in two ranking lists.
Given subset Ct, let R = {ri}(i = 1,. . . ,|Ct|), ri
is the final ranking of si to estimate, optimize the
following objective cost function O(R),
O(R) =?
|Ct|?
i=1
Gi?
ri
?i
? r
?
i
Gi
?2
+ ?
|Ct|?
i=1
Li?
ri
?i
? r
?
i
Li
?2
(6)
where Gi is the global biased ranking score while Li
is the local biased ranking score. ?i is expected to
be the merged ranking score, namely sentence im-
portance, which will be defined later. Among the
two components in the objective function, the first
component means that the refined rank should not
deviate too much from the global biased rank. We
use ? ri?i ?
r?i
Gi ?
2 instead of ?ri? r?i ?2 in order to dis-
tinguish the differences between sentences from the
same rank gap. The second component is similar by
refining rank from local biased summarization.
Our goal is to find R = R? to minimize the cost
function, i.e.,R? = argmin{O(R)}. R? is the final
rank merged by our algorithm. To minimize O(R),
we compute its first-order partial derivatives.
?O(R)
?ri
= 2??i
( Gi?i
ri ? r?i ) +
2?
?i
(Li?i
ri ? r?i ) (7)
Let ?O(R)?ri = 0, we get
r?i =
??ir?i + ??ir
?
i
?Gi + ?Li
(8)
Two special cases are that if (1) ? = 0, ? 6= 0:
we obtain ri = ?ir?i /Li, indicating we only use the
local ranking score. (2) ? 6= 0, ? = 0, indicating we
ignore local ranking score and only consider global
biased summarization using inter-date dependency.
There can be many ways to calculate the sen-
tence importance ?i. Here we define ?i as the
weighted combination of itself with ranking scores
from global biased and local biased summarization:
?(z)i =
?Gi + ?Li + ??(z?1)i
?+ ? + ? . (9)
To save one parameter we let ?+?+? = 1. In the z-
th iteration, r(z)i is dependent on ?(z?1)i and ?(z)i is
indirectly dependent on r(z)i via ?(z?1)i . ?(0)i = 0.
We iteratively approximate final ?i for the ultimate
rank listR?. The expectation of stable ?i is obtained
when ?(z)i = ?(z?1)i . Final ?i is expected to satisfy
?i = ?Gi + ?Li + ??i:
?i =
?Gi + ?Li
1? ? =
?Gi + ?Li
?+ ? (10)
Final ?i is dependent only on original global/local
biased ranking scores. Equation (8) becomes more
concise with no ? or ?: r? is a weighted combina-
tion of global and local ranks by ?? (? 6= 0, ? 6= 0):
r?i =
?
?+ ? r
?
i +
?
?+ ? r
?
i
= 11 + ?/?r
?
i +
1
1 + ?/? r
?
i
(11)
4 Experiments and Evaluation
4.1 Datasets
There is no existing standard test set for ETTS meth-
ods. We randomly choose 6 news subjects with
special coverage and handcrafted timelines by ed-
itors from 10 selected news websites: these 6 test
sets consist of news datasets and golden standards to
evaluate our proposed framework empirically, which
amount to 10251 news articles. As shown in Ta-
ble 2, three of the sources are in UK, one of them
437
(a) An illustrative network. (a) PageRank on t. (b) DivRank on t (c) DivRank on t?
Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from
DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect
temporal biased processes on t {1,9} in (c) and t? {2,12} in (d).
is in China and the rest are in the US. We choose
these sites because many of them provide timelines
edited by professional editors, which serve as refer-
ence summaries. The news belongs to different cate-
gories of Rule of Interpretation (ROI) (Kumaran and
Allan, 2004). More detailed statistics are in Table 3.
Table 2: News sources of 6 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 6 datasets.
News Subjects #size #docs #stamps #RT AL
1.Influenza A 115026 2557 331 5 83
2.Financial Crisis 176435 2894 427 2 118
3.BP Oil Spill 63021 1468 135 6 76
4.Haiti Earthquake 12073 247 83 2 32
5.Jackson Death 37819 925 168 3 64
6.Obama Presidency 79761 2160 349 5 92
size: the whole sentence counts; #stamps: the number of timestamps;
Note average size of subsets is calculated as: avg.size=#size/#stamps;
RT: reference timelines; AL: avg. length of RT measured in sentences.
4.2 Experimental System Setups
? Preprocessing. As ETTS faces with much larger
corpus compared with traditional MDS, we apply
further data preprocessing besides stemming and
stop-word removal. We extract text snippets repre-
senting atomic ?events? from all documents with a
toolkit provided by Yan et al (2010; 2011a), by
which we attempt to assign more fine-grained and
accurate timestamps for every sentence within the
text snippets. After the snippet extraction procedure,
we filter the corpora by discarding non-event texts.
? Compression Rate and Date Selection. After
preprocessing, we obtain numerous snippets with
fine-grained timestamps, and then decompose them
into temporally tagged sentences as the global col-
lection C. We partition C according to timestamps
of sentences, i.e., C = C1 ? C2 ? ? ? ? ? C |T |.
Each component summary is generated from its cor-
responding sub-collection. The sizes of component
summaries are not necessarily equal, and moreover,
not all dates may be represented, so date selection
is also important. We apply a simple mechanism
that users specify the overall compression rate ?, and
we extract more sentences for important dates while
fewer sentences for others. The importance of dates
is measured by the burstiness, which indicates prob-
able significant occurrences (Chieu and Lee, 2004).
The compression rate on ti is set as ?i = |Ci||C| .
4.3 Evaluation Metrics
The ROUGE measure is widely used for evaluation
(Lin and Hovy, 2003): the DUC contests usually of-
ficially employ ROUGE for automatic summariza-
tion evaluation. In ROUGE evaluation, the summa-
rization quality is measured by counting the num-
ber of overlapping units, such as N-gram, word se-
quences, and word pairs between the candidate time-
lines CT and the reference timelines RT . There are
several kinds of ROUGE metrics, of which the most
important one is ROUGE-N with 3 sub-metrics:
1 ROUGE-N-R is an N-gram recall metric:
ROUGE-N-R =
?
I?RT
?
N-gram?I
Countmatch(N-gram)
?
I?RT
?
N-gram?I
Count (N-gram)
438
2 ROUGE-N-P is an N-gram precision metric:
ROUGE-N-P =
?
I?CT
?
N-gram?I
Countmatch(N-gram)
?
I?CT
?
N-gram?I
Count (N-gram)
3 ROUGE-N-F is an N-gram F1 metric:
ROUGE-N-F = 2? ROUGE-N-P? ROUGE-N-RROUGE-N-P + ROUGE-N-R
I denotes a timeline. N in these metrics stands for
the length of N-gram and N-gram?RT denotes the
N-grams in reference timelines while N-gram?CT
denotes the N-grams in the candidate timeline.
Countmatch(N-gram) is the maximum number of N-
gram in the candidate timeline and in the set of ref-
erence timelines. Count(N-gram) is the number of N-
grams in reference timelines or candidate timelines.
According to (Lin and Hovy, 2003), among all
sub-metrics, unigram-based ROUGE (ROUGE-1)
has been shown to agree with human judgment most
and bigram-based ROUGE (ROUGE-2) fits summa-
rization well. We report three ROUGE F-measure
scores: ROUGE-1, ROUGE-2, and ROUGE-W,
where ROUGE-W is based on the weighted longest
common subsequence. The weight W is set to be
1.2 in our experiments by ROUGE package (version
1.55). Intuitively, the higher the ROUGE scores, the
similar the two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used sum-
marization algorithms as baseline systems. They
are designed for traditional summarization without
trans-temporal dimension. The first intuitive way to
generate timelines by these methods is via a global
summarization on collection C and then distribu-
tion of selected sentences to their source dates. The
other one is via an equal summarization on all local
sub-collections. For baselines, we average both in-
tuitions as their performance scores. For fairness we
conduct the same preprocessing for all baselines.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according
to the following three parameters: centroid value,
positional value, and first-sentence overlap.
GMDS: The graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
Chieu: (Chieu and Lee, 2004) present a simi-
lar timeline system with different goals and frame-
works, utilizing interest and burstiness ranking but
neglecting trans-temporal news evolution.
ETTS: ETTS is an algorithm with optimized
combination of global/local biased summarization.
RefTL: As we have used multiple human time-
lines as references, we not only provide ROUGE
evaluations of the competing systems but also of the
human timelines against each other, which provides
a good indicator as to the upper bound ROUGE
score that any system could achieve.
4.5 Overall Performance Comparison
We use a cross validation manner among 6 datasets,
i.e., train parameters on one subject set and exam-
ine the performance on the others. After 6 training-
testing processes, we take the average F-score per-
formance in terms of ROUGE-1, ROUGE-2, and
ROUGE-W on all sets. The overall results are shown
in Figure 4 and details are listed in Tables 4?6.
Figure 4: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected.
? The results of Centroid are better than those of
Random, mainly because the Centroid method takes
439
Table 4: Overall performance comparison on Influenza
A (ROI? category: Science) and Financial Crisis (ROI
category: Finance). ?=0.4, kernel=Gaussian, ?=60.
1. Influenza A 2. Financial Crisis
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.491 0.114 0.161 0.458 0.112 0.159
Random 0.257 0.039 0.081 0.230 0.030 0.071
Centroid 0.331 0.050 0.114 0.305 0.041 0.108
GMDS 0.364 0.062 0.130 0.327 0.054 0.110
Chieu 0.350 0.059 0.128 0.325 0.052 0.109
ETTS 0.375 0.071 0.132 0.339 0.058 0.112
Table 5: Overall performance comparison on BP Oil
(ROI category: Accidents) and Haiti Quake (ROI cate-
gory: Disasters). ?=0.4, kernel=Gaussian, ?=30.
3. BP Oil 4. Haiti Quake
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.517 0.135 0.183 0.528 0.139 0.187
Random 0.262 0.041 0.096 0.266 0.043 0.093
Centroid 0.369 0.062 0.128 0.362 0.060 0.129
GMDS 0.389 0.084 0.139 0.380 0.106 0.137
Chieu 0.384 0.083 0.139 0.383 0.110 0.138
ETTS 0.441 0.107 0.158 0.436 0.111 0.145
Table 6: Overall performance comparison on Jackson
Death (ROI category: Legal Cases) and Obama Presi-
dency (ROI category: Politics). ?=0.4, kernel=Gaussian,
?=30.
5. Jackson Death 6. Obama Presidency
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.482 0.113 0.161 0.495 0.115 0.163
Random 0.232 0.033 0.080 0.254 0.039 0.084
Centroid 0.320 0.051 0.109 0.325 0.053 0.111
GMDS 0.341 0.059 0.127 0.359 0.061 0.129
Chieu 0.344 0.059 0.128 0.346 0.060 0.125
ETTS 0.358 0.061 0.130 0.369 0.074 0.133
?ROI: news categorization defined by Linguistic Data Consortium.
into account positional value and first-sentence over-
lap, which facilitate main aspects summarization.
? The GMDS system outperforms centroid-based
summarization methods. This is due to the fact that
PageRank-based framework ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
Traditional MDS only consider sentence selection
from either the global or the local scope, and hence
bias occurs. Mis-selected sentences result in a low
recall. Generally the performance of global priority
intuition (i.e. only global summarization and then
distribution to temporal subsets) is better than local
priority methods (only local summarization). Proba-
ble bias is enlarged by searching for worthy sentence
in single dates. However, precision drops due to ex-
cessive choice of global timeline-worthy sentences.
Figure 5: ?/?: global/local combination.
Figure 6: ? on long topics (?1 year).
Figure 7: ? on short topics (<1 year).
? In general, the result of Chieu is better than
Centroid but unexpectedly, worse than GMDS. The
reason may be that Chieu does not capture suffi-
cient timeline attributes. The ?interest? modeled
440
in the algorithms actually performs flat clustering-
based summarization which is proved to be less use-
ful (Wang and Li, 2010). GMDS utilizes sentence
linkage, and partly captures ?correlativeness?.
? ETTS under our proposed framework outper-
forms baselines, indicating that the properties we
use for timeline generation are beneficial. We also
add a direct comparison between ETTS and ETS
(Yan et al, 2011b). We notice that both balanced
algorithms achieve comparable performance (0.386
v.s. 0.412: a gap of 0.026 in terms of ROUGE-
1), but ETTS is much faster than ETS. It is under-
standable that ETS refines timelines based on neigh-
boring component summaries iteratively while for
ETTS neighboring information is incorporated in
temporal projection and hence there is no such pro-
cedure. Furthermore, ETS has 8 free parameters to
tune while ETTS has only 2 parameters. In other
words, ETTS is more simple to control.
? The performance on intensive focused news
within short time range (|last timestamp?first times-
tamp |<1 year) is better than on long lasting news.
Having proved the effectiveness of our proposed
methods, we carry the next move to identity how
global?local combination ratio ?/? and projection
kernels take effects to enhance the quality of a sum-
mary in parameter tuning.
4.6 Parameter Tuning
Each time we tune one parameter while others are
fixed. To identify how global and local biased sum-
marization combine, we provide experiments on the
performance of varying ?/? in Figure 5. Results in-
dicate that a balance between global and local biased
summarization is essential for timeline generation
because the performance is best when ?? ? [10, 100]and outperforms global and local summarization in
isolation, i.e., when ?=0 or ? = 0 in Figure 5. Inter-
estingly, we conclude an opposite observation com-
pared with ETS. Different approaches might lead to
different optimum of global/local combination.
Another key parameter ? measures the temporal
projection influence from global collection to local
collection and hence the size of neighboring sen-
tence set. 6 datasets are classified into two groups.
Subject 1, 2, 6 are grouped as long news with a time
span of more than one year and the others are short
news. The effect of ? varies on long news sets and
short news sets. In Figure 6 ? is best around 60 and
in Figure 7 it is best at about 20?40, indicating long
news has relatively wider semantic scope.
We then examine the effect of different projection
kernels. Generally, Gaussian kernel outperforms
others and window kernel is the worst, probably be-
cause Gaussian kernel provides the best smoothing
effect with no arbitrary cutoffs. Window kernel fails
to distinguish different weights of neighboring sets
by temporal proximity, so its performance is as ex-
pected. Other 3 kernels are comparable.
4.7 Sample Output and Case Study
Sample output is presented in Table 7 and it shares
major information similarity with the human time-
line in Table 1. Besides, we notice that a dynamic
?i is reasonable. Important burstiness is worthy of
more attention. Fewer sentences are selected on the
dates when nothing new occurs.
Interesting Findings. We notice that humans have
biases to generate timelines for they have (1) pref-
erence on local occurrences and (2) different writ-
ing styles. For instance, news outlets from United
States tend to summarize reactions by US govern-
ment while UK websites tend to summarize British
affairs. Some editors favor statistical reports while
others prefer narrative style, and some timelines
have detailed explanations while others are ex-
tremely concise with no more than two sentences for
each entry. Our system-generated timelines have a
large variance among all golden standards. Proba-
bly a new evaluation metric should be introduced to
measure the quality of human generated timelines
to mitigate the corresponding biases. A third in-
teresting observation is that subjects have different
volume patterns, e.g., H1N1 has a slow start and a
bursty evolution and BP Oil has a bursty start and a
quick decay. Obama is different in nature because
the report volume is temporally stable and scattered.
5 Conclusion
We present a novel solution for the important
web mining problem, Evolutionary Trans-Temporal
Summarization (ETTS), which generates trajectory
timelines for news subjects from massive data. We
formally formulate ETTS as a combination of global
and local summarization, incorporating affinity and
441
Table 7: Selected part of timeline generated by ETTS for BP Oil.
April 20, 2010
s1: An explosion on the Deepwater Horizon offshore oil drilling rig in
the Gulf of Mexico, around 40 miles south east of Louisiana, causing
several kills and injuries.
s2: The rig was drilling in about 5,000ft (1,525m) of water, pushing
the boundaries of deepwater drilling technology.
s3: The rig is owned and operated by Transocean, a company hired by
BP to carry out the drilling work.
s4: Deepwater Horizon oil rig fire leaves 11 missing.
April 22, 2010
s1: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s2: The Deepwater Horizon sinks to the bottom of the Gulf after burn-
ing for 36 hours, raising concerns of a catastrophic oil spill.
s3: Deepwater Horizon rig sinks in 5,000ft of water.
April 23, 2010
s1: The US coast guard suspends the search for missing workers, who
are all presumed dead.
s2: The Coast Guard says it had no indication that oil was leaking from
the well 5,000ft below the surface of the Gulf.
s3: Underwater robots try to shut valves on the blowout preventer to
stop the leak, but BP abandons that failed effort two weeks later.
s4: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s5: Deepwater Horizon clean-up workers fight to prevent disaster.
April 24, 2010
s1: Oil is found to be leaking from the well.
April 26, 2010
s1: BP?s shares fall 2% amid fears that the cost of cleanup and legal
claims will hit the London-based company hard.
s2: Roughly 15,000 gallons of dispersants and 21,000ft of containment
boom are placed at the spill site.
April 27, 2010
s1: BP reports a rise in profits, due in large part to oil price increases,
as shares rise again.
s2: The US departments of interior and homeland security announce
plans for a joint investigation of the explosion and fire.
s3: Minerals Management Service (MMS) approves a plan for two re-
lief wells.
s4: BP chairman Tony Hayward says the company will take full re-
sponsibility for the spill, paying for legitimate claims and cleanup cost.
April 28, 2010
s1: The coast guard says the flow of oil is 5,000bpd, five times greater
than first estimated, after a third leak is discovered.
s2: BP?s attempts to repair a hydraulic leak on the blowout preventer
valve are unsuccessful.
s3: BP reports that its first-quarter profits more than double to ?3.65
billion following a rise in oil prices.
s4: Controlled burns begin on the giant oil slick.
diversity into a unified ranking framework. We im-
plement a system under such framework for ex-
periments on real web datasets to compare all ap-
proaches. Through our experiment we notice that
the combination plays an important role in timeline
generation, and global optimization weights slightly
higher (?/? ? [10, 100]), but auxiliary local infor-
mation does help to enhance performance in ETTS.
Acknowledgments
This work was partially supported by NSFC with
Grant No.61073082, 60933004, 70903008 and
61073081, and Xiaojun Wan was supported by
NSFC with Grant No.60873155 and Beijing Nova
Program (2008B03).
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 121?128.
Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010.
Topic initiator detection on the world wide web. In
Proceedings of the 19th international conference on
WWW?10, pages 481?490.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL?03, pages 71?78.
442
Yuanhua Lv and ChengXiang Zhai. 2009. Positional lan-
guage models for information retrieval. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09, pages 299?306.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD?10, pages 1009?1018.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Russell Swan and James Allan. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR?00, pages 49?56.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Dingding Wang and Tao Li. 2010. Document update
summarization using incremental hierarchical cluster-
ing. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ?10, pages 279?288.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In Information Retrieval Tech-
nology - 6th Asia Information Retrieval Societies Con-
ference, AIRS 2010, pages 490?501.
Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming
Li. 2011a. A fine-grained digestion of news webpages
through event snippet extraction. In Proceedings of
the 20th international conference companion on world
wide web, WWW ?11, pages 157?158.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings of
the 34th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?11.
443
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1281?1291,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Summarizing Complex Events: a Cross-modal Solution of Storylines
Extraction and Reconstruction
Shize Xu
xsz@pku.edu.cn
Shanshan Wang
cheers echo mch@163.com
Yan Zhang?
zhy@cis.pku.edu.cn
Department of Machine Intelligence, Peking University, Beijing, China
Key Laboratory on Machine Perception, Ministry of Education, Beijing, China
Abstract
The rapid development of Web2.0 leads to
significant information redundancy. Espe-
cially for a complex news event, it is diffi-
cult to understand its general idea within a
single coherent picture. A complex event of-
ten contains branches, intertwining narratives
and side news which are all called storylines.
In this paper, we propose a novel solution to
tackle the challenging problem of storylines
extraction and reconstruction. Specifically, we
first investigate two requisite properties of an
ideal storyline. Then a unified algorithm is
devised to extract all effective storylines by
optimizing these properties at the same time.
Finally, we reconstruct all extracted lines and
generate the high-quality story map. Exper-
iments on real-world datasets show that our
method is quite efficient and highly compet-
itive, which can bring about quicker, clearer
and deeper comprehension to readers.
1 Introduction
News reports usually consist of various modalities
of tremendous information, especially all kinds of
textual information and visual information, which
make web users dazzled and lost. The situation gets
worse on complex news events. To help readers
quickly grasp the general information of the news,
a more concise and convenient system over multi-
modality information should be provided. For ex-
ample, given a large collection of texts and images
related to a specified news event (e.g., East Japan
?Corresponding author
Earthquake), such a system should present a terse
and brief summarization about the event by showing
different clues of its development, and thus helping
readers to effectively find out ?when, where, what,
how and why? at a glance.
The researches (Goldstein et al, 2000) on auto-
matic multi-document summarization (MDS) have
helped a lot when we generate a description for a
specific event. However, it traditionally exhibits in a
very simple style like a ?0-dimensional? point. The
appearance of Timeline (Allan et al, 2001) brings
about a visual progress for massive documents anal-
yses. Readers can not only get the most important
ideas, but also browse the story evolution in chrono-
logical order. Previous news summarization systems
with structured output (Yan et al, 2011) have fo-
cused on timeline generation. Timeline becomes a
?1-dimensional? line. This style of summarization
only works for simple stories, which are linear in na-
ture. However, the structure of complex stories usu-
ally turns out to be non-linear. These stories branch
into storylines, dead ends, intertwining narratives
and side news. To explore these lines, we need a
map to reorganize all the information. Therefore,
a ?2-dimensional? story map is in bad need. Fig-
ure 1 shows a part of the story map generated by our
system for representing East Japan Earthquake. We
notice that the whole event evolves into 4 branches.
Each of them focuses on a specific sub-topic and is
distinct from other lines. Figure 2 takes a close look
at the 4 nodes from different lines, and they differ a
lot from each other as expected.
Text information is more precise and exquisite
when compared with images. Nevertheless, as the
1281
 Japan's nuclear safety agency says the cooling system of a third nuclear reactor at Fukushima has failed.     A government spokesman says the blast destroyed a building which housed a nuclear reactor, but the reactor escaped unscathed.     Around 170,000 people have been evacuated from a 12-mile radius around the Fukushima number one nuclear plant.     ??.. 
Mar 12 Mar 13 Mar 14 Mar 17, 2011 Mar 15 Mar 16 Mar 11 
    A 9.0 magnitude quake triggers a devastating tsunami off northeast Japan, leaving some 19,000 people dead or missing.     A massive earthquake, 8.9 on the Richter scale, unleashes a huge tsunami which crashes through Japan's eastern coastline, sweeping buildings, boats, cars and people miles inland.     Japan's most powerful earthquake since records began has struck the north-east coast, triggering a massive tsunami.      ??.. 
    A Japanese rescue team member walks through the completely leveled village of Saito, in northeastern Japan.     Half a million people have been made homeless by the devastating quake.     Fire crews from Greater Manchester and Lancashire have flown out to Japan as part of the UK's International Search and Rescue team.      ??.. 
     Japan's cabinet on Friday approved a $49 billion budget to help in the reconstruction of areas decimated by last month's earthquake and tsunami.     Japan faces a reconstruction bill of at least $180 billion,  or 3 percent of its annual economic output.     In an televised statement after the blast, prime minister Kan urges those within 19 miles of the area to stay indoors. ???? 
???? ???? 
             Nuclear Crisis                     Post-disaster                    Rescue                     Tsunamis Blue GreenRedBlack
? Storyline 
? Story node 
? Candidate 
Figure 1: Four storylines are obtained in the story map of ?East Japan Earthquake?. They focus on Tsunamis, Nuclear
Crisis, Rescue and Post-disaster respectively.
saying goes, ?a picture paints a thousand words?,
an image could provide far more information than
words do. In fact, a summarization including both
texts and images will absolutely yield a more pow-
erful and intuitive description about the news event.
Under this motivation, we study on extracting and
reconstructing tracks with different sub-topics for a
complex event. To the best of our knowledge, the ex-
ploration and analysis of 2-dimensional cross-modal
summarization is academically novel.
We are faced with two main problems. The first
is how to select the most important sentences and
images to make up the final story map. The previ-
ous work by Shahaf et al presents a 2-D story map
called ?metro map?, which summarizes the com-
plex topics (Shahaf et al, 2012). They study on the
document-level, and use the entire news document
as one story node. But on real web, this may con-
front some difficulties. On one hand, news articles
may report the event from different perspectives, es-
pecially those reviews or retrospective reports. This
kind of documents contains many useful saliency in-
formation of different sub-topics, but they cannot
be further subdivided to help understand each bet-
ter. On the other hand, some documents, such as
Line of ?Tsunamis? on Mar 11 Line of ?Nuclear Explosion? on Mar 12 
Line of ?Rescue? on Mar 13  Line ?Post-disaster? on Mar 17  
Figure 2: Word distributions vary a lot among nodes in
different storylines
cover news and interviewing reports, contain one or
two famous remarks. Since these documents also in-
clude too much useless information, it?s inappropri-
ate to use the whole document as a story node. So
our work, the sentence-level story map extraction,
is just aimed at this brand new problem. The sec-
ond problem is the way to utilize the cross-modal
1282
information suitably. Our goal is not only to fuse
sentences and images together, but also to provide a
unified framework to improve them mutually.
In this paper, we introduce a novel solution for
the story map summarization problem. All the sen-
tences and images are the candidates for making up
the final map. The analysis of complicated infor-
mation usually requires a semantic-level knowledge
study. We address this in the pre-processing of data
in Section 3.1. The key task of our research is the
extraction of storylines. We reveal two fundamental
properties of an ideal storyline, and propose an op-
timization algorithm in Section 3.2. A highly com-
patible MDS sub-algorithm is also fused in and of-
fers help to the sentences/images selection. In Sec-
tion 3.3, the extracted storylines are reconstructed
as a final story map. The experimental results con-
ducted on four real datasets show that our approach
can perform effectively.
The rest of this paper is organized as follows.
Some related researches are demonstrated in Sec-
tion 2. We introduce our methodology in Section 3.
The experimental results in Section 4 prove the ef-
fectiveness of our approach. Finally, we conclude
this paper and present our future work in Section 5.
2 Related Work
Generally speaking, multi-document summarization
can be either extractive or abstractive. Researchers
mainly focus on the former which extracts the in-
formation deemed most important to the summary.
Various techniques have been used for this type of
MDS (Haghighi and Vanderwende, 2009; Contrac-
tor et al, 2012). Graph-based text summarization
techniques have been widely used for years. The
algorithms, used in TextRank (Mihalcea and Tarau,
2005) and LexPageRank (Radev et al, 2004), which
are meant to compute sentence importance, are sim-
ilar to those in PageRank and HITS.
Recently, timeline becomes a popular style to
present a schedule of events and attracts many re-
searchers consequently. For example, Yan et al
make use of timestamps to generate an evolutionary
timeline (Yan et al, 2011). Shahaf et al present a
2-D story map called ?metro map? to summarize the
complex topic (Shahaf et al, 2012). But previous
work only studies on the document level, which in-
evitably brings about much information redundancy.
Previous studies show that the use of visual ma-
terials not only leads to the conservation of infor-
mation but also promotes comprehension (Panjwani
et al, 2009). Thus the cross-modal fusion is nec-
essary. Wu et al propose a framework of multi-
modal information fusion for multimedia data anal-
ysis by learning the optimal combination of multi-
modal information with the superkernel fusion (Wu
et al, 2004). Borrowing the idea of recommendation
in heterogeneous network into the cross-modal news
summarization is also a convincing research. Xu et
al. tackle this task and bring out an 1-D cross-media
timeline generation framework (Xu et al, 2013).
Summarization of multimedia involves researches
on information retrieval of multimedia. Since tex-
tual and visual information are quite different from
each other, how to make a good transformation to
mine latent knowledge from unannotated images is
of great concern. Feng and Lapata (2010) use visual
words to describe visual features and then propose
a probabilistic model based on the assumption that
images and their co-occurring textual data are gen-
erated by mixtures of latent topics.
However, to the best of our knowledge, no exist-
ing research manages to generate a 2-dimensional
story map automatically and integrate images and
texts into a unified framework at the same time.
3 Methodology
The original data of one event is a collection of news
documents on different days, or in a finer granu-
larity, a set of sentences and images with different
timestamps. Each item in the data collection is a
candidate for selection to form the final map. We
denote the data collection as C, and C = Cs ? Cv,
where Cs is the subset containing all sentences and
Cv contains all images. In the following elaboration
of our method, dateset C is the important knowl-
edge base. As the ultimate goal for a specific event,
we would like to generate the ?2-D? story mapM,
whose main component is a set of ?1-D? storylines
L = {L1, L2, . . .}. Each storyline L ? L is made
up of a set of ?0-D? story nodes, L = {I1, I2, . . .},
each of which is composed of a set of candidates
sharing the same timestamp I = {c1, c2, . . .}. For
more concise, our method can be scheduled with the
1283
following three steps:
1. Prepare semantic knowledge for each candi-
date, and then purify data collection C by eliminat-
ing the noisy candidates;
2. Conduct OPT-LSH algorithm to extract L that
contains all qualified storylines;
3. Reconstruct the storylines as the final mapM.
3.1 Pre-processing
Before we extract the storylines, we have to solve
two problems first. Since our work is cross-modal,
the semantic knowledge under the literal and visual
surface is basically required. Recall from Section 2,
there exist many effective ways to dig into the se-
mantic level of image and text. In this paper, we em-
ploy the approach proposed by Jiang and Tan (2006).
They present a convincing approach which employs
a multilingual retrieval model to apply knowledge
mining on semantic level. The first is the sentence
feature vector generation. With the preprocessing
such as stemming and stop words removal, they ex-
tract the textual TF-IDF feature V ecs of each sen-
tence. The second, also the challenging part, is
the image feature vector generation. In this step,
for each region they extract visual features that are
consisting of 6 color features and 60 gabor features
which have been proven to be useful in many ap-
plications. Color features are the means and vari-
ances of the RGB color spaces. Gabor features are
extracted by calculating the means and variations of
the filtered image regions on 6 orientations. After
the visual feature vectors of the image regions are
extracted, all image regions are clustered using the
k-means algorithm. The generated clusters, called
?visterms? or ?visual words?, are treated as a vocab-
ulary for the images. Besides visual features, they
also utilize the context textual feature of each im-
age as the semantic supplement to generate to final
feature vector V ecv.
Based on these feature vector of each canti-
date, they further calculating the intra-modal sim-
ilarity with classical IR methods, they obtain the
inter-modal similarity through the vague transfor-
mation (Mandl T, 1998). We also note that trans-
lation tools such as VIPS (Cai et al, 2003) and We-
bKit1 can help us to segment web documents and
1http://www.webkit.org
pick those text blocks whose coordinates are neigh-
boring to each specified image. In this way we can
successfully obtain images, text contexts and con-
tent sentences. Three kinds of semantic similar-
ity are now ready. They are uniformly denoted as
sim(ci, cj), representing the similarity between two
candidates. ci and cj can be any type of modalities.
Another problem is the noise from irregular data.
We would like to utilize the sentences and images of
high quality. The intuitive assumption is that a good
candidate should have substance in speech, and be
coherent with other good candidates. Fortunately,
many useful measures are now available. They can
be used to choose better candidates. Inspired by the
analysis analogy to information retrieval, we extend
the idea of the classical PageRank algorithm to esti-
mate the authority for each candidate. The similarity
between two candidates is regarded as the weighted
?link? between them.
Inspired by the idea of classic ?update summa-
rization? task, we try to avoid those chronologically
ordered documents sets focusing on a constant topic.
Therefore, given the particularity of our task, we
also have to develop the weighting function ? with
the temporal factor before starting the ranking algo-
rithm. Our fundamental assumption is that the inter-
date and inter-modal ?links? have different influence
comparing with the intra ones. Therefore the core
formula calculating the authority of ci is adapted as
follows to make it become weight-compatible:
Auth(ci) =
1? q
|C|
+ q ? [ ?
?
cj?C?
?(ci, cj) ?
Auth(cj)
O(cj)
+ (1? ?)
?
ck?C??
?(ci, ck) ?
Auth(ck)
O(ck)
]
C ? is the subset of C which only includes the can-
didates of the same modality with ci, and C ?? is the
cross-modal candidates subset. O(cj) denotes the
out-degree of cj , and smoothing parameter q is set
as the common value 0.85. Parameter ? is used to
balance the biases of intra- and inter-modal impact.
If ? is set to 1, it means the cross-modal information
is abandoned, and vice versa. ?(ci, cj) contains two
terms as follows:
?(ci, cj) = sim(ci, cj) ? e
??ci.t?cj .t?
2?2
1284
The second term of ??s formula is Gaussian Ker-
nel (Aliguliyev R, 2009), which is used to measure
the temporal gap between two candidates (?.t de-
notes the date-based timestamp). Note that the simi-
larity metric is content-based and time-independent,
since the time decay function is only used to adjust
the ranking impact strength. In this way, we can
give higher authority to the more informative and
coherent candidates. The optimal value of ?, which
controls the spread of kernel curves, is sensitive to
datasets and will be discussed later.
We eliminate the candidates whose authority goes
under threshold. The data collection C is then sig-
nificantly downsized and purified for later work.
Authority is also used to determine the presentation
sequence inside each story node of final map.
3.2 Extraction: LSH-OPT Algorithm
Other traditional relative methods need to pre-decide
how many sub-topics are going to be obtained, like
clustering or other supervised models. They fail
in the unsupervised automatic storylines extraction
problem. In this Section, we propose a concrete al-
gorithm to extract storyline set L from C. Our pro-
posed LSH-based algorithm can automatically op-
timize the number of storylines according to their
self-evaluation.
3.2.1 Task Formalization
Let?s investigate the storyline first. We may think
of some basic attributes as well as many exten-
sion properties. The number of nodes in the sto-
ryline L (denoted as |L|) is intuitively one of its
basic attributes. We would like to define another
basic attribute called the SUPPORT of L. In de-
tail, Support(L) = minIk?L |Ik|, which denotes
the smallest size among all the story nodes it has.
The most challenging part is to properly model
extension properties. We observe that an effective
storyline should meet three key requirements: (1)
Coherence. Within one storyline, news changes
gradually as time goes and the evolution indicates
consistency among component story nodes. We rely
on the notion of coherence developed in Connect-
the-Dots (Shahaf and Guestrin, 2010) and transform
it to what we exactly need in this research; (2) Di-
versity. According to MMR principle (Goldstein et
al., 1999), though the work is about summary, we
still can draw an analogy and derive that a good sto-
ryline should be concise and contain redundant in-
formation as few as possible, i.e., two sentences pro-
viding information of similar content should not be
presented in different storylines; (3) Coverage. The
extracted storyline set L should keep alignment with
the source collection C, which is intuitive and even
proved to be significant as proposed in (Li et al,
2009). However, Coverage in some ways is tech-
nically redundant in front of Diversity. We decide
to use the first two criteria in extraction process and
use the last one to verify the effectiveness.
Since a storyline is composed of several nodes,
we can select or abandon nodes mainly according
to these two requirements. In fact, both of them in-
volve a measurement of similarity between two story
nodes, denoted by two word distributions (see Fig-
ure 2). Specifically, for story node Ii, its distribu-
tion probability of word w is estimated as p(w|Ii) =
?
c?Ii
TF (w)
?
w
?
c?Ii
TF (w) where the denominator is used for
normalization. Then Kullback-Leibler divergence is
employed to denote the distance between two nodes
Ii and Ij :
DKL(Ii, Ij) =
?
w
p(w|Ii) log
p(w|Ii)
p(w|Ij)
In addition, we introduce the decreasing and
increasing variants based on logistic functions,
D?KL = 1/(1 + eDKL) and D
?
KL = eDKL/(1 +
eDKL), to map the distance into [0, 1]. Given the
measurement, we can formulate the two properties.
For Coherence, a storyline Li consists of a series
of individual but correlated nodes, which do not nec-
essarily have the serial timestamps. We would like
to choose such a set of nodes {I1, I2, . . .}, and at the
same time guarantee this criterion:
Cor(Li) =
1
|Li|
?
1?k<|Li|
D?KL(Ik, Ik+1)
For Diversity, each storyline Li ? L should
demonstrate quite different subtopics with other sto-
rylines. This is the most essential motivation for us
to step into 2-dimensional field. This criterion can
be used to maximize the minimum diversity value
among all storylines:
Div(L) = min
Li,Lj?L
{
?
Ik?Li
?
Ik??Lj
D?KL(Ik, Ik?)
|Li| ? |Lj |
}
1285
Then the problem can be transformed into the fol-
lowing optimization problem. Parameters ?1, ?2 and
?3 denote the minimum number of nodes in each
line, the smallest size of candidates in each node and
the coherence lower bound respectively. The task is
to extract an optimal L out of C, such that:
?L ? L, |L| ? ?1 & Support(L) ? ?2;
?L ? L, Cor(L) ? ?3;
Div(L) is maximized.
3.2.2 Optimization Algorithm
It can be proved that finding the optimal set L is
an NP-Complete problem (not presented due to the
limited space). Thus the brute-force exhaustive ap-
proach is crashed. We develop a near-optimal al-
gorithm based on locality sensitive hashing (OPT-
LSH). The original LSH solution is a popular tech-
nique used to solve the nearest neighbor search
problems in high dimensions. Its basic idea is to
hash similar input items into the same bucket (i.e.,
uniquely definable hash signature) with high proba-
bility. All potential storylines can be targeted fast if
we make good use of this idea.
LSH performs probabilistic dimension reduction
of high dimensional data by projecting a higher d-
dimensional vector V ecc (recall from Section 3.1)
to a lower d?-dimensional vector (d?<<d), such that
the candidates which are in close proximity in the
higher dimension get mapped into the same item in
the lower dimensional space with high probability.
It guarantees a lower bound on the probability that
two similar input items fall into the same bucket in
the projected space and also the upper bound on the
probability that two dissimilar vectors fall into the
same bucket (Indyk and Motwani, 1998).
One of the key requirements for good perfor-
mance of LSH is the careful selection of the fam-
ily of hashing functions. In OPT-LSH, we use the
hashing scheme proposed by Charikar (Charikar M,
2002). In detail, d? random unit d-dimensional vec-
tors r?1, r?2, . . . , r?d? are generated first. Each of the
d entries of r?i is drawn from a standardized normal
distribution N(0,1). Then the d? hashing functions
are defined as:
hi(c) =
{
1, if r?i ? V ec(c) ? 0
0, if r?i ? V ec(c) < 0
1 ? i ? d?
We represent the d?-dimensional bucket feature h
for c, h(c) := [h1(c), . . . , hd?(c)]. There are 2d
? dif-
ferent buckets at most. Each denotes a potential sto-
ryline, so we have to verify the probability of similar
candidates falling into the same bucket, whose lower
bound is given by Charikar (Charikar M, 2002).
Simply filtering and searching among all poten-
tial lines in single pass may lead to empty result
set if in post-processing no bucket satisfies all con-
straints. This could probably happen because the
input parameter d? is set so large that the optimal
set of candidates is separated into different buck-
ets. However, we will get a suboptimal result in
turn when d? is too small. We are then motivated
to tune LSH by iterative relaxation that varies d? in
each iteration. Changing the value of d? balances
the leverage between expected number of potential
storylines and their properties. We perform a binary
search between 1 and d? to identify the ideal number
of hash functions to employ. Algorithm 1 shows the
pseudo code of our OPT-LSH algorithm. The LSH
time is bounded byO(d?|C| log |C|) since the binary
search relaxation iteration runs for log |C| times in
the worst case and the hashing time is O(d?|C|).
3.3 Storylines Reconstruction
At last, we manage to reconstruct all the storylines
in Lopt. A real-world storyline may sometimes in-
tertwine with another, educe other branches, and
end its own evolvement. The way to reconstruct a
more effective layout of the story map requires fur-
ther study and provides a good research direction in
the future. However, in this paper we order the sen-
tences/images in each story node according to their
authority scores. Next, all storylines are arranged to
proceed along the timestamps, thus a storyline never
turns back in the map. Then we adjust the structure
to make story nodes sharing the same timestamp stay
close, though they belong to different lines. Figure 1
shows the sample output of our system.
4 Experiments
4.1 Dataset
There is no existing standard evaluation data set for
2-dimensional cross-modal summarization methods.
We randomly choose 4 news topics from 4 selected
news websites: New York Times, BBC, CNN and
1286
Algorithm 1 OPT-LSH Algorithm
Input: Candidate set C, similarity function sim,
bucket dimensions d?
Output: A near-optimal storylines set Lopt
// Main Algorithm
1: Initialize left = 1, right = d?, max = ?1
2: repeat
3: d? = (left + right)/2
4: L ? LSH(C, d?)
5: Revise all word distributions p(w|I) in L
6: if ?L?L, |L|??1 and Support(L)??2 and
Cor(L)??3 then
7: if Div(L) > max then
8: Lopt ? L, max? Div(L)
9: end if
10: left = d? + 1
11: else
12: right = d? ? 1
13: end if
14: until left > right
15: return Lopt
// LSH(C, d?) : Buckets
16: Generate d? unit vectors randomly
17: for j = 1 to |C| do
18: for i = 1 to d? do
19: if r?i ? V ec(cj) ? 0 then
20: hi(cj)? 1
21: else
22: hi(cj)? 0
23: end if
24: end for
25: h(cj) = [h1(cj), . . . , hd?(cj)]
26: end for
27: return Buckets? {h(cj)|cj ? C}
Reuters. We query each event confined to these sites
and crawl webpages? html docs. Referring to Sec-
tion 3.1, timestamps, text contents, images and their
text contexts are extracted. Table 1 shows the de-
tails. These 4 datasets all contain massive informa-
tion and complex evolutions.
4.2 Analysis of Our System
Since there is no standard for us to verify the ef-
fectiveness of our solution, we have to utilize con-
vincing criteria based on manual evaluation of ex-
Table 1: Statistics of Datasets.
Event (Query) Document Time Span ? ?
EJE 504 Mar 11-Apr 8, 2011 3 0.9
OWS 638 Sept 17-Dec 10, 2011 12 0.7
NBA 489 July 1-Dec 28, 2011 17 0.6
ME 437 June 21-Aug 31, 2011 9 0.7
* Abbreviations EJE, OWS, NBA, ME denote East Japan
Earthquake, Occupy Wall Street, NBA Lockout and Murdock?s
Eavesdropping respectively.
perts, and then we can compare them with other ap-
proaches. In order to setup the system, based on the
optimization problem shown in Section 3.2, we as-
sign the value of 1 to both ?1 and ?2, and empiri-
cally set ?3 as 0.6 which can balance the number of
potential lines and the quality of story map as well.
Then the problem can be re-interpreted in natural
language as follows. Given the data collection, we
manage to find out a set of storylines such that every
line in it contains at least one non-null story node
and keeps self-coherence not less than 0.6. What?s
more, the diversity of the whole set is maximized.
Before comparing OPT-LSH with other systems, we
do some further analysis of inherent properties first.
4.2.1 Compactness
The essential idea of summarization is to reduce
the data size, so that a more concise representation
will be generated and help users to fast grasp the
main points. Therefore the Compactness of a story
map needs to be guaranteed. In the pre-processing
module, we have already excluded significant num-
ber of inferior candidates with the extended PageR-
ank. Nevertheless what we really care is the com-
pactness that OPT-LSH brings about. In the candi-
dates and story nodes selection processes, only the
most saliency and coherent candidates can appear
in the final representation. We count the number of
sentences and images in Lopt, denoted as ||Lopt||,
and then we compare it with the collection size |C|
(both before and after pre-processing) to test the
compactness. Table 2 shows that OPT-LSH further
reduces the representation scale significantly.
4.2.2 Coverage
Obviously, only the verification of compactness
is far from enough. As mentioned in Section 3.2,
the storyline set L we extract should keep alignment
with the source collection, and contain informative
as well as comprehensive information inC. Thus we
1287
Table 2: The compactness of OPT-LSH
Dataset EJE OWS NBA ME
|C|-before 12049 22890 20403 10237
|C|-after 1454 2357 1187 1042
||L|| 87 168 113 92
Downsizing 94.0% 92.9% 90.5% 91.2%
need to verify another property of our summariza-
tion, the Coverage. Inspired by (Shannon C, 2001),
we employ the Information Entropy to represent the
information quantity based on solid mathematical
theory. The less information quantity decreases af-
ter summarizing, the more story comprehensiveness
is maintained. In this way we verify the property of
coverage. Particularly, Shannon denotes the entropy
H as follows of a discrete random variableX , which
in fact is a word distribution. The base knowledge
in our work is the global probability mass function
P (WC) based on the entire vocabulary of C, with
possible values {p(w1), . . . , p(w|WC |)}.
H(X) = E{? log (P (WC))?X}
= ?
?
wi?X
p(wi) log (p(wi))
Although different word distributions may have
the sameH , we do not focus on the similarity of two
corpora, but the difference of information quantities
they are carrying. So H is an ideal criterion.
Besides the comparison of the entropies of C and
L, it gives us a chance to study different modules?
contributions in our solution. There are two places
that we may simplify the solution. One is the fea-
ture of temporal gap. If we set the parameter ? to
infinity, then we can remove the second term of
the calculation of ? (i.e. ???) and then bring out
a time-insensitive system. The other is the cross-
modal feature. We set parameter ? as 1 to make
the system work in one single modality, and ignore
all images (only the textual sentences are available)
to make up story map. The work then becomes the
study with text-bias. We also implement the sim-
plest system that blocks images as well as tempo-
ral feature. Figure 3 highlights the good perfor-
mance of our system. We are maintaining infor-
mation by a larger proportion of the original data
collection. Considering the property of high com-
pactness, our solution tackles the information re-
H(X)
 
0 
10 
20 
30 
40 
EJE OWS NBA ME 
Original C 
Our system 
Time-insensitive 
Text-bias modal 
Simplest 
Figure 3: The y-axis denotes the entropy. And the larger
H is, the richer information it brings.
dundancy quite well, and promisingly delivers entire
knowledge with compact structure.
During our experiments on Coverage, we have
some interesting findings. Datasets perform differ-
ently when we take different values of ? and ?,
which controls the temporal decaying rate and cross-
modal learning respectively. The events with short
life-cycles prefer a smaller value of ? to dominate
the influence from neighbors, as well as the intra-
modal bias. On the contrary, long-living events pre-
fer lager ? and more inter-modal bias to get informa-
tion replenishment from different dates and modal-
ity. Due to the limited space we don?t present the
tuning details, but the optimal values are shown in
Table 1. In fact, using the cross-modal mutual influ-
ence can, more or less, help to improve the effective-
ness of information extraction and summarization.
4.3 User Study
Before we introduce other existing methods that can
also tackle the cross-modal 2-dimensional summa-
rization problem, we have to setup the appropriate
standards to quantify users? evaluation.
4.3.1 Metrics
In the user study, we evaluate the effectiveness of
our story maps in aiding users to integrate different
aspects of multi-faceted information. (Shahaf et al,
2012) also focuses on story map generation and puts
forward two convincing metrics to answer the fol-
lowing questions:
? Micro-Knowledge: Can the maps help users re-
trieve information faster than other methods?
? Macro-Knowledge: Can the maps help users un-
derstand the big picture better than other methods?
1288
For micro-knowledge, we wish to see how maps
help users answer specific questions. We compare
the level of knowledge attained by users using our
method with two other systems: Google News and
TDT. Google News is a computer-generated site that
aggregates headlines from news sources worldwide.
News-viewing tools are dominated by portal and
search approaches, and Google News is a typical
representative of those tools. TDT (Nallapati et al,
2004) is a successful system which captures the rich
structure and dependencies of news events.
We have noticed that making comparisons be-
tween different systems is not convincing, since the
output of Google News and TDT is different both in
content and in presentation (and in particular, can-
not be double-blind). In order to isolate the effects
of sentence selection vs. map organization, we in-
troduce a hybrid system into the study: the system
with structureless story map displays the same sen-
tences and images as our system but with none of the
structure. Its output is basically the same with our
full system but with a single storyline and merges
node content for each date. And each story nodes
are sorted chronologically and displayed similarly to
Google News. We implement TDT based on (Nalla-
pati et al, 2004) (cos + TD + SimpleThresholding),
and pick a representative article from each cluster.
The purpose of the study is to test a single query.
We also obtain the results from Google News using
the same queries.
We recruit 64 volunteers to browse all the four
events, and one of the four systems is assigned to
each person randomly. After browsing, users are
asked to answer a short questionnaire (8 questions),
composed by domain experts. Users answer as many
questions as possible in limited time (8 minutes).
The statistics of their answers are promised to eval-
uate the micro-knowledge on different systems. In
order to aid in comprehension, we give some exam-
ples about those asked questions. For the event of
EJE, we ask that
1. How many magnitude was initially reported by
the USGS, and what about the finally report?
2. List at least six countries that had dispatched
their rescue teams.
3. . . .
And for OWS, we ask that
Table 3: Macro-knowledge performance on four datasets
Dataset Our System Google News TDT
EJE 56.3% 23.2% 20.5%
OWS 62.2% 22.1% 15.7%
NBA 58.3% 18.9% 22.8%
ME 47.2% 26.3% 26.5%
1. What was the attitude of President Obama about
the protesters on October?
2. When did the protesters begin dressing ?corpo-
rate zombies? in New York?
3. . . .
These questions can effectively help us to investi-
gate users? micro-knowledge about the events.
As for macro-knowledge, unlike the retrieval
study that evaluates users? ability to answer ques-
tions, we are interested in the use of story maps as
high-level overviews, allowing users to understand
the big picture. We believe that the ability to ex-
plain a certain issue is the only proof of understand-
ing. Therefore, the 64 volunteers are then asked to
write four paragraphs to summarize the four events
respectively. This time, all three systems? (the struc-
tureless system presents the same content as our sys-
tem) results are provided and we let users choose the
sentences with complete freedom. Then we count
the number of sentences they employed from each
system and derive the average proportions. Accord-
ing to the results we can research on the macro-
knowledge that different systems deliver.
4.3.2 Results
We take the time cost and the average numbers of
correct answers of different systems to evaluate on
the micro-knowledge. Figure 4 shows the results.
We can find out that our system outperforms the
others significantly when users taking less time to
learn the knowledge. The failure of structureless
system proves that our work of storyline reconstruc-
tion makes lots of advantages to help reading.
On the other hand, Table 3 analyzes the statistics
of macro-knowledge. It?s also obvious that users
would like to refer to the sentences that our sys-
tem provides. A reasonable explanation is that the
story maps we generate can clarify users? thoughts
and views on the complicated events.
1289
minutes 
Corre
ct Num
ber 
East Japan Earthquake 
minutes 
Corre
ct Num
ber 
Occupy Wall Street 
minutes 
Corre
ct Num
ber 
NBA Lockout 
minutes 
Corre
ct Num
ber 
Murdock's Eavesdropping 
Figure 4: Micro-knowledge performance on four datasets
Table 4: Average runtime of different datasets
Dataset EJE OWS NBA ME
|C| 1454 2357 1187 1042
Iterations 3.5 4.7 5.4 3.7
Runtime (ms) 1582 3214 3089 1314
4.4 Runtime Analysis
At last, we analyze the time performance of our
OPT-LSH algorithm on a PC server (16G RAM,
2.67GHz 4-processors CPU). The average iterations
for different initial value of d? and the runtime are
shown in Table 4. The results are acceptable.
5 Conclusions
In this paper, we study the feasibility of automati-
cally generating cross-modal story maps and present
a novel solution to this challenging problem. Our
works mainly tackle the problems of storylines ex-
traction and reconstruction. Specifically, we inves-
tigate two requisite properties of an ideal storyline,
Coherence and Diversity. Then the convincing cri-
teria are devised to model both. We formalize the
task as an optimization problem and design an al-
gorithm to solve it. Classical IR and text analyzing
techniques like PageRank are fused into the unified
framework, and a near-optimal solution is employed
to deal with the NP-complete problem. Experiments
on web datasets show that our method is quite effi-
cient and competitive. We also verify that it brings
quicker, clearer and deeper comprehension to users.
As a future work, we plan to adapt parame-
ters automatically on the basis of different types of
datasets. Improving the layout quality of story map
by concerning the interactivity of different media
(e.g. images order) is also significant. Furthermore,
our framework is universal, so that the media other
than text and image can be adopted as well.
Acknowledgments
We sincerely thank all the anonymous reviewers for
their valuable comments, which have helped to im-
prove this paper greatly.
This work is supported by NSFC with Grant No.
61073081 and 61370054, and 973 Program with
Grant No. 2014CB340405.
1290
References
Agrawal R, Gollapudi S, Kannan A, et al 2011 En-
riching textbooks with images. Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, ACM, pages 1847-1856.
Aliguliyev R M. 2009 A new sentence similarity mea-
sure and sentence based extractive technique for auto-
matic text summarization. Expert Systems with Appli-
cations, 2009, 36(4): 7764-7772.
Allan J, Gupta R, Khandelwal V. 2001 Temporal sum-
maries of new topics. Proceedings of the 24th Annual
International ACM Conference on SIGIR, pages 10-
18.
Cai D, Yu S, Wen J R, et al 2003 VIPS: a visionbased
page segmentation algorithm. Microsoft Technical Re-
port, MSR-TR-2003-79.
Charikar M S. 2002 Similarity estimation techniques
from rounding algorithms. Proceedings of the 34th
Annual ACM Symposium on Theory of Computing,
ACM, pages 380-388.
Chen Y, Jin O, Xue G R, et al 2010 Visual contex-
tual advertising: Bringing textual advertisements to
images. Proceedings of the 24th AAAI Conference,
AAAI, pages 1314-1320.
Contractor D, Guo Y, Korhonen A. 2012. Using Argu-
mentative Zones for Extractive Summarization of Sci-
entific Articles. COLING, pages 663-678.
Evans D K, McKeown K, Klavans J L. 2005 Similarity-
based multilingual multi-document summarization.
IEEE Transactions on Information Theory, pages
1858C1860.
Feng Y, Lapata M. 2010 Topic models for image annota-
tion and text illustration. Human Language Technolo-
gies: The 2010 Annual Conference of NAACL, pages
831-839.
Goldstein J, Mittal V, Carbonell J, Kantrowitz M.
2000 Multi-document summarization by sentence
extraction. Proceedings of the 2000 NAACL-
ANLPWorkshop on Automatic summarization-
Volume 4. Association for Computational Linguistics,
pages 40-48.
Goldstein J, Kantrowitz M, Mittal V, et al 1999 Summa-
rizing text documents: sentence selection and evalua-
tion metrics. Proceedings of the 22nd Annual Interna-
tional ACM Conference on SIGIR, ACM, pages 121-
128.
Haghighi A, Vanderwende L. 2009. Exploring content
models for multi-document summarization. Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of NAACL, Association for Compu-
tational Linguistics, pages 362-370.
Indyk P, Motwani R. 1998 Approximate nearest neigh-
bors: towards removing the curse of dimensionality.
Proceedings of the 30th Annual ACM Symposium on
Theory of Computing, ACM, pages 604-613.
Jiang T, Tan A H. 2006 Discovering image-text associa-
tions for cross-media web information fusion. Knowl-
edge Discovery in Databases: PKDD 2006, pages 561-
568.
Li L, Zhou K, Xue G R, et al 2009 Enhancing diver-
sity, coverage and balance for summarization through
structure learning. Proceedings of the 18th Interna-
tional Conference on World Wide Web, ACM, pages
71-80.
Mandl T. 1998 Vague transformations in information
retrieval. ISI 1998, pages 312-325.
Mihalcea R, Tarau P. 2005 A language independent al-
gorithm for single and multiple document summariza-
tion. Proceedings of IJCNLP 2005.
Nallapati R, Feng A, Peng F, et al 2004 Event threading
within news topics. Proceedings of the 13rd ACM In-
ternational Conference on Information and Knowledge
Management, ACM, pages 446-453.
Panjwani S, Micallef L, Fenech K, et al 2009 Effects of
integrating digital visual materials with textbook scans
in the classroom. International Journal of Education
and Development using ICT, 2009, 5(3).
Radev D R, Jing H, et al 2004 Centroid-based summa-
rization of multiple documents. Information Process-
ing & Management, 2004, 40(6): 919-938.
Radev D, Winkel A, Topper M. 2002 Multi document
centroid-based text summarization. ACL Demo Ses-
sion, 2002.
Shahaf D, Guestrin C. 2010 Connecting the dots be-
tween news articles. Proceedings of the 16th ACM
Conference on SIGKDD, ACM, pages 623-632.
Shahaf D, Guestrin C, Horvitz E. 2012 Trains of
thought: Generating information maps. Proceedings
of the 21st International Conference on World Wide
Web, ACM, pages 899-908.
Shannon C E. 2001 A mathematical theory of commu-
nication. ACM SIGMOBILE Mobile Computing and
Communications Review, 2001, 5(1): 3-55.
Wu Y, Chang E Y, Chang K C C, et al 2004 Optimal
multimodal fusion for multimedia data analysis. Pro-
ceedings of the 12th annual ACM International Con-
ference on Multimedia, ACM, 2004: 572-579.
Xu S, Kong L, Zhang Y. 2013 A cross-media evolution-
ary timeline generation framework based on iterative
recommendation. Proceedings of the 3rd ACM confer-
ence on International Conference on Multimedia Re-
trieval, ACM, pages 73-80.
Yan R, Wan X, Otterbacher J, et al 2011 Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. Proceedings of
the 34th International ACM Conference on SIGIR,
ACM, pages 745-754.
1291
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070?1080,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Tailor knowledge graph for query understanding: linking intent topics by
propagation
Shi Zhao
z.s@pku.edu.cn
Yan Zhang
zhy@cis.pku.edu.cn
Department of Machine Intelligence, Peking University, Beijing, China
Key Laboratory on Machine Perception, Ministry of Education, Beijing, China
Abstract
Knowledge graphs are recently used for
enriching query representations in an
entity-aware way for the rich facts or-
ganized around entities in it. How-
ever, few of the methods pay attention to
non-entity words and clicked websites in
queries, which also help conveying user
intent. In this paper, we tackle the prob-
lem of intent understanding with innova-
tively representing entity words, refiners
and clicked urls as intent topics in a uni-
fied knowledge graph based framework,
in a way to exploit and expand knowl-
edge graph which we call ?tailor?. We
collaboratively exploit global knowledge
in knowledge graphs and local contexts in
query log to initialize intent representa-
tion, then propagate the enriched features
in a graph consisting of intent topics us-
ing an unsupervised algorithm. The ex-
periments prove intent topics with knowl-
edge graph enriched features significantly
enhance intent understanding.
1 Introduction
Query understanding is the process of generating a
representation which characterizes a user?s search
intent (Croft et al., 2010), which is of vital im-
portance for information retrieval. However, users
are remarkably laconic in describing their infor-
mation needs due to anomalous state of knowledge
(Belkin et al., 1982), resulting in vague and under-
specified queries, which makes it especially dif-
ficult to understand and locate what they intended
for in mountains of web data. The problem is often
significantly compounded that people convey their
intent rather in a series of behaviors called a search
session than a single query, leaving a wealth of
clues including query reformulations, page visits,
dwell times, etc. What?s more, as entities are tak-
ing center stage (Yin and Shah, 2010), string-level
or phrase-level modeling of intent soon hits the
bottleneck, calling for an entity-aware perspective.
Knowledge repositories, better known as
knowledge graphs, such as Wikipedia, DBpedia
and Freebase, have been recently utilized for en-
hancing query understanding for the large amounts
of world knowledge they?ve harvested about en-
tities and facts. A widely accepted way to use
knowledge graph is tying queries with it by anno-
tating entities in them, also known as entity link-
ing.
However, information need is conveyed through
more than entities. Quite a few non-entity words,
aka refiners or modifiers, as well as many urls are
barely included in knowledge graph, while they
play an irreplaceable role in intent understand-
ing. For example, a user may query toyota, volvo
or just enter car soup, cars for sale and click
www.carsoup.com, which should be encoded in
a form that we could perceive their closeness in
intent. That?s why at-a-glance info cards about
merely recognized entity in the query are far from
enough and previous methods disregarding refin-
ers and urls are too limited to cover queries in ma-
jority.
We move one step further to tailor knowledge
graph for representing more than entity words. We
collect refiners and clicked urls along with en-
tity words and model intents they represent us-
ing knowledge graph based features. We use
Freebase
1
, one of the largest available knowledge
graph, in our work and our method can be easily
generalized to other knowledge repositories.
We put up an idea of intent topic which can
be query words or urls, whether mean an entity
or not, representing an atomic information need.
We identify them with intent features by exploit-
ing global knowledge in Freebase and local con-
1
http://www.freebase.com
1070
texts in query sessions. Notice the new concept
here is distinguished from query intent or query
facet in previous literature for it is in a holistic
view, not specifically meaning subtopics around a
certain query.
Our intuitive observations as follows inspire us
to represent intent features with topics and do-
mains in knowledge graph and propagate the en-
riched features in the intent topic graph.
1) Query words and urls within the same session
tend to indicate the same query intent.
2) Intent topics sharing similar query intent often
relate to similar topics in knowledge graph.
3) Knowledge graph domains sketch the query in-
tent briefly.
Observation 1 indicates domain coherency
within sessions is a good starting point to gener-
ate intent features, along with Observation 2 and 3
lay the basis of proximity that the propagation rely
on.
To the best of our knowledge, we?re the first to
represent intent behind entity words, refiners and
urls in a unified knowledge graph based frame-
work, in a way to exploit and expand knowledge
graph which we call ?tailor?.
Our contributions include:
? An innovative and unified framework to rep-
resent intent topics, whether they can directly
link to an entity in knowledge graph or not.
? A novel algorithm to generate a specified in-
tent topic graph, which enables learning in-
tent features in an unsupervised propagation
method.
? With intent topic graph we can better under-
stand user intent conducting session-based
contextualization and potentially find highly-
related intent topic.
The rest of the paper is organized as follows. Sec-
tion 2 tells our methods to map queries to Freebase
and initialize intent features. Section 3 is about
how we model intent topics in a unified graph and
the propagation framework to learn intent features.
We provide experiments and analysis in Section 4.
Related work and conclusions are presented at the
end of the paper.
2 Labeling intent topic nodes with
Freebase-enriched features
In Freebase, facts around a certain topic and multi-
faceted intents they reflect is more like a global
domain distribution, what facet do users exactly
intend for is difficult to locate until in a specified
context, namely a query session.
We take a line in query log as a query, exhibit-
ing an interaction with the search engine, includ-
ing query words and page clicks. And a sequence
of queries with a certain time interval constitute
a session, completely conveying an information
need.
In existing knowledge graph, only a small part
of urls are contained in views of web pages be-
yond number online. Even for query words, we
can merely get access to some of them, which we
call entity words and the rest refiners. To avoid
misunderstanding, the url intent topics in the fol-
lowing will specially refer to the clicks without di-
rectly matched concepts in knowledge graph, oth-
erwise they?ll be taken as entity intent topic.
In this section, we propose a framework of
knowledge graph enriched representation of in-
tent topics, the following propagation in Section3
bases on it.
2.1 Freebase as a knowledge graph
Freebase has over 39 million concepts, aka top-
ics, about real-world entities like people, places
and things stored as nodes in a graph. They?re
linked to each other with annotated egdes named
as property. These edges actually represent facts.
There are over a billion such facts or relations that
make up the graph and they?re all available for
free. Properties are grouped into types, types are
grouped into domains, which gives a broad view
of knowledge in addtion to specific topics.
We can tap into Freebase through dump data or
API
2
. In our work, we retrieve related Freebase
topics with relevance scores for entity words via
Freebase search API, which is based on combina-
tion of topic?s inbound and outbound link counts
in Freebase and Wikipedia as well as a popularity
score computed by Google, and all the facts about
a given topic through Freebase topic API. We use
T = {t
1
, t
2
, ...t
n
}, D = {d
1
, d
2
, ...d
n
} to denote
all Freebase topics and domains used in our work.
2.2 Enriching entities and queries with
Freebase
We represent a query?s candidate intent topics by
three sets, E
q
, R
q
, C
q
, where E
q
includes entity
words and clicks which have equivalents in Free-
base, R
q
the refiner words and C
q
the rest clicks.
2
http://developers.google.com/freebase/
1071
Global knowledge in Freebase can directly enrich
each e in E
q
with Freebase topics represented in
vector t
e
, for each candidate topic there?s a Free-
base domain distribution vector d
t
. As for the rest
inR
q
and C
q
, they can learn features in later prop-
agation process.
For any topic t
i
in t
e
, the relevance of entity
words e and knowledge graph topic t
i
is estimated
as follows:
t
e
i
=
RelevanceScore(e, t
i
)
max
t
j
?T
RelevanceScore(e, t
j
)
(1)
And the domain vector d
t
i
for t
i
is:
d
t
i
j
=
pr(d
j
|t
i
)
?
d
k
?D
pr(d
k
|t
i
)
(2)
pr(d
j
|t
i
) =
# of links of t
i
in domain d
j
# of all links in domain d
j
(3)
Then we?ll get a knowledge graph enriched in-
tent description of the query by combining that of
e, r, c.
t
q
i
=
?
e?E
q
t
e
i
w
q
(e) +
?
r?R
q
t
r
i
w
q
(r) +
?
c?C
q
t
c
i
w
q
(c)
(4)
w
q
(e) = NCount
q
(e)?(e) (5)
Here t
e
t
r
t
c
correspond to the topic vector of each
entity, refiner and click respectively. The weight
indicates how dominant it is in conveying intent
in the query. It is in proportion to the normalized
count as well as each occurrence?s quality denoted
by ?(e). Such as for entity words in Equation (5),
the quality ?(e) can be estimated with the help of
entity linking methods, which describes the proba-
bility of e as a candidate reference. That for clicks
and refiners will be explained later.
The query?s domain feature can be calculated as
follows:
d
q
i
=
?
t
j
?T
d
t
j
i
t
q
j
?
d
k
?D
?
t
j
?T
d
t
j
k
t
q
j
(6)
It describes the probability of query q in domain
d
i
, in which t
q
j
can be calculated by Equation (4)
and d
t
j
i
via facts around topic t
j
by Equation (2).
2.3 Contextualized intent depiction of
sessions
The aforesaid enriched features we get about
queries rely heavily on global knowledge in Free-
base, reflecting prior distribution in the feature
space. In this part, we derive a contextualized de-
scription of session intent in a local view by aggre-
gating all the global knowledge we get about the
session?s queries. The ambiguity of a single query
can be alleviated by looking at the dominant do-
main within the session.
The intent features t
s
and d
s
of session s can
be represented by computations on its query set
Q
s
= {q
1
, q
2
, ...q
n
} with time-order decay.
t
s
i
=
?
q?Q
s
t
q
i
?
rank(q)
?
t
j
?T
?
q?Q
s
t
q
j
?
rank(q)
(7)
where we put an exponential decay controlled by
decay factor ?. We get domain feature the same
way as Equation (6).
We?ll put up an unsupervised method of learn-
ing knowledge graph based intent representation
of refiners and clicks in the following part.
3 Propagating intent features in the
intent topic graph
In this section, our idea is to characterize entities,
refiners and urls uniformly as intent topics, tailor-
ing knowledge graph to intent topic graph so as to
enrich representations by propagation.
3.1 Modeling intent topic graph
As in last section, with d
s
featuring the context,
candidate intent topics in sessions can make intent
topic nodes now. We use the concept intent topic
to stress words with local contexts tell a specified
information need, thus making a node. Taking en-
tity word fl as an example, it can be recognized
as the topic Florida in Freebase, while the intent
behind it can hardly be mapped to a single intent
topic, such as travel domain in hollywood fl, ed-
ucation domain in community college in florida,
and florida department of health actually convey
intent in government domain.
So each intent topic node is identified with its
name string and Freebase-enriched intent features
t and d. They?re directly linked by co-occurring
in the same line in the query log and implicitly
related via intent features similarities, so that con-
stitute a large graph G =< V,E,W >, where
?w ? W denotes an explicit edge weight and
?v ? V an intent topic. With intent topics and
their relations modeled in a graph, we can better
understand the query space so as to find the in-
tended query faster. We realize it by aggregating
massive sessions.
1072
The implicit intent similarity ISim of any node
pair n and v can be encoded as follows.
ISim
n,v
= ?SSim
n,v
+?DSim
n,v
+?TSim
n,v
(8)
where SSim denotes the names? string similar-
ity, DSim the similarity of their domain feature
and TSim the topic vector similarity, with ?, ?
and ? controlling the weight. The parameters may
vary due to different scenarios. We just provide
a framework of modeling nodes? intent features,
which actually mirror their proximity in query in-
tent.
To put it in more details, we use jaccard sim-
ilarity for name shinglings and cosine similarity
for domain and topic vector. As query log in-
duced intent topic graph is of considerable large
size, the pair-wise similarity is computationally
prohibitive, hence we use Local Sensitive Hash
(Indyk and Motwani, 1998) for each similarity
metric so as to compute ISim just in candidate
set. We use random hyperplane based hash fam-
ily proposed in (Charikar, 2002) and set the hash
code dimension and hash table numbers empiri-
cally to ensure the number of nodes falling into
each bucket is relatively stable.
3.2 Merging nodes
Although our idea of specifying intent topics by
context better models the multi-facets of queries,
it obviously also brings a sparse issue. For exam-
ple, in one session user query beep lyrics and click
www.lyricsandsongs.com, lyrics is tagged with the
song beep and the musician Pussycat Dolls, in an-
other scenario lyrics occurs with the song what
you know and url www.dapslyrics.com, intents be-
hind these two nodes are so similar that they
should come into one, otherwise connections be-
tween the two intent-coherent urls may be lost.
To avoid that, we conduct a merge process to
integrate nodes with exactly the same names and
contexts into one, combing linked nodes and intent
features together.
For a set of nearly duplicate nodes ? the cal-
culation of new node?s features can be written as:
?
t =
?
u??
t
u
|?|
(9)
?
d =
?
u??
d
u
|?|
(10)
In other words, we gather candidate nodes re-
trieved by LSH and then calculate ISim for them
with ? setting to 0. Only node pairs with ISim
higher than a merge threshold ? can be seen as
duplicates. The merge process is summarized in
Algorithm 1.
Algorithm 1: Merging similar nodes
Input: G =< V,E,W >, ?, ?, ?, ?
Output:
?
G =<
?
V ,
?
E,
?
W >
begin
Initialize ?? ?
for v ? V do
Find dupset ?
v
with ISim
?,?,?
if ?u ? V, ?
u
? ? and ?
v
? ?
u
6= ?
then
?
v
? ?
v
? ?
u
Remove ?
u
from ?
Add ?
v
to ?
for ? ? ? do
Merge nodes in ? into new node v?
Update G with replacing nodes in ?
with v?
3.3 Label propagation
We utilize knowledge graph induced intent fea-
tures instead of manually labels as constraints to
conduct label propagation(Zhu and Ghahramani,
2002). The idea is that node labels are propa-
gated to nearby nodes via weighted edges until
convergence, as highly weighted edges indicate
high probability of sharing labels.
Nodes in our work have soft labels, where each
dimension of intent features denotes a label, such
as a topic or domain of knowledge graph. As de-
scribed in aforesaid observations, it is intuitively
reasonable to propagate on the basis of explicit
edges and implicit intent similarities. We illustrate
the propagation with topic feature, that of domain
feature is similar.
We use matrix Y
t
? R
|V |?|T|
to denote the in-
tent topic graph?s initial topic feature labels, with
element Y
t
ik
indicating node v
i
?s relevance to t
k
,
wherer t
k
? T. Y
t
is initialized based on the
results of the feature enriching step in Section 2,
with no manually-labelled instances needed in our
model. As only part of nodes can directly map
to Freebase topics, those are initialized as labelled
nodes, then propagate t to their linked neighbors.
The number of unlabelled data is written as u,
while that of labelled data l and the total number
of nodes N .
1073
The transition matrix T indicates the impact of
nodes on each other. Note that here the w
ij
can
be replaced by other similarity measures such as
ISim in Section 3.2.
T
ij
=
w
ij
?
N
k=1
w
kj
(11)
LetD denote anN?N diagonal matrix with d
ii
=
?
j
T
ij
. Then we can get a normalized version of
transition matrix P = D
?1
T .
The normalized transition matrix can be split
into 4 sub-matrices.
P =
[
P
ll
P
lu
P
ul
P
uu
]
(12)
At each step, we propagate and clamp the labelled
data and repeat until Y converges, the propagation
step can be written as:
?
Y
u
= P
uu
Y
u
+ P
ul
Y
l
(13)
As is shown in (Zhu and Ghahramani, 2002; Zhu
et al., 2003) the solution to the propagation con-
verges to:
?
Y
u
= (I ? P
uu
)
?1
P
ul
Y
l
(14)
3.4 The propagation framework for intent
features
We carry the propagation in an iterative process
illustrated in Algorithm 2.
Algorithm 2: Intent feature propagation
Input: G, Y
t
l
,Y
d
l
Output:
?
G,
?
Y
t
u
,
?
Y
d
u
Initialize Y
t
l
Y
d
with results of Section2
repeat
Merge similar nodes according to
Algorithm 1
Compute matrix P
repeat
?
Y
t
u
= P
uu
Y
t
u
+ P
ul
Y
t
l
until Convergence;
Recompute
?
P with
?
Y
t
repeat
?
Y
d
u
=
?
P
uu
Y
d
u
+
?
P
ul
Y
d
l
until Convergence;
until no dups;
Since intent features include both domain vec-
tor and topic vector, we propagate them in an alter-
nating way. At first we label nodes as described in
Section 2, though missing refiners? and some urls?
intent features, they are just used for initialization.
Then we propagate Freebase topic features based
on explicit edge weights, so that more nodes in
intent topic graph have topic features now. Then
fetching the learned topic features, we reinput it
into domain feature propagation, which means we
recalculate the transition matrix combining the im-
plicit learned TSim into edge weight, then prop-
agate domain vector of labelled nodes through the
graph. At each iteration, we first update Y
t
, then
input it to update Y
d
, therefore merge near dupli-
cate intent topics to update the whole graph.
4 Experiments
4.1 Data preparation
4.1.1 Search logs
We use AOL search log data for experiments. It
includes 20 million web queries collected covering
500K users over three months in 2006.
Table 1: The query set
# of sessions 35140
# of queries 271127
# of users 21378
# of urls 63019
We preprocess the query log by keeping urls oc-
curring more than 3 times and queries with 2 to
40 characters, then extract sessions considering 25
minutes duration. While user session segmenta-
tion can be improved with more sophisticated al-
gorithms, this simple low-cost heuristic performs
adequately for our purposes. We then move on to
map queries to Freebase and empirically filter ses-
sions that are less entity-centric. We use an anno-
tation tool especially for short text (Ferragina and
Scaiella, 2012) called Tagme
3
to recognize entities
and observe only 16% of all the queries are ex-
actly an entity itself, which means most of queries
do have refiner words to convey information need.
To ensure the precision of recognized entities, we
set a significant threshold and bottom line thresh-
old , queries should have at least one recognized
entity with a likelihood above significant level,
and those below bottom line are ignored. They
are 0.19 and 0.05 in our work, which may vary
with entity recognition method. The normalized
3
http://tagme.di.unipi.it/
1074
loca
?on
	 ?
orga
niza
?on
	 ?
busi
ness
	 ?
boo
k	 ?
inte
rnet
	 ?
trav
el	 ? film
	 ?
educ
a?o
n	 ?
mus
ic	 ?
spor
ts	 ?
gove
rnm
ent	 ?
broa
dcas
t	 ?
peo
ple	 ?
com
pute
r	 ? tv	 ?
avia
?on
	 ?
cele
bri?s	 ? med
icine
	 ?
fic?o
nal_
univ
erse
	 ?
peri
odic
als	 ?
biolo
gy	 ?
arch
itect
ure	 ? cvg	 ?
med
ia_c
omm
on	 ?
visu
al_a
rt	 ?
milit
ary	 ?
auto
mo?
ve	 ?
influ
ence
	 ?
food
	 ?
per
cen
tag
e 
Query	 ?set	 ?
Test	 ?session	 ?set	 ?
Freebase	 ?topics	 ?
Freebase	 ?facts	 ?
Figure 1: Unbalanced domain distributions in Freebase comparing against query set. Only domains with
top proportions are shown.
Table 2: Examples of labelled intent topic nodes with learned feature
Intent topic nodes Original in Freebase After propagation Annotation
travel.yahoo.com Yahoo! Travel
(internet, 0.87),
(projects, 0.13)
(location, 0.13),
(travel, 0.11),
(organization, 0.08),
(business, 0.08) ...
Yahoo! Travel offers
travel guides, booking
and reservation services.
map quest
www.mapquest.com
MapQuest
(organization, 0.6),
(book, 0.4)
(location, 0.13),
(organization, 0.09),
(travel, 0.09),
(automotive, 0.06)...
MapQuest is an Ameri-
can free online web map-
ping service.
likelihood is used as w
q
(e). Then we drop ses-
sions where tagged entity words weight less than
refiners as well as the ones with too many entity
words spotted indicating disperse intents. For each
recognized entity, only Freebase topics with rele-
vance over 0.3 are kept. The query set we finally
get is shown in Table 1.
4.1.2 Freebase
To enrich query representations, we collect a sub-
set of Freebase including more than 7 millions
facts and 4 millions topics in total which also
contain 150 thousand topical equivalent websites,
though less than 3% urls in query set are covered.
The facts and entities in Freebase is rather un-
balanced across domains especially against that of
recoginized entities in query set as shown in Fig-
ure 1. Thus the original global knowledge we use
about domain distribution may cause bias, which
makes tailoring necessary for intent understand-
ing.
For both generality and precision, we keep most
of Freebase domains except several extreme in-
complete ones, instead of retaining a small number
of representative domains like many researchers
do (Li et al., 2013; Yu et al., 2014; Lin et al.,
2012). But generality comes at a price that some
domains are confusing and mixed used which we
then choose to merge, like celebrities and people,
periodicals and books, tv and broadcast, etc. We
finally keep 50 of all 76 domains.
4.2 Intent topic graph
4.2.1 Building the graph
We leverage both Freebase and search sessions to
enrich intent topics. We set ? to 0.9 in calcula-
tion of session?s intent features. After labeling
the session log, we roughly make a graph with
335206 intent topic nodes, 119364 of them have
been labelled with Freebase topic feature, others
only have domain feature. Then we conduct a
merge process with ? set to 0.7, ? to 0.3 and ? to
0.75 in order to merge nodes with duplicate names
and similar contexts. We find 46659 duplicate sets
covering 140768 nodes. Then we ignore nodes
with few links and rare names to reduce sparsity.
Finally we?ve got a graph of 209351 intent topics
to initialize the propagation, including 78932 la-
belled nodes. The merge and propagation progress
get converged in less than 4 rounds.
We?ll further evaluate the graph with case study
and a session intent understanding task.
4.2.2 Case Study
We demonstrate intent features are good interpre-
tations for query intent, whether they?re labelled in
Section 2 or learned by propagation in Section 3.
We can see in Table 2 that as nodes? original
1075
Table 3: Examples of unlabelled intent topic nodes with learned feature
Intent topic node intent features Annotation Similarity nodes
www.bnm.com (The Hertz Corporation, 0.25), (South-
west Florida International Airport,
0.17), (Punta Gorda Airport, 0.13),
(Supercar, 0.09), (Sports car, 0.08)...
(aviation, 0.23), (business, 0.21), (lo-
cation, 0.14), (automotive, 0.11)...
Online booking
of discount
rentals at ma-
jor airports,
worldwide.
www.arac.com
www.rentalcars.com
www.hertz.com
www.alamo.com
rent a car
cheap rental cars
www.mobtime.com (Software, 0.18), (Mobile phone,
0.11), (100% Totally Free Ringtones,
0.10), (Motorola, 0.09), (Free Cell,
0.08), (Verizon Wireless, 0.04)...
(computer, 0.23), (cvg, 0.21), (music,
0.19), (business, 0.11) ...
MobTime Cell
Phone Manager
is a PC soft-
ware to manage
or sync mobile
phones.
cellphones.about.com
cell software
cell to pc
reviews of
cellphone wallpaper
types in Freebase are not proper for describing in-
tent, the intent features they get after propagation
tend to be more explainable, such as the travel
site often co-occurs with city names, tourist attrac-
tions, hotels and so on, thus indicating its intent in
travel and location domain.
Table 3 shows examples which have no equiv-
alents in Freebase. Although some of them may
be accessible in other ontologies, we only take
them as examples to show our propagation method
makes it possible to depict intents behind urls and
words in a knowledge graph based way while be-
yond the capacity of knowledge graph.
4.3 Session intent understanding task
4.3.1 Experiment Setup
The evaluation of query understanding has long
been a challenging task. To judge whether the con-
cepts in query are successfully recognized seems
too straightforward, and it can hardly be consid-
ered understanding the intent until the big idea
about what kind of topics users emphasis is cap-
tured, which can be briefly sketched by distribu-
tion across Freebase domains. Also it is difficult to
translate results of previous log analysis methods
into knowledge graph domain information, thus
hardly fit into our evaluation schema. We take
popularity-based method as baseline.
We have few choices but to tag ground truth our-
selves for intent understanding evaluation.
We randomly select 150 sessions as test set,
the domain distribution of which agrees with the
whole query set as shown in Figure 1. As mas-
tering meanings of all Freebase domains is too
challenging, we ask 5 accessors to describe each
session?s intent broadly with a few natural lan-
guage terms, then an expert familiar with Freebase
schema translates the words into matched Free-
base domains. Each test session is tagged by 2
accessors and 1 expert, we choose to use the tags
of the cases in which the accessors reached agree-
ment as the gold stantard. For example, if acces-
sors tag session intent as pictures, then experts can
translate it into Freebase visual art domain. Each
session has 1?4 tags and 1.6 tags in average. The
tags cover 30 domains.
For each session, we derive the local intent do-
main vector d
s
following the method in Section 2.
Here we simply set quality function ?(r) to a con-
stant ?
r
for all refiners and?(c) to ?
c
for all clicks,
we?ll dive into more specialized weighting method
in future work. ?
r
and ?
c
are parameters to control
impact of different kinds of intent topics. Based on
whether to exploit global intent features of non-
entity words, we compare four variations against
one baseline.
? Popularity-based (GP). We use domains? fre-
quency in the query set as a baseline.
? Entity-based (E). We only use entity nodes?
original intent features without propagation.
? Entity+Clicks (EC). Both intent features of
entity words and clicks are used, controlled
with ?
c
.
? Entity+Refiners (ER). Intent features of en-
tity words and refiner words are used, refin-
ers? impact is controlled by ?
r
.
? Entity+Clicks+Refiners (ECR). All intent
topics are combined, controlled by ?
c
, ?
r
.
1076
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.672
0.678
0.684
0.690
0.696
0.702
0.708
0.714
0.720
(a) MAP@5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.18
0.21
0.24
0.27
0.30
0.33
0.36
0.39
(b) GMAP@5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.708
0.714
0.720
0.726
0.732
0.738
0.744
0.750
0.756
(c) MAP@10
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.375
0.400
0.425
0.450
0.475
0.500
0.525
0.550
0.575
(d) GMAP@10
Figure 2: The impact of ?
r
and ?
c
on ECR methods in four metrics, with vertical axis indicating ?
r
,
horizontal axis as ?
c
. The first column on the left denotes ER method, while the bottom row the EC
method.
4.3.2 Evaluation metrics
We use each approach to rank domains accord-
ing to its derived weight, then compare it with
golden standard set. It can be evaluated using
Mean Average Precision (MAP), Geometric MAP
and Precision@K. We use GMAP because it is
more robust to outliers than the arithmetic mean.
For test set of size N , the MAP and GMAP can
be calculated as follows:
MAP@k =
1
N
N
?
i=1
AP
i
@k (15)
GMAP@k =
N
?
?
?
?
N
?
i=1
AP
i
@k (16)
4.3.3 Results and analysis
We first study impact of parameters ?
r
and ?
c
,
which is shown in Figure 2.
It roughly demonstrates different combinations
of parameters? impact on ECR methods, perfor-
mance is evaluated in four metrics, with deeper
color indicating better result.
Best results comes with a ?
c
larger than ?
r
in
all four subfigures. This trend seems more obvi-
ous in (d) where right part with larger ?
c
get better
results. Also, deeper colors around diagonal line
in (a) (c) indicate a more balanced combination
of refiners and urls are more likely to enhance in-
tent understanding. Thus we conclude clicks has a
weak advantage over refiners in improving the re-
sult, while combining both with proper parameters
can get the best result.
When comparing between MAP and GMAP, we
can see while GMAP stays a high value when am-
plifying the impact of clicks, MAP changes with
the variation of ?
r
for better or worse. As GMAP
is a more robust metric, we can then infer that in-
creasing weight of refiners could bring more out-
liers, implying refiners? intent features are more
susceptible to noise.
Then we use ER with ?
r
= 0.5 as ER
opt
, EC
with ?
c
= 0.5 as EC
opt
and ECR with ?
r
=
0.2, ?
c
= 0.5 as ECR
opt
.
Figure 3 clearly shows the superior performance
of our model, especially at top positions. Table 4
shows the detailed comparisons between different
methods. We can see our knowledge graph based
intent representations perform well in session in-
tent understanding. And refiners? and clicks? in-
tent features which we learn by propagation con-
1077
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
1	 ? 3	 ? 5	 ? 10	 ? 20	 ? 50	 ?
Prec
ision
@K 
K 
GL	 ?
E	 ?
ER	 ?
EC	 ?
ECR	 ?
Figure 3: Precision@K results for different ap-
proaches, by varying number of k
Table 4: Comparisons among different methods
K=5 K=10
MAP GMAP MAP GMAP
GP 0.177 0.000 0.232 0.002
E 0.676 0.166 0.707 0.355
EC
opt
0.708 0.412 0.739 0.579
ER
opt
0.688 0.227 0.723 0.421
ECR
opt
0.722 0.412 0.756 0.594
tribute a lot to improve naive entity-based method,
which do validate an complment effect of their
learned intent features.
5 Related Work
5.1 Query intent understanding
Query intent or search intent has been studied in-
tensively from various views.
A popular paradigm is to label several intents
for each query, also called facets subgoals and
subtopics in the literature, manully or by min-
ing methods and then do classification (Hu et al.,
2009; Li et al., 2008) based on that. Manually in-
tent schemas range from 3 top level (Broder, 2002)
to fine-grained subcatogories (Rose and Levinson,
2004) and taxonomy (Yin and Shah, 2010). Intent
tasks in NTCIR-10 (Sakai et al., 2013) also pro-
vide subtopic pools made by accessors.
Another view of intent is more generic, min-
ing or learning search intents without any kind of
pre-defined intent category and clustering method
is often used. Methods including (Sadikov et al.,
2010; Yamamoto et al., 2012; Cheung and Li,
2012) cast intent as represented by a pattern or
template consisting of a sequence of semantic con-
cepts or lexical items. (Tan et al., 2012) encode
intent in language models, aware of long-lasting
interests. (Ren et al., 2014) uses an unsupervised
heterogeneous clustering. (Yin and Shah, 2010)
capture generic intents around a certain named en-
tities and model their relationships in a tree tax-
onomy and (Wang et al., 2009) mine broad latent
modifiers of intent aspect , which are similar to
our motivation, while we model more than intent
phrases, but intent topics. We do not split queries
into clusters or subtopics relevant to the original
query to indicate a intent, but link them in an graph
with intent feature similarity, weakly or strongly,
in a holistical view.
On the other hand, previous research can be
categorized by what kind of resources they rely
on. Quite an amount of work leverage query logs
(Jiang et al., 2013), including query reformula-
tions (Radlinski et al., 2010), click-through data
(Li et al., 2008). There are also works using spon-
sered data (Yamamoto et al., 2012) and interactive
data (Ruotsalo et al., 2013). The new trend of in-
tegrating knowledge graph will be discussed next.
5.2 Knowledge graph on intent
understanding
Instead of summarizing queries into concepts by
clustering, recently there appears a tendency to use
concpets from knowledge graph resources. Some
researchers manage to build entity graph from
queries (Bordino et al., 2013a) (Bordino et al.,
2013b; Yu et al., 2014), some in a structure view,
interpret quries into knowledge base fit template
(Pound et al., 2012; Li et al., 2013). (Pantel et al.,
2012) models latent intent to mine entity type dis-
tributions. (Ren et al., 2014) utilizes knowledge
graph resources in a hetrogeneous view. (Lin et
al., 2012) also pays attention to refiners, but re-
stricted to limited domains, while our method is
more general.
6 Conclusion
In this paper, we tailor knowledge graph to rep-
resent query intent behind entity words, refiners
and clicked urls in a unified framework, taking
them as intent topic nodes connected in a large
graph. We manage to get a contextualized intent
depiction exploiting global knowledge in Free-
base, then propagate the feature to cover more in-
tent topics. We show in experiments the knowl-
edge graph enriched representation is reasonable
and explainable, and the intents feature of refiners
and clicks can better enhance intent understanding
than methods simply relying on entities.
1078
There are several directions for future work, in-
cluding using both types and domains in Free-
base schema, diving into refiners and looking for a
proper weighting method, developing a query rec-
ommendation framework based on the intent topic
graph and user interest modeling.
Acknowledgments
We sincerely thank all the anonymous reviewers
for their valuable comments, which have helped to
improve this paper greatly. This work is supported
by NSFC with Grant No.61370054, and 973 Pro-
gram with Grant No.2014CB340405.
References
Nicholas J Belkin, Robert N Oddy, and Helen M
Brooks. 1982. Ask for information retrieval: Part i.
background and theory. Journal of documentation,
38(2):61?71.
Ilaria Bordino, Gianmarco De Francisci Morales, Ing-
mar Weber, and Francesco Bonchi. 2013a. From
machu picchu to rafting the urubamba river: antici-
pating information needs via the entity-query graph.
In Proceedings of the sixth ACM international con-
ference on Web search and data mining, pages 275?
284. ACM.
Ilaria Bordino, Yelena Mejova, and Mounia Lalmas.
2013b. Penguins in sweaters, or serendipitous entity
search on user-generated content. In Proceedings
of the 22nd ACM international conference on Con-
ference on information & knowledge management,
pages 109?118. ACM.
Andrei Broder. 2002. A taxonomy of web search. SI-
GIR Forum, 36(2):3?10, September.
Moses S Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the thiry-fourth annual ACM symposium on Theory
of computing, pages 380?388. ACM.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383?392. ACM.
W Bruce Croft, Michael Bendersky, Hang Li, and
Gu Xu. 2010. Query representation and understand-
ing workshop. In SIGIR Forum, volume 44, pages
48?53.
Paolo Ferragina and Ugo Scaiella. 2012. Fast and
accurate annotation of short texts with wikipedia
pages. IEEE software, 29(1).
Jian Hu, Gang Wang, Fred Lochovsky, Jian-tao Sun,
and Zheng Chen. 2009. Understanding user?s query
intent with wikipedia. In Proceedings of the 18th
international conference on World wide web, pages
471?480. ACM.
Piotr Indyk and Rajeev Motwani. 1998. Approxi-
mate nearest neighbors: towards removing the curse
of dimensionality. In Proceedings of the thirtieth
annual ACM symposium on Theory of computing,
pages 604?613. ACM.
Daxin Jiang, Jian Pei, and Hang Li. 2013. Mining
search and browse logs for web search: A survey.
ACM Trans. Intell. Syst. Technol., 4(4):57:1?57:37,
October.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?08, pages 339?346,
New York, NY, USA. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information &#38; knowledge
management, CIKM ?13, pages 2029?2038, New
York, NY, USA. ACM.
Thomas Lin, Patrick Pantel, Michael Gamon, Anitha
Kannan, and Ariel Fuxman. 2012. Active objects:
Actions for entity-centric search. In Proceedings
of the 21st international conference on World Wide
Web, pages 589?598. ACM.
Patrick Pantel, Thomas Lin, and Michael Gamon.
2012. Mining entity types from query logs via user
intent modeling. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 563?571.
Association for Computational Linguistics.
Jeffrey Pound, Alexander K Hudek, Ihab F Ilyas, and
Grant Weddell. 2012. Interpreting keyword queries
over web knowledge bases. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 305?314. ACM.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations
and clicks. In Proceedings of the 19th international
conference on World wide web, pages 1171?1172.
ACM.
Xiang Ren, Yujing Wang, Xiao Yu, Jun Yan, Zheng
Chen, and Jiawei Han. 2014. Heterogeneous graph-
based intent learning with queries, web pages and
wikipedia concepts. In Proceedings of the 7th ACM
International Conference on Web Search and Data
Mining, WSDM ?14, pages 23?32, New York, NY,
USA. ACM.
Daniel E. Rose and Danny Levinson. 2004. Un-
derstanding user goals in web search. In Proceed-
ings of the 13th International Conference on World
1079
Wide Web, WWW ?04, pages 13?19, New York, NY,
USA. ACM.
Tuukka Ruotsalo, Jaakko Peltonen, Manuel Eugster,
Dorota G?owacka, Ksenia Konyushkova, Kumari-
paba Athukorala, Ilkka Kosunen, Aki Reijonen,
Petri Myllym?aki, Giulio Jacucci, et al. 2013. Di-
recting exploratory search with interactive intent
modeling. In Proceedings of the 22nd ACM interna-
tional conference on Conference on information &
knowledge management, pages 1759?1764. ACM.
Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon
Halevy. 2010. Clustering query refinements by user
intent. In Proceedings of the 19th international con-
ference on World wide web, pages 841?850. ACM.
Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto,
Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and
M Iwata. 2013. Overview of the ntcir-10 intent-2
task. Proceedings of NTCIR-10, pages 94?123.
Bin Tan, Yuanhua Lv, and ChengXiang Zhai. 2012.
Mining long-lasting exploratory user interests from
search history. In Proceedings of the 21st ACM in-
ternational conference on Information and knowl-
edge management, pages 1477?1481. ACM.
Xuanhui Wang, Deepayan Chakrabarti, and Kunal
Punera. 2009. Mining broad latent query aspects
from search sessions. In Proceedings of the 15th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 867?876.
ACM.
Takehiro Yamamoto, Tetsuya Sakai, Mayu Iwata, Chen
Yu, Ji-Rong Wen, and Katsumi Tanaka. 2012. The
wisdom of advertisers: mining subgoals via query
clustering. In Proceedings of the 21st ACM inter-
national conference on Information and knowledge
management, pages 505?514. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries.
In Proceedings of the 19th international conference
on World wide web, pages 1001?1010. ACM.
Xiao Yu, Hao Ma, Bo-June (Paul) Hsu, and Jiawei Han.
2014. On building entity recommender systems us-
ing user click log and freebase knowledge. In Pro-
ceedings of the 7th ACM International Conference
on Web Search and Data Mining, WSDM ?14, pages
263?272, New York, NY, USA. ACM.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
Xiaojin Zhu, Zoubin Ghahramani, John Lafferty, et al.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In ICML, volume 3,
pages 912?919.
1080
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 66?70,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Combining Syntactic and Semantic Features by SVM for Unrestricted
Coreference Resolution
Huiwei Zhou1, Yao Li2, Degen Huang3, Yan Zhang4, Chunlong Wu5, Yuansheng Yang6
Dalian University of Technology
Dalian, Liaoning, China
{1zhouhuiwei,3huangdg,6yangys}@dlut.edu.cn
2tianshanyao@mail.dlut.edu.cn
4zhangyan zyzy@yeah.net
5wuchunlong@gmail.com
Abstract
The paper presents a system for the CoNLL-
2011 share task of coreference resolution. The
system composes of two components: one for
mentions detection and another one for their
coreference resolution. For mentions detec-
tion, we adopted a number of heuristic rules
from syntactic parse tree perspective. For
coreference resolution, we apply SVM by ex-
ploiting multiple syntactic and semantic fea-
tures. The experiments on the CoNLL-2011
corpus show that our rule-based mention iden-
tification system obtains a recall of 87.69%,
and the best result of the SVM-based corefer-
ence resolution system is an average F-score
50.92% of the MUC, B-CUBED and CEAFE
metrics.
1 Introduction
Coreference resolution, defined as finding the dif-
ferent mentions in a document which refer to the
same entity in reality, is an important subject in Nat-
ural Language Processing. In particular, coreference
resolution is a critical component of information ex-
traction systems (Chinchor and Nancy, 1998; Sund-
heim and Beth, 1995) and a series of coreference
resolution tasks have been introduced and evaluated
from MUC (MUC-6, 1995). Some machine learning
approaches have been applied to coreference resolu-
tion (Soon et al, 2001; Ng and Cardie, 2002; Bengt-
son and Roth, 2008; Stoyanov et al, 2009). Soon
et al(2001) use a decision tree classifier to decide
whether two mentions in a document are coreferen-
t. Bergsma and Lin (2006) exploit an effective fea-
ture of gender and number to a pronoun resolution
system and improve the performance significantly,
which is also appeared in our feature set. Howev-
er, automatic coreference resolution is a hard task
since it needs both syntactic and semantic knowl-
edge and some intra-document knowledge. To im-
prove the performance further, many deep knowl-
edge resources like shallow syntactic and seman-
tic knowledge are exploited for coreference resolu-
tion (Harabagiu et al, 2001; McCallum and Well-
ner, 2004; Denis and Baldridge, 2007; Ponzetto and
Strube, 2005; Versley, 2007; Ng, 2007). In order to
make use of more syntactic information, Kong et al
(2010) employ a tree kernel to anaphoricity determi-
nation for coreference resolution and show that ap-
plying proper tree structure in corefernce resolution
can achieve a good performance.
The CoNLL-2011 Share Task (Pradhan et
al., 2011) ?Modeling Unrestricted Coreference in
OntoNotes? proposes a task about unrestricted
coreference resolution, which aims to recognize
mentions and find coreference chains in one docu-
ment. We participate in the closed test.
In this paper, we exploit multi-features to a
coreference resolution system for the CONLL-2011
Share Task, including flat features and a tree struc-
ture feature. The task is divided into two steps in
our system. In the first step, we adopt some heuristic
rules to recognize mentions which may be in a coref-
erence chain; in the second step, we exploit a num-
ber of features to a support vector machine (SVM)
classifier to resolute unrestricted coreference. The
experiments show that our system gets a reasonable
result.
The rest of the paper is organized as follows. In
66
Section 2, we describe in detail how our system does
the work of coreference resolution, including how
we recognize mentions and how we mark the coref-
erence chains. The experimental results are dis-
cussed in Section 3. Finally in Section 4, we give
some conclusion.
2 The Coreference Resolution System
The task of coreference resolution is divided into
two steps in our system: mentions detection and
coreference resolution. In the first step, we use some
heuristic rules to extract mentions which may re-
fer to an entity. In the second step, we make up
mention-pairs with the mentions extracted in the
first step, and then classify the mention-pairs in-
to two groups with an SVM model: Coreferent or
NotCoreferent. Finally we get several coreference
chains in a document according to the result of clas-
sification. Each coreference chain stands for one en-
tity.
2.1 Rule-based Identification of Mentions
The first step for coreference resolution is to identify
mentions from a sequence of words. We have tried
the machine-learning method detecting the bound-
ary of a mention. But the recall cannot reach a high
level, which will lead to bad performance of coref-
erence resolution. So we replace it with a rule-based
method. After a comprehensive study, we find that
mentions are always relating to pronouns, named en-
tities, definite noun phrases or demonstrative noun
phrases. So we adopt the following 5 heuristic rules
to extract predicted mentions:
1. If a word is a pronoun, then it is a mention.
2. If a word is a possessive pronoun or a posses-
sive, then the smallest noun phrase containing
this word is a mention.
3. If a word string is a named entity, then it is a
mention.
4. If a word string is a named entity, then the s-
mallest noun phrase containing it is a mention.
5. If a word is a determiner (a, an, the, this, these,
that, etc.), then all the noun phrase beginning
with this word is a mention.
2.2 Coreference Resolution with
Multi-Features
The second step is to mark the coreference chain us-
ing the model trained by an SVM classifier. We ex-
tract the marked mentions from the training data and
take mention-pairs in one document as instances to
train the SVM classifier like Soon et al(2001) . The
mentions with the same coreference id form the pos-
itive instances while those between the nearest posi-
tive mention-pair form the negative instance with the
second mention of the mention-pair.
The following features are commonly used in
NLP processes, which are also used in our system:
? i-NamedEntity/j-NamedEntity: the named en-
tity the mention i/j belongs to
? i-SemanticRole/j-SemanticRole: the semantic
role the mention i/j belongs to which
? i-POSChain/j-POSChain: the POS chain of the
mention i/j
? i-Verb/j-Verb: the verb of the mention i/j
? i-VerbFramesetID/j-VerbFramesetID: the verb
frameset ID of the mention i/j, which works to-
gether with i/j-Verb
All the 5 kinds of features above belong to a sin-
gle mention. For mention-pairs, there are another 4
kinds of features as below:
? StringMatch: after cutting the articles, 1 if the
two mentions can match completely, 2 if one is
a substring of the other, 3 if they partly match,
4 else.
? IsAlias: after cutting the articles, 1 if one men-
tion is the name alias or the abbreviation of the
other one, 0 else
? Distance: it is the number of sentences between
two mentions, 0 if the two mentions are from
one sentenci-Verb/j-Verb: the verb of the men-
tion i/j
? SpeakerAgreement: 1 if both the speakers of
the two mentions are unknown, 2 if both the
two mentions come from the same speaker, 3 if
the mentions comes from different speakers.
67
All of the 14 simple and effective features above
are applied in the baseline system, which use the
same method with our system. But coreference res-
olution needs more features to make full use of the
intra-documental knowledge, so we employ the fol-
lowing 3 kinds of features to our system to catch
more information about the context.
? i-GenderNumber/j-GenderNumber (GN): 7
values: masculine, feminine, neutral, plu-
ral, ?rst-person singular, ?rst-person plural,
second-person.
? SemanticRelation (SR): the semantic relation
in WordNet between the head words of the t-
wo mentions: synonym, hyponym, no relation,
unknown.
? MinimumTree (MT): a parse tree represents the
syntactic structure of a sentence, but corefer-
ence resolution needs the overall context in a
document. So we add a super root to the forest
of all the parse trees in one document, and then
we get a super parse tree. The minimum tree
(MT) of a mention-pair in a super parse tree is
the minimum sub-tree from the common par-
ent mention to the two mentions, just like the
method uesd by Zhou(2009). And the similari-
ty of two trees is calculated using a convolution
tree kernel (Collins and Duffy, 2001), which
counts the number of common sub-trees.
We try all the features in our system, and get some
interesting results which is given in Experiments and
Results Section.
3 Experiments and Results
Our experiments are all carried out on CONLL-2011
share task data set (Pradhan et al, 2007).
The result of mention identification in the first
step is evaluated through mention recall. And the
performance of coreference resolution in the second
step is measured using the average F1-measures of
MUC, B-CUBED and CEAFE metrics (Recasens et
al., 2010). All the evaluations are implemented us-
ing the scorer downloaded from the CONLL-2011
share task website 1 .
1http://conll.bbn.com/index.php/software.html
3.1 Rule-based Identification of Mentions
The mention recall of our system in the mention i-
dentification step reaches 87.69%, which can result
in a good performance of the coreference resolution
step. We also do comparative experiments to inves-
tigate the effect of our rule-based mention identifica-
tion. The result is shown in Table 1. The CRF-based
method in Table 1 is to train a conditional random
field (CRF) model with 6 basic features, including
Word, Pos, Word ID, Syntactic parse label, Named
entity, Semantic role.
Method Recall Precision F-score
Rule-based 87.69 32.16 47.06
CRF-based 59.66 50.06 54.44
Table 1: comparative experiments of CRF-based and
rule-based methods of mention identification(%)
Table 1 only shows one kind of basic machine-
learning methods performs not so well as our rule-
based method in recall measure in mention iden-
tification, but the F1-measure of the CRF-based
method is higher than that of the rule-based method.
In our system, the mention identification step should
provide as many anaphoricities as possible to the
coreference resolution step to avoid losing corefer-
ent mentions, which means that the higher the recal-
l of mention identification is, the better the system
performs.
3.2 Coreference Resolution with
Multi-Features
In the second step of our system, SVM-LIGHT-
TK1.2 implementation is employed to coreference
resolution. We apply the polynomial kernel for
the flat features and the convolution tree kernel for
the minimum tree feature to the SVM classifier, in
which the parameter d of the polynomial kernel is
set to 3 (polynomial (a ? b + c)d) and the combin-
ing parameter r is set to 0.2 (K = tree? forest ?
kernel ? r + vector ? kernel). All the other pa-
rameters are set to the default value. All the exper-
iments are done on the broadcast conversations part
of CoNLL-2011 corpus as the calculating time of
SVM-LIGHT-TK1.2 is so long.
Experimental result using the baseline method
with the GenderNumber feature added is shown in
68
d=? MUC B3 CEAFE AVE
2 47.49 61.14 36.15 48.26
3 51.37 62.82 38.26 50.82
Table 2: parameter d in polynomial kernel in coreference
resolution using the baseline method with the GN fea-
ture(%)
Talbe 2. The result shows that the parameter d in
polynomial kernel plays an important role in our
coreference resolution system. The score when d is
3 is 2.56% higher than when d is 2, but the running
time becomes longer, too.
r=? MUC B3 CEAFE AVE
1 31.41 45.08 22.72 33.07
0.25 34.15 46.87 23.63 34.88
0 51.37 62.82 38.26 50.82
Table 3: combining parameter r (K = tree ? forest ?
kernel ? r + vector? kernel) in coreference resolution
using the baseline with the GN and MT features(%)
In Table 3, we can find that the lower the combin-
ing parameter r is, the better the system performs,
which indicates that the MT feature plays a negative
role in our system. There are 2 possible reasons for
that: the MT structure is not proper for our coref-
erence resolution system, or the simple method of
adding a super root to the parse forest of a document
is not effective.
Method MUC B3 CEAFE AVE
baseline 42.19 58.12 33.6 44.64
+GN 51.37 62.82 38.26 50.82
+GN+SR 49.61 64.18 38.13 50.64
+GN 50.97 62.53 37.96 50.49
+SEMCLASS
Table 4: effect of GN and SR features in coreference res-
olution using no MT feature (%)
Table 4 shows the effect of GenderNumber fea-
ture and SemanticRelation feature, and the last item
is the method using the SemanticClassAgreement-
Feature (SEMCLASS) used by (Soon et al, 2001)
instead of the SR feature of our system. The GN fea-
ture significantly improves the performance of our
system by 6.18% of the average score, which may
be greater if we break up the gender and number
feature into two features. As the time limits, we
haven?t separated them until the deadline of the pa-
per. The effect of the SR feature is not as good as
we think. The score is lower than the method with-
out SR feature, but is higher than the method using
SEMCLASS feature. The decreasing caused by S-
R feature may be due to that the searching depth in
WordNet is limited to one to shorten running time.
To investigate the performance of the second step,
we do an experiment for the SVM-based corefer-
ence resolution using just all the anaphoricities as
the mention collection input. The result is shown in
Table 5. As the mention collection includes no in-
correct anaphoricity, any mistake in coreference res-
olution step has double effect, which may lead to a
relatively lower result than we expect.
MUC B3 CEAFE AVE
65.55 58.77 39.96 54.76
Table 5: using just all the anaphoricities as the mention
collection input in coreference resolution step (%)
In the three additional features, only the GN fea-
ture significantly improves the performance of the
coreference resolution system, the result we finally
submitted is to use the baseline method with GN fea-
ture added. The official result is shown in Table 6.
The average score achieves 50.92%.
MUC B3 CEAFE AVE
48.96 64.07 39.74 50.92
Table 6: official result in CoNLL-2011 Share Task using
baseline method with GN feature added (%)
4 Conclusion
This paper proposes a system using multi-features
for the CONLL-2011 share task. Some syntactic and
semantic information is used in our SVM-based sys-
tem. The best result (also the official result) achieves
an average score of 50.92%. As the MT and S-
R features play negative roles in the system, future
work will focus on finding a proper tree structure
for the intra-documental coreference resolution and
combining the parse forest of a document into a tree
to make good use of the convolution tree kernel.
69
References
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Advances in Neural Information Processing
Systems (NIPS), 2004.
Chinchor, Nancy A. 1998. Overview of MUC-7/MET-2.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
Eric Bengtson, Dan Roth. 2008. Understanding the Val-
ue of Features for Coreference Resolution Proceed-
ings of the 2008 Conferenceon Empirical Methods in
Natural Language Processing, pages294C303.
Fang Kong, Guodong Zhou, Longhua Qian, Qiaoming
Zhu. 2010. Dependency-driven Anaphoricity Deter-
mination for Coreference Resolution Proceedings of
the 23rd International Conferenceon Computational
Linguistics (Coling2010), pages599C607.
Guodong Zhou, Fang Kong. 2009. Global Learning of
Noun Phrase Anaphoricity in Coreference Resolution
via Label Propagation. In Proceedings of the 2009
Coreference on Empirical Methods in Natural Lan-
guage Processing, pages 978-986, 2009.
M. Collins, N.Duffy. 2001. Convolution Kernels for Nat-
ural Language Resolution NIPS? 2001.
Marta Recasens, Llu?s Mrquez, Emili Sapena, M. Antnia
Mart?, Mariona Taul, Vronique Hoste, Massimo Poe-
sio, Yannick Versley 2010. SemEval-2010 Task 1:
Coreference Resolutionin Multiple Languages In Pro-
ceeding SemEval 2010 Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, 2010.
MUC-6. 1995. Coreference task definition (v2.3, 8 Sep
95) In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 335-344.
P.Denis, J.Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT/NAACL, 2007.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of ACL, 2002.
V. Ng. 2007. Shallow semantics for coreference resolu-
tion. In Proceedings of IJCAI, 2007.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff. 2009. Conundrums in Noun Phrase Corefer-
ence Resolution: Making Sense of the State-of-the-Art
Proceeding ACL ?09 Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL.
W.Soon,H.Ng,and D.Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrase. Com-
putational Linguistics, 27(4):521-544,2001.
S. M.Harabagiu,R.C.Bunescu,and S.J. Maiorano. 2001.
Text and knowledge mining for coreference resolution.
In Proceedings of NAACL, 2001.
S.Ponzetto, M.Strube. 2005. Semantic role labeling for
coreference resolution. In Proceedings of EACL, Italy,
April 2005.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing, 2007.
Shane Bergsma, Dekang Lin. 2006. Bootstrapping Path-
Based Pronoun Resolution. In Proceedings of the 21st
International Conference on Computational Linguis-
tics, 2006.
Sundheim, Beth M. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 13-
31.
Y.Versley. 2007. Antecedent selection techniques for
high-recall coreference resolution. In Proceedings of
EMNLP/CoNLL, 2007.
70
Proceedings of the SIGDIAL 2013 Conference, pages 457?461,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialog State Tracking using Conditional Random Fields
Hang Ren, Weiqun Xu, Yan Zhang,Yonghong Yan
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
21 North 4th Ring West Road, Beijing, China, 100190
{renhang, xuweiqun, zhangyan, yanyonghong}@hccl.ioa.ac.cn
Abstract
This paper presents our approach to dialog
state tracking for the Dialog State Track-
ing Challenge task. In our approach we
use discriminative general structured con-
ditional random fields, instead of tradi-
tional generative directed graphic models,
to incorporate arbitrary overlapping fea-
tures. Our approach outperforms the sim-
ple 1-best tracking approach.
1 Introduction
Spoken dialog systems have been widely devel-
oped in recent years. However, when dialogs are
conducted in noisy environments or the utterance
itself is noisy, it is difficult for machines to cor-
rectly recognize or understand user utterances. In
this paper we present a novel dialog state track-
ing method, which directly models the joint prob-
ability of hypotheses onN -best lists. Experiments
are then conducted on the DSTC shared corpus,
which provides a common dataset and an evalua-
tion framework
The remainder of this paper is organized as fol-
lows. Section 2 reviews relevant studies in dia-
log state tracking. Section 3 introduces our new
approach and presents the model and features we
used in detail. Section 4 describes experiment set-
tings and gives the result. Section 5 concludes this
paper with a discussion for possible future direc-
tions.
2 Previous Work
For the task of dialog state tracking, previous
research focused on dynamic Bayesian models
(DBN)(Young et al, 2013). User goal, dialog his-
tory and other variables are modeled in a graphi-
cal model. Usually, Markov assumptions are made
and in each turn the dialog state is dependent on
the ASR outputs and the dialog state of the pre-
vious turn. Dependency on other features, such
as system action, dialog history could be assumed
as long as their likelihood is modeled. For a
POMDP-based dialog model, the state update rule
is as follows:
bt+1(st+1) = ?P (ot+1|st+1, at)?
st
P (st+1|st, at)bt(st) (1)
where bt(st) is the belief state at time t, ot+1 is the
observation at time t+ 1, at is the machine action.
Thus the dialog states are estimated incrementally
turn by turn.
Since each node has hundreds, or even thou-
sands, of possible assignments, approximation is
necessary to make efficient computation possible.
In POMDP-based dialog systems, two common
approaches are adopted (Young et al, 2013), i.e.,
N -best approximation and domain factorization.
In theN -best approach, the probability distribu-
tion of user goals are approximated using N -best
list. The hidden information state (HIS) model
(Young et al, 2010) makes a further simplification
that similar user goals are grouped into a single
entity called partition, inside which all user goals
are assigned the same probabilities. The Bayesian
update of dialog state (BUDS) model (Thomson
and Young, 2010) is a representative of the second
approach and adopts a different approximation
strategy, where each node is further divided into
sub-nodes for different domain concepts and in-
dependence assumptions of sub-nodes across con-
cepts are made. Recent studies have suggested
that a discriminative model may yield better per-
formance than a generative one (Bohus and Rud-
nicky, 2006). In a discriminative model, the emis-
sion part of the state update rule is modeled dis-
criminatively. Possible flawed assumptions in a
completely generative models could be mitigated
457
in this way, such as the approximation of obser-
vation probability using SLU scores (Williams,
2012a; Williams, 2012b).
3 Proposed Method
3.1 Discriminative State Tracking Model
Most previous methods model the distribution of
user goals for each turn explicitly, which can lead
to high computation cost. In our work, the joint
probability of all items on the N -best lists from
SLU is modeled directly and the state tracking re-
sult is generated at a post-processing stage. Thus
the state tracking problem is converted into a la-
beling task as is shown in equation 2, which in-
volves modeling the joint probability of the N -
best hypotheses.
bt(st) = P (H1,1, H1,2, ...,Ht,m?1, Ht,m) (2)
where Ht,m is a binary variable indicating the
truthfulness of the m-th hypothesis at turn t.
For each turn, the model takes into account all
the slots on theN -best lists from the first turn up to
the current one, and those slots predicted to be true
are added to the dialog state. The graphical model
is illustrated in figure 1. To predict dialog state at
turn t, the N -best items from turn 1 to t are all
considered. Hypotheses assigned true labels are
included in the dialog state. Compared to the DBN
approach, the dialog states are built ?jointly?. This
approach is reasonable because what the tracker
generates is just some combinations of all N -best
lists in a session, and there is no point guessing be-
yond SLU outputs. We leverage general structured
Conditional Random Fields (CRFs) to model the
probabilities of the N -best items, where factors
are used to strengthen local dependency. Since
CRF is a discriminative model, arbitrary overlap-
ping features can be added, which is commonly
considered as an advantage over generative mod-
els.
3.2 Conditional Random Fields
CRF is first introduced to address the problem
of label bias in sequence prediction (Lafferty et
al., 2001). Linear-chain CRFs are widely used to
solve common sequence labeling problem in nat-
ural language processing. General structured CRF
has also been reported to be successful in various
tasks (Sutton and McCallum, 2012).
In general structured CRF, factor templates are
utilized to specify both model structure and pa-
...
Hyp1
Hyp2
HypN
Turn t
Slot1=...
Slot2=...
...
Turn t-1
Figure 1: Dialog state update using CRFs, where
the 8 rectangles above denote N -best hypothe-
ses for each turn, and the box below represents
the dialog state up to the current turn. Con-
nections between rectangles denote ?Label-Label?
factors. ?Label-Observation? factors are not shown
for simplicity.
rameter tying (Sutton and McCallum, 2012). Fac-
tors are partitioned into a series of templates, and
factors inside each template share the same param-
eters.
p(y|x) = 1Z(x)
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p), (3)
where C is the set of factor templates and x,y are
inputs and labels respectively. Template factors
are written as
?c(xc,yc; ?p) = exp
K(p)?
k=1
?pkfpk (xc,yc) (4)
and Z(x) is the normalizing function
Z(x) =
?
y
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p) (5)
In the experiment we use Factorie1 to define and
train the model.
3.3 Model Structure and Features
In the model, slots in every N -best item up
to the current turn are represented as binary
variables. For simplification of data structure,
each slot in a single N -best item is extracted
and represented using different label vari-
ables, with the same rank indicating their
1Available from https://github.com/
factorie/factorie.
458
original places in the N -best list. For exam-
ple, the item slots: [from: Pittsburgh,
data: Tuesday], score: 0.85, rank: 2,
is converted to two slots: slots: [from:
Pittsburgh], score: 0.85, rank: 2 and
slots: [date: Tuesday], score: 0.85,
rank: 2. Label-label connections are specified
using factor templates between slot pairs, and
Label-observation templates are used to add
slot-wise features. Without label-label connection
the model is reduced to a maximum entropy
model, and with more connections added, the
graph tends to have loopy structures.
Two classes of feature sets (templates) in the ex-
periment are defined as follows.
(1) Label-Label factor templates are used to
strengthen the bond between certain slots.
Pairwise-slots of the same rank This template is
built for pairs of slots in a turn with the same
rank to bind their boolean assignment. To
avoid creating too many loops and make in-
ference efficient, the factors are added in such
an order that the slots involved in a single turn
are linked in a linear way.
Pairwise-slots with identical value Slots with
identical value may appear in the N -best
list for multiple times. Besides, user can
mention the same slot in different turns,
making these slots more reliable. Similar
ordering mechanism is utilized to avoid
redundant loops.
(2) Label-observation templates are used to add
features for the identification of the truthfulness of
slots.
SLU score and rank of slot The score generated
by the ASR and SLU components is a direct
indicator of the correctness degree of slots.
However, a slot?s true reliability is not neces-
sarily linear with its score. The relationship is
quite different for various ASR and SLU al-
gorithms, and scores produced by some ASR
are not valid probabilities. As we adopt a
data-driven approach, we are able to learn
this relationship from data. In addition to the
SLU score, the slot rank is also added to the
feature set.
Dialog history (grounding information) In
most spoken dialog systems, explicit and
implicit groundings are adapted to indicate
the correctness of the system belief. This
information is useful to determine the
correctness of slots. The grounding infor-
mation includes grounding type (implicit
or explicit grounding), user reply (negation
or confirmation) and corresponding SLU
scores.
Count of slots with identical value As previ-
ously mentioned, slots with identical values
can appear several times and slots with more
frequent occurrences are more likely to be
correct.
Domain-specific features Slots for some domain
concepts often have values with specific
forms. For example, in the DSTC data sets,
the route slots are usually filled with values
like ?61d?, ?35b?, and SLU often generates
noisy outputs like ?6d?, ?3d?. Thus the lexi-
cal form is a very useful feature.
Baseline Tracker The simple and fast 1-best
tracking algorithm is used as the baseline
tracker and exhibits a satisfying performance.
Thus the tracking result is added as an addi-
tional feature. This indicates the possibility
of combining tracking outputs from differ-
ent algorithms in this discriminative model,
which may improve the overall tracking per-
formance.
4 Experiment
4.1 Task and Data
The Dialog State Tracking Challenge (DSTC)2
aims at evaluating dialog state tracking algorithms
on shared real-user dialog corpus. In each dia-
log session, ASR and SLU results are annotated,
making it possible to conduct direct comparison
among various algorithms. For further details,
please refer to the DSTC handbook (Williams et
al., 2013b).
4.2 Corpus Preprocessing
The ASR and SLU components can generate many
noisy hypotheses which are completely wrong,
rendering the dialog corpus seriously imbalanced
at the level of slots (there are more wrong slots
than true slots). We use resampling to prevent
2http://research.microsoft.com/en-us/
events/dstc/
459
the model from biasing towards making negative
judgements. Before training, the total number of
correct slots in a set is counted, and equal num-
ber of wrong slots are sampled from the subset of
all the wrong slots. These chosen negative slots
plus all the positive slots together constitute the
effective slot set for model training, with remain-
ing slots fixed to their true value and regarded as
observed variables. To make full use of the dia-
log corpus, this process is repeated for eight times,
producing a bigger and balanced corpus.
4.3 Model Training
In the training phase, the log-likelihood function
is optimized using the LBFGS method with L2-
regularization. Loopy belief propagation is used
as the inference routine. It can be shown that for
factor graphs without loops, the marginal proba-
bilities produced by loopy belief propagation are
exact. Model selection is done according to the
log-likelihood on the development set.
4.4 Predicting and Tracking
For each dialog session, the predicted slot labels
are mapped to tracking results. To produce a N -
best list of tracking results, the top N configura-
tions of slots and corresponding probability scores
are generated. Gibbs sampling is adopted. The
sampling is repeated for 1000 times in each cor-
pus, after each sampling the configuration of slots
is mapped to certain tracking state. More efficient
inference routines, such as M-best belief propaga-
tion (Yanover and Weiss, 2004), could be utilized,
which would be suitable for practical real-time ap-
plication.
4.5 Results
After tracker outputs are generated based on the
sampling results, they are scored using evaluation
tools provided by the DSTC organizers. Several
metrics are evaluated, including precisions, ROC
performance, etc. Individual and joint slots are
scored respectively. And different schedules are
used, which indicats the turns included for evalu-
ation. Partial results are shown in table 1. A sys-
tematic analysis by the organizers is in the DSTC
overview paper (Williams et al, 2013a). The com-
plete challenge results can be found on DSTC
website. On the test sets of test1, test2 and test3,
the CRF-based model achieves better performance
than the simple baseline in most metrics. How-
ever, on test4, the performance degrades seriously
when there is a mismatch between training data
and test data, since test4 is produced by a different
group and does not match the training set. It shows
that the CRF-based model is very flexible and is
able to learn the properties of ASR and SLU, thus
adapting to a specific system. But it has a tendency
of overfitting .
Test1 Test4
Metric CRF BASE CRF BASE
ACC 0.987 0.983 0.960 0.986
L2 0.020 0.021 0.046 0.017
MRR 0.990 0.988 0.980 0.990
CA05 0.987 0.983 0.960 0.986
EER 0.015 0.983 0.021 0.012
Table 1: Results of slot ?Date? on Test1 and Test4
(schedule1 is used). The tracker used on Test4 is
trained on Test3. Details of the metrics can be
found in DSTC handbook(Williams et al, 2013b)
5 Conclusions and Future Directions
We proposed a CRF-based discriminative ap-
proach for dialog state tracking. Preliminary re-
sults show that it achieves better performance than
the 1-best baseline tracker in most metrics when
the training set and testing set match. This indi-
cates the feasibility of our approach which directly
models joint probabilities of the N -best items.
In the future, we will focus on the following
possible directions to improve the performance.
Firstly, we will enrich the feature set and add more
domain-related features. Secondly, interactions of
slots between dialog turns are not well modeled
currently. This problem can be alleviated by tun-
ing graph structures, which deservers further stud-
ies. Moreover, it is challenging to use online train-
ing methods, so that the performance could be im-
proved incrementally when more training samples
are available.
6 Acknowledgments
This work is partially supported by the Na-
tional Natural Science Foundation of China (Nos.
10925419, 90920302, 61072124, 11074275,
11161140319, 91120001), the Strategic Prior-
ity Research Program of the Chinese Academy
of Sciences (Grant Nos. XDA06030100,
XDA06030500), the National 863 Program (No.
2012AA012503) and the CAS Priority Deploy-
ment Project (No. KGZD-EW-103-2).
460
References
Dan Bohus and Alex Rudnicky. 2006. A ?k hypotheses
+ other? belief updating model. In Proceedings of
the 2006 AAAI Workshop on Statistical and Empiri-
cal Approaches for Spoken Dialogue Systems, pages
13?18, Menlo Park, California. The AAAI Press.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Charles A. Sutton and Andrew McCallum. 2012. An
introduction to conditional random fields. Founda-
tions and Trends in Machine Learning, 4(4):267?
373.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013a. The dialog state track-
ing challenge. In Proceedings of the 14th SIGdial
workshop on Discourse and Dialogue.
Jason D. Williams, Antoine Raux, Deepak Ra-
machandran, and Alan Black. 2013b. Dia-
log state tracking challenge handbook. Avail-
able from http://research.microsoft.
com/apps/pubs/?id=169024.
Jason D. Williams. 2012a. Challenges and opportu-
nities for state tracking in statistical spoken dialog
systems: Results from two public deployments. Se-
lected Topics in Signal Processing, IEEE Journal of,
6(8):959 ?970.
Jason D. Williams. 2012b. A critical analysis of two
statistical spoken dialog systems in public use. In
SLT, pages 55?60. IEEE.
Chen Yanover and Yair Weiss. 2004. Finding the
m most probable configurations using loopy belief
propagation. Advances in Neural Information Pro-
cessing Systems, 16:289?296.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
461
