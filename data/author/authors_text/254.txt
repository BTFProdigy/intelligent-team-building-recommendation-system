Study of Practical Effectiveness for Machine Translation Using
Recursive Chain-link-type Learning
Hiroshi Echizen-ya
Dept. of Electronics and Information
Hokkai-Gakuen University
S 26-Jo, W 11-Chome, Chuo-ku
Sapporo, 064-0926 Japan
echi@eli.hokkai-s-u.ac.jp
Kenji Araki
Division of Electronics and Information
Hokkaido University
N 13-Jo, W 8-Chome, Kita-ku
Sapporo, 060-8628 Japan
araki@media.eng.hokudai.ac.jp
Yoshio Momouchi
Dept. of Electronics and Information
Hokkai-Gakuen University
S 26-Jo, W 11-Chome, Chuo-ku
Sapporo, 064-0926 Japan
momouchi@eli.hokkai-s-u.ac.jp
Koji Tochinai
Division of Business Administration
Hokkai-Gakuen University
4-Chome, Asahi-machi, Toyohira-ku
Sapporo, 060-8790 Japan
tochinai@econ.hokkai-s-u.ac.jp
Abstract
A number of machine translation systems based
on the learning algorithms are presented. These
methods acquire translation rules from pairs
of similar sentences in a bilingual text cor-
pora. This means that it is difficult for the
systems to acquire the translation rules from
sparse data. As a result, these methods require
large amounts of training data in order to ac-
quire high-quality translation rules. To over-
come this problem, we propose a method of ma-
chine translation using a Recursive Chain-link-
type Learning. In our new method, the system
can acquire many new high-quality translation
rules from sparse translation examples based on
already acquired translation rules. Therefore,
acquisition of new translation rules results in
the generation of more new translation rules.
Such a process of acquisition of translation rules
is like a linked chain. From the results of evalua-
tion experiments, we confirmed the effectiveness
of Recursive Chain-link-type Learning.
1 Introduction
Rule-Based Machine Translation(MT)(Hutchins
and Somers, 1992) requires large-scale knowl-
edge to analyze both source language(SL)
sentences and target language(TL) sentences.
Moreover, it is difficult for a developer to com-
pletely describe large-scale knowledge that can
analyze various linguistic phenomena. There-
fore, Rule-Based MT is time-consuming and
expensive. Statistical MT and Example-Based
MT have been proposed to overcome the dif-
ficulties of Rule-Based MT. These approaches
correspond to Corpus-Based approach. Corpus-
Based approach uses translation examples that
keep including linguistic knowledge. This
means that the system can improve the quality
of its translation only by adding new translation
examples. However, in Statistical MT(Brown
et al, 1990), large amounts of translation
examples are required in order to obtain
high-quality translation. Moreover, Example-
Based MT(Sato and Nagao, 1990; Watanabe
and Takeda, 1998; Brown, 2001; Carl, 2001)
which relies on various knowledge resources
results in the same difficulties as Rule-Based
MT. Therefore, Example-Based MT, which
automatically acquires the translation rules
from only bilingual text corpora, is very effec-
tive. However, existing Example-Based MT
systems using the learning algorithms require
large amounts of translation pairs to acquire
high-quality translation rules.
In Example-Based MT based on analogical
reasoning(Malavazos, 2000; Guvenir, 1998), the
different parts are replaced by variables to gen-
eralize translation examples as shown in (1) of
Figure 1. However, the number of different
parts of the two SL sentences must be same
as the number of different parts of the two TL
sentences. This means that the condition of ac-
quisition of translation rules is very strict be-
cause this method allows only n:n mappings in
the number of the different parts between the
SL sentences and the TL sentences. As a re-
sult, many translation rules cannot be acquired.
(McTait, 2001) generalizes both the different
parts and the common parts as shown in Fig-
ure 1(2). This means that (McTait, 2001) al-
lows m:n mappings in the number of the differ-
ent parts, or the number of the common parts.
However, it is difficult to acquire the translation
rules that correspond to the lexicon level. On
the other hand, we have proposed a method of
Machine Translation using Inductive Learning
with Genetic Algorithms(GA-ILMT)(Echizen-
ya et al, 1996). This method automatically
generates the similar translation examples from
only given translation examples by applying ge-
netic algorithms(Goldberg, 1989) as shown in
(3a) of Figure 1. Moreover, the system per-
forms Inductive Learning. By using Inductive
Learning, the abstract translation rules are ac-
quired by performing phased extraction of dif-
ferent parts as shown in Figure 1(3b) and (3c).
In all methods shown in Figure 1, the condi-
tion of acquisition of translation rules is that
two similar translation examples must exist. As
a result, the systems require large amounts of
translation examples.
We propose a method of MT using Recur-
sive Chain-link-type Learning as a method to
overcome the above problem. In our method,
the system acquires new translation rules from
sparse data using other already acquired trans-
lation rules. For example, first, translation
rule B is acquired by using translation rule A
when the translation rule A exists in the dictio-
nary. Moreover, translation rule C is acquired
by using the translation rule B. Such a pro-
cess of acquisition of translation rules is like a
chain where each ring is linked. Therefore, we
call this mechanism Recursive Chain-link-type
Learning(RCL). This method can effectively ac-
quire many translation rules from sparse data
without depending on the different parts of sim-
ilar translation pairs. In this paper, we describe
the effectiveness of RCL through evaluation ex-
periments.
2 Basic Idea
RCL is a method with an ability that automat-
ically acquires translation knowledge in a com-
puter without any analytical knowledge, such as
GA-ILMT. This is the ability to extract corre-
	
		 	
Coling 2008: Companion volume ? Posters and Demonstrations, pages 111?114
Manchester, August 2008
A Complete and Modestly Funny System for Generating and Performing
Japanese Stand-Up Comedy
Jonas Sj
?
obergh
Hokkaido University
js@media.eng.hokudai.ac.jp
Kenji Araki
Hokkaido University
araki@media.eng.hokudai.ac.jp
Abstract
We present a complete system that gen-
erates Japanese stand-up comedy. Differ-
ent modules generating different types of
jokes are tied together into a performance
where all jokes are connected in some way
to the other jokes. The script is converted
to speech and two robots perform the com-
edy routine. Evaluations show that the per-
formances are perceived as funny by many,
almost half the evaluation scores for the to-
tal impression were 4 or 5 (top score).
1 Introduction
When it comes to computer processing of humor
two main areas exist, humor recognition and hu-
mor generation (Binsted et al, 2006). This paper
falls under generation. We present a system that
automatically creates short stand-up comedy like
performances. Most generation systems only gen-
erate simple types of jokes, by themselves. There
are few systems generating complete comic shows.
Our system combines several different methods for
generating quite simple jokes and then combines
these into one short performance made for two per-
formers. This is then automatically converted into
speech audio, and presented by two small robots.
The performances generated are in Japanese,
and similar to Japanese manzai, a form of stand
up comedy. Manzai is generally performed by two
comedians, one straight-man (tsukkomi) and one
funny man (boke). Boke misunderstands or says
stupid things, and tsukkomi has to berate or cor-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
rect, for instance by exclaiming ?Idiot!? and hit-
ting boke on the head.
2 System
Several components are combined to produce short
comedy performances. We first give an overview
of the system and then explanations of the com-
ponents. Though the system only generates jokes
in Japanese, for ease of understanding examples in
English similar to the Japanese original jokes are
used in the explanations.
2.1 Overall System
First a script for the performance is generated. It
starts with an introduction like ?Hi, we are two
not very proficient joke robots, please listen to
our performance.?, simply selected from a short
list. Next, one robot uses a proverb or saying in
Japanese, along the lines of ?Recently my life has
felt like ?jack of all trades, master of none?, you
know.? The other robot then makes a vulgar joke
by modifying this, perhaps like ?For me it has been
more like ?jacking off all trades, masturbate none?,
I must say.?. The way of berating your stupid part-
ner common in Japanese comedy has been incor-
porated in the system. After the vulgar joke above,
the first (tsukkomi) robot says a phrase from a list
of fairly generic put-down phrases, like ?What the
hell are you saying??.
Then the boke robot tells a joke from a database
of wordplay jokes, selected from those with one
noun already in the script. So in the example
above, perhaps: ?Speaking of ?life? [mentioned be-
fore], ?shotgun wedding: a case of wife or death?
comes to mind.? Again followed by a put-down by
the tsukkomi robot.
Next comes a simple punning riddle generator.
A noun already used that also sounds like a rude
111
word is selected and a riddle is created. The rid-
dle jokes are quite weak, similar to: ??speaking?
[used earlier] is ?speaking?, but what is a naughty
kind of speaking??, ?What??, ?Spanking the mon-
key!? (?speak? sounds like ?spank? and ?spanking
the monkey? is a euphemism). Again followed by
a put-down, ?Idiot! Would you please stop.?.
Finally, one more joke from the database and
another put-down are used. The robots then close
with ?Thank you, the end.? or similar. All the lines
are then converted to speech using a text-to-speech
tool. The audio files are then input into two small
robots, that perform the routine.
2.2 Proverb Jokes
The proverb joke module has a list of almost 1,000
proverbs and sayings in Japanese. These were
collected by simply downloading a few lists of
Japanese proverbs. Since many of these are quite
rare and thus people in general would not un-
derstand them or any joke based on them, rare
proverbs are filtered out automatically. This is
done by simply searching the web for the proverb
verbatim. If it occurs more times than some thresh-
old value (currently arbitrarily set to 50) it is con-
sidered common and can be used to make jokes,
starting with a generic statement like ?Recently my
life has felt like <proverb>?.
To make a joke, a proverb is twisted into a
new variant by changing words to similar sound-
ing dirty words instead. The dirty words are taken
from a collection a few hundred dirty words in
Japanese. These have been grouped into three
categories, sex related, feces related, and insults.
Words can belong to several or all of these groups
(e.g. ?asshole?) and are then present in all groups.
A dirty variant of a proverb has to contain at
least two new words, and these must be of the
same type, and they must also sound reasonably
similar to the words they replace. This is deter-
mined using a table of which sounds are how sim-
ilar in Japanese, which is almost the same as the
one used in (Takizawa et al, 1996). Since the
same character in Japanese can have several dif-
ferent readings, we use a standard morphological
analyzer for Japanese called ChaSen (Matsumoto
et al, 1997) to get the pronunciation for the words.
This leads to some problems, since the analyzer
does not work all that well on proverbs (often us-
ing rare grammatical constructions, words, etc.),
nor on dirty words (often missing from ChaSen?s
lexicons). If there are more than one way to change
a proverb into a new variant, one is selected at ran-
dom. The joke is then presented as described in the
overview section, i.e. one robot saying the original
proverb and the other saying the variant.
2.3 Riddle Jokes
There have been a few systems that generate word
play riddles (Binsted, 1996; Binsted and Takizawa,
1998) and our module is not very innovative, it fol-
lows the same basic ideas. First, the script that has
been created so far is run through the ChaSen mor-
phological analyzer also used earlier. Nouns and
their pronunciations are then checked against the
collection of dirty words to see if there are any
dirty words with similar pronunciation. A random
noun sounding similar to a dirty word is then used.
The riddle is built with this noun and the corre-
sponding dirty word using a simple pattern. The
boke robot says :?A <noun> is a <noun>, but
what kind of <noun> is <hint>?? followed by:
?What??, and the answer: ?<Dirty word>?. The
most difficult part is finding a hint that describes
the dirty word in a good but not too obvious
way without also being a good description of the
original word. Hints are generated by searching
the Internet for phrases like ?a <dirty word> is
<hint>.? Things found in this way are then as-
sumed to be reasonable descriptions of the dirty
word (often not true, unfortunately), and are then
checked to see if they are also often used for the
original word. This is done by checking the co-
occurrences of the hint and the original noun, and
the hint and the dirty word, also using web fre-
quencies. The log-likelihood ratios are then com-
pared, and if the hint is more closely connected to
the dirty word it is used. There is also a short stop
list of hints that are very common but useless, such
as Japanese particles similar to the word ?exist?.
Since the dirty words in our collection are not
that common on the Internet, it happens that no
usable hints are found at all. In such cases a sim-
ple hint meaning ?naughty?, ?rude?, or ?dirty?, is
used for sex related words, insults, and feces re-
lated words respectively. It is also happens that no
noun used in the script sounds similar to a dirty
word. Currently, for such cases, the whole script is
abandoned and the system starts over.
2.4 Database of Puns
We automatically collected a database of word
play jokes in Japanese, using a few seed jokes. If
112
for instance a seed joke occurred in an HTML list
on the Internet, all other list items were taken as
jokes too. The database consists of almost 2,200
jokes, mostly very weak word play jokes, though
some are perceived as quite funny by many peo-
ple. The jokes are often written using contrac-
tions (e.g. ?dontcha?), dialectal pronunciation in-
stead of standard orthography, strange punctuation
or choice of alphabets etc. This causes problems
for the morphological analyzer, leading to errors.
When a joke from the database is needed, all
the nouns from the script up until this point are
extracted as above. A joke from the database con-
taining at least one of these is then selected and
presented along the lines of ?Speaking of<noun>,
this reminds me of <joke with noun>?.
2.5 Put-Downs and Come-Backs
We asked an amateur comedian to write a short
list of generic put-down phrases, giving things like
?Ha ha, very funny?, ?What the hell are you talk-
ing about??, ?Idiot?, ?That is not what I meant?,
and similar. Put-downs are drawn at random from
the list, excluding any phrase already used.
For database jokes, two other put-downs are also
possible. There is a a simple web frequency check
to see if the joke is old. Any joke occurring more
than 20 times on the Internet is currently consid-
ered ?Old!?. Jokes that are not old can instead get
the ?Stupid foreigner!? put-down (quite common
in Japanese comedy). This is used on jokes with
words written either in English letters or katakana
letters. Katakana is mainly used for foreign loan
words in Japanese, but is also other things (simi-
lar perhaps to using upper case in English), which
leads to some errors.
For some put-downs it is also possible for the
boke robot to make a come-back. When possible
this is also added to the script. For instance, when
the tsukkomi robot says ?Old!? it goes on to say for
example: ?By the way, how is the new apartment
you moved into??, and the boke robot replies with
the phrase used on him, ?Old!?.
2.6 Robots
The script is converted into audio for the robots
using the AquesTalk
1
text-to-speech system, and
the robots are given different synthetic voices. The
text-to-speech conversion works fairly well, but
sometimes the speech is hard to understand.
1
http://www.a-quest.com/aquestal/
Figure 1: The robots used.
The two robots used in the performances are
both Robovie-i robots, see Figure 1, one blue and
one gold. The Robovie-i can move its legs and
lean its body sideways. It has a small speaker at-
tached, to produce the speech. This is the weak-
est link in the system so far, since the speaker is
quite weak. The sound quality is not great, and
the volume is low. This is also compounded by
the text-to-speech system output sometimes being
quite hard to understand to begin with, and also
by the generated jokes sometimes being incompre-
hensible. The main merits of the Robovie-i are that
it is easily programmable, cheap, and cute. The
robots did not move very much. They walked a
little bit forward and bowed during the introduc-
tion, then remained stationary, leaning their torsos
a little to one side when speaking.
3 Evaluation
We generated two scripts and had the robots per-
form them for evaluators. Script 1 was shown first,
then a short questionnaire for script 1 was filled
out, then script 2 was performed and another ques-
tionnaire filled out. The impression of each whole
performance was rated from 1 (not funny) to 5
(funny). Each individual joke was also rated.
Evaluators were found by going to a student
cafeteria and offering chocolate for participating.
Since the speech from the robot was a bit diffi-
cult to understand it was sometimes very difficult
to hear some jokes when there was a lot of back-
ground noise. The evaluators where also given the
script in written form after they had watched the
performance and could thus read any parts they did
not hear before evaluating the funniness. 33 eval-
uators took part in the evaluations. How funny the
jokes are thought to be of course varies a lot from
person to person. The highest and lowest means
of an evaluator were 4.2 and 1.2 respectively. The
results are shown in Tables 1 and 2.
Table 1 shows the overall impression of the
scripts, 3.3 on a scale from 1 to 5. Joke genera-
113
Script 1 Script 2 Both
Score 3.4 (0.9) 3.2 (1.0) 3.3 (1.0)
4 or 5 16 (48%) 14 (42%) 30 (45%)
Table 1: Mean (and standard deviation) evaluation
scores and the number of 4s or 5s assigned, for the
total impression of the two evaluated scripts.
tion systems tend to get fairly low scores, so we
believe a score of over 3 is good. What meaning
evaluators put into a 3 on a scale from 1 to 5 is hard
to estimate, but many seemed to enjoy the perfor-
mances. It was also not uncommon to laugh a lot
during the performance and still rate everything as
1 or 2 so, for some, laughter does not equal funny.
A score of 4 or more should reasonably indicate a
funny joke. For the total impression of the perfor-
mances, 30 scores (of 66) were either a 4 or a 5, so
almost half of the evaluators though it was funny
in this sense. We believe this is a good result, con-
sidering that individual tastes in humor vary a lot.
In Table 2 the scores of the individual jokes are
shown. It seems that the proverb jokes are of about
the same level as the human made jokes from the
Internet. The riddle jokes lag a little behind, as
does the come-back joke that was included.
While the system makes mistakes, joke gen-
eration seems rather robust to errors. Since the
robots are supposed to say stupid things anyway,
if they do so by mistake instead of on purpose it
can still be funny. There were comments from
evaluators about mistakes that they disliked too,
though: ?This put-down is inappropriate for that
joke?, ?They should bow while saying thank you,
not after.?, ?The dirty jokes are too direct, subtle is
funnier?.
The biggest problem was the robot speakers.
This should be fairly easy to fix. The other prob-
lems stem mainly from the generated jokes not be-
ing overly funny, which seems harder to deal with.
4 Conclusions
We have implemented a complete system for auto-
matically generating and performing short stand-
up comedy routines in Japanese. Different mod-
ules generate different types of jokes then tied to-
gether so that the jokes used have something in
common. This is then converted to speech and up-
loaded into two robots that perform the comedy.
In the evaluation, the performances were rated
Score 4 or 5
Proverb 1 2.6 (1.2) 9 (27%)
Proverb 2 3.0 (1.0) 11 (33%)
Proverb Avg. 2.8 (1.1) 20 (30%)
Riddle 1 2.4 (1.1) 4 (12%)
Riddle 2 2.3 (1.1) 5 (15%)
Riddle Avg. 2.4 (1.1) 9 (13%)
Comeback 2.6 (1.1) 6 (18%)
Database 1a 3.6 (1.1) 19 (57%)
Database 1b 2.5 (1.2) 6 (18%)
Database 2a 3.1 (1.1) 13 (39%)
Database 2a 2.9 (1.1) 13 (39%)
Database Avg. 3.0 (1.2) 51 (38%)
Table 2: Mean (and standard deviation) evaluation
scores and the number of 4s or 5s assigned to the
different jokes.
as 3.3 on a scale from 1 (not funny) to 5 (funny)
and many evaluators enjoyed the performances.
Almost half of the evaluation scores assigned to
the total impression of the system were 4 or 5. This
seems quite promising, though there are still many
things that can be improved in the system.
References
Binsted, Kim and Osamu Takizawa. 1998. BOKE:
A Japanese punning riddle generator. Journal
of the Japanese Society for Artificial Intelligence,
13(6):920?927.
Binsted, Kim, Benjamin Bergen, Seana Coulson, Anton
Nijholt, Oliviero Stock, Carlo Strapparava, Graeme
Ritchie, Ruli Manurung, Helen Pain, Annalu Waller,
and Dave O?Mara. 2006. Computational humor.
IEEE Intelligent Systems, 21(2):59?69.
Binsted, Kim. 1996. Machine Humour: An Imple-
mented Model of Puns. Ph.D. thesis, University of
Edinburgh, Edinburgh, United Kingdom.
Matsumoto, Y., A. Kitauchi, T. Yamashita, Y. Hirano,
O. Imaichi, and T. Imamura. 1997. Japanese mor-
phological analysis system ChaSen manual. Techni-
cal Report NAIST-IS-TR97007, NAIST.
Takizawa, Osamu, Masuzo Yanagida, Akira Ito, and
Hitoshi Isahara. 1996. On computational processing
of rhetorical expressions - puns, ironies and tautolo-
gies. In International Workshop on Computational
Humor, pages 39?52, Enschede, The Netherlands.
114
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 382?390,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Casual Conversation System Using Modality and Word Associations
Retrieved from the Web
Shinsuke Higuchi Rafal Rzepka
Graduate School of Information Science and Technology
Hokkaido University , Sapporo Japan 060-0814
{shin h,kabura,araki}@media.eng.hokudai.ac.jp
Kenji Araki
Abstract
In this paper we present a textual dialogue
system that uses word associations retrieved
from the Web to create propositions. We also
show experiment results for the role of modal-
ity generation. The proposed system automat-
ically extracts sets of words related to a con-
versation topic set freely by a user. After the
extraction process, it generates an utterance,
adds a modality and verifies the semantic re-
liability of the proposed sentence. We evalu-
ate word associations extracted form the Web,
and the results of adding modality. Over 80%
of the extracted word associations were evalu-
ated as correct. Adding modality improved the
system significantly for all evaluation criteria.
We also show how our system can be used as
a simple and expandable platform for almost
any kind of experiment with human-computer
textual conversation in Japanese. Two exam-
ples with affect analysis and humor generation
are given.
1 Introduction
Many task-oriented dialogue systems (Liu et al,
2003; Reitter et al, 2006) have been developped.
Research on non-task-oriented dialogue systems like
casual conversation dialogue systems (?chatbots?) is
on the other hand not very common, perhaps due to
the many amateurs who try to build naturally talking
systems using sometimes very clever, but rather un-
scientific methods although there are systems with
chatting abilities as (Bickmore and Cassell, 2001)
but concentrate on applying strategies to casual con-
versation rather than their automatic generation of
those conversations. However, we believe that the
main reason is that an unrestricted domain is dispro-
portionately difficult compared to the possible use
such a system could have. It is for example very hard
to predict the contents and topics of user utterances,
and therefore it is almost impossible to prepare con-
versational scenarios. Furthermore, scenarios need
more or less specific goals to be useful. However
in our opinion, sooner or later non-task-oriented di-
alogue systems will have to be combined with task
oriented systems and used after recognizing that the
user?s utterance does not belong to a given task. This
would lead to more natural interfaces for e.g. infor-
mation kiosks or automatic guides placed in public
places where anyone can talk to them about anything
(Gustafson and Bell, 2000; Kopp et al, 2005) re-
gardless of the role the developers intended. For this
reason we have also started implementing emotive-
ness recognition and joke generation modules that
are presented later in the paper.
Well-known examples of non-task-oriented dia-
logue systems are ELIZA (Weizenbaum, 1966) and
A.L.I.C.E 1, though the former was built to parody a
Rogerian therapist which can be regarded as a task.
Both systems and their countless imitators 2 use a
lot of rules coded by hand. ELIZA is able to make
a response to any input, but these responses are only
information requests without providing any new in-
formation to the user. In the case of A.L.I.C.E,
1Wallace, R. The Anatomy of A.L.I.C.E.
http://www.alicebot.org/anatomy.html.
2Many of them have been quite successful in the Loeb-
ner Prize and the Chatterbox Challenge (competitions only for
English-speaking bots) but explanations of their algorithms are
not available.
382
the knowledge resource is limited to the existing
database. Creating such databases is costly and
a programmer must learn the AIML mark-up lan-
guage to build it. Although there have been attempts
at updating AIML databases automatically (Pietro et
al., 2005), the scale was rather limited.
As mentioned above, these examples and many
other ?chatbots? need hand-crafted rules, and are
thus often ignored by computer scientists and rarely
become a research topic. However, they have proved
to be useful for e-learning (Pietro et al, 2005) and
machine learning (Araki and Kuroda, 2006) support.
Building a system using automatic methods, like
we do, seems to be the most realistic way for unre-
stricted domains. Considering the large cost of de-
veloping a program that can talk about any topic, it
is appealing to turn to the huge and cheap textual
source that is the Internet.
In this very moment millions of people (Kumar et
al, 2003) are updating their blogs and writing articles
on every possible topic. These are available on the
Web which we can access any time, and in a faster
and faster manner, the search engines grow more and
more efficient. Thus, the Web is well suited to ex-
tracting word associations triggered by words from
user utterances made in a topic-free dialogue sys-
tem.. We present a system making use of this type
of information. It automatically extracts word asso-
ciation lists using all keywords in a given utterance
without choosing a specific one (which most other
systems that ignore the context do) then generates a
reply using the only one strongest association from
the nouns, verbs and adjectives association groups.
Modality is then added to the reply, and then it is
output.
Our system is built upon the idea that human utter-
ances consist of a proposition and a modality (Nitta
et al, 1989). In this paper we present an algorithm
for extracting word associations from the Web and
a method for adding modality to statements. We
evaluate both the word associations and the use of
modality. We also suggest some future possible ex-
tensions of the system and show a small experiment
with adding humor to the system.
In this paper, the system described works for
Japanese and uses text as input and output. Though
the final goal of our research is to help developing
freely talking car navigation systems that by their
chatting abilities can help to avoid drowsiness while
driving and so on. in this part of the development we
concentrate on proposition generation and modality
processing. Therefore, we work only with text now.
We plan to combine this project with research on in
car voice recognition and generation.
2 Extracting Word Associations
In this chapter, we present a method for automatic
extraction of word associations based on keywords
from user utterances. We use the Google3 search
engine snippets to extract word associations in real
time without using earlier prepared resources, such
as off-line databases.
2.1 Extracting Word Associations from the
Web
In the first step, the system analyzed user utterances
using the morphological analyzer MeCab4 in order
to spot query keywords for extracting word associ-
ations lists. We define nouns, verbs, adjectives, and
unknown words as query keywords. The reason we
chose these word classes is that these word classes
can be treated as important and, to some extent, de-
scribe the context. We define a noun as the longest
set of nouns in a compound noun. For example,
the compound noun shizen gengo shori5 (natural
language processing) is treated by MeCab as three
words: (shizen - natural), (gengo - language) and
(shori - processing). Our system, however, threats
it as one noun.
In the next step, the system uses these keywords
as query words for the Google search engine. The
system extracts the nouns from the search results and
sorts them in frequency order. This process is based
on the idea that words which co-occur frequently
with the input words are of high relevance to them.
The number of extracted snippets is 500. This value
was set experimentally, taking the processing time
and output quality into account. The top ten words
of a list are treated as word associations, see Table 1
for an example.
3Google, http://www.google.co.jp/
4MeCab: Yet Another Part-of-Speech and Morphological
Analyzer, http://mecab.sourceforge.jp/
5All Japanese transcriptions will be written in italics.
383
Table 1: Examples of noun associations triggered by a
user utterance
Sapporo wa samui. (Sapporo city is cold.)
Association frequency ranking:
1 yuki (snow) 52
2 fuyu (winter) 50
3 kion (temperature) 16
4 jiki (season) 12
5 Tokyo (Tokyo) 12
6 tenki (weather) 11
7 chiiki (area) 10
8 heya (room) 10
2.2 Evaluation
We asked volunteers to use our system and to eval-
uate the correctness of word lists generated by the
system. First, a participant freely inputs an utter-
ance, for which the system retrieves ten association
words. Next, a participant rated these words using a
scale of one to three with 3 meaning ?perfectly cor-
rect?, 2 -?partially correct? and 1 - ?incorrect?. In
this experiment we consider words that receive a 2
or 3 as usable. The reason associations rated 2 or 3
are considered as usable is that the definition of what
makes a good word association here is difficult to
specify. When it comes to topic-free conversations
we have observed that associations have an effect
on a certain context. Three volunteers repeated the
experiment ten times, so the final amount of evalu-
ated words was 300. Table 2 shows the results of the
top 10 words, sorted by the frequency of appearance.
Table 3 shows the results of the top 5 words.
What constitutes a correct word association was
left to each volunteer to decide subjectively since in
a casual conversation setting associations are hard to
define strictly.
Table 2: Top 10 word associations
score participant(A?B?C) total
3 40?52?57 149
2 37?17?27 81
1 23?31?16 70
usability (%) 77?69?84 77
As shown in Table 2 approximately 77% of the
word associations were judged as usable but there
Table 3: Top 5 word associations
score participant?A?B?C? total
3 20?29?36 85
2 17?9?10 36
1 13?12?4 29
usability (%) 74?76?92 81
were individual differences between the evaluators.
This shows that the definition of word associations
is different for each participant. Table 3 shows that
approximately 80% of the word associations were
judged as usable. It is thus highly likely that the top
words from the frequency lists are correct associa-
tions. The results show that automatic extracting of
word associations using a Web search engine is fea-
sible. The main reason for extracting word associa-
tions from the Web is that thanks to this method, the
system can handle new information, proper names,
technical terms and so on. by using only the snip-
pets from the search engine. The word association
extraction takes no more than few seconds. For the
evaluation we used only nouns but we expect al-
though verbs and adjectives are often more abstract
than nouns, the word associations for them will im-
prove the results.
3 General Description of the System
The system generates replies in the following way:
? extraction of keywords from user utterance
? extraction of word associations from the Web
? generation of sentence proposition using the
extracted associations
? addition of modality to the sentence proposi-
tion
3.1 Extraction of Keywords from User
Utterances
The system applies morphological analysis to the
use utterances in the same way as described in sec-
tion 2.1 and extracts keywords based on part of
speech.
384
 
	 

 Detecting the Countability of English Compound Nouns Using 
Web-based Models 
Jing Peng 
Language Media Laboratory  
Hokkaido University 
Kita-14, Nishi-9, Kita-ku,  
Sapporo, JAPAN  
pj@media.eng.hokudai.ac.jp 
Kenji Araki 
Language Media Laboratory  
Hokkaido University 
Kita-14, Nishi-9, Kita-ku,  
Sapporo, JAPAN  
araki@media.eng.hokudai.ac.jp
 
Abstract 
In this paper, we proposed an approach 
for detecting the countability of English 
compound nouns treating the web as 
a large corpus of words. We classified 
compound nouns into three classes: 
countable, uncountable, plural only. 
Our detecting algorithm is based on 
simple, viable n-gram models, whose 
parameters can be obtained using the 
WWW search engine Google. The de-
tecting thresholds are optimized on the 
small training set. Finally we 
experimentally showed that our 
algorithm based on these simple 
models could perform the promising 
results with a precision of 89.2% on the 
total test set. 
1 Introduction 
In English, a noun can be countable or uncount-
able. Countable nouns can be "counted", they 
have a singular and plural form. For example: an 
apple, two apples, three apples. Uncountable 
nouns cannot be counted. This means they have 
only a singular form, such as water, rice, wine. 
Countability is the semantic property that de-
termines whether a noun can occur in singular 
and plural forms. We can obtain the information 
about countability of individual nouns easily 
from grammar books or dictionaries. Several 
researchers have explored automatically learn-
ing the countability of English nouns (Bond and 
Vatikiotis-Bateson, 2002; Schwartz, 2002; 
Baldwin and Bond, 2003). However, all the pro-
posed approaches focused on learning the 
countability of individual nouns. 
A compound noun is a noun that is made up 
of two or more words. Most compound nouns in 
English are formed by nouns modified by other 
nouns or adjectives. In this paper, we concen-
trate solely on compound nouns made up of only 
two words, as they account for the vast majority 
of compound nouns. There are three forms of 
compound words: the closed form, in which the 
words are melded together, such as ?songwriter?, 
?softball?, ?scoreboard?; the hyphenated form, 
such as ?daughter-in-law?, ?master-at-arms?; 
and the open form, such as ?post office?, ?real 
estate?, ?middle class?.  
Compound words create special problems 
when we need to know their countability. Ac-
cording to ?Guide to English Grammar and 
Writing?, the base element within the compound 
noun will generally function as a regular noun 
for the countability, such as  ?Bedrooms?. How-
ever this rule is highly irregular. Some uncount-
able nouns occur in their plural forms within 
compound nouns, such as ?mineral waters? (wa-
ter is usually considered as uncountable noun). 
The countability of some words changes when 
occur in different compound nouns. ?Rag? is 
countable noun, while ?kentish rag? is uncount-
able; ?glad rags? is plural only. ?Wages? is plu-
ral only, but ?absolute wage? and ?standard 
wage? are countable. So it is obvious that de-
termining countability of a compound noun 
should take all its elements into account, not 
consider solely on the base word.  
The number of compound nouns is so large 
that it is impossible to collect all of them in one 
103
dictionary, which also need to be updated fre-
quently, for newcoined words are being created 
continuously, and most of them are compound 
nouns, such as ?leisure sickness?, ?Green fam-
ine?.  
Knowledge of countability of compound 
nouns is very important in English text genera-
tion. The research is motivated by our project: 
post-edit translation candidates in machine 
translation. In Baldwin and Bond (2003), they 
also mentioned that many languages, such as 
Chinese and Japanese, do not mark countability, 
so how to determine the appropriate form of 
translation candidates is depend on the knowl-
edge of countability. For example, the correct 
translation for ?????1? is ?growing pains?, 
not ?growing pain?.  
In this paper, we learn the countability of 
English compound nouns using WWW as a 
large corpus. For many compound nouns, espe-
cially the relatively new words, such as genetic 
pollution, have not yet reached any dictionaries. 
we believe that using the web-scale data can be 
a viable alternative to avoid the sparseness prob-
lem from smaller corpora. We classified com-
pound nouns into three classes: countable (eg., 
bedroom), uncountable (eg,. cash money), plural 
only (eg,. crocodile tears). To detect which class 
a compound noun is, we proposed some simple, 
viable n-gram models, such as freq(N) (the fre-
quency of the singular form of the noun) whose 
parameters? values (web hits of literal queries) 
can be obtained with the help of WWW search 
engine Google. The detecting thresholds (a noun 
whose value of parameter is above the threshold 
is considered as plural only) are estimated on the 
small countability-tagged training set. Finally 
we evaluated our detecting approach on a test 
set and showed that our algorithm based on the 
simple models performed the promising results.  
Querying in WWW adds noise to the data, 
we certainly lose some precision compared to 
supervised statistical models, but we assume that 
the size of the WWW will compensate the rough 
queries. Keller and Lapata (2003) also showed 
the evidence of the reliability of the web counts 
for natural language processing. In (Lapata and 
Keller, 2005), they also investigated the count-
ability leaning task for nouns. However they 
                                                          
1 ??????(fa yu xing tong) which is Chinese compound 
noun means ?growing pains?. 
only distinguish between countable and un-
countable for individual nouns. The best model 
is the determiner-noun model, which achieves 
88.62% on countable and 91.53% on uncount-
able nouns.  
 In section 2 of the paper, we describe The 
main approach used in the paper. The prepara-
tion of the training and test data is introduced in 
section 3. The details of the experiments and 
results are presented in section 4. Finally, in sec-
tion 5 we list our conclusions. 
2  Our approach 
We classified compound nouns into three 
classes, countable, uncountable and plural only. 
In Baldwin and Bond (2003), they classified 
individual nouns into four possible classes. Be-
sides the classes mentioned above, they also 
considered bipartite nouns. These words can 
only be plural when they head a noun phrase 
(trousers), but singular when used as a modifier 
(trouser leg). We did not take this class into ac-
count in the paper, for the bipartite words is 
very few in compound nouns. 
 
C-noun 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Detecting processing flow 
For plural only compound noun, we assume 
that the frequency of the word occurrence in the 
plural form is much larger than that in the singu-
lar form, while for the uncountable noun, the 
frequency in the singular form is much larger 
than that in the plural form. The main processing 
flow is shown in Figure 1. In the figure, ?C-
noun? and ?Ns? mean compound noun and the 
plural form of the word respectively. 
?F(Ns)>>F(N)? means that the frequency of the 
plural form of the noun is much larger than that 
of the singular form. 
F(N)>>F(Ns)
plural only
uncountablity
countablity
F(Ns)>>F(N) Y
N
Y
N
104
Our approach for detecting countability is 
based on some simple unsupervised models.  
??
)(
)(
Nf
Nsf
             (1) 
In (1), we use the frequency of a word in the 
plural form against that in the singular form. ?  
is the detecting threshold above which the word 
can be considered as a plural only. 
??
),(
),(
Nsmanyf
Nmuchf
  (2) 
In (2), we use the frequency of a word in the 
singular form co-occurring with the determiner 
?much? against the frequency of the word in the 
plural form with many, if above ? , the word can 
be considered as uncountable word. (2) is used 
to distinguish between countable and uncount-
able compound nouns. 
??
),(
),(
isNf
areNsf
     (3) 
The model 3 that compares the frequencies 
of noun-be pairs (eg,. f(?account books are?), 
f(?account book is?) is used to distinguish plural 
only and countable compound nouns. 
With the help of WWW search engine 
Google, the frequencies (web hits) in the models 
can be obtained using quoted n-gram queries 
(?soft surroundings?). Although in Keller and 
Lapata (2002), they experimentally showed that 
web-based approach can overcome data sparse-
ness for bigrams, but the problem still exists in 
our experiments. When the number of pages 
found is zero, we smooth zero hits by adding 
them to 0.01. 
Countable compound nouns create some 
problems when we need to pluralize them. For 
no real rules exist for how to pluralize all the 
words, we summarized from ?Guide to English 
Grammar and Writing? for some trends. We 
processed our experimental data following the 
rules below.  
1. Pluralize the last word of the compound 
noun. Eg,. bedrooms, film stars. 
2. When ?woman? or ?man? are the modi-
fiers in the compound noun, pluralize 
both of the words. Eg,. Women-drivers. 
3. When the compound noun is made up as 
?noun + preposition (or prep. phrase)?, 
pluralize the noun. Eg,. fathers-in-law. 
4. When the compound noun is made up as 
?verb (or past participle) + adverb?, plu-
ralize the last word. Eg,. grown-ups, 
stand-bys. 
Although the rules cannot adapt for each 
compound noun, in our experimental data, all 
the countable compound nouns follow the rules. 
We are sure that the rules are viable for most 
countable compound nouns. 
Although we used Google as our search en-
gine, we did not use Google Web API service 
for programme realization, for Google limits to 
1000 automated queries per day. As we just 
need web hits returned for each search query, we 
extracted the numbers of hits from the web 
pages found directly.  
3 Experimental Data 
The main experimental data is from Webster?s 
New International Dictionary (Second Edition). 
The list of compound words of the dictionary is 
available in the Internet2. We selected the com-
pound words randomly from the list and keep 
the nouns, for the word list also mixes com-
pound verbs and adjectives with nouns together. 
We repeated the process several times until got 
our experimental data. We collected 3000 words 
for the training which is prepared for optimizing 
the detecting thresholds, and 500 words for the 
test set which is used to evaluate our approach. 
In the sets we added 180 newcoined compound 
nouns (150 for training; 30 for test). These rela-
tively new words that were created over the past 
seven years have not yet reached any dictionari-
es3.  
 
Countability Training set Test set 
Plural only 80 21 
Countable 2154 352 
Uncountable 766 127 
Total 3000 500 
Table 1. The make-up of the experimental data  
We manually annotated the countability of 
these compound nouns, plural only, countable, 
uncountable. An English teacher who is a native 
speaker has checked and corrected the annota-
tions.  The make-up of the experimental data is 
listed in Table 1. 
                                                          
2 The compound word list is available from http://www. 
puzzlers.org/wordlists/dictinfo.php. 
3 The new words used in the paper can be found in http: 
//www.worldwidewords.org/genindex-pz.htm 
105
4 Experiments and Results 
4.1 Detecting plural only compound nouns 
Plural only compound nouns that have not sin-
gular forms always occur in plural forms. The 
frequency of their singular forms should be zero. 
Considering the noise data introduced by search 
engine, we used model (1) and (3) in turn to de-
tect plural noun. We detected plural only com-
pound nouns with the following algorithm 
(Figure 2), which is used to distinguish between 
plural only and non-plural only compound. 
 
 
 
 
 
 
 
 
 
 
Figure 2. Detecting algorithm for plural only  
 The problem is how to decide the two 
thresholds. We preformed exhaustive search to 
adjust ? 1, ? 2 optimized on the training set. 
With 0  ? ? 1,? 2 20, all possible pair values 
are tried with the stepsize of 1. 
?
AB
Acall =Re                  (4) 
AC
Aecision =Pr             (5) 
callecision
callecisionscoreF
RePr
RePr2
+
??=?  (6) 
We use Recall and Precision to evaluate the 
performance with the different threshold pairs. 
The fundamental Recall/Precision definition is 
adapted to IE system evaluation. We borrowed 
the measures using the following definition for 
our evaluation. For one experiment with a cer-
tain threshold pair, A stands for the number of 
plural found correctly; AB stands for the total 
number of plural only compound nouns in train-
ing set (80 words); AC stands for the total num-
ber of compound nouns found. The Recall and 
Precision are defined in (4) and (5). We also 
introduced F-score when we need consider the 
Recall and Precision at the same time, and in the 
paper, F-score is calculated according to (6). 
Figure 3 shows the performance  evaluated 
by the three measures when ? 1=8 and 0 ?  
? 2? 10 with a stepsize of 1. We set ? 2 to 5 for 
the test later, and accordingly the values of Re-
call, Precision and F-score are 91.25%, 82.95% 
and 87.40% respectively. 
Recall/Precision/F-score
0%
20%
40%
60%
80%
100%
0 1 2 3 4 5 6 7 8 9 10
threshold
Ac
cu
ra
cy Recall
Precision
F-score
Figure 3. The Recall/Precision/F-score graph 
(? 1=8 and 0?  ? 2? 10) 
if  ( 1
)(
)( ??
Nf
Nsf
) 
    then plural only; 
else if ( 2
),(
),( ??
isNf
areNsf
)  
then plural only; 
else  
countable or uncountable; 
4.2 Detecting uncountable compound 
nouns 
Uncountable compound nouns that have not plu-
ral form always occur in singular form.  
 
 if ( N is not plural only) 
 
then if  ( 3
)(
)( ??
Nsf
Nf
)   
 
then uncountable;  
else if ( 4
),(
),( ??
Nsmanyf
Nmuchf
) 
 
 
    then uncountable; 
else  
countable;  
 
 
 
Figure 4. Detecting algorithm for uncountable 
compound nouns 
The algorithm detecting uncountable compound 
nouns is shown in Figure 4. Using model (1) and 
(2), we attempted to fully make use of the char-
acteristic of uncountable compound nouns, that 
is the frequencies of their occurrence in the sin-
gular forms are much larger than that in the plu-
ral forms.  
106
The method to obtain the optimal threshold 
? 3 and ? 4 is the same to 4.1. We set ? 3 to 24, 
? 4 to 2, and the values of Recall, Precision and 
F-score are 88.38%, 80.27% and 84.13% respec-
tively. 
4.3 Performance on the test suite 
We evaluated our complete algorithm with the 
four thresholds (? 1=8,? 2=5, ? 3=24, ? 4=2) 
on the test set, and the detecting results are 
summarized in Table 2. There are 352 countable 
compound nouns in our test set, then when clas-
sify all the test words as countable, we can at 
least get the accuracy of 70.4%. We used it as 
our baseline. The accuracy on the total test date 
is 89.2% that significantly outperforms the base-
line. For the 30 newcoined compound nouns, the 
detecting accuracy is 100%. This can be ex-
plained by their infrequence. Newcoined words 
are not prone to produce noise data than others 
just because they are not occurring regularly.  
 
 Correct Incorrect Recall Precision F-score 
Plural only 18 4 85.71% 81.81% 83.71% 
Countable 320 22 90.90% 93.57% 92.22% 
Uncountable 108 28 85.04% 79.41% 82.15% 
Total 446 54 89.2% 89.2% 89.2% 
Table 2. The accuracy on the test suit
5 Conclusion 
From the results, we show that simple 
unsupervised web-based models can achieve the 
promising results on the test data. For we 
roughly adjusted the threshold with stepsize of 1, 
better performance is expected with stepsize of 
such as 0.1.  
It is unreasonable to compare the detecting 
results of individual and compound nouns with 
each other since using web-based models, com-
pound nouns made up of two or more words are 
more likely to be affected by data sparseness, 
while individual nouns are prone to produce 
more noise data because of their high occur-
rence frequencies. 
Anyway using WWW is an exciting direc-
tion for NLP, how to eliminate noise data is the 
key to improve web-based methods. Our next 
step is aiming at evaluating the internet resource, 
distinguishing the useful and noise data. 
References 
Baldwin, Timothy and Francis Bond.2003. Learning 
the countability of English nouns from corpus data. 
In Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics. Sap-
poro, Japan, 463-470. 
Francis Bond and Caitlin Vatikiotis-Basteson. 202. 
Using an ontology to determine English countabil-
ity. In Proceeding of the 19th International confer-
ence on computational Linguistics (COLING 
2002), Taipei, Taiwan. 
Keller, F, Lapata, M. and Ourioupina, O. 2002. Us-
ing the web to overcome data sparseness. In Pro-
ceedings of the Conference on Empirical Methods 
in Natural Language Processing. Philadelphia. 
230-237.  
Keller, F and Lapata, M. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computa-
tional Linguistics 29, 3, 459-484. 
Lapata, M and Keller, F. 2004. The web as a baseline: 
Evaluating the performance of unsupervised web-
based models for a range of NLP tasks. In Pro-
ceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics. Boston,  
Lapata, M and Keller, F. Web-based Models for 
Natural Language Processing. To appear in 2005 
ACM Transactions on Speech and Language 
Processing. 
Lane O.B. Schwartz. 2002. Corpus-based acquisition 
of head noun countability features. Master?s thesis, 
Cambridge University, Cambridge, UK. 
Guide to English Grammar and Writing. http:// cctc. 
commnet.edu/grammar/
107
Proceedings of NAACL HLT 2007, Companion Volume, pages 189?192,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Modifying SO-PMI for Japanese Weblog Opinion Mining by Using a
Balancing Factor and Detecting Neutral Expressions
Guangwei Wang
Graduate School of Information
Science and Technology
Hokkaido University
Sapporo, Japan 060-0814
wgw@media.eng.hokudai.ac.jp
Kenji Araki
Graduate School of Information
Science and Technology
Hokkaido University
Sapporo, Japan 060-0814
araki@media.eng.hokudai.ac.jp
Abstract
We propose a variation of the SO-PMI al-
gorithm for Japanese, for use in Weblog
Opinion Mining. SO-PMI is an unsuper-
vised approach proposed by Turney that
has been shown to work well for English.
We first used the SO-PMI algorithm on
Japanese in a way very similar to Turney?s
original idea. The result of this trial leaned
heavily toward positive opinions. We then
expanded the reference words to be sets of
words, tried to introduce a balancing fac-
tor and to detect neutral expressions. After
these modifications, we achieved a well-
balanced result: both positive and negative
accuracy exceeded 70%. This shows that
our proposed approach not only adapted
the SO-PMI for Japanese, but also modi-
fied it to analyze Japanese opinions more
effectively.
1 Introduction
Recently, more and more websites add information
in the form of personal opinions to the Web, e.g.
customer reviews of products, forums, discussion
groups, and blogs. Here, we use the term Weblog for
these sites. This type of information is often useful.
However, we have to deal with an enormous amount
of unstructured and/or semi-structured data. These
data are subjective, in free format and mostly tex-
tual, thus using them is difficult and time consum-
ing. Therefore, how to mine the Weblog opinions
automatically more effectively has attracted more
and more attention (Gamon, 2005; Popescu, 2005;
Chaovalit, 2005).
Turney (2002) has presented an unsupervised
opinion classification algorithm called SO-PMI (Se-
mantic Orientation Using Pointwise Mutual Infor-
mation). The main use of SO-PMI is to estimate
the semantic orientation (i.e. positive or negative)
of a phrase by measuring the hits returned from a
search engine of pairs of words or phrases, based on
the mutual information theory. This approach has
previously been successfully used on English. The
average accuracy was 74% when evaluated on 410
reviews from Epinions1.
However, according to our preliminary experi-
ment, directly translating Turney?s original idea into
Japanese gave a very slanted result, with a positive
accuracy of 95% and a negative accuracy of only
8%. We found that the balance between the posi-
tive and negative sides is influenced greatly by the
page hits of reference words/sets, since a search en-
gine is used. Therefore, we introduced a balancing
factor according for the difference in occurrence be-
tween positive and negative words. And then we
added several threshold rules to detect neutral ex-
pressions. The proposed approach is evaluated on
200 positive and 200 negative Japanese opinion sen-
tences and yielded a well-balanced result.
In the remainder of this paper, we review the SO-
PMI Algorithm in Section 2, then adapt the SO-PMI
for Japanese and present the modifications in Sec-
tion 3. In section 4, we evaluate and discuss the
experimental results. Section 5 gives concluding re-
marks.
2 Details of the SO-PMI Algorithm
The SO-PMI algorithm (Turney, 2002) is used to es-
timate the semantic orientation (SO) of a phrase by
1http://www.epinions.com
189
measuring the similarity of pairs of words or phrases
using the following formula:
PMI(word1,word2)=log2
[
p(word1&word2)
p(word1)p(word2)
]
(1)
SO(phrase) = PMI(phrase,?excellent?)
?PMI(phrase,?poor?) (2)
The reference words ?excellent? and ?poor? are
used, thus SO is positive when a phrase is more
strongly associated with ?excellent? and negative
when a phrase is more strongly associated with
?poor?. Let hits(query) be the number of hits re-
turned when using a search engine, the following
estimate of SO can be derived from Formula (2) and
(1) with some minor algebraic manipulation.
SO(phrase) = log2 [A]
A = hits(phrase NEAR?excellent?)?hits(?poor?)hits(phrase NEAR?poor?)?hits(?excellent?) (3)
Turney used AltaVista2 search engine because it
has a NEAR operator. This operator constrains the
search to documents that contain the words within
ten words of one another, in either order. Turney?s
previous work has shown that NEAR performs bet-
ter than AND when measuring the strength of se-
mantic association between words.
3 Our Proposed Approach
The first step of our approach is to extract opin-
ion phrases using word POS (part of speech) tem-
plates based on our analysis of opinions in Japanese
Weblog and the results of related work (Kobayashi,
2003; Taku, 2002; Wang, 2006). The second step is
to estimate the semantic orientation of the extracted
phrases, using the SO-PMI algorithm.
3.1 Adapting SO-PMI for Japanese
Following Turney?s original idea, we first translated
the SO formula to the one shown in Formula (4) for
Japanese.
SO(phrase) = log2 [B] (4)
We used the Google search engine3 to get the
hits(query) even though Google does not have a
NEAR operator. The AltaVista NEAR operator does
not work well for Japanese and Google indexes more
2http://www.altavista.com/sites/search/adv
3http://www.google.co.jp/
pages than AltaVista, thus we used Google and re-
placed the NEAR operator with the AND operator in
the SO formula. ? ? and ? ? were se-
lected because they correspond to the English words
?excellent? and ?poor?.
For testing the performance of this trial, we used
200 positive and 200 negative Japanese opinion sen-
tences which have been labeled by hand. The re-
sults were very slanted. Many phrases, whether pos-
itive or negative in meaning, still received a posi-
tive SO. Some possible causes could be that ?
(poor)? has more hits than ? (excellent)?,
as shown in Table 1, and that the AND operator is
less useful than the NEAR operator.
3.2 Modifying SO-PMI for Japanese
In Japanese, there are many expressions when
people evaluate something. For example, ?
(good)?, ? (good)?, ? (satisfaction)? , ?
(excellent)? are usually used when some-
one wants to convey a positive opinion. Hence
we tried to replace the reference words ?excellent?
and ?poor? with two reference sets: ?p?basic? and
?n?basic?:
SO(phrase) = log2 [C]
C = hits(phrase ANDp?basic)?hits(n?basic)hits(phrase ANDn?basic)?hits(p?basic) (5)
?p?basic? is a set of common strong positive
words in Japanese. ?n?basic? is a set of common
weak negative words. The hit counts of these words
from Google is shown in Table 1 (All data from
2007/01/12). The hits(query) was calculated by
hits(phrase AND (? (good)? OR ? (like)?)
OR ? (good)? OR ...).
Table 1: Frequency of p?basic/n?basic words on the Web
2.57 26,000?????(excellent)
2.81 28,400?????(interesting)
5.89 59,500????(happy)
7.40 74,700??(lovely)
7.48 75,500???(happy)
7.89 79,700???(interesting)
7.98 80,600??(satisfaction)
9.59 96,900??(good)
10.20 103,000??(good)
10.59 107,000???(delightful)
11.39 115,000???(want)
11.39 115,000???(favorite) 
14.85 150,000??(charm)
20.89 211,000??(good)
23.96 242,000??(like)
36.83 372,000??(good)
R(%)Hits (K)p_basic words
1.02 10,300 ???(bad)
1.54 15,600 ???(not good) 
1.64 16,600 ??? (fault)
2.05 20,700 ??(worst)
2.20 22,200 ??(dissatisfaction)
2.58 26,100 ??(dissatisfaction)
2.62 26,500 ??(painful)
2.62 26,500 ??(useless)
3.67 37,100 ??(dislike)
3.75 37,900 ????(not good)
6.44 65,000 ?(dislike)
7.68 77,600 ???(hard)
7.71 77,900 ??(fault)
8.22 83,000 ??(worry)
10.89 110,000 ??(bad)
11.78 119,000 ??(poor)
R(%)Hits (K)n_basic words
We evaluated this modification using the same
190
data as in Section 3.1. We obtained a slightly bet-
ter result. However the SO values were still slanted.
This time many phrases, whether positive or nega-
tive in meaning, still received a negative SO. All of
these test results are shown in detail in Section 4.2.
In the experiments above, we obtained heavily
slanted results. We consider that the large differ-
ence in page hits between the positive and negative
reference words/sets are the main cause for this phe-
nomenon. To mitigate this problem, we decided to
introduce a balancing factor to adjust the balance be-
tween the positive and negative sides. The SO for-
mula was modified from (5) to (6).
SO(phrase) = log2 [C] + f (?) (6)
The balancing factor f(?) was calculated by For-
mula (7).
f (?) = ? ? log2
[hits(p?basic)
hits(n?basic)
]
(7)
The log2 of ?p?basic? and ?n?basic? is a fac-
tor that adjusts the balance of the similarity of
?p?basic?/?n?basic? and phrases automatically by
the hits of ?p?basic?/?n?basic? itself. ? is a weight
value. We evaluated different values of ? from ?0.0?
to ?1.0? on the benchmark dataset, which is shown
in detail in Section 4.2.
From these preliminary trials, we also found that
many neutral phrases often receive positive or neg-
ative SO. Therefore we added detection of neu-
tral expressions. The idea is that if the phrase is
strongly or faintly associated with both ?p?basic?
and ?n?basic?, it is considered a neutral phrase. Be-
cause this means that this phrase has an ambiguous
connection with both ?p?basic? and ?n?basic?. We
use the following rules (Figure 1) to separate neutral
phrases from positive/negative phrases. The thresh-
old values ta, tb and tc are obtained from a small,
hand-labeled corpus.
1. hits( phrase AND p_basic) > ta AND hits( phrase AND n_basic) > ta
2.  hits( phrase AND p_basic) < tb AND  hits( phrase AND n_basic) < tb
3.  | hits( phrase AND p_basic) ? hits( phrase AND n_basic) | < tc
4. SO( phrase ) = 0 
Figure 1: Rules for Detecting Neutral Expressions
4 Experimental Performance Evaluation
4.1 Gold Standard and Evaluation Metrics
As a gold standard, we collected a benchmark
dataset which has 200 positive opinion sentences
and 200 negative opinion sentences from the reviews
about Electronic Dictionary and MP3 Player prod-
ucts that have been labeled as either positive or neg-
ative reviews in ?Kakaku.com?4. ?Kakaku.com? is
the largest Japanese Weblog specializing in product
comparison of consumer goods, including price and
user opinions, etc. Lots of people exchange mis-
cellaneous product information and reviews. These
reviews are classified as questions, positive re-
views, negative reviews, rumors, sale information or
?other? category.
To classify a sentence as positive (P) or negative
(N), the average SO of the phrases in the sentence is
used. If the average SO is P, the sentence is a posi-
tive sentence; otherwise it is a negative sentence. As
evaluation metrics, we measured our proposed ap-
proach?s performance by accuracy. accuracy was
measured as the number of sentences correctly clas-
sified as P/N sentences to the total number of P/N
sentences in the benchmark dataset (200). PA means
positive accuracy, NA means negative accuracy, i.e.
the accuracy on only positive or negative sentences
respectively.
4.2 Experiments and Results
First we did the balancing factor experiment to
determine the value of ???, using the benchmark
dataset. The results are shown in Figure 2. (a)
and (b) show the dashed line indicates average ac-
curacy (74%) on English Data from Turney?s Study
(2002). Turney didn?t evaluate positive and nega-
tive accuracy respectively. The full drawn line indi-
cates the result after translating the original SO-PMI
to Japanese (PA:95%, NA: 8%). PA series (the line
with triangle mark)/NA series (the line with circle
mark) when values of ??? from ?0.0? to ?1.0? were
used.
Changing the ? tends to be a tradeoff, lowering
PA when NA is improved and vice versa. There-
fore, we used Harmonic?Mean by the following
formula to find a proper value of ???.
Harmonic?Mean = 2 ? PA ?NAPA+NA (8)
Figure 2, (c) shows PA, NA and
Harmonic?Mean curves for different values
4http://www.kakaku.com/
191
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
?
A
c
c
u
r
a
c
y
Positive Accuracy by Modified SO-PMI (PA)
Positive Accuracy by Original SO-PMI for Japanese
Accuracy on English Data from Turney's Study
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
?
A
c
c
u
r
a
c
y
Negative Accuracy by modified SO-PMI (NA)
Negative Accuracy by Original SO-PMI for Japanese
Accuracy on English Data from Turney's Study
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
?
A
c
c
u
r
a
c
y
Harmonic_Mean of PA & NA
Positive Accuracy by Modified SO-PMI (PA)
Negative Accuracy by Modified SO-PMI (NA)
(a) Positive Accuracy (PA) (b) Negative Accuracy (NA) (c) Harmonic-Mean of PA/NA
Figure 2: Experiment for ? in Balance Factor
of ???. We selected the ??=0.9? giving the highest
Harmonic?Mean value, thus giving a good
balance between PA (75%) and NA (70%).
The comparative experiment results between the
SO-PMI for Japanese (Test 1), and our modifications
(Test 2, 3, 4) are shown in Table 2.
Table 2: Comparative Experiment Results
7278Test 3 + Modification 3: Neutral Phrase DetectionTest 4
PA: Positive Accuracy         NA: Negative Accuracy 
Test 3
Test 2
Test 1
9912Modification 1: Two Reference Sets
7075Test 2 + Modification 2: Balancing Factor [? =  0.9] 
895Naive translation of Turney?s Approach for Japanese
NA(%)PA(%)Test Content
In Test 1 and 2, we obtained extreme results, lean-
ing to the positive or negative end, whether using the
Turney?s original approach or expanding the refer-
ence word as ?p?basic? and ?n?basic?. In Test 3,
we added a balancing factor as described in section
3.2, and obtained a comparatively well-balanced re-
sult. Finally, after adding the neutral expressions de-
tection, we achieved a PA of 78% and NA of 72%
(Test 4). The balance between positive and negative
sides was quite improved by contrast with Test 1 and
2.
5 Conclusions
This study first proposed a modified unsupervised
approach (SO-PMI) for Japanese Weblog Opinion
Mining. Some parts of Turney?s approach, such as
the NEAR operator, does not work for Japanese,
thus some modifications must be done. In a prelim-
inary experiment, the negative accuracy (8%) was
very poor while the positive accuracy (95%) was
high. To deal with this phenomenon, we presented
three modifications based on the characteristics of
Japanese and the results of related work. The ex-
periment results (positive accuracy: 78%, negative
accuracy: 72%) show that our proposal achieved
a considerably improved performance, comparing
with directly translating the SO-PMI. Hence it
would be expected that the balancing factor and neu-
tral expressions detection would work effectively
also for other reference words or languages. In the
future, we will evaluate different choices of words
for the sets of positive and negative reference words.
We also plan to appraise our proposal on other lan-
guages.
References
Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
Proceedings 40th Annual Meeting of the ACL, pp. 417-424.
Popescu, Ana-Maria, and Oren Etzioni. 2005. Extracting Prod-
uct Features and Opinions from Reviews. Proceedings of
HLT-EMNLP.
Michael Gamon, Anthony Aue, Simon Corston-Oliver and Eric
K. Ringger. 2005. Pulse: Mining Customer Opinions from
Free Text. Proceedings of the 2005 Conference on Intelligent
Data Analysis (IDA), pp.121-132.
Pimwadee Chaovalit and Lina Zhou. 2005. Movie Review Min-
ing: a Comparison between Supervised and Unsupervised
Classification Approaches. Proceedings of the 38th Annual
HICSS.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi and Toshikazu Fukushima. 2003. Collecting eval-
uative expressions by a text mining technique. IPSJ SIG
NOTE, Vol.154, No.12, In Japanese.
Taku Kudoh and Yuji Matsumoto. 2002. Applying Cascaded
Chunking to Japanese Dependency Structure Analysis. In-
formation Processing Society of Japan (IPSJ)Academic
Journals, Vol 43, No 6, pp. 1834-1842, In Japanese.
Guangwei Wang and Kenji Araki. 2006. A Decision Support
System Using Text Mining Technology. IEICE SIG Notes
WI2-2006-6, pp. 55-56.
192
NAACL HLT Demonstration Program, pages 19?20,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
OMS-J: An Opinion Mining System for Japanese Weblog Reviews Using a
Combination of Supervised and Unsupervised Approaches
Guangwei Wang
Graduate School of Information
Science and Technology
Hokkaido University
Sapporo, Japan 060-0814
wgw@media.eng.hokudai.ac.jp
Kenji Araki
Graduate School of Information
Science and Technology
Hokkaido University
Sapporo, Japan 060-0814
araki@media.eng.hokudai.ac.jp
Abstract
We introduce a simple opinion mining
system for analyzing Japanese Weblog re-
views called OMS-J. OMS-J is designed
to provide an intuitive visual GUI of opin-
ion mining graphs for a comparison of
different products of the same type to
help a user make a quick purchase de-
cision. We first use an opinion mining
method using a combination of supervised
(a Naive Bayes Classifier) and unsuper-
vised (an improved SO-PMI: Semantic
Orientation Using Pointwise Mutual In-
formation) learning.
1 Introduction
Nowadays, there are numerous Web sites containing
personal opinions, e.g. customer reviews of prod-
ucts, forums, discussion groups, and blogs. Here,
we use the term Weblog for these sites. How to ex-
tract and analyze these opinions automatically, i.e.
?Opinion Mining?, has seen increasing attention in
recent years.
This paper presents a simple opinion mining sys-
tem (OMS-J) for analyzing Japanese Weblog re-
views automatically. The novelty of OMS-J is two-
fold: First, it provides a GUI using intuitive visual
mining graphs aimed at inexperienced users who
want to check opinions on the Weblog before pur-
chasing something. These graphs can help the user
to make a quick decision on which product is suit-
able. Secondly, this system combines a supervised
and an unsupervised approach to perform opinion
mining. In related work (Chaovalit, 2005; Tur-
ney, 2002), both supervised and unsupervised ap-
proaches have been shown to have their pros and
cons. Based on the merits of these approaches and
the characteristics of Japanese (Kobayashi, 2003),
we proposed an opinion mining method using a
Naive Bayes Classifier (supervised approach) and an
improved SO-PMI method (unsupervised approach)
to perform different parts of the classification task
(Wang, 2006).
OMS-J implements Weblog opinion mining by
the steps shown in Figure 1. In the next section, we
describe the proposed system in detail.
1. Information Search
2. Weblog Content Extraction
3. Opinion Mining
DB
4. Mining Grap s GUI
User
Search Engine (Google), Keyword
Lynx (Text Browser)
Cabocha (Structure Analyzer)
Template content extraction
Feature Classification
Supervised Approach (Na?ve Bayes)
P/N Classification
Unsupervised Approach (SO-PMI)
Figure 1: System Flow
2 Proposed System
2.1 Information Search
The first step is information search. We used the
Google search engine1 to get al the information on
one product category or one specific product in the
Japanese weblog on the Internet. The search key-
word is the product category name or the product
name. The URL range of the search is restricted by
the URL type (e.g. blog, bbs, review).
2.2 Weblog Content Extraction
The Content Extraction step first analyzes the We-
blog content using a dependency structure analyzer
for Japanese, Cabocha2. Based on the syntactic
characteristics of Japanese reviews and the results
1http://www.google.co.jp/
2http://www.chasen.org/?taku/software/cabocha/
19
of related work (Kobayashi, 2003; Taku, 2002), we
designed the following templates to extract opinion
phrases:
< noun + auxiliary word + adj / verb / noun >
< adj + noun / undefined / verb >
< noun + verb >
< noun + symbol + adj / verb / noun >
Except the above < adj >
2.3 Opinion Mining
Opinion mining methods can usually be divided into
two types: supervised and unsupervised approaches.
Supervised approaches are likely to provide more
accurate classification results but need a training cor-
pus. Unsupervised approaches on the other hand re-
quire no training data but tend to produce weaker
results.
We propose a combined opinion mining method
by performing feature classification and P/N classi-
fication (Wang, 2006). The purpose of these classifi-
cations is to know what the opinion expresses about
a certain product?s features. Feature means a prod-
uct?s attribute, i.e. price, design, function or battery
feature. Based on our previous study, it is easy to
create a feature corpus. Therefore feature classifica-
tion is performed by a supervised approach, a Naive
Bayes Classifier. P/N classification classifies repu-
tation expressions into positive or negative meaning
using an unsupervised approach, SO-PMI. The SO-
PMI approach measures the similarity of pairs of
words or phrases based on the mutual information
theory, in our case the closeness of an opinion and
words for ?good? or ?bad?.
No human effort is required when mining a new
product or category. Only inputting the name of the
product or category is needed. It does however re-
quire quite a lot of processing time, since the SO-
PMI approach using a search engine is very time
consuming. Adding new features requires manual
work, since a small hand labeled training corpus is
used. Similar categories of products, for instance
cameras and mp3 players, use the same features
though, so this is not done very often.
2.4 Mining Graphs GUI
Finally, OMS-J provides a GUI with mining graphs
showing the opinion mining data in the database, as
shown in Figure 2. These graphs show the distribu-
tion of positive and negative opinions of each feature
type such as ?design?, and for each product. The
distribution of positive opinions among the different
product choices are shown in a pie chart, as is the
same for negative opinions. This GUI can also show
graphs for a single product?s mining results, show-
ing the positive/negative opinion distribution of each
feature.
Figure 2: OMS-J?s GUI Screenshot for One Product Category
3 Demonstration
During the demonstration, we will show that OMS-
J is an intuitive opinion mining system that can help
people to make a quick decision on purchasing some
product. OMS-J?s trial version has been developed
and tested with three kinds of products: Electronic
Dictionaries, MP3 Players and Notebook PCs. The
experiment results were positive. We will show how
the system works when a user wants to buy a good
MP3 player or wants to get a feel for the general
opinions on a specific Notebook PC etc.
References
Pimwadee Chaovalit and Lina Zhou. 2005. Movie Review Min-
ing: a Comparison between Supervised and Unsupervised
Classification Approaches. Proceedings of the 38th Annual
HICSS.
Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
Proceedings 40th Annual Meeting of the ACL, pp. 417-424.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi and Toshikazu Fukushima. 2003. Collecting eval-
uative expressions by a text mining technique. IPSJ SIG
NOTE, Vol.154, No.12.
Guangwei Wang and Kenji Araki. 2006. A Decision Support
System Using Text Mining Technology. IEICE SIG Notes
WI2-2006-6, pp. 55-56.
Taku Kudoh and Yuji Matsumoto. 2002. Applying Cascaded
Chunking to Japanese Dependency Structure Analysis. In-
formation Processing Society of Japan (IPSJ)Academic
Journals, Vol 43, No 6, pp. 1834-1842.
20
Proceedings of NAACL HLT 2009: Short Papers, pages 65?68,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluation of a System for Noun Concepts Acquisition from Utterances
about Images (SINCA) Using Daily Conversation Data
Yuzu UCHIDA
Graduate School of
Information Science and Technology
Hokkaido University
Sapporo, 060-0814, Japan
yuzu@media.eng.hokudai.ac.jp
Kenji ARAKI
Graduate School of
Information Science and Technology
Hokkaido University
Sapporo, 060-0814, Japan
araki@media.eng.hokudai.ac.jp
Abstract
For a robot working in an open environment,
a task-oriented language capability will not
be sufficient. In order to adapt to the en-
vironment, such a robot will have to learn
language dynamically. We developed a Sys-
tem for Noun Concepts Acquisition from ut-
terances about Images, SINCA in short. It is
a language acquisition system without knowl-
edge of grammar and vocabulary, which learns
noun concepts from user utterances. We
recorded a video of a child?s daily life to
collect dialogue data that was spoken to and
around him. The child is a member of a fam-
ily consisting of the parents and his sister. We
evaluated the performance of SINCA using
the collected data. In this paper, we describe
the algorithms of SINCA and an evaluation
experiment. We work on Japanese language
acquisition, however our method can easily be
adapted to other languages.
1 Introduction
There are several other studies about language ac-
quisition systems. Rogers et al (1997) proposed
?Babbette?, which learns language rules from pro-
vided examples. Levinson et al (2005) describe
their research with a robot which acquires language
from interaction with the real world. Kobayashi et
al. (2002) proposed a model for child vocabulary ac-
quisition based on an inductive logic programming
framework. Thompson (1995) presented a lexical
acquisition system that learns a mapping of words
to their semantic representation from training exam-
ples consisting of sentences paired with their seman-
tic representations.
As mentioned above, researchers are interested in
making a robot learn language. Most studies seem
to be lacking in the ability to adapt to the real world.
In addition, they should be more independent from
language rules. We believe that it is necessary to
simulate human language ability in order to create a
complete natural language understanding system.
As the first step in our research, we devel-
oped a System for Noun Concepts Acquisition from
utterances about Images, called SINCA in short
(which means ?evolution? in Japanese) (Uchida et
al., 2007). It is a language acquisition system with-
out knowledge of grammar and vocabulary, which
learns noun concepts from a user?s input. SINCA
uses images as a meaning representation in order to
eliminate ambiguity of language. SINCA can only
acquire concrete nouns.
Currently, SINCA is for Japanese only. The lan-
guage acquisition method of this system is very gen-
eral and it is independent of language rules. SINCA
is expected to work successfully using any language.
In this paper, we describe the algorithms of
SINCA and an experiment to test what kind of input
would be appropriate for our system. We would em-
phasize that we prepared a large video data of daily
life of a family with young children.
2 The Algorithms of SINCA
Figure 1 shows the SINCA user interface. The situ-
ation shown in Fig.1 is that the affection of SINCA
is directed to an eraser by the user, and after the
recognition process, SINCA asks ?KESHIGOMU?
65
Figure 1: The SINCA Interface recognizing an eraser
(Eraser?).?
We describe SINCA?s process in detail in the fol-
lowing subsections.
2.1 Input
A user input consists of an image paired with a spo-
ken utterance.
First, a user chooses an object O which he or she
likes and captures an image of it with a web camera
with 300,000 pixels effective sensor resolution. The
user has to try to capture the whole object O in the
image.
Next, a user imagines an utterance that an infant
might be exposed to when listening to caregivers
while gazing at the object O in the environment.
The user enters the utterance on the keyboard as a
linguistic input. The linguistic input is written in
Hiragana, which are Japanese phonetic characters,
to avoid the linguistic input containing some direct
meanings as in the case of Chinese Kanji ideograms.
This is also intended to standardize the transcrip-
tion. SINCA does not carry out morphological anal-
ysis of the linguistic input, because we believe that
infant capability for word segmentation is not per-
fect (Jusczyk et al, 1999).
Figure 2 shows some example inputs. 1
2.2 Image Processing
The ERSP 3.1 Software Development Kit 2 provides
cutting edge technologies for vision, navigation, and
1The Japanese words are written in italics in all following
figures.
2Evolution Robotics, Inc.:ERSP 3.1 Robotic Development
Platform OEM Software by Evolution Robotics
Kore-ha KAPPU-tte iu-n-da-yo.
(This is a thing called a cup.)
KAPPU-ni gyunyu ireyoka.
(Let?s pour some milk into the cup.)
Strings indicated by boldface are labels.
Figure 2: Examples of input data
system development. ERSP Vision included in the
ERSP enables a robot or device to recognize 2D and
3D objects in real world settings where lighting and
placement are not controlled. We use the ERSP vi-
sion for image processing. ERSP Vision informs the
system whether the object in the present input image
appears in the previously input images or not.
2.3 Common Parts
When a user inputs an image of an object O and
an utterance, the system extracts all sections of the
string matching section of previously input utter-
ances accompanied by the image of the same object
O. We call these strings common parts. After this
process, the system deals with them as candidates
for a label for the object O.
The system provides every common part with a
?basic score?. The basic score is based on frequency
of appearance and the number of characters, and in-
dicates how appropriate as a label the common part
is. The higher the score, the more appropriate the
common part is. The basic score is defined as fol-
lows:
SCORE = ?? FPN ?
?
L (1)
where, ? is a coefficient which reduces the basic
score if the common part has appeared with other
objects than O, F is frequency of appearance of the
common part with the images of O, PN is the num-
ber of use inputs with images of O, and L is the num-
ber of characters of the common part.
66
2.4 Output
If the system finds a common part whose basic score
exceeds a threshold, it outputs it as text. The reason
for doing this is the assumption that there is a high
possibility that such common parts are appropriate
as labels.
A user evaluates an output by choosing one of the
following keywords:
? Good : It is appropriate as a label.
? Almost : It makes some sense but is not
proper for the label.
? Bad : It makes no sense.
Infants cannot understand these keywords com-
pletely, but they can get a sense of some meanings
from the tone of an adult?s voice or facial expres-
sions. In our research, we use the keywords as a
substitute for such information. The system recalcu-
lates the basic score based on the keyword chosen by
the user. Specifically, the system multiplies the basic
score by the coefficient ? dependent on the keyword.
2.5 Acquisition of the Noun Concepts
After repeating these processes, if there is a com-
mon part whose score is more than 30.0 and which
has been rated as ?Good?, the system acquires the
common part as the label for O.
2.6 Label Acquisition Rules
Humans can use their newfound knowledge to learn
their native language effectively. This system imi-
tates humans? way with ?label acquisition rules?.
A label acquisition rule is like a template, which
enables recursive learning for acquisition of noun
concepts. The system generates label acquisition
rules after acquisition of a label. When the system
acquires a string S as a label for an object, the system
picks up the previous linguistic inputs with the im-
ages of the object which contain the string S. Then,
the system replaces the string S in the linguistic in-
puts with a variable ???. These abstracted sentences
are called label acquisition rules. An example of the
label acquisition rules is shown in Fig.3.
If the rules match other parts of previoiusly input
strings, the parts corresponding to the ??? variable
are extracted. The scores of these extracted strings
are then increased.
Acquired Label : WAN-CHAN (a doggy)
Previous Input : Acchi-ni WAN-CHAN-ga iru-yo.
(There is a doggy over there.)
Label Acquisition Rule : Acchi-ni ?1-ga iru-yo.
(There is ?1 over there.)
Strings indicated by boldface are labels.
Figure 3: An example of a label acquisition rule
3 Evaluation Experiment
We carried out an experiment to test what kinds of
input would be appropriate for SINCA. This section
describes the experiment.
3.1 Experimental Procedure
Two types of linguistic input data were collected
in two different ways: a questionnaire and a video
recording. We had SINCA acquire labels for 10 im-
ages using the linguistic input data. The following
are the details about the data collection methods.
3.1.1 Questionnaire
10 images were printed on the questionnaire, and
it asked ?What would you say to a young child if
he or she pays attention to these objects??. The re-
spondents are allowed to answer with whatever they
come up with. 31 people responded to this question-
naire, and 13 of them have children of their own.
We collected 324 sentences, and the average mora
length of them was 11.0.
3.1.2 Video recording
We recorded a video of a child?s daily life to col-
lect dialogue data that was spoken to and around
him. The child is a member of a family consisting
of his parents and his sister.
The recordings are intended to collect daily con-
versation, therefore we did not set any tasks. The
total recording period comprised 125 days and we
recorded about 82 hours of video data. The first au-
thor watched about 26 hours of the video data, and
wrote parents? dictation in Hiragana. We selected
353 sentences for linguistic input data that were spo-
ken when joint attention interactions between a par-
ent and a child were recognized. On average, their
mora length was 9.8.
67
3.2 Experimental Result
We input sentences from the collected inputs one at
a time until SINCA acquired a noun concept for an
image. SINCA was able to acquire labels for 10 im-
ages, with each type of linguistic input. When we
used the questionnaire data, SINCA needed on aver-
age 6.2 inputs to acquire one label, and SINCA ac-
quired 52 rules through the experiment. They cover
83.8% of the total number of inputs. When we used
the video data, SINCA needed on average 5.3 inputs
to acquire one label, and SINCA acquired 44 rules
through the experiment. They cover 83.0% of the
total number of inputs.
3.3 Considerations
The experimental results indicate that using video
data makes the acquisition of labels more efficient.
There are 3 factors that contribute to this.
The first factor is the number of one-word sen-
tences. There are 66 one-word sentences in the
video data (18.6% of the total). Therefore, the length
of the sentences from the video data tends to be
short.
The second factor is the lack of particles. The re-
spondents of the questionnaire hardly ever omit par-
ticles. By contrast, of the 53 sentences which were
input, 23 sentences lack particles (42.6% of the to-
tal) in video data. Spoken language is more likely
to have omitted particles compared with written lan-
guage.
The third factor is the variety of words. We ran-
domly selected 100 sentences from both sets of lin-
guistic input data and checked the words adjacent to
a label. Table 1 shows the number of different words
that occur adjacent to a label. Because the respon-
dents of the questionnaire all try to explain some-
thing in an image, they use similar expressions.
When SINCA uses the video data, it can extract
labels more easily than using the questionnaire data
because of the factors listed above. This means that
SINCA is well suited for spoken language. If we
assume one application of SINCA is for communi-
cation robots, this result is promising.
4 Conclusions and Future Work
In this paper, we described the algorithms of
SINCA. SINCA can acquire labels for images with-
Table 1: Variety of words
Previous(WA) following(WB)
Video 19 42
Questionnaire 15 22
Sentence : W1 W2 ... WA label WB ... .
out ready-made linguistic resources, lexical infor-
mation, or syntactic rules. Additionally, it targets
images of real world objects.
We collected linguistic input data in two ways.
One method is videos of a family?s daily life. The
other method is a questionnaire. We had SINCA ac-
quire noun concepts using both video and question-
naire data. As a result, we have showed that spoken
language is well suited to SINCA?s algorithm for ac-
quiring noun concepts.
In the next step, we will focus on acquisition of
adjectives.
References
Jusczyk, P. W. Houston, D. M. and Newsome, M. 1999.
The beginnings of word segmentation in english-
learning infants. Cognitive Psychology. 39. pp.159?
207.
Kobayashi, I. Furukawa, K. Ozaki, T. and Imai, M.
2002. A Computational Model for Children?s Lan-
guage Acquisition Using Inductive Logic Program-
ming. Progress in Discovery Science. 2281 pp.140?
155.
Levinson S. E. Squire, K. Lin, R. S. and McClain, M.
2005. Automatic language acquisition by an au-
tonomous robot. AAAI Spring Symposium on Devel-
opmental Robotics.
Rogers, P. A. P. and Lefley, M. 1997. The baby project.
Machine Conversations. ed. Wilks, Y. Kluwer Aca-
demic Publishers.
Thompson, C, A. 1997. Acquisition of a Lexicon from
Semantic Representations of Sentences. Proceedings
of the 33rd Annual Meeting of the Association for
Computational Linguistics. pp.335?337.
Uchida, Y. and Araki, K. 2007. A System for Acquisition
of Noun Concepts from Utterances for Images Using
the Label Acquisition Rules. Springer-Verlag Lecture
Notes in Artificial Intelligence (LNAI). pp.798?802.
68
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 5?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Is It Correct? - Towards Web-Based Evaluation of Automatic Natural
Language Phrase Generation
Calkin S. Montero and Kenji Araki
Graduate School of Information Science and Technology, Hokkaido University,
Kita 14-jo Nishi 9-chome, Kita-ku, Sapporo, 060-0814 Japan
 
calkin,araki  @media.eng.hokudai.ac.jp
Abstract
This paper describes a novel approach for
the automatic generation and evaluation of
a trivial dialogue phrases database. A tri-
vial dialogue phrase is defined as an ex-
pression used by a chatbot program as the
answer of a user input. A transfer-like ge-
netic algorithm (GA) method is used to
generating the trivial dialogue phrases for
the creation of a natural language genera-
tion (NLG) knowledge base. The auto-
matic evaluation of a generated phrase is
performed by producing n-grams and re-
trieving their frequencies from the World
Wide Web (WWW). Preliminary experi-
ments show very positive results.
1 Introduction
Natural language generation has devoted itself to
studying and simulating the production of writ-
ten or spoken discourse. From the canned text
approach, in which the computer prints out a
text given by a programmer, to the template ll-
ing approach, in which predetermined templates
are filled up to produce a desired output, the ap-
plications and limitations of language generation
have been widely studied. Well known applica-
tions of natural language generation can be found
in human-computer conversation (HCC) systems.
One of the most famous HCC systems, ELIZA
(Weizenbaum, 1966), uses the template filling ap-
proach to generate the system?s response to a user
input. For a dialogue system, the template filling
approach works well in certain situations, however
due to the templates limitations, nonsense is pro-
duced easily.
In recent research Inui et al (2003) have used
a corpus-based approach to language generation.
Due to its flexibility and applicability to open do-
main, such an approach might be considered as
more robust than the template filling approach
when applied to dialogue systems. In their ap-
proach, Inui et al (2003), applied keyword match-
ing in order to extract sample dialogues from a di-
alogue corpus, i.e., utterance-response pairs. Af-
ter applying certain transfer or exchange rules, the
sentence with maximum occurrence probability is
given to the user as the system?s response. Other
HCC systems, e.g. Wallace (2005), have applied
the corpus based approach to natural language ge-
neration in order to retrieve system?s trivial di-
alogue responses. However, the creation of the
hand crafted knowledge base, that is to say, a dia-
logue corpus, is a highly time consuming and hard
to accomplish task1. Therefore we aim to auto-
matically generate and evaluate a database of tri-
vial dialogue phrases that could be implemented as
knowledge base language generator for open do-
main dialogue systems, or chatbots.
In this paper, we propose the automatic gene-
ration of trivial dialogue phrases through the ap-
plication of a transfer-like genetic algorithm (GA)
approach. We propose as well, the automatic eval-
uation of the correctness2 of the generated phrase
using the WWW as a knowledge database. The
generated database could serve as knowledge base
to automatically improve publicly available chat-
bot3 databases, e.g. Wallace (2005).
1The creation of the ALICE chatbot database (ALICE
brain) has cost more that 30 researchers, over 10 years
work to accomplish. http://www.alicebot.org/superbot.html
http://alicebot.org/articles/wallace/dont.html
2Correctness implies here whether the expression is gram-
matically correct, and whether the expression exists in the
Web.
3Computer program that simulates human conversation.
5
2 Overview and Related Work
Figure 1: System Overview
We apply a GA-like transfer approach to au-
tomatically generate new trivial dialogue phrases,
where each phrase is considered as a gene, and the
words of the phrase represent the DNA. The trans-
fer approach to language generation has been used
by Arendse (1998), where a sentence is being re-
generated through word substitution. Problems of
erroneous grammar or ambiguity are solved by re-
ferring to a lexicon and a grammar, re-generating
substitutes expressions of the original sentence,
and the user deciding which one of the genera-
ted expressions is correct. Our method differs in
the application of a GA-like transfer process in
order to automatically insert new features on the
selected original phrase and the automatic eval-
uation of the newly generated phrase using the
WWW. We assume the automatically generated
trivial phrases database is desirable as a know-
ledge base for open domain dialogue systems. Our
system general overview is shown in Figure 1. A
description of each step is given hereunder.
3 Trivial Dialogue Phrases Generation:
Transfer-like GA Approach
3.1 Initial Population Selection
In the population selection process a small popu-
lation of phrases are selected randomly from the
Phrase DB4. This is a small database created be-
forehand. The Phrase DB was used for setting
the thresholds for the evaluation of the generated
phrases. It contains phrases extracted from real
human-human trivial dialogues (obtained from
the corpus of the University of South Califor-
nia (2005)) and from the hand crafted ALICE
4In this paper DB stands for database.
database. For the experiments this DB contained
15 trivial dialogue phrases. Some of those trivial
dialogue phrases are: do you like airplanes ?, have you
have your lunch ?, I am glad you are impressed, what are
your plans for the weekend ?, and so forth. The initial
population is formed by a number of phrases ran-
domly selected between one and the total number
of expressions in the database. No evaluation is
performed to this initial population.
3.2 Crossover
Since the length, i.e., number of words, among the
analyzed phrases differs and our algorithm does
not use semantical information, in order to avoid
the distortion of the original phrase, in our system
the crossover rate was selected to be 0%. This is
in order to ensure a language independent method.
The generation of the new phrase is given solely
by the mutation process explained below.
3.3 Mutation
During the mutation process, each one of the
phrases of the selected initial population is mu-
tated at a rate of  , where N is the total number
of words in the phrase. The mutation is performed
through a transfer process, using the Features DB.
This DB contains descriptive features of different
topics of human-human dialogues. The word ?fea-
tures? refers here to the specific part of speech
used, that is, nouns, adjectives and adverbs5 . In
order to extract the descriptive features that the
Feature DB contains, different human-human dia-
logues, (USC, 2005), were clustered by topic6 and
the most descriptive nouns, adjectives and adverbs
of each topic were extracted. The word to be re-
placed within the original phrase is randomly se-
lected as well as it is randomly selected the substi-
tution feature to be used as a replacement from the
Feature DB. In order to obtain a language indepen-
dent system, at this stage part of speech tagging
was not performed7 . For this mutation process, the
total number of possible different expressions that
could be generated from a given phrase is 
	 ,
where the exponent  is the total number of
features in the Feature DB.
5For the preliminary experiment this database contained
30 different features
6Using agglomerative clustering with the publicly avail-
able Cluto toolkit
7POS tagging was used when creating the Features DB.
Alternatively, instead of using POS, the features might be
given by hand
6
Total no Phrases Gen Unnatural Usable Completely Natural Precision Recall
Accepted Rejected Accepted Rejected Accepted Rejected Accepted Rejected
80 511 36 501 18 8 26 2 0.550 0.815
Total 591 Total 537 Total 26 Total 28
Table 3. Human Evaluation - Naturalness of the Phrases
3.4 Evaluation
In order to evaluate the correctness of the newly
generated expression, we used as database the
WWW. Due to its significant growth8, the WWW
has become an attractive database for differ-
ent systems applications as, machine translation
(Resnik and Smith, 2003), question answering
(Kwok et al, 2001), commonsense retrieval (Ma-
tuszek et al, 2005), and so forth. In our approach
we attempt to evaluate whether a generated phrase
is correct through its frequency of appearance in
the Web, i.e., the tness as a function of the fre-
quency of appearance. Since matching an entire
phrase on the Web might result in very low re-
trieval, in some cases even non retrieval at all, we
applied the sectioning of the given phrase into its
respective n-grams.
3.4.1 N-Grams Production
For each one of the generated phrases to evalu-
ate, n-grams are produced. The n-grams used are
bigram, trigram, and quadrigram. Their frequency
of appearance on the Web (using Google search
engine) is searched and ranked. For each n-gram,
thresholds have been established9 . A phrase is
evaluated according to the following algorithm10:
if ffEvaluation of Direct Speech Translation Method Using Inductive Learning
for Conversations in the Travel Domain
Koji MURAKAMI
Makoto HIROSHIGE
Kenji ARAKI
Graduate school of Engineering
Hokkaido University, Japan
{mura, hiro, araki}@media.eng.hokudai.ac.jp
Koji TOCHINAI
Graduate school of Business Administration
Hokkai Gakuen University, Japan
tochinai@econ.hokkai-s-u.ac.jp
Abstract
This paper evaluates a direct speech trans-
lation Method with waveforms using the
Inductive Learning method for short con-
versation. The method is able to work
without conventional speech recognition
and speech synthesis because syntactic ex-
pressions are not needed for translation in
the proposed method. We focus only on
acoustic characteristics of speech wave-
forms of source and target languages with-
out obtaining character strings from ut-
terances. This speech translation method
can be utilized for any language because
the system has no processing dependent
on an individual character of a specific
language. Therefore, we can utilize the
speech of a handicapped person who is
not able to be treated by conventional
speech recognition systems, because we
do not need to segment the speech into
phonemes, syllables, or words to realize
speech translation. Our method is real-
ized by learning translation rules that have
acoustic correspondence between two lan-
guages inductively. In this paper, we deal
with a translation between Japanese and
English.
1 Introduction
Speech is the most common means of communi-
cation for us because the information contained in
 	
  

  

	A Word Segmentation Method With Dynamic Adapting To Text
Using Inductive Learning
Zhongjian Wang
Hokuto System Co.,LTD
Oyachi Higashi1-3-23, Atubetuku, Sapporo, 004-0041 Japan
wang@hscnet.co.jp
Kenji Araki
Graduate School of Engineering, Hokkaido University
N13-W8, Kita-ku, Sapporo, 060-8628 Japan
araki@media.eng.hokudai.ac.jp
Koji Tochinai
Graduate School of Business Administration, Hokkai-Gakuen University,
Asahimachi 4-1-40, Toyohira-ku, Sapporo, 062-8605 Japan
Abstract
We have proposed a method of word segmen-
tation for non-segmented language using Induc-
tive Learning. This method uses only surface
information of a text, so that it has an ad-
vantage that is entirely not dependent on any
specific language. In this method, we consider
that a character string of appearing frequently
in a text has a high possibility as a word. The
method predicts unknown words by recursively
extracting common character strings. With the
proposed method, the segmentation results can
adapt to different users and fields. To evaluate
effectivety for Chinese word segmentation and
adaptability for different fields, we have done
the evaluation experiment with Chinese text of
the two fields.
1 Instruction
In NLP applications, word segmentation of non-
segmented language is a very necessary initial
stage(Sun et al, 1998). In the other hands,
with the development of the Internet and popu-
larization of computers, a large amount of text
information in different languages on the In-
ternet are increasing explosively, so it is nec-
essary to develop a common method to deal
with multi-language(Yamasita and Matsumoto,
2000). Furthermore, the standard of word seg-
mentation is dependent on a user and destina-
tion of use(Sproat et al, 1996), so that it is nec-
essary that word segmentation can adapt users,
can deal with multi languages.
In our method, we extract recursively a
common character string that occur frequently
in text and call it a common part. When
some common parts contain still same charac-
ter strings, furthermore we extract the same
character string as high dimensional common
parts and the remain parts is called different
parts. The high dimensional common parts
maybe have higher possibility as words because
it is extracted by multi steps. Those extracted
common parts and different parts are called
WS(Word Segment), and classified into some
ranks according to extracting condition. The
proposed method segments a non-segmented
sentence into words using the ranks of WS in
order of the higher value of the certainty de-
grees as words. When there are multiple seg-
mentation candidates, the system gets a list of
segmentable candidates, and picks a correct seg-
mentation candidate from the list by using a
value of LEF (Likelihood Evaluation Function,
Section 2.1) and so on. In addition, it is not
necessary to prepare a dictionary and any word
segmentation rules beforehand. A dictionary of
adapting to the user or the field is generated
with increasing of processed text. Because only
surface information of a text is used, it is possi-
ble the method is used to deal with general non-
segmented language. Here Inductive Learning
is the procedure to extract recursively WS by
multi steps(Araki et al, 1995).
2 Algrithm
Fig. 1 shows the outline of the proposed
method.
(1) Input sentences are segmented by word
candidates that were acquired in the dictionary
so far.
Segment by a update dictionary
Prediction over
No
Yes
Input sentences
Predict unknown words using 
Inductive Learning 
Segment by known words
Output segmentation results
Proofreading
Feedback processing
Segmentation
results
Corrected results
Recursively
prediction
Dictionary
Figure 1: Outline
(2) For the remaining part of the charac-
ter strings that are unsegmented by the known
words, the system predicts unknown words by
extracting WS using Inductive Learning.
The system extracts WS as word candidates.
This process is based on the supposition that a
common character string of appearing repeat-
edly in text has high probability as a word.
(3) The user judges whether the results of the
word segmentation is correct or not. If there are
errors in the result, the user will correct errors.
(4) The system compares the proofread re-
sults with the segmentation result to update
the information in the dictionary. Through this
procedure, the certainty of WS as a word is con-
firmed and increased.
Here, the WS those are used in correct seg-
mentation are called CW (Correct Word).
2.1 Segmentation by Known Words
Input sentence and then the system segments
it into words by registered CW and WS that
the system has got by using Inductive Learning
until that time.
(1) In the first step, the system compares the
registered CW or WS in the dictionary with
the character string in the input sentence from
the beginning of the sentence, and finds out
the same character strings with the registered
words. The system repeats this comparison pro-
cess until the end of the sentence is reached.
A list of segmentation candidate is established.
Then the system segments the sentence into
words.
(2) In the second step, however, for the char-
acter strings of multiple segmentations, we use
the registered candidates in order of their ranks
in the dictionary(Section 2.3). When there are
more than one word candidate with the same
rank, we decide the correct segmentation from
the list of segmentation candidates by the value
of LEF. We define LEF as follows:
LEF = FR + ?CS ? ?ES + ?LE
FR +CS ? ES + LE
(1)
Where: FR, CS, ES and LE are the fre-
quency of CW or WS appearing in the text, the
frequency of the correct segmentation, the fre-
quency of the erroneous segmentation and the
length of CW orWS respectively. ?, ? and ? are
coefficients. The optimum coefficients of LEF
are decided by the preliminary experiments us-
ing Greedy method, ?=10, ?=1 and ?=5.
The word that has the maximum value of
LEF is decided as the correct segmentation can-
didate.
(3) When LEF value of the set of possible
segmentations is equal to each other, the cor-
rect segmentation candidate is decided by the
word candidate that the value of ES is mini-
mum, the value of CS is maximum, the value of
FR is maximum, the value of LE is the longest
or the location of segmentation is the leftmost
in a sentence in turn.
2.2 Prediction for Unknown Words
Fig. 2 shows an example of a non-segmented
sentence. In this example, every character rep-
resents a Chinese character, so we use this ex-
ample to express a general sentence of non-
segmented language to present the proposed
method. Those words that are not registered in
the dictionary are predicted by using Inductive
Learning. After the sentences were segmented
by known words, which have been registered in
the dictionary, the unsegmented part of char-
acter string will be used to extract WS. The
prediction method is to find the common char-
acter string in text. The extraction procedure
is carried out as Fig. 3 shows: the extraction
of common parts, sift out the common part of
the most possibility as a word, the re-extraction
of common parts and the extraction of different
parts.
???????????????????
????????????????.
Figure 2: An example of non-segmented sen-
tence.
2.2.1 Extraction of a Common Part
A common part in non-segmented text is ex-
tracted by two steps:
(1) When a character string appears in text
frequently, we call it a common character string.
If the common character string consists of more
than two characters, we extract it as a word can-
didate and call it common part and represent it
by S1(Segment one). Here, we use length, fre-
quency and location of S1 in the sentence to sift
out it, to get the S1 of the most possible as a
word. At this step, we acquired S1 from the
sentence that is shown in Fig. 2: ??????,????
and ????????.
(2) When the character string appears in the
sentence only one times but meanwhile it is
included in other extracted common part and
made up by more than two characters, we also
extract it as a word candidate. For example in
Fig. 2: ????? is included in ????????. There-
fore ????? is extracted and belongs to S1.
2.2.2 Extraction of a High Dimensional
Common Part and a Different
Part
The extracted S1 at 2.2.1 may still include a
common character string. At this situation, the
common character string can be re-extracted
moreover from the extracted S1. We consider
it has a higher probability as a word that re-
extracted common parts at this procedure. The
conditions of re-extraction are presented as fol-
lows:
(1) The common part can be re-extracted
from the extracted S1 when it includes a com-
mon character string that is more than two
characters. For example, ???????? contains
????? which can be extracted from ????????,
so ???????(S1)? is equal to ????(S2)? +
????(S3)?.
The part of re-extraction is called high di-
mensional common part and represented by S2
(Segment two). The part of remain is called
different part and represented by S3 (Segment
three). The S1 is deleted from the dictionary
Extraction of high dimension 
common parts and different parts
Sift out common parts of the 
most possibility as words
Extraction of common parts
Get S1
Get S2 and S3
Unsegmented Character String
Figure 3: WS extraction procedure
when it is divided into S2 and S3.
(2) Furthermore one character can also be ex-
tracted as a word candidate when both sides
of it are extracted as a word candidate or
both sides were segmented by known words.
Like ??? in ????????????? is surrounded by
???????? and ??????, and ??? is extracted
as a word candidate belonging to S2.
The extraction procedure is carried out re-
peatedly until the new WS can not be extracted
and the input can not be segmented.
2.3 Segmentation by a Update
Dictionary
The extracted WS are classified to ?S1?, ?S2?,
and ?S3?. Those WS that are confirmed
as a word by proofreading process are called
?CW? (Correct Word). Furthermore, the
FR(appearing FRequency), CS(Correct Seg-
mentation frequency), ES(Erroneous Segmen-
tation frequency), LE(LEength) and rank of a
word candidate are rigestered simultaneously.
Word Segmentation is carried out by the up-
date dictionary as 2.1.
2.4 Feedback Process
After the system segments the sentence into
words, the results are judged whether they are
correct or not by the user. Then the user cor-
rects the errors if there are errors in the results.
The system updates the rank of the registered
CW andWS in the dictionary by comparing the
corrected results with the segmentation results.
And the system increases the priority degree of
the words that were used in correct segmenta-
tion and decreases the priority degree of words
that were used in erroneous segmentations. The
Table 1: Experimental results
Fields Economics Engineering Average
words 92,085 70,017 162,102
CSR[%] 87.50 90.80 89.44
ESR[%] 5.40 5.60 5.45
USR[%] 7.10 3.60 5.11
feedback process is described in detail as follows:
(1) For the Correct Segmentation Results:
 When the result of segmentation is correct,
the value of FR and CS of a word that is
used to segment are added one.
 If the rank of the words does not belong to
CW, it is changed to CW.
(2) For the Erroneous Segmentation Results:
 If the dictionary does not has the correct
words, the system registers the words in the
dictionary. In this case, their FRs are 1,
their ranks are CW.
 If the dictionary has the correct words, the
system adds one to the value of FR for a
word and changes the value of CL to CW
if it does not belong to CW.
 If the reason of erroneous segmentation is
that the erroneous word was used, then the
ES of erroneous word is added one.
(3) For the Unsegmented Parts:
 The system registers the words in the dic-
tionary, as FR of the words equal 1 and
rank equal CW.
3 Evaluation Experiments
3.1 Experimental Data And Procedure
To evaluate the adaptability of the proposed
method for different fields and the effectivity for
Chinese word segmentation. We use the Chi-
nese text of two specialized fields from Sinica
Corpus1: the economics contains 92,085 words
and the engineering contains 70,017 words. To-
tal words is 162,102. The economics consists of
the text of economic system, economic policy
and economic theory. The engineering consists
of the text of electronics, communication engi-
neering, machine engineering and nuclear indus-
try.
1http : //www.sinica.edu.tw/ftms ? bin/kiwi.sh
0
10
20
30
40
50
60
70
80
90
100
0 2 4 6 8 10 12 14 16
Se
gm
en
ta
tio
n 
Ra
te
(%
)
Number of Words
Economics Engineering
Correct Segmentation Rate
Unsegmentation Rate
Erroneous Segmentation Rate
(x10,000)
Figure 4: The changes of segmentation rates
In order to confirm the adaptability of pro-
posed method to user, we let the initial dic-
tionary empty. We input a paragraph about
hundred words one times and two fields text in
turns.
3.2 Experimental Results
The results of experiment are shown in Table
1. Fig. 4 shows the change of CSR, ESR and
USR. In our method, the correct segmentation
number is the number of correct segmentation
that is judged by a user. The unsegmentation
number is the number when all unsegmented
strings are segmented correctly. The erroneous
segmentation number is the number that sub-
tracts the number of correct segmentation and
unsegmentation from the number of all words
in the input text. To evaluate the experiment
result, we use these formulas of CSR (Correct
Segmentation Rate), ESR (Erroneous Segmen-
tation Rate) and USR (Unsegmented Rate) as
follows:
CSR[%] = Correct segmentationnumber
Total number of words
? 100 (2)
ESR[%] = Erroneous segmentation numberTotal number of words ? 100 (3)
USR[%] = UnsegmentationnumberTotal number of words ? 100 (4)
4 Discussion
4.1 Adaptability To Different Fields
Fig. 4 shows the experimental results of two
fields. When the text is changed to different
domain, because appearance of some new words
of different fields, the correct segmentation rate
010
20
30
40
50
60
70
80
90
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Pr
ec
is
io
n 
& 
Re
ca
ll
Number of words (x10,000)
Precosion
Recall
Figure 5: The ability to predict unknown words
is fall down temporary. However with increasing
of processed sentence, the correct segmentation
rate goes on increasing quickly.
We may consider that the proposed method
has adaptability for different fields. Sometimes
the correct segmentation rate is a little lower
because the domain of text is a little difference,
for example: the economics consists of the text
of economic system, economic policy and eco-
nomic theory and so on.
4.2 Evaluation of Ability for Predicting
Unknown Words
We use 50,000 words to discuss the predicting
ability of proposed method for unknown words.
Precision[%] = CWN
TWN
? 100 (5)
Recall[%] = CWN
TUN
? 100 (6)
Where, CWN is the number of words that are
predicted correctly. TWN is the total number
of words that are predicted. TUN is the total
number of unknown words.
The precision and recall are shown in Fig. 5.
The average precision is 26.0%. The average
recall is 31.0%. With increasing of registered
words in the dictionary, prediction effect for
unknown words is becoming well, after 40,000
words are processed the precision and the recall
are 85.0%, 40.0% respectively.
4.3 Analysis of Erroneous
Segmentation
We select 1,000 words from the beginning of the
experimental date and the end of the experi-
mental date respectively, to analysis the reason
of an erroneous segmentation. At the begin-
ning, ESR that is because of unregistered words
is 18.0%, but after 16,000 words are processed,
ESR that is because of unregistered words is
0.9%. However ESR that is caused by ambigu-
ity goes on increasing from 1.6% to 7.0%. ESR
caused by ambiguity is increasing with increas-
ing of registered word in the dictionary. Am-
biguous segmentation is still a difficult problem,
so that it is necessary to improve the ability to
deal with ambiguity.
5 Conclusion
The experiment results show the prediction
ability for unknown words by using Inductive
Learning. The experiment results of two fields
shown the proposed method can adapt to dif-
ferent fields text. In this paper, the emphasis is
to evaluate the adaptivity of the method to dif-
ferent user and fields. About comparison with
other existed methods will be done in the future.
The proposed method may be used to
computer-aided acquisition of language re-
source. The experimental results show our
proposed method has ability of learning, pre-
dictability for unknown words and effectivity
for Chinese word segmentation. For the future
works, we plan to improve the ability of dealing
with segmentation ambiguity, and use this pro-
posed method for Chinese morphological anal-
ysis.
References
Kenji Araki, Yoshio Momouchi, and Koji Tochi-
nai. 1995. Evaluation for adaptability of
kana-kanji translation of non-segmentation
japanese kana sentences using inductive
learning. PACLING-II, pages 1?7.
Richard Sproat, Chilin Shih, William Gale,
and Nancy Chang. 1996. A stochastic finite-
state word-segmentation algorithm for Chi-
nese. Association for Computational Linguis-
tics, 22(3):377?404.
Maosong Sun, Dayang Shen, and Benjamin K
Tsou. 1998. Chinese word segmentation
without using lexicon and hand-crafted train-
ing data. 17th International Conference on
Computational Linguistics, pages 1265?1271.
T. Yamasita and Y. Matsumoto. 2000. Journal
of natural language processing(in japanese).
Framework for Language Independent Mor-
phological Analysis, 7(3):39?56.
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 87?96,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
87
88
89
90
91
92
93
94
95
96
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 61?64,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Semi-supervised Algorithm for Human-Computer Dialogue Mining
Calkin S. Montero and Kenji Araki
Graduate School of Information Science and Technology
Hokkaido University, Kita 14-jo Nishi 9-chome
Kita-ku, Sapporo 060-0814 Japan
{calkin,araki}@media.eng.hokudai.ac.jp
Abstract
This paper describes the analysis of weak
local coherence utterances during human-
computer conversation through the appli-
cation of an emergent data mining tech-
nique, data crystallization. Results reveal
that by adding utterances with weak local
relevance the performance of a baseline
conversational partner, in terms of user
satisfaction, showed betterment.
1 Introduction
Data mining can be defined as the process of find-
ing new and potentially useful knowledge from data.
An enhanced trend of data mining is chance discov-
ery, which in spite of being an emergent field of re-
search has been applied to different branches of sci-
ence. Recently, data crystallization (Ohsawa, 2005)
has been proposed as a chance discovery extension
devoted to find desired unobservable events within
a given data. This method of data mining has been
intended for revealing events that are significant but
are not included in the analyzed data (sparseness).
The knowledge database of a conversational part-
ner computer program is a good example of data
sparseness, making very difficult to realize a human-
like conversation between a user and a computer,
since the database is missing relevant or coherent
answers for a given user input, e.g., ELIZA sys-
tem (Weizenbaum, 1966), corpus-based dialogue
systems (Inui et al, 2003), and so forth. Herring
(1999) noticed the weak sequential coherence dur-
ing human-computer interaction that is caused by
weak local relevance, pointing out the violation of
the Gricean maxim of local relevance during the di-
alogue. However, we argue that conversational re-
sponses - often of very weak local relevance - gener-
ated by a conversational partner computer program
(or ?chatbot?) could successfully simulate human-
like coherent dialogue (see example given by (Her-
ring, 1999), where topical coherence is stated in
spite of violation of local relevance Gricean maxim).
Moreover, we argue that weak local relevance utter-
ances could be regarded as topic-shifting utterances.
These utterances are vital in order to keep the flow of
the dialogue and its overall coherence. Determining
the moment when the topic-shifting utterances are
needed is hence essential to the success of a chatbot.
In this paper KeyGraph and data crystallization
techniques are applied to analyze human-computer
trivial dialogue, chat, considering as chances to dis-
cover the missing topic-shifting utterances, used to
improve the overall sequential coherence and the
flow of the computer dialogue.
2 Human-Computer Conversation
Analysis Approach
During the process of dialogue mining, in order to
analyze the behavior of human-computer conversa-
tional interaction, the interrelationship of utterances
of a dialogue section was obtained. The following is
a fragment of an analyzed dialogue section between
a user and ALICE chatbot (Wallace, 2005):
[1] User: hi
[2] Chatbot: hello there! what is your name?
[3] User: Joe
[4] Chatbot: Ok, I will call you Joe
61
[5] User: what should I call you?
[6] Chatbot: my name is Alice
[7] User: cool [....]
(Conversation continues)
[17] User: what do you mean by robot? there are lots of differ-
ent robots
[18] Chatbot: It is a good question. Have you tried a web
search? Would you like to know more?
[19] User: sure
[20] Chatbot: Ok let?s do it. What would you like to know?
[21] User: what Alice thinks a robot means
[22] Chatbot: is that a rhetorical question? I would search for it
[23] User: when I hear robot, I think of something that moves
[24] Chatbot: you never know what to expect from life. Why?
oh yeah, we are talking about something that moves [....]
For the analysis, the application of data crystal-
lization is introduced in order to identify the miss-
ing topic-shifting utterances. The interrelationship
of utterances (user-chatbot) is represented in a co-
occurrence document created as described below.
2.1 The KeyGraph
The KeyGraph has been used as a data-mining tool
for extracting patterns of the appearance of chance
events (Ohsawa et al(2003)). The KeyGraph iden-
tifies relationships between terms in a document
particularly focusing on co-occurrence relationships
of both high-probability and low-probability events.
Montero et al (2005) have applied this tool for ana-
lyzing the dynamic behavior of human-human chat,
identifying criticality.
In this paper the KeyGraph is applied in combi-
nation with data crystallization in order to visual-
ized utterances that do not appear during human-
computer chat. The interrelationship of utterances
(user-chatbot) is represented in a co-occurrence doc-
ument created by the following algorithm: a) Each ut-
terance (from both, the user and the chatbot) was considered as
one sentence. b) Each sentence was segmented into words. c)
High frequency words were eliminated, i.e., I, you, is, follow-
ups and the like, as to avoid false co-occurrence. d) A vectorial
representation of each sentence (at word level) was obtained and
sentences co-occurrence relationship was determined as1:
D= w1:: S1, S2, S4 .../ w2:: S9, S25 .../
w3:: S1, S3, S10 .../ ... / wn:: S24, S25, ... Sm
1Since follow-ups were eliminated, the number of sentences
in D might be smaller than the actual number of sentences in
the dialogue.
where: wk (k = 1, 2, 3, ..., n), represents a word in a sentence.
Sl (l = 1, 2, 3, ..., m), represents a sentence.
Then it could be said that the obtained D docu-
ment contains the co-occurrence relationship of the
utterances during the analyzed dialogue section. In
the graph, the most frequent items in D are shown
as black nodes and the most strongly co-occurring
item-pairs are linked by black lines according to the
Jaccard coefficient:
J(Sx, Sy) = p(Sx ? Sy)/p(Sx ? Sy)
where p(Sx ? Sy) is the probability that both el-
ements Sx and Sy co-occur in a line in D, and
p(Sx ? Sy) is the probability that either Sx or Sy
appears in a line. In the graph, nodes are interpreted
as sentences (from D) and clusters of nodes as par-
ticular topics (Figure 1).
2.2 Data Crystallization
Data crystallization (Ohsawa, 2005), is dedicated to
experts working in real domains where discoveries
of events that are important but are not included in
the analyzed data are desired. The process of data
crystallization involves to insert dummy items in
the given data in order to represent unobservable
events. In this paper, each dummy item inserted in
the D document (one in each vector of D) is named
XY , where X represents the level of the insertion
and Y represents the line where the dummy item
was inserted. The KeyGraph is applied to the new D
document and all of the dummy nodes that did not
appear linking clusters in the graph are eliminated
from the data, and then the cycle is iterated to higher
levels. In the case of the D document of Sec.2.1,
after the first level of insertion it becomes:
D?= w1:: S1, S2, S4 ...11 / w2:: S9, S25 ...12 /
w3:: S1, S3, S10 ...13 / ... / wn:: S24, S25, ... Sm 1n
where 1 o (o = 1, 2, 3, ..., n), represents each dummy item
inserted in each vector of D.
After feeding the KeyGraph with D?, all the
dummy items that did not appear linking clusters as
bridges in the outputted graph are deleted. At this
point new dummy items with higher hierarchy (2x)
are inserted in D?, and the cycle iterates. Unobserv-
able events and their relations with other events are
to be visualized by the application of KeyGraph iter-
atively to the data that is been crystallized (Figure 2).
62
Figure 1: User-Computer Chat Graph Figure 2: Crystallized Data Graph
3 Experiment and Visual Results
The performed experiment was carried out in three
stages. In the first stage of the experiment, three dif-
ferent dialogue sections (including the one shown in
Sec.2) between three native English speakers and a
chatbot (Wallace, 2005) were analyzed in order to
find co-occurrence between the users? utterance and
the chatbot replies, i.e., D document. This D docu-
ment was then examined by the KeyGraph (unsuper-
vised process). Figure 1 shows the graphical view of
the dialogue in Sec.2 (48 turns, user - chatbot, in to-
tal). A characteristic of the KeyGraph is the visual-
ization of co-occurring events by means of clusters.
In Figure 1, the nodes represent sentences from the
D document, the clusters represent the relationship
among those sentences, i.e., a specific topic, and the
nodes that link the clusters represent the transition
from one topic to the next. It can be observed that
the main clusters are not interconnected, leading to
the conclusion that the chatbot in many cases could
not keep a smooth and natural flow of the dialogue.
In the second stage of the experiment, a crystal-
lized document of utterance co-occurrence, i.e., D?
document, was obtained for the same dialogue sec-
tions, following the process described in Sec.2.2.
The graphical output of the dialogue in Sec.2, af-
ter crystallization, can be observed in Figure 2. It
can be seen in this figure how the two main clusters
appear to be interconnected by the dummy item 1 3.
Although this dummy item was inserted in the third
line of the D document, it appears in the graph con-
necting the two main clusters. The dummy item 1 3
branches from utterance [24]. This interconnecting
point can be regarded as the system considering it
appropriate to insert a topic-shifting utterance at this
point of the conversation. In doing so, a well in-
terconnected graph is obtained (Figure 2). This in-
formation is valuable for making the chatbot to ask
?intelligent questions? as a mean of conversational
responses to keep the interest from the user.
In the third stage of the experiment, the informa-
tion yielded by the previous analysis, i.e., regard-
ing the timing where a topic-shifting utterance might
be needed, was used to feed the chatbot database.
Topic-shifting responses were inserted by hand (su-
pervised process) as general patterns (around one
hundred patterns) for smoothly change the topic
when there is not a pattern that matches a given ut-
terance. In this way a bridge, represented in Figure
2 by the dummy item, is created giving to the di-
alogue the desired smoothness. Seven users (four
native English speakers, three non native speakers)
were requested to perform a chat with the plain chat-
bot and with the enhanced chatbot (the users did
not know which chatbot was plain or which was en-
hanced). The time set up was maximum 30 minutes-
chat with each program, the user was free to stop
at any time before the time limit. The evaluation of
the chatbots performances was made through a ques-
63
Turns % V.R.* Accuracy/relevance
Baseline 74 21.11% fair
Enhanced 128 7.37% good
V.R.* = Vague Reply (This table contains the average of Turns and VR)
Table 1: Chatbots Overall Performance
tionnaire given to the users. In the questionnaire the
degree of satisfaction of the user was surveyed based
on the accuracy/relevance of the chatbot responses
and its ability to keep the dialogue flowing. The ac-
curacy/relevance had a ranking scale: 1. excellent, 2.
good, 3. average, 4. fair, and 5. poor, while the user satis-
faction ranked : 1. very satisfied, 2. satisfied, 3. not sure, 4.
dissatisfied, and 5. very dissatisfied.
The four native English speaker users globally
agreed ranking enhanced chatbot as having ?good?
accurate/relevant responses during the overall dia-
logue, giving remarks like ?interesting to talk with?,
?better at keeping the conversation going?, ?easier
to talk to?, and ?more impression of consciousness?.
In the case of the plain chatbot, the native speakers
showed ?dissatisfied? at its performance, giving re-
marks like ?loop responses?, ?slow in keeping the
conversation going?, ?no so nice responses?, ?no
constant memory?. Table 1 shows a resume of the
average performance of the chatbot for all of the
users. An example of a vague reply is given in the
following fragment:
Chatbot: What is like living in Sapporo?
User: sweet as
Chatbot: In the context of living in Sapporo I don?t under-
stand ?what is like living in Sapporo?? [vague reply]
Two non native English speaker users ranked the
enhanced chatbot as having ?fair? and ?average?
accurate/relevant responses while the plain chat-
bot was ranked as having ?poor? and ?fair? accu-
rate/relevant responses. The third non native En-
glish speaker user ranked both chatbots as ?poor?
due to ?the chatbots lack of understanding deixis,
and anaphor?.
As a mean of discussion, in Figure 2 it could be
expected that the dummy item 1 3 would branch
from utterance [25] {User: no, you asked me who is the
best robot}, which is in the same cluster with utterance
[24]. However, under closer examination it becomes
clear that utterance [24] has stronger co-occurrence
with utterance [38] {Chatbot: I know you are but what am
I} than utterance [25]. Hence, the algorithm suggests
to link the clusters via utterance [24].
In other aspect, based on the feedback given by
the seven users of the experiment, the overall per-
formance of the enhanced chatbot can be considered
better than the plain chatbot. It is worth noticing
that the evaluation of the non native English speaker
users tended to emphasize the grammatical aspect
of the chatbots responses. On the other hand, the
evaluation of the native English speaker users tended
to emphasize the smoothness of the dialogue. Al-
though there is still plenty of room for improve-
ment and research a betterment in the chatbot per-
formance could be seen through this approach.
4 Conclusion
In this paper the application of a novel data mining
method, data crystallization, for visualizing missing
topic-shifting utterances during human-computer
chat has been described. Based on this informa-
tion, during the experiment, the use of weak local
relevance utterances, i.e., topic-shifting responses,
despite of violation of Grecian maxim of local rel-
evance, showed to meliorate the overall dialogue
flow. Future research will be oriented to the ex-
tended implementation of the obtained results for
enhancing the chat flow modeling of a conversa-
tional partner program.
References
Susan Herring. 1999. Interactional coherence in cmc. Journal
of Computer Mediated Communication, 4(1).
Nobuo Inui, Takuya Koiso, Junpei Nakamura, and Yoshiyuki
Kotani. 2003. Fully corpus-based natural language dia-
logue system. In Natural Language Generation in Spoken
and Written Dialogue, AAAI Spring Symposium.
Yukio Ohsawa and Peter McBurney, editors. 2003. Chance
Discovery. Springer, Berlin Heidelberg New York.
Yukio Ohsawa. 2005. Data crystallization: Chance discovery
extended for dealing with unobservable events. New Mathe-
matics and Natural Computation, 1(3):373?392.
Calkin S Montero and Kenji Araki. 2005. Human chat and self-
organized criticality: A chance discovery application. New
Mathematics and Natural Computation, 1(3):407?420.
Richard Wallace. 2005. A.l.i.c.e. artificial intelligence founda-
tion. http://www.alicebot.org.
Joseph Weizenbaum. 1966. Eliza a computer program for the
study of natural language communication between man and
machine. Commun. ACM, 9(1):36?45.
64
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 108?117,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation Method for Machine Translation using
Noun-Phrase Chunking
Hiroshi Echizen-ya
Hokkai-Gakuen University
S 26-Jo, W 11-chome, Chuo-ku,
Sapporo, 064-0926 Japan
echi@eli.hokkai-s-u.ac.jp
Kenji Araki
Hokkaido University
N 14-Jo, W 9-Chome, Kita-ku,
Sapporo, 060-0814 Japan
araki@media.eng.hokudai.ac.jp
Abstract
As described in this paper, we propose
a new automatic evaluation method for
machine translation using noun-phrase
chunking. Our method correctly deter-
mines the matching words between two
sentences using corresponding noun
phrases. Moreover, our method deter-
mines the similarity between two sen-
tences in terms of the noun-phrase or-
der of appearance. Evaluation experi-
ments were conducted to calculate the
correlation among human judgments,
along with the scores produced us-
ing automatic evaluation methods for
MT outputs obtained from the 12 ma-
chine translation systems in NTCIR-
7. Experimental results show that
our method obtained the highest cor-
relations among the methods in both
sentence-level adequacy and fluency.
1 Introduction
High-quality automatic evaluation has be-
come increasingly important as various ma-
chine translation systems have developed. The
scores of some automatic evaluation meth-
ods can obtain high correlation with human
judgment in document-level automatic evalua-
tion(Coughlin, 2007). However, sentence-level
automatic evaluation is insufficient. A great
gap exists between language processing of au-
tomatic evaluation and the processing by hu-
mans. Therefore, in recent years, various au-
tomatic evaluation methods particularly ad-
dressing sentence-level automatic evaluations
have been proposed. Methods based on word
strings (e.g., BLEU(Papineni et al, 2002),
NIST(NIST, 2002), METEOR(Banerjee and
Lavie., 2005), ROUGE-L(Lin and Och, 2004),
and IMPACT(Echizen-ya and Araki, 2007))
calculate matching scores using only common
words between MT outputs and references
from bilingual humans. However, these meth-
ods cannot determine the correct word corre-
spondences sufficiently because they fail to fo-
cus solely on phrase correspondences. More-
over, various methods using syntactic analyt-
ical tools(Pozar and Charniak, 2006; Mutton
et al, 2007; Mehay and Brew, 2007) are pro-
posed to address the sentence structure. Nev-
ertheless, those methods depend strongly on
the quality of the syntactic analytical tools.
As described herein, for use with MT sys-
tems, we propose a new automatic evaluation
method using noun-phrase chunking to obtain
higher sentence-level correlations. Using noun
phrases produced by chunking, our method
yields the correct word correspondences and
determines the similarity between two sen-
tences in terms of the noun phrase order of ap-
pearance. Evaluation experiments using MT
outputs obtained by 12 machine translation
systems in NTCIR-7(Fujii et al, 2008) demon-
strate that the scores obtained using our sys-
tem yield the highest correlation with the hu-
man judgments among the automatic evalua-
tion methods in both sentence-level adequacy
and fluency. Moreover, the differences be-
tween correlation coefficients obtained using
our method and other methods are statisti-
cally significant at the 5% or lower signifi-
cance level for adequacy. Results confirmed
that our method using noun-phrase chunking
is effective for automatic evaluation for ma-
chine translation.
2 Automatic Evaluation Method
using Noun-Phrase Chunking
The system based on our method has four pro-
cesses. First, the system determines the corre-
108
spondences of noun phrases between MT out-
puts and references using chunking. Secondly,
the system calculates word-level scores based
on the correct matched words using the deter-
mined correspondences of noun phrases. Next,
the system calculates phrase-level scores based
on the noun-phrase order of appearance. The
system calculates the final scores combining
word-level scores and phrase-level scores.
2.1 Correspondence of Noun Phrases
by Chunking
The system obtains the noun phrases from
each sentence by chunking. It then determines
corresponding noun phrases between MT out-
puts and references calculating the similarity
for two noun phrases by the PER score(Su et
al., 1992). In that case, PER scores of two
kinds are calculated. One is the ratio of the
number of match words between an MT out-
put and reference for the number of all words
of the MT output. The other is the ratio of the
number of match words between the MT out-
put and reference for the number of all words
of the reference. The similarity is obtained as
an F -measure between two PER scores. The
high score represents that the similarity be-
tween two noun phrases is high. Figure 1
presents an example of the determination of
the corresponding noun phrases.
MT output :
in general , [NP the amount ] of [NP the crowning fall ] 
is large like [NP the end ] .
Reference :
generally , the closer [NP it ] is to [NP the end part ] , 
the larger [NP the amount ] of [NP crowning drop ] is .
(1) Use of noun phrase chunking
MT output :
in general , [NP the amount ] of [NP the crowning fall ] 
is large like [NP the end ] .
Reference :
generally , the closer [NP it ] is to [NP the end part ] , 
the larger [NP the amount ] of [NP crowning drop ] is .
(2) Determination of corresponding noun phrases
1.0000
0.3714
0.7429
Figure 1: Example of determination of corre-
sponding noun phrases.
In Fig. 1, ?the amount?, ?the crowning fall?
and ?the end? are obtained as noun phrases
in MT output by chunking, and ?it?, ?the end
part?, ?the amount? and ?crowning drop? are
obtained in the reference by chunking. Next,
the system determines the corresponding noun
phrases from these noun phrases between the
MT output and reference. The score between
?the end? and ?the end part? is the highest
among the scores between ?the end? in the
MT output and ?it?, ?the end part?, ?the
amount?, and ?crowning drop? in the refer-
ence. Moreover, the score between ?the end
part? and ?the end? is the highest among the
scores between ?the end part? in reference
and ?the amount?, ?the crowning fall?, ?the
end? in the MT output. Consequently, ?the
end? and ?the end part? are selected as noun
phrases with the highest mutual scores: ?the
end? and ?the end part? are determined as one
corresponding noun phrase. In Fig. 1, ?the
amount? in the MT output and ?the amount?
in reference, and ?the crowning fall? in the
MT output and ?crowning drop? in the ref-
erence also are determined as the respective
corresponding noun phrases. The noun phrase
for which the score between it and other noun
phrases is 0.0 (e.g., ?it? in reference) has no
corresponding noun phrase. The use of the
noun phrases is effective because the frequency
of the noun phrases is higher than those of
other phrases. The verb phrases are not used
for this study, but they can also be generated
by chunking. It is difficult to determine the
corresponding verb phrases correctly because
the words in each verb phrase are often fewer
than the noun phrases.
2.2 Word-level Score
The system calculates the word-level scores
between MT output and reference using the
corresponding noun phrases. First, the sys-
tem determines the common words based on
Longest Common Subsequence (LCS). The
system selects only one LCS route when sev-
eral LCS routes exist. In such cases, the sys-
tem calculates the Route Score (RS) using the
following Eqs. (1) and (2):
RS =
?
c?LCS
(
?
w?c
weight(w)
)?
(1)
109
weight(w) =
?
?
?
?
?
?
?
words in corresponding
2 noun phrase
words in non
1 corresponding noun phrase
(2)
In Eq. (1), ? is a parameter for length
weighting of common parts; it is greater than
1.0. Figure 2 portrays an example of deter-
mination of the common parts. In the first
process of Fig. 2, LCS is 7. In this example,
several LCS routes exist. The system selects
the LCS route which has ?,?, ?the amount
of?, ?crowning?, ?is?, and ?.? as the com-
mon parts. The common part is the part
for which the common words appear contin-
uously. In contrast, IMPACT selects a differ-
ent LCS route that includes ?, the?, ?amount
of?, ?crowning?, ?is?, and ?.? as the com-
mon parts. In IMPACT, using no analytical
knowledge, the LCS route is determined using
the information of the number of words in the
common parts and the position of the com-
mon parts. The RS for LCS route selected
using our method is 32 (= 12.0 + (2 + 2 +
1)2.0 + 22.0 + 12.0 + 12.0) when ? is 2.0. The
RS for LCS route selected by IMPACT is 19
(= (1 + 1)2.0 + (2 + 1)2.0 + 22.0 + 12.0 + 12.0).
In the LCS route selected by IMPACT, the
weight of ?the? in the common part ?, the?
is 1 because ?the? in the reference is not in-
cluded in the corresponding noun phrase. In
the LCS route selected using our method, the
weight of ?the? in ?the amount of? is 2 because
?the? in MT output and ?the? in the reference
are included in the corresponding noun phrase
?NP1?. Therefore, the system based on our
method can select the correct LCS route.
Moreover, the word-level score is calculated
using the common parts in the selected LCS
route as the following Eqs. (3), (4), and (5).
Rwd =
?
?
?RN
i=0
(
?
i?
c?LCS length(c)
?
)
m
?
?
?
1
?
(3)
Pwd =
?
?
?RN
i=0
(
?
i?
c?LCS length(c)
?
)
n
?
?
?
1
?
(4)
MT output :
in general , [NP1 the amount ] of [NP2 the crowning fall ]
is large like [NP3 the end ] .
Reference :
generally , the closer [NP it ] is to [NP3 the end part ] , the 
larger [NP1 the amount ] of [NP2 crowning drop ] is .
(1) First process for determination of common parts : 
LCS = 7
(2) Second process for determination of common parts : 
LCS=3 
Our method
MT output :
in general , [NP1 the amount ] of [NP2 the crowning fall ]
is large like [NP3 the end ] .
Reference :
generally , the closer [NP it ] is to [NP3 the end part ] , the 
larger [NP1 the amount ] of [NP2 crowning drop ] is .
Our method
MT output :
in general , [NP1 the amount ] of [NP2 the crowning fall ]
is large like [NP3 the end ] .
Reference :
generally , the closer [NP it ] is to [NP3 the end part ] , the 
larger [NP1 the amount ] of [NP2 crowning drop ] is .
IMPACT
12.0 (2+2+1)2.0 22.012.012.0
(1+1)2.0(2+1)2.0 22.0 12.012.0
Figure 2: Example of common-part determi-
nation.
scorewd =
(1 + ?2)RwdPwd
Rwd + ?2Pwd
(5)
Equation (3) represents recall and Eq. (4)
represents precision. Therein, m signifies the
word number of the reference in Eq. (3), and
n stands for the word number of the MT out-
put in Eq. (4). Here, RN denotes the repe-
tition number of the determination process of
the LCS route, and i, which has initial value 0,
is the counter for RN . In Eqs. (3) and (4), ?
is a parameter for the repetition process of the
determination of LCS route, and is less than
1.0. Therefore, Rwd and Pwd becomes small
as the appearance order of the common parts
between MT output and reference is different.
Moreover, length(c) represents the number of
words in each common part; ? is a param-
eter related to the length weight of common
parts, as in Eq. (1). In this case, the weight
of each common word in the common part is
1. The system calculates scorewd as the word-
level score in Eq. (5). In Eq. (5), ? is deter-
mined as Pwd/Rwd. The scorewd is between
0.0 and 1.0.
110
In the first process of Fig. 2,
?
i?
c?LCS length(c)
? is 13.0 (=0.50 ?
(12.0 + 32.0 + 12.0 + 12.0 + 12.0)) when ? and
? are 0.5 and 2.0, respectively. In this case,
the counter i is 0. Moreover, in the second
process of Fig. 2, ?i
?
c?LCS length(c)
? is 2.5
(=0.51?(12.0 +22.0)) using two common parts
?the? and ?the end?, except the common
parts determined using the first process.
In Fig. 2, RN is 1 because the system
finishes calculating ?i
?
c?LCS length(c)
?
when counter i became 1: this means that
all common parts were processed until
the second process. As a result, Rwd is
0.1969 (=
?
(13.0 + 2.5)/202.0 =
?
0.0388),
and Pwd is 0.2625 (=
?
(13.0 + 2.5)/152.0 =
?
0.0689). Consequently, scorewd is 0.2164
(= (1+1.3332
2
)?0.1969?0.2625
0.1969+1.33322?0.2625 ). In this case, ?
becomes 1.3332 (=0.2625
0.1969). The system can
determine the matching words correctly using
the corresponding noun phrases between the
MT output and the reference.
The system calculates scorewd multi using
Rwd multi and Pwd multi which are, respec-
tively, maximum Rwd and Pwd when multiple
references are used as the following Eqs. (6),
(7) and (8). In Eq. (8), ? is determined as
Pwd multi/Rwd multi. The scorewd multi is be-
tween 0.0 and 1.0.
Rwd multi =
maxuj=1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
RN
?
i=0
(
?
i ?
c?LCS
length(c)?
))
j
m
?
j
?
?
?
?
?
?
?
1
?
?
?
?
?
?
?
?
?
(6)
Pwd multi =
maxuj=1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
RN
?
i=0
(
?
i ?
c?LCS
length(c)?
))
j
n
?
j
?
?
?
?
?
?
?
1
?
?
?
?
?
?
?
?
?
(7)
scorewd multi =
(1 + ?2Rwd multi)Pwd multi
Rwd multi + ?2Pwd multi
(8)
2.3 Phrase-level Score
The system calculates the phrase-level score
using the noun phrases obtained by chunking.
First, the system extracts only noun phrases
from sentences. Then it generalizes each noun
phrase as each word. Figure 3 presents exam-
ples of generalization by noun phrases.
MT output :
in general , [NP1 the amount ] of [NP2 the crowning fall ] 
is large like [NP3 the end ] .
Reference :
generally , the closer [NP it ] is to [NP3 the end part ] , 
the larger [NP1 the amount ] of [NP2 crowning drop ] is .
(1) Corresponding noun phrases
(2) Generalization by noun phrases
MT output :
NP1  NP2  NP3
Reference :
NP  NP3   NP1   NP2
Figure 3: Example of generalization by noun
phrases.
Figure 3 presents three corresponding noun
phrases between the MT output and the refer-
ence. The noun phrase ?it?, which has no cor-
responding noun phrase, is expressed as ?NP?
in the reference. Consequently, the MT output
is generalized as ?NP1 NP2 NP3?; the refer-
ence is generalized as ?NP NP3 NP1 NP2?.
Subsequently, the system obtains the phrase-
level score between the generalized MT output
and reference as the following Eqs. (9), (10),
and (11).
Rnp =
?
?
?
?RN
i=0
(
?
i?
cnpp?LCS length(cnpp)
?
)
(
mcnp ?
?
mno cnp
)?
?
?
?
1
?
(9)
Pnp =
?
?
?
?RN
i=0
(
?
i?
cnpp?LCS length(cnpp)
?
)
(
ncnp ?
?
nno cnp
)?
?
?
?
1
?
(10)
111
Table 1: Machine translation system types.
System No. 1 System No. 2 System No. 3 System No. 4 System No. 5 System No. 6
Type SMT SMT RBMT SMT SMT SMT
System No. 7 System No. 8 System No. 9 System No. 10 System No. 11 System No. 12
Type SMT SMT EBMT SMT SMT RBMT
scorenp =
(1 + ?2)RnpPnp
Rnp + ?2Pnp
(11)
In Eqs. (9) and (10), cnpp denotes the
common noun phrase parts; mcnp and ncnp
respectively signify the quantities of common
noun phrases in the reference and MT output.
Moreover, mno cnp and nno cnp are the quanti-
ties of noun phrases except the common noun
phrases in the reference and MT output. The
values of mno cnp and nno cnp are processed
as 1 when no non-corresponding noun phrases
exist. The square root used for mno cnp and
nno cnp is to decrease the weight of the non-
corresponding noun phrases. In Eq. (11), ? is
determined as Pnp/Rnp. In Fig. 3, Rnp and
Pnp are 0.7071 (=
?
1?2
2.0
+0.5?12.0
(3?1)
2.0
) when ? is
0.5 and ? is 2.0. Therefore, scorenp is 0.7071.
The system obtains scorenp multi calculat-
ing the average of scorenp when multiple ref-
erences are used as the following Eq. (12).
scorenp multi =
?u
j=0 (scorenp)j
u
(12)
2.4 Final Score
The system calculates the final score by com-
bining the word-level score and the phrase-
level score as shown in the following Eq. (13).
score =
scorewd + ? ? scorenp
1 + ?
(13)
Therein, ? represents a parameter for the
weight of scorenp: it is between 0.0 and 1.0.
The ratio of scorewd to scorenp is 1:1 when ? is
1.0. Moreover, scorewd multi and scorenp multi
are used for Eq. (13) in multiple references.
In Figs. 2 and 3, the final score between
the MT output and the reference is 0.4185
(=0.2164+0.7?0.7071
1+0.7 ) when ? is 0.7. The system
can realize high-quality automatic evaluation
using both word-level information and phrase-
level information.
3 Experiments
3.1 Experimental Procedure
We calculated the correlation between the
scores obtained using our method and scores
produced by human judgment. The system
based on our method obtained the evaluation
scores for 1,200 English output sentences re-
lated to the patent sentences. These English
output sentences are sentences that 12 ma-
chine translation systems in NTCIR-7 trans-
lated from 100 Japanese sentences. Moreover,
the number of references to each English sen-
tence in 100 English sentences is four. These
references were obtained from four bilingual
humans. Table 1 presents types of the 12 ma-
chine translation systems.
Moreover, three human judges evaluated
1,200 English output sentences from the per-
spective of adequacy and fluency on a scale of
1?5. We used the median value in the evalua-
tion results of three human judges as the final
scores of 1?5. We calculated Pearson?s correla-
tion efficient and Spearman?s rank correlation
efficient between the scores obtained using our
method and the scores by human judgments in
terms of sentence-level adequacy and fluency.
Additionally, we calculated the correlations
between the scores using seven other methods
and the scores by human judgments to com-
pare our method with other automatic evalua-
tion methods. The other seven methods were
IMPACT, ROUGE-L, BLEU1, NIST, NMG-
WN(Ehara, 2007; Echizen-ya et al, 2009),
METEOR2, and WER(Leusch et al, 2003).
Using our method, 0.1 was used as the value of
the parameter ? in Eqs. (3)-(10) and 1.1 was
used as the value of the parameter ? in Eqs.
(1)?(10). Moreover, 0.3 was used as the value
of the parameter ? in Eq. (13). These val-
1BLEU was improved to perform sentence-level
evaluation: the maximum N value between MT output
and reference is used(Echizen-ya et al, 2009).
2The matching modules of METEOR are the exact
and stemmed matching module, and a WordNet-based
synonym-matching module.
112
Table 2: Pearson?s correlation coefficient for sentence-level adequacy.
No. 1 No. 2 No. 3 No. 4 No. 5 No. 6 No. 7
Our method 0.7862 0.4989 0.5970 0.5713 0.6581 0.6779 0.7682
IMPACT 0.7639 0.4487 0.5980 0.5371 0.6371 0.6255 0.7249
ROUGE-L 0.7597 0.4264 0.6111 0.5229 0.6183 0.5927 0.7079
BLEU 0.6473 0.2463 0.4230 0.4336 0.3727 0.4124 0.5340
NIST 0.5135 0.2756 0.4142 0.3086 0.2553 0.2300 0.3628
NMG-WN 0.7010 0.3432 0.6067 0.4719 0.5441 0.5885 0.5906
METEOR 0.4509 0.0892 0.3907 0.2781 0.3120 0.2744 0.3937
WER 0.7464 0.4114 0.5519 0.5185 0.5461 0.5970 0.6902
Our method II 0.7870 0.5066 0.5967 0.5191 0.6529 0.6635 0.7698
BLEU with our method 0.7244 0.3935 0.5148 0.5231 0.4882 0.5554 0.6459
No. 8 No. 9 No. 10 No. 11 No. 12 Avg. All
Our method 0.7664 0.7208 0.6355 0.7781 0.5707 0.6691 0.6846
IMPACT 0.7007 0.7125 0.5981 0.7621 0.5345 0.6369 0.6574
ROUGE-L 0.6834 0.7042 0.5691 0.7480 0.5293 0.6228 0.6529
BLEU 0.5188 0.5884 0.3697 0.5459 0.4357 0.4607 0.4722
NIST 0.4218 0.4092 0.1721 0.3521 0.4769 0.3493 0.3326
NMG-WN 0.6658 0.6068 0.6116 0.6770 0.5740 0.5818 0.5669
METEOR 0.3881 0.4947 0.3127 0.2987 0.4162 0.3416 0.2958
WER 0.6656 0.6570 0.5740 0.7491 0.5301 0.6031 0.5205
Our method II 0.7676 0.7217 0.6343 0.7917 0.5474 0.6632 0.6774
BLEU with our method 0.6395 0.6696 0.5139 0.6611 0.5079 0.5698 0.5790
ues of the parameter are determined using En-
glish sentences from Reuters articles(Utiyama
and Isahara, 2003). Moreover, we obtained
the noun phrases using a shallow parser(Sha
and Pereira, 2003) as the chunking tool. We
revised some erroneous results that were ob-
tained using the chunking tool.
3.2 Experimental Results
As described in this paper, we performed com-
parison experiments using our method and
seven other methods. Tables 2 and 3 respec-
tively show Pearson?s correlation coefficient for
sentence-level adequacy and fluency. Tables 4
and 5 respectively show Spearman?s rank cor-
relation coefficient for sentence-level adequacy
and fluency. In Tables 2?5, bold typeface
signifies the maximum correlation coefficients
among eight automatic evaluation methods.
Underlining in our method signifies that the
differences between correlation coefficients ob-
tained using our method and IMPACT are
statistically significant at the 5% significance
level. Moreover, ?Avg.? signifies the aver-
age of the correlation coefficients obtained by
12 machine translation systems in respective
automatic evaluation methods, and ?All? are
the correlation coefficients using the scores of
1,200 output sentences obtained using the 12
machine translation systems.
3.3 Discussion
In Tables 2?5, the ?Avg.? score of our method
is shown to be higher than those of other meth-
ods. Especially in terms of the sentence-level
adequacy shown in Tables 2 and 4, ?Avg.?
of our method is about 0.03 higher than that
of IMPACT. Moreover, in system No. 8 and
?All? of Tables 2 and 4, the differences be-
tween correlation coefficients obtained using
our method and IMPACT are statistically sig-
nificant at the 5% significance level.
Moreover, we investigated the correlation of
machine translation systems of every type. Ta-
ble 6 shows ?All? of Pearson?s correlation co-
efficient and Spearman?s rank correlation coef-
ficient in SMT (i.e., system Nos. 1?2, system
Nos. 4?8 and system Nos. 10?11) and RBMT
(i.e., system Nos. 3 and 12). The scores of
900 output sentences obtained by 9 machine
113
Table 3: Pearson?s correlation coefficient for sentence-level fluency.
No. 1 No. 2 No. 3 No. 4 No. 5 No. 6 No. 7
Our method 0.5853 0.3782 0.5689 0.4673 0.5739 0.5344 0.7193
IMPACT 0.5581 0.3407 0.5821 0.4586 0.5768 0.4852 0.6896
ROUGE-L 0.5551 0.3056 0.5925 0.4391 0.5666 0.4475 0.6756
BLEU 0.4793 0.0963 0.4488 0.3033 0.4690 0.3602 0.5272
NIST 0.4139 0.0257 0.4987 0.1682 0.3923 0.2236 0.3749
NMG-WN 0.5782 0.3090 0.5434 0.4680 0.5070 0.5234 0.5363
METEOR 0.4050 0.1405 0.4420 0.1825 0.4259 0.2336 0.4873
WER 0.5143 0.3031 0.5220 0.4262 0.4936 0.4405 0.6351
Our method II 0.5831 0.3689 0.5753 0.3991 0.5610 0.5445 0.7186
BLEU with our method 0.5425 0.2304 0.5115 0.3770 0.5358 0.4741 0.6142
No. 8 No. 9 No. 10 No. 11 No. 12 Avg. All
Our method 0.5796 0.6424 0.3241 0.5920 0.4321 0.5331 0.5574
IMPACT 0.5612 0.6320 0.3492 0.6034 0.4166 0.5211 0.5469
ROUGE-L 0.5414 0.6347 0.3231 0.5889 0.4127 0.5069 0.5387
BLEU 0.5040 0.5521 0.2134 0.4783 0.4078 0.4033 0.4278
NIST 0.3682 0.3811 0.1682 0.3116 0.4484 0.3146 0.3142
NMG-WN 0.5526 0.5799 0.4509 0.6308 0.4124 0.5007 0.5074
METEOR 0.2511 0.4153 0.1376 0.3351 0.2902 0.3122 0.2933
WER 0.5492 0.6421 0.3962 0.6228 0.4063 0.4960 0.4478
Our method II 0.5774 0.6486 0.3428 0.5975 0.4197 0.5280 0.5519
BLEU with our method 0.5660 0.6247 0.2536 0.5495 0.4550 0.4770 0.5014
translation systems in SMT and the scores of
200 output sentences obtained by 2 machine
translation systems in RBMT are used respec-
tively. However, EBMT is not included in Ta-
ble 6 because EBMT is only system No. 9.
In Table 6, our method obtained the highest
correlation among the eight methods, except
in terms of the adequacy of RBMT in Pear-
son?s correlation coefficient. The differences
between correlation coefficients obtained us-
ing our method and IMPACT are statistically
significant at the 5% significance level for ad-
equacy of SMT.
To confirm the effectiveness of noun-phrase
chunking, we performed the experiment using
a system combining BLEU with our method.
In this case, BLEU scores were used as scorewd
in Eq. (13). This experimental result is shown
as ?BLEU with our method? in Tables 2?5. In
the results of ?BLEU with our method? in Ta-
bles 2?5, underlining signifies that the differ-
ences between correlation coefficients obtained
using BLEU with our method and BLEU alone
are statistically significant at the 5% signif-
icance level. The coefficients of correlation
for BLEU with our method are higher than
those of BLEU in any machine translation sys-
tem, ?Avg.? and ?All? in Tables 2?5. More-
over, for sentence-level adequacy, BLEU with
our method is significantly better than BLEU
in almost all machine translation systems and
?All? in Tables 2 and 4. These results indicate
that our method using noun-phrase chunking
is effective for some methods and that it is
statistically significant in each machine trans-
lation system, not only ?All?, which has large
sentences.
Subsequently, we investigated the precision
of the determination process of the corre-
sponding noun phrases described in section
2.1: in the results of system No. 1, we cal-
culated the precision as the ratio of the num-
ber of the correct corresponding noun phrases
for the number of all noun-phrase correspon-
dences obtained using the system based on our
method. Results show that the precision was
93.4%, demonstrating that our method can de-
termine the corresponding noun phrases cor-
rectly.
Moreover, we investigated the relation be-
114
Table 4: Spearman?s rank correlation coefficient for sentence-level adequacy.
No. 1 No. 2 No. 3 No. 4 No. 5 No. 6 No. 7
Our method 0.7456 0.5049 0.5837 0.5146 0.6514 0.6557 0.6746
IMPACT 0.7336 0.4881 0.5992 0.4741 0.6382 0.5841 0.6409
ROUGE-L 0.7304 0.4822 0.6092 0.4572 0.6135 0.5365 0.6368
BLEU 0.5525 0.2206 0.4327 0.3449 0.3230 0.2805 0.4375
NIST 0.5032 0.2438 0.4218 0.2489 0.2342 0.1534 0.3529
NMG-WN 0.7541 0.3829 0.5579 0.4472 0.5560 0.5828 0.6263
METEOR 0.4409 0.1509 0.4018 0.2580 0.3085 0.1991 0.4115
WER 0.6566 0.4147 0.5478 0.4272 0.5524 0.4884 0.5539
Our method II 0.7478 0.4972 0.5817 0.4892 0.6437 0.6428 0.6707
BLEU with our method 0.6644 0.3926 0.5065 0.4522 0.4639 0.4715 0.5460
No. 8 No. 9 No. 10 No. 11 No. 12 Avg. All
Our method 0.7298 0.7258 0.5961 0.7633 0.6078 0.6461 0.6763
IMPACT 0.6703 0.7067 0.5617 0.7411 0.5583 0.6164 0.6515
ROUGE-L 0.6603 0.6983 0.5340 0.7280 0.5281 0.6012 0.6435
BLEU 0.4571 0.5827 0.3220 0.4987 0.4302 0.4069 0.4227
NIST 0.4255 0.4424 0.1313 0.2950 0.4785 0.3276 0.3062
NMG-WN 0.6863 0.6524 0.6412 0.7015 0.5728 0.5968 0.5836
METEOR 0.4242 0.4776 0.3335 0.2861 0.4455 0.3448 0.2887
WER 0.6234 0.6480 0.5463 0.7131 0.5684 0.5617 0.4797
Our method II 0.7287 0.7255 0.5936 0.7761 0.5798 0.6397 0.6699
BLEU with our method 0.5850 0.6757 0.4596 0.6272 0.5452 0.5325 0.5474
tween the correlation obtained by our method
and the quality of chunking. In ?Our method?
shown in Tables 2?5, noun phrases for which
some erroneous results obtained using the
chunking tool were revised. ?Our method II?
of Tables 2?5 used noun phrases that were
given as results obtained using the chunk-
ing tool. Underlining in ?Our method II? of
Tables 2?5 signifies that the differences be-
tween correlation coefficients obtained using
our method II and IMPACT are statistically
significant at the 5% significance level. Fun-
damentally, in both ?Avg.? and ?All? of Ta-
bles 2?5, the correlation coefficients of our
method II without the revised noun phrases
are lower than those of our method using the
revised noun phrases. However, the difference
between our method and our method II in
?Avg.? and ?All? of Tables 2?5 is not large.
The performance of the chunking tool has no
great influence on the results of our method
because scorewd in Eqs. (3), (4), and (5) do
not depend strongly on the performance of
the chunking tool. For example, in sentences
shown in Fig. 2, all common parts are the
same as the common parts of Fig. 2 when ?the
crowning fall? in the MT output and ?crown-
ing drop? in the reference are not determined
as the noun phrases. Other common parts are
determined correctly because the weight of the
common part ?the amount of? is higher than
those of other common parts by Eqs. (1) and
(2). Consequently, the determination of the
common parts except ?the amount of? is not
difficult.
In other language sentences, we already per-
formed the experiments using Japanese sen-
tences from Reuters articles(Oyamada et al,
2010). Results show that the correlation co-
efficients of IMPACT with our method, for
which IMPACT scores were used as scorewd in
Eq. (13), were highest among some methods.
Therefore, our method might not be language-
dependent. Nevertheless, experiments using
various language data are necessary to eluci-
date this point.
4 Conclusion
As described herein, we proposed a new auto-
matic evaluation method for machine transla-
115
Table 5: Spearman?s rank correlation coefficient for sentence-level fluency.
No. 1 No. 2 No. 3 No. 4 No. 5 No. 6 No. 7
Our method 0.5697 0.3299 0.5446 0.4199 0.5733 0.5060 0.6459
IMPACT 0.5481 0.3285 0.5572 0.3976 0.5960 0.4317 0.6334
ROUGE-L 0.5470 0.3041 0.5646 0.3661 0.5638 0.3879 0.6255
BLEU 0.4157 0.0559 0.4286 0.2018 0.4475 0.2569 0.4909
NIST 0.4209 0.0185 0.4559 0.1093 0.3186 0.1898 0.3634
NMG-WN 0.5569 0.3461 0.5381 0.4300 0.5052 0.5264 0.5328
METEOR 0.4608 0.1429 0.4438 0.1783 0.4073 0.1596 0.4821
WER 0.4469 0.2395 0.5087 0.3292 0.4995 0.3482 0.5637
Our method II 0.5659 0.3216 0.5484 0.3773 0.5638 0.5211 0.6343
BLEU with our method 0.5188 0.1534 0.4793 0.3005 0.5255 0.3942 0.5676
No. 8 No. 9 No. 10 No. 11 No. 12 Avg. All
Our method 0.5646 0.6617 0.3319 0.6256 0.4485 0.5185 0.5556
IMPACT 0.5471 0.6454 0.3222 0.6319 0.4358 0.5062 0.5489
ROUGE-L 0.5246 0.6428 0.2949 0.6159 0.3928 0.4858 0.5359
BLEU 0.4882 0.5419 0.1407 0.4740 0.4176 0.3633 0.3971
NIST 0.4150 0.4193 0.0889 0.3006 0.4752 0.2980 0.2994
NMG-WN 0.5684 0.5850 0.4451 0.6502 0.4387 0.5102 0.5156
METEOR 0.2911 0.4267 0.1735 0.3264 0.3512 0.3158 0.2886
WER 0.5320 0.6505 0.3828 0.6501 0.4003 0.4626 0.4193
Our method II 0.5609 0.6687 0.3629 0.6223 0.4384 0.5155 0.5531
BLEU with our method 0.5470 0.6213 0.2184 0.5808 0.4870 0.4495 0.4825
Table 6: Correlation coefficient for SMT and RBMT.
Pearson?s correlation coefficient Spearman?s rank correlation coefficient
Adequacy Fluency Adequacy Fluency
SMT RBMT SMT RBMT SMT RBMT SMT RBMT
Our method 0.7054 0.5840 0.5477 0.5016 0.6710 0.5961 0.5254 0.5003
IMPACT 0.6721 0.5650 0.5364 0.4960 0.6397 0.5811 0.5162 0.4951
ROUGE-L 0.6560 0.5691 0.5179 0.4988 0.6225 0.5701 0.4942 0.4783
NMG-WN 0.5958 0.5850 0.5201 0.4732 0.6129 0.5755 0.5238 0.4959
tion. Our method calculates the scores for MT
outputs using noun-phrase chunking. Conse-
quently, the system obtains scores using the
correctly matched words and phrase-level in-
formation based on the corresponding noun
phrases. Experimental results demonstrate
that our method yields the highest correlation
among eight methods in terms of sentence-
level adequacy and fluency.
Future studies will improve our method,
enabling it to achieve high correlation in
sentence-level fluency. Future studies will also
include experiments using data of various lan-
guages.
Acknowledgements
This work was done as research under the
AAMT/JAPIO Special Interest Group on
Patent Translation. The Japan Patent In-
formation Organization (JAPIO) and the Na-
tional Institute of Informatics (NII) provided
corpora used in this work. The author grate-
fully acknowledges JAPIO and NII for their
support. Moreover, this work was partially
supported by Grants from the High-Tech Re-
search Center of Hokkai-Gakuen University
and the Kayamori Foundation of Informa-
tional Science Advancement.
116
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An Automatic Metric for MT Eval-
uation with Improved Correlation with Hu-
man Judgments. In Proc. of ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summariza-
tion, 65?72.
Deborah Coughlin. 2003. Correlating Automated
and Human Assessments of Machine Translation
Quality. In Proc. of MT Summit IX, 63?70.
Hiroshi Echizen-ya and Kenji Araki. 2007. Auto-
matic Evaluation of Machine Translation based
on Recursive Acquisition of an Intuitive Com-
mon Parts Continuum. In Proc. of MT Summit
XII, 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shi-
mohata, Atsushi Fujii, Masao Utiyama, Mikio
Yamamoto, Takehito Utsuro and Noriko Kando.
2009. Meta-Evaluation of Automatic Evaluation
Methods for Machine Translation using Patent
Translation Data in NTCIR-7. In Proc. of the
3rd Workshop on Patent Translation, 9?16.
Terumasa Ehara. 2007. Rule Based Machine
Translation Combined with Statistical Post Ed-
itor for Japanese to English Patent Transla-
tion. In Proc. of MT Summit XII Workshop
on Patent Translation, 13?18.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto
and Takehito Utsuro. 2008. Overview of the
Patent Translation Task at the NTCIR-7 Work-
shop. In Proc. of 7th NTCIR Workshop Meeting
on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answer-
ing and Cross-lingual Information Access, 389?
400.
Gregor Leusch, Nicola Ueffing and Hermann Ney.
2003. A Novel String-to-String Distance Mea-
sure with Applications to Machine Translation
Evaluation. In Proc. of MT Summit IX, 240?
247.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Qual-
ity Using Longest Common Subsequence and
Skip-Bigram Statistics. In Proc. of ACL?04,
606?613.
Dennis N. Mehay and Chris Brew. 2007.
BLEUA?TRE: Flattening Syntactic Dependen-
cies for MT Evaluation. In Proc. of MT Summit
XII, 122?131.
Andrew Mutton, Mark Dras, Stephen Wan and
Robert Dale. 2007. GLEU: Automatic Eval-
uation of Sentence-Level Fluency. In Proc. of
ACL?07, 344?351.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurrence Statistics.
http://www.nist.gov/speech/tests/mt/doc/
ngram-study.pdf.
Takashi Oyamada, Hiroshi Echizen-ya and Kenji
Araki. 2010. Automatic Evaluation of Machine
Translation Using both Words Information and
Comprehensive Phrases Information. In IPSJ
SIG Technical Report, Vol.2010-NL-195, No. 3
(in Japanese).
Kishore Papineni, Salim Roukos, Todd Ward and
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In
Proc. of ACL?02, 311?318.
Michael Pozar and Eugene Charniak. 2006. Bllip:
An Improved Evaluation Metric for Machine
Translation. Brown University Master Thesis.
Fei Sha and Fernando Pereira. 2003. Shallow Pars-
ing with Conditional Random Fields. In Proc.
of HLT-NAACL 2003, 134?141.
Keh-Yih Su, Ming-Wen Wu and Jing-Shin Chang.
1992. A New Quantitative Quality Measure for
Machine Translation Systems. In Proc. of GOL-
ING?92, 433?439.
Masao Utiyama and Hitoshi Isahara. 2003. Re-
liable Measures for Aligning Japanese?English
News Articles and Sentences. In Proc. of the
ACL?03, pp.72?79.
117
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 89?98,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Automatically Annotating A Five-Billion-Word Corpus of Japanese Blogs
for Affect and Sentiment Analysis
Michal Ptaszynski ? Rafal Rzepka ? Kenji Araki ? Yoshio Momouchi ?
? JSPS Research Fellow / High-Tech Research Center, Hokkai-Gakuen University
ptaszynski@hgu.jp
? Graduate School of Information Science and Technology, Hokkaido University
{kabura,araki}@media.eng.hokudai.ac.jp
? Department of Electronics and Information Engineering, Faculty of Engineering, Hokkai-Gakuen University
momouchi@eli.hokkai-s-u.ac.jp
Abstract
This paper presents our research on automatic
annotation of a five-billion-word corpus of
Japanese blogs with information on affect and
sentiment. We first perform a study in emotion
blog corpora to discover that there has been
no large scale emotion corpus available for
the Japanese language. We choose the largest
blog corpus for the language and annotate it
with the use of two systems for affect anal-
ysis: ML-Ask for word- and sentence-level
affect analysis and CAO for detailed anal-
ysis of emoticons. The annotated informa-
tion includes affective features like sentence
subjectivity (emotive/non-emotive) or emo-
tion classes (joy, sadness, etc.), useful in affect
analysis. The annotations are also general-
ized on a 2-dimensional model of affect to ob-
tain information on sentence valence/polarity
(positive/negative) useful in sentiment analy-
sis. The annotations are evaluated in several
ways. Firstly, on a test set of a thousand sen-
tences extracted randomly and evaluated by
over forty respondents. Secondly, the statistics
of annotations are compared to other existing
emotion blog corpora. Finally, the corpus is
applied in several tasks, such as generation of
emotion object ontology or retrieval of emo-
tional and moral consequences of actions.
1 Introduction
There is a lack of large corpora for Japanese ap-
plicable in sentiment and affect analysis. Although
there are large corpora of newspaper articles, like
Mainichi Shinbun Corpus1, or corpora of classic lit-
erature, like Aozora Bunko2, they are usually un-
suitable for research on emotions since spontaneous
1http://www.nichigai.co.jp/sales/mainichi/mainichi-data.html
2http://www.aozora.gr.jp/
emotive expressions either appear rarely in these
kinds of texts (newspapers), or the vocabulary is not
up to date (classic literature). Although there ex-
ist speech corpora, such as Corpus of Spontaneous
Japanese3, which could become suitable for this
kind of research, due to the difficulties with com-
pilation of such corpora they are relatively small.
In research such as the one by Abbasi and Chen
(2007) it was proved that public Internet services,
such as forums or blogs, are a good material for af-
fect analysis because of their richness in evaluative
and emotive information. One kind of these services
are blogs, open diaries in which people encapsu-
late their own experiences, opinions and feelings to
be read and commented by other people. Recently
blogs have come into the focus of opinion mining or
sentiment and affect analysis (Aman and Szpakow-
icz, 2007; Quan and Ren, 2010). Therefore creating
a large blog-based emotion corpus could help over-
come both problems: the lack in quantity of corpora
and their applicability in sentiment and affect anal-
ysis. There have been only a few small Japanese
emotion corpora developed so far (Hashimoto et al,
2011). On the other hand, although there exist large
Web-based corpora (Erjavec et al, 2008; Baroni and
Ueyama, 2006), access to them is usually allowed
only from the Web interface, which makes addi-
tional annotations with affective information diffi-
cult. In this paper we present the first attempt to au-
tomatically annotate affect on YACIS, a large scale
corpus of Japanese blogs. To do that we use two sys-
tems for affect analysis of Japanese, one for word-
and sentence-level affect analysis and another espe-
cially for detailed analysis of emoticons, to annotate
on the corpus different kinds of affective informa-
tion (emotive expressions, emotion classes, etc.).
3http://www.ninjal.ac.jp/products-k/katsudo/seika/corpus/public/
89
The outline of the paper is as follows. Section
2 describes the related research in emotion corpora.
Section 3 presents our choice of the corpus for anno-
tation of affect- and sentiment-related information.
Section 4 describes tools used in annotation. Sec-
tion 5 presents detailed data and evaluation of the
annotations. Section 6 presents tasks in which the
corpus has already been applied. Finally the paper
is concluded and future applications are discussed.
2 Emotion Corpora
Research on Affect Analysis has resulted in a
number of systems developed within several years
(Aman and Szpakowicz, 2007; Ptaszynski et al,
2009c; Matsumoto et al, 2011). Unfortunately,
most of such research ends in proposing and evaluat-
ing a system. The real world application that would
be desirable, such as annotating affective informa-
tion on linguistic data is limited to processing a usu-
ally small test sample in the evaluation. The small
number of annotated emotion corpora that exist are
mostly of limited scale and are annotated manually.
Below we describe and compare some of the most
notable emotion corpora. Interestingly, six out of
eight emotion corpora described below are created
from blogs. The comparison is summarized in Table
1. We also included information on the work de-
scribed in this paper for better comparison (YACIS).
Quan and Ren (2010) created a Chinese emotion
blog corpus Ren-CECps1.0. They collected 500
blog articles from various Chinese blog services,
such as sina blog (http://blog.sina.com.cn/), qq blog
(http://blog.qq.com/), etc., and annotated them with
a large variety of information, such as emotion class,
emotive expressions or polarity level. Although syn-
tactic annotations were simplified to tokenization
and POS tagging, this corpus can be considered a
state-of-the-art emotion blog corpus. The motiva-
tion for Quan and Ren is also similar to ours - deal-
ing with the lack of large corpora for sentiment anal-
ysis in Chinese (in our case - Japanese).
Wiebe et al (2005) report on creating the MPQA
corpus of news articles. The corpus contains 10,657
sentences in 535 documents4. The annotation
schema includes a variety of emotion-related infor-
4The new MPQA Opinion Corpus version 2.0 contains ad-
ditional 157 documents, 692 documents in total.
mation, such as emotive expressions, emotion va-
lence, intensity, etc. However, Wiebe et al focused
on detecting subjective (emotive) sentences, which
do not necessarily convey emotions, and classifying
them into positive and negative. Thus their annota-
tion schema, although one of the richest, does not
include emotion classes.
A corpus of Japanese blogs, called KNB, rich in
the amount and diversification of annotated informa-
tion was developed by Hashimoto et al (2011). It
contains 67 thousand words in 249 blog articles. Al-
though it is not a small scale corpus, it developed
a certain standard for preparing corpora, especially
blog corpora for sentiment and affect-related stud-
ies in Japan. The corpus contains all relevant gram-
matical annotations, including POS tagging, depen-
dency parsing or Named Entity Recognition. It also
contains sentiment-related information. Words and
phrases expressing emotional attitude were anno-
tated by laypeople as either positive or negative.
One disadvantage of the corpus, apart from its small
scale, is the way it was created. Eighty one students
were employed to write blogs about different topics
especially for the need of this research. It could be
argued that since the students knew their blogs will
be read mostly by their teachers, they selected their
words more carefully than they would in private.
Aman and Szpakowicz (2007) constructed a
small-scale English blog corpus. They did not in-
clude any grammatical information, but focused on
affect-related annotations. As an interesting remark,
they were some of the first to recognize the task
of distinguishing between emotive and non-emotive
sentences. This problem is usually one of the most
difficult in text-based Affect Analysis and is there-
fore often omitted in such research. In our research
we applied a system proved to deal with this task
with high accuracy for Japanese.
Das and Bandyopadhyay (2010) constructed an
emotion annotated corpus of blogs in Bengali. The
corpus contains 12,149 sentences within 123 blog
posts extracted from Bengali web blog archive
(http://www.amarblog.com/). It is annotated with
face recognition annotation standard (Ekman, 1992).
Matsumoto et al (2011) created Wakamono Ko-
toba (Slang of the Youth) corpus. It contains un-
related sentences extracted manually from Yahoo!
blogs (http://blog-search.yahoo.co.jp/). Each sen-
90
Table 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization,
POS=part-of-speech tagging, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition).
corpus scale language annotated affective information syntactic
name (in senten-ces / docs)
emotion class
standard
emotive
expressions
emotive/
non-emot.
valence/
activation
emotion
intensity
emotion
objects
annota-
tions
YACIS 354 mil./13 mil. Japanese
10 (language and
culture based) ? ? ?/? ? ? T,POS,L,DP,NER;
Ren-CECps1.0 12,724/500 Chinese 8 (Yahoo! news) ? ? ?/? ? ? T,POS;
MPQA 10,657/535 English none (no standard) ? ? ?/? ? ? T,POS;
KNB 4,186/249 Japanese none (no standard) ? ? ?/? ? ? T,POS,L,DP,NER;
Minato et al 1,191sent. Japanese 8 (chosen subjectively)? ? ?/? ? ? POS;
Aman&Szpak. 5,205/173 English 6 (face recognition) ? ? ?/? ? ? ?
Das&Bandyo. 12,149/123 Bengali 6 (face recognition) ? ? ?/? ? ? ?
Wakamono 4773sen- Japanese 9 (face recognition + ? ? ?/? ? ? ?
Kotoba tences 3 added subjectively)
Mishne ?/815,494 English 132 (LiveJournal) ? ? ?/? ? ? ?
tence contains at least one word from a slang lexicon
and one word from an emotion lexicon, with addi-
tional emotion class tags added per sentence. The
emotion class set used for annotation was chosen
subjectively, by applying the 6 class face recogni-
tion standard and adding 3 classes of their choice.
Mishne (2005) collected a corpus of English blogs
from LiveJournal (http://www.livejournal.com/)
blogs. The corpus contains 815,494 blog posts,
from which many are annotated with emotions
(moods) by the blog authors themselves. The
LiveJournal service offers an option for its users to
annotate their mood while writing the blog. The
list of 132 moods include words like ?amused?, or
?angry?. The LiveJournal mood annotation standard
offers a rich vocabulary to describe the writer?s
mood. However, this richness has been considered
troublesome to generalize the data in a meaningful
manner (Quan and Ren, 2010).
Finally, Minato et al (2006) collected a 14,195
word, 1,191 sentence corpus. The corpus was a col-
lection of sentence examples from a dictionary of
emotional expressions (Hiejima, 1995). The dictio-
nary was created for the need of Japanese language
learners. Differently to the dictionary applied in our
research (Nakamura, 1993), in Hiejima (1995) sen-
tence examples were mostly written by the author of
the dictionary himself. The dictionary also does not
propose any coherent emotion class list, but rather
the emotion concepts are chosen subjectively. Al-
though the corpus by Minato et al is the smallest
of all mentioned above, its statistics is described in
detail. Therefore in this paper we use it as one of the
Japanese emotion corpora to compare our work to.
All of the above corpora were annotated manu-
ally or semi-automatically. In this research we per-
formed the first attempt to annotate a large scale blog
corpus (YACIS) with affective information fully au-
tomatically. We did this with systems based on pos-
itively evaluated affect annotation schema, perfor-
mance, and standardized emotion class typology.
3 Choice of Blog Corpus
Although Japanese is a well recognized and de-
scribed world language, there have been only few
large corpora for this language. For example, Er-
javec et al (2008) gathered a 400-million-word scale
Web corpus JpWaC, or Baroni and Ueyama (2006)
developed a medium-sized corpus of Japanese blogs
jBlogs containing 62 million words. However, both
research faced several problems, such as character
encoding, or web page metadata extraction, such as
the page title or author which differ between do-
mains. Apart from the above mentioned medium
sized corpora at present the largest Web based blog
corpus available for Japanese is YACIS or Yet
Another Corpus of Internet Sentences. We chose
this corpus for the annotation of affective informa-
tion for several reasons. It was collected automati-
cally by Maciejewski et al (2010) from the pages of
Ameba blog service. It contains 5.6 billion words
within 350 million sentences. Maciejewski et al
were able to extract only pages containing Japanese
posts (pages with legal disclaimers or written in lan-
guages other than Japanese were omitted). In the
initial phase they provided their crawler, optimized
to crawl only Ameba blog service, with 1000 links
91
Figure 1: The example of YACIS XML structure.
Table 2: General Statistics of YACIS.
# of web pages 12,938,606
# of unique bloggers 60,658
average # of pages/blogger 213.3
# of pages with comments 6,421,577
# of comments 50,560,024
average # of comment/page 7.873
# of words 5,600,597,095
# of all sentences 354,288,529
# of words per sentence (average) 15
# of characters per sentence (average) 77
taken from Google (response to one simple query:
?site:ameblo.jp?). They saved all pages to disk as
raw HTML files (each page in a separate file) and
afterward extracted all the posts and comments and
divided them into sentences. The original structure
(blog post and comments) was preserved, thanks to
which semantic relations between posts and com-
ments were retained. The blog service from which
the corpus was extracted (Ameba) is encoded by de-
fault in Unicode, thus there was no problem with
character encoding. It also has a clear and stable
HTML meta-structure, thanks to which they man-
aged to extract metadata such as blog title and au-
thor. The corpus was first presented as an unanno-
tated corpus. Recently Ptaszynski et al (2012b) an-
notated it with syntactic information, such as POS,
dependency structure or named entity recognition.
An example of the original blog structure in XML
is represented in Figure 1. Some statistics about the
corpus are represented in Table 2.
4 Affective Information Annotation Tools
Emotive Expression Dictionary (Nakamura, 1993)
is a collection of over two thousand expressions de-
scribing emotional states collected manually from a
wide range of literature. It is not a tool per se, but
Figure 2: Output examples for ML-Ask and CAO.
Table 3: Distribution of separate expressions across emo-
tion classes in Nakamura?s dictionary (overall 2100 ex.).
emotion nunber of emotion nunber of
class expressions class expressions
dislike 532 fondness 197
excitement 269 fear 147
sadness 232 surprise 129
joy 224 relief 106
anger 199 shame 65
sum 2100
was converted into an emotive expression database
by Ptaszynski et al (2009c). Since YACIS is a
Japanese language corpus, for the affect annotation
we needed the most appropriate lexicon for the lan-
guage. The dictionary, developed for over 20 years
by Akira Nakamura, is a state-of-the art example
of a hand-crafted emotive expression lexicon. It
also proposes a classification of emotions that re-
flects the Japanese culture: ki/yorokobi5 (joy),
do?/ikari (anger), ai/aware (sorrow, sadness,
gloom), fu/kowagari (fear), chi/haji (shame,
shyness), ko?/suki (fondness), en/iya (dislike),
ko?/takaburi (excitement), an/yasuragi (relief),
and kyo?/odoroki (surprise). All expressions in the
dictionary are annotated with one emotion class or
more if applicable. The distribution of expressions
across all emotion classes is represented in Table 3.
ML-Ask (Ptaszynski et al, 2009a; Ptaszynski et al,
2009c) is a keyword-based language-dependent sys-
tem for affect annotation on sentences in Japanese.
It uses a two-step procedure: 1) specifying whether
an utterance is emotive, and 2) annotating the partic-
ular emotion classes in utterances described as emo-
tive. The emotive sentences are detected on the ba-
sis of emotemes, emotive features like: interjections,
mimetic expressions, vulgar language, emoticons
5Separation by ?/? represents two possible readings of the character.
92
Table 4: Evaluation results of ML-Ask and CAO.
emotive/ emotion 2D (valence
non-emotive classes and activation)
ML-Ask 98.8% 73.4% 88.6%
CAO 97.6% 80.2% 94.6%
ML-Ask+CAO 100.0% 89.9% 97.5%
and emotive markers. The examples in Japanese
are respectively: sugee (great!), wakuwaku (heart
pounding), -yagaru (syntactic morpheme used in
verb vulgarization), (? ?) (emoticon expressing joy)
and ?!?, ???? (markers indicating emotive engage-
ment). Emotion class annotation is based on Naka-
mura?s dictionary. ML-Ask is also the only present
system for Japanese recognized to implement the
idea of Contextual Valence Shifters (CVS) (Zaenen
and Polanyi, 2005) (words and phrases like ?not?,
or ?never?, which change the valence of an evalua-
tive word). The last distinguishable feature of ML-
Ask is implementation of Russell?s two dimensional
affect model (Russell, 1980), in which emotions
are represented in two dimensions: valence (posi-
tive/negative) and activation (activated/deactivated).
An example of negative-activated emotion could
be ?anger?; a positive-deactivated emotion is, e.g.,
?relief?. The mapping of Nakamura?s emotion
classes on Russell?s two dimensions was proved re-
liable in several research (Ptaszynski et al, 2009b;
Ptaszynski et al, 2009c; Ptaszynski et al, 2010b).
With these settings ML-Ask detects emotive sen-
tences with a high accuracy (90%) and annotates af-
fect on utterances with a sufficiently high Precision
(85.7%), but low Recall (54.7%). Although low Re-
call is a disadvantage, we assumed that in a corpus
as big as YACIS there should still be plenty of data.
CAO (Ptaszynski et al, 2010b) is a system for
affect analysis of Japanese emoticons, called kao-
moji. Emoticons are sets of symbols used to con-
vey emotions in text-based online communication,
such as blogs. CAO extracts emoticons from in-
put and determines specific emotions expressed by
them. Firstly, it matches the input to a predeter-
mined raw emoticon database (with over ten thou-
sand emoticons). The emoticons, which could not be
estimated with this database are divided into seman-
tic areas (representations of ?mouth? or ?eyes?). The
areas are automatically annotated according to their
Table 5: Statistics of emotive sentences.
# of emotive sentences 233,591,502
# of non-emotive sentence 120,408,023
ratio (emotive/non-emotive) 1.94
# of sentences containing emoteme class:
- interjections 171,734,464
- exclamative marks 89,626,215
- emoticons 49,095,123
- endearments 12,935,510
- vulgarities 1,686,943
ratio (emoteme classes in emotive sentence) 1.39
co-occurrence in the database. The performance of
CAO was evaluated as close to ideal (Ptaszynski et
al., 2010b) (over 97%). In this research we used
CAO as a supporting procedure in ML-Ask to im-
prove the overall performance and add detailed in-
formation about emoticons.
5 Annotation Results and Evaluation
It is physically impossible to manually evaluate all
annotations on the corpus6. Therefore we applied
three different types of evaluation. First was based
on a sample of 1000 sentences randomly extracted
from the corpus and annotated by laypeople. In sec-
ond we compared YACIS annotations to other emo-
tion corpora. The third evaluation was application
based and is be described in section 6.
Evaluation of Affective Annotations: Firstly, we
needed to confirm the performance of affect anal-
ysis systems on YACIS, since the performance is
often related to the type of test set used in evalu-
ation. ML-Ask was positively evaluated on sepa-
rate sentences and on an online forum (Ptaszynski
et al, 2009c). However, it was not yet evaluated
on blogs. Moreover, the version of ML-Ask sup-
ported by CAO has not been evaluated thoroughly
as well. In the evaluation we used a test set cre-
ated by Ptaszynski et al (2010b) for the evaluation
of CAO. It consists of thousand sentences randomly
extracted from YACIS and manually annotated with
emotion classes by 42 layperson annotators in an
anonymous survey. There are 418 emotive and 582
non-emotive sentences. We compared the results
on those sentences for ML-Ask, CAO (described in
detail by Ptaszynski et al (2010b)), and both sys-
tems combined. The results showing accuracy, cal-
6Having one sec. to evaluate one sentence, one evaluator
would need 11.2 years to verify the whole corpus (354 mil.s.).
93
Table 6: Emotion class annotations with percentage.
emotion
class
# of
sentences %
emotion
class
# of
sentences %
joy 16,728,452 31% excitement 2,833,388 5%
dislike 10,806,765 20% surprize 2,398,535 5%
fondness 9,861,466 19% gloom 2,144,492 4%
fear 3,308,288 6% anger 1,140,865 2%
relief 3,104,774 6% shame 952,188 2%
culated as a ratio of success to the overall number
of samples, are summarized in Table 4. The perfor-
mance of discrimination between emotive and non-
emotive sentences of ML-Ask baseline was a high
98.8%, which is much higher than in original eval-
uation of ML-Ask (around 90%). This could indi-
cate that sentences with which the system was not
able to deal with appear much less frequently on
Ameblo. As for CAO, it is capable of detecting the
presence of emoticons in a sentence, which is par-
tially equivalent to detecting emotive sentences in
ML-Ask, since emoticons are one type of features
determining sentence as emotive. The performance
of CAO was also high, 97.6%. This was due to the
fact that grand majority of emotive sentences con-
tained emoticons. Finally, ML-Ask supported with
CAO achieved remarkable 100% accuracy. This was
a surprisingly good result, although it must be re-
membered that the test sample contained only 1000
sentences (less than 0.0003% of the whole corpus).
Next we verified emotion class annotations on sen-
tences. The baseline of ML-Ask achieved slightly
better results (73.4%) than in its primary evalua-
tion (Ptaszynski et al, 2009c) (67% of balanced F-
score with P=85.7% and R=54.7%). CAO achieved
80.2%. Interestingly, this makes CAO a better affect
analysis system than ML-Ask. However, the condi-
tion is that a sentence must contain an emoticon. The
best result, close to 90%, was achieved by ML-Ask
supported with CAO. We also checked the results
when only the dimensions of valence and activation
were taken into account. ML-Ask achieved 88.6%,
CAO nearly 95%. Support of CAO toML-Ask again
resulted in the best score, 97.5%.
Statistics of Affective Annotations: There were
nearly twice as many emotive sentences than non-
emotive (ratio 1.94). This suggests that the cor-
pus is biased in favor of emotive contents, which
could be considered as a proof for the assumption
that blogs make a good base for emotion related re-
Table 7: Comparison of positive and negative sentences
between KNB and YACIS.
positive negative ratio
KNB* emotional 317 208 1.52
attitude
opinion 489 289 1.69
merit 449 264 1.70
acceptation 125 41 3.05
or rejection
event 43 63 0.68
sum 1,423 865 1.65
YACIS** only 22,381,992 12,837,728 1.74
only+mostly 23,753,762 13,605,514 1.75
* p<.05, ** p<.01
search. When it comes to statistics of each emo-
tive feature (emoteme), the most frequent class were
interjections. Second frequent was the exclamative
marks class, which includes punctuation marks sug-
gesting emotive engagement (such as ?!?, or ????).
Third frequent emoteme class was emoticons, fol-
lowed by endearments. As an interesting remark,
emoteme class that was the least frequent were vul-
garities. As one possible interpretation of this re-
sult we propose the following. Blogs are social
space, where people describe their experiences to
be read and commented by other people (friends,
colleagues). The use of vulgar language could dis-
courage potential readers from further reading, mak-
ing the blog less popular. Next, we checked the
statistics of emotion classes annotated on emotive
sentences. The results are represented in Table 6.
The most frequent emotions were joy (31%), dislike
(20%) and fondness (19%), which covered over 70%
of all annotations. However, it could happen that
the number of expressions included in each emotion
class database influenced the number of annotations
(database containing many expressions has higher
probability to gather more annotations). Therefore
we verified if there was a correlation between the
number of annotations and the number of emotive
expressions in each emotion class database. The
verification was based on Spearman?s rank corre-
lation test between the two sets of numbers. The
test revealed no statistically significant correlation
between the two types of data, with ?=0.38.
Comparison with Other Emotion Corpora:
Firstly, we compared YACIS with KNB. The KNB
corpus was annotated mostly for the need of sen-
timent analysis and therefore does not contain any
94
Table 8: Comparison of number of emotive expressions
in three different corpora including ratio within this set of
emotions and results of Spearman?s rank correlation test.
Minato et al YACIS Nakamura
dislike 355 (26%) 14,184,697 (23%) 532 (32%)
joy 295 (21%) 22,100,500 (36%) 224 (13%)
fondness 205 (15%) 13,817,116 (22%) 197 (12%)
sorrow 205 (15%) 2,881,166 (5%) 232 (14%)
anger 160 (12%) 1,564,059 (3%) 199 (12%)
fear 145 (10%) 4,496,250 (7%) 147 (9%)
surprise 25 (2%) 3,108,017 (5%) 129 (8%)
Minato et al Minato et al YACIS and
and Nakamura and YACIS Nakamura
Spearman?s ? 0.88 0.63 0.25
information on specific emotion classes. However,
it is annotated with emotion valence for different
categories valence is expressed in Japanese, such
as emotional attitude (e.g., ?to feel sad about X?
[NEG], ?to like X? [POS]), opinion (e.g., ?X is won-
derful? [POS]), or positive/negative event (e.g., ?X
broke down? [NEG], ?X was awarded? [POS]). We
compared the ratios of sentences expressing posi-
tive to negative valence. The comparison was made
for all KNB valence categories separately and as a
sum. In our research we do not make additional sub-
categorization of valence types, but used in the com-
parison ratios of sentences in which the expressed
emotions were of only positive/negative valence and
including the sentences which were mostly (in ma-
jority) positive/negative. The comparison is pre-
sented in table 7. In KNB for all valence categories
except one the ratio of positive to negative sentences
was biased in favor of positive sentences. Moreover,
for most cases, including the ratio taken from the
sums of sentences, the ratio was similar to the one in
YACIS (around 1.7). Although the numbers of com-
pared sentences differ greatly, the fact that the ratio
remains similar across the two different corpora sug-
gests that the Japanese express in blogs more posi-
tive than negative emotions.
Next, we compared the corpus created by Minato
et al (2006). This corpus was prepared on the ba-
sis of an emotive expression dictionary. Therefore
we compared its statistics not only to YACIS, but
also to the emotive lexicon used in our research (see
section 4 for details). Emotion classes used in Mi-
nato et al differ slightly to those used in our re-
search (YACIS and Nakamura?s dictionary). For
example, they use class name ?hate? to describe
what in YACIS is called ?dislike?. Moreover, they
have no classes such as excitement, relief or shame.
To make the comparison possible we used only the
emotion classes appearing in both cases and unified
all class names. The results are summarized in Ta-
ble 8. There was no correlation between YACIS and
Nakamura (?=0.25), which confirms the results cal-
culated in previous paragraph. A medium correla-
tion was observed between YACIS and Minato et al
(?=0.63). Finally, a strong correlation was observed
between Minato et al and Nakamura (?=0.88),
which is the most interesting observation. Both Mi-
nato et al and Nakamura are in fact dictionaries of
emotive expressions. However, the dictionaries were
collected in different times (difference of about 20
years), by people with different background (lexi-
cographer vs. language teacher), based on differ-
ent data (literature vs. conversation) assumptions
and goals (creating a lexicon vs. Japanese language
teaching). The only similarity is in the methodol-
ogy. In both cases the dictionary authors collected
expressions considered to be emotion-related. The
fact that they correlate so strongly suggests that for
the compared emotion classes there could be a ten-
dency in language to create more expressions to de-
scribe some emotions rather than the others (dislike,
joy and fondness are often some of the most frequent
emotion classes). This phenomenon needs to be ver-
ified more thoroughly in the future.
6 Applications
6.1 Extraction of Evaluation Datasets
In evaluation of sentiment and affect analysis sys-
tems it is very important to provide a statistically
reliable random sample of sentences or documents
as a test set (to be further annotated by laypeople).
The larger is the source, the more statistically reli-
able is the test set. Since YACIS contains 354 mil.
sentences in 13 mil. documents, it can be considered
sufficiently reliable for the task of test set extraction,
as probability of extracting twice the same sentence
is close to zero. Ptaszynski et al (2010b) already
used YACIS to randomly extract a 1000 sentence
sample and used it in their evaluation of emoticon
analysis system. The sample was also used in this
research and is described in more detail in section 5.
95
6.2 Generation of Emotion Object Ontology
One of the applications of large corpora is to
extract from them smaller sub-corpora for specified
tasks. Ptaszynski et al (2012a) applied YACIS
for their task of generating an robust emotion
object ontology. They used cross-reference of
annotations of emotional information described
in this paper and syntactic annotations done
by Ptaszynski et al (2012b) to extract only
sentences in which expression of emotion was
proceeded by its cause, like in the example below.
Kanojo ni furareta kara kanashii...
Girlfriend DAT dump PAS CAUS sad ...
I?m sad becausemy girlfriend dumped me...
The example can be analyzed in the following way.
Emotive expression (kanashii, ?sad?) is related with
the sentence contents (Kanojo ni furareta, ?my
girlfriend dumped me?) with a causality morpheme
(kara, ?because?). In such situation the sentence
contents represent the object of emotion. This can
be generalized to the following meta-structure,
OE CAUS XE ,
where OE=[Emotion object], CAUS=[causal
form], and XE=[expression of emotion].
The cause phrases were cleaned of irrelevant
words like stop words to leave only the object
phrases. The evaluation showed they were able to
extract nearly 20 mil. object phrases, from which
80% was extracted correctly with a reliable signifi-
cance. Thanks to rich annotations on YACIS corpus
the ontology included such features as emotion class
(joy, anger, etc.), dimensions (valence/activation),
POS or semantic categories (hypernyms, etc.).
6.3 Retrieval of Moral Consequence of Actions
Third application of the YACIS corpus annotated
with affect- and sentiment-related information has
been in a novel research on retrieval of moral con-
sequences of actions, first proposed by Rzepka and
Araki (2005) and recently developed by Komuda et
al. (2010)7. The moral consequence retrieval agent
was based on the idea of Wisdom of Crowd. In
particular Komuda et al (2010) used a Web-mining
7See also a mention in Scientific American, by Anderson and
Anderson (2010).
technique to gather consequences of actions apply-
ing causality relations, like in the research described
in section 6.2, but with a reversed algorithm and
lexicon containing not only emotional but also eth-
ical notions. They cross-referenced emotional and
ethical information about a certain phrase (such as
?To kill a person.?) to obtain statistical probability
for emotional (?feeling sad?, ?being in joy?, etc.)
and ethical consequences (?being punished?, ?being
praised?, etc.). Initially, the moral agent was based
on the whole Internet contents. However, multiple
queries to search engine APIs made by the agent
caused constant blocking of IP address an in effect
hindered the development of the agent.
The agent was tested on over 100 ethically-
significant real world problems, such as ?killing a
man?, ?stealing money?, ?bribing someone?, ?help-
ing people? or ?saving environment?. In result 86%
of recognitions were correct. Some examples of the
results are presented in the Appendix on the end of
this paper.
7 Conclusions
We performed automatic annotation of a five-
billion-word corpus of Japanese blogs with informa-
tion on affect and sentiment. A survey in emotion
blog corpora showed there has been no large scale
emotion corpus available for the Japanese language.
We chose YACIS, a large-scale blog corpus and
annotated it using two systems for affect analysis
for word- and sentence-level affect analysis and for
analysis of emoticons. The annotated information
included affective features like sentence subjectivity
(emotive/non-emotive) or emotion classes (joy, sad-
ness, etc.), useful in affect analysis and information
on sentence valence/polarity (positive/negative) use-
ful in sentiment analysis obtained as generalizations
of those features on a 2-dimensional model of af-
fect. We evaluated the annotations in several ways.
Firstly, on a test set of thousand sentences extracted
and evaluated by over forty respondents. Secondly,
we compared the statistics of annotations to other
existing emotion corpora. Finally, we showed sev-
eral tasks the corpus has already been applied in,
such as generation of emotion object ontology or re-
trieval of emotional and moral consequences of ac-
tions.
96
Acknowledgments
This research was supported by (JSPS) KAKENHI
Grant-in-Aid for JSPS Fellows (Project Number: 22-
00358).
References
Ahmed Abbasi and Hsinchun Chen. ?Affect Intensity Analysis
of Dark Web Forums?, Intelligence and Security Informatics
2007, pp. 282-288, 2007
Saima Aman and Stan Szpakowicz. 2007. ?Identifying Ex-
pressions of Emotion in Text?. In Proceedings of the 10th
International Conference on Text, Speech, and Dialogue
(TSD-2007), Lecture Notes in Computer Science (LNCS),
Springer-Verlag.
Michael Anderson and Susan Leigh Anderson. 2010. ?Robot be
Good?, Scientific American, October, pp. 72-77.
Dipankar Das, Sivaji Bandyopadhyay, ?Labeling Emotion in
Bengali Blog Corpus ? A Fine Grained Tagging at Sentence
Level?, Proceedings of the 8thWorkshop on Asian Language
Resources, pages 47?55, 2010.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, Eros
Zanchetta. 2008. ?The WaCky Wide Web: A Collection
of Very Large Linguistically Processed Web-Crawled Cor-
pora?, Kluwer Academic Publishers, Netherlands.
Marco Baroni and Motoko Ueyama. 2006. ?Building General-
and Special-Purpose Corpora by Web Crawling?, In Pro-
ceedings of the 13th NIJL International Symposium on
Language Corpora: Their Compilation and Application,
www.tokuteicorpus.jp/result/pdf/2006 004.pdf
Ju?rgen Broschart. 1997. ?Why Tongan does it differently: Cate-
gorial Distinctions in a Language without Nouns and Verbs.?
Linguistic Typology, Vol. 1, No. 2, pp. 123-165.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Hale
and Mark Johnson. 2000. ?BLLIP 1987-89 WSJ Corpus
Release 1?, Linguistic Data Consortium, Philadelphia,
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalog
Id=LDC2000T43
Paul Ekman. 1992. ?An Argument for Basic Emotions?. Cogni-
tion and Emotion, Vol. 6, pp. 169-200.
Irena Srdanovic Erjavec, Tomaz Erjavec and Adam Kilgarriff.
2008. ?A web corpus and word sketches for Japanese?, Infor-
mation and Media Technologies, Vol. 3, No. 3, pp.529-551.
Katarzyna G?owin?ska and Adam Przepio?rkowski. 2010. ?The
Design of Syntactic Annotation Levels in the National Cor-
pus of Polish?, In Proceedings of LREC 2010.
Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras Rung,
Istvan Szakadat and Vikto Tron. 2004. ?Creating open lan-
guage resources for Hungarian?. In Proceedings of the
LREC, Lisbon, Portugal.
Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawahara,
Keiji Shinzato andMasaaki Nagata. 2011. ?Construction of a
Blog Corpus with Syntactic, Anaphoric, and Sentiment An-
notations? [in Japanese], Journal of Natural Language Pro-
cessing, Vol 18, No. 2, pp. 175-201.
Ichiro Hiejima. 1995. A short dictionary of feelings and emo-
tions in English and Japanese, Tokyodo Shuppan.
Paul J. Hopper and Sandra A. Thompson. 1985. ?The Iconic-
ity of the Universal Categories ?Noun? and ?Verbs??. In Ty-
pological Studies in Language: Iconicity and Syntax. John
Haiman (ed.), Vol. 6, pp. 151-183, Amsterdam: John Ben-
jamins Publishing Company.
Daisuke Kawahara and Sadao Kurohashi. 2006. ?A Fully-
Lexicalized Probabilistic Model for Japanese Syntactic and
Case Structure Analysis?, Proceedings of the Human Lan-
guage Technology Conference of the North American Chap-
ter of the ACL, pp. 176-183.
Radoslaw Komuda, Michal Ptaszynski, Yoshio Momouchi,
Rafal Rzepka, and Kenji Araki. 2010. ?Machine Moral De-
velopment: Moral Reasoning Agent Based on Wisdom of
Web-Crowd and Emotions?, Int. Journal of Computational
Linguistics Research, Vol. 1 , Issue 3, pp. 155-163.
Taku Kudo and Hideto Kazawa. 2009. ?Japanese Web N-gram
Version 1?, Linguistic Data Consortium, Philadelphia,
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalog
Id=LDC2009T08
Vinci Liu and James R. Curran. 2006. ?Web Text Corpus for
Natural Language Processing?, In Proceedings of the 11th
Meeting of the European Chapter of the Association for
Computational Linguistics (EACL), pp. 233-240.
Maciejewski, J., Ptaszynski, M., Dybala, P. 2010. ?Developing
a Large-Scale Corpus for Natural Language Processing and
Emotion Processing Research in Japanese?, In Proceedings
of the International Workshop on Modern Science and Tech-
nology (IWMST), pp. 192-195.
Kazuyuki Matsumoto, Yusuke Konishi, Hidemichi Sayama,
Fuji Ren. 2011. ?Analysis of Wakamono Kotoba Emotion
Corpus and Its Application in Emotion Estimation?, Interna-
tional Journal of Advanced Intelligence, Vol.3,No.1,pp.1-24.
Junko Minato, David B. Bracewell, Fuji Ren and Shingo
Kuroiwa. 2006. ?Statistical Analysis of a Japanese Emotion
Corpus for Natural Language Processing?, LNCS 4114.
Gilad Mishne. 2005. ?Experiments with Mood Classification in
Blog Posts?. In The 1st Workshop on Stylistic Analysis of
Text for Information Access, at SIGIR 2005, August 2005.
Akira Nakamura. 1993. ?Kanjo hyogen jiten? [Dictionary of
Emotive Expressions] (in Japanese), Tokyodo Publishing,
Tokyo, 1993.
Jan Pomika?lek, Pavel Rychly? and Adam Kilgarriff. 2009. ?Scal-
ing to Billion-plusWord Corpora?, In Advances in Computa-
tional Linguistics, Research in Computing Science, Vol. 41,
pp. 3-14.
Michal Ptaszynski, Pawel Dybala, Wenhan Shi, Rafal Rzepka
and Kenji Araki. 2009. ?A System for Affect Analysis of Ut-
terances in Japanese Supported with Web Mining?, Journal
of Japan Society for Fuzzy Theory and Intelligent Informat-
ics, Vol. 21, No. 2, pp. 30-49 (194-213).
Michal Ptaszynski, Pawel Dybala, Wenhan Shi, Rafal Rzepka
and Kenji Araki. 2009. ?Towards Context Aware Emotional
Intelligence in Machines: Computing Contextual Appro-
priateness of Affective States?. In Proceedings of Twenty-
first International Joint Conference on Artificial Intelligence
(IJCAI-09), Pasadena, California, USA, pp. 1469-1474.
Michal Ptaszynski, Pawel Dybala, Rafal Rzepka and Kenji
Araki. 2009. ?Affecting Corpora: Experiments with Au-
tomatic Affect Annotation System - A Case Study of
97
the 2channel Forum -?, In Proceedings of the Conference
of the Pacific Association for Computational Linguistics
(PACLING-09), pp. 223-228.
Michal Ptaszynski, Rafal Rzepka and Kenji Araki. 2010a. ?On
the Need for Context Processing in Affective Computing?,
In Proceedings of Fuzzy System Symposium (FSS2010), Or-
ganized Session on Emotions, September 13-15.
Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala, Rafal
Rzepka and Kenji Araki. 2010b. ?CAO: Fully Automatic
Emoticon Analysis System?, In Proc. of the 24th AAAI Con-
ference on Artificial Intelligence (AAAI-10), pp. 1026-1032.
Michal Ptaszynski, Rafal Rzepka, Kenji Araki and Yoshio Mo-
mouchi. 2012a. ?A Robust Ontology of Emotion Objects?, In
Proceedings of The Eighteenth Annual Meeting of The Asso-
ciation for Natural Language Processing (NLP-2012), pp.
719-722.
Michal Ptaszynski, Rafal Rzepka, Kenji Araki and Yoshio Mo-
mouchi. 2012b. ?Annotating Syntactic Information on 5.5
Billion Word Corpus of Japanese Blogs?, In Proceedings
of The 18th Annual Meeting of The Association for Natural
Language Processing (NLP-2012), pp. 385-388.
Changqin Quan and Fuji Ren. 2010. ?A blog emotion corpus
for emotional expression analysis in Chinese?, Computer
Speech & Language, Vol. 24, Issue 4, pp. 726-749.
Rafal Rzepka, Kenji Araki. 2005. ?What Statistics Could Do
for Ethics? - The Idea of Common Sense Processing Based
Safety Valve?, AAAI Fall Symposium on Machine Ethics,
Technical Report FS-05-06, pp. 85-87.
James A. Russell. 1980. ?A circumplex model of affect?. J. of
Personality and Social Psychology, Vol. 39, No. 6, pp. 1161-
1178.
Peter D. Turney and Michael L. Littman. 2002. ?Unsupervised
Learning of Semantic Orientation from a Hundred-Billion-
Word Corpus?, National Research Council, Institute for In-
formation Technology, Technical Report ERB-1094. (NRC
#44929).
Masao Utiyama and Hitoshi Isahara. 2003. ?Reliable Mea-
sures for Aligning Japanese-English News Articles and Sen-
tences?. ACL-2003, pp. 72-79.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. ?An-
notating expressions of opinions and emotions in language?.
Language Resources and Evaluation, Vol. 39, Issue 2-3, pp.
165-210.
Theresa Wilson and Janyce Wiebe. 2005. ?Annotating Attribu-
tions and Private States?, In Proceedings of the ACL Work-
shop on Frontiers in Corpus Annotation II, pp. 53-60.
Annie Zaenen and Livia Polanyi. 2006. ?Contextual Valence
Shifters?. In Computing Attitude and Affect in Text, J. G.
Shanahan, Y. Qu, J. Wiebe (eds.), Springer Verlag, Dor-
drecht, The Netherlands, pp. 1-10.
Appendix. Examples of emotional and
ethical consequence retrieval.
SUCCESS CASES
emotional
conseq. results score
ethical
conseq. results score
?To hurt somebody.?
anger 13.01/54.1 0.24 penalty/ 4.01/7.1 0.565
fear 12.01/54.1 0.22 punishment
sadness 11.01/54.1 0.2
?To kill one?s own mother.?
sadness 9.01/35.1 0.26 penalty/ 5.01/5.1 0.982
surprise 6.01/35.1 0.17 punishment
anger 5.01/35.1 0.14
?To steal an apple.?
surprise 2.01/6.1 0.33 reprimand/ 3.01/3.1 0.971
anger 2.01/6.1 0.33 scold
?To steal money.?
anger 3.01/9.1 0.33 penalty/punish.3.01/6.1 0.493
sadness 2.01/9.1 0.22 reprimand/sco. 2.01/6.1 0.330
?To kill an animal.?
dislike 7.01/23.1 0.3 penalty/ 36.01/45.1 0.798
sadness 5.01/23.1 0.22 punishment
?To drive after drinking.?
fear 6.01/19.1 0.31 penalty/punish.24.01/36.1 0.665
?To cause a war.?
dislike 7.01/15.1 0.46 illegal 2.01/3.1 0.648
fear 3.01/15.1 0.2
?To stop a war.?
joy 6.01/13.1 0.46 forgiven 1.01/1.1 0.918
surprise 2.01/13.1 0.15
?To prostitute oneself.?
anger 6.01/19.1 0.31 illegal 12.01/19.1 0.629
sadness 5.01/19.1 0.26
?To have an affair.?
sadness 10,01/35.1 0.29 penalty/punish.8.01/11.1 0.722
anger 9.01/35.1 0.26
INCONSISTENCY BETWEEN EMOTIONS AND ETHICS
?To kill a president.?
joy 2.01/4.1 0.49 penalty/ 2.01/2.1 0.957
likeness 1.01/4.1 0.25 punishment
?To kill a criminal.?
joy 8.01/39.1 0.2 penalty/ 556/561 0.991
excite 8.01/39.1 0.2 punishment
anger 7.01/39.1 0.18
CONTEXT DEPENDENT
?To act violently.?
anger 4.01/11.1 0.36 penalty/punish.1.01/2.1 0.481
fear 2.01/11.1 0.18 agreement 1.01/2.1 0.481
NO ETHICAL CONSEQUENCES
?Sky is blue.?
joy 51.01/110,1 0.46 none 0 0
sadness 21.01/110,1 0.19
98
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 59?65,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Emotive or Non-emotive: That is The Question
Michal Ptaszynski Fumito Masui
Department of Computer Science,
Kitami Institute of Technology
{ptaszynski,f-masui}@
cs.kitami-it.ac.jp
Rafal Rzepka Kenji Araki
Graduate School of Information Science
and Technology, Hokkaido University
{rzepka,araki}@
ist.hokudai.ac.jp
Abstract
In this research we focus on discriminat-
ing between emotive (emotionally loaded)
and non-emotive sentences. We define the
problem from a linguistic point of view as-
suming that emotive sentences stand out
both lexically and grammatically. We
verify this assumption experimentally by
comparing two sets of such sentences in
Japanese. The comparison is based on
words, longer n-grams as well as more so-
phisticated patterns. In the classification
we use a novel unsupervised learning algo-
rithm based on the idea of language com-
binatorics. The method reached results
comparable to the state of the art, while
the fact that it is fully automatic makes it
more efficient and language independent.
1 Introduction
Recently the field of sentiment analysis has at-
tracted great interest. It has become popular to
try different methods to distinguish between sen-
tences loaded with positive and negative senti-
ments. However, a few research focused on a task
more generic, namely, discriminating whether a
sentence is even loaded with emotional content or
not. The difficulty of the task is indicated by three
facts. Firstly, the task has not been widely un-
dertaken. Secondly, in research which addresses
the challenge, the definition of the task is usually
based on subjective ad hoc assumptions. Thirdly,
in research which do tackle the problem in a sys-
tematic way, the results are usually unsatisfactory,
and satisfactory results can be obtained only with
large workload.
We decided to tackle the problem in a standard-
ized and systematic way. We defined emotionally
loaded sentences as those which in linguistics are
described as fulfilling the emotive function of lan-
guage. We assumed that there are repetitive pat-
terns which appear uniquely in emotive sentences.
We performed experiments using a novel unsu-
pervised clustering algorithm based on the idea
of language combinatorics. By using this method
we were also able to minimize human effort and
achieve F-score comparable to the state of the art
with much higher Recall rate.
The outline of the paper is as follows. We
present the background for this research in Section
2. Section 3 describes the language combinatorics
approach which we used to compare emotive and
non-emotive sentences. In section 4 we describe
our dataset and experiment settings. The results of
the experiment are presented in Section 5. Finally
the paper is concluded in Section 6.
2 Background
There are different linguistic means used to in-
form interlocutors of emotional states in an ev-
eryday communication. The emotive meaning is
conveyed verbally and lexically through exclama-
tions (Beijer, 2002; Ono, 2002), hypocoristics (en-
dearments) (Kamei et al., 1996), vulgarities (Crys-
tal, 1989) or, for example in Japanese, through
mimetic expressions (gitaigo) (Baba, 2003). The
function of language realized by such elements of
language conveying emotive meaning is called the
emotive function of language. It was first distin-
guished by B?uhler (1934-1990) in his Sprachthe-
orie as one of three basic functions of language
1
.
B?uhler?s theory was picked up later by Jakobson
(1960), who by distinguishing three other func-
tions laid the grounds for structural linguistics and
communication studies.
2.1 Previous Research
Detecting whether sentences are loaded with emo-
tional content has been undertaken by a number
1
The other two being descriptive and impressive.
59
of researchers, most often as an additional task
in either sentiment analysis (SA) or affect analy-
sis (AA). SA, in great simplification, focuses on
determining whether a language entity (sentence,
document) was written with positive or negative
attitude toward its topic. AA on the other hand
focuses on specifying which exactly emotion type
(joy, anger, etc.) has been conveyed. The fact,
that the task was usually undertaken as a subtask,
influences the way it was formulated. Below we
present some of the most influential works on the
topic, but formulating it in slightly different terms.
Emotional vs. Neutral: Discriminating whe-
ther a sentence is emotional or neutral is to answer
the question of whether it can be interpreted as
produced in an emotional state. This way the task
was studied byMinato et al. (2006), Aman and Sz-
pakowicz (2007) or Neviarouskaya et al. (2011).
Subjective vs. Objective: Discriminating be-
tween subjective and objective sentences is to
say whether the speaker presented the sentence
contents from a first-person-centric perspective or
from no specific perspective. The research formu-
lating the problem this way include e.g, Wiebe et
al. (1999), who classified subjectivity of sentences
using naive Bayes classifier, or later Wilson and
Wiebe (2005). In other research Yu and Hatzi-
vassiloglou (2003) used supervised learning to de-
tect subjectivity and Hatzivassiloglou and Wiebe
(2012) studied the effect of gradable adjectives on
sentence subjectivity.
Emotive vs. Non-emotive: Saying that a sen-
tence is emotive means to specify the linguistic
features of language which where used to produce
a sentence uttered with emphasis. Research that
formulated and tackled the problem this way was
done by, e.g., Ptaszynski et al. (2009).
Each of the above nomenclature implies sim-
ilar, though slightly different assumptions. For
example, a sentence produced without any emo-
tive characteristics (non-emotive) could still im-
ply emotional state in some situations. Also Bing
and Zhang (2012) notice that ?not all subjective
sentences express opinions and those that do are
a subgroup of opinionated sentences.? A compari-
son of the scopes and overlaps of different nomen-
clature is represented in Figure 1. In this research
we formulate the problem similarly to Ptaszynski
et al. (2009), therefore we used their system to
compare with our method.
Figure 1: Comparison of between different
nomenclature used in sentiment analysis research.
3 Language Combinatorics
The idea of language combinatorics (LC) assumes
that patterns with disjoint elements provide bet-
ter results than the usual bag-of-words or n-gram
approach (Ptaszynski et al., 2011). Such patterns
are defined as ordered non-repeated combinations
of sentence elements. They are automatically ex-
tracted by generating all ordered combinations of
sentence elements and verifying their occurrences
within a corpus.
In particular, in every n-element sentence there
is k-number of combination clusters, such as that
1 ? k ? n, where k represents all k-element com-
binations being a subset of n. The number of com-
binations generated for one k-element cluster of
combinations is equal to binomial coefficient, like
in eq. 1. Thus the number of all possible combina-
tions generated for all values of k from the range
of {1, ..., n} is equal to the sum of all combina-
tions from all k-element clusters, like in eq. 2.
(
n
k
)
=
n!
k!(n? k)!
(1)
n
?
k=1
(
n
k
)
=
n!
1!(n? 1)!
+
n!
2!(n? 2)!
+ ... +
n!
n!(n? n)!
= 2
n
? 1
(2)
One problem with combinatorial approach is the
phenomenon of exponential and rapid growth of
function values during combinatorial manipula-
tions, called combinatorial explosion (Krippen-
dorff, 1986). Since this phenomenon causes long
processing time, combinatorial approaches have
been often disregarded. We assumed however,
that it could be dealt with when the algorithm
is optimized to the requirements of the task. In
preliminary experiments Ptaszynski et al. (2011)
used a generic sentence pattern extraction archi-
tecture SPEC to compare the amounts of generated
sophisticated patterns with n-grams, and noticed
that it is not necessary to generate patterns of all
lengths, since the most useful ones usually appear
in the group of 2 to 5 element patterns. Follow-
ing their experience we limit the pattern length in
our research to 6 elements. All non-subsequent el-
60
Table 1: Some examples from the dataset representing emotive and non-emotive sentences close in
content, but differing in emotional load expressed in the sentence (Romanized Japanese / Translation).
emotive non-emotive
Takasugiru kara ne / ?Cause its just too expensive K?ogaku na tame desu. / Due to high cost.
Un, umai, kangeki da. / Oh, so delicious, I?m impressed. Kono kar?e wa karai. / This curry is hot.
Nanto ano hito, kekkon suru rashii yo! / Have you heard? She?s getting married! Ano hito ga kekkon suru rashii desu. / They say she is gatting married.
Ch?o ha ga itee / Oh, how my tooth aches! Ha ga itai / A tooth aches
Sugoku kirei na umi da naaa / Oh, what a beautiful sea! Kirei na umi desu / This is a beautiful sea
ements are also separated with an asterisk (?*?) to
mark disjoint elements.
The weight w
j
of each pattern generated this
way is calculated, according to equation 3, as a
ratio of all occurrences of a pattern in one corpus
O
pos
to the sum of occurrences in two compared
corporaO
pos
+O
neg
. The weights are also normal-
ized to fit in range from +1 (representing purely
emotive patterns) to -1 (representing purely non-
emotive patterns). The normalization is achieved
by subtracting 0.5 from the initial score and mul-
tiplying this intermediate product by 2. The score
of one sentence is calculated as a sum of weights
of patterns found in the sentence, like in eq. 4.
w
j
=
(
O
pos
O
pos
+ O
neg
? 0.5
)
? 2 (3)
score =
?
w
j
, (1 ? w
j
? ?1) (4)
The weight can be further modified by either
? awarding length k, or
? awarding length k and occurrence O.
The list of generated frequent patterns can also be
further modified. When two collections of sen-
tences of opposite features (such as ?emotive vs.
non-emotive?) are compared, a generated list will
contain patterns appearing uniquely on only one
of the sides (e.g. uniquely emotive patterns and
uniquely non-emotive patterns) or in both (am-
biguous patterns). Therefore the pattern list can
be modified by deleting
? all ambiguous patterns, or
? only ambiguous patterns appearing in the same
number on both sides (later called ?zero pat-
terns?, since their weight is equal 0).
Moreover, since a list of patterns will contain both
the sophisticated patterns as well usual n-grams,
the experiments were performed separately for all
patterns and n-grams only. Also, if the initial col-
lection was biased toward one of the sides (sen-
tences of one kind were longer or more numer-
ous), there will be more patterns of a certain sort.
To mitigate this bias, instead of applying a rule of
thumb, the threshold was optimized automatically.
4 Experiments
4.1 Dataset Preparation
In the experiments we used a dataset developed by
Ptaszynski et al. (2009) for the needs of evaluating
their affect analysis system ML-Ask for Japanese
language. The dataset contains 50 emotive and 41
non-emotive sentences. It was created as follows.
Thirty people of different age and social groups
participated in an anonymous survey. Each partic-
ipant was to imagine or remember a conversation
with any person they know and write three sen-
tences from that conversation: one free, one emo-
tive, and one non-emotive. Additionally, the par-
ticipants were asked to make the emotive and non-
emotive sentences as close in content as possible,
so the only difference was whether a sentence was
loaded with emotion or not. The participants also
annotated on their own free utterances whether or
not they were emotive. Some examples from the
dataset are represented in Table 1.
In our research the above dataset was further
preprocessed to make the sentences separable into
elements. We did this in three ways to check how
the preprocessing influences the results. We used
MeCab
2
, a morphological analyzer for Japanese
to preprocess the sentences from the dataset in the
three following ways:
? Tokenization: All words, punctuation marks,
etc. are separated by spaces.
? Parts of speech (POS): Words are replaced
with their representative parts of speech.
? Tokens with POS: Both words and POS infor-
mation is included in one element.
The examples of preprocessing are represented
in Table 2. In theory, the more generalized a sen-
tence is, the less unique patterns it will produce,
but the produced patterns will be more frequent.
This can be explained by comparing tokenized
sentence with its POS representation. For exam-
ple, in the sentence from Table 2 we can see that
a simple phrase kimochi ii (?feeling good?) can be
2
https://code.google.com/p/mecab/
61
Table 2: Three kinds of preprocessing of a sen-
tence in Japanese; N = noun, TOP = topic marker,
ADV = adverbial particle, ADJ = adjective, COP
= copula, EXCL = exclamation mark.
Sentence:
Transliteration: Ky?owanantekimochiiihinanda!
Glossing: Today TOP what pleasant day COP EXCL
Translation: What a pleasant day it is today!
Preprocessing examples
1. Words: Ky?o wa nante kimochi ii hi nanda !
2. POS: N TOP ADV N ADJ N COP EXCL
3.Words+POS: Ky?o[N] wa[TOP] nante[ADV]
kimochi[N] ii[ADJ] hi[N] nanda[COP] ![EXCL]
represented by a POS pattern N ADJ. We can eas-
ily assume that there will be more N ADJ patterns
than kimochi ii, because many word combinations
can be represented as N ADJ. Therefore POS pat-
terns will come in less variety but with higher oc-
currence frequency. By comparing the result of
classification using different preprocessing meth-
ods we can find out whether it is better to represent
sentences as more generalized or as more specific.
4.2 Experiment Setup
The experiment was performed three times, once
for each kind of preprocessing. Each time 10-
fold cross validation was performed and the results
were calculated using Precision (P), Recall (R)
and balanced F-score (F) for each threshold. We
verified which version of the algorithm achieves
the top score within the threshold span. However,
an algorithm could achieve the best score for one
certain threshold, while for others it could perform
poorly. Therefore we also looked at which ver-
sion achieves high scores for the longest threshold
span. This shows which algorithm is more bal-
anced. Finally, we checked the statistical signifi-
cance of the results. We used paired t-test because
the classification results could represent only one
of two classes (emotive or non-emotive). We also
compared the performance to the state of the art,
namely the affect analysis system ML-Ask devel-
oped by Ptaszynski et al. (2009).
5 Results and Discussion
The overall F-score results were generally the best
for the datasets containing in order: both tokens
and POS, tokens only and POS only. The F-
scores for POS-preprocessed sentences revealed
the least constancy. For many cases n-grams
scored higher than all patterns, but almost none of
Table 3: Best results for each version of the
method compared with the ML-Ask system.
ML-Ask
SPEC
tokenized POS token-POS
n-grams patterns n-grams patterns n-grams patterns
Precision 0.80 0.61 0.6 0.68 0.59 0.65 0.64
Recall 0.78 1.00 0.96 0.88 1.00 0.95 0.95
F-score 0.79 0.75 0.74 0.77 0.74 0.77 0.76
the results reached statistical significance. The F-
score results for the tokenized dataset were also
not unequivocal. For higher thresholds patterns
scored higher, while for lower thresholds the re-
sults were similar. The scores were rarely sig-
nificant, utmost at 5% level (p<0.05), however,
in all situations where n-grams visibly scored
higher, the differences were not statistically sig-
nificant. Finally, for the preprocessing including
both tokens and POS information, pattern-based
approach achieved significantly better results (p-
value <0.01 or <0.001). The algorithm reached
its plateau at F-score around 0.73?0.74 for to-
kens and POS separately, and 0.75?0.76 for to-
kens with POS together. In the POS dataset the
elements were more abstracted, while in token-
POS dataset the elements were more specific, pro-
ducing a larger number, but less frequent patterns.
Lower scores for POS dataset could suggest that
the algorithm works better with less abstracted
preprocessing. Examples of F-score comparison
between n-grams and patterns for tokenized and
token-POS datasets are represented in Figures 2
and 3, respectively.
Results for Precision showed similar tenden-
cies. They were the most ambiguous for POS pre-
processing. For the tokenized dataset, although
there always was one or two thresholds for which
n-grams scored higher, scores for patterns were
more balanced, starting with a high score and de-
creasing slowly. As for the token-POS preprocess-
ing patterns achieved higher Precision for most of
the threshold span. The highest Precision of all
was achieved in this dataset by patterns with P =
0.87 for R = 0.50.
As for Recall, the scores were consistent for
all kinds of preprocessing, with higher scores for
patterns within most of the threshold span and
equaling while the threshold decreases. The high-
est scores achieved for each preprocessing for n-
grams and patterns are represented in Table 3.
The affect analysis system ML-Ask (Ptaszynski
et al., 2009) on the same dataset reached F = 0.79,
P = 0.8 and R = 0.78. The results were generally
62
comparable, however slightly higher for ML-Ask
when it comes to P and F-score. R was always bet-
ter for the proposed method. However, ML-Ask is
a system requiring handcrafted lexicons, while our
method is fully automatic, learning the patterns
from data, not needing any particular preparations,
which makes it more efficient.
5.1 Detailed Analysis of Learned Patterns
Within some of the most frequently appearing
emotive patterns there were for example: !
(exclamation mark), n*yo, cha (emotive verb
modification), yo (exclamative sentence ending
particle), ga*yo, n*! or naa (interjection). Some
examples of sentences containing those patterns
are below (patterns underlined). Interestingly,
most elements of those patterns appear in ML-Ask
handcrafted databases, which suggests it could
be possible to improve ML-Ask performance by
extracting additional patterns with SPEC.
Ex. 1. Megane, soko ni atta nda yo. (The glasses
were over there!)
Ex. 2. Uuun, butai ga mienai yo. (Ohh, I cannot
see the stage!)
Ex. 3. Aaa, onaka ga suita yo. (Ohh, I?m so
hungry)
Another advantage of our method is the fact that
it can mark both emotive and non-emotive ele-
ments in sentence, while ML-Ask is designed to
annotate only emotive elements. Some examples
of extracted non-emotive patterns were for exam-
ple: desu, wa*desu, mashi ta, or te*masu. All of
them were patterns described in linguistic litera-
ture as typically non-emotive, consisting in copu-
las (desu), verb endings (masu, mashi ta). Some
sentence examples with those patterns include:
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sc
or
e
threshold
F-score
all_patterns
ngrams
Figure 2: F-score comparison between n-grams
and patterns for tokenized detaset (p = 0.0209).
Ex. 4. K?ogaku na tame desu. (Due to high cost.)
Ex. 5. Kirei na umi desu (This is a beautiful sea)
Ex. 6. Kyo wa yuki ga futte imasu. (It is snowing
today.)
6 Conclusions and Future Work
We presented a method for automatic extraction
of patterns from emotive sentences. We assumed
emotive sentences are distinguishable both lex-
ically and grammatically and performed experi-
ments to verify this assumption. In the experi-
ments we used a set of emotive and non-emotive
sentences preprocessed in different ways (tokens,
POS, token-POS) The patterns extracted from
sentences were applied to recognize emotionally
loaded sentences.
The algorithm reached its plateau for F-score
around 0.75?0.76 for patterns containing both to-
kens and POS information. Precision for patterns
was balanced, while for n-grams, although occa-
sionally achieving high scores, it was quickly de-
creasing. Recall scores were almost always better
for patterns. The generally lower results for POS-
represented sentences suggest that the algorithm
works better with less abstracted elements.
The results of the proposed method and the af-
fect analysis system ML-Ask were comparable.
ML-Ask achieved better Precision, but lower Re-
call. However, our method is more efficient as
it does not require handcrafted lexicons. More-
over, automatically extracted patterns overlap with
handcrafted databases of ML-Ask, which suggests
it could be possible to improve ML-Ask perfor-
mance with our method. In the near future we plan
to perform experiments on larger datasets, also in
other languages, such as English or Chinese.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sc
or
e
threshold
F-score
all_patterns
ngrams
Figure 3: F-score comparison for n-grams and pat-
terns for dataset with tokens and POS (p = 0.001).
63
References
Saima Aman and Stan Szpakowicz. 2007. Iden-
tifying expressions of emotion in text. In Pro-
ceedings of the 10th International Conference
on Text, Speech, and Dialogue (TSD-2007),
Lecture Notes in Computer Science (LNCS),
Springer-Verlag.
Junko Baba. 2003. Pragmatic function of Japanese
mimetics in the spoken discourse of varying
emotive intensity levels. Journal of Pragmatics,
Vol. 35, No. 12, pp. 1861-1889, Elsevier.
Fabian Beijer. 2002. The syntax and pragmatics
of exclamations and other expressive/emotional
utterances. Working Papers in Linguistics 2,
The Dept. of English in Lund.
Bing Liu, Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text
Data, pp. 415-463. Springer.
Karl B?uhler. 1990. Theory of Language. Represen-
tational Function of Language. John Benjamins
Publ. (reprint from Karl B?uhler. Sprachtheorie.
Die Darstellungsfunktion der Sprache, Ullstein,
Frankfurt a. M., Berlin, Wien, 1934.)
David Crystal. 1989. The Cambridge Encyclope-
dia of Language. Cambridge University Press.
Vasileios Hatzivassiloglou and Janice Wiebe. Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of Inter-
national Conference on Computational Linguis-
tics (COLING-2000), pp. 299-305, 2000.
Roman Jakobson. 1960. Closing Statement: Lin-
guistics and Poetics. Style in Language, pp.350-
377, The MIT Press.
Takashi Kamei, Rokuro Kouno and Eiichi Chino
(eds.). 1996. The Sanseido Encyclopedia of Lin-
guistics, Vol. VI, Sanseido.
Klaus Krippendorff. 1986. Combinatorial Explo-
sion, In: Web Dictionary of Cybernetics and
Systems. Princia Cybernetica Web.
Junko Minato, David B. Bracewell, Fuji Ren and
Shingo Kuroiwa. 2006. Statistical Analysis of a
Japanese Emotion Corpus for Natural Language
Processing. LNCS 4114, pp. 924-929.
Alena Neviarouskaya, Helmut Prendinger and
Mitsuru Ishizuka. 2011. Affect analysis model:
novel rule-based approach to affect sensing
from text. Natural Language Engineering, Vol.
17, No. 1 (2011), pp. 95-135.
Hajime Ono. 2002. An emphatic particle DA and
exclamatory sentences in Japanese. University
of California, Irvine.
Christopher Potts and Florian Schwarz. 2008. Ex-
clamatives and heightened emotion: Extracting
pragmatic generalizations from large corpora.
Ms., UMass Amherst.
Michal Ptaszynski, Pawel Dybala, Rafal Rzepka
and Kenji Araki. 2009. Affecting Corpora: Ex-
periments with Automatic Affect Annotation
System - A Case Study of the 2channel Forum
-, In Proceedings of The Conference of the Pa-
cific Association for Computational Linguistics
(PACLING-09), pp. 223-228.
Michal Ptaszynski, Rafal Rzepka, Kenji Araki and
Yoshio Momouchi. 2011. Language combina-
torics: A sentence pattern extraction architec-
ture based on combinatorial explosion. Inter-
national Journal of Computational Linguistics
(IJCL), Vol. 2, Issue 1, pp. 24-36.
Kaori Sasai. 2006. The Structure of Modern
Japanese Exclamatory Sentences: On the Struc-
ture of the Nanto-Type Sentence. Studies in the
Japanese Language, Vol, 2, No. 1, pp. 16-31.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas
P. O?Hara. 1999. Development and use of a
gold-standard data set for subjectivity classi-
fications. In Proceedings of the Association
for Computational Linguistics (ACL-1999), pp.
246-253, 1999.
Theresa Wilson and Janyce Wiebe. 2005. Anno-
tating Attributions and Private States. Proceed-
ings of the ACL Workshop on Frontiers in Cor-
pus Annotation II, pp. 53-60.
Hong Yu and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: separat-
ing facts from opinions and identifying the po-
larity of opinion sentences. In Proceedings of
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pp. 129-
136, 2003.
64
Appendix: Comparison of experiment results in all experiment settings for all three ways of
dataset preprocessing.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(a) F-score comparison for tokenized
dataset.
(b) Precision comparison for tok-
enized dataset.
(c) Recall comparison for tokenized
dataset.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(d) F-score comparison for POS-
tagged dataset.
(e) Precision comparison for POS-
tagged dataset.
(f) Recall comparison for POS-tagged
dataset.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(g) F-score comparison for tokenized
dataset with POS tags.
(h) Precision comparison for tok-
enized dataset with POS tags.
(i) Recall comparison for tokenized
dataset with POS tags.
65
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 381?386,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Application of Prize based on Sentence Length in Chunk-based
Automatic Evaluation of Machine Translation
Hiroshi Echizen?ya
Hokkai-Gakuen University
S26-Jo, W11-Chome, Chuo-ku,
Sapporo 064-0926 Japan
echi@lst.hokkai-s-u.ac.jp
Kenji Araki
Hokkaido University
N 14-Jo, W 9-Chome, Kita-ku,
Sapporo 060-0814 Japan
araki@ist.hokudai.ac.jp
Eduard Hovy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
hovy@cmu.edu
Abstract
As described in this paper, we pro-
pose a new automatic evaluation met-
ric for machine translation. Our met-
ric is based on chunking between the
reference and candidate translation.
Moreover, we apply a prize based on
sentence-length to the metric, dissim-
ilar from penalties in BLEU or NIST.
We designate this metric as Automatic
Evaluation of Machine Translation in
which the Prize is Applied to a Chunk-
based metric (APAC). Through meta-
evaluation experiments and compari-
son with several metrics, we confirmed
that our metric shows stable correla-
tion with human judgment.
1 Introduction
In the field of machine translation, various
automatic evaluation metrics have been pro-
posed. Among them, chunk-based metrics
such as METEOR(A. Lavie and A. Agarwal,
2007), ROUGE-L(Lin and Och, 2004), and
IMPACT(H. Echizen-ya and K. Araki, 2007)
are effective. In general, BLEU(K. Papineni et
al., 2002), NIST(NIST, 2002), and RIBES(H.
Isozaki et al., 2010) use a penalty for calcula-
tion of scores because the high score is often
given extremely when the candidate transla-
tion is short. Therefore, the penalty is effective
to obtain high correlation with human judg-
ment. On the other hand, almost all chunk-
based metrics use the F -measure based on a
precision by candidate translation and a re-
call by reference. Moreover, they assign a
penalty for the difference of chunk order be-
tween the candidate translation and the refer-
ence, not the penalty for the difference of sen-
tence length. Nevertheless, it is also impor-
tant for chunk-based metrics to examine the
sentence length. In chunk-based metrics, each
word?s weight depends on the sentence length.
For example, the weight of each word is 0.2
(=1/5) when the number of words in a sen-
tence is 5; it is 0.1 (=1/10) when the number
of words in a sentence is 10. Therefore, the
weight of the non-matched word in the short
sentence is large.
To resolve this problem, it is effective for
short sentences to give a prize based on the
sentence length in the chunk-based metrics.
Therefore, we propose a new metric using a
prize based on the sentence length. We des-
ignate this metric as Automatic Evaluation
of Machine Translation in which the Prize is
Applied to a Chunk-based metric (APAC). In
our metric, the weight of a non-matched word
becomes small for the short sentence by award-
ing of the prize. It is almost identical to that
for a long sentence by awarding of the prize.
Therefore, our metric does not depend heavily
on sentence length because the weight of non-
matched words is constantly small. We con-
firmed the effectiveness of APAC using meta-
evaluation experiments.
2 Score calculation in APAC
The APAC score is calculated in two phases.
In the first phase, the chunk sequence is
determined between a candidate translation
and the reference. The chunk sequence
381
is determined using the Longest Common
Subsequence (LCS). Generally, several chunk
sequences are obtained using LCS. In that
case, APAC determines only one chunk se-
quence using the number of words in each
chunk and the position of each chunk.
For example, in between the candidate
translation ?In this case, the system power
supply is accessory battery 86.? and ?In this
case, the system power supply is the accessory
power supply battery 86.?, the chunk sequence
is ?in this case, the system power supply is?,
?accessory? and ?battery 86.?, and the chunk
sequence is ony one in these sentences. Only
one chunk sequence is determined using the
number of words in each chunk and the po-
sition of each chunk when several chunk se-
quences are obtained.
The second phase is calculation of the score
based on the determined chunk sequence. The
Ch score in Eq. (3) is calculated using the de-
termined chunk sequence. In Eq. (3), ch de-
notes each chunk and ch num represents the
number of chunks. Moreover, length(ch) is the
word number of each chunk. ? is the weight
parameter for the length of each chunk. For
example, in between the candidate translation
?In this case, the system power supply is ac-
cessory battery 86.? and ?In this case, the
system power supply is the accessory power
supply battery 86.?, ch num is 3 (?in this
case, the system power supply is?, ?accessory?
and ?battery 86.?). Therefore, Ch score is 91
(=9
2.0
+ 1
2.0
+ 3
2.0
) when ? is 2.0.
P =
{(
?
RN?1
i=0
(
?
i
? Ch score
)
m
?
)
1
?
+0.5? Prize m
}
/2.0 (1)
R =
{(
?
RN?1
i=0
(
?
i
? Ch score
)
n
?
)
1
?
+0.5? Prize n
}
/2.0 (2)
Ch score =
?
ch?ch num
length(ch)
?
(3)
Prize m =
1
log(m) + 1
(4)
Prize n =
1
log(n) + 1
(5)
APAC score =
(1 + ?
2
)RP
R + ?
2
P
(6)
The P and R in Eqs. (1) and (2) re-
spectively denote precision by candidate
translation and recall by reference. These
are calculated using the Ch score obtained
using Eq. (3). Therein, m and n respectively
represent the word numbers of the candidate
translation and the reference. Moreover,
the chunk sequence determination process is
repeated recursively to all common words.
The number of determination processes of
the chunk sequence is high when the word
order of the candidate translation differs
from that of the reference. The RN is the
number of determination processes of the
chunk sequence. Here, ? is the parameter for
the chunk order. It is less than 1.0. The value
of the Ch score is small when the chunk order
between the candidate translation and refer-
ences differs because the value of length(ch)
in each chunk becomes small. For example,
in between the candidate translation ?In this
case, the system power supply is accessory
battery 86.? and ?In this case, the system
power supply is the accessory power sup-
ply battery 86.?,
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
is 0.773 (=
?
91
169
=
?
?
1?1
i=0
(0.1
0
?91)
13
2.0
)
and
(
?
RN?1
i=0
(
?
i
?Ch score
)
n
?
)
1
?
is 0.596
(=
?
91
256
=
?
?
1?1
i=0
(0.1
0
?91)
16
2.0
) when ? and ?
respectively stand for 0.1 and 2.0. The
value of RN is 1 because there is no more
matching words after the determined chunks
(?in this case, the system power supply is?,
?accessory? and ?battery 86.?) are removed
from the candidate translation ?In this case,
the system power supply is accessory battery
86.? and ?In this case, the system power
supply is the accessory power supply battery
86.?.
Moreover, Prize m and Prize n in Eqs. (1)
and (2) are calculated respectively using Eqs.
382
(4) and (5). Each is less than 1.0. For ex-
ample, in the candidate translation ?In this
case, the system power supply is accessory
battery 86.? and ?In this case, the system
power supply is the accessory power supply
battery 86.?, Prize m and Prize n respec-
tively stand for 0.473 (=
1
1.114+1
=
1
log(13)+1
) and
0.454 (=
1
1.204+1
=
1
log(16)+1
). These values be-
come large in the short sentences. They be-
come small in the long sentences. Therefore,
the weight of each non-matched word is small
in the short sentences. It is kept small in
the long sentences. Finally, the score is cal-
culated using Eq. (6). This equation shows
the f -measure based on P and R. In Eq. (6),
? is determined as P/R(C. J. V. Rijsbergen,
1979). The APAC score is between 0.0 and
1.0. For example, in the candidate transla-
tion ?In this case, the system power supply is
accessory battery 86.? and ?In this case, the
system power supply is the accessory power
supply battery 86.?, P and R respectively
stand for 0.505 (=
0.773+0.5?0.473
2.0
) and 0.412
(=
0.596+0.5?0.454
2.0
). Therefore, APAC score is
0.445 (=
0.521
1.171
=
(1+1.503)?0.412?0.505
0.412+1.503?0.505
) and ? is
1.226 (=
0.505
0.412
)
3 Experiments
3.1 Experimental Procedure
Meta-evaluation experiments are performed
using WMT2012(C. Callison-Burch et al.,
2012) data and WMT2013(O. Bojar et al.,
2013) data, and NTCIR-7(A. Fujii et al., 2008)
data and NTCIR-9(A. Goto et al., 2011) data.
All sentences by NTCIR data are English
patent sentences obtained through Japanese-
to-English translation. The number of refer-
ences is 1. In NTICR-7 data, the average
value in the evaluation results of three hu-
man judgments is used as the scores of 1?
5 from the perspective of adequacy and flu-
ency. In NTCIR-9 data, the evaluation results
of one human judgment is used as the scores
of 1?5 from the view of adequacy and accep-
tance. For this meta-evaluation, we used only
English and Japanese candidate translations
because we can evaluate them in comparison
with other languages correctly.
We calculated the correlation between the
scores by automatic evaluation and the scores
by human judgments at the system level and
the segment level, respectively. Spearman?s
rank correlation coefficient is used at the sys-
tem level. The Kendall tau rank correlation
coefficient is used in the segment level.
Moreover, we used BLEU (ver. 13a),
NIST (ver. 13a), METEOR (ver. 1.4), and
APAC with no prize (APAC no p) as the
automatic evaluation metrics for comparison
with APAC as shown in Eqs. (4) and (5).
In APAC no p,
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
as P
and
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
as R are used re-
spectively in Eqs. (1) and (2).
3.2 Experimental Results
Tables 1 and 2 respectively present Spear-
man?s rank correlation coefficients of system-
level and Kendall tau rank correlation coef-
ficients of segment-level in WMT2012 data.
Tables 3 and 4 respectively show Spearman?s
rank correlation coefficients of the system-level
and Kendall tau rank correlation coefficients of
segment-level in WMT2013 data. Moreover,
Tables 5 and 6 respectively present Spear-
man?s rank correlation coefficients of system-
level and Kendall tau rank correlation coeffi-
cients of segment-level in NTCIR-7 data. Ta-
bles 7 and 8 respectively show Spearman?s
rank correlation coefficients of system-level
and Kendall tau rank correlation coefficients
of the segment level in NTCIR-9 data.
In APAC, 0.1 and 1.2 were used as the values
of parameters ? and ? by the preliminarily ex-
perimentally obtained results. In Tables 1?8,
?Rank? denotes the ranking based on ?Avg.?
The value of ?()? denotes the number of MT
systems in Tables 1, 3, 5, and 7. The value of
?()? represents the number of sentence pairs
in Tables 2, 4, 6, and 8. These values depend
on the data.
3.3 Discussion
The results presented in Tables 1?8 indicate
that APAC can obtain the most stable corre-
lation coefficients among some metrics. The
ranking of APAC is No. 1 through NTCIR
data in Tables 5?8. In WMT data of Ta-
bles 1?4, the ranking of APAC is the lowest
except for Table 3. However, the difference
383
cs-en(6) de-en(16) es-en(12) fr-en(15) Avg. Rank
APAC 0.886 0.650 0.958 0.811 0.826 5
APAC no p 0.886 0.676 0.958 0.807 0.832 3
METEOR 0.943 0.841 0.979 0.818 0.895 1
BLEU 0.886 0.674 0.958 0.796 0.828 4
NIST 0.943 0.700 0.944 0.779 0.841 2
Table 1: Spearman?s rank correlation coefficient of system-level in WMT2012 data.
cs-en(11,155) de-en(12,042) es-en(9,880) fr-en(11,682) Avg. Rank
APAC 0.185 0.204 0.209 0.226 0.206 3
APAC no p 0.189 0.207 0.208 0.226 0.207 2
METEOR 0.223 0.279 0.248 0.243 0.248 1
Table 2: Kendall tau rank correlation coefficient of the segment level in WMT2012 data.
between the ranking of METEOR, which is
the highest, and that of APAC is not larger
in WMT data. The correlation coefficients of
APAC in NTCIR data of Tables 5?8 are higher
than those of METEOR. In Tables 5 and 6,
underlining in APAC signifies that the differ-
ences between correlation coefficients obtained
using APAC and METEOR are statistically
significant at the 5% significance level. In Ta-
ble 7, the correlation coefficients of METEOR,
BLEU, and NIST are extremely low. Only one
human judgment was used in NTCIR-9 data.
As a result, APAC is fundamentally effective
for various languages independent of the differ-
ences in the grammatical structures between
languages: these experimentally obtained re-
sults indicate that APAC is the most stable
metric.
Moreover, in APAC, the correlation coeffi-
cients of the segment level in NTCIR data were
increased using the prize of Eqs. (4) and (5).
In WMT data, the correlation coefficients are
almost identical using the prize. Therefore,
use of the prize was fundamentally effective
at the segment level. The evaluation quality
of segment level is generally very low in the
automatic evaluation metrics. Therefore, it is
extremely important to improve the correla-
tion coefficient of segment level. Application
of the prize is effective to improve the evalua-
tion quality of the segment level.
4 Conclusion
As described in this paper, we proposed a new
chunk-based automatic evaluation metric us-
ing the prize based on the sentence length.
The experimentally obtained results indicate
that APAC is the most stable metric.
We will improve APAC to obtain higher
correlation coefficients in future studies.
Particularly, we will strive to improve
the correlation coefficients at the segment
level. The APAC software will be re-
leased by http://www.lst.hokkai-s-u.ac.
jp/~echi/automatic_evaluation_mt.html.
Acknowledgments
This work was done as research under the
AAMT/JAPIO Special Interest Group on
Patent Translation. The Japan Patent In-
formation Organization (JAPIO) and the Na-
tional Institute of Information (NII) provided
corpora used in this work. The author grate-
fully acknowledges support from JAPIO and
NII.
References
O. Bojar, C. Buck, C. Callison-Burch, C. Feder-
mann, B. Haddow, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2013. Findings of the
2013 Workshop on Statistical Machine Transla-
tion. Proceedings of the Eighth Workshop on
Statistical Machine Translation. pp.1?44.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Transla-
tion. Proceedings of the Seventh Workshop on
Statistical Machine Translation. pp.10?51.
H. Echizen-ya and K. Araki. 2007. Automatic
Evaluation of Machine Translation based on
384
cs-en(11) de-en(17) es-en(12) fr-en(13) ru-en(19) Avg. Rank
APAC 0.900 0.904 0.916 0.934 0.709 0.873 3
APAC no p 0.909 0.909 0.937 0.934 0.721 0.882 2
METEOR 0.982 0.946 0.923 0.967 0.889 0.941 1
BLEU 0.945 0.897 0.853 0.951 0.614 0.852 4
NIST 0.900 0.828 0.804 0.786 0.465 0.757 5
Table 3: Spearman?s rank correlation coefficient of the system level in WMT2013 data.
cs-en de-en es-en fr-en ru-en
Metrics
(85,469) (128,668) (67,832) (80,741) (151,422)
Avg. Rank
APAC 0.144 0.163 0.169 0.139 0.121 0.147 3
APAC no p 0.148 0.167 0.176 0.142 0.123 0.151 2
METEOR 0.222 0.236 0.241 0.194 0.226 0.224 1
Table 4: Kendall tau rank correlation coefficient of the segment level in WMT2013 data.
Recursive Acquisition of an Intuitive Common
Parts Continuum. Proceedings of the Eleventh
Machine Translation Summit. pp.151?158.
A. Fujii, M. Utiyama, M. Yamamoto and T. Ut-
suro. 2008. Overview of the Patent Translation
Task at the NTCIR-7 Workshop. Proceedings
of the Seventh NTCIR Workshop Meeting on
Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and
Cross-lingual Information Access. pp.389?400.
I. Goto, B. Lu, K. P. Chow, E. Sumita and B. K.
Tsou. 2011. Overview of the Patent Translation
Task at the NTCIR-9 Workshop. Proceedings of
the Ninth NTCIR Workshop Meeting. pp.559?
578.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh and
H. Tsukada. 2010. Automatic Evaluation of
Translation Quality for Distant Language Pairs.
Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing.
pp.944?952.
A. Lavie and A. Agarwal. 2007. Meteor: An Auto-
matic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. Pro-
ceedings of the Second Workshop on Statistical
Machine Translation.
Chin-Yew Lin and F. J. Och. 2004. Automatic
Evaluation of Machine Translation Quality Us-
ing the Longest Common Subsequence and Skip-
Bigram Statistics. In Proc. of ACL?04, 606?613.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurrence Statistics.
http://www.nist.gov/speech/tests/mt/doc/
ngram-study.pdf.
K. Papineni, S. Roukos, T. Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL). pp.311?
318.
C. J. Van Rijsbergen. 1979. Information Retrieval
(2nd ed.), Butterworths.
385
Adequacy(15) Fluency(15) Avg. Rank
APAC 0.872 0.805 0.839 1
APAC no p 0.872 0.805 0.839 1
METEOR 0.424 0.380 0.402 5
BLEU 0.582 0.586 0.584 3
NIST 0.578 0.568 0.573 4
Table 5: Spearman?s rank correlation coefficient of the system level in NTCIR-7 data.
Adequacy (1,500) Fluency (1,500) Avg. Rank
APAC 0.494 0.489 0.491 1
APAC no p 0.482 0.476 0.479 2
METEOR 0.366 0.383 0.375 3
Table 6: Kendall tau rank correlation coefficient of the segment level in NTCIR-7 data.
Adequacy (19) Acceptance (14) Avg. Rank
APAC 0.182 0.298 0.240 1
APAC no p 0.182 0.298 0.240 1
METEOR -0.081 0.015 -0.033 4
BLEU -0.123 0.059 -0.032 3
NIST -0.344 -0.275 -0.309 5
Table 7: Spearman?s rank correlation coefficient of the system level in NTCIR-9 data.
Adequacy (5,700) Acceptance (5,700) Avg. Rank
APAC 0.250 0.261 0.256 1
APAC no p 0.242 0.250 0.246 2
METEOR 0.167 0.217 0.192 3
Table 8: Kendall tau rank correlation coefficient of segment-level in NTCIR-9 data.
386
