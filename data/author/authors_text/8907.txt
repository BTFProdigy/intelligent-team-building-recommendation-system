Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 73?80
Manchester, August 2008
A Classification of Dialogue Actions in Tutorial Dialogue
Mark Buckley and Magdalena Wolska
Dept. of Computational Linguistics
Saarland University
66041 Saarbr?ucken, Germany
{buckley|magda}@coli.uni-sb.de
Abstract
In this paper we present a taxonomy of di-
alogue moves which describe the actions
that students and tutors perform in tutorial
dialogue. We are motivated by the need for
a categorisation of such actions in order to
develop computational models for tutorial
dialogue. As such, we build both on exist-
ing work on dialogue move categorisation
for tutorial dialogue as well as dialogue
taxonomies for general dialogue. Our tax-
onomy has been prepared by analysing a
corpus of tutorial dialogues on mathemati-
cal theorem proving. We also detail an an-
notation experiment in which we apply the
taxonomy and discuss idiosyncrasies in the
data which influence the decisions in the
dialogue move classification.
1 Introduction
The field of Intelligent Tutoring Systems has seen
recent developments moving towards adding natu-
ral language capabilities to computer-based tutor-
ing (Graesser et al, 1999; Zinn, 2004; Litman and
Silliman, 2004), motivated by empirical investiga-
tions which point to the effectiveness of human tu-
tors (Bloom, 1984; Moore, 1993; Graesser et al,
1995). However, to be able to interact with a stu-
dent through the medium of natural language dia-
logue, the system must have a model of how such
tutorial dialogues can progress and what utterances
are licenced. In order to develop such a model
of dialogue, we need to understand and describe
the ?actions? performed with words, i.e. speech
acts (Austin, 1955) or dialogue moves. This in-
volves identifying and categorising the functions
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
that utterances may have in dialogue and their re-
lationships to each other.
Researchers in conversation and dialogue the-
ory have proposed various general categorisa-
tions of dialogue moves. DIT++ (Bunt, 2000)
is an example of a comprehensive multidimen-
sional taxonomy of dialogue acts for informa-
tion dialogues based on DAMSL (Allen and Core,
1997), a general-purpose extensible taxonomy pro-
posed as a standard for dialogue annotation. The
DAMSL dialogue act taxonomy characterises utter-
ances along four dimensions which correspond to
four levels of functions utterances may have. The
forward looking function describes the utterance?s
effect on the following interaction, the backward
looking function, its relation to previous dialogue,
the communicative status describes the compre-
hensibility or interpretability of the utterance, and
the information level characterises the content of
the utterance.
Tsovaltzi and Karagjosova (2004) proposed an
extension of the DAMSL classification based on an
analysis of tutorial dialogue corpora. The pro-
posed taxonomy adds a Task dimension which
concentrates on tutor actions in the dialogue.
1
Building on this work, we propose a further ex-
tension to this taxonomy inspired by our analysis
from the point of view of task-related goals. The
classification we present (i) includes modifications
of the DAMSL categorisation motivated by tutorial
dialogue, (ii) accounts for student?s actions, and
(iii) introduces a Task progress dimension whose
purpose is to characterise the completion status of
a generally viewed ?task?, instantiated for the pur-
pose of tutorial dialogue. We validated our dia-
logue act categorisation in a small-scale annotation
experiment whose results we present.
This paper is organised as follows: In Section 2
we introduce our data and taxonomy development
1
This classification has not been, to our knowledge, quan-
titatively evaluated.
73
methodology. In Section 3 we present the taxon-
omy. The results of the annotation experiment and
their discussion are presented in Section 4. Sec-
tion 5 presents related work and Section 6 con-
cludes the paper.
2 The Data and Analysis Methodology
Our work is based on an analysis of two corpora of
tutorial dialogues on mathematical theorem prov-
ing. We use the data to (i) verify the general
dialogue dimensions of the DAMSL taxonomy in
the context of tutorial dialogues, and (ii) extend
the taxonomy by developing the task dimension.
While the specific task-level moves are instanti-
ated for the mathematics tutoring domain, our aim
is to maintain generality that would allow us to
model task-level moves in other tutoring domains
as well as task-oriented dialogues in general. Be-
low we briefly introduce the corpora and outline
our methodology.
Corpora We analysed two corpora of tutorial
dialogues in the domain of mathematical theo-
rem proving collected in Wizard-of-Oz experi-
ments in which a human tutor (wizard) simulated
the system?s behaviour (Benzm?uller et al, 2003;
Benzm?uller et al, 2006a). The domains of mathe-
matics in the first (Corpus-I) and second (Corpus-
II) corpora are naive set theory and binary relations
respectively (Wolska et al, 2004; Benzm?uller et
al., 2006b).
In both experiments the dialogues were con-
ducted in German using the keyboard and a graph-
ical user interface. The tutoring in the experiment
described in (Benzm?uller et al, 2003) was per-
formed under three experimental conditions. The
control group (8 subjects) were tutored according
to the minimal feedback strategy in which the tu-
tor?s reactions were limited to informing the stu-
dent as to the correctness and completeness of their
contributions. In the didactic group (7 subjects)
and socratic group (7 subjects) the tutor?s strategy
focused on disclosing partial solutions to the stu-
dent in case of lack of progress and leading toward
the solution (hinting) respectively. Given the tutor-
ing strategy the verbosity of the minimal feedback
tutors was limited, while in both other conditions
as well as in the other experiment, the subjects and
the tutors were unconstrained in terms of the lin-
guistic realisation of their turns. Table 1 shows the
overview of the size of the corpora in terms of the
number of turns.
Corpus-I Corpus-II
no. subjects 22 37
no. turns 775 1917
no. student turns 332 937
no. tutor turns 443 980
Table 1: Overview of the corpora
Methodology In developing the taxonomy we
pursued the following iterative methodology:
First, in order to build the initial taxonomy, we
analysed 18 dialogues from Corpus-I containing
299 utterances (the development set). The purpose
of this analysis was to (i) verify the general suit-
ability of the DAMSL scheme in the tutoring do-
main,
2
(ii) identify features of dialogues moves rel-
evant in tutoring that were not present in the origi-
nal taxonomy (see the discussion in Section 4), (iii)
identify an initial set of task-level moves. We de-
scriptively defined the move types and wrote draft
annotation guidelines.
Second, we applied the initial taxonomy to 4 di-
alogues (108 utterances) taken from the two cor-
pora in an annotation task performed indepen-
dently by the authors of this paper (a preliminary
test set), after which we extended the taxonomy
and refined the existing category definitions. Fi-
nally, we randomly
3
selected an 64-utterance sub-
set taken from both corpora (validation set) to test
the coverage of the final taxonomy.
3 A Dialogue Move Taxonomy
Our goal in the analysis of the development set
and preliminary test set of the corpus was to deter-
mine a categorisation of the actions that can be per-
formed by students and tutors in tutorial dialogues.
The taxonomy which we have created from this
categorisation contains the dialogue moves which
realise these actions. Now the utterances per-
formed by students and tutors realise actions which
may or may not address or have an effect on the
current task. We thus speak of the task-level func-
tion of an utterance in addition to a general di-
alogue level function which all utterances have.
Task-level vs. general dialogue level function is
therefore the basic split in our taxonomy.
2
We expected this to be suitable because DAMSL is a tax-
onomy with general applicability.
3
We avoided dialogues in which the student?s utterances
contained formulas only.
74
The full definition of the taxonomy is presented
in Table 2. For each dialogue move we give a short
explanation of what function it is intended to cap-
ture as well as a short example. In the following
sections we discuss some of our design decisions.
3.1 The Forward and Backward Dimensions
At the general dialogue level we follow the DAMSL
taxonomy and categorise the functions of utter-
ances according to their relationship with the pre-
vious dialogue and their effect on the dialogue to
follow. For these functions we use a Forward
dimension and a Backward dimension, respec-
tively. In general, we try to accommodate the
DAMSL categories in order to build as much as
possible on existing generally accepted work on
dialogue moves. The forward dimension captures
utterances which are either assertions, requests or
commands. The backward dimension captures ut-
terances which agree or disagree with previous
utterances, address questions, signal the under-
standing status of previous utterances, or stand
in some information relation to previous utter-
ances. The main differences between DAMSL and
our categorisation within these two dimensions are
the following: (i) we combine DAMSL?s Assert
and Re-assert in a single category Assert which
may be optionally marked as repeating informa-
tion, (ii) we combine DAMSL?s Action-directive
and Info-request in a higher-level category of Re-
quests, (iii) in place of DAMSL?s Answer, we in-
troduce a more general Address category in the
backward dimension with subcategories Answer,
Deflect, and Neutral, where Deflect accounts for
avoiding answering and Neutral refers to those ut-
terances which simply address a previous informa-
tion request without answering it or a previous ac-
tion directive without acceding to it. The remain-
ing DAMSL categories were left unchanged; they
are also not presented in Table 2.
3.2 The Task and Task Progress Dimensions
At the task level we have utterances which address
the task at hand. These can mean altering the state
of the task solution, for instance by performing a
step in the solution, or talking about the task solu-
tion without altering it, for instance making state-
ments about previously performed steps. We di-
vide the task related actions in those which address
the task directly and those which address the solu-
tion construction process, and capture these in the
task and task progress dimensions respectively.
The Task dimension contains most of the task
related dialogue moves. We follow Tsovaltzi and
Karagjosova (2004) by splitting the task dimen-
sion into two subdivisions which relate to the par-
allel tasks being carried out and the roles of the
dialogue participants. Since the roles of student
and tutor restrict what actions can be performed
by the speakers, we split the task dimension into
actions which contribute to the solving task and
those which contribute to the teaching task. Ac-
tions in the solving task are typically performed
by the student, actions in the teaching task are typ-
ically performed by the tutor and are pedagogically
motivated. This is important for example to differ-
entiate between requests for task level information
? requests coming from the student are of an in-
formation seeking nature, those that come from the
tutor contribute to diagnostic or pedagogical goals.
Within the solving task, changes or additions to
the solution are captured by Solution-step, which
may be a new step or an extension of an exist-
ing step, and Solution-strategy. Solution-strategy
is divided into stating a strategy which will be fol-
lowed and stating a solution step which will be per-
formed in the future. The difference between these
is that the statement of a future step refers to a sin-
gle step in the solution which the student is com-
mitting to perform, whereas a strategy is a more
abstract concept. A strategy is more like a solution
approach which may consist of a number of steps,
however which actual steps are to be performed is
left open. In the domain of mathematical theorem
proving a strategy may refer to a particular proving
technique, for instance a proof by induction or by
contradiction, which may be realised by an utter-
ance such as ?I will now do a proof by induction?.
Exactly what constitutes a step and a strategy is a
matter of how the domain is modelled.
Request-assistance covers actions which ask for
help with the task. Within the teaching task,
Solution-step-evaluation refers to utterances that
convey the correctness or otherwise of steps. Do-
main relevant requests ask for domain related in-
formation such as definitions. Hint covers direct
hints about how to continue the task, for instance
by giving away a concept which should be used by
the student. The hint category will need to cover
many different kinds of actions depending on the
tutorial strategy which tutors follow ? we have
not subdivided this category and refer to Tsovaltzi
et al (2004) where this is further elaborated.
75
The Task Progress dimension equates in part to
the task management dimension in DAMSL. Here
the dialogue moves related to the current task are
those which start, finish, restart or abandon it. The
student can indicate the status of the solution con-
struction process to be on-track or finished or sig-
nal that he is lost.
In summary, we have prepared our taxonomy of
dialogue moves for tutoring by adding a Task and
Task Progress dimensions to the original DAMSL
taxonomy. We have tried to keep as close to the
DAMSL specification as possible with regard to the
general dialogue level function of dialogue moves,
while at the same time adapting it to capture the
phenomena of tutorial dialogue. Although some
moves will typically by performed by either the
student or the tutor (for example, only the tutor
will realistically give hints) we do not introduce
any constraints which restrict this.
4 Validating the Taxonomy
We used the taxonomy to perform a small-scale
annotation experiment on a validation set taken
from the two corpora introduced in Section 2. The
data had previously been segmented into utter-
ances. The goal of this experiment was to see
whether our categorisation can be reliably applied
to data and to validate the coverage of the taxon-
omy. The annotation was carried out by two an-
notators (the authors of this paper), following the
definitions of the dialogue moves informally pre-
sented above. We did not consider the category
information-relation because no definition is given
by the original DAMSL taxonomy, however we will
return to the question of information relation later
in the discussion.
Results Inter-annotator agreement was calcu-
lated using Cohen?s kappa (Cohen, 1960) and the
results of the experiment are given in the following
table.
Dimension ? value
Forward 0.87
Backward 0.47
Task 0.75
Task Progress 0.91
These results can be considered very good for the
Forward and Task Progress dimensions, good
for the Task dimension, and low for the Back-
ward dimension. Among the categories with the
lowest agreement were Neutral at 0.11 and Step-
augmentation at 0.37. In this preliminary evalua-
tion our strategy was not to use an category ?other?
for utterances which did not appear to belong to
any existing category, but rather to try to fit the an-
notation to the categories as they are. We marked
possibly problematic utterances for further discus-
sion.
Example We give examples of two fully anno-
tated dialogue excerpts in Figure 1. The exam-
ples illustrate some of the types of problematic ut-
terances which the corpora contain. For instance
both utterances ?Really?? and ?Yes?? are ques-
tions and could appear to be information requests,
but in fact act more like prompts to continue, for
which we had no category. Similarly the functions
of the questions in sequence in the second exam-
ple are difficult to abstract. We have tagged these
as Neutral, since they discharge the obligations
introduced by the questions before them, but the
link between consecutive interrogative utterances
is elusive.
Discussion We will now briefly discuss the re-
sults and findings of our annotation experiment
and allude to some of the possible causes of the
difficulties we encountered.
The nature of tutorial dialogue is an underlying
factor which makes it difficult to annotate cate-
gories reliably. Students tend to be very concise,
which makes it difficult to determine how the stu-
dent intended to relate the latest input to the pre-
vious discourse. This is reflected in our agreement
score for the backward dimension, which at 0.47
is much lower than the other dimensions, as well
as in the agreement score of 0.37 for the Solution-
step augmentation category, which is heavily de-
pendent on previous context. This result may even
point to a general characteristic of tutorial dialogue
which makes computational modelling challeng-
ing. In particular the Neutral category resulted in
conflicting annotations because it is often unclear,
as in the examples shown above, whether Requests
are being answered or merely addressed.
We have found that tutors typically perform ut-
terances which contribute to many different goals
? for instance they can simultaneously reject pro-
posed solution steps while giving hints on how to
continue in the task. The purpose of multidimen-
sional dialogue move taxonomies is to handle this
very multifunctionality, and while this is success-
76
Utterance Forward Backward Task
S: It holds that P (C ? (A ?B)) ? P (C) ? . . . assert solution-step:new
T: Really? info-request reject signal-incorrect
S: no it?s not, answer
S: the other way around assert solution-step:new
T: that?s right at last assert accept signal-correct
S: R ? S := {(x,y)| ? z(z ? M ? (x,z) ? R ? . . . assert solution-step:new
T: That?s right! assert accept signal-correct
S: now i want the inverse of that assert state-future-step
T: yes? neutral hint
S: (R ? S)
?1
assert neutral solution-step:new
T: = ? info-request request-clar request-explanation
S: How will the system answer? info-request neutral
T: What?s the question? info-request neutral
S: Can the system conclude (R ? S)
?1
from R ? S info-request neutral
T: yes assert answer
T: But try it yourself! action-dir hint
Figure 1: Annotated example from the corpus
ful to a point, conflicts in the annotation experi-
ment have highlighted some dual functions within
the same category. For instance, utterances simul-
taneously rejecting steps and requesting explana-
tions of the errors in the steps were found a number
of times.
We have found at least three categories that may
need to be added to the current taxonomy to make
it cover tutorial dialogue more completely. As dis-
cussed above, a prompt type in the forward dimen-
sion seems necessary. In addition, we would fore-
see a backward category which corrects a previ-
ous utterance, a category in the solving task which
requests the next step in the solution, and a cate-
gory in the task progress dimension to check if the
current task is being restarted. Similar categories
are proposed by Tsovaltzi and Karagjosova (2004),
and may be taken up.
We can draw attention to the fact that there are
many interrelations between the dimensions which
are not captured by our presentation of the tax-
onomy, and which may for instance be accounted
for by introducing constraints on label combina-
tions. We observe that many utterances stand in
some information relation (a DAMSL category) to
the previous discourse, although we have not fur-
ther specified what this relation might be. Such
utterances are typically step augmentations, and
could be described for instance (in RST terms) as
elaborations.
Finally we have adopted Tsovaltzi and
Karagjosova?s top-level structure, with Task as a
dimension. However, we observe that it would be
equally valid and more in keeping with the original
DAMSL categorisation of utterance functions to
make use of the existing Task sub-category of
the Info-level dimension. Similarly, our Task
progress corresponds to Info-level?s sub-category
Task management. This is a straightforward struc-
tural change which will not affect the annotation
results within these categories.
5 Related Work
The original DAMSL taxonomy was applied to and
evaluated on the TRAINS corpus of problem solv-
ing dialogues (Core and Allen, 1997). In this an-
notation a single label Task was used to mark all
task-related utterances. In the Verbmobil project,
a set of dialogue moves specific to negotiation
was proposed (Alexandersson et al, 1997). These
moves capture only the task-specific functions of
the utterances. Similarly, the HCRC Map Task
coding scheme concentrates on the task functions
of utterances, here specific to instruction-giving
dialogues. This classification is based on con-
versational games approach to dialogue seman-
tics (Houghton, 1986).
The DIT++ taxonomy of dialogue acts (Bunt,
2006) provides a more fine grained categorisation
of general dialogue actions, however there is no
one category or dimension dedicated to task spe-
cific functions. The category closest to (a subset
of) our Task dimension would be Activity-Specific
Functions, which however is defined in terms of
performative verbs or graphical actions. In the tu-
toring domain not all task-related actions are re-
alised by performatives.
There are a number of categorisations of dia-
logue actions specific to tutoring and motivated
by the development of tutorial dialogue systems.
Closely related to our work is a recent study by
77
Porayska-Pomsta et al (2008), who categorise task
related student actions and tutor feedback in a in-
vestigation of student affect. (Dzikovska et al,
2006) propose a flat coding scheme for tutorial di-
alogues on mathematics and relate it to a model of
collaborative problem solving dialogue. A simpler
taxonomy is presented by Marineau et al (2000)
which differs from our approach in that it was de-
veloped with the goal of automatic classification in
an intelligent tutoring system. In a pedagogically
motivated analysis of a corpus of tutorial dialogues
on computer literacy, Graesser et al (1999) cate-
gorise tutors? actions in order to propose a model
of tutorial dialogue structure.
6 Conclusions and Future Work
In this paper we have presented a taxonomy of dia-
logue moves which captures the actions performed
in by students and tutors in a corpus of tutorial dia-
logues. We then detailed an annotation experiment
which applied the taxonomy to a validation data
set and achieved good inter-annotator agreement.
This preliminary study showed that we are able to
cover the data well. We did however find a num-
ber of problematic phenomena in the data, such as
that of relating task level actions to the previous
discourse, which are of particular importance for
classifying tutorial dialogue actions.
In our future work we plan a larger scale anno-
tation of a further test set of our corpus, which we
believe will confirm the tendencies found so far.
We also intend to apply our taxonomy in an an-
notation of tutorial dialogue dealing with differ-
ent task domains, for example, tutorial dialogue
corpora in domains other than mathematics and
general problem-solving dialogues (e.g. TRAINS).
One of the goals of our work is to inform the de-
velopment of models for tutorial dialogue, and so
with a view towards operationalisation of the di-
alogue moves in our taxonomy, we will work on
an axiomatic formalisation of the dialogue moves.
This can form important input into developing a
plan-based model for tutorial dialogue.
References
Alexandersson, Jan, Bianka Buschbeck-Wolf, Tsutomu
Fujinami, Elisabeth Maier, Norbert Reithinger, Birte
Schmitz, and Melanie Siegel. 1997. Dialogue acts
in verbmobil-2. Technical report, DFKI.
Allen, James and Mark Core. 1997. Draft of
DAMSL: Dialogue act markup in several layers.
DRI: Discourse Research Initiative, University of
Pennsylvania. http://www.cs.rochester.
edu/research/cisd/resources/damsl/
RevisedManual/.
Austin, John L. 1955. How to do things with Words.
2005, second edition. William James Lectures.
Benzm?uller, Christoph, Armin Fiedler, Malte Gabsdil,
Helmut Horacek, Ivana Kruijff-Korbayov?a, Manfred
Pinkal, J?org Siekmann, Dimitra Tsovaltzi, Bao Quoc
Vo, and Magdalena Wolska. 2003. A Wizard-of-
Oz experiment for tutorial dialogues in mathemat-
ics. In Aleven, Vincent, Ulrich Hoppe, Judy Kay,
Riichiro Mizoguchi, Helen Pain, Felisa Verdejo,
and Kalina Yacef, editors, AIED2003 Supplementary
Proceedings, volume VIII: Advanced Technologies
for Mathematics Education, pages 471?481, Sydney,
Australia. School of Information Technologies, Uni-
versity of Sydney.
Benzm?uller, Christoph, Helmut Horacek, Ivana Kruijff-
Korbayov?a, Henri Lesourd, Marvin Schiller, and
Magdalena Wolska. 2006a. DiaWozII ? A Tool
for Wizard-of-Oz Experiments in Mathematics. In
Proceedings of the 29th Annual German Conference
on Artificial Intelligence (KI-06), Lecture Notes in
Computer Science, number 4314, pages 159?173,
Bremen, Germany. Springer-Verlag.
Benzm?uller, Christoph, Helmut Horacek, Henri
Lesourd, Ivana Kruijff-Korbayov?a, Marvin Schiller,
and Magdalena Wolska. 2006b. A corpus of tuto-
rial dialogs on theorem proving; the influence of the
presentation of the study-material. In Proceedings
of the 5th International Conference on Language
Resources and Evaluation (LREC-06), pages 1766?
1769, Genoa, Italy. ELDA.
Bloom, B. 1984. The 2 Sigma Problem: The Search for
Methods of Group Instruction as Effective as One-to-
One Tutoring. Educational Researcher, 13(6):4?16.
Bunt, Harry. 2000. Dialogue pragmatics and context
specification. In Bunt, Harry and William Black,
editors, Abduction, Belief and Context in Dialogue.
Studies in Computational Pragmatics, volume 1,
pages 81?150. Benjamins.
Bunt, Harry. 2006. Dimensions in Dialogue Act An-
notation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC-06), pages 919?924, Genova, Italy.
Cohen, Jacob. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Core, Mark G. and James F. Allen. 1997. Coding
dialogues with the DAMSL annotation scheme. In
Traum, David, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35. AAAI, Menlo Park,
CA, USA.
78
Dzikovska, Myroslava O., Charles B. Callaway,
Matthew Stone, and Johanna D. Moore. 2006. Un-
derstanding student input for tutorial dialogue in pro-
cedural domains. In Schlangen, David and Raquel
Fernandez, editors, Proceedings of Brandial, the
10th Workshop on the Semantics and Pragmatics of
Dialogue, pages 10?17.
Graesser, A. C., N. K. Person, and J. P. Magliano.
1995. Collaborative dialogue patterns in naturalistic
one-on-one tutoring. Applied Cognitive Psychology,
9:495?522.
Graesser, Arthur C., Katja Wiemer-Hastings, Peter
Wiemer-Hastings, and Roger Kreuz. 1999. Auto-
tutor: A simulation of a human tutor. Cognitive Sys-
tems Research, 1:35?51.
Houghton, G. 1986. The Production of Language in
Dialogue: A Computational Model. Ph.D. thesis,
University of Sussex.
Litman, Diane J. and Scott Silliman. 2004. ITSPOKE:
An Intelligent Tutoring Spoken Dialogue System.
In Proceedings of the Human Language Technol-
ogy Conference: 4th Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (HLT/NAACL) (Companion Proceedings),
Boston, MA.
Marineau, Johanna, Peter Wiemer-Hastings, Derek
Harter, Brent Olde, Patrick Chipman, Ashish Kar-
navat, Victoria Pomeroy, Sonya Rajan, and Art
Graesser. 2000. Classification of speech acts in tu-
torial dialogue. In Proceedings of the Workshop on
Modeling Human Teaching Tactics and Strategies,
ITS 2000, pages 65?71.
Moore, Johanna. 1993. What makes human explana-
tions effective? In Proceedings of the 15
th
Meet-
ing of the Cognitive Science Society, pages 131?136,
Hillsdale, NJ.
Porayska-Pomsta, Ka?ska, Manolis Mavrikis, and He-
len Pain. 2008. Diagnosing and acting on student
affect: the tutor?s perspective. User Modeling and
User-Adapted Interaction, 18(1-2):125?173.
Tsovaltzi, Dimitra and Elena Karagjosova. 2004. A
View on Dialogue Move Taxonomies for Tutorial
Dialogues. In Strube, Michael and Candy Sidner,
editors, Proceedings of 5th SIGdial Workshop on
Discourse and Dialogue, pages 35?38, Cambridge,
Massachusetts, USA. Association for Computational
Linguistics.
Tsovaltzi, Dimitra, Armin Fiedler, and Helmut Ho-
racek. 2004. A multi-dimensional taxonomy for au-
tomating hinting. In Lester, James C., Rosa Maria
Vicari, and F?abio Paraguac?u, editors, Intelligent
Tutoring Systems ? 7th International Conference
(ITS-04), number 3220 in LNCS, pages 772?781.
Springer.
Wolska, M., B. Q. Vo, D. Tsovaltzi, I. Kruijff-
Korbayova, E. Karagjosova, H. Horacek, M. Gabs-
dil, A. Fiedler, and C. Benzm?uller. 2004. An an-
notated corpus of tutorial dialogs on mathematical
theorem proving. In Proceedings of the Fourth In-
ternational Conference on Language Resources and
Evaluation (LREC-04), pages 1007?1010, Lisbon.
Zinn, Claus. 2004. Flexible dialogue management
in natural-language enhanced tutoring. In Konvens
2004 Workshop on Advanced Topics in Modeling
Natural Language Dialog, pages 28?35, Vienna,
Austria.
79
Label Explanation Example
Forward Dimension
Assert Makes a claim about the world ?It holds that P ?
Request Introduces an obligation on the hearer to answer
Action-directive The obligation is that an action is performed ?Please show the following ?
Info-request Request for a piece of information ?What is the definition of...??
Open-option Suggestion of future action without obligation ?You could do a proof by induction?
Backward Dimension
Agreement Acceptance or rejection of plans or propositions
Accept Accepts a proposal ?Ok?
Reject Rejects a proposal ?That?s incorrect?
Address Responses to requests
Answer Answers a previously posed info-request ?Yes?/?No?
Deflect Shows inability or unwillingness to answer ?I can?t answer that?
Neutral Addresses without answering or deflecting ?Why do you ask??
Information relation Relation to an antecedent utterance
Understanding related Refers to problems understanding the speaker
Request clarification Asks to clarify a previous utterance ?What do you mean by X??
Request rephrase Asks for a repeat/rephrase of an utterance ?Could you repeat that??
Signal non-understanding Catch-all for signalling understanding problems
Task Dimension: Solving Task
Solution-step Refers to a step to the current solution
Step augmentation Adds to an existing step ?Concluded using Rule X?
New Contributes a new step ?Rewrite formula A to B?
Solution-strategy Refers to a solution strategy
State strategy States a solution strategy which will be used ?I now do a case split?
state future step State a step that will be executed later ?I will use DeMorgan2?
Request assistance Ask for help with the task
Request concept explanation Ask to explain a domain concept ?What does P mean??
Request worked example Ask for an example to be presented ?Could you give me an example??
Request solution strategy Ask what strategy to proceed with ?How should i do this task??
Task Dimension: Teaching Task
Solution-step-evaluation References to evaluations of soln steps
Signal correct Indicates the step was correct ?Correct!?
Signal incorrect Indicates the step was incorrect ?Wrong!?
Hint Give a hint towards solving the task
Give-away-concept Give away a concept to help with the task ?You should use rule X?
Request domain relevant Requests which refer to domain concepts
Explain Ask for an explanation to be given ?T: What is the defn of powerset??
Identify Ask for a concept to be identified ?T: What does ? denote??
Define Ask for a definition ?What is the definition of...??
Task Progress
Start task Starts the solution construction process ?Please prove P = Q?
Finish task Indicates end of the solution construction process ?I?m done?, ?Q.E.D?
Restart task Indicates solution being started again ?Start again?
Give-up task Abandons the current solution attempt ?I give up?
Task solution status References to solution progress
On-track Solution construction is on track ?Am i ok??, ?You?re doing fine?
Lost Indicates speaker is lost in current solution ?I?m lost?
Table 2: The full taxonomy. Each type is given along with an explanation and an example
80
35
36
37
38
39
40
41
42
Analysis of Mixed Natural and Symbolic Language Input
in Mathematical Dialogs
Magdalena Wolska Ivana Kruijff-Korbayova?
Fachrichtung Computerlinguistik
Universita?t des Saarlandes, Postfach 15 11 50
66041 Saarbru?cken, Germany
 
magda,korbay  @coli.uni-sb.de
Abstract
Discourse in formal domains, such as mathemat-
ics, is characterized by a mixture of telegraphic nat-
ural language and embedded (semi-)formal sym-
bolic mathematical expressions. We present lan-
guage phenomena observed in a corpus of dialogs
with a simulated tutorial system for proving theo-
rems as evidence for the need for deep syntactic and
semantic analysis. We propose an approach to input
understanding in this setting. Our goal is a uniform
analysis of inputs of different degree of verbaliza-
tion: ranging from symbolic alone to fully worded
mathematical expressions.
1 Introduction
Our goal is to develop a language understanding
module for a flexible dialog system tutoring math-
ematical problem solving, in particular, theorem
proving (Benzm u?ller et al, 2003a).1 As empirical
findings in the area of intelligent tutoring show, flex-
ible natural language dialog supports active learn-
ing (Moore, 1993). However, little is known about
the use of natural language in dialog setting in for-
mal domains, such as mathematics, due to the lack
of empirical data. To fill this gap, we collected a
corpus of dialogs with a simulated tutorial dialog
system for teaching proofs in naive set theory.
An investigation of the corpus reveals various
phenomena that present challenges for such input
understanding techniques as shallow syntactic anal-
ysis combined with keyword spotting, or statistical
methods, e.g., Latent Semantic Analysis, which are
commonly employed in (tutorial) dialog systems.
The prominent characteristics of the language in our
corpus include: (i) tight interleaving of natural and
symbolic language, (ii) varying degree of natural
language verbalization of the formal mathematical
1This work is carried out within the DIALOG project: a col-
laboration between the Computer Science and Computational
Linguistics departments of the Saarland University, within
the Collaborative Research Center on Resource-Adaptive
Cognitive Processes, SFB 378 (www.coli.uni-sb.de/
sfb378).
content, and (iii) informal and/or imprecise refer-
ence to mathematical concepts and relations.
These phenomena motivate the need for deep
syntactic and semantic analysis in order to ensure
correct mapping of the surface input to the under-
lying proof representation. An additional method-
ological desideratum is to provide a uniform treat-
ment of the different degrees of verbalization of the
mathematical content. By designing one grammar
which allows a uniform treatment of the linguistic
content on a par with the mathematical content, one
can aim at achieving a consistent analysis void of
example-based heuristics. We present such an ap-
proach to analysis here.
The paper is organized as follows: In Section 2,
we summarize relevant existing approaches to in-
put analysis in (tutorial) dialog systems on the one
hand and analysis of mathematical discourse on the
other. Their shortcomings with respect to our set-
ting become clear in Section 3 where we show ex-
amples of language phenomena from our dialogs.
In Section 4, we propose an analysis methodology
that allows us to capture any mixture of natural and
mathematical language in a uniform way. We show
example analyses in Section 5. In Section 6, we
conclude and point out future work issues.
2 Related work
Language understanding in dialog systems, be it
with text or speech interface, is commonly per-
formed using shallow syntactic analysis combined
with keyword spotting. Tutorial systems also suc-
cessfully employ statistical methods which com-
pare student responses to a model built from pre-
constructed gold-standard answers (Graesser et al,
2000). This is impossible for our dialogs, due to
the presence of symbolic mathematical expressions.
Moreover, the shallow techniques also remain obliv-
ious of such aspects of discourse meaning as causal
relations, modality, negation, or scope of quanti-
fiers which are of crucial importance in our setting.
When precise understanding is needed, tutorial sys-
tems either use menu- or template-based input, or
use closed-questions to elicit short answers of lit-
tle syntactic variation (Glass, 2001). However, this
conflicts with the preference for flexible dialog in
active learning (Moore, 1993).
With regard to interpreting mathematical
texts, (Zinn, 2003) and (Baur, 1999) present DRT
analyses of course-book proofs. However, the
language in our dialogs is more informal: natural
language and symbolic mathematical expressions
are mixed more freely, there is a higher degree and
more variety of verbalization, and mathematical
objects are not properly introduced. Moreover, both
above approaches rely on typesetting and additional
information that identifies mathematical symbols,
formulae, and proof steps, whereas our input does
not contain any such information. Forcing the user
to delimit formulae would reduce the flexibility
of the system, make the interface harder to use,
and might not guarantee a clean separation of the
natural language and the non-linguistic content
anyway.
3 Linguistic data
In this section, we first briefly describe the corpus
collection experiment and then present the common
language phenomena found in the corpus.
3.1 Corpus collection
24 subjects with varying educational background
and little to fair prior mathematical knowledge par-
ticipated in a Wizard-of-Oz experiment (Benzm u?ller
et al, 2003b). In the tutoring session, they were
asked to prove 3 theorems2:
(i)  	


 
 


 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 377?384,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Transformation-based Interpretation of Implicit Parallel Structures:
Reconstructing the meaning of vice versa and similar linguistic operators
Helmut Horacek
Fachrichtung Informatik
Universit?at des Saarlandes
66041 Saarbr?ucken, Germany
horacek@ags.uni-sb.de
Magdalena Wolska
Fachrichtung Allgemeine Linguistik
Universit?at des Saarlandes
66041 Saarbr?ucken, Germany
magda@coli.uni-sb.de
Abstract
Successful participation in dialogue as
well as understanding written text re-
quires, among others, interpretation of
specifications implicitly conveyed through
parallel structures. While those whose re-
construction requires insertion of a miss-
ing element, such as gapping and ellip-
sis, have been addressed to a certain extent
by computational approaches, there is vir-
tually no work addressing parallel struc-
tures headed by vice versa-like operators,
whose reconstruction requires transforma-
tion. In this paper, we address the mean-
ing reconstruction of such constructs by
an informed reasoning process. The ap-
plied techniques include building deep se-
mantic representations, application of cat-
egories of patterns underlying a formal
reconstruction, and using pragmatically-
motivated and empirically justified prefer-
ences. We present an evaluation of our al-
gorithm conducted on a uniform collection
of texts containing the phrases in question.
1 Introduction
Specifications implicitly conveyed through paral-
lel structures are an effective means of human
communication. Handling these utterances ade-
quately is, however, problematic for a machine
since a formal reconstruction of the representation
may be associated with ambiguities, typically re-
quiring some degree of context understanding and
domain knowledge in their interpretation. While
parallel structures whose reconstruction mainly re-
quires insertion, such as gapping and ellipsis, have
been addressed to a certain extent by computa-
tional approaches, there is virtually no work ad-
dressing parallel structures whose reconstruction
requires transformation. Several linguistic opera-
tors create specifications of this kind, including:
the other way (a)round, vice-versa, and analo-
gously. Consider, for example, the following state-
ment made by a student in an experiment with a
simulated tutoring system for proving theorems in
elementary set theory (Benzmu?ller et al, 2003):
?If all A are contained in K(B) and this also holds
the other way round, these must be identical sets?
(K stands for set complement). The interpreta-
tion of the the other way round operator is am-
biguous here in that it may operate on immediate
dependents (?all K(B) are contained in A?) or on
the embedded dependents (?all B are contained in
K(A)?) of the verb ?contain?. The fact that the
Containment relation is asymmetric and the con-
text of the task ? proving that ?If A ? K(B), then
B ? K(A)? holds ? suggest that the second inter-
pretation is meant. Assuming this more plausible
reading enables a more goal-oriented dialog: the
tutorial system can focus on a response to the false
conclusion made by the student about the identity
of the sets in question, rather than starting a boring
clarification subdialog.
The above example and several similar others
motivated us to look more systematically at lexi-
cal devices that create specifications of this kind.
We address the interpretation of such structures by
a well-informed reasoning process. Applied tech-
niques include building deep semantic represen-
tations, application of patterns underlying formal
reconstruction, and using pragmatically-motivated
and empirically justified preferences.
The outline of the paper is as follows: We de-
scribe phenomena in question. Then we illustrate
our natural language analysis techniques. We cate-
377
gorize underlying interpretation patterns, describe
the reconstruction algorithm, and evaluate it.
2 Data Collected From Corpora
In order to learn about cross-linguistic regularities
in reconstructing the underlying form of propo-
sitions specified by vice versa or similar opera-
tors, we first looked at several English and Ger-
man corpora. These included, among others, the
Negra, the Frankfurter Rundschau, the Europarl
corpora and a corpus of tutorial dialogs on math-
ematics (Wolska et al, 2004). We also performed
several internet searches. We looked at the Ger-
man phrases andersrum and umgekehrt, and their
English equivalents vice versa and the other way
(a)round. We only considered instances where the
parallel structure with a pair of items swapped is
not stated explicitly. We excluded cases of the
use of umgekehrt as a discourse marker, cases in
which the transformation needed is of purely lex-
ical nature, such as turning ?augment? into ?re-
duce?, and instances of andersrum as expressing a
purely physical change, such as altering the orien-
tation of an object (cf. the Bielefeld corpus1).
The classification of vice versa utterances pre-
sented in Figure 1, reflects the role of the items
that must be swapped to build the parallel propo-
sition conveyed implicitly. The examples demon-
strate that the task of reconstructing the proposi-
tion left implicit in the text may be tricky.
The first category concerns swapping two case
role fillers or Arguments of a predicate head. This
may be applied to Agent and Patient dependents,
as in (1), or to two directional roles as in (2). In
the last example in this category, complications
arise due to the fact that one of the arguments
is missing on the surface and needs to be con-
textually inserted prior to building the assertions
with exchanged directional arguments. Moreover,
the swap can also work across clauses as in (3).
Complex interrelations may occur when the fillers
themselves are composed structures, is in (4),
which also makes swapping other pairs of items
structurally possible. In this example, the need for
exchanging the persons including their mentioned
body parts rather than the mere body parts or just
the persons requires world knowledge.
The second category comprises swapping ap-
plied to modifiers of two arguments rather than the
arguments themselves. An example is (5); the ut-
1http://www.sfb360.uni-bielefeld.de/
terance is ambiguous since, from a purely struc-
tural point of view, it could also be categorized as
an Argument swap, however, given world knowl-
edge, this interpretation is rather infelicitous. Sim-
ilarly to (3), a contextually-motivated enhance-
ment prior to applying a swapping operation is re-
quired in (6); here: a metonymic extension, i.e.
expanding the ?strings? to ?the strings? tones?.
The third category comprises occurrences of a
?mixed? form of the first two with a modifier sub-
stituted for an argument which, in turn, takes the
role of the modifier in the reconstructed form. The
first example, (7), has already been discussed in
the Introduction. The next one, (8), illustrates re-
peated occurrences of the items to be swapped.
Moreover, swapping the items A and B must be
propagated to the included formula. The next ex-
ample, (9), is handled by applying the exchange
on the basis of the surface structure: swapping the
properties of a triangle for the reconstructed asser-
tion. If a deeper structure of the sentence?s mean-
ing is built, this would amount to an implication
expressing the fact that a triangle with two sides
of equal length is a triangle that has two equal
angles. For such a structure, the reconstruction
would fall into the next category, exchange of the
order of two propositions: here, reversing the im-
plication. In (10), the lexeme ?Saxophonist? needs
to be expanded into ?Saxophone? and ?Spieler?
(?player?), prior to performing the exchange.
The fourth category involves a swap of entire
Propositions; in the domain of mathematics, this
may pertain to formulas. In (11), swapping applies
to the sides of the equation descriptively referred
to by the distributivity law. In (12), this applies to
the arguments of the set inclusion relation, when
the arguments are interpreted as propositions. The
last example, (13), requires a structural recasting
in order to apply the appropriate swapping oper-
ation. When the utterance is rebuilt around the
RESULT relation, expressed as an optional case
role on the surface, swapping the two propositions
? ?branching out of languages? and ?geographical
separation? ? yields the desired result.
3 The Interpretation Procedure
In this section, we illustrate our technical contri-
bution. It consists of three parts, each dealt with in
a separate subsection: (1) the linguistic/semantic
analysis, (2) definitions of rules that support build-
ing parallel structures, and (3) the algorithm.
378
Arg
um
ent
sw
ap ( 1) Technological developments influence the regulatory framework and vice versa.
( 2) It discusses all modes of transport from the European Union to these third countries and viceversa.
( 3) Ok ? so the affix on the verb is the trigger and the NP is the target. . . . No; the other way round
( 4) Da traf Vo?ller mit seinem Unterarm auf die Hu?fte des fu?r Glasgow Rangers spielenden Ukrain-ers, oder umgekehrt
Then Vo?ller with his lower arm hit the hip of the Ukrainian playing for Glasgow Rangers, or
the other way round
Mo
difi
ers
wa
p
( 5) Nowadays, a surgeon in Rome can operate on an ill patient ? usually an elderly patient ? inFinland or Belgium and vice versa.
( 6) Der Ton der Klarinette ist wirklich ganz komplementa?r zu den Seiteninstrumenten undumgekehrt
The clarinet?s tone is really very complimentary to strings and vice-versa
Mi
xed
swa
p
( 7) Wenn alle A in K(B) enthalten sind und dies auch umgekehrt gilt, mu? es sich um zwei iden-tische Mengen handeln
If all A are contained in K(B) and this also holds vice-versa, these must be identical sets
( 8) Dann ist das Komplement von Menge A in Bezug auf B die Differenz A/B = K(A) undumgekehrt
Then the complement of set A in relation to B is the difference A/B = K(A) and vice-versa
( 9) Ein Dreieck mit zwei gleichlangen Seiten hat zwei gleichgro?e Winkel und umgekehrt
A triangle with two sites of equal length has two angles of equal size, and vice-versa
( 10) . . . Klarinette fu?r Saxophonist und umgekehrt . . .
. . . a clarinet for a saxophonist and vice-versa . . .
Pro
pos
itio
ns
wa
p ( 11) Man mu? hier das Gesetz der Distributivita?t von Durchschnitt u?ber Vereinigung umgekehrtanwenden
It is necessary here to apply the law of distributivity of intersection over union in reverse
direction
( 12) Es gilt: P (C ? (A ?B)) ? P (C) ? P (A ?B). . . . . Nein, andersrum.
It holds: P (C ? (A ?B)) ? P (C) ? P (A ?B). . . . . No, the other way round.
( 13) Wir wissen, da? sich Sprachen in Folge von geographischer Separierung auseinanderentwick-eln, und nicht umgekehrt
We know that languages branch out as a result of geographical separation, not the other way
round
Figure 1: Examples of utterances with vice versa or similar operators
379
contain.PRED : Containment ? ?,?,?
TERM:K(B).ACT : Container TERM:A.PAT : Containee
Figure 2: Interpreted representation of the utter-ance ?all A are contained in K(B)?
3.1 Linguistic Analysis
The linguistic analysis consists of semantic pars-
ing followed by contextually motivated embed-
ding and enhancements. We assume a deep se-
mantic dependency-based analysis of the source
text. The input to our reconstruction algorithm is
a relational structure representing a dependency-
based deep semantics of the utterance, e.g. in the
sense of Prague School sentence meaning, as em-
ployed in the Functional Generative Description
(FGD) at the tectogrammatical level (Sgall et al,
1986). In FGD, the central frame unit of a clause
is the head verb which specifies the tectogram-
matical relations (TRs) of its dependents (partici-
pants/modifications). Every valency frame spec-
ifies, moreover, which modifications are obliga-
tory and which optional. For example, the utter-
ance (7) (see Figure 1.) obtains the interpretation
presented in Figure 2.2 which, in the context of
an informal verbalization of a step in a naive set
theory proof, translates into the following formal
statement: ??x.x ? A? x ? K(B)?.
The meaning representations are embedded
within discourse context and discourse relations
between adjacent utterances are inferred where
possible, based on the linguistic indicators (dis-
course markers). The nodes (heads) and de-
pendency relations of the interpreted dependency
structures as well as discourse-level relations serve
as input to instantiate the reconstruction pat-
terns. Contextual enhancements (e.g. lexical or
metonymic extensions) driven by the reconstruc-
tion requirements may be carried out.
Based on analysis of corpora, we have iden-
tified combinations of dependency relations that
commonly participate in the swapping operation
called for by the vice versa phrases. Examples of
pairs of such relations at sentence level are shown
in Figure 3.3 Similarly, in the discourse context,
arguments in, for example, CAUSE, RESULT ,
CONDITION , SEQUENCE or LIST rela-
2We present a simplified schematic representation ofthe tectogrammatical representations. Where necessary, forspace reasons, irrelevant parts are omitted.3PRED is the immediate predicate head of the corre-sponding relation.
Exchangeable(ACTOR, PATIENT)
Exchangeable(DIRECTION-WHERE-FROM,DIRECTION-WHERE-TO)
Exchangeable(TIME-TILL-WHEN,TIME-FROM-WHEN)
Exchangeable(CAUSE, PRED)
Exchangeable(CONDITION, PRED)
Figure 3: Examples of exchangeable relations
tions are likely candidates for a swapping opera-
tion. During processing, we use the association
table as a preference criterion for selecting candi-
date relations to instantiate patterns. If one of the
elements of a candidate pair is an optional argu-
ment that is not realized in the given sentence, we
look at the preceding context to find the first in-
stance of the missing element. Additionally, utter-
ance (10) would call for more complex procedures
to identify the required metonymic expansion.
3.2 Interpretation Patterns
In order to accomplish the formal reconstruction
task, we define rules that encapsulate specifica-
tions for building the implicit parallel text on the
basis of the corresponding co-text. The rules con-
sist of a pattern and an action part. Patterns are
matched against the output of a parser on a text
portion in question, by identifying relevant case
roles, and giving access to their fillers. Moreover,
the patterns test constraints on compatibility of
candidates for swapping operations. The actions
apply recasting operations on the items identified
by the patterns to build the implicit parallel text.
Within patterns, we perform category member-
ship tests on the representation. Assuming x re-
ferring to a semantic representation, Pred(x) is
a logical function that checks if x has a Pred-
feature, i.e., it is an atomic proposition. Simi-
larly, Conj(x) and Subord(x) perform more spe-
cific tests for complex propositions: coordina-
tion or subordination, respectively. Moreover,
Pred1(x, x1) accesses the first proposition andbinds it to x1, while Pred2(x, x2) does the samefor the second one. Within a proposition, argu-
ments and modifiers are accessed by Case(x, y),
where y specifies the filler of Case in x, and in-
dices express constraints on identity or distinc-
tiveness of the relations. Case+ is a generaliza-
tion of Case for iterative embeddings, where in-
dividual cases in the chain are not required to be
380
1a. Argument swap within the same clause
Pred(x) ? Case1(x, y) ?Case2(x, z)?
Type? compatible(y, z) ?
Exchangeable(Case1, Case2)?
Swap(x, y, z, xp)
1b. Argument swap across two clauses
Conj(x) ? Case1(x, y) ?Case(y, u) ?
Case2(x, z) ? Case(z, v)? Swap(x, u, v, xp)
2. Modifier swap
Pred(x) ? Case1(x, y) ? Case+11(y, u) ?
Case2(x, z) ?Case+21(z, v)?
?(Case1 = Case2) ? Type?
compatible(u, v)? Swap(x, u, v, xp)
3. Mixed swap
Pred(x) ? Case1(x, y) ? Case11(y, u) ?
Case2(x, z)?
?(Case1 = Case2) ? Type?
compatible(u, z)? Swap(x, u, z, xp)
4. Proposition swap
Subord(x) ? Case1(x, y) ? Case2(x, z) ?
?(Case1 = Case2)? Swap(x, y, z, xp)
Figure 4: Reconstruction patterns
identical. In addition to access predicates, there
are test predicates that express constraints on the
identified items. The most basic one is Type-
compatible(x, y), which tests whether the types
of x and y are compatible according to an underly-
ing domain ontology. A more specific test is per-
formed by Exchangeable(Case1, Case2) to ac-cess the associations specified in the previous sec-
tion. The action part of the patterns is realized by
Swap(x, y, z, xp) which replaces all occurrencesof x in z by y and vice-versa, binding the result to
xp. Different uses of this operation result in dif-ferent instantiations of y and z with respect to the
overarching structure x.
There are patterns for each category introduced
in Section 2 (see Figure 4). All patterns are tested
on a structure x and, if successful, the result is
bound to xp. For Argument swap there are twopatterns. If the scope of the swap is a single
clause (1a), two arguments (case roles) identified
as exchangeable are picked. Their fillers must be
compatible in types. If the swapping overarches
two clauses (1b), the connecting relation must be
a conjunction and subject to swapping are argu-
ments in the same relations. For Modifier swap
(2), type compatible modifiers of distinct argu-
ments are picked. For Mixed swap (3), a depen-
1. Lexical expansion
Pred(x) ? Case1(x, y) ? Lex?
Expand(y, u, Case, v)?
Case2(x, z) ? ?(Case1 =
Case2) ? Type? compatible(v, z) ?
Swap(x, y, Case(u, v), xp) ? Swap(xp, z, v, xp)
2. Recast optional case as head of an obligatory
Pred(x) ?Case1(x, u) ?Case2(x, v) ?
Type(u, tu) ? Type(v, tv)?
Recastable(tv, Case2, tu, Case3) ?
Case3(x,w) ? Type? compatible(v, w)?
?(Case1 = Case2) ? ?(Case1 =
Case3) ? ?(Case2 = Case3)?
Swap(x, u, v, xp) ?Add(xp, Case3(v, u)) ?
Remove(xp, Case2)
3. Recast an optional case as a discourse relation
Pred(x) ? Case(x, y) ?
Member(Case, Subords)?
Build(Case(xp, Case2(xp, y) ?
Case1(xp, Remove(x, y))
Figure 5: Recasting rules
dent is picked, as in (1a) and a type-compatible
modifier of another argument, as in (2). Proposi-
tion swap (4) inverts the order of the two clauses.
In addition to the the pattern matching tests,
the Argument and the Proposition swap operations
undergo a feasibility test if knowledge is avail-
able about symmetry or asymmetry of the relation
(the Pred feature) whose cases are subject to the
swapping operation: if such a relation is known as
asymmetric, the result is considered implausible
due to semantic reasons, if it is symmetric, due to
pragmatic reasons since the converse proposition
conveys no new information; in both cases such a
swapping operation is not carried out.
To extend the functionality of the patterns, we
defined a set of recasting rules (Figure 5) invoked
to reorganize the semantic representation prior to
testing applicability of a suitable reconstruction
rule. In contrast to inserting incomplete informa-
tion contextually and expanding metonymic rela-
tions the recasting operations are intended purely
to accommodate semantic representations for this
purpose. We have defined three recasting rules
(numbered accordingly in Figure 5):
1. Lexical recasting
The semantics of some lexemes conflates the
meaning of two related items. If one of them
is potentially subject to swapping, it is not ac-
cessible for the operation without possibly af-
381
Build-Parallel-Structure (x)
1. Determine scopes for applying swap operations
Structures? ?
if Pred(x) then Scopes? {x} else
if Subord(x) ? Conj(x) ? Case2(x, z)
then Scopes? {z, x}
endif endif
2. Match patterns and build swapped structures
forall Scope1 in Scopes do
Structures? Structures?
< X ? swap(Scope1) >
< X ? swap(Y ? recast(Scope1)) >
end forall
return Sort(Apply ? priorities(Structures))
Figure 6: Reconstruction algorithm
fecting the other so closely related to it. The
representation of such lexemes is expanded,
provided there is a sister case with a filler that
is type compatible.
2. Case recasting
The dependency among items may not be re-
flected by the dependencies in the linguistic
structure. Specifically, a dependent item may
appear as a sister case in overarching case
frame. The purpose of this operation is to
build a uniform representation, by removing
the dependent case role filler and inserting it
as a modifier of the item it is dependent on.
3. Proposition recasting
Apart from expressing a discourse relation
by a connective, a proposition filling a sub-
ordinate relation may also be expressed as a
case role (argument). Again, uniformity is
obtained through lifting the argument (case
filler) and expressing the discourse relation as
a multiple clause construct.
Additional predicates are used to implement re-
casting operations. For example, the predicate
Lex?Expand(y, u, Case, v) re-expresses the se-
mantics of y by u, accompanied by a Case role
filled by v. Type(x, y) associates the type y
with x. The type information is used to access
Recastable(t1, C1, t2, C2) table to verify whethercase C1 with a t1-type filler can also be expressedas case C2 with type t2. Build(x) creates a newstructure x. Remove(x, y) is realized as a func-
tion, deleting occurrences of y in x, and Add(x, y)
expands x by an argument y.
3.3 The Structure Building Algorithm
In this section, we describe how we build implic-
itly conveyed parallel structures based on the def-
initions of swapping operations with optional in-
corporation of recasting operations if needed. The
procedure consists of two main parts (see Fig-
ure 6). In the first part, the scope for applying the
swapping rules defined in Figure 4 is determined,
and in the second part, the results obtained by ex-
ecuting the rules are collected. Due to practical
reasons, we introduce simplifications concerning
the scope of vice-versa in the current formulation
of the procedure. While the effect of this operator
may range over entire paragraphs in some involved
texts, we only consider single sentences with at
most two coordinated clauses or one subordinated
clause. We feel that this restriction is not severe
for uses in application-oriented systems.
The procedure Build-Parallel-Structure takes
the last input sentence x, examines its clause
structure, and binds potential scopes to vari-
able Scopes. For composed sentences, the en-
tire sentence (x) as well as the second clause
(Case2(x, z)) is a potential scope for building par-allel structures.
In the second part of the procedure, each swap-
ping pattern is tested for the two potential scopes,
and results are accumulated in Structures. The
call < X ? swap(Scope1) >, with X beingeither Case, Argument, Mixed, or Prop ex-
presses building a set of all possible instantiations
of the pattern specified when applied to Scope1.Some of these operations are additionally invoked
with alternative parameters which are accommo-
dated by a recasting operation fitting to the pat-
tern used, that call being < X ? swap(Y ?
recast(Scope1)) >, where Y is Case, Lex, or
Prop. Finally, if multiple readings are generated,
they are ranked according to the following priori-
tized criteria:
1. The nearest scope is preferred;
2. Operations swapping ?duals?, such as left-right, aregiven priority;
3. Candidate phrases are matched against the corpus;items with higher bigram frequencies are preferred.
Linguistic analysis, structure reconstruction
patterns, recasting rules, and the algorithms oper-
ating on top of these structures are formulated in
a domain-independent way, also taking care that
the tasks involved are clearly separated. Hence, it
is up to a concrete application to elaborate lexical
382
semantic definitions required (e.g. for a saxophon-
ist to capture example (10) in Figure 1) to define
the tables Exchangeable and Recastable, and to
enhance preference criteria.
4 Evaluation
We conducted an evaluation of the parallel struc-
ture building algorithm on a sample of sentences
from Europarl (Koehn, 2002), a parallel corpus of
professionally translated proceedings of the Euro-
pean Parliament aligned at the document and sen-
tence level. At this point, we were able to conduct
only manual evaluation. This is mainly due to the
fact that we did not have access to a wide-coverage
semantic dependency parser for English and Ger-
man.4 In this section, we present our corpus sam-
ple and the evaluation results.
Evaluation sample To build the evaluation sam-
ple, we used sentence- and word-tokenized En-
glish German part of Europarl. Using regular ex-
pressions, we extracted sentences with the follow-
ing patterns: (i) for English, phrases the other way
a*round or vice versa (ii) for German: (ii-1) the
word umgekehrt preceded by a sequence of und
(?and?), oder (?or?), sondern (?but?), aber (?but?)
or comma, optional one or two tokens and op-
tional nicht (?not?), (ii-2) the word umgekehrt pre-
ceded by a sequence gilt (?holds?) and one or two
optional tokens, (ii-3): the word anders(he)*rum.
We obtained 137 sentences.
Next, given the present limitation of our algo-
rithm (see Section 3.3), we manually excluded
those whose interpretation involved the preceding
sentence or paragraph,5 as well as those in which
the interpretation was explicitly spelled out. There
were 27 such instances. Our final evaluation sam-
ple consisted of 110 sentences: 82 sentences in
English?German pairs and 28 German-only.6
4In the future, we are planning an automated evaluation inwhich as input to the implemented algorithm we would passmanually built dependency structures.5For example, sentences such as: ?Mr President , concern-ing Amendment No 25 , I think the text needs to be lookedat because in the original it is the other way round to how itappears in the English text .?6The reason for this split is that the English equivalentsof the German sentences containing the word umgekehrt maycontain phrases other than the other way round or vice versa.Depending on context, phrases such as conversely, in or the
reverse, the opposite, on the contrary may be used. Here, wetargeted only the other way round and vice versa phrases. Ifthe German translation contained the word umgekehrt, andthe English source one of the alternatives to our target, in theevaluation we included only the German sentence.
Category No. of instances
Arg 64
Modifier 5
Arg/Mod 3
Mixed 6
Arg/Mixed 2
Prop 1
Arg/Prop 1
Lex 18
Other 10
Total 110
Table 1: Distribution of patterns
Distribution of categories We manually cate-
gorized the structures in our sample and marked
the elements of the dependency structures that par-
ticipate in the transformation. Table 1. presents
the distribution of structure categories. We ex-
plicitly included counts for alternative interpreta-
tions. For example Arg/Mod means that either
the Argument or Modifier transformation can be
applied with the same effect, as in the sentence
?External policy has become internal policy, and
vice versa?: either the words ?external? and ?in-
ternal? may be swapped (Modifier), or the whole
NPs ?external policy? and ?internal policy? (Ar-
gument). Lex means that none of the patterns was
applicable and a lexical paraphrase (such as use of
an antonym) needed to be performed in order to re-
construct the underlying semantics (i.e. no paral-
lel structure was involved). Other means that there
was a parallel structure involved, however, none of
our patterns covered the intended transformation.
Evaluation results The evaluation results are
presented in Tables 2. and 3. Table 2. shows an
overview of the results. The interpretation of the
result categories is as follows:
Correct: the algorithm returned the intended reading asa unique interpretation (this includes correct identi-fication of ?lexical paraphrases? (the Lex categoryin Table 1.);
Ambig.: multiple results were returned with the intendedreading among them;
Wrong: the algorithm returned a wrong result (if multi-ple results, then the intended one was not included);
Failed: the algorithm failed to recognize a parallel struc-ture where one existed because no known patternmatched.
Table 3. shows within-category results. Here, Cor-
rect result for Other means that the algorithm cor-
rectly identified 8 cases to which no current pat-
tern applied. The two Wrong results for Other
383
Result No. of instances
Correct 75
Ambig. 21
Wrong 4
Failed 10
Total 110
Table 2: Evaluation results
Category Correct Ambig. Wrong Failed Total
Arg 46 17 0 1 64
Mod 3 2 0 0 5
Arg/Mod 3 ? 0 0 3
Mixed 4 2 0 0 6
Arg/Mixed 2 ? 0 0 2
Prop 1 0 0 0 1
Arg/Prop 0 ? 0 1 1
Lex 16 0 2 0 18
Other 8 0 2 0 10
Table 3: Within-category results
mean that a pattern was identified, however, this
pattern was not the intended one. In two cases
(false-negatives), the algorithm failed to identify
a pattern even though it fell into one of the known
categories (Argument and Prop).
Discussion The most frequently occurring pat-
tern in our sample is Argument. This is often a
plausible reading. However, in 3 of the 4 false-
positives (Wrong results), the resolved incorrect
structure was Arg. If we were to take Arg as base-
line, aside from missing the other categories (al-
together 12 instances), we would obtain the final
result of 63 Correct (as opposed to 96; after col-
lapsing the Correct and Ambig. categories) and
15 (as opposed to 4) Wrong results.
Let us take a closer look at the false-negative
cases and the missed patterns. Two missed known
categories involved multiple arguments of the
main head: a modal modifier (modal verb) and an
additive particles (?also?) in one case, and in the
other, rephrasing after transformation. To improve
performance on cases such as the former, we could
incorporate an exclusion list of dependents that the
transformation should disregard.
Among the patterns currently unknown to the
algorithm, we found four types (one instance of
each in the sample) that we can anticipate as fre-
quently recurring: aim and recipient constructs
involving a head and its Aim- and Beneficiary-
dependent respectively, a temporal-sequence in
which the order of the sequence elements is re-
versed, and a comparative structure with swapped
relata. The remaining 6 structures require a more
involved procedure: either the target dependent is
deeply embedded or paraphrasing and/or morpho-
logical transformation of the lexemes is required.
5 Conclusions and Future Research
In this paper, we presented techniques of for-
mal reconstruction of parallel structures implicitly
specified by vice versa or similar operators. We
addressed the problem by a domain-independent
analysis method that uses deep semantics and con-
textually enhanced representations, exploits re-
casting rules to accommodate linguistic variations
into uniform expressions, and makes use of pat-
terns to match parallel structure categories.
Although we dedicated a lot of effort to building
a principled method, the success is limited with
respect to the generality of the problem: in some
cases, the scope of reconstruction overarches en-
tire paragraphs and deciding about the form re-
quires considerable inferencing (cf. collection at
http://www.chiasmus.com/). For our purposes, we
are interested in expanding our method to other
kinds of implicit structures in the tutorial context,
for example, interpretations of references to analo-
gies, in the case of which structure accommoda-
tion and swapping related items should also be
prominent parts.
References
C. Benzmu?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayova?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B.Q.
Vo, and M. Wolska. 2003. A Wizard-of-Oz experiment
for tutorial dialogues in mathematics. In Supplementary
Proceedings of the 11th Conference on Artificial Intelli-
gence in Education (AIED-03); Vol. VIII. Workshop on
Advanced Technologies for Mathematics Education, pages
471?481, Sydney, Australia.
P. Koehn. 2002. Europarl: A multilingual corpus for evalua-
tion of machine translation, Draft, Unpublished.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The meaning of
the sentence in its semantic and pragmatic aspects. Reidel
Publishing Company, Dordrecht, The Netherlands.
M. Wolska, B.Q. Vo, D. Tsovaltzi, I. Kruijff-Korbayova?,
E. Karagjosova, H. Horacek, M. Gabsdil, A. Fiedler, and
C. Benzmu?ller. 2004. An annotated corpus of tutorial
dialogs on mathematical theorem proving. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC-04), pages 1007?1010,
Lisbon, Potugal.
384
Lexical-Semantic Interpretation of Language Input
in Mathematical Dialogs
Magdalena Wolska1 Ivana Kruijff-Korbayova?1 Helmut Horacek2
1Fachrichtung Computerlinguistik 2Fachrichtung Informatik
Universita?t des Saarlandes, Postfach 15 11 50
66041 Saarbru?cken, Germany
{magda,korbay}@coli.uni-sb.de, horacek@ags.uni-sb.de
Abstract
Discourse in formal domains, such as mathematics,
is characterized by a mixture of telegraphic natu-
ral language and embedded (semi-)formal symbolic
mathematical expressions. Due to the lack of em-
pirical data, little is known about the suitability of
input analysis methods for mathematical discourse
in a dialog setting. We present an input understand-
ing method for a tutoring system teaching mathe-
matical theorem proving. The adopted deep anal-
ysis strategy is motivated by the complexity of the
language phenomena observed in a corpus collected
in a Wizard-of-Oz experiment. Our goal is a uni-
form input interpretation, in particular, considering
different degrees of formality of natural language
verbalizations.
1 Introduction
In the DIALOG1 project (Benzm u?ller et al, 2003a),
we are investigating and modeling semantic and
pragmatic phenomena in tutorial dialogs focused on
problem solving skills in mathematics. Our goal is
(i) to empirically investigate the use of flexible natu-
ral language dialog in tutoring mathematics, and (ii)
to develop a dialog-based tutoring system for teach-
ing mathematical theorem proving. The experimen-
tal system will engage in a dialog in written natural
language to help a student understand and construct
mathematical proofs. In this paper, we address a
strategy for user input interpretation in our setting.
Because of the lack of empirical dialog-data on
the use of natural language in formal domains, such
as mathematics, we conducted a Wizard-of-Oz ex-
periment to collect a corpus of dialogs with a sim-
ulated system teaching proofs in naive set theory.
An investigation of the corpus reveals language phe-
nomena that present challenges to the existing com-
monly used input understanding methods. The chal-
1The DIALOG project is a collaboration between the Com-
puter Science and Computational Linguistics departments of
University of the Saarland, and is a part of the Collaborative
Research Center on Resource-Adaptive Cognitive Processes,
SFB 378 (www.coli.uni-sb.de/sfb378).
lenges lie in (i) the tight interleaving of natural and
symbolic language, (ii) varying degree of natural
language verbalization of the formal mathematical
content, and (iii) informal and/or imprecise refer-
ence to mathematical concepts and relations.
These phenomena motivate the use of deep syn-
tactic and semantic analysis of user input. We de-
veloped a grammar that allows a uniform treatment
of the linguistic content on a par with the math-
ematical content and thus supports analysis of in-
puts of different degrees of verbalization. We em-
ploy a domain-motivated semantic lexicon to medi-
ate between the domain-independent semantic rep-
resentation obtained through semantic construction
during parsing and domain-specific interpretation.
This serves to achieve a consistent semantic anal-
ysis while avoiding example-based heuristics.
The paper is organized as follows: In Sect. 2,
we present the setup of the system and the corpus
collection experiment. In Sect. 3 and show exam-
ples of language phenomena from our dialogs. In
Sect. 4, we first summarize the approach to pars-
ing the mixed symbolic and natural language in-
put and then present a lexical-semantic interface to
a domain-specific interpretation of the input. We
show example analyzes in Sect. 5. In Sect. 6,
we summarize relevant existing approaches to input
analysis in (tutorial) dialog systems on the one hand
and analysis of mathematical discourse on the other.
Sect. 7 is the conclusion.
2 System setup and corpus collection
Our system scenario is illustrated in Fig. 1:
? Learning Environment: Students take an inter-
active course in the relevant subfield of mathe-
matics.
? Mathematical Proof Assistant (MPA): Checks
the appropriateness of user specified inference
steps with respect to the problem-solving goal;
based on ?MEGA.
? Proof Manager (PM): In the course of tutor-
ing session the student may explore alternative
PEDAGOGICAL
KNOWLEDGE
US
ER
MO
DE
L
LEARNING
ENVIRONMENT
MATHEMATICAL
PROOF ASSISTANT
DIALOG MANAGERG
EN
ER
A
TI
O
N
PRO
O
F M
A
N
A
G
ER
A
N
A
LY
SIS
MATHEMATICAL
KNOWLEDGE
(MBASE)
ACTIVEMATH OMEGA
RESOURCES
LINGUISTIC DIALOG
RESOURCES
TUTORING
RESOURCES /
MANAGER
U
SE
R
Figure 1: DIALOG project scenario.
proofs. The PM builds and maintains a repre-
sentation of constructed proofs and communi-
cates with the MPA to evaluate the appropriate-
ness of the student?s contributions for the proof
construction.
? Dialog Manager: We employ the Information-
State (IS) Update approach developed in the
TRINDI project2
? Tutorial Manager (TM): This component
incorporates extensions to handle tutorial-
specific dialog moves, such as hinting.
? Knowledge Resources: This includes peda-
gogical knowledge (teaching strategies), and
mathematical knowledge.
In order to empirically investigate the use of nat-
ural language in mathematics tutoring, we collected
and analyzed a corpus of dialogs with a simulated
tutoring system.
24 subjects with varying educational background
and little/fair prior mathematical knowledge partic-
ipated in a Wizard-of-Oz experiment (Benzm u?ller et
al., 2003b). The experiment consisted of 3 phases:
(i) preparation and pre-test (on paper), (ii) tutor-
ing session (mediated by a WOz tool (Fiedler and
Gabsdil, 2002)), (iii) post-test and evaluation ques-
tionnaire (on paper). At the tutoring session, they
were asked to prove 3 theorems3: (i) K((A ? B) ?
(C ? D)) = (K(A) ? K(B)) ? (K(C) ? K(D));
(ii) A ? B ? P ((A ? C) ? (B ? C)); (iii) If
A ? K(B), then B ? K(A). The subjects were
instructed to enter proof steps, rather than complete
proofs at once, to encourage interaction with the
system. The subjects and the tutor were free in for-
mulating their turns.4
2http://www.ling.gu.se/research/projects/trindi/
3K stands for set complement and P for power set.
4Buttons were available in the interface for inserting math-
ematical symbols, while literals were typed on the keyboard.
The collected corpus consists of 66 dialog log-
files, containing on average 12 turns. The total num-
ber of sentences is 1115, of which 393 are student
sentences. The students? turns consisted on aver-
age of 1 sentence, the tutor?s of 2. More details on
the corpus itself and annotation efforts that guide
the development of the system components can be
found in (Wolska et al, 2004).
3 Linguistic data
In this section, we present an overview of the lan-
guage phenomena prominent in the collected di-
alogs to indicate the overall complexity of input un-
derstanding in our setting.5
Interleaved natural language and formulas The
following examples illustrate how the mathematical
language, often semi-formal, is interleaved with the
natural language informally verbalizing proof steps.
A auch ? K(B) [Aalso ? K (B)]
A?B ist ? von C?(A?B) [... is ? of ...]
(da ja A?B=?) [(because A?B=?)]
B enthaelt kein x?A [B contains no x?A]
The mixture affects the way parsing needs to be
conducted: mathematical content has to be identi-
fied before it is interpreted within the utterance. In
particular, mathematical objects (or parts thereof)
may lie within the scope of quantifiers or negation
expressed in natural language (as in the last example
above).
Imprecise or informal naming Domain relations
and concepts are described informally using impre-
cise and/or ambiguous expressions.
A enthaelt B [A contains B]
A muss in B sein [A must be in B]
B vollstaendig ausserhalb von A liegen muss, also im
Komplement von A
[B has to be entirely outside of A, so in the complement of A]
dann sind A und B (vollkommen) verschieden, haben keine
gemeinsamen Elemente
[then A and B are (completely) different, have no common
elements]
In the above examples, contain and be in can ex-
press domain relations of (strict) subset or element,
while be outside of and be different are informal
descriptions of the empty intersection of sets.
To handle imprecision and informality, we have
designed an ontological knowledge base that in-
cludes domain-specific interpretations of concep-
tual relations that have corresponding formal coun-
terparts in the domain of naive set theory.
The dialogs were typed in German.
5As the tutor was also free in wording his turns, we include
observations from both student and tutor language behavior.
Metonymy Metonymic expressions are used to
refer to structural sub-parts of formulas, resulting
in predicate structures acceptable informally, yet in-
compatible in terms of selection restrictions.
Dann gilt fuer die linke Seite, wenn
C ? (A ? B) = (A ? C) ? (B ?C), der Begriff A ? B dann ja
schon dadrin und ist somit auch Element davon
[Then for the left hand side it is valid that..., the term A ? B is already
there, and so an element of it]
where the predicate be valid for, in this domain,
normally takes an argument of sort CONSTANT,
TERM or FORMULA, rather than LOCATION;
de morgan regel 2 auf beide komplemente angewendet
[de morgan rule 2 applied to both complements]
where the predicate apply takes two arguments: one
of sort RULE and the other of sort TERM or FOR-
MULA, rather than OPERATION ON SETS.
Informal descriptions of proof-step actions
Wende zweimal die DeMorgan-Regel an
[I?m applying DeMorgan rule twice]
damit kann ich den oberen Ausdruck wie folgt schreiben:. . .
[given this I can write the upper term as follows:. . . ]
Sometimes, ?actions? involving terms, formulae
or parts thereof are verbalized before the appropri-
ate formal operation is performed. The meaning of
the ?action verbs? is needed for the interpretation of
the intended proof-step.
Discourse deixis
der obere Ausdruck [the above term]
der letzte Satz [the last sentence]
Folgerung aus dem Obigen [conclusion from the above]
aus der regel in der zweiten Zeile
[from the rule in the second line]
This class of referring expressions includes also
references to structural parts of terms and formu-
las such as ?the left side? or ?the inner parenthe-
sis? which are incomplete specifications: the former
refers to a part of a formula, the latter, metonymic,
to an expression enclosed in parenthesis. More-
over, they require discourse referents for sub-parts
of mathematical expressions to be available.
Generic vs. specific reference
Potenzmenge enthaelt alle Teilmengen, also auch (A?B)
[A power set contains all subsets, hence also(A?B)]
Generic and specific references can appear within
one utterance as above, where ?a power set? is a
generic reference, whereas ?A?B? is a specific ref-
erence to a subset of a specific instance of a power
set introduced earlier.
Co-reference6
Da, wenn Ai?K(Bj) sein soll, Ai Element von K(Bj) sein
muss. Und wenn Bk?K(Al) sein soll, muss esk auch
Element von K(Al) sein.
[Because if it should be that Ai?K(Bj), Ai must be an
element of K(Bj). And if it should be that Bk?K(Al), it
must be an element of K(Al) as well.]
DeMorgan-Regel-2 besagt: K(Ai ? Bj) = K(Ai) ? K(Bj)
In diesem Fall: z.B. K(Ai) = dem Begriff
K(Ak ? Bl) K(Bj) = dem Begriff K(C ? D)[DeMorgan-Regel-2 means:
K(Ai ? Bj) = K(Ai) ? K(Bj) In this case: e.g. K(Ai) =
the term K(Ak ? Bl) K(Bj) = the term K(C ?D)]
Co-reference phenomena specific to informal
mathematical discourse involve (parts of) mathe-
matical expressions within text. In particular, enti-
ties denoted with the same literals may not co-refer,
as in the second utterance.
In the next section, we present the input interpre-
tation procedure up to the level of lexical-semantic
interpretation. We concentrate on the interface be-
tween the linguistic meaning representation (ob-
tained from the parser) and the representation of
domain-knowledge (encoded in a domain ontol-
ogy), which we realize through a domain-motivated
semantic lexicon.
4 Interpretation strategy
The task of the input interpretation component is
two-fold. Firstly, it is to construct a representation
of the utterance?s linguistic meaning. Secondly, it is
to identify within the utterance, separate, and con-
struct interpretations of:
(i) parts which constitute meta-communication
with the tutor (e.g., ?Ich habe die Aufgaben-
stellung nicht verstanden.? [I don?t understand
what the task is.] that are not to be processed
by the domain reasoner; and
(ii) parts which convey domain knowledge that
should be verified by a domain reasoner; for
example, the entire utterance ?K((A ? B)) ist
laut deMorgan-1 K(A) ? K(B)? [... is, ac-
cording to deMorgan-1,...] can be evaluated
in the context of the proof being constructed;
on the other hand, the reasoner?s knowledge
base does not contain appropriate representa-
tions to evaluate the appropriateness of the fo-
cusing particle ?also? in ?Wenn A = B, dann ist
A auch ? K(B) und B ? K(A).? [If A = B,
then A is also ? K(B) and B ? K(A).].
Domain-specific interpretation(s) of the proof-
relevant parts of the input are further processed by
6To indicate co-referential entities, we inserted the indices
which are not present in the dialog logfiles.
Proof Manager, a component that directly commu-
nicates with a domain-reasoner7 . The task of the
Proof Manager is to: (i) build and maintain a repre-
sentation of the proof constructed by the student;8
(ii) check appropriateness of the interpretation(s)
found by the input understanding module with the
state of the proof constructed so far; (iii) given the
current proof state, evaluate the utterance with re-
spect to soundness, relevance, and completeness.
The semantic analysis proceeds in 2 stages:
(i) After standard pre-processing9 , mathematical
expressions are identified, analyzed, catego-
rized, and substituted with default lexicon en-
tries encoded in the grammar. The input is then
syntactically parsed, and an formal abstract
representation of its meaning is constructed
compositionally along with the parse;
(ii) The obtained meaning representation is subse-
quently merged with discourse context and in-
terpreted by consulting a semantic lexicon of
the domain and a domain-specific ontology.
In the next sections, we first briefly summa-
rize the syntactic and semantic parsing part of the
input understanding process10 and show the for-
mat of meaning encoding constructed at this stage
(Sect. 4.1). Then, we show the lexical-semantic in-
terface to the domain ontology (Sect. 4.2).
4.1 Linguistic Meaning
By linguistic meaning (LM), we understand the
dependency-based deep semantics in the sense of
the Prague School sentence meaning as employed in
the Functional Generative Description (FGD) (Sgall
et al, 1986; Kruijff, 2001). It represents the lit-
eral meaning of the utterance rather than a domain-
specific interpretation.11 In FGD, the central frame
unit of a sentence/clause is the head verb which
specifies the tectogrammatical relations (TRs) of
7We are using a version of ?MEGA adapted for assertion-
level proving (Vo et al, 2003)
8The discourse content representation is separated from the
proof representation, however, the corresponding entities must
be co-indexed in both.
9Standard pre-processing includes sentence and word to-
kenization, (spelling correction and) morphological analysis,
part-of-speech tagging.
10We are concentrating on syntactically well-formed utter-
ances. In this paper, we are not discussing ways of combin-
ing deep and shallow processing techniques for handling mal-
formed input.
11LM is conceptually related to logical form, however, dif-
fers in coverage: while it does operate on the level of deep
semantic roles, such aspects of meaning as the scope of quan-
tifiers or interpretation of plurals, synonymy, or ambiguity are
not resolved.
its dependents (participants). Further distinction is
drawn into inner participants, such as Actor, Pa-
tient, Addressee, and free modifications, such as Lo-
cation, Means, Direction. Using TRs rather than
surface grammatical roles provides a generalized
view of the correlations between the conceptual
content of an utterance and its linguistic realization.
At the pre-processing stage, mathematical ex-
pressions embedded within input are identified, ver-
ified as to syntactic validity, categorized, and sub-
stituted with default lexical entries encoded in the
parser grammar for mathematical expression cate-
gories. For example, the expression K((A ? B) ?
(C ?D)) = (K(A?B)?K(C ?D)) given its top
node operator, =, is of type formula, its ?left side?
is the expression K((A ? B) ? (C ? D)), the list
of bracketed sub-expressions includes: A?B, C?D,
(A ? B) ? (C ? D), etc.
Next, the pre-processed input is parsed with
a lexically-based syntactic/semantic parser built
on Multi-Modal Combinatory Categorial Gram-
mar (Baldridge, 2002; Baldridge and Kruijff, 2003).
The task of the deep parser is to produce an FGD-
based linguistic meaning representation of syntac-
tically well-formed sentences and fragments. The
linguistic meaning is represented in the formalism
of Hybrid Logic Dependency Semantics. Details on
the semantic construction in this formalism can be
found in (Baldridge and Kruijff, 2002).
To derive our set of TRs we generalize and sim-
plify the collection of Praguian tectogrammatical
relations from (Hajic?ova? et al, 2000). One rea-
son for simplification is to distinguish which re-
lations are to be understood metaphorically given
the domain-specific sub-language. The most com-
monly occurring relations in our context (aside from
the roles of Actor and Patient) are Cause, Condi-
tion, and Result-Conclusion (which coincide with
the rhetorical relations in the argumentative struc-
ture of the proof):
Da [A ? K(B) gilt]<CAUSE>, alle x, die in A sind sind nicht in B
[As A?K(B) applies, all x that are in A are not in B]
Wenn [A ? K(B)]<COND>, dann A ? B=?
[If A?K(B), then A?B=?]
For example, in one of the readings of ?B en-
thaelt x ? A?, the verb ?enthaelten? represents
Figure 2: TRs in ?B contains x ? A?.
contain
FORMULA:B
<ACT>
FORMULA:x ? A
<PAT>
the meaning contain and in this frame takes de-
pendents in the relations Actor and Patient, shown
schematically in Fig. 2 (FORMULA represents the
default lexical entry for the identified mathematical
expressions categorized as formulas). The linguis-
tic meaning of this utterance returned by the parser
obtains the following representation:
@h1(contain ? <ACT>(f1 ? FORMULA:B) ? <PAT>(f2 ?
FORMULA: x ? A)
where h1 is the state where the proposition contain
is true, and the nominals f1 and f2 represent depen-
dents of the head contain, in the relations Actor and
Patient, respectively.
More details on our approach to parsing inter-
leaved natural and symbolic expressions can be
found in (Wolska and Kruijff-Korbayova?, 2004a)
and more information on investigation into tec-
togrammatical relations that build up linguistic
meaning of informal mathematical text can be found
in (Wolska and Kruijff-Korbayova?, 2004b).
4.2 Conceptual Semantics
At the final stage of input understanding, the lin-
guistic meaning representations obtained from the
parser are interpreted with respect to the given
domain. We encode information on the domain-
specific concepts and relations in a domain ontol-
ogy that reflects the knowledge base of the domain-
reasoner, and which is augmented to allow res-
olution of ambiguities introduced by natural lan-
guage (Horacek and Wolska, 2004). We interface
to the domain ontology through an upper-level on-
tology of concepts at the lexical-semantics level.
Domain specializations of conceptual relations
are encoded in the domain ontology, while a seman-
tic lexicon assigns conceptually-oriented semantics
in terms of linguistic meaning frames and provides a
link to the domain interpretation(s) through the do-
main ontology. Lexical semantics in combination
with the knowledge encoded in the ontology allows
us to identify those parts of utterances that have an
interpretation in the given domain. Moreover, pro-
ductive rules for treatment of metonymic expres-
sions are encoded through instantiation of type com-
patible concepts. If more than one lexical-semantic
interpretation is plausible, no disambiguation is per-
formed. Alternative conceptual representations are
further interpreted using the domain ontology, and
passed on to the Proof Manager for evaluation. Be-
low we explain some of the entries the semantic lex-
icon encodes:
Containment The Containment relation special-
izes into the domain relations of (strict) SUB-
SET and ELEMENT. Linguistically, it can be re-
alized, among others, with the verb ?enthalten?
(?contain?). The tectogrammatical frame of
?enthalten? involves the roles of Actor (ACT)
and Patient (PAT):
contain(ACTtype:F ORMULA, PATtype:F ORMULA) ?
(SUBFORMULAP AT , embeddingACT )
contain(ACTtype:OBJECT , PATtype:OBJECT ) ?
CONTAINMENT(containerACT , containeeP AT )
Location The Location relation, realized linguisti-
cally by the prepositional phrase introduced by
?in?, involves the tectogrammatical relations
HasProperty-Location (LOC) and the Actor of
the predicate ?sein?. We consider Location
in our domain as synonymous with Contain-
ment. Another realization of this relation, dual
to the above, occurs with the adverbial phrase
?au?erhalb von ...(liegen)? (?lie outside of?)
and is defined as negation of Containment:
in(ACTtype:OBJECT ,LOCtype:OBJECT )
? CONTAINMENT(containerLOC , containeeACT )
outside(ACTtype:OBJECT ,LOCtype:OBJECT )
? not(in(ACTtype:OBJECT ,LOCtype:OBJECT ))
Common property A general notion of ?common
property? we define as follows:
common(Property, ACTplural(A:SET,B:SET))
? Property(p1, A) ? Property(p1, B)
Property is a meta-object that can be instanti-
ated with any relational predicate, for example
as in ?(A und B)<ACT> haben (gemeinsame
Elemente)<PAT>? (?A and B have common
elements?):
common(ELEMENT, ACTplural(A:SET,B:SET))
? ELEMENT(p1 ,A) ? ELEMENT(p1 , B)
Difference The Difference relation, realized
linguistically by the predicates ?verschieden
(sein)? (?be different?; for COLLECTION or
STRUCTURED OBJECTS) and ?disjunkt (sein)?
(?be disjoint?; for objects of type COLLEC-
TION) involves a plural Actor (e.g. coordinated
noun phrases) and a HasProperty TRs. De-
pending on the type of the entity in the Actor
relation, the interpretations are:
different(ACTplural(A:SET,B:SET)) ? A 6= B
different(ACTplural(A:SET,B:SET))
? (e1 ELEMENT A ? e2 ELEMENT B ? e1 6= e2)
different(ACTplural(A:ST RUCTUREDOBJECT
,B:STRUCT UREDOBJECT ))
? (Property1(p1, A) ? Property2(p2, B) ?
Property1 = Property2 ? p1 6= p2)
Mereological relations Here, we encode part-
of relations between domain objects. These
concern both physical surface and ontologi-
cal properties of objects. Commonly occurring
part-of relations in our domain are:
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED OBJECTSUBT ERM,SUBF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTENCLOSEDT ERM,ENCLOSEDF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTT ERMCOMP ONENT,FORMULACOMP ONENT )
Moreover, from the ontology we have:
Property(STRUCTURED OBJECTterm,formula ,
componentterm?side,formula?side)
Using these definitions and polysemy rules
such as polysemous(Object, Property), we can
obtain interpretation of utterances such as
?Dann gilt f u?r die linke Seite, . . . ? (?Then
for the left side it holds that . . . ?) where the
predicate ?gilt? normally takes two arguments
of types STRUCTURED OBJECTterm,formula ,
rather than an argument of type Property.
For example, the previously mentioned predicate
contain (Fig. 2) represents the semantic relation of
Containment which, in the domain of naive set the-
ory, is ambiguous between the domain relations EL-
EMENT, SUBSET, and PROPER SUBSET. The al-
ternative specializations are encoded in the domain
ontology, while the semantic lexicon provides the
conceptual structure of the head predicate. At the
domain interpretation stage, the semantic lexicon is
consulted to translate the tectogrammatical frame of
the predicate into a semantic relation represented
in the domain ontology. For the predicate contain,
from the semantic lexicon, we obtain:
contain(ACTtype:F ORMULA, PATtype:F ORMULA)
? (SUBFORMULAP AT , embeddingACT )
[?a Patient of type FORMULA is a subformula embedded within a
FORMULA in the Actor relation with respect to the head contain?]
contain(ACTtype:OBJECT , PATtype:OBJECT )
? CONTAINMENT(containerACT , containeeP AT )
[?the Containment relation involves a predicate contain and its Actor
and Patient dependents, where the Actor and Patient are the container
and containee parameters respectively?]
Translation rules that consult the domain ontology
expand the conceptual structure representation into
alternative domain-specific interpretations preserv-
ing argument structure. As it is in the capacity of
neither sentence-level nor discourse-level analysis
to evaluate the appropriateness of the alternative in-
terpretations in the proof context, this task is dele-
gated to the Proof Manager.
5 Example analysis
In this section, we illustrate the mechanics of the
approach on the following example:
A enthaelt keinesfalls Elemente, die auch in B sind.
[A contains no elements that are also in B]
The analysis proceeds as follows.
The mathematical expression tagger first iden-
tifies the expressions A and B. If there was no
prior discourse entity for ?A? and ?B? to verify
their types, they are ambiguous between constant,
term, and formula12 . The expressions are substi-
tuted with generic entries FORMULA, TERM, CONST
represented in the parser grammar. The sentence is
assigned alternative readings: ?CONST contains no
elements that are also in CONST?, ?CONST contains
no elements that are also in TERM?, ?CONST con-
tains no elements that are also in FORMULA?, etc.
Here, we continue only with ?CONST contains no
elements that are also in CONST?; the other readings
would be discarded at later stages of processing be-
cause of sortal incompatibilities.
The linguistic meaning of the utterance obtained
from the parser is represented by the following for-
mula13:
@n1(no ? <Restr>e1 ?
<Body>(p1 ? contain ? <ACT>(a1 ? A) ? <PAT> e1)) ?
@e1(element ?
<GenRel>(b1 ? be ? <ACT>e1 ? <HasProp-Loc>(b2 ? B)))
[?(set) A contains no elements that are in (set) B?]
Next, the semantic lexicon is consulted to trans-
late the linguistic meaning representation into a con-
ceptual structure. The relevant lexical semantic en-
tries are Containment and Location (see Sect. 4.2).
The transformation is presented schematically be-
low:
contain(ACTOBJECT :A, PATOBJECT :element) ?
CONTAINMENT(containerA , containeeelement)
(ACTOBJECT :element, HasProp-LocOBJECT :B )
? CONTAINMENT(containerB , containeeelement)
Finally, in the domain ontology, we find that the
conceptual relation of Containment, in naive set the-
ory, specializes into the domain relations of ELE-
MENT, SUBSET, STRICT SUBSET. Using the lin-
guistic meaning, the semantic lexicon, and the do-
main ontology, we obtain all the combinations of
interpretations, including the target one paraphrased
below:
?it is not the case that there exist elements e, such that e ? A and e ? B?,
Using translation rules the final interpretations
are translated into first-order logic formulas and
passed on for evaluation to the Proof Manager.
6 Related work
Language understanding in dialog systems, be it
with speech or text interface, is commonly per-
formed using shallow syntactic analysis combined
12In prior discourse, there may have been an assignment
A := ?, where ? is a formula, in which case, A would be known
from discourse context to be of type FORMULA (similarly for
term assignment); by CONST we mean a set or element variable
such as A, x denoting a set A or an element x respectively.
13Irrelevant parts of the meaning representation are omitted;
glosses of the formula are provided.
with keyword spotting. Tutorial systems also suc-
cessfully employ statistical methods which com-
pare student responses to a model built from pre-
constructed gold-standard answers (Graesser et al,
2000). This is impossible for our dialogs, due to
the presence of symbolic mathematical expressions
and because of such aspects of discourse meaning
as causal relations, modality, negation, or scope
of quantifiers which are of crucial importance in
our setting, but of which shallow techniques remain
oblivious (or handle them in a rudimentary way).
When precise understanding is needed, tutorial sys-
tems use closed-questions to elicit short answers of
little syntactic variation (Glass, 2001) or restricted
format of input is allowed. However, this conflicts
with the preference for flexible dialog do achieve
active learning (Moore, 1993).
With regard to interpreting mathematical
texts, (Zinn, 1999) and (Baur, 1999) present DRT
analyzes of course-book proofs. The language in
our dialogs is more informal: natural language and
symbolic mathematical expressions are mixed more
freely, there is a higher degree and more variety
of verbalization, and mathematical objects are not
properly introduced. Both above approaches rely on
typesetting information that identifies mathematical
symbols, formulas, and proof steps, whereas our
input does not contain any such information.
Forcing the user to delimit formulas would not
guarantee a clean separation of the natural language
and the non-linguistic content, while might reduce
the flexibility of the system by making the interface
harder to use.
7 Conclusion and Further Work
In this paper, we reported on the use of deep syn-
tactic and semantic analysis in the interpretation
of mathematical discourse in a dialog setting. We
presented an approach that uses domain-motivated
semantic lexicon to mediate between a domain-
independent representation of linguistic meaning of
utterances and their domain-specific interpretation.
We are incrementally extending the coverage of
the deep analysis components. Our current parser
grammar and upper-level ontology cover most of
the constructions and concepts that occur most fre-
quently in our corpus. The module will be evaluated
as part of the next Wizard-of-Oz experiment.
We are planning to investigate the possibility
of using FrameNet resources developed within the
SALSA project (Erk et al, 2003) at the intermedi-
ate interpretation stage between the linguistic mean-
ing and domain-specific interpretation. Presently,
the semantic lexicon we have constructed encodes,
for instance, a general conceptual relation of CON-
TAINMENT evoked by the verb ?enthalten? (?con-
tain?), with dependents in relations Actor and Pa-
tient, which corresponds to the FrameNet CON-
TAINING domain with frame elements CONTAINER
and CONTENTS. In the course of further work, we
would like to investigate ways of establishing inter-
face between the linguistic meaning TRs and frame
elements, and attempt to use FrameNet to interpret
predicates unknown to our semantic lexicon. Tak-
ing a hypothetical example, if our parser grammar
encoded the meaning of the verb ?beinhalten? (with
the intended meaning contain) in the same linguis-
tic meaning frame as ?enthalten? (contain), while
the sense of ?beinhalten? were not explicitly defined
in the semantic lexicon, we could attempt to inter-
pret it using the FrameNet CONTAINING domain
and the existing lexical semantic entry for ?enthal-
ten?.
References
J. Baldridge. 2002. Lexically Specified Derivational Control
in Combinatory Categorial Grammar. Ph.D. Thesis, Uni-
versity of Edinburgh, Edinburgh.
J. M. Baldridge and G.J. M. Kruijff. 2002. Coupling CCG with
hybrid logic dependency semantics. In Proc. of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia PA.
J. M. Baldridge and G.J. M. Kruijff. 2003. Multi-modal com-
binatory categorial grammar. In Proc. of the 10th Annual
Meeting of the European Chapter of the Association for
Computational Linguistics (EACL?03), Budapest.
J. Baur. 1999. Syntax und Semantik mathematischer Texte.
Diplomarbeit, Fachrichtung Computerlinguistik, Universit a?t
des Saarlandes, Saarbr u?cken, Germany.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003a. Tutorial dialogs on mathematical
proofs. In Proc. of IJCAI?03 Workshop on Knowledge Rep-
resentation and Automated Reasoning for E-Learning Sys-
tems, Acapulco, Mexico.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003b. A Wizard-of-Oz experiment for tu-
torial dialogues in mathematics. In Proc. of the AIED?03
Workshop on Advanced Technologies for Mathematics Edu-
cation, Sydney, Australia.
K. Erk, A. Kowalski, and M. Pinkal. 2003. A corpus re-
source for lexical semantics. In Proc. of the 5th Interna-
tional Workshop on Computational Semantics, Tilburg, The
Netherlands.
A. Fiedler and M. Gabsdil. 2002. Supporting Progressive Re-
finement of Wizard-of-Oz Experiments. In Proc. of the
ITS?02 Workshop on Empirical Methods for Tutorial Dia-
logue, San Sebastian, Spain.
M. Glass. 2001. Processing language input in the CIRCSIM-
Tutor intelligent tutoring system. In Proc. of the 10th Con-
ference on Artificial Intelligence in Education (AIED?01),
San Antonio.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, D. Har-
ter, and N. Person. 2000. Using latent semantic analysis to
evaluate the contributions of students in autotutor. Interac-
tive Learning Environments, 8.
E. Hajic?ov a?, J. Panevov a?, and P. Sgall. 2000. A manual for tec-
togrammatical tagging of the Prague Dependency Treebank.
TR-2000-09, Charles University, Prague, Czech Republic.
H. Horacek and M. Wolska. 2004. Interpreting Semi-Formal
Utterances in Dialogs about Mathematical Proofs. In Proc.
of the 9th International Conference on Application of Nat-
ural Language to Information Systems (NLDB?04), Salford,
Manchester, Springer. To appear.
G.J.M. Kruijff. 2001. A Categorial-Modal Logical Architec-
ture of Informativity: Dependency Grammar Logic & In-
formation Structure. Ph.D. Thesis, Institute of Formal and
Applied Linguistics ( ?UFAL), Faculty of Mathematics and
Physics, Charles University, Prague, Czech Republic.
J. Moore. 1993. What makes human explanations effective?
In Proc. of the 15th Annual Conference of the Cognitive Sci-
ence Society, Hillsdale, NJ.
P. Sgall, E. Hajic?ov a?, and J. Panevov a?. 1986. The meaning of
the sentence in its semantic and pragmatic aspects. Reidel
Publishing Company, Dordrecht, The Netherlands.
Q.B. Vo, C. Benzm u?ller, and S. Autexier. 2003. An approach
to assertion application via generalized resolution. SEKI
Report SR-03-01, Fachrichtung Informatik, Universit a?t des
Saarlandes, Saarbr u?cken, Germany.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Analysis of mixed
natural and symbolic language input in mathematical di-
alogs. In Proc.of the 42nd Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain. To ap-
pear.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Building a
dependency-based grammar for parsing informal mathemat-
ical discourse. In Proc. of the 7th International Conference
on Text, Speech and Dialogue (TSD?04), Brno, Czech Re-
public, Springer. To appear.
M. Wolska, B. Q. Vo, D. Tsovaltzi, I. Kruijff-Korbayov a?,
E. Karagjosova, H. Horacek, M. Gabsdil, A. Fiedler,
C. Benzm u?ller, 2004. An annotated corpus of tutorial di-
alogs on mathematical theorem proving. In Proc. of 4th In-
ternational Conference On Language Resources and Evalu-
ation (LREC?04), Lisbon, Portugal. To appear.
C. Zinn. 1999. Understanding mathematical discourse. In
Proc. of the 3rd Workshop on the Semantics and Pragmat-
ics of Dialogue (Amstelogue?99), Amsterdam, The Nether-
lands.
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 55?65,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Extracting glossary sentences from scholarly articles:
A comparative evaluation of pattern bootstrapping and deep analysis
Melanie Reiplinger1 Ulrich Scha?fer2 Magdalena Wolska1?
1Computational Linguistics, Saarland University, D-66041 Saarbru?cken, Germany
2DFKI Language Technology Lab, Campus D 3 1, D-66123 Saarbru?cken, Germany
{mreiplin,magda}@coli.uni-saarland.de, ulrich.schaefer@dfki.de
Abstract
The paper reports on a comparative study of
two approaches to extracting definitional sen-
tences from a corpus of scholarly discourse:
one based on bootstrapping lexico-syntactic
patterns and another based on deep analysis.
Computational Linguistics was used as the tar-
get domain and the ACL Anthology as the
corpus. Definitional sentences extracted for a
set of well-defined concepts were rated by do-
main experts. Results show that both meth-
ods extract high-quality definition sentences
intended for automated glossary construction.
1 Introduction
Specialized glossaries serve two functions: Firstly,
they are linguistic resources summarizing the ter-
minological basis of a specialized domain. Sec-
ondly, they are knowledge resources, in that they
provide definitions of concepts which the terms de-
note. Glossaries find obvious use as sources of ref-
erence. A survey on the use of lexicographical aids
in specialized translation showed that glossaries are
among the top five resources used (Dura?n-Mun?oz,
2010). Glossaries have also been shown to facil-
itate reception of texts and acquisition of knowl-
edge during study (Weiten et al, 1999), while self-
explanation of reasoning by referring to definitions
has been shown to promote understanding (Aleven
et al, 1999). From a machine-processing point of
view, glossaries may be used as input for domain
ontology induction; see, e.g. (Bozzato et al, 2008).
?Corresponding author
The process of glossary creation is inherently de-
pendent on expert knowledge of the given domain,
its concepts and language. In case of scientific do-
mains, which constantly evolve, glossaries need to
be regularly maintained: updated and continually
extended. Manual creation of specialized glossaries
is therefore costly. As an alternative, fully- and
semi-automatic methods of glossary creation have
been proposed (see Section 2).
This paper compares two approaches to corpus-
based extraction of definitional sentences intended
to serve as input for a specialized glossary of a scien-
tific domain. The bootstrapping approach acquires
lexico-syntactic patterns characteristic of definitions
from a corpus of scholarly discourse. The deep ap-
proach uses syntactic and semantic processing to
build structured representations of sentences based
on which ?is-a?-type definitions are extracted. In
the present study we used Computational Linguis-
tics (CL) as the target domain of interest and the
ACL Anthology as the corpus.
Computational Linguistics, as a specialized do-
main, is rich in technical terminology. As a cross-
disciplinary domain at the intersection of linguistics,
computer science, artificial intelligence, and mathe-
matics, it is interesting as far as glossary creation
is concerned in that its scholarly discourse ranges
from descriptive informal to formal, including math-
ematical notation. Consider the following two de-
scriptions of Probabilistic Context-Free Grammar
(PCFG):
(1) A PCFG is a CFG in which each production
A ? ? in the grammar?s set of productions
R is associated with an emission probabil-
55
ity P (A ? ?) that satisfies a normalization
constraint
?
?:A???R
P (A? ?) = 1
and a consistency or tightness constraint [...]
(2) A PCFG defines the probability of a string
of words as the sum of the probabilities of
all admissible phrase structure parses (trees)
for that string.
While (1) is an example of a genus-differentia
definition, (2) is a valid description of PCFG which
neither has the typical copula structure of an ?is-a?-
type definition, nor does it contain the level of de-
tail of the former. (2) is, however, well-usable for a
glossary. The bootstrapping approach extracts defi-
nitions of both types. Thus, at the subsequent glos-
sary creation stage, alternative entries can be used to
generate glossaries of different granularity and for-
mal detail; e.g., targeting different user groups.
Outline. Section 2 gives an overview of related
work. Section 3 presents the corpora and the prepro-
cessing steps. The bootstrapping procedure is sum-
marized in Section 4 and deep analysis in Section 5.
Section 6 presents the evaluation methodology and
the results. Section 7 presents an outlook.
2 Related Work
Most of the existing definition extraction methods
? be it targeting definitional question answering or
glossary creation ? are based on mining part-of-
speech (POS) and/or lexical patterns typical of def-
initional contexts. Patterns are then filtered heuris-
tically or using machine learning based on features
which refer to the contexts? syntax, lexical content,
punctuation, layout, position in discourse, etc.
DEFINDER (Muresan and Klavans, 2002), a rule-
based system, mines definitions from online medical
articles in lay language by extracting sentences us-
ing cue-phrases, such as ?x is the term for y?, ?x
is defined as y?, and punctuation, e.g., hyphens and
brackets. The results are analyzed with a statistical
parser. Fahmi and Bouma (2006) train supervised
learners to classify concept definitions from medi-
cal pages of the Dutch Wikipedia using the ?is a?
pattern and apply a lexical filter (stopwords) to the
classifier?s output. Besides other features, the posi-
tion of a sentence within a document is used, which,
due to the encyclopaedic text character of the cor-
pus, allows to set the baseline precision at above
75% by classifying the first sentences as definitions.
Westerhout and Monachesi (2008) use a complex set
of grammar rules over POS, syntactic chunks, and
entire definitory contexts to extract definition sen-
tences from an eLearning corpus. Machine learn-
ing is used to filter out incorrect candidates. Gaudio
and Branco (2009) use only POS information in a
supervised-learning approach, pointing out that lex-
ical and syntactic features are domain and language
dependent. Borg et al (2009) use genetic program-
ming to learn rules for typical linguistic forms of
definition sentences in an eLearning corpus and ge-
netic algorithms to assign weights to the rules. Ve-
lardi et al (2008) present a fully-automatic end-to-
end methodology of glossary creation. First, Term-
Extractor acquires domain terminology and Gloss-
Extractor searches for definitions on the web using
google queries constructed from a set of manually
lexical definitional patterns. Then, the search results
are filtered using POS and chunk information as well
as term distribution properties of the domain of in-
terest. Filtered results are presented to humans for
manual validation upon which the system updates
the glossary. The entire process is automated.
Bootstrapping as a method of linguistic pattern
induction was used for learning hyponymy/is-a re-
lations already in the early 90s by Hearst (1992).
Various variants of the procedure ? for instance, ex-
ploiting POS information, double pattern-anchors,
semantic information ? have been recently pro-
posed (Etzioni et al, 2005; Pantel and Pennacchiotti,
2006; Girju et al, 2006; Walter, 2008; Kozareva et
al., 2008; Wolska et al, 2011). The method pre-
sented here is most similar to Hearst?s, however, we
acquire a large set of general patterns over POS tags
alone which we subsequently optimize on a small
manually annotated corpus subset by lexicalizing the
verb classes.
3 The Corpora and Preprocesssing
The corpora. Three corpora were used in this
study. At the initial stage two development corpora
were used: a digitalized early draft of the Jurafsky-
56
Martin textbook (Jurafsky and Martin, 2000) and the
WeScience Corpus, a set of Wikipedia articles in the
domain of Natural Language Processing (Ytrest?l et
al., 2009).1 The former served as a source of seed
domain terms with definitions, while the latter was
used for seed pattern creation.
For acquisition of definitional patterns and pat-
tern refinement we used the ACL Anthology, a dig-
ital archive of scientific papers from conferences,
workshops, and journals on Computational Linguis-
tics and Language Technology (Bird et al, 2008).2
The corpus consisted of 18,653 papers published be-
tween 1965 and 2011, with a total of 66,789,624
tokens and 3,288,073 sentences. This corpus was
also used to extract sentences for the evaluation us-
ing both extraction methods.
Preprocessing. The corpora have been sentence
and word-tokenized using regular expression-based
sentence boundary detection and tokenization tools.
Sentences have been part-of-speech tagged using the
TnT tagger (Brants, 2000) trained on the Penn Tree-
bank (Marcus et al, 1993).3
Next, domain terms were identified using the C-
Value approach (Frantzi et al, 1998). C-Value is
a domain-independent method of automatic multi-
word term recognition that rewards high frequency
and high-order n-gram candidates, but penalizes
those which frequently occur as sub-strings of an-
other candidate. 10,000 top-ranking multi-word to-
ken sequences, according to C-Value, were used.
Domain terms. The set of domain terms was com-
piled from the following sub-sets: 1) the 10,000 au-
tomatically identified multi-word terms, 2) the set
of terms appearing on the margins of the Jurafsky-
Martin textbook; the intuition being that these are
domain-specific terms which are likely to be defined
or explained in the text along which they appear,
3) a set of 5,000 terms obtained by expanding fre-
quent abbreviations and acronyms retrieved from the
ACL Anthology corpus using simple pattern match-
ing. The token spans of domain terms have been
marked in the corpora as these are used in the course
of definition pattern acquisition (Section 4.2).
1http://moin.delph-in.net/WeScience
2http://aclweb.org/anthology/
3The accuracy of tokenization and tagging was not verified.
S
ee
d
te
rm
s
machine translation language model
neural network reference resolution
finite(-| )state automaton hidden markov model
speech synthesis semantic role label(l)?ing
context(-| )free grammar ontology
generative grammar dynamic programming
mutual information
S
ee
d
pa
tt
er
ns
T .* (is|are|can be) used
T .* called
T .* (is|are) composed
T .* involv(es|ed|e|ing)
T .* perform(s|ed|ing)?
T \( or .*? \)
task of .* T .*? is
term T .*? refer(s|red|ring)?
Table 1: Seed domain terms (top) and seed patterns (bot-
tom) used for bootstrapping; T stands for a domain term.
4 Bootstrapping Definition Patterns
Bootstrapping-based extraction of definitional sen-
tences proceeds in two stages: First, aiming at recall,
a large set of lexico-syntactic patterns is acquired:
Starting with a small set of seed terms and patterns,
term and pattern ?pools? are iteratively augmented
by searching for matching sentences from the ACL
Anthology while acquiring candidates for definition
terms and patterns. Second, aiming at precision,
general patterns acquired at the first stage are sys-
tematically optimized on set of annotated extracted
definitions.
4.1 Seed Terms and Seed Patterns
As seed terms to initialize pattern acquisition, we
chose terms which are likely to have definitions.
Specifically, from the top-ranked multi-word terms,
ordered by C-value, we selected those which were
also in either the Jurafsky-Martin term list or the list
of expanded frequent abbreviations. The resulting
13 seed terms are shown in the top part of Table 1.
Seed definition patterns were created by inspect-
ing definitional contexts in the Jurafsky-Martin and
WeScience corpora. First, 12 terms from Jurafsky-
Martin and WeScience were selected to find domain
terms with which they co-occurred in simple ?is-a?
patterns. Next, the corpora were searched again to
find sentences in which the term pairs in ?is-a? rela-
tion occur. Non-definition sentences were discarded.
57
Finally, based on the resulting definition sentences,
22 seed patterns were constructed by transforming
the definition phrasings into regular expressions. A
subset of the seed phrases extracted in this way is
shown in the bottom part of Table 1.4
4.2 Acquiring Patterns
Pattern acquisition proceeds in two stages: First,
based on seed sets, candidate defining terms are
found and ranked. Then, new patterns are acquired
by instantiating existing patterns with pairs of likely
co-occurring domain terms, searching for sentences
in which the term pairs co-occur, and creating POS-
based patterns. These steps are summarized below.
Finding definiens candidates. Starting with a set
of seed terms and a set of definition phrases, the first
stage finds sentences with the seed terms in the T-
placeholder position of the seed phrases. For each
term, the set of extracted sentences is searched for
candidate defining terms (other domain terms in the
sentence) to form term-term pairs which, at the sec-
ond stage, will be used to acquire new patterns.
Two situations can occur: a sentence may con-
tain more than one domain term (other than one of
the seed terms) or the same domain terms may be
found to co-occur with multiple seed terms. There-
fore, term-term pairs are ranked.
Ranking. Term-term pairs are ranked using four
standard measures of association strength: 1) co-
occurrence count, 2) pointwise mutual information
(PMI), 3) refined PMI; compensates bias toward
low-frequency events by multiplying PMI with co-
occurrence count (Manning and Schu?tze, 1999), and
4) mutual dependency (MD); compensates bias to-
ward rare events by subtracting co-occurrence?s self-
information (entropy) from its PMI (Thanopoulos et
al., 2002). The measures are calculated based on the
corpus for co-occurrences within a 15-word window.
Based on experimentation, mutual dependency
was found to produce the best results and therefore it
was used in ranking definiens candidates in the final
evaluation of patterns. The top-k candidates make
up the set of defining terms to be used in the pattern
acquisition stage. Table 2 shows the top-20 candi-
4Here and further in the paper, regular expressions are pre-
sented in Perl notation.
Domain term Candidate defining terms
lexical functional phrase structure grammar
grammar (LFG) formal system
functional unification grammar
grammatical representation
phrase structure
generalized phrase
functional unification
binding theory
syntactic theories
functional structure
grammar formalism(s)
grammars
linguistic theor(y|ies)
Table 2: Candidate defining phrases of the term ?Lexical
Functional Grammar (LFG)?.
date defining terms for the term ?Lexical Functional
Grammar?, according to mutual dependency.
Pattern and domain term acquisition. At the
pattern acquisition stage, definition patterns are re-
trieved by 1) coupling terms with their definiens can-
didates, 2) extracting sentences that contain the pair
within the specified window of words, and finally
3) creating POS-based patterns corresponding to the
extracted sentences. All co-occurrences of each
term together with each of its defining terms within
the fixed window size are extracted from the POS-
tagged corpus. At each iteration also new definien-
dum and definiens terms are found by applying the
new abstracted patterns to the corpus and retrieving
the matching domain terms.
The newly extracted sentences and terms are fil-
tered (see ?Filtering? below). The remaining data
constitute new instances for further iterations. The
linguistic material between the two terms in the ex-
tracted sentences is taken to be an instantiation of a
potential definition pattern for which its POS pattern
is created as follows:
? The defined and defining terms are replaced by
placeholders, T and DEF,
? All the material outside the T and DEF anchors
is removed; i.e. the resulting patterns have the
form ?T ... DEF? or ?DEF ... T?
? Assuming that the fundamental elements of a
definition pattern, are verbs and noun phrases,
58
all tags except verb, noun, modal and the in-
finitive marker ?to? are replaced with by place-
holders denoting any string; punctuation is pre-
served, as it has been observed to be infor-
mative in detecting definitions (Westerhout and
Monachesi, 2008; Fahmi and Bouma, 2006),
? Sequences of singular and plural nouns and
proper nouns are replaced with noun phrase
placeholder, NP; it is expanded to match com-
plex noun phrases when applying the patterns
to extract definition sentences.
The new patterns and terms are then fed as input
to the acquisition process to extract more sentences
and again abstract new patterns.
Filtering. In the course of pattern acquisition in-
formation on term-pattern co-occurrence frequen-
cies is stored and relative frequencies are calculated:
1) for each term, the percentage of seed patterns it
occurs with, and 2) for each pattern, the percentage
of seed terms it occurs with. These are used in the
bootstrapping cycles to filter out terms which do not
occur as part of a sufficient number of patterns (pos-
sibly false positive definiendum candidates) and pat-
terns which do not occur with sufficient number of
terms (insufficient generalizing behavior).
Moreover, the following filtering rules are ap-
plied: Abstracted POS-pattern sequences of the
form ?T .+ DEF?5 and ?DEF T? are discarded;
the former because it is not informative, the latter
because it is rather an indicator of compound nouns
than of definitions. From the extracted sentences,
those containing negation are filtered out; negation
is contra-indicative of definition (Pearson, 1996).
For the same reason, auxiliary constructions with
?do? and ?have? are excluded unless, in case of the
latter, ?have? is followed by a two past participle
tags as in, e.g., ?has been/VBN defined/VBN (as).?
4.3 Manual Refinement
While the goal of the bootstrapping stage was to find
as many candidate patterns for good definition terms
as possible, the purpose of the refinement stage is to
aim at precision. Since the automatically extracted
patterns consist only of verb and noun phrase tags
5?.+? stands for at least one arbitrary character.
# Definitions # Non-definitions
25 is/VBZ 24 is/VBZ
8 represents/VBZ 14 contains/VBZ
6 provides/VBZ 9 employed/VBD
6 contains/VBZ 6 includes/VBZ
6 consists/VBZ 4 reflects/VBZ
3 serves/VBZ 3 uses/VBZ
3 describes/VBZ 3 typed/VBN
3 constitutes/VBZ 3 provides/VBZ
3 are/VBP 3 learning/VBG
Table 3: Subset of verbs occurring in sentences matched
by the most frequently extracted patterns.
between the definiendum and its defining term an-
chors, they are too general.
In order to create more precise patterns, we tuned
the pattern sequences based on a small development
sub-corpus of the extracted sentences which we an-
notated. The development corpus was created by ex-
tracting sentences using the most frequent patterns
instantiated with the term which occurred with the
highest percentage of seed patterns. The term ?on-
tology? appeared with more than 80% of the patterns
and was used for this purpose. The sentences were
then manually annotated as to whether they are true-
positive or false examples of definitions (101 and
163 sentences, respectively).
Pattern tuning was done by investigating which
verbs are and which are not indicative of defini-
tions based on the positive and negative example
sentences. Table 3 shows the frequency distribu-
tion of verbs (or verb sequences) in the annotated
corpus which occurred more than twice. Abstract-
ing over POS sequences of the sentences contain-
ing definition-indicative verbs, we created 13 pat-
terns, extending the automatically found patterns,
that yielded 65% precision on the development set,
matching 51% of the definition sentences, and re-
ducing noise to 17% non-definitions. Patterns re-
sulting from verb tuning were used in the evaluation.
Examples of the tuned patterns are shown below:
T VBZ DT JJ? NP .* DEF
T , NP VBZ IN NP .* DEF
T , .+ VBZ DT .+ NP .* DEF
T VBZ DT JJ? NP .* DEF
The first pattern matches our both introductory
59
example definitions of the term ?PCFG? (cf. Sec-
tion 1) with ?T? as a placeholder for the term it-
self, ?NP? denoting a noun phrase, and ?DEF? one
of the term?s defining phrases, in the first case, (1),
?grammar?, in the second case, (2), ?probabilities?.
The examples annotated with matched pattern ele-
ments are shown below:6
[PCFG]T [is]VBZ [a]DT [CFG]NP [in which each
production A ? ? in the].? [grammar]DEF ?s
set of productionsR is associated with an emis-
sion probability . . .
A [PCFG]T [defines]VBZ [the]DT
[probability]DEF of a string of words as
the sum of the probabilities of all admissible
phrase structure parses (trees) for that string.
5 Deep Analysis for Definition Extraction
An alternative, largely domain-independent ap-
proach to the extraction of definition sentences is
based on the sentence-semantic index generation of
the ACL Anthology Searchbench (Scha?fer et al,
2011).
Deep syntactic parsing with semantic predicate-
argument structure extraction of each of the approx.
3.3 million sentences in the 18,653 papers ACL An-
thology corpus is used for our experiments. We
briefly describe how in this approach we get from
the sentence text to the semantic representation.
The preprocessing is shared with the
bootstrapping-based approach for definition
sentence extraction, namely PDF-to-text extraction,
sentence boundary detection (SBR), and trigram-
based POS tagging with TnT (Brants, 2000). The
tagger output is combined with information from
a named entity recognizer that in addition delivers
hypothetical information on citation expressions.
The combined result is delivered as input to the
deep parser PET (Callmeier, 2000) running the open
source HPSG grammar (Pollard and Sag, 1994)
grammar for English (ERG; Flickinger (2002)).
The deep parser is made robust and fast through
a careful combination of several techniques; e.g.:
(1) chart pruning: directed search during parsing to
6Matching pattern elements in square brackets; tags from
the pattern subscripted.
increase performance and coverage for longer sen-
tences (Cramer and Zhang, 2010); (2) chart map-
ping: a framework for integrating preprocessing in-
formation from PoS tagger and named entity recog-
nizer in exactly the way the deep grammar expects it
(Adolphs et al, 2008)7; (3) a statistical parse rank-
ing model (WeScience; (Flickinger et al, 2010)).
The parser outputs sentence-semantic represen-
tation in the MRS format (Copestake et al, 2005)
that is transformed into a dependency-like vari-
ant (Copestake, 2009). From these DMRS represen-
tations, predicate-argument structures are derived.
These are indexed with structure (semantic subject,
predicate, direct object, indirect object, adjuncts) us-
ing a customized Apache Solr8 server. Matching
of arguments is left to Solr?s standard analyzer for
English with stemming; exact matches are ranked
higher than partial matches.
The basic semantics extraction algorithm consists
of the following steps: 1) calculate the closure for
each (D)MRS elementary predication based on the
EQ (variable equivalence) relation and group the
predicates and entities in each closure respectively;
2) extract the relations of the groups, which results in
a graph as a whole; 3) recursively traverse the graph,
form one semantic tuple for each predicate, and fill
information under its scope, i.e. subject, object, etc.
The semantic structure extraction algorithm gen-
erates multiple predicate-argument structures for
coordinated sentence (sub-)structures in the in-
dex. Moreover, explicit negation is recognized and
negated sentences are excluded for the task for the
same reasons as in the bootstrapping approach above
(see Section 4.2, ?Filtering?).
Further details of the deep parsing approach are
described in (Scha?fer and Kiefer, 2011). In the
Searchbench online system9, the definition extrac-
tion can by tested with any domain term T by using
statement queries of the form ?s:T p:is?.
6 Evaluation
For evaluation, we selected 20 terms, shown in Ta-
ble 4, which can be considered domain terms in the
7PoS tagging, e.g., helps the deep parser to cope with words
unknown to the deep lexicon, for which default entries based on
the PoS information are generated on the fly.
8http://lucene.apache.org/solr
9http://aclasb.dfki.de
60
integer linear programming (ILP)
conditional random field (CRF)
support vector machine (SVM)
latent semantic analysis (LSA)
combinatory categorial grammar (CCG)
lexical-functional grammar (LFG)
probabilistic context-free grammar (PCFG)
discourse representation theory (DRT)
discourse representation structure (DRS)
phrase-based machine translation (PSMT;PBSMT)
statistical machine translation (SMT)
multi-document summarization (MDS)
word sense disambiguation (WSD)
semantic role labeling (SRL)
coreference resolution
conditional entropy
cosine similarity
mutual information (MI)
default unification (DU)
computational linguistics (CL)
Table 4: Domain-terms used in the rating experiment
domain of computational linguistics. Five general
terms, such as ?English text? or ?web page?, were
also included in the evaluation as a control sample;
since general terms of this kind are not likely to be
defined in scientific papers in CL, their definition
sentences were of low quality (false positives). We
do not include them in the summary of the evalua-
tion results for space reasons. ?Computational lin-
guistics?, while certainly a domain term in the do-
main, is not likely to be defined in the articles in the
ACL Anthology, however, the term as such should
rather be included in a glossary of computational lin-
guistics, therefore, we included it in the evaluation.
Due to the lack of a gold-standard glossary defi-
nitions in the domain, we performed a rating exper-
iment in which we asked domain experts to judge
top-ranked definitional sentences extracted using the
two approaches. Below we briefly outline the evalu-
ation setup and the procedure.
6.1 Evaluation Data
A set of definitional sentences for the 20 domain
terms was extracted as follows:
Lexico-syntactic patterns (LSP). For the lexico-
syntactic patterns approach, sentences extracted by
the set of refined patterns (see Section 4.3) were
considered for evaluation only if they contained at
least one of the term?s potential defining phrases as
identified by the first stage of the glossary extraction
(Section 4.2). Acronyms were allowed as fillers of
the domain term placeholders.
The candidate evaluation sentences were ranked
using single linkage clustering in order to find sub-
sets of similar sentences. tf.idf-based cosine be-
tween vectors of lemmatized words was used as a
similarity function. As in (Shen et al, 2006), the
longest sentence was chosen from each of the clus-
ters. Results were ranked by considering the size of
the clusters as a measure of how likely it represents
a definition. The larger the cluster, the higher it was
ranked. Five top-ranked sentences for each of the 20
terms were used for the evaluation.
Deep analysis (DA). The only pattern used for
deep analysis extraction was ?subject:T predi-
cate:is?, with ?is? restricted by the HPSG grammar
to be the copula relation and not an auxiliary such as
in passive constructions, etc. Five top-ranked sen-
tences ? as per the Solr?s matching algorithm ? ex-
tracted with this pattern were used for the evaluation.
In total, 200 candidate definition sentences for
20 domain terms were evaluated, 100 per extraction
methods. Examples of candidate glossary sentences
extracted using both methods, along with their rat-
ings, are shown in the appendix.
6.2 Evaluation Method
Candidate definition sentences were presented to 6
human domain experts by a web interface display-
ing one sentence at a time in random order. Judges
were asked to rate sentences on a 5-point ordinal
scale with the following descriptors:10
5: The passage provides a precise and concise de-
scription of the concept
4: The passage provides a good description of the
concept
3: The passage provides useful information about
the concept, which could enhance a definition
10Example definitions at each scale point selected by the au-
thors were shown for the concept ?hidden markov model?.
61
DALSP
100,0%
80,0%
60,0%
40,0%
20,0%
0,0%
21,6727,33
18,1715,33
32,5026,17
16,3316,00
11,3315,17
1
2
3
4
5
Rating
Figure 1: Distribution of ratings across the 5 scale points;
LSP: lexico-syntactic patterns, DA: deep analysis
2: The passage is not a good enough description
of the concept to serve as a definition; for in-
stance, it?s too general, unfocused, or a subcon-
cept/superconcept of the target concept is de-
fined instead
1: The passage does not describe the concept at all
The judges participating in the rating experiment
were PhD students, postdoctoral researchers, or re-
searchers of comparable expertise, active in the ar-
eas of computational linguistics/natural language
processing/language technology. One of the raters
was one of the authors of this paper. The raters were
explicitly instructed to think along the lines of ?what
they would like to see in a glossary of computational
linguistics terms?.
6.3 Results
Figure 1 shows the distribution of ratings across
the five scale points for the two systems. Around
57% of the LSP ratings and 60% of DA ratings fall
within the top three scale-points (positive ratings)
and 43% and 40%, respectively, within the bottom
two scale-points (low ratings). Krippendorff?s or-
dinal ? (Hayes and Krippendorff, 2007) was 0.66
(1,000 bootstrapped samples) indicating a modest
degree of agreement, at which, however, tentative
conclusions can be drawn.
ILP
CRF
SVM
LSA
CCG
LFG
PCFG
DRT
DRS
PSMT;PBSMT
SMT
MDS
WSD
SRL
coref. resolution
cond. entropy
cos similarity
MI
DU
CL
Mode ratings
54321
DALSP
Method
Figure 2: Mode values of ratings per method for the indi-
vidual domain terms; see Table 4
Figure 2 shows the distribution of mode ratings
of the individual domain terms used in the evalua-
tion. Definitions of 6 terms extracted using the LSP
method were rated most frequently at 4 or 5 as op-
posed to the majority of ratings at 3 for most terms
in case of the DA method.
A Wilcoxon signed-rank test was conducted to
evaluate whether domain experts favored defini-
tional sentences extracted by one the two methods.11
The results indicated no significant difference be-
tween ratings of definitions extracted using LSP and
DA (Z = 0.43, p = 0.68).
Now, considering that the ultimate purpose of the
sentence extraction is glossary creation, we were
also interested in how the top-ranked sentences were
rated; that is, assuming we were to create a glossary
using only the highest ranked sentences (according
to the methods? ranking schemes; see Section 6.1)
we wanted to know whether one of the methods pro-
poses rank-1 candidates with higher ratings, inde-
pendently of the magnitude of the difference. A sign
test indicated no statistical difference in ratings of
the rank-1 candidates between the two methods.
11Definition sentences for each domain term were paired by
their rank assigned by the extraction methods: rank-1 DA sen-
tence with rank-1 LSP, etc.; see Section 6.1.
62
7 Conclusions and Future Work
The results show that both methods have the poten-
tial of extracting good quality glossary sentences:
the majority of the extracted sentences provide at
least useful information about the domain concepts.
However, both methods need improvement.
The rating experiment suggests that the concept of
definition quality in a specialized domain is largely
subjective (borderline acceptable agreement overall
and ? = 0.65 for rank-1 sentences). This calls for
a modification of the evaluation methodology and
for additional tests of consistency of ratings. The
low agreement might be remedied by introducing
a blocked design in which groups of judges would
evaluate definitions of a small set of concepts with
which they are most familiar, rather than a large set
of concepts from various CL sub-areas.
An analysis of the extracted sentences and their
ratings12 revealed that deep analysis reduces noise in
sentence extraction. Bootstrapping, however, yields
more candidate sentences with good or very good
ratings. While in the present work pattern refine-
ment was based only on verbs, we observed that also
the presence and position of (wh-)determiners and
prepositions might be informative. Further exper-
iments are needed 1) to find out how much speci-
ficity can be allowed without blocking the patterns?
productivity and 2) to exploit the complementary
strengths of the methods by combining them.
Since both approaches use generic linguistic re-
sources and preprocessing (POS-tagging, named-
entity extraction, etc.) they can be considered
domain-independent. To our knowledge, this is,
however, the first work that attempts to identify
definitions of Computational Linguistics concepts.
Thus, it contributes to evaluating pattern bootstrap-
ping and deep analysis in the context of the defini-
tion extraction task in our own domain.
Acknowledgments
The C-Value algorithm was implemented by Mi-
hai Grigore. We are indebted to our colleagues
from the Computational Linguistics department and
DFKI in Saarbru?cken who kindly agreed to partic-
ipate in the rating experiment as domain experts.
12Not included in this paper for space reasons
We are also grateful to the reviewers for their feed-
back. The work described in this paper has been
partially funded by the German Federal Ministry
of Education and Research, projects TAKE (FKZ
01IW08003) and Deependance (FKZ 01IW11003).
References
P. Adolphs, S. Oepen, U. Callmeier, B. Crysmann,
D. Flickinger, and B. Kiefer. 2008. Some Fine Points
of Hybrid Natural Language Parsing. In Proceedings
of the 6th LREC, pages 1380?1387.
V. Aleven, K. R. Koedinger, and K. Cross. 1999. Tutor-
ing Answer Explanation Fosters Learning with Under-
standing. In Artificial Intelligence in Education, pages
199?206. IOS Press.
S. Bird, R. Dale, B. Dorr, B. Gibson, M. Joseph, M.-
Y. Kan, D. Lee, B. Powley, D. Radev, and Y. F. Tan.
2008. The ACL Anthology Reference Corpus: A Ref-
erence Dataset for Bibliographic Research in Compu-
tational Linguistics. In Proceedings of the 6th LREC,
pages 1755?1759.
C. Borg, M. Rosner, and G. Pace. 2009. Evolutionary
Algorithms for Definition Extraction. In Proceedings
of the 1st Workshop on Definition Extraction, pages
26?32.
L. Bozzato, M. Ferrari, and A. Trombetta. 2008. Build-
ing a Domain Ontology from Glossaries: A General
Methodology. In Proceedings of the 5th Workshop on
Semantic Web Applications and Perspectives, pages 1?
10.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of ANLP, pages 224?231.
U. Callmeier. 2000. PET ? A Platform for Experimenta-
tion with Efficient HPSG Processing Techniques. Nat-
ural Language Engineering, 6(1):99?108.
A. Copestake, D. Flickinger, I. A. Sag, and C. Pollard.
2005. Minimal Recursion Semantics: an Introduction.
Research on Language and Computation, 3(2?3):281?
332.
A. Copestake. 2009. Slacker semantics: why superficial-
ity, dependency and avoidance of commitment can be
the right way to go. In Proceedings of the 12th EACL
Conference, pages 1?9.
B. Cramer and Y. Zhang. 2010. Constraining robust
constructions for broad-coverage parsing with preci-
sion grammars. In Proceedings of the 23rd COLING
Conference, pages 223?231.
I. Dura?n-Mun?oz, 2010. eLexicography in the 21st cen-
tury: New challenges, new applications, volume 7,
chapter Specialised lexicographical resources: a sur-
vey of translators? needs, pages 55?66. Presses Uni-
versitaires de Louvain.
63
O. Etzioni, M. Cafarella, D. Downey, A-M. Popescu,
T. Shaked, S. Soderland, D.S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165:91?134.
I. Fahmi and G. Bouma. 2006. Learning to identify defi-
nitions using syntactic features. In Proceedings of the
EACL Workshop on Learning Structured Information
in Natural Language Applications, pages 64?71.
D. Flickinger, S. Oepen, and G. Ytrest?l. 2010. Wiki-
Woods: Syntacto-semantic annotation for English
Wikipedia. In Proceedings of the 7th LREC, pages
1665?1671.
D. Flickinger. 2002. On building a more efficient gram-
mar by exploiting types. In Collaborative Language
Engineering. A Case Study in Efficient Grammar-
based Processing, pages 1?17. CSLI Publications,
Stanford, CA.
K. Frantzi, S. Ananiadou, and H. Mima. 1998. Au-
tomatic recognition of multi-word terms: the C-
value/NC-value method. In Proceedings of the 2nd
European Conference on Research and Advanced
Technology for Digital Libraries, pages 585?604.
R. Del Gaudio and A. Branco. 2009. Language inde-
pendent system for definition extraction: First results
using learning algorithms. In Proceedings of the 1st
Workshop on Definition Extraction, pages 33?39.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1):83?135.
A. F. Hayes and K. Krippendorff. 2007. Answering the
call for a standard reliability measure for coding data.
Communication Methods and Measures, 1(1):77?89.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
COLING Conference, pages 539?545.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics and
Speech Recognition. Prentice Hall Series in Artificial
Intelligence. 2nd Ed. Online draft (June 25, 2007).
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic Class Learning from the Web with Hyponym Pat-
tern Linkage Graphs. In Proceedings of the 46th ACL
Meeting, pages 1048?1056.
C. D. Manning and H. Schu?tze. 1999. Foundations of
statistical natural language processing. MIT Press,
Cambridge, MA, USA.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish. The Penn Treebank. Computational Linguistics,
19:313?330.
S. Muresan and J. Klavans. 2002. A method for automat-
ically building and evaluating dictionary resources. In
Proceedings of the 3rd LREC, pages 231?234.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging Generic Patterns for Automatically Harvesting
Semantic Relations. In Proceedings of the 21st COL-
ING and the 44th ACL Meeting, pages 113?120.
J. Pearson. 1996. The expression of definitions in spe-
cialised texts: A corpus-based analysis. In Proceed-
ings of Euralex-96, pages 817?824.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago.
U. Scha?fer and B. Kiefer. 2011. Advances in deep
parsing of scholarly paper content. In R. Bernardi,
S. Chambers, B. Gottfried, F. Segond, and I. Za-
ihrayeu, editors, Advanced Language Technologies for
Digital Libraries, number 6699 in LNCS, pages 135?
153. Springer.
U. Scha?fer, B. Kiefer, C. Spurk, J. Steffen, and R. Wang.
2011. The ACL Anthology Searchbench. In Proceed-
ings of ACL-HLT 2011, System Demonstrations, pages
7?13, Portland, Oregon, June.
Y. Shen, G. Zaccak, B. Katz, Y. Luo, and O. Uzuner.
2006. Duplicate Removal for Candidate Answer Sen-
tences. In Proceedings of the 1st CSAIL Student Work-
shop.
A. Thanopoulos, N. Fakotakis, and G. Kokkinakis. 2002.
Comparative evaluation of collocation extraction met-
rics. In Proceedings of the 3rd Language Resources
Evaluation Conference, pages 620?625.
P. Velardi, R. Navigli, and P. D?Amadio. 2008. Mining
the Web to Create Specialized Glossaries. IEEE Intel-
ligent Systems, pages 18?25.
S. Walter. 2008. Linguistic description and automatic
extraction of definitions from german court decisions.
In Proceedings of the 6th LREC, pages 2926?2932.
W. Weiten, D. Deguara, E. Rehmke, and L. Sewell.
1999. University, Community College, and High
School Students? Evaluations of Textbook Pedagogi-
cal Aids. Teaching of Psychology, 26(1):19?21.
E. Westerhout and P. Monachesi. 2008. Creating glos-
saries using pattern-based and machine learning tech-
niques. In Proceedings of the 6th LREC, pages 3074?
3081.
M. Wolska, U. Scha?fer, and The Nghia Pham. 2011.
Bootstrapping a domain-specific terminological taxon-
omy from scientific text. In Proceedings of the 9th In-
ternational Conference on Terminology and Artificial
Intelligence (TIA-11), pages 17?23. INALCO, Paris.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Extract-
ing and annotating wikipedia sub-domains. In Pro-
ceedings of the 7th Workshop on Treebanks and Lin-
guistic Theories, pages 185?197.
64
Appendix
Rated glossary sentences for ?word sense disambiguation (WSD)? and ?mutual information (MI)?. As shown
in Figure 2, for WSD, mode ratings of LSP sentences were higher, while for MI it was the other way round.
word sense disambiguation (WSD)
mode ratings of LSP sentences:
WSD is the task of determining the sense of a polysemous word within a specific context (Wang et al, 2006). 5
Word sense disambiguation or WSD, the task of identifying the correct sense of a word in context, is a central problem
for all natural language processing applications, and in particular machine translation: different senses of a word translate
differently in other languages, and resolving sense ambiguity is needed to identify the right translation of a word.
4
Unlike previous applications of co-training and self-training to natural languagelearning, where one general classifier is
build to cover the entire problem space, supervised word sense disambiguation implies a different classifier for each in-
dividual word, resulting eventually in thousands of different classifiers, each with its own characteristics (learning rate,
sensitivity to new examples, etc.).
3
NER identifies different kinds of names such as ?person?, ?location? or ?date?, while WSD distinguishes the senses of
ambiguous words.
3
This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian
classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context.
1
DA sentences:
Word Sense Disambiguation (WSD) is the task of formalizing the intended meaning of a word in context by selecting an
appropriate sense from a computational lexicon in an automatic manner.
5
Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs. {4,5}
Word sense disambiguation (WSD) is a difficult problem in natural language processing. 2
word sense disambiguation, Hownet, sememe, co-occurrence Word sense disambiguation (WSD) is one of the most difficult
problems in NLP.
{1,2}
There is a general concern within the field of word sense disambiguation about the inter-annotator agreement between
human annotators.
1
mutual information (MI)
mode ratings of LSP sentences:
According to Fano (1961), if two points (words), x and y, have probabilities P (x) and P (y), then their mutual information,
I(x, y), is defined to be I(x, y) = log2
P (x,y)
P (x)P (y) ); informally, mutual information compares the probability of observing x
and y together (the joint probability) with the probabilities of observing x and y independently (chance).
5
Mutual information, I(v; c/s), measures the strength of the statistical association between the given verb v and the candi-
date class c in the given syntactic position s.
3
In this equation, pmi(i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g.
consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples.
{1,3}
Note that while differential entropies can be negative and not invariant under change of variables, other properties of entropy
are retained (Huber et al, 2008), such as the chain rule for conditional entropy which describes the uncertainty in Y given
knowledge of X , and the chain rule for mutual information which describes the mutual dependence between X and Y .
2
The first term of the conditional probability measures the generality of the association, while the second term of the mutual
information measures the co-occurrence of the association.
2
DA sentences:
Mutual information (Shannon and Weaver, 1949) is a measure of mutual dependence between two random variables. 4
3 Theory Mutual information is a measure of the amount of information that one random variable contains about another
random variable.
4
Conditional mutual information is the mutual information of two random variables conditioned on a third one. {1,3}
Thus, the mutual information is log25 or 2.32 bits, meaning that the joint probability is 5 times more likely than chance. 1
Thus, the mutual information is log20, meaning that the joint is infinitely less likely than chance. 1
65
