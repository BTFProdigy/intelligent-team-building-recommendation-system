Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 1?4,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Grammatical Inference and Computational Linguistics
Menno van Zaanen
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
mvzaanen@uvt.nl
Colin de la Higuera
University of Saint- ?Etienne
France
cdlh@univ-st-etienne.fr
1 Grammatical inference and its links to
natural language processing
When dealing with language, (machine) learning
can take many different faces, of which the most
important are those concerned with learning lan-
guages and grammars from data. Questions in
this context have been at the intersection of the
fields of inductive inference and computational
linguistics for the past fifty years. To go back
to the pioneering work, Chomsky (1955; 1957)
and Solomonoff (1960; 1964) were interested, for
very different reasons, in systems or programs that
could deduce a language when presented informa-
tion about it.
Gold (1967; 1978) proposed a little later a uni-
fying paradigm called identification in the limit,
and the term of grammatical inference seems to
have appeared in Horning?s PhD thesis (1969).
Out of the scope of linguistics, researchers and
engineers dealing with pattern recognition, under
the impulsion of Fu (1974; 1975), invented algo-
rithms and studied subclasses of languages and
grammars from the point of view of what could
or could not be learned.
Researchers in machine learning tackled related
problems (the most famous being that of infer-
ring a deterministic finite automaton, given ex-
amples and counter-examples of strings). An-
gluin (1978; 1980; 1981; 1982; 1987) introduced
the important setting of active learning, or learn-
ing for queries, whereas Pitt and his colleagues
(1988; 1989; 1993) gave several complexity in-
spired results with which the hardness of the dif-
ferent learning problems was exposed.
Researchers working in more applied areas,
such as computational biology, also deal with
strings. A number of researchers from that
field worked on learning grammars or automata
from string data (Brazma and Cerans, 1994;
Brazma, 1997; Brazma et al, 1998). Simi-
larly, stemming from computational linguistics,
one can point out the work relating language learn-
ing with more complex grammatical formalisms
(Kanazawa, 1998), the more statistical approaches
based on building language models (Goodman,
2001), or the different systems introduced to au-
tomatically build grammars from sentences (van
Zaanen, 2000; Adriaans and Vervoort, 2002). Sur-
veys of related work in specific fields can also
be found (Natarajan, 1991; Kearns and Vazirani,
1994; Sakakibara, 1997; Adriaans and van Zaa-
nen, 2004; de la Higuera, 2005; Wolf, 2006).
2 Meeting points between grammatical
inference and natural language
processing
Grammatical inference scientists belong to a num-
ber of larger communities: machine learning (with
special emphasis on inductive inference), com-
putational linguistics, pattern recognition (within
the structural and syntactic sub-group). There is
a specific conference called ICGI (International
Colloquium on Grammatical Inference) devoted
to the subject. These conferences have been held
at Alicante (Carrasco and Oncina, 1994), Mont-
pellier (Miclet and de la Higuera, 1996), Ames
(Honavar and Slutski, 1998), Lisbon (de Oliveira,
2000), Amsterdam (Adriaans et al, 2002), Athens
(Paliouras and Sakakibara, 2004), Tokyo (Sakak-
ibara et al, 2006) and Saint-Malo (Clark et al,
2008). In the proceedings of this event it is pos-
sible to find a number of technical papers. Within
this context, there has been a growing trend to-
wards problems of language learning in the field
of computational linguistics.
The formal objects in common between the
two communities are the different types of au-
tomata and grammars. Therefore, another meet-
ing point between these communities has been the
different workshops, conferences and journals that
focus on grammars and automata, for instance,
1
FSMNLP,GRAMMARS, CIAA, . . .
3 Goal for the workshop
There has been growing interest over the last few
years in learning grammars from natural language
text (and structured or semi-structured text). The
family of techniques enabling such learning is usu-
ally called ?grammatical inference? or ?grammar
induction?.
The field of grammatical inference is often sub-
divided into formal grammatical inference, where
researchers aim to proof efficient learnability of
classes of grammars, and empirical grammatical
inference, where the aim is to learn structure from
data. In this case the existence of an underlying
grammar is just regarded as a hypothesis and what
is sought is to better describe the language through
some automatically learned rules.
Both formal and empirical grammatical infer-
ence have been linked with (computational) lin-
guistics. Formal learnability of grammars has
been used in discussions on how people learn lan-
guage. Some people mention proofs of (non-
)learnability of certain classes of grammars as ar-
guments in the empiricist/nativist discussion. On
the more practical side, empirical systems that
learn grammars have been applied to natural lan-
guage. Instead of proving whether classes of
grammars can be learnt, the aim here is to pro-
vide practical learning systems that automatically
introduce structure in language. Example fields
where initial research has been done are syntac-
tic parsing, morphological analysis of words, and
bilingual modelling (or machine translation).
This workshop organized at EACL 2009 aimed
to explore the state-of-the-art in these topics. In
particular, we aimed at bringing formal and empir-
ical grammatical inference researchers closer to-
gether with researchers in the field of computa-
tional linguistics.
The topics put forward were to cover research
on all aspects of grammatical inference in rela-
tion to natural language (such as, syntax, seman-
tics, morphology, phonology, phonetics), includ-
ing, but not limited to
? Automatic grammar engineering, including,
for example,
? parser construction,
? parameter estimation,
? smoothing, . . .
? Unsupervised parsing
? Language modelling
? Transducers, for instance, for
? morphology,
? text to speech,
? automatic translation,
? transliteration,
? spelling correction, . . .
? Learning syntax with semantics,
? Unsupervised or semi-supervised learning of
linguistic knowledge,
? Learning (classes of) grammars (e.g. sub-
classes of the Chomsky Hierarchy) from lin-
guistic inputs,
? Comparing learning results in different
frameworks (e.g. membership vs. correction
queries),
? Learning linguistic structures (e.g. phonolog-
ical features, lexicon) from the acoustic sig-
nal,
? Grammars and finite state machines in ma-
chine translation,
? Learning setting of Chomskyan parameters,
? Cognitive aspects of grammar acquisition,
covering, among others,
? developmental trajectories as studied by
psycholinguists working with children,
? characteristics of child-directed speech
as they are manifested in corpora such
as CHILDES, . . .
? (Unsupervised) Computational language ac-
quisition (experimental or observational),
4 The papers
The workshop was glad to have as invited speaker
Damir ?Cavar, who presented a talk titled: On boot-
strapping of linguistic features for bootstrapping
grammars.
The papers submitted to the workshop and re-
viewed by at least three reviewers each, covered a
very wide range of problems and techniques. Ar-
ranging them into patterns was not a simple task!
There were three papers focussing on transduc-
ers:
2
? Jeroen Geertzen shows in his paper Dialogue
Act Prediction Using Stochastic Context-Free
Grammar Induction, how grammar induction
can be used in dialogue act prediction.
? In their paper (Experiments Using OSTIA for
a Language Production Task), Dana Angluin
and Leonor Becerra-Bonache build on previ-
ous work to see the transducer learning algo-
rithm OSTIA as capable of translating syn-
tax to semantics.
? In their paper titled GREAT: a finite-state
machine translation toolkit implementing a
Grammatical Inference Approach for Trans-
ducer Inference (GIATI), Jorge Gonza?lez and
Francisco Casacuberta build on a long his-
tory of GOATI learning and try to eliminate
some of the limitations of previous work.
The learning concerns finite-state transducers
from parallel corpora.
Context-free grammars of different types were
used for very different tasks:
? Alexander Clark, Remi Eyraud and Amaury
Habrard (A note on contextual binary fea-
ture grammars) propose a formal study of
a new formalism called ?CBFG?, describe
the relationship of CBFG to other standard
formalisms and its appropriateness for mod-
elling natural language.
? In their work titled Language models for con-
textual error detection and correction, Her-
man Stehouwer and Menno van Zaanen look
at spelling problems as a word prediction
problem. The prediction needs a language
model which is learnt.
? A formal study of French treebanks is made
by Marie-He?le`ne Candito, Benoit Crabbe? and
Djame? Seddah in their work: On statistical
parsing of French with supervised and semi-
supervised strategies.
? Franco M. Luque and Gabriel Infante-Lopez
study the learnability of NTS grammars with
reference to the Penn treebank in their paper
titled Upper Bounds for Unsupervised Pars-
ing with Unambiguous Non-Terminally Sep-
arated Grammars.
One paper concentrated on morphology :
? In A comparison of several learners for
Boolean partitions: implications for morpho-
logical paradigm, Katya Pertsova compares a
rote learner to three morphological paradigm
learners.
References
P. Adriaans and M. van Zaanen. 2004. Computational
grammar induction for linguists. Grammars, 7:57?
68.
P. Adriaans and M. Vervoort. 2002. The EMILE
4.1 grammar induction toolbox. In Adriaans et al
(Adriaans et al, 2002), pages 293?295.
P. Adriaans, H. Fernau, and M. van Zaannen, editors.
2002. Grammatical Inference: Algorithms and Ap-
plications, Proceedings of ICGI ?02, volume 2484
of LNAI, Berlin, Heidelberg. Springer-Verlag.
D. Angluin. 1978. On the complexity of minimum
inference of regular sets. Information and Control,
39:337?350.
D. Angluin. 1980. Inductive inference of formal lan-
guages from positive data. Information and Control,
45:117?135.
D. Angluin. 1981. A note on the number of queries
needed to identify regular languages. Information
and Control, 51:76?87.
D. Angluin. 1982. Inference of reversible languages.
Journal of the Association for Computing Machin-
ery, 29(3):741?765.
D. Angluin. 1987. Queries and concept learning. Ma-
chine Learning Journal, 2:319?342.
A. Brazma and K. Cerans. 1994. Efficient learning
of regular expressions from good examples. In AII
?94: Proceedings of the 4th International Workshop
on Analogical and Inductive Inference, pages 76?90.
Springer-Verlag.
A. Brazma, I. Jonassen, J. Vilo, and E. Ukkonen. 1998.
Pattern discovery in biosequences. In Honavar and
Slutski (Honavar and Slutski, 1998), pages 257?270.
A. Brazma, 1997. Computational learning theory and
natural learning systems, volume 4, chapter Effi-
cient learning of regular expressions from approxi-
mate examples, pages 351?366. MIT Press.
R. C. Carrasco and J. Oncina, editors. 1994. Gram-
matical Inference and Applications, Proceedings of
ICGI ?94, number 862 in LNAI, Berlin, Heidelberg.
Springer-Verlag.
N. Chomsky. 1955. The logical structure of linguis-
tic theory. Ph.D. thesis, Massachusetts Institute of
Technology.
3
N. Chomsky. 1957. Syntactic structure. Mouton.
A. Clark, F. Coste, and L. Miclet, editors. 2008.
Grammatical Inference: Algorithms and Applica-
tions, Proceedings of ICGI ?08, volume 5278 of
LNCS. Springer-Verlag.
C. de la Higuera. 2005. A bibliographical study
of grammatical inference. Pattern Recognition,
38:1332?1348.
A. L. de Oliveira, editor. 2000. Grammatical Infer-
ence: Algorithms and Applications, Proceedings of
ICGI ?00, volume 1891 of LNAI, Berlin, Heidelberg.
Springer-Verlag.
K. S. Fu and T. L. Booth. 1975. Grammatical infer-
ence: Introduction and survey. Part I and II. IEEE
Transactions on Syst. Man. and Cybern., 5:59?72
and 409?423.
K. S. Fu. 1974. Syntactic Methods in Pattern Recogni-
tion. Academic Press, New-York.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10(5):447?474.
E. M. Gold. 1978. Complexity of automaton identi-
fication from given data. Information and Control,
37:302?320.
J. Goodman. 2001. A bit of progress in language mod-
eling. Technical report, Microsoft Research.
V. Honavar and G. Slutski, editors. 1998. Gram-
matical Inference, Proceedings of ICGI ?98, number
1433 in LNAI, Berlin, Heidelberg. Springer-Verlag.
J. J. Horning. 1969. A study of Grammatical Inference.
Ph.D. thesis, Stanford University.
M. Kanazawa. 1998. Learnable Classes of Categorial
Grammars. CSLI Publications, Stanford, Ca.
M. J. Kearns and U. Vazirani. 1994. An Introduction
to Computational Learning Theory. MIT press.
L. Miclet and C. de la Higuera, editors. 1996. Pro-
ceedings of ICGI ?96, number 1147 in LNAI, Berlin,
Heidelberg. Springer-Verlag.
B. L. Natarajan. 1991. Machine Learning: a Theoret-
ical Approach. Morgan Kauffman Pub., San Mateo,
CA.
G. Paliouras and Y. Sakakibara, editors. 2004. Gram-
matical Inference: Algorithms and Applications,
Proceedings of ICGI ?04, volume 3264 of LNAI,
Berlin, Heidelberg. Springer-Verlag.
L. Pitt and M. Warmuth. 1988. Reductions among
prediction problems: on the difficulty of predicting
automata. In 3rd Conference on Structure in Com-
plexity Theory, pages 60?69.
L. Pitt and M. Warmuth. 1993. The minimum consis-
tent DFA problem cannot be approximated within
any polynomial. Journal of the Association for
Computing Machinery, 40(1):95?142.
L. Pitt. 1989. Inductive inference, DFA?s, and com-
putational complexity. In Analogical and Induc-
tive Inference, number 397 in LNAI, pages 18?44.
Springer-Verlag, Berlin, Heidelberg.
Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and
E. Tomita, editors. 2006. Grammatical Infer-
ence: Algorithms and Applications, Proceedings of
ICGI ?06, volume 4201 of LNAI, Berlin, Heidelberg.
Springer-Verlag.
Y. Sakakibara. 1997. Recent advances of grammatical
inference. Theoretical Computer Science, 185:15?
45.
R. Solomonoff. 1960. A preliminary report on a gen-
eral theory of inductive inference. Technical Report
ZTB-138, Zator Company, Cambridge, Mass.
R. Solomonoff. 1964. A formal theory of inductive
inference. Information and Control, 7(1):1?22 and
224?254.
M. van Zaanen. 2000. ABL: Alignment-based learn-
ing. In Proceedings of COLING 2000, pages 961?
967. Morgan Kaufmann.
G. Wolf. 2006. Unifying computing and cognition.
Cognition research.
4
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41?48,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Language models for contextual error detection and correction
Herman Stehouwer
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
j.h.stehouwer@uvt.nl
Menno van Zaanen
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
mvzaanen@uvt.nl
Abstract
The problem of identifying and correcting
confusibles, i.e. context-sensitive spelling
errors, in text is typically tackled using
specifically trained machine learning clas-
sifiers. For each different set of con-
fusibles, a specific classifier is trained and
tuned.
In this research, we investigate a more
generic approach to context-sensitive con-
fusible correction. Instead of using spe-
cific classifiers, we use one generic clas-
sifier based on a language model. This
measures the likelihood of sentences with
different possible solutions of a confusible
in place. The advantage of this approach
is that all confusible sets are handled by
a single model. Preliminary results show
that the performance of the generic clas-
sifier approach is only slightly worse that
that of the specific classifier approach.
1 Introduction
When writing texts, people often use spelling
checkers to reduce the number of spelling mis-
takes in their texts. Many spelling checkers con-
centrate on non-word errors. These errors can be
easily identified in texts because they consist of
character sequences that are not part of the lan-
guage. For example, in English woord is is not
part of the language, hence a non-word error. A
possible correction would be word .
Even when a text does not contain any non-
word errors, there is no guarantee that the text is
error-free. There are several types of spelling er-
rors where the words themselves are part of the
language, but are used incorrectly in their context.
Note that these kinds of errors are much harder
to recognize, as information from the context in
which they occur is required to recognize and cor-
rect these errors. In contrast, non-word errors can
be recognized without context.
One class of such errors, called confusibles,
consists of words that belong to the language, but
are used incorrectly with respect to their local,
sentential context. For example, She owns to cars
contains the confusible to. Note that this word is
a valid token and part of the language, but used
incorrectly in the context. Considering the con-
text, a correct and very likely alternative would be
the word two. Confusibles are grouped together in
confusible sets. Confusible sets are sets of words
that are similar and often used incorrectly in con-
text. Too is the third alternative in this particular
confusible set.
The research presented here is part of a
larger project, which focusses on context-sensitive
spelling mistakes in general. Within this project
all classes of context-sensitive spelling errors are
tackled. For example, in addition to confusibles,
a class of pragmatically incorrect words (where
words are incorrectly used within the document-
wide context) is considered as well. In this arti-
cle we concentrate on the problem of confusibles,
where the context is only as large as a sentence.
2 Approach
A typical approach to the problem of confusibles
is to train a machine learning classifier to a specific
confusible set. Most of the work in this area has
concentrated on confusibles due to homophony
(to , too , two) or similar spelling (desert , dessert ).
However, some research has also touched upon in-
flectional or derivational confusibles such as I ver-
sus me (Golding and Roth, 1999). For instance,
when word forms are homophonic, they tend to
get confused often in writing (cf. the situation with
to, too , and two, affect and effect , or there , their ,
and they?re in English) (Sandra et al, 2001; Van
den Bosch and Daelemans, 2007).
41
Most work on confusible disambiguation using
machine learning concentrates on hand-selected
sets of notorious confusibles. The confusible sets
are typically very small (two or three elements)
and the machine learner will only see training
examples of the members of the confusible set.
This approach is similar to approaches used in ac-
cent restoration (Yarowsky, 1994; Golding, 1995;
Mangu and Brill, 1997; Wu et al, 1999; Even-
Zohar and Roth, 2000; Banko and Brill, 2001;
Huang and Powers, 2001; Van den Bosch, 2006).
The task of the machine learner is to decide, us-
ing features describing information from the con-
text, which word taken from the confusible set re-
ally belongs in the position of the confusible. Us-
ing the example above, the classifier has to decide
which word belongs on the position of the X in
She owns X cars , where the possible answers for
X are to , too , or two. We call X, the confusible
that is under consideration, the focus word.
Another way of looking at the problem of con-
fusible disambiguation is to see it as a very spe-
cialized case of word prediction. The problem is
then to predict which word belongs at a specific
position. Using similarities between these cases,
we can use techniques from the field of language
modeling to solve the problem of selecting the best
alternative from confusible sets. We will investi-
gate this approach in this article.
Language models assign probabilities to se-
quences of words. Using this information, it
is possible to predict the most likely word in
a certain context. If a language model gives
us the probability for a sequence of n words
PLM (w1, . . . , wn), we can use this to predict the
most likely word w following a sequence of n? 1
words arg maxw PLM (w1, . . . , wn?1, w). Obvi-
ously, a similar approach can be taken with w in
the middle of the sequence.
Here, we will use a language model as a classi-
fier to predict the correct word in a context. Since
a language model models the entire language, it is
different from a regular machine learning classifier
trained on a specific set of confusibles. The advan-
tage of this approach to confusible disambiguation
is that the language model can handle all potential
confusibles without any further training and tun-
ing. With the language model it is possible to take
the words from any confusible set and compute the
probabilities of those words in the context. The
element from the confusible set that has the high-
est probability according to the language model is
then selected. Since the language model assigns
probabilities to all sequences of words, it is pos-
sible to define new confusible sets on the fly and
let the language model disambiguate them with-
out any further training. Obviously, this is not
possible for a specialized machine learning clas-
sifier approach, where a classifier is fine-tuned to
the features and classes of a specific confusible set.
The expected disadvantage of the generic (lan-
guage model) classifier approach is that the accu-
racy is expected to be less than that of the specific
(specialized machine learning classifier) approach.
Since the specific classifiers are tuned to each spe-
cific confusible set, the weights for each of the
features may be different for each set. For in-
stance, there may be confusibles for which the cor-
rect word is easily identified by words in a specific
position. If a determiner, like the , occurs in the po-
sition directly before the confusible, to or too are
very probably not the correct answers. The spe-
cific approach can take this into account by assign-
ing specific weights to part-of-speech and position
combinations, whereas the generic approach can-
not do this explicitly for specific cases; the weights
follow automatically from the training corpus.
In this article, we will investigate whether it is
possible to build a confusible disambiguation sys-
tem that is generic for all sets of confusibles using
language models as generic classifiers and investi-
gate in how far this approach is useful for solving
the confusible problem. We will compare these
generic classifiers against specific classifiers that
are trained for each confusible set independently.
3 Results
To measure the effectiveness of the generic clas-
sifier approach to confusible disambiguation, and
to compare it against a specific classifier approach
we have implemented several classification sys-
tems. First of these is a majority class baseline sys-
tem, which selects the word from the confusible
set that occurs most often in the training data.1
We have also implemented several generic classi-
fiers based on different language models. We com-
pare these against two machine learning classi-
fiers. The machine learning classifiers are trained
separately for each different experiment, whereas
1This baseline system corresponds to the simplest lan-
guage model classifier. In this case, it only uses n-grams with
n = 1.
42
the parameters and the training material of the lan-
guage model are kept fixed throughout all the ex-
periments.
3.1 System description
There are many different approaches that can be
taken to develop language models. A well-known
approach is to use n-grams, or Markov models.
These models take into account the probability
that a word occurs in the context of the previous
n ? 1 words. The probabilities can be extracted
from the occurrences of words in a corpus. Proba-
bilities are computed by taking the relative occur-
rence count of the n words in sequence.
In the experiments described below, we will use
a tri-gram-based language model and where re-
quired this model will be extended with bi-gram
and uni-gram language models. The probability
of a sequence is computed as the combination of
the probabilities of the tri-grams that are found in
the sequence.
Especially when n-grams with large n are used,
data sparseness becomes an issue. The training
data may not contain any occurrences of the par-
ticular sequence of n symbols, even though the
sequence is correct. In that case, the probability
extracted from the training data will be zero, even
though the correct probability should be non-zero
(albeit small). To reduce this problem we can ei-
ther use back-off or smoothing when the probabil-
ity of an n-gram is zero. In the case of back-off,
the probabilities of lower order n-grams are taken
into account when needed. Alternatively, smooth-
ing techniques (Chen and Goodman, 1996) redis-
tribute the probabilities, taking into account previ-
ously unseen word sequences.
Even though the language models provide us
with probabilities of entire sequences, we are
only interested in the n-grams directly around the
confusible when using the language models in
the context of confusible disambiguation. The
probabilities of the rest of the sequence will re-
main the same whichever alternative confusible
is inserted in the focus word position. Fig-
ure 1 illustrates that the probability of for example
P (analysts had expected ) is irrelevant for the de-
cision between then and than because it occurs in
both sequences.
The different language models we will consider
here are essentially the same. The differences lie
in how they handle sequences that have zero prob-
ability. Since the probabilities of the n-grams are
multiplied, having a n-gram probability of zero re-
sults in a zero probability for the entire sequence.
There may be two reasons for an n-gram to have
probability zero: there is not enough training data,
so this sequence has not been seen yet, or this se-
quence is not valid in the language.
When it is known that a sequence is not valid
in the language, this information can be used to
decide which word from the confusible set should
be selected. However, when the sequence simply
has not been seen in the training data yet, we can-
not rely on this information. To resolve the se-
quences with zero probability, we can use smooth-
ing. However, this assumes that the sequence is
valid, but has not been seen during training. The
other solution, back-off, tries not to make this as-
sumption. It checks whether subsequences of the
sequence are valid, i.e. have non-zero probabili-
ties. Because of this, we will not use smoothing to
reach non-zero probabilities in the current exper-
iments, although this may be investigated further
in the future.
The first language model that we will investi-
gate here is a linear combination of the differ-
ent n-grams. The probability of a sequence is
computed by a linear combination of weighted n-
gram probabilities. We will report on two different
weight settings, one system using uniform weight-
ing, called uniform linear, and one where uni-
grams receive weight 1, bi-grams weight 138, and
tri-grams weight 437.2 These weights are normal-
ized to yield a final probability for the sequence,
resulting in the second system called weighted lin-
ear.
The third system uses the probabilities of the
different n-grams separately, instead of using the
probabilities of all n-grams at the same time as is
done in the linear systems. The continuous back-
off method uses only one of the probabilities at
each position, preferring the higher-level probabil-
ities. This model provides a step-wise back-off.
The probability of a sequence is that of the tri-
grams contained in that sequence. However, if the
probability of a trigram is zero, a back-off to the
probabilities of the two bi-grams of the sequence
is used. If that is still zero, the uni-gram probabil-
ity at that position is used. Note that this uni-gram
probability is exactly what the baseline system
2These weights are selected by computing the accuracy of
all combinations of weights on a held out set.
43
. . . much stronger most analysts had expected .
than then
P (much stronger than) P (much stronger then)
?P (stronger than most) ?P (stronger then most)
?P (than most analysts) ?P (then most analysts)
Figure 1: Computation of probabilities using the language model.
uses. With this approach it may be the case that
the probability for one word in the confusible set
is computed based on tri-grams, whereas the prob-
ability of another word in the set of confusibles is
based on bi-grams or even the uni-gram probabil-
ity. Effectively, this means that different kinds of
probabilities are compared. The same weights as
in the weighted linear systems are used.
To resolve the problem of unbalanced probabil-
ities, a fourth language model, called synchronous
back-off, is proposed. Whereas in the case of the
continuous back-off model, two words from the
confusible set may be computed using probabil-
ities of different level n-grams, the synchronous
back-off model uses probabilities of the same level
of n-grams for all words in the confusible set, with
n being the highest value for which at least one of
the words has a non-zero probability. For instance,
when word a has a tri-gram probability of zero and
word b has a non-zero tri-gram probability, b is se-
lected. When both have a zero tri-gram probabil-
ity, a back-off to bi-grams is performed for both
words. This is in line with the idea that if a proba-
bility is zero, the training data is sufficient, hence
the sequence is not in the language.
To implement the specific classifiers, we used
the TiMBL implementation of a k-NN classifier
(Daelemans et al, 2007). This implementation of
the k-NN algorithm is called IB1. We have tuned
the different parameter settings for the k-NN clas-
sifier using Paramsearch (Van den Bosch, 2004),
which resulted in a k of 35.3 To describe the in-
stances, we try to model the data as similar as pos-
sible to the data used by the generic classifier ap-
proach. Since the language model approaches use
n-grams with n = 3 as the largest n, the features
for the specific classifier approach use words one
and two positions left and right of the focus word.
3We note that k is handled slightly differently in TiMBL
than usual, k denotes the number of closest distances consid-
ered. So if there are multiple instances that have the same
(closest) distance they are all considered.
The focus word becomes the class that needs to
be predicted. We show an example of both train-
ing and testing in figure 2. Note that the features
for the machine learning classifiers could be ex-
panded with, for instance, part-of-speech tags, but
in the current experiments only the word forms are
used as features.
In addition to the k-NN classifier, we also run
the experiments using the IGTree classifier, which
is denoted IGTree in the rest of the article, which is
also contained in the TiMBL distribution. IGTree
is a fast, trie based, approximation of k-nearest
neighbor classification (Knuth, 1973; Daelemans
et al, 1997). IGTree allows for fast training and
testing even with millions of examples. IGTree
compresses a set of labeled examples into a deci-
sion tree structure similar to the classic C4.5 algo-
rithm (Quinlan, 1993), except that throughout one
level in the IGTree decision tree, the same feature
is tested. Classification in IGTree is a simple pro-
cedure in which the decision tree is traversed from
the root node down, and one path is followed that
matches the actual values of the new example to
be classified. If a leaf is found, the outcome stored
at the leaf of the IGTree is returned as the clas-
sification. If the last node is not a leaf node, but
there are no outgoing arcs that match a feature-
value combination of the instance, the most likely
outcome stored at that node is produced as the re-
sulting classification. This outcome is computed
by collating the outcomes of all leaf nodes that can
be reached from the node.
IGTree is typically able to compress a large
example set into a lean decision tree with high
compression factors. This is done in reasonably
short time, comparable to other compression al-
gorithms. More importantly, IGTree?s classifica-
tion time depends only on the number of features
(O(f)). Indeed, in our experiments we observe
high compression rates. One of the unique char-
acteristics of IGTree compared to basic k-NN is
its resemblance to smoothing of a basic language
44
Training . . . much stronger than most analysts had expected .
?much, stronger, most, analysts? ?than
Testing . . . much stronger most analysts had expected .
?much, stronger, most, analysts? ??
Figure 2: During training, a classified instance (in this case for the confusible pair {then , than}) are
generated from a sentence. During testing, a similar instance is generated. The classifier decides what
the corresponding class, and hence, which word should be the focus word.
model (Zavrel and Daelemans, 1997), while still
being a generic classifier that supports any number
and type of features. For these reasons, IGTree is
also included in the experiments.
3.2 Experimental settings
The probabilities used in the language models of
the generic classifiers are computed by looking at
occurrences of n-grams. These occurrences are
extracted from a corpus. The training instances
used in the specific machine learning classifiers
are also extracted from the same data set. For
training purposes, we used the Reuters news cor-
pus RCV1 (Lewis et al, 2004). The Reuters cor-
pus contains about 810,000 categorized newswire
stories as published by Reuters in 1996 and 1997.
This corpus contains around 130 million tokens.
For testing purposes, we used the Wall Street
Journal part of the Penn Treebank corpus (Marcus
et al, 1993). This well-known corpus contains ar-
ticles from the Wall Street Journal in 1987 to 1989.
We extract our test-instances from this corpus in
the same way as we extract our training data from
the Reuters corpus. There are minor tokenization
differences between the corpora. The data is cor-
rected for these differences.
Both corpora are in the domain of English lan-
guage news texts, so we expect them to have simi-
lar properties. However, they are different corpora
and hence are slightly different. This means that
there are also differences between the training and
testing set. We have selected this division to cre-
ate a more realistic setting. This should allow for a
more to real-world use comparison than when both
training and testing instances are extracted from
the same corpus.
For the specific experiments, we selected a
number of well-known confusible sets to test
the different approaches. In particular, we
look at {then, than}, {its, it?s}, {your, you?re},
{their, there, they?re}. To compare the difficulty
of these problems, we also selected two words at
random and used them as a confusible set.
The random category consists of two words that
where randomly selected from all words in the
Reuters corpus that occurred more than a thousand
times. The words that where chosen, and used for
all experiments here are refugees and effect . They
occur around 27 thousand times in the Reuters cor-
pus.
3.3 Empirical results
Table 1 sums up the results we obtained with the
different systems. The baseline scores are gen-
erally very high, which tells us that the distribu-
tion of classes in a single confusible set is severely
skewed, up to a ten to one ratio. This also makes
the task hard. There are many examples for one
word in the set, but only very few training in-
stances for the other(s). However, it is especially
important to recognize the important aspects of the
minority class.
The results clearly show that the specific clas-
sifier approaches outperform the other systems.
For instance, on the first task ({then, than}) the
classifier achieves an accuracy slightly over 98%,
whereas the language model systems only yield
around 96%. This is as expected. The classifier
is trained on just one confusible task and is there-
fore able to specialize on that task.
Comparing the two specific classifiers, we see
that the accuracy achieved by IB1 and IGTree is
quite similar. In general, IGTree performs a bit
worse than IB1 on all confusible sets, which is
as expected. However, in general it is possible
for IGTree to outperform IB1 on certain tasks. In
our experience this mainly happens on tasks where
the usage of IGTree, allowing for more compact
internal representations, allows one to use much
more training data. IGTree also leads to improved
45
{then, than} {its, it?s} {your, you?re} {their, there, they?re} random
Baseline 82.63 92.42 78.55 68.36 93.16
IB1 98.01 98.67 96.36 97.12 97.89
IGTree 97.07 96.75 96.00 93.02 95.79
Uniform linear 68.27 50.70 31.64 32.72 38.95
Weighted linear 94.43 92.88 93.09 93.25 88.42
Continuous back-off 81.49 83.22 74.18 86.01 63.68
Synchronous back-off 96.42 94.10 92.36 93.06 87.37
Number of cases 2,458 4,830 275 3,053 190
Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%).
The Number of cases denotes the number of instances in the testset.
performance in cases where the features have a
strong, absolute ordering of importance with re-
spect to the classification problem at hand.
The generic language model approaches per-
form reasonably well. However, there are clear
differences between the approaches. For instance
the weighted linear and synchronous back-off ap-
proaches work well, but uniform linear and con-
tinuous back-off perform much worse. Especially
the synchronous back-off approach achieves de-
cent results, regardless of the confusible problem.
It is not very surprising to see that the contin-
uous back-off method performs worse than the
synchronous back-off method. Remember that
the continuous back-off method always uses lower
level n-grams when zero probabilities are found.
This is done independently of the probabilities of
the other words in the confusible set. The contin-
uous back-off method prefers n-grams with larger
n, however it does not penalize backing off to an
n-gram with smaller n. Combine this with the fact
that n-gram probabilities with large n are compar-
atively lower than those for n-grams with smaller
n and it becomes likely that a bi-gram contributes
more to the erroneous option than the correct tri-
gram does to the correct option. Tri-grams are
more sparse than bi-grams, given the same data.
The weighted linear approach outperforms the
uniform linear approach by a large margin on all
confusible sets. It is likely that the contribution
from the n-grams with large n overrules the prob-
abilities of the n-grams with smaller n in the uni-
form linear method. This causes a bias towards the
more frequent words, compounded by the fact that
bi-grams, and uni-grams even more so, are less
sparse and therefore contribute more to the total
probability.
We see that the both generic and specific clas-
sifier approaches perform consistently across the
different confusible sets. The synchronous back-
off approach is the best performing generic clas-
sifier approach we tested. It consistently outper-
forms the baseline, and overall performs better
than the weighted linear approach.
The experiments show that generic classifiers
based on language model can be used in the con-
text of confusible disambiguation. However, the
n in the different n-grams is of major importance.
Exactly which n grams should be used to com-
pute the probability of a sequence requires more
research. The experiments also show that ap-
proaches that concentrate on n-grams with larger
n yield more encouraging results.
4 Conclusion and future work
Confusibles are spelling errors that can only be de-
tected within their sentential context. This kind
of errors requires a completely different approach
compared to non-word errors (errors that can be
identified out of context, i.e. sequences of char-
acters that do not belong to the language). In
practice, most confusible disambiguation systems
are based on machine learning classification tech-
niques, where for each type of confusible, a new
classifier is trained and tuned.
In this article, we investigate the use of language
models in the context of confusible disambigua-
tion. This approach works by selecting the word
in the set of confusibles that has the highest prob-
ability in the sentential context according to the
language model. Any kind of language model can
be used in this approach.
The main advantage of using language models
as generic classifiers is that it is easy to add new
sets of confusibles without retraining or adding ad-
ditional classifiers. The entire language is mod-
46
eled, which means that all the information on
words in their context is inherently present.
The experiments show that using generic clas-
sifiers based on simple n-gram language models
yield slightly worse results compared to the spe-
cific classifier approach, where each classifier is
specifically trained on one confusible set. How-
ever, the advantage of the generic classifier ap-
proach is that only one system has to be trained,
compared to different systems for each confusible
in the specific classifier case. Also, the exact com-
putation of the probabilities using the n-grams, in
particular the means of backing-off, has a large
impact on the results.
As future work, we would like to investigate the
accuracy of more complex language models used
as classifiers. The n-gram language models de-
scribed here are relatively simple, but more com-
plex language models could improve performance.
In particular, instead of back-off, smoothing tech-
niques could be investigated to reduce the impact
of zero probability problems (Chen and Goodman,
1996). This assumes that the training data we are
currently working with is not enough to properly
describe the language.
Additionally, language models that concentrate
on more structural descriptions of the language,
for instance, using grammatical inference tech-
niques (de la Higuera, 2005), or models that ex-
plicitly take long distance dependencies into ac-
count (Griffiths et al, 2005) can be investigated.
This leads to much richer language models that
could, for example, check whether there is already
a verb in the sentence (which helps in cases such
as {its, it?s}).
A different route which we would also like to in-
vestigate is the usage of a specific classifier, such
as TiMBL?s IGTree, as a language model. If a
classifier is trained to predict the next word in the
sentence or to predict the word at a given position
with both left and right context as features, it can
be used to estimate the probability of the words in
a confusible set, just like the language models we
have looked at so far. Another type of classifier
might estimate the perplexity at a position, or pro-
vide some other measure of ?surprisedness?. Ef-
fectively, these approaches all take a model of the
entire language (as described in the training data)
into account.
References
Banko, M. and Brill, E. (2001). Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of the 39th Annual Meeting of the As-
sociation for Computational Linguistics, pages 26?
33. Association for Computational Linguistics.
Chen, S. and Goodman, J. (1996). An empirical study
of smoothing techniques for language modelling. In
Proceedings of the 34th Annual Meeting of the ACL,
pages 310?318. ACL.
Daelemans, W., Van den Bosch, A., and Weijters, A.
(1997). IGTree: using trees for compression and
classification in lazy learning algorithms. Artificial
Intelligence Review, 11:407?423.
Daelemans, W., Zavrel, J., Van der Sloot, K., and Van
den Bosch, A. (2007). TiMBL: Tilburg Memory
Based Learner, version 6.1, reference guide. Techni-
cal Report ILK 07-07, ILK Research Group, Tilburg
University.
de la Higuera, C. (2005). A bibliographical study
of grammatical inference. Pattern Recognition,
38(9):1332 ? 1348. Grammatical Inference.
Even-Zohar, Y. and Roth, D. (2000). A classification
approach to word prediction. In Proceedings of the
First North-American Conference on Computational
Linguistics, pages 124?131, New Brunswick, NJ.
ACL.
Golding, A. and Roth, D. (1999). A Winnow-Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1?3):107?130.
Golding, A. R. (1995). A Bayesian hybrid method for
context-sensitive spelling correction. In Proceed-
ings of the 3rd workshop on very large corpora,
ACL-95.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax. In
In Advances in Neural Information Processing Sys-
tems 17, pages 537?544. MIT Press.
Huang, J. H. and Powers, D. W. (2001). Large scale ex-
periments on correction of confused words. In Aus-
tralasian Computer Science Conference Proceed-
ings, pages 77?82, Queensland AU. Bond Univer-
sity.
Knuth, D. E. (1973). The art of computer program-
ming, volume 3: Sorting and searching. Addison-
Wesley, Reading, MA.
Lewis, D. D., Yang, Y., Rose, T. G., Dietterich, G., Li,
F., and Li, F. (2004). Rcv1: A new benchmark col-
lection for text categorization research. Journal of
Machine Learning Research, 5:361?397.
Mangu, L. and Brill, E. (1997). Automatic rule ac-
quisition for spelling correction. In Proceedings of
the International Conference on Machine Learning,
pages 187?194.
47
Marcus, M., Santorini, S., and Marcinkiewicz, M.
(1993). Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Quinlan, J. (1993). C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Sandra, D., Daems, F., and Frisson, S. (2001). Zo
helder en toch zoveel fouten! wat leren we uit psy-
cholingu??stisch onderzoek naar werkwoordfouten
bij ervaren spellers? Tijdschrift van de Vereniging
voor het Onderwijs in het Nederlands, 30(3):3?20.
Van den Bosch, A. (2004). Wrapped progressive
sampling search for optimizing learning algorithm
parameters. In Verbrugge, R., Taatgen, N., and
Schomaker, L., editors, Proceedings of the Sixteenth
Belgian-Dutch Conference on Artificial Intelligence,
pages 219?226, Groningen, The Netherlands.
Van den Bosch, A. (2006). Scalable classification-
based word prediction and confusible correction.
Traitement Automatique des Langues, 46(2):39?63.
Van den Bosch, A. and Daelemans, W. (2007). Tussen
Taal, Spelling en Onderwijs, chapter Dat gebeurd
mei niet: Computationele modellen voor verwarbare
homofonen, pages 199?210. Academia Press.
Wu, D., Sui, Z., and Zhao, J. (1999). An information-
based method for selecting feature types for word
prediction. In Proceedings of the Sixth European
Conference on Speech Communication and Technol-
ogy, EUROSPEECH?99, Budapest.
Yarowsky, D. (1994). Decision lists for lexical ambi-
guity resolution: application to accent restoration in
Spanish and French. In Proceedings of the Annual
Meeting of the ACL, pages 88?95.
Zavrel, J. and Daelemans, W. (1997). Memory-based
learning: Using similarity for smoothing. In Pro-
ceedings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 436?443.
48
ABL: Alignment-Based Learning 
Menno van  Zaanen 
School of Computer  Studies 
University of Leeds 
LS2 9 JT  L(~eds 
UK 
menno@scs, leeds ,  ac.  uk 
Abst rac t  
This \])al)er introdu(:es a new tyl)e of grammar 
learning algorithm, iilst)ired l)y sl;ring edit dis- 
tance (Wagner and Fis(:her, 1974). The algo- 
rithm takes a (:ortms of tlat S(~lltell(:es as input 
and returns a (:ortms of lat)elled, l)ra(:ket(~(1 sen-~ 
ten(:(~s. The method works on 1)airs of unstru(:- 
tllr(?(l SelltellC(~,s that have one or more words in 
(:onunon. W\]lc, ll two senten('es are (tivided into 
parts that are the same in 1)oth s(mten(:es and 
parl;s tha|; are (litl'erent, this intbrmation is used 
to lind l)arl;s that are interchangeal)le. These 
t)arts are taken as t)ossil)le (:onstituents of the 
same tyl)e. After this alignment learning stel) ,
the selection learning stc l) sel(~('ts the most l)rot)- 
at)le constituents from all 1)ossit)le (:onstituents. 
This m(;thod was used to t)ootstrat) structure 
(m the ATIS (:ortms (Mar(:us et al, 1f)93) and 
on the OVIS ~ (:ort)us (Bommma et ~d., 1997). 
While the results are en(:om:aging (we ol)|;ained 
Ul) to 89.25 % non-crossing l)ra(:kets precision), 
this 1)at)er will 1)oint out some of the shortcom- 
ings of our at)l)roa(:h and will suggest 1)ossible 
solul;ions. 
1 In t roduct ion  
Unsupervised learning of syntactic structure is 
one of the hardest 1)rol)lems in NLP. Although 
people are adept at learning grammatical struc- 
ture, it is ditficult to model this 1)recess and 
therefore it is hard to make a eomtmter learn 
strllCtllre. 
We do not claim that the algorithm described 
here models the hmnan l)rocess of language 
learning. Instead, the algorithm should, given 
unstructured sentences, find the best structure. 
This means that the algorithm should assign 
1Opcnbam" Vcrvoer hfformatie Systeeln (OVIS) 
stands for Pul)lic Transt)ort hfformation System. 
sl;ru('ture to sentences whi(:h are similar to the 
,~;tru(:ture peot)le would give to sentences, lint 
not ne(:essarily in the same |li l le or Sl);~(;e l'e- 
strictions. 
The algorithm (:onsists of two t)hases. The 
tirst t)hase is a constituent generator, whi(:\]l gen- 
erates a m()tiw~ted set of possible constituents 
1)y aligning sentenc(:s. The se(:ond i)hase re- 
stri(:ts tllis set l)y selecting the best constituents 
from the set. 
The rest of this t)aper is organized as ibl- 
lows. Firstly, we will start t)y describing l)revi- 
ous work in machine learning of language stru(:- 
ture and then we will give a descrit)tion of the 
ABL algorithm. Next, some results of al)t)lying 
the ABL algorithm to different corpora will 1)e 
given, followed 1)y a discussion of the algorithm 
alia flltllre resear(;h. 
2 Prev ious  Work  
I,e;wning metl,o(ls can t)e grouped into suitor- 
vised and unsut)ervised nmthods. Sul)ervised 
methods are initial|seal with structured input 
(i.e. stru(:ture(\] sent(m(:es for grannnar learning 
methods), while mlsut)ervised methods learn l)y 
using mlstru(:tured ata only. 
In 1)ractice, SUl)ervised methods outpertbrm 
mlsut)ervised methods, since they can adapt 
their output based on the structured exami)les 
in the initial|sat|on t)hase whereas unSUl)ervised 
lnethods emmet. However, it is worthwhile 
to investigate mlsupcrvised gramlnar learning 
methods, since "the costs of annotation are pro- 
hibitively time and ext)ertise intensive, and the 
resulting corpora may 1)e too suscet)tible to re- 
stri(:tion to a particular domain, apt)lication, or 
genre". (Kehler and Stolcke, 1.999) 
There have 1)een several approaches to the un- 
supervised learning of syntactic structures. We 
will give a short overview here. 
961 
Memory based learifing (MBL) keeps track of 
possible contexts and assigns word types based 
on that information (Daelemans, 1995). Red- 
ington et al (1998) present a method that 
bootstraps yntactic categories using distribu- 
tional information and Magerman and Marcus 
(1990) describe a method that finds constituent 
boundaries using mutual information values of 
the part of speech n-grams within a sentence. 
Algorithms that use the minimmn description 
length (MDL) principle build grammars that 
describe the input sentences using the minimal 
nunfl)er of bits. This idea stems from intbrnm- 
tion theory. Examples of these systems can be 
found in (Grfinwald, 1994) and (de Marcken, 
1996). 
The system by Wolff (1982) pertbrms a 
heuristic search while creating and Inerging 
symbols directed by an evaluation function. 
Chen (1.995) presents a Bayesian grammar in- 
duction method, which is tbllowed by a post- 
pass using the inside-outside algorithm (Baker, 
1979; Lari and Young, 1990). 
Most work described here cmmot learn com- 
plex structures uch as recursion, while other 
systems only use limited context to find con- 
stituents. However, the two phases in ABL 
are closely related to some previous work. 
Tim alignment learning phase is etlb.ctively a 
compression technique comparat)le to MDL or 
Bayesian grammar induction methods. ABL 
remembers all possible constituents, building 
a search space. The selection h;arning phase 
searches this space, directed by a probabilistic 
evaluation function. 
3 Algor i thm 
We will describe an algorithm that learns struc- 
ture using a corpus of plain (mlstructured) sen- 
tences. It does not need a structured train- 
ing set to initialize, all structural information 
is gathered from the unstructured sentences. 
The output of the algorithm is a labelled, 
bracketed version of the inlmt corpus. Although 
the algorithm does not generate a (context-fl'ee) 
grammar, it is trivial to deduce one from the 
structured corpus. 
The algorithm builds on Harris's idea (1951) 
that states that constituents of the same type 
can be replaced by each oth, er. Consider the sen- 
Wh, at is a family fare 
Wh, at is th, e payload of an African Swallow 
Wh, at is & family fare)x 
Wh, at is (the payload of an African Swallow)x 
Figure 1: Example bootstrapping structure 
For each sentence sl in the corpus: 
For every other sentence s2 in the corpus: 
Align s~ to s2 
Find the identical and distinct parts 
between s~ and s2 
Assign non-terminals to the constituents 
(i.e. distinct parts of s~ and s2) 
Figure 2: Alignment learning algorithm 
fences as shown in figure 1. 2 The constituents a 
.family fare and the payload of an African Swal- 
low both have the same syntactic type (they 
are both NPs), so they can be replaced by each 
other. This means that when the constituent in 
the first sentence is replaced by the constituent 
in the second sentence, the result is a wflid sen- 
tence in the language; it is the second sentence. 
The main goal of the algorithm is to estab- 
lish that a family .fare and the payload of art, 
African Swallow are constituents and have the 
same type. This is done by reversing Harris's 
idea: 'i1" (a group o.f) words car-,, be; replaced by 
each other, they are constituents and h.ave th, e 
same type. So the algorithm now has to find 
groups of words that can be replaced by each 
other and after replacement s ill generate valid 
sentences. 
The algorithm consists of two steps: 
1. Alignment Leanfing 
2. Selection Learning 
3.1 A l ignment  Learning 
The model learns by comparing all sentences 
in the intmt corpus to each other in pairs. An 
overview of the algorithm can be tbund in fig- 
ure 2. 
Aligning sentences results in "linking" iden- 
tical words in the sentences. Adjacent linked 
words are then grouped. This process reveals 
2All sentences in the examlfles can be fbund in the 
ATIS corlms. 
962 
.f,'o,,,. Sa,,. F,'a,.ci.,'co (to Dallas).  
./'rout (Dallas to)| San Francisco 02 
(Sa,, l.o)  Dallas 02 
O, DaUas #o Sa,,. J';'a,.cisco)2 
? \[;1"0 ~II, 
.fF()Ii't 
(San Francisco), to (Dallas)2 
(Dalla.gj to (Sa,,. 
Figure 3: Ambiguous al ignments 
1;t1(; groul)S of identical words, 1)ut it also llIlC()v- 
ers the groups of distinct wor(ls in the sentences. 
In figure 1 What is is the identical part  of the 
sentences and a fam, ily J'a~v, and the payload of 
an A./ricau, Swallow are the dist inct l)arts. The  
distinct parts are interchangeable, so they are 
(tetermilmd to 1)e const ituents o17 the same I;yl)e. 
We will now Cxl)lain the stel)s in the align- 
men|  learning i)hase in more de, tail. 
3.1.1 Ed i t  D is tance  
q\[b find the identi(:al word grouI)S in |;he sen- 
tences, we use the edit; distan(:e a lgor i thm by 
Wagner and Fischer (197d:), which finds the 
min imum nmnl)er of edit operat ions (insertion, 
(lelei;ion and sul)stii;ul;ion) l;o change one sen- 
te, nce into the other, ld(mti(:al wor(ts in the sen- 
t(races can 1)e t'(mnd at \])\]a(;es W\]l(~,l'e lie edit op- 
eration was al)plied. 
The insl;antia,tiol~ of the algoril;hm that  fin(is 
l;}le longest  COllllllOll Slll)S(}(\]ll(}ll(;( ~,ill two  Sell- 
tences sometimes "links" words that  are, too 
far apart, in figure 3 when: 1)esides the o(:cm'- 
rences of.from,, the ocem:rences of San }4"au, ci.sco 
or Dallas are linked, this results in unintended 
constituents. We woukt r;d;her have the lnodel 
linking to, result ing in a sl;1"u(;I;llre with the 1101111 
phrases groul)ed with the same type corre(:tly. 
Linking San Francisco or Dallas results i~l 
const ituents that  vary widely in size. This  stems 
from the large distance between the linked 
urords in the tirsi; sentence mid in th(; s(:cond 
sentence. This  type of al ignlnent can t)e ruled 
out by biasing the cost f imction using distances 
between words. 
3.1.2 Group ing  
An edit distance algor i thm links identical words 
in two sentences. When adjacent wor(ls are 
linked in l)oth sentences, they can l)e grouped. 
A groul) like this is a part  of a senten(:e that  can 
also be tbmM in the other sentence. (In figure 1, 
What is is a group like this.) 
The rest of the sentences can also be grouped. 
The words in these grout)s arm words that  are 
distinct in the two sentences. When all of these 
groups fl:om sentence, one would 1)e relflaced by 
the respective groups of sentence two, sentence 
two is generated. (a family fare and th, c pay- 
load of an African Swallow art: of this type of 
group in figure 1.) Each pair of these distinct 
groups consists of possilfle constil;uents Of the 
same type. :~ 
As can be, seen in tigure 3, it is possible that  
empty groups can lm learned. 
a . l .a  Ex is t ing  Const i tuents  
At seine 1)oint it may be t)ossible that  the model 
lem'ns a co11stituent that  was already stored. 
This may hal)l)en when a new sentence is com- 
pared to a senlaen(;e in the part ial ly structured 
corpus. In this case,, no new tyl)e, is intro(hu:ed~ 
lint the, consti|;ucnl; in l;he new sentence gel;s l;he 
same type of the const i tuent in the sentence in 
the part ial ly structm:ed corpus. 
It may even t)e the case that  a part ial ly si;ruc- 
tured sentence is compared to another  part ial ly 
sl;rtlctllre(1 selll;elR,e. This occm:s whel~ a s(:n- 
fence that (;onl;ains some sl;ructure, which was 
learner1 1)y COlnl)aring to a sentelme in the par- 
t;\]ally structure(l  (;Ol;pllS~ is (;Olllt)ar(~,(\] 1;o al l -  
other (t)art;ially stru(:ture(t) sente, n(:e. When 
the ('omparison of these two se, nl;ence, s yields 
a const i tuent hai: was ah:ea(ly t)resent in both 
senten(:es, the tyl)es of these constitueld;S are 
merged. All const i tuents of these types are ut)- 
dated, so the, y have the same tyl)e. 
By merging tyl)es of const i tuents we make t;he 
assuml)tion that  co\]lstil;uents in a (:ertain con- 
text can only have one tyl)e. In section 5.2 we 
discuss the, imt)li(:atiolls of this assmnpl;ion and 
propose an alternative at)t)roach. 
3.2 Se lect ion  Learn ing  
The first step in the algorithm may at some 
point generate COllstituents that overlap with 
other constituents, hi figure 4 Give me all 
flights .from Dallas to Boston receives two over- 
lal)ping structures. One constituent is learned 
3Since the alger||Inn does not know any (linguist;|c) 
llalIICS for the types, the alger|finn chooses natural num- 
bers to denote different types. 
963 
( Book Delta 128 ) f lwn Dallas to Boston 
?'Give m?(all.fligh, ts)'f,'om Dallas to Boston) 
Give me ( help on classes ) 
l?igure 4: Overlapping constituents 
by comparing against Book Delta 128 f i rm Dal- 
las to Boston and the other (overlapl)ing) con- 
stituent is tbund by aligning with Give me help 
on classes. 
The solution to this problem has to do with 
selecting the correct constituents (or at least 
the better constituents) out of the possible con- 
stitnents. Selecting constituents can be done in 
several dittbrent ways. 
ABL : incr  Assume that the first constituent 
learned is the correct one. This means that 
when a new constituent overlaps with older 
constituents, it can 1)e ignored (i.e. they are 
not stored in the cortms). 
ABL : lea f  The model corot)rites the probabil- 
ity of a constituent counting the nmnber of 
times the particular words of the constituent 
have occurred in the learned text as a con- 
stituent, normalized by the total number of 
constituents. 
Ple,f(c) = \]c' C C:  yield(c') = yicld(c)l 
ICI 
where C is the entire set: of constituents. 
ABL :braneh In addition to the words of the 
sentence delimited by the constituent, the 
model computes the probability based on the 
part of the sentence delimited by the words 
of the constituent and its non-terminal (i.e. 
a normalised probability of ABL:leaf). 
Pb, .~na, , (c l root(c  ) = r )  = 
e c :  y/el( l ( , - ' )  - -  y ie ld (c )  A ; "1 
Ic" c :  ,'oot(c") = 
The first method is non-probabilistic and may 
be applied every time a constituent is found that 
overlaps with a known constituent (i.e. while 
learning). 
The two other methods are probabilistic. The 
model computes the probability of the con- 
stituents and then uses that probability to select 
constituents with the highest probability. These 
methods are ~pplied afl;er the aligmnent learn- 
ing phase, since more specific informatioil (in 
the form of 1)etter counts) can be found at that 
time. 
In section 4 we will ewfluate all three methods 
on the ATIS and OVIS corpus. 
3.2.1 Vi terb i  
Since more than just two constituents can over- 
lap, all possible combinations of overlapping 
constitueni;s hould be considered when com- 
Imting the best combination of constituents, 
which is the product of the probabilities of the 
separate constituents as in SCFGs (cf. (Booth, 
1969)). A Viterbi style algorithm optimization 
(1967) is used to etficiently select the best com- 
bination of constituents. 
When conll)uting the t)r()t)ability of a com- 
bination of constituents, multiplying the sepa- 
rate probabilities of the constituents biases to- 
wards a low nnmber of constituents. Theretbre, 
we comtmte the probability of a set of con- 
stituents using a normalized version, the geo- 
metric mean 4, rather than its product. (Cara- 
ballo and Charniak, 1998) 
4 Resu l ts  
The three different ABL algorithms m~d two 
1)aseline systems have been tested on the ATIS 
and OVIS corpora. 
The ATIS corlms ti'om the P(;nn Treebank 
consists of 716 sentences containing 11,777 (:on- 
stituents. The larger OVIS corpus is a Dutch 
corpus containing sentences on travel intbrma- 
tion. It consists of exactly 10,000 sentences. We 
have removed all sentences containing only one 
word, resulting in a corpus of 6,797 sentences 
and 48,562 constituents. 
The sentences of the corpora are stript)ed 
of their structures. These plain sentences are 
used in the learning algorithms and the result- 
ing structure is compared to the structure of the 
original corpus. 
All ABL methods are tested ten times. Th(, 
ABL:incr method is applied to random orders of 
the input corpus. The probabilistic ABL meth- 
ods select constituents at random when differ- 
ent combinations of constituents have the same 
probability. The results in table 1 show the 
4The geometric mean of a set of constituents 
^. . .  A = VFI  =, P( d 
964 
LEFT 
I{IGI/T 
ABL:INCll 
AI3L:LEAF 
ABL:BI/ANCII 
NCBP 
32.6O 
82.70 
83.24 (1.17) 
81.42 (0.11) 
8, .31 (0.01) 
AT1S OVIS 
NCBI{ ZCS NCB\] ) NCBR ZCS 
76.82 
92.91 
87.21 (0.67) 
86.27 (0.06) 
89.31 (0.01) 
1.J2 
38.83 
18.56 (2.32) 
21.63 (0.5O) 
29.75 (0.00) 
51.23 
75.85 
88.71 (0.79) 
85.32 (0.02) 
89.2.5 (0.oo) 
73.17 
86.66 
84.36 (1.\]0) 
79.96 (0.03) 
8>o4 (0.0|)) 
25.22 
48.08 
30.87 (0.09) 
42.20 (0.01) 
laJ)h, 1: Results of I;he 
mean ;rod standard deviations (between bra(:k- 
ets). 
The two base, line systcnis, left and right, onty 
t)uiM left: mid right brnnching trees respectively. 
Three, metrics hnve been compnl;cd. NCBP 
stmlds for Non-(\]rossing Bra.(:kets Precision, 
which denotes the percentage, of learned (:on- 
stituents th~,t do not overlai) with any con- 
sl;it;uent;s in I;he m'igi'n, al (:orpus. NCIH~ is the 
Non-Crossing Brackets ll.e(:all mid shows |;he 
t)(;rt'ent~ge of constituents in the original co l  
t)us thai; (1o not overlap with :my constituents 
in the learned (:oft)us. Finnlly, Z(LS' strums ti)l' 
Zero-(Jrossing Sentences a,nd r(',l)reseuts he per- 
c(ml;age of sentence, s that (t(1 not have m~y over- 
lnt)l)ing constii;uenl;s. 
4.1 Eva luat ion  
'l-'tm incr modet 1)erfi)rms (tuii:e well (:onsi(hwing 
the t'~mt hat it; (:;mnot re(:ov(w t'roln incorre(:t 
(:()nstituents, with a t)re(:ision a,nd re(:~dl of ()V(~l' 
8t) %. The order of the senl;en(:es how(we, r is 
quite iml)orbmt , since |tie sl;ml(tard deviation 
of the inc'r model is quite high (est)e~(:ialty with 
the ZCS, reaching 3.22 % on the OV!S (:orpus). 
We expected the prot)nl)ilistic nmtho(ts to 
i)erform t)o,l;ter, trot the lc((f modet performs 
slightly worse. The, ZCS, however, is somewhat 
better, re, suiting in 21.63 % on the AT1S cor- 
pus. Furthermore, d;he standard deviations of 
the le,:f model (&lid Of the branch, model) are 
c\]ose to 0 %. The st;~tisti(:al methods generate 
more precise, results. 
Ttm bra'n, ch, modet dearly outl)erfornl all 
o~,her models. Using more Sl)e(:itic statistics gen- 
erate better results. 
Although the resull;s of the N FIS (:orpus mM 
OVlS corIms differ, the, conclusions that (:ml })e 
reached are similm:. 
ATIS and OVIS corpora 
4.2 ABL  Compared  to Other  Methods  
It; is difficult to corot)are the results of the ABL 
model ag~dnst other lnethods, since, often d i f  
thrent corpora or m(',trics m:e used. The meth- 
ods describe, d by Pereira and Schabcs (1.9(.)2) 
comes reasonably close to ours. The unsuper- 
vised nmthod le~rns tructure on plain sentences 
from the ATIS corlms resulting in 37.35 % pre- 
cision, while the "un.supcrvised ABL signili(:mltly 
outperforms this method, reaching 85.31% l)re- 
cision. Only their s'uperviscd version results in 
n slightly higher pre('ision of 90.36 %. 
The syste, nl th;d; simt)ly buihts right branch- 
ins structures results in 82.70 % precision mid 
92.91% teeM1 on the ATIS cortms, where ABL 
got 85.31% and 89.31%. This wa,s expected, 
sin(:e English is a right |)rmmhing language; a 
left branching sysl;Clll t)(~rff)l.'ltle(| lllllCh woFsc 
(32.60 % pre(:ision and 76.82 % rccnll). C(m- 
versely, right branching wouht not do very well 
on ~ ,l~q)mmse, corpus (~ left 1)r~m(:hing lan- 
gua.ge). Sin(:e A\]31, does not have a 1)ref(~renc(~ 
fi)r direction built; in, we exi)ect ABL to t)ertbrm 
similarly on n Ja,t)anese (:orpus. 
5 D iscuss ion  and  Future  Extens ions  
5.1 Recurs |on  
All ABL methods des('ribed here can lem:n re- 
cursive structures and have been fomtd when 
~t)plying ABI, to the NIl?IS and OVIS (:orlms. 
As (:ml be sc(m in figure 5, the learned recur- 
sive structure, is similm: to the, original struc- 
ture. Some structure has t)een removed to make 
it easier to see where the recurs|on occurs. 
Roughly, recursive structures arc built in two 
steps. First, the algorithm generates the struc- 
ture with difl'cro, nt non-terminals. Then, the 
two nonq;ermimds are merged as described in 
so, el;ion 3.1.3. The merging of the non-terminals 
m~y occur anywhere in the cortms , sin(:e all 
merged non-terndnals are ut)dated. 
965 
learned 
original 
learned 
original 
Please ezplain the (field FLT DAY in the (table)is)is 
Please explain (the .field FLT DAY in (the table)NP)Np 
Explain classes QW and (QX and (Y)a2)~'e 
Explain classes ((QW)Np and (QX)NI, and (Y)NP)NP 
Fignre 5: Recursive structures learned in the A TIS corpus 
Show me the ( morning )x flights 
Show me the ( nonstop )x fli.qhts 
Figure 6: Wrong syntactic type 
5.2 Wrong Syntactic Type 
In section 3.1.3 we made the assumt)tion that a 
constituent in a certain context can only have 
one type. This assumption introduces ome 
problems. 
The sentence John likes visiting relatives il- 
lustrates uch a problem. The constituent vis- 
iting relatives can be a noun phrase or n verb 
phrase. 
Another prol)lem is ilhlstrated in figure 6. 
When applying the ABL learning algorithm to 
these sentences, it will determine that morning 
and nonstop are of the same type. Untbrtu- 
nately, morning is a noun, while nonstop is an 
adverb) 
A fixture extension will not only look at the 
type of the constituents, lint also at the con- 
text; of the constituents. Ii5 the example, the 
constituent morning nlay also take the t)lace of 
a subject position in other sentences~ 1)ut the 
constituent nonstop never will. This intbrnm- 
tion can be used to determine when to merge 
constituent types, efl'ectively loosening the as- 
sunlption. 
5.3 Weakening Exact Match 
When the ABL algorithms try to learn with two 
conlpletely distinct sentences, nothing can be 
learned. If we weaken the exact match between 
words in the alignment step of the algorithm, it 
is possible to learn structure ven with distinct 
sentences. 
Instead of linking exactly matching words, 
the algorithm should match words that are 
equivalent. An obvious way of implementing 
this is by making use of cquivalence classes. (See 
5Harris's implication does hold in these sentences. 
nonstop can also be replaced by for example cheap (an- 
other adverb) and morning can be replaced by evenin.q 
(another noun). 
for example (Redington et al, 1998).) The idea 
1)ehind equivalence classes is that words which 
are closely related are grouped together. 
A big advantage of equivalence classes is that 
they can be learned in an unsupervised way, so 
the resulting algorithm remains nnsui)ervised. 
Words that are in the same equivalence class 
are. said to be sufficiently equivalent, so the 
aligmnent algoritlnn may assunm they are sin> 
ilar and may thus link them. Now sentences 
that do not have words in common, but do have 
words in the same equivalence class in common, 
can be used to learn structure. 
When using equivalence classes, more con- 
stituents are learned and more terminals in con- 
stitnents may l)e seen as similar (according to 
the equivalence classes). This results in a much 
richer structm'ed corlms. 
5.4 Alternative Statistics 
At the moment we have tested two diflbrent 
ways of computing the probal)ility of a con- 
stituent: ABL:leaf which computes  the t ) ro t )  - 
ability of the occurrence of the terminals in a 
constituent, and ABL:b','anch which coml)utes 
the probability of the occurrence of |;11(; termi- 
nals together with the root non-terminal in a 
(-onstitueut, based on the learned corpus. 
Of course, other models can bc imt)lemented. 
One interesting possibility takes a DOP-like ap- 
proach (Bod, 1998), which also takes into ac- 
count the inner structure of the constituents. 
6 Conc lus ion 
Wc have introduced a new grammar learning al- 
gorithm based 055 c()mparing and aligning plain 
sentences; neither pre-labelled or bracketed sen- 
tences, nor pre-tagged sentences arc used. It 
uses distinctions between sentences to find pos- 
sible constituents and afterwards elects the 
most probable ones. The output of the algo- 
rithm is a structured version of the corpus. 
By l;aking entire sentences into account, the 
context used by the model is not limited by win- 
dow size, instead arbitrarily large contexts are 
966 
used. Furthermore, the model has the ability to 
learn recursion. 
~\['ln'ee ditl'erent instances of the algorithm 
have l)een al)t)lied to two corpora of differ- 
eat size, the ATIS corpus (716 sentences) and 
the OVIS corpus (6,797 sentences), generating 
promising results. Although t;he OVIS corpus 
is almost ten t;imes the size of the ATIS cor- 
pus, these corpora describe a small conceptual 
domain. We plan to ~l)t)ly the ~flgori~hms to 
larger domain corpora in the near fllture. 
Re ferences  
J. K. Barker. 1979. Trainabh; grammars for 
speech recognition. In J. J. Wolf and 1). H. 
Klatt, editors, Speech, Communication Papers 
for the Ninety-seventh Meetin.q of the Acous- 
tical Society of America, pages 547-550. 
R,ens Bod. 1998. Beyond Grammar An 
_F, zpcricncc-Bascd Th, eory of Language. Stan- 
Jbrd, CA: CSLI Publications. 
R.. Bonnema, R. Bod, and R,. Scha. 1997. A 
DOP model for semantic iil|;ertn'el;ation. In 
Proceedings of the Association for Compu~ 
tational Ling'aistics/Eurwpean Ch, apter of th, c 
Association for Computational Linguistics, 
Madrid, p~ges 159 167. Sommerset, N J: A s- 
soci~tion tbr Compul;ational Linguistics. 
T. Booth. 1969. Probal)ilistic representation f 
formed languages. In Co'~@',rcnce ll,cco'rd o/" 
1959 "lEnth, Annual Symposium on ,5'witcl~,in.q 
and Automata Theory, pages 74 8:1. 
Sharon A. Caraballo and Eugene Charniak. 
1998. New figures of merit for best-first prol)- 
abilist;ie chart parsing. Computational Lin- 
guistics, 24(2):275 298. 
Stanley F. Chert. 1995. Bayesian gralmnar in- 
duction for language modeling. \]:ll Proceed- 
ings of the Association J'or Computational 
Linguistics, pages 228 235. 
Walter Daelemmls. 1995. Memory-based lexi- 
cal acquisition and 1)rocessing. In P. Stefh;ns, 
editor, Mach, inc Translation and the Lexicon, 
vohmm 898 of Lecture Notes in Artificial In- 
telligence, pages 85 98. Berlin: Springer Ver- 
lag. 
Carl G. de Marcken. 1996. Unsupervised Lan- 
g'aage Acquisition. I)h.D. thesis, Departnmnt 
of Electrical Engineering mid Comtmter Sci- 
ence, Massachusetts Institute of Technology, 
Cambridge, MA, sep. 
Peter Oriinwald. 1994. A nfinimmn de, scription 
lengl;h approach to grammar inference. In 
G. Scheler, S. Wernter, and E. R,iloif, editors, 
Connectionist, Statistical and S!pnbolic Ap- 
proaches to Learning for Natural Language, 
vohnne 1004 of Lecture Notes in dl~ pages 
203-216. Berlin: Springer Verlag. 
Zellig Harris. 1951. Methods in Structural Lin- 
guistics. Chicago, IL: University of Chicago 
Press. 
Andrew Kehler and Andreas Stoleke. 1999. 
Preface. In A. Kehler and A, Stolcke, edi- 
tors, Unsuper'viscd Learning in Natural Lan- 
guage Processing. Association for Comlmta- 
tional Linguistics. Proceedings of the work- 
shop. 
K. Lari and S. J. Young. 1990. The estima- 
tion of stochastic ontext-free grammars us- 
ing the inside-outside ~dgorithm. Computer 
Speech and Language, 4:35 56. 
\]). Magerman and M. Marcus. 1990. Pars- 
ing natural language using mutual intbrma- 
tion statistics. In Pwcecdin.qs o,f th, e National 
Con.fcrcnce on Artificial Intelli.qence, p~ges 
984 989. Cambridge, MA: MIT Press. 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 
1993. Building a large annotated corpus of 
english: the Penn tr(,ebank. Computational 
Linguistics, 19(2):31.3 330. 
F. Pereira and Y. Schgd)e,s. 1992. Inside-outside 
reestimation fl:om pm:tially t)racketed cor- 
pora. In l'rocccdings of th, c Association for 
Computational Lin.quistics, pages :128-135, 
Newark, Debm~are. 
Martin Redington, Nick Chater, and Steven 
Finch. 1998. Distrilmtional information: A 
powerflfl cue for acquiring synt;actic cate- 
gories. Cwnitivc Science, 22(4):4:25 469. 
A. Viterbi. t967. Error bmmds for convoh> 
tiona\] codes and an asymptotically ol)timum 
decoding algorithm. Institute of Electrical 
and Electronics Engineers Transactions on 
Information Th, cory, 13:260 269. 
Robert A. Wagner and Michael J. Fischer. 
1974:. The string-to-string correction prob- 
lem. Journal of th, e Association for Comput- 
ing Machinery, 21(1):168-173, jmL 
J. G. Wollf. 1982. Lmlguage acquisition, data 
compression and generalization. Langv, agc ~ 
Communication, 2:57-89. 
967 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 124?128, Dublin, Ireland, August 23-29 2014.
OpenSoNaR: user-driven development of the SoNaR corpus interfaces
Martin Reynaert
TiCC / Tilburg University
CLST / Radboud
Universiteit Nijmegen
reynaert@uvt.nl
Matje van de Camp
De Taalmonsters
matje@taalmonsters.nl
Menno van Zaanen
TiCC / Tilburg University
mvzaanen@uvt.nl
Abstract
OpenSoNaR is an online system that allows for analyzing and searching the large scale Dutch
reference corpus SoNaR. Due to the size of the corpus, accessing the information contained in
the dataset has proven to be difficult for less technically inclined researchers. The OpenSoNaR
project aims to facilitate the use of the SoNaR corpus by providing a user-friendly online inter-
face. To make sure that the resulting system is practically useful, several user groups have been
identified, who drive the interface development process by providing practical use cases. The
current system is already used in educational and research settings.
1 Introduction
Concerted efforts over the past years
1
in the Dutch language area in Europe, the Netherlands and the
northern half of Belgium, Flanders, have yielded a corpus of over 500 million words of richly linguis-
tically annotated contemporary written Dutch, called SoNaR (Oostdijk et al., 2013). After the SoNaR
project finished, it became clear that the corpus is difficult to handle for most potential users because
of its size and technical formats. The OpenSoNaR project
2
aims to resolve the practical problems of
dealing with the SoNaR dataset. On the basis of an available corpus back-end system, BlackLab, that
allows for searching through all the information contained in the SoNaR dataset, user interfaces, called
WhiteLab, are developed that allow for user-desired types of corpus searches and investigations. We will
first briefly describe the SoNaR dataset. Next, we discuss the OpenSoNaR project, treating the ideas
behind the project as well as the description of the system, focusing mainly on WhiteLab.
2 SoNaR
The SoNaR project developed a large scale reference corpus for contemporary, written Dutch. This
balanced corpus consists of about 540 million tokens of Dutch across a wide range of text types, such
as books, magazine articles, reports, subtitles, but also data from the ?new? media, such as chat texts,
SMS and tweets. A unique aspect of this corpus is that for all texts, IPR regulations are explicitly
known, in most cases settled by contract with the copyright holders. All texts are linguistically annotated
on several layers. Documents, paragraphs, sentences and tokens are uniquely identified and lemmata,
part-of-speech (POS), named entity information and morphological analyses have been automatically
annotated in the data. All information, from tokenization, linguistic annotation to metadata, is marked in
XML structures. The Folia XML format (van Gompel and Reynaert, 2013) is used to hold all the textual
information and linguistic annotations. Metadata, which includes, for example, origin, author, text genre
(as far as known), is stored in a separate document that is linked to the text document. Metadata is stored
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
Funded in large part by the Dutch Language Union in the STEVIN programme described in the Open Access book
?Essential Speech and Language Technology for Dutch? http://www.springer.com/education+%26+language/
linguistics/book/978-3-642-30909-0
2
Homepage: http://opensonar.uvt.nl
124
in CMDI files (Broeder et al., 2011). The metadata files duplicate the number of documents in the corpus.
SoNaR consists of approximately 2.4 million files.
3 OpenSoNaR
The aim of the OpenSoNaR project is to develop and implement a practically useful system that allows
easy access to the SoNaR corpus. The project has been set up to make sure that the functionality of the
system serves a wide range of users. As is implied by the name of the project, our wish is to make the
system we build as open as possible to all users. We will provide a fully open online service to all, from
schoolchildren onwards, based on the IPR-settlements negotiated during the SoNaR corpus building
project. The SoNaR corpus has subsections which were obtained on the basis of Creative Common
licenses, e.g. Wikipedia, which will be open. The system will further under CLARIN login be fully open
and free to all non-commercial researchers. Restrictions on commercial research and/or use apply to
some important subsections, e.g. the newspapers and periodicals incorporated in the corpus. For these,
interested commercial parties need to negotiate access separately with the copyright owners.
3.1 Project
Technical issues stand in the way of most potential users actively conducting research on the SoNaR cor-
pus. The OpenSoNaR project is to resolve these. The SoNaR-500 corpus
3
requires information retrieval
tools for efficient searching, as trivial solutions are simply too slow or cumbersome to use. The XML
format requires the search system to know about the XML structure that describes the linguistic infor-
mation in somewhat more complex searches, for example, using a combination of token and linguistic
information. Metadata selections are required just as well.
The aim of the OpenSoNaR system is to solve all these practical problems. Two components are
developed: BlackLab that enables searching in the data, and WhiteLab providing the user-interface. In
order to provide the most useful search interface, four main user groups have been identified. These con-
sist of researchers in the areas of (corpus and cognitive) linguistics, communication and media studies,
literary sciences, and cultural sciences. Each of these groups are asked to provide typical use cases, i.e.
search operations that they would like to be able to ask the SoNaR corpus. To test practical usability,
the system is also incorporated in education. Currently, OpenSoNaR has been successfully tested by
students in courses on linguistics and research methodology. The results of these test provide feedback
on what further developments the interfaces require to be practically of best possible use to all.
3.2 Related work
Throughout Europe, most national corpora available online are based on BlackLab?s predecessor and
source of inspiration, the notable Corpus Workbench system
4
(Christ, 1994). In the Netherlands, there
is another concurrent project in which a corpus exploration and exploitation environment is being de-
veloped. This is the much larger and far more ambitious project Nederlab
5
. Nederlab aims to create a
research portal to all digital corpora available for Dutch from its earliest days. There is cross-fertilization
between both projects.
3.3 System
The OpenSoNaR system consists of two components that interact with each other. The BlackLab com-
ponent is the back-end of the system and provides the actual search functionality. On top of BlackLab,
the WhiteLab component provides the front-end, user interface.
3
Available free for research from the Dutch HLT Agency TST-Centrale http://tst-centrale.org/nl/
producten/corpora/sonar-corpus/6-85. The documentation link on the page offers access to the user manual.
4
Homepage: http://cwb.sourceforge.net/
5
http://www.nederlab.nl/docs/Nederlab_NWO_Groot_English_aanvraagformulier.pdf
125
3.3.1 BlackLab
As explained on its official GitHub site
6
, BlackLab is a Java-based corpus retrieval engine built on top
of Apache Lucene. It allows fast, complex searches with accurate hit highlighting on large, annotated,
bodies of text, in our case text in FoLiA XML. This back-end system is further being developed at the
Dutch Institute for Lexicology (INL) by Jan Niestadt. OpenSoNaR had a head start in its further interface
development efforts in that BlackLab comes equipped with a fine basic user interface as is evidenced by
the online INL corpora ?Letters as Loot?
7
and the corpus of Medieval Dutch ?Corpus Gysseling?
8
.
3.3.2 WhiteLab
The Whitelab user interface
9
is currently available in two languages, Dutch and English. We hope to
be able to extend the languages available. The Whitelab user by default lands on the Search page of
OpenSoNaR when logging in. Next to this page we have the Explore page and the Home page.
Home The Home page provides information about the system. It provides a first-user manual which
gives an overview of the main possibilities OpenSoNaR offers. It also provides the actual user manual of
the SoNaR corpus which offers in-depth information on the composition of this corpus of contemporary
written Dutch. Next, short films are available that provide tutorials of how to use the system.
Explore The Explore page gives statistical information about the corpus contents, providing insight
into the distribution of the texts available per genre and according to their provenance, basically whether
they were collected in the Netherlands or in Flanders. A third category contains texts with uncertain
provenance, e.g. the texts from various European Union organizations or from Wikipedia. This page also
affords access to n-gram (where n is 1 to 5) frequency lists derived from the whole corpus and its genre
subsections for word forms, lemmata and the combination of lemmata with POS-tags.
Search The Search environment is the most elaborate. It provides four levels of access to the contents:
Simple, Extended, Advanced and Expert.
The Simple search option provides Google-style, single query box access. Entering a search term
here will instantiate a search over the full contents of the corpus. The search is for word forms, which
may be phrases (n-grams), in which case exact matches are sought, i.e. respecting the actual sequence of
words. This functionality is also provided by the next two search environments.
The Extended search environment allows one to impose selection filters on the search effected. These
filters are of two kinds. First, there are filters on the metadata. Second, there are filters on the lexical
level, allowing one to search for either word forms, lemmata or by POS-tags.
The metadata filters are at first hidden behind a bar visible above the actual lexical query fields. When
the user wants to impose metadata filters the bar is expanded by a simple mouse click and the user is
presented with a row consisting of three drop-down boxes. The middle box has just two options: ?is? or
?is not?. The left box gives access to all the metadata fields available in the corpus CMDI metadata files.
The right box, upon selection of a particular metadata field in the left box, dynamically expands with the
list of available metadata contents, where applicable. Metadata filters can be stacked. Through a ?plus?
button to the right of the query row, one may obtain further rows in each of which further restrictions
on the query may be imposed. The metadata selection interface further provides the option of grouping
the query results obtained by a range of features. E.g, if one here selects the option of having the results
presented by country of origin of the hit texts, one is not presented directly with the KWIC list of results,
but rather with a bar representation of the number of hits per country. One may then click on one of these
bars and be presented with the KWIC list. Alternatively, having made a selection of texts, one may opt
to be presented with a word cloud of its most salient terms, for exploratory purposes.
The lexical filters allow one to perform optionally case-sensitive searches for either word forms, for
lemmata and for POS-tags. When the search is for lemmata, all the word forms sharing the same lemma
6
https://github.com/INL/BlackLab
7
URL: http://brievenalsbuit.inl.nl/zeebrieven/page/search
8
http://gysseling.corpus.taalbanknederlands.inl.nl/gysseling/page/search
9
We provide screenshots of the interfaces at http://opensonar.uvt.nl
126
will be retrieved. For POS-tag searches the user is presented with a drop-down list which presents a
layman?s translation in plain language for the actual POS-tags involved. Combinations of, for instance,
word forms and POS searches are possible to direct the search for the word ?drink? (ibidem in English)
towards the first person singular of the present tense verb form, rather than its use as a noun.
For the Advanced search option we fully acknowledge to emulate the elegant interface to CQL-
query building as provided by the Swedish Spr?akbanken
10
. Users are first presented with a single box
containing three query fields. By horizontally or vertically adding further boxes as in Figure 1 they may
build quite complex queries without the need to know the query language behind them. Users get to see
the query they have built and have the option of further extending it, manually. Results are subsequently
presented graphically, cf. Figure 2.
Figure 1: An advanced CQL-query having been built with Advanced Search query boxes
The Expert search requires knowledge of the query language incorporated in the system. It is CQL,
the Corpus Query Language
11
. In its essence, this search option?s limitations are defined mainly by the
user?s CQL proficiency.
Regardless of the search option one has chosen, by default, eventually a KWIC list of results is pre-
sented. One may then choose to ?Toggle titles? and by clicking on a title, move to full text view. There,
moving the cursor over any of the words in the text, one gets to see a small window with the word form?s
unique ID, lemma and POS-tag.
A feature of the Extended and Advanced search options we have not seen in other corpus exploration
environments is that multiple queries can be performed in one operation. This is facilitated by the fact
that by clicking on the ?list? button to the right of the query boxes the user may effortlessly upload a pre-
prepared list of query terms. After uploading, these query terms are converted by the system into actual,
separate CQL queries which are accessible via a drop-down list above the query boxes. The user then has
the option of having the output presented separately, per query, or mixed. As soon as the queries have
run, the user has the further option of downloading the results. If in the Advanced search environment a
user uploads more than one query list, the system makes a combination of all the query terms in the lists.
Given x terms in list A and y terms in list B, this results in x times y queries. If this is not what the user
intended, then he has the option of uploading a list of, for instance, word bigrams to be searched for in
10
See ?Korp? at http://spraakbanken.gu.se/eng/start
11
A nice tutorial is at: http://cwb.sourceforge.net/files/CQP_Tutorial/
127
Figure 2: Grouped results of the advanced CQL-query having been built with Advanced Search query
boxes. After grouping, results are first presented graphically after which the user may further explore the
text snippets retrieved.
the Extended search environment.
The query results are in a tab-separated format suitable for loading in a spreadsheet. The format should
be easily convertible to the specific formats required by statistical packages such as R or SPSS.
4 Conclusion
In this OpenSoNaR system demonstration paper we have given an overview of ongoing work in the
Netherlands to provide to all online access to the new richly annotated reference corpus for contemporary
written Dutch called SoNaR.
Acknowledgements
The authors, TiCC senior scientific programmer Ko van der Sloot as Software Quality Control Offi-
cer, and Max Louwerse as Project Coordinator gratefully acknowledge support from CLARIN-NL in
project OpenSoNaR (CLARIN-NL-12-013). The first author further acknowledges support from NWO
in project Nederlab. We would like to specifically thank our colleagues at INL: Katrien Depuydt, Jesse
de Does and Jan Niestadt.
References
Daan Broeder, Oliver Schonefeld, Thorsten Trippel, Dieter Van Uytvanck, and Andreas Witt. 2011. A pragmatic
approach to XML interoperability ? the Component Metadata Infrastructure (CMDI). In Balisage: The Markup
Conference 2011, volume 7.
Oliver Christ. 1994. A Modular and Flexible Architecture for an Integrated Corpus Query System.
Nelleke Oostdijk, Martin Reynaert, V?eronique Hoste, and Ineke Schuurman. 2013. The construction of a 500-
million-word reference corpus of contemporary written Dutch. In Essential Speech and Language Technology
for Dutch: Results by the STEVIN-programme, chapter 13. Springer Verlag.
Maarten van Gompel and Martin Reynaert. 2013. FoLiA: A practical XML Format for Linguistic Annotation - a
descriptive and comparative study. Computational Linguistics in the Netherlands Journal, 3.
128
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal and Empirical Grammatical Inference
Jeffrey Heinz, Colin de la Higuera and Menno van Zaanen
heinz@udel.edu, cdlh@univ-nantes.fr, mvzaanen@uvt.nl
1
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Outline of the tutorial
I. Formal GI and learning theory (de la Higuera)
II. Empirical approaches to regular and subregular natural
language classes (Heinz)
III. Empirical approaches to nonregular natural language
classes (van Zaanen)
2
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
I Formal GI and learning theory
What is grammatical inference?
What does learning or having learnt imply?
Reasons for considering formal learning
Some criteria to study learning in a probabilistic and a non
probabilistic setting
3
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A simple definition
Grammatical inference is about learning a grammar given
information about a language
Vocabulary
Learning = building, inferring
Grammar= finite representation of a possibly infinite set of
strings, or trees, or graphs
Information=you can learn from text, from an informant, by
actively querying
Language= possibly infinite set of strings, or trees, or graphs
4
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A Dfa (Ack: Jeffrey Heinz)
The (CV)* language representing licit sequences of sounds in many
languages in the world. Consonants and vowels must alternate;
words must begin with C and must end with V. States show the
regular expression indicating its ?good tails?.
(CV )? V (CV )?
C
V
5
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A context free grammar and a parse tree
(de la Higuera 2010)
S
NP VP
John V NP
hit Det N
the ball
S ? NP VP
VP? V NP
NP? Det N
6
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A categorial dependency grammar (Be?chet et al 2011)
elle 7? [pred ],
la 7? [#(? clit ? a? obj)]?clit?a?obj ,
lui 7? [#(? clit ? 3d ? obj)]?clit?3d?obj ,
a 7? [#(? clit ? 3d ? obj)\#(?
clit ? a ? obj)\pred\S/aux ? a ? d ],
donne?e 7? [aux ? a ? d ]?clit?3d?obj?clit?a?obj
7
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A finite state transducer (Ack: Jeffrey Heinz)
A subsequential transducer illustrating a common phonological rule
of palatalization ( k ?? >tS / i). States are labelled with a
number and then the output string given by the ? function for that
state.
0,? 1,k
k:?
k:kk, C:kC, V:kV
i:>tSi
C,V,i k
? = {C ,V , k , i}
8
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
So for example:
w t(w)
kata kata
kita >tSita
tak tak
taki ta>tSi
. . .
9
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
Questions
Why grammar and not language?
Why a and not the?
10
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why not write ?learn a language??
Because you always learn a representation of a language
Paradox
Take two learners learning a context-free language, one is learning
a quadratic normal form and the other a Greibach normal form,
they cannot agree that they have learnt the same thing
(undecidable question).
Worth thinking about. . . is it a paradox? Do two English speakers
agree they speak the same language?
11
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
How can a become the?
Ask for the grammar to be the smallest, best (re a score). ?
Combinatorial characterisation
The learning problem becomes an optimisation problem!
Then we often have theorems saying that
If our algorithm does solve the optimisation problem, what we
have learnt is correct
If we can prove that we can?t solve the optimisation problem,
then the class is not learnable
12
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Optimal with respect of some score
Score should take into account:
Simplicity
Coverage
Usefulness
What scores?
Occam argument
Compression argument
Kolmogorov complexity
MDL argument
13
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Moreover
GI is not only about building a grammar from some data. It is
concerned with saying something about:
the quality of the result,
the quality of the learning process,
the properties of the process.
14
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Naive example
Suppose you are building a random number generator.
How are you convinced that it works?
Because it follows sound principles as defined by number
theory specialists?
Because you have tested and the number 772356191 has been
produced?
Because you have proved that the series of numbers that will
be produced is incompressible?
Empirical approach
Experimental approach
Formal approach
15
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical approach: using good (safe?) ideas
For example, genetic algorithms or neural networks
Or some mathematical principle (Occam, Kolmogorov,
MDL,. . . )
Can become a principled approach
Alternative point of view
Empirical approach is about imitating what nature (or humans) do
16
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Experimental approach
Benchmarks
Competitions
Necessary but not sufficient
How do we know that all the cases are covered?
How do we know that we dont have a hidden bias?
17
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal approach: showing that the algorithm has converged
Is impossible:
Just one run
Can?t prove that 23 is random
But we can say something about the algorithm:
That in the near future, given some string, we can predict if
this string belongs to the language or not;
Choose between defining clearly ?near future? and accepting
probable truths (or error bounds) or leaving it undefined and
using identification.
18
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What else would we like to say?
That if the solution we have returned is not good, then that is
because the initial data was bad (insufficient, biased)
Idea:
Blame the data, not the algorithm
19
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Suppose we cannot say anything of the sort?
Then that means that we may be terribly wrong even in a
favourable setting
Thus there is a hidden bias
Hidden bias: the learning algorithm is supposed to be able to
learn anything inside class L1, but can really only learn things
inside class L2, with L2 ? L1
20
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Saying something about the process itself
Key idea: if there is something to learn and the data is not
corrupt, then, given enough time, we will learn it
Replace the notion of learning by that of identifying
21
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
In practise, does it make sense?
No, because we never know if we are in the ideal conditions
(something to learn + good data + enough of it)
Yes, because at least we get to blame the data, not the
algorithm
22
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Complexity issues
Complexity theory should be used: the total or update
runtime, the size of the data needed, the number of mind
changes, the number and weight of errors. . .
. . . should be measured and limited.
23
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A linguistic criterion
One argument appealing to linguists (we hope) is that if the
criteria are not met for some class of languages that a human
is supposed to know how to learn, something is wrong
somewhere
(preposterously, the maths can?t be wrong. . . )
24
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Non probabilistic settings
Identification in the limit
Resource bounded identification in the limit
Active learning (query learning)
25
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
Information is presented to the learner who updates its
hypothesis after inspecting each piece of data
At some point, always, the learner will have found the correct
concept and not change from it
(Gold 1967 & 1978)
26
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example
Number Presentation Analysis of hy-
pothesis
New hypothesis
(regexp)
1 a + a
2 aaa + inconsistent a?
3 aaaa - inconsistent a(aa)?
4 aaaaaa - consistent a(aa)?
9234 aaaaaaaa - consistent a(aa)?
45623416 aaaaaaaaa + consistent a(aa)?
27
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A presentation is
a function ? : N ? X
where X is some set,
and such that ? is associated to a language L through a
function Yields : Yields(?) = L
If ?(N) = ?(N) then Yields(?) = Yields(?)
28
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
text presentation
A text presentation of a language L ? ?? is a function
? : N ? ?? such that ?(N) = L
? is an infinite succession of all the elements of L
(note : small technical difficulty with ?)
informed presentation
An informed presentation (or an informant) of L ? ?? is a
function ? : N ? ?? ?{?,+} such that
?(N) = (L,+) ? (L,?)
? is an infinite succession of all the elements of ?? labelled to
indicate if they belong or not to L
29
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Active presentation
The learner interacts with the environment (modelled as an
oracle) through queries
A membership query
Learner presents string x
Oracle answer yes or no
A correction query (Becerra-Bonache et al 2005 & 2008)
Learner presents string x
Oracle answer yes or returns a close correction
An equivalence query
Learner presents hypothesis H
Oracle answer yes or returns a counter-example
30
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentations for {anbn : n ? N}
Legal presentation from text: ?, a2b2, a7b7,. . .
Illegal presentation from text: ab, ab, ab,. . .
Legal presentation from informant : (?,+), (abab,?),
(a2b2,+), (a7b7,+), (aab,?), (abab,?),. . .
31
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentation for Spanish
Legal presentation from text: En un lugar de la Mancha. . .
Illegal presentation from text: Goooool
Legal presentation from informant : (en,+), (whatever,-),
(un,+), (lugar,+), (lugor,-), (xwszrrzt,-),
32
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What happens before convergence?
On two occasions I have been asked [by members of Parliament],
?Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?? I am not able rightly to apprehend
the kind of confusion of ideas that could provoke such a question.
Charles Babbage
33
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Further definitions
Given a presentation ?, ?n is the set of the first n elements in
?.
A learning algorithm (learner) A is a function that takes as
input a set ?n and returns a grammar of a language.
Given a grammar G , L(G ) is the language
generated/recognised/ represented by G .
34
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Convergence to a hypothesis
A converges to G with ? if
?n ? N : A(?n) halts and gives an answer
?n0 ? N : n ? n0 =? A(?n) = G
If furthermore L(G ) = Yields(?) then we have identified.
35
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
L
G
Pres(L)
L
Yields
A
Figure: The learning setting.
from (de la Higuera 2010)
36
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Consistency and conservatism
We say that the learner A is consistent if ?n is consistent with
A(?n) ?n
A consistent learner is always consistent with the past
Consistency and conservatism
We say that the learner A is conservative if whenever ?(n + 1)
is consistent with A(?n), we have A(?n) = A(?n+1)
A conservative learner doesn?t change his mind needlessly
37
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning from data
A learner is order dependent if it learns something different
depending on the order in which it receives the data.
Usually an order independent learner is better.
38
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What about efficiency?
We can try to bound
global time
update time
errors before converging (IPE)
mind changes (MC)
queries
good examples needed (characteristic samples)
(Pitt 1989, de la Higuera et al 2008)
39
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial number of implicit prediction errors
Denote by G 6|= x if G is incorrect with respect to an element
x of the presentation (i.e. the learner producing G has made
an implicit prediction error.
G is polynomially identifiable in the limit from Pres if there exists
an identification learner A and a polynomial p() such that given
any G in G, and given any presentation ? of L(G ),
]i : A(?i ) 6|= ?(i + 1) ? p(|G |).
40
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial characteristic sample
G has polynomial characteristic samples for identification learner A
if there exists a polynomial p() such that: given any G in G, ?Y
correct sample for G , such that whenever Y ? ?n, A(?n) ? G and
?Y ? ? p(?G?)
As soon as the CS is in the data, the result is correct;
The CS is small.
41
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial queries
(Angluin 1987)
Algorithm A learns with a polynomial number of queries if the
number of queries made before halting with a correct
grammar is polynomial in
the size of the target,
the size of the information received.
42
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Main negative results
Cannot learn Nfa, Cfgs from an informant in most
polynomial settings (Pitt 1989, de la Higuera 1997)
Cannot learn Dfa from text (Gold 1967)
Cannot learn Dfa from membership nor equivalence queries
(Angluin 1981 & 1987).
Main positive results
Can learn Dfa from an informant with polynomial resources
(Oncina and Garc??a 1992);
Can learn Dfa from membership and equivalence queries
(Angluin 1987).
43
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Probabilistic settings
Pac learning (about learning yes-no machines with fixed but
unknown distributions)
Identification with probability 1 (about identifying
distributions)
Pac learning distributions (about approximately learning
distributions)
44
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning a language from sampling
We have a distribution over ??
We sample twice:
once to learn,
once to see how well we have learned
The Pac setting: Les Valiant, Turing award 2010
45
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Pac-learning
(Valiant 1984, Pitt 1989)
L a class of languages
G a class of grammars
 > 0 and ? > 0
m a maximal length over the strings
n a maximal size of machines
H is -AC (approximately correct)*
if
PrD [H(x) 6= G (x)] < 
46
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial Pac learning
There is a polynomial p(?, ?, ?, ?) such that
in order to learn -AC machines of size at most n with error at
most ? we require at most p(m, n, 1? , 1? ) data and time;
we want the errors to be less than  and bad luck to be less
than ?.
(French radio)
Unless there is a surprise there should be no surprise
French radio, (after the last primary elections, on 3rd of June
2008)
First surprise is ?, second surprise is 
47
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results (Kearns and Valiant 1989, Kearns and Vazirani 1994)
Using cryptographic assumptions, we cannot Pac-learn Dfa
Cannot Pac-learn Nfa, Cfgs with membership queries either
Learning can be seen as finding the encryption function from
examples (Kearns & Vazirani)
48
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alternatively
Instead of learning classifiers in a probabilistic world, learn
directly the distributions!
Learn probabilistic finite automata (deterministic or not)
49
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
No error (Angluin 1988)
This calls for identification in the limit with probability 1
Means that the probability of not converging is 0
Goal is to identify the structure and the probabilities
Mainly a (nice) theoretic setting
Results
If probabilities are computable, we can learn with probability 1
finite state automata (Carrasco and Oncina, 1994)
But not with bounded (polynomial) resources (de la Higuera
and Oncina, 2004)
50
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
With error
Pac definition applies
But error should be measured by a distance between the
target distribution and the hypothesis
How do we measure the distance: L1, L2, L?,
Kullback-Leibler?
51
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results
Too easy to learn with L?
Too hard to learn with L1
Both results hold for the same algorithm! (de la Higuera and
Oncina, 2004)
Nice algorithms for biased classes of distributions
52
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Open problems
We conclude this section on ?what is language learning about?
with some open questions:
What is a good definition of polynomial identification?
How do we deal with shifting targets? (robustness issues)
Alternative views on learnability?
Is being learnable a good indicator of being linguistically
reasonable?
Can we learn transducers? Probabilistic transducers?
53
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
II. GI of Regular Patterns
Why regular?
What are the general GI strategies?
What are the main results?
The main techniques?
The main lessons?
54
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Logically Possible Computable Patterns
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
55
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
56
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#2. Target non-superfinite cross-cutting classes
(instructor?s bias)
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
57
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Common Theme
1 Different learning frameworks may better characterize the
data presentations learners actually get (strategy #1).
2 Classes of formal languages may exist which better
characterize the patterns we are interested in (strategy #2).
3 Hard problems are easier to solve with better characterizations
because the instance space of the problem is smaller.
58
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why Begin with Regular?
Insights obtained here can be (and have been) applied fruitfully to
nonregular classes.
Angluin 1982 showed a subclass of regular languages (the
reversible languages) was identifiable in the limit from positive
data by an incremental learner.
Yokomori?s (2004) Very Simple Languages are a subclass of
the context-free languages, but draws on ideas from the
reversible languages.
Similarly, Clark and Eryaud?s (2007) substitutable languages
(also subclass of context-free) are also based on insights from
this paper.
59
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Section Outline
1 Targets of Learning
2 Learning Frameworks
3 State-merging
4 Results for learning regular languages, relations, and
distributions
60
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Languages
Multiple grammars (i.e. representations) for regular languages:
1 Regular expressions
2 Generalized regular expressions
3 Finite state acceptors
4 Words which satisfy formulae in monadic second order logic
5 Right or left branching rewrite rules
6 . . .
61
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Relations
Multiple grammars (i.e. representations) for regular relations:
Regular expressions (for relations)
Generalized regular expressions (for relations)
Finite state transducers
. . .
62
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular distributions
Multiple grammars (i.e. representations) for distributions over
regular sets and relations:
Weighted finite state automata
Hidden Markov Models
Weighted right or left branching rewrite rules
. . .
63
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
This tutorial: Finite State Automata
Acceptors and subsequential transducers admit canonical forms
1 The smallest deterministic acceptor, syntactic monoids, . . .
2 Canonical forms relate to algebraic properties (Nerode
equivalence relation, i.e. states represent sets of ?good tails?)
3 In contrast, canonical regular expressions have yet to be
determined. For example, there are no canonical (e.g.
shortest) regular expressions for regular languages.
64
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Choices
Success required on which input data streams?
All possible vs. some restricted set
i.e. ?distribution-free? vs. ?non distribution-free?
What kind of samples?
Positive data vs. postive and negative data
Other choices (e.g. query learning) are not discussed here.
65
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? w/ positive and negative data
1 The class of r.e. languages is identifiable in the limit (Gold
1967)
2 Non-enumerative algorithms for regular languages:
1 Gold (1978)
2 RPNI (Oncina and Garc??a 1992)
66
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only
1 No superfinite class (including regular, cf, etc.) is identifiable
in the limit (Gold 1967)
2 Not even the finite class is PAC-learnable (Blumer et al 1989)
3 No superfinite class is identifiable in the limit with probability
p (p > 2/3) (Pitt 1985, Wiehagen et al 1986, Angluin 1988)
4 But many subregular classes are learnable in this difficult
setting.
67
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only: learnable subregular
classes
1 reversible languages (Angluin 1982)
2 strictly local languages (Garcia et al 1990)
3 locally testable and piecewise testable (Garcia and Ruiz 2004)
4 left-to-right and right-to-left iterative languages (Heinz 2008)
5 strictly piecewise languages (Heinz 2010)
6 . . .
7 subsequential functions (Oncina et al 1993)
8 . . .
68
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Non distribution-free? w/ positive data only
1 The class of r.e. languages are identifiable in the limit from
computable classes of r.e. texts (Gold 1967)
2 The class of r.e. distributions are identifiable from
?approximately computable? sequences (Angluin 1988, Chater
and Vitany?? 2007)
3 The class of distributions describable with Probabilistic
Deterministic FSAs (PDFAs) is learnable with probability one
(de la Higuera and Thollard 2000)
4 The class of distributions describable with PDFAs is learnable
in a modified PAC setting (Clark and Thollard, 2004)
69
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning regular languages: Key technique
State-merging
Angluin 1982 (reversible languages)
Muggleton 1990 (contextual languages)
Garcia et al 1990 (strictly local languages)
Oncina et al 1993 (subsequential functions)
Clark and Thollard 2004 (PDFA distributions)
. . .
70
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other techniques
Lattice-climbing
Heinz 2010 (strictly local languages, strictly piecewise
languages, many others)
Kasprizk and Ko?tzing 2010 (function-distinguishable
lanaguages, pattern languages, many others)
State-splitting
Tellier (2008)
71
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Only so much can be covered. . .
It?s impossible to be fair to all
those who have contributed
and to cover all the variants,
even all the algorithms in a
short tutorial. That?s why
there are books!
72
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of State-merging
1 Builds a FSA representation of the input
2 Generalize by merging states
73
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
a. pa??a ?earth? ?? ?
b. tju??aya ?many? ?? ? ?
c. ma??awa`na ?through from behind? ?? ? ?` ?
d. pu??iNka`latju ?we (sat) on the hill? ?? ? ?` ? ?
e. tja?mul`?mpatju`Nku ?our relation? ?? ? ?` ? ?` ?
f. ????ir`iNula`mpatju ?the fire for our benefit
flared up?
?? ? ?` ? ?` ? ?
g. ku?ranju`lul`?mpatju`?a ?the first one who is our
relation?
?? ? ?` ? ?` ? ?` ?
h. yu?ma?`?Nkama`ratju`?aka ?because of mother-in-
law?
?? ? ?` ? ?` ? ?` ? ?
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
74
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
Minimal deterministic FSA for Pintupi Stress
0 1 2
3
4
?? ?
?
?`
?
75
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Structured representations of Input
1 Each word its own FSA (Nondeterministic)
2 Prefix Trees (deterministic)
3 Suffix Trees (reverse determinstic)
76
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Examples of Prefix and Suffix Trees
S =
?
?
?
?? ?? ?
?? ? ? ?? ? ?` ?
?? ? ?` ? ? ?? ? ?` ? ?` ?
?
?
?
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
ST(S)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
16
??
?
??
?
?`
?`
??
?
??
?`
?
??
??
?
77
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Informally
Eliminate redundant environments by state-merging.
States are identified as equivalent and then merged.
All transitions are preserved.
This is one way in which generalizations may occur?because
the post-merged machine accepts everything the pre-merged
machine accepts, possibly more.
Machine A Machine B
0 1 2 3a a a 0 1-2 3a a
a
The merged machine may not be deterministic.
78
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Formally
Definition
Given an acceptor A = (Q, I ,F , ?) and a partition pi of its states
state-merging returns the acceptor A/pi = (Q ?, I ?,F ?, ??):
1 Q ? = pi (the states are the blocks of pi)
2 I ? = {B ? pi : I ? B 6= ?}
3 F ? = {B ? pi : F ? B 6= ?}
4 For all B ? pi and a ? ?,
??(B , a) = {B ? ? pi : ?q ? B , q? ? B ? such that q? ? ?(q, a)}
79
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Theorem
Theorem
Given any regular language L, let A(L) denote the minimal
deterministic acceptor recognizing L. There exists a finite sample
S ? L and a partition pi over PT (S) such that PT (S)/pi = A(L).
Notes
The finite sample need only exercise every transition in A(L).
What is pi?
80
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example
Let?s merge states with the same incoming paths of length 2!
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
81
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Result of State Merging
0 1 2
3-6
4-7
5-8
??
?`
?
?`
?
?
?
This acceptor is not the canonical acceptor we saw earlier but it
recognizes the same language.
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
82
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary of Algorithm
1 States in the prefix tree are merged if they have the same
k-length suffix.
u ? v def?? ?x , y ,w such that |w | = k , u = xw , v = yw
2 The algorithm then is simply:
G = PT (S)/pi?
3 This algorithm provably identifies in the limit from positive
data the Strictly (k + 1)-Local class of languages (Garcia et
al. 1990).
83
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Back to the Illustrative Example
Results for stress patterns more generally
Out of 109 distinct stress patterns in the world?s languages
(encoded as FSAs), this state-merging strategy works for only
44 of them
If we merge states with the same paths up to length 5(!), only
81 are learned.
This is the case even permitting very generous input samples.
In other words, 44 attested stress patterns are Strictly 3-Local and
81 are Strictly 6-Local. 28 are not Strictly 6-Local In fact those 28
are not Strictly k-Local for any k (Edlefsen et al 2008).
84
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
If the current structure is ?ill-formed? then merge states to
eliminate source of ill-formedness
State equivalence relations
1 merge state with same incoming paths of length k (Garcia et. al 1990)
2 recursively eliminate reverse non-determinism (Angluin 1982)
3 merge states with same ?contexts? (Muggleton 1990, Clark and Eryaud
2007)
4 merge final states (Heinz 2008)
5 merge states with same ?neighborhood? (Heinz 2009)
6 . . .
7 merge states to maximize posterior probability (for HMMs, Stolcke 1994)
8 . . .
85
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
Merge states indiscriminately unless ?ill-formedness? arises
Merge unless something tells us not to
1 unless ?onward subsequentiality? is lost (for transducers,
Oncina et al 1993)
2 unless they are ??-distinguishable? (Clark and Thollard 2004)
3 . . .
86
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
Strictly k-Local languages (Garcia et al 1990)
merge states with same incoming paths of length k
?u, v ,w ? ?? : uv ,wv ,? Prefix(L) and |v | = k
?
TailsL(uv) = TailsL(wv) ? L
87
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
0-Reversible languages (Angluin 1982)
recursively eliminate reverse non-determinism
?u, v ,w , y ? ?? : uv ,wv , uy ? L ? wy ? L
88
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging summary
1 Distinctions maintained in the prefix tree are lost by state
merging, which results in generalizations.
2 The choice of partition corresponds to the generalization
strategy (i.e. which distinctions will be maintained and which
will be lost)
Gleitman (1990:12):
The trouble is that an observer who notices everything
can learn nothing for there is no end of categories known
and constructible to describe a situation [emphasis in
original].
89
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results for regular languages
Distribution-free with positive data
Identification in the limit from positive data
1 strictly k-local languages (each state corresponds to suffixes of
up to length k) (Garcia et al 1990)
2 reversible languages (acceptors are both forward and reverse
k-deterministic for some k) (Angluin 1982)
3 k-contextual languages (Muggleton 1990)
4 . . .
90
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Regular relations
Regular relations in CL
1 transliteration
2 translation
3 . . .
4 anything with finite state transducers
91
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
distribution-free with positive data
OSTIA
1 identifies subsequential functions in the limit from positive
data.
2 Merges states greedily unless subsequentiality is violated
3 If the function is partial, exactness is guaranteed only where
the function is defined.
92
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
Subsequential relations
1 are a subclass of the regular relations, recognizing functions.
2 are those which are recognized by subsequential transducers,
which are determinstic on the input and which have an
?output? string associated with every state.
3 have a canonical form.
4 have been generalized to permit up to p outputs for each
input (Mohri 1997).
93
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA for learning phonological rules
Gildea and Jurafsky 1996
1 Show that OSTIA doesn?t learn the English tapping rule or
German word-final devoicing rule from data present in
adapted dictionaries of English or German
2 Applied additional phonologically motivated heuristics to
improve state-merging choices.
What about well-defined subclasses of subsequential relations?
94
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Weighted finite-state automata
non-distribution-free with positive data
The problem
Given a finite multiset of words drawn independently from the
target distribution, what grammar accurately describes the
distribution?
Theorem
The class of distributions describable with Non-deterministic
Probabilistic Finite-State Automata (NPFA) exactly matches the
class of distributions describable with Hidden Markov Models
(Vidal et al 2005).
95
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Maximum Likelihood Estimation
A : 13
a : 0
b : 13
c : 13
M
A : 15
a : 15
b : 15
c : 15
M?
?
{bc}
M represents a family of
distributions with 4 parameters.
M? represents a particular
distribution in this family.
Theorem
For a sample S and deterministic finite-state acceptor M, counting the
parse of S through M and normalizing at each state optimizes the
maximum-likelihood estimate.
(Vidal et. al 2005, de la Higuera 2010) 96
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly 2-Local Distributions are bigram models
?
a?
b?
c ?
a
b
c
a
b
c
a
b
c
a
b
c
Figure: The structure of a bigram model. The 16 parameters of this
model are given by associating probabilities to each transition and to
?ending? at each state.
97
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular distributions
RegularFinite
Some well-defined
subregular class
1 When the structure of a Deterministic FSA is known in
advance, MLE is easy to do.
2 The DFA represents a subregular class of distributions.
98
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
1 N-gram models can?t describe long-distance dependencies.
Long-distance dependencies in phonology
1 Consonantal harmony
(Jensen 1974, Odden 1994, Hansson 2001, Rose and Walker
2004, and many others)
2 Vowel harmony
(Ringen 1988, Bakovic? 2000, and many others)
99
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Sibilant Harmony example from Samala (Inesen?o
Chumash)
[StojonowonowaS] ?it stood upright? (Applegate 1972:72)
cf. *[stojonowonowaS] and
cf. *[Stojonowonowas]
Hypothesis: *[stojonowonowaS] and *[Stojonowonowas] are
ill-formed because the discontiguous subsequences sS and Ss are
ill-formed.
100
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise languages
Rogers et al 2010
1 solely make distinctions on the basis of potentially
discontiguous subsequences up to some length k
2 are mathematically natural. They have several chacterizations
in terms of formal language theory, automata theory, logic,
model theory, and the
3 algebraic theory of automata (Fu et al 2011)
101
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
Heinz and Rogers 2010
1 are defined in terms of the factored automata-theoretic
representations (Rogers et al 2010)
2 along with the co-emission probability as the product (Vidal et
al. 2005)
3 Estimation over the factors permits learnability of the patterns
like the ones in Samala.
Example with ? = {a, b, c} and k = 2.
A0 A1 B0 B1 C0 C1? ?a b c
a
b
c
a
b
c
a
b
c
b
c
a
c
a
b
102
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
SP2 learning results for Chumash
Training corpus 4800 words from a dictionary of Samala
x
P(x | y <)
s >ts S >tS
y
s 0.0325 0.0051 0.0013 0.0002
ts 0.0212 0.0114 0.0008 0.
S 0.0011 0. 0.067 0.0359
>
tS 0.0006 0. 0.0458 0.0314
Table: SP2 probabilities of sibilant occuring sometime after another one
(collapsing laryngeal distinctions)
103
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning larger classes of regular distributions
More non-distribution-free with positive data
The class of distributions describable with PDFA
1 are identifiable in the limit with probability one (de la Higuera
and Thollard 2000).
2 are learnable in modified-PAC setting (Clark and Thollard
2004).
3 The algorithms presented employ state-merging methods.
1 This is a (much!) larger class than that which is describable
with n-gram distributions or with SP distributions.
2 To my knowledge these approaches have not been applied to
tasks in CL.
104
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Oncina et al 1993, de la Higuera and Thollard 2000, Clark and
Thollard 2004, . . .
105
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#2. Target non-superfinite cross-cutting classes
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Angluin 1982, Muggleton 1990, Garcia et al 1990, Heinz 2010, . . .
106
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
1 So far we have discussed algorithms that learn various classes
of languages.
2 But shouldn?t we first know which classes are relevant for our
goals?
3 E.g. for phonology, while ?being regular? may be a necessary
property of phonological patterns, it certainly is not sufficient.
107
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
Research strategy
Patterns ? Characterizations ? Learning algorithms
1 Identify the range and kind of patterns (linguistics).
2 Characterize the range and kind of patterns (computational
linguistics).
3 Create learning algorithms for these classes, prove their
success in a variety of settings, and otherwise demonstrate
their success (grammatical inference, formal learning theory,
computational linguistics)
108
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular classes of regular sets
Regular
Star-Free=NonCounting
TSL LTT
LT PT
SL SP
Proper inclusion
relationships among
subregular language
classes.
instructor?s hunch for
phonology
TSL Tier-based Strictly Local PT Piecewise Testable
LTT Locally Threshold Testable SL Strictly Local
LT Locally Testable SP Strictly Piecewise
(McNaughton and Papert 1971, Simon 1975, Rogers and Pullum 2007, in
press, Rogers et al 2010, Heinz et al 2011)
109
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 1
1 State-merging is a well-studied strategy for inferring
automata, including acceptors, transducers, and weighted
acceptors and transducers.
2 It has yielded theoretical results in many learning frameworks
including both distribution-free and non-distribution-free
learning frameworks.
110
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 2
1 Many subclasses of regular languages are learnable even in the
hardest learning settings.
2 Recent advances yield algorithms for large classes
(probabilistic DFAs)
3 Computational linguists can explore which are relevant to
natural language and consequently which are useful for NLP!
4 There is a rich literature in GI which speaks to these classes,
and how such patterns in these classes can be learned.
111
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview
Empirical grammatical inference
Family of languages
Information contained in input
Overview of systems
Evaluation issues
From empirical to formal GI
112
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Introduction
Language learning
Starting from family of languages
Given set of samples
Identify language that is used to generate samples
Formal grammatical inference
Identify family of languages that can be learned efficiently
Under certain restrictions
Empirical grammatical inference
Exact underlying family of languages is unknown
Target language is approximation
113
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical GI
Try to identify language given samples
E.g. sentences (syntax), words (morphology), . . .
Underlying language class is unknown
For algorithm we still need to make a choice
If identification is impossible, provide approximation
Evaluation of empirical GI is different from formal GI
114
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
What is the underlying family of languages?
Choice has impact on learning algorithm
Many possibilities
Use simple, fixed structures (n-grams)
Find probabilities
Extract structure from treebanks
Slightly more flexible structure
Find probabilities
Learn structure
Flexible structure
Find probabilities
115
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
N-grams
1 Starting from a plain text or collection of texts (corpus)
2 Extract all subsequences of length n (n-grams)
3 Count occurrences of n-grams in texts
4 Assign probabilities to each n-gram based on counts
Issues
Unseen n-grams
Back-off: use n-grams with smaller n
Smoothing: adjust probabilities for unseen n-grams
116
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Using n-gram models
How likely is the sentence ?John likes Mary??
Unigram language model
P(John likes Mary) ? P(John)P(likes)P(Mary)
Bigram language model
P(John likes Mary) ? P(John|?s?)P(likes|John)P(Mary|likes)
Trigram language model
P(John likes Mary) ?
P(John|?s??s?)P(likes|?s?John)P(Mary|John likes)
N-gram language model
P(wn1 ) ?
?n
k=1 P(wk |wk?1k?N+1)
N-grams provide a probability for each sequence
Probability describes how well sequence fits language
117
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from treebanks
1 Starting from a treebank (sentences with structure)
2 Extract grammar rules that are used to create tree structures
For instance, context-free grammars (Charniak 1993)
or sub-trees (Data-Oriented Parsing) (Bod 1998)
3 Count occurrences of grammar rules in treebank
4 Assign probabilities to grammar rules based on counts
Issues
Over-generalization, ?incorrect? probabilities
Add information on applicability of grammar rules
(Johnson 1998)
Reestimate probabilities (EM)
(Dempster et al1977, Lari and Young 1990)
118
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from tree
VB
PRP
He
VB1
adores
VB2
VB
listening
TO
TO
to
NN
music
VB ?PRP VB1 VB2
PRP?He
VB1?adores
VB2?VB TO
VB ?listening
TO ?TO NN
TO ?to
NN ?music
Extract counts from treebank ? probabilities
Reestimate probabilities
Improve fit of grammar and sentences
119
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn structure
1 Starting from a corpus
2 Identify regularities that may serve as grammar rules
3 Output:
Structure assigned to sentences ? extract grammar
Extracted grammar rules (and probabilities) ? parse
Issues
Learning system has to deal with both
flexibility in structure
probabilities of structure
120
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summarizing fixed versus flexible structure
Fixed versus flexible is really a sliding scale
Language modelling using n-grams
Structure is very simple and very rigid
Requires plain sequences as input
Corresponds to k-testable languages (Garc??a 1990)
Language modelling using extracted grammar rules
Structure is more flexible, but restricted by treebank
Requires structured sequences as input
Corresponds to e.g. (limited) context-free languages
?Learning structure?
Structure is flexible, restricted by learning algorithm
Requires plain sequences as input
Corresponds to e.g. context-free languages
121
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical grammatical inference
Choices:
What type of grammar are we learning?
Regular language
K -testable language (n-grams)
Context-free language
. . .
What kind of input do we require?
Sequence of words (sentence)
Sequence of part-of-speech tags
(Partial) tree structures
. . .
What kind of output do we want?
Structured version of input
Explicit grammar
Binary or n-ary (context-free rules)
. . .
122
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of systems
EMILE
Alignment-Based Learning (ABL)
ADIOS
CCM+DMV
U-DOP
. . .
123
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying approach
Given a collection of plain sentences
On what basis are we going to assign structure?
Should structure be linguistically motivated?
or similar to what linguists would assign?
Perhaps we can use tests for constituency to find structure
124
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Substitutability
Elements of the same type are substitutable
Test for constituency (Harris, 1951)
What is (a family fare)NP
Replace noun phrase with another noun phrase
What is (the payload of an African Swallow)NP
Learning by reversing test
What is (a family fare)X
What is (the payload of an African Swallow)X
125
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
EMILE
Learns context-free grammars
Using plain sentences
Originally used to show formal learnability
of (a form of) Categorial Grammars in a PAC learning setting
(Adriaans 1992, Adriaans and Vervoort 2002, Vervoort 2000)
Approach
1 Starting from simple sentences
identify recurring subsequences
2 Store recurring subsequences and contexts
3 Introduce grammar rules when there is enough evidence
Practical implementation allows for several constraints
Context length, subsequence length, . . .
126
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example matrix
John walks
Mary walks
John sees Mary
(.) walks John (.) (.) sees Mary . . . contexts
John x x . . .
walks x . . .
Mary x . . .
sees . . .
... ... ... ... . . .
terms
127
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn grammar rules
Terms that share (approximately) same context are clustered
?John? and ?Mary? are grouped together
Occurrences of terms in cluster are replaced by new symbol
Modified sequences may again contain terms/contexts
Terms may consist of multiple words
Example
John walks ?X walks
Mary walks ?X walks
John sees Mary ?X sees X
Mary slaps John?X slaps X
?sees? and ?slaps? now also share the same context
128
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Based on substitutability test
Using plain sentences
Similar to EMILE, but
Clustered terms are not explicitly replaced by symbol
Terms and contexts are always separated
All terms are considered (and only selected afterwards)
Output is structured version of input or grammar
(van Zaanen 2000a, b, 2002)
129
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Corpus Alignment
Learning
Hypothesis
Space
Hypothesis
Space
Selection
Learning
Structured
Corpus
Structured
Corpus
Grammar
Extraction
Grammar
130
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Alignment learning
Align pairs of sentences
Unequal parts of sentences are stored as hypotheses
(Clustering)
Group hypotheses in same context together
Selection learning
Remove overlapping hypotheses
131
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment learning
Align pairs of sentences
using edit distance (Wagner and Fischer 1974)
or suffixtrees (Geertzen and van Zaanen 2004, Ukkonen 1995)
Unequal parts of sentences are stored as hypotheses
Align all sentences in a corpus to all others
Example
(Y1 I need (X1a dinner during the flight)X1)Y1
(Z1 I need)Z1 (X1to return on (Z2tuesday)Z2)X1
(Y1(Z1he wants)Z1 to return on (Z2wednesday)Z2)Y1
132
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Selection Learning
Alignment learning can generate overlapping brackets
Underlying grammar is considered context-free
Structure describes parse according to underlying grammar
?Wrong? brackets have to be removed
Based on e.g. chronological order or statistics
Example
from (Y1Tilburg (X2to)Y1 Portland)X2
from (X1Portland (Y2to)X1 Tilburg)Y2
133
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
ADIOS
Automatic Distillation of Structure (ADIOS) (Solan 2005)
Idea
1 Represent language as a graph
2 Compress graph
3 As long as possible, find significant patterns in paths
Using substitutability and significance tests
4 (Recursion may be added as a post-processing step)
134
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
sees Mary
S John walks E
Mary slaps John
135
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Phases
1 Initialization
Load all sentences (as paths) in the graph
2 Pattern distilation
Find sub-paths
shared by significant number of partially-aligned paths
using motif-extraction (MEX) algorithm
3 Generalization
Group all nodes that occur in same pattern together
Cluster words/subsequences similarly to EMILE
4 Repeat 2 and 3 until no new patterns are found
136
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
S e1 e2 e3 e4 e5 E
If e2 e3 e4 is a significant pattern
S e1 e2 e3 e4 e5 E
137
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
MEX
Compute probabilities depending on in-/out-degree of nodes
PR(e1; e2) =
# paths from e1 to e2
# paths to e1
PR(e1; e3) =
# paths from e1 to e3
# paths to e1
DR(e1; e3) =
PR(e1; e4)
PR(e1; e3)
PR describes path to the right
similarly PL describes path to the left
Significance is computed based on DR and DL wrt parameter
Informally: find significant changes in number of paths
Pick most significant pattern
138
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Constituent-Context Model (CCM)
Consider all possible binary tree structures on POS sequences
Define a probability distribution over the possible bracketings
A bracketing is a particular structure on a sequence
P(s,B) = Pbin(B)P(s|B)
P(s|B) = ?i ,j :i?jPspan(sij |Bij)Pctx(si?1, sj |Bij)
Run (iterative) Expectation-Maximization (EM) algorithm
to maximize likelihood ?s?SP(s)
(Klein 2002)
139
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Dependency Model with Valence (DMV)
DMV aims to learn dependency relations
in contrast to CCM which learns context-free grammar rules
Dependency parse links words in a head-dependent relation
Model describes likelihood of
left dependencies
right dependencies
stop condition (no more dependencies)
Again, iterative EM is used to maximize likelihood of corpus
140
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CCM+DMV
CCM and DMV can be combined
Both models have different view on structure
Results of combined system are better than either systems
Strengths of both systems are combined
(Klein 2004)
141
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
U-DOP
Similar to CCM in that it
finds probability distribution over ?all? structures
uses POS sequences
U-DOP uses Data-Oriented Parsing (DOP) as formalism
Extends probabilistic model of context-free grammars
Requires practical implementation choices
Random sampling due to huge size of search space
(Bod 2006a, b)
Procedure
1 Generate all possible binary trees on example sentences
2 Extract all subtrees
3 Estimate probabilities on subtrees using EM
142
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subtrees
S
NP
PN
VP
V NP
S
NP VP
V NP
S
NP
PN
VP
S
NP VP
VP
V NP
NP
PN
Remove either all or no elements on a level
Leads to many subtrees
Each subtree receives a probability
Longer distance dependencies may be modeled
143
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Parsing
Subtrees can be recombined into a larger tree
Similar to context-free grammar rules
Same parse may be created using different derivations
Statistical model has to take this into account
Example
S
NP VP
V NP
? NP
PN
? NP
PN
= S
NP
PN
VP
V NP
PN
144
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying idea
U-DOP works because span of subtrees reoccur in a corpus
Likelihood of ?useful? spans increase
Hence, likelihood of contexts (also subtrees) increase
Essentially, U-DOP uses implied substitutability
while system leans heavily on probabilities
145
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation
Base
treebank
Extract
sentences
Compare
treebanks
Results
Plain
corpus
Learning
system
Learned
treebank
Recall (completeness)
Precision (correctness)
F-Score (combination of Precision and Recall)
(van Zaanen and Adriaans 2001)
146
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation settings
Air Travel Information System (ATIS)
Taken from Penn Treebank II
568 English sentences
Example
list the flights from baltimore to seattle that stop in minneapolis
does this flight serve dinner
the flight should arrive at eleven a.m. tomorrow
what airline is this
147
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
Micro Macro Macro2
Precision 47.01 46.18 46.18
Recall 44.94 50.98 50.98
F-Score 44.60 47.10 48.46
Explanation
Micro Count constituents, weighted average per sentence
Macro Count constituents and average per sentence
Macro2 Compute Macro Precision/Recall, average at end
148
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
remove remove remove
sentence empty both
Micro Precision 47.01 47.67 77.10 79.07
Micro Recall 44.94 45.30 44.95 45.29
Micro F-Score 44.60 45.09 55.31 56.13
Macro Precision 46.18 47.66 77.08 81.18
Macro Recall 50.98 52.96 51.07 52.80
Macro F-Score 47.10 48.62 60.00 62.47
Macro2 F-Score 48.46 50.17 61.43 63.99
Example
(bla bla bla)?bla bla bla
bla () bla ?bla bla
(bla () bla) ?bla bla
149
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation insights
No standard evaluation exists
but de facto evaluation datasets arise
ATIS (van Zaanen and Adriaans 2001)
WSJ10, WSJ40 (WSJ with sentence length limitations)
NEGRA10 (German)
CTB10 (Chinese)
Systems have different input/output
Evaluation settings influence results
Different metrics (micro/macro/macro2)
Included constituents (sentence/empty)
Formal grammatical inference does not have this problem
Evaluation performed through formal proofs
150
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Context-sensitive grammars
Learning context-free grammars is hard
Is learning context-sensitive grammars impossible?
That depends
To what degree is the grammar context-sensitive?
We may not need ?full? context-sensitiveness
Grammar rules: ?A? ? ???
Mildly context-sensitive grammars may be enough for NL
(Huybrechts 1984, Shieber 1985)
Perhaps the full power of context-freeness is not needed
151
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
RegCFCSUnres b
Family to learn
152
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning context-sensitive languages
Open research area
Some work has already been done
Augmented Regular Expressions (Alque?zar 1997)
Variants of substitutability (Yoshinaka 2009)
Distributional Lattice Grammars (Clark 2010)
153
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Relationship between empirical and formal GI
Is there a relationship between empirical GI and formal GI?
Example: consider the case of substitutability
There are situations in which substitutability breaks:
John eats meat
John eats much
This suggests that learning based on substitutability
learns a different family of languages (not CFG)
Non-terminally separated (NTS) languages
Subclass of deterministic context-free grammars
154
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
Grammar G=??,V ,P ,S? is NTS
? is vocabulary
V is set of non-terminals
P is set of production rules
S ? V is the start symbol
Additional restriction:
If N ? V
N ?? ???
M ?? ?
then N ?? ?M?
In other words:
non-terminals correspond exactly with substitutability
(Clark and Eyraud 2005, Clark 2006, Clark and Eyraud 2007)
155
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
It can be shown that NTS grammars are
identifiable in the limit
PAC learnable
Unfortunately, natural language is not an NTS language
Ultimate goal:
Find family of languages that fits natural language
and is learnable in the right learning setting
156
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal GI and empirical GI
Relation between formal GI and empirical GI
Formal GI can show learnability
Under certain conditions
Emprical GI tries to learn structure from real data
Practically shows possibilities and limitations
Ultimate aim: Find family of languages that is
learnable under different conditions
fits natural languages
157
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CONCLUSIONS
1 There have been new strong positive results in a recent past
for all the cases mentioned (subclasses of regular, PFA,
transducers, CFGs, MCSGs)
2 Look for ICGI! It?s the conference where these exciting results
happen (as well as exciting challenges, competitions,
benchmarks etc.)
3 The use of GI techniques both in computational linguistics
and natural language processing is taking place.
4 The future is bright!
158
References
P. W. Adriaans and M. van Zaanen. 2004. Computational
grammar induction for linguists. Grammars, 7:57?68.
Special issue with the theme ?Grammar Induction?.
P. W. Adriaans and M. van Zaanen. 2006. Computa-
tional grammatical inference. In D. E. Holmes and
L. C. Jain, editors, Innovations in Machine Learning,
volume 194 of Studies in Fuzziness and Soft Com-
puting, chapter 7. Springer-Verlag, Berlin Heidelberg,
Germany. To be published. ISBN: 3-540-30609-9.
P. W. Adriaans and M. Vervoort. 2002. The EMILE 4.1
grammar induction toolbox. In P. W. Adriaans, H. Fer-
nau, and M. van Zaanen, editors, Grammatical Infer-
ence: Algorithms and Applications (ICGI); Amster-
dam, the Netherlands, volume 2482 of Lecture Notes
in AI, pages 293?295, Berlin Heidelberg, Germany,
September 23?25. Springer-Verlag.
P. W. Adriaans. 1992. Language Learning from a Cat-
egorial Perspective. Ph.D. thesis, University of Ams-
terdam, Amsterdam, the Netherlands, November.
R. Alque?zar and A. Sanfeliu. 1997. Recognition and
learning of a class of context-sensitive languages de-
scribed by augmented regular expressions. Pattern
Recognition, 30(1):163?182.
D. Angluin and M. Kharitonov. 1991. When won?t mem-
bership queries help? In Proceedings of 24th ACM
Symposium on Theory of Computing, pages 444?454,
New York. ACM Press.
D. Angluin. 1981. A note on the number of queries
needed to identify regular languages. Information and
Control, 51:76?87.
D. Angluin. 1982. Inference of reversible languages.
Journal for the Association of Computing Machinery,
29(3):741?765.
D. Angluin. 1987a. Learning regular sets from
queries and counterexamples. Information and Con-
trol, 39:337?350.
D. Angluin. 1987b. Queries and concept learning. Ma-
chine Learning Journal, 2:319?342.
D. Angluin. 1988. Identifying languages from stochas-
tic examples. Technical Report YALEU/DCS/RR-614,
Yale University, March.
R. B. Applegate. 1972. Inesen?o Chumash Grammar.
Ph.D. thesis, University of California, Berkeley.
L. Beccera-Bonache, C. Bibire, and A. Horia Dediu.
2005. Learning DFA from corrections. In Henning
Fernau, editor, Proceedings of the Workshop on The-
oretical Aspects of Grammar Induction (TAGI), WSI-
2005-14, pages 1?11. Technical Report, University of
Tu?bingen.
L. Becerra-Bonache, C. de la Higuera, J. C. Janodet, and
F. Tantini. 2008. Learning balls of strings from edit
corrections. Journal of Machine Learning Research,
9:1841?1870.
D. Be?chet, A. Dikovsky, and A. Fore?t. 2011. Sur
les ite?rations disperse?es et les choix itr?e?s pour
l?apprentissage incre?mental des types dans les gram-
maires de de?pendances. In Proceedings of Confe?rence
d?Apprentissage.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K.
Warmuth. 1989. Learnability and the Vapnik-
Chervonenkis dimension. J. ACM, 36(4):929?965.
R. Bod. 1998. Beyond Grammar?An Experience-
Based Theory of Language, volume 88 of CSLI Lec-
ture Notes. Center for Study of Language and Infor-
mation (CSLI) Publications, Stanford:CA, USA.
R. Bod. 2006a. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the 21st International
Conference on Computational Linguistics (COLING)
and 44th Annual Meeting of the Association of Com-
putational Linguistics (ACL); Sydney, Australia, pages
865?872. Association for Computational Linguistics.
R. Bod. 2006b. Unsupervised parsing with u-dop. In
CoNLL-X ?06: Proceedings of the Tenth Conference
on Computational Natural Language Learning, pages
85?92, Morristown, NJ, USA. Association for Com-
putational Linguistics.
R. C. Carrasco and J. Oncina. 1994. Learning stochastic
regular grammars by means of a state merging method.
In R. C. Carrasco and J. Oncina, editors, Grammatical
Inference and Applications, Proceedings of ICGI ?94,
number 862 in LNAI, pages 139?150. Springer-Verlag.
E. Charniak. 1993. Statistical Language Learning.
Massachusetts Institute of Technology Press, Cam-
bridge:MA, USA and London, UK.
N. Chater and P. Vita?nyi. 2007. ?ideal learning? of natu-
ral language: Positive results about learning from pos-
itive evidence. Journal of Mathematical Psychology,
51(3):135?163.
N. Chomsky. 1957. Syntactic Structures. Mouton & Co.,
Printers, The Hague.
A. Clark and R. Eyraud. 2005. Identification in the limit
of substitutable context-free languages. In S. Jain,
H. U. Simon, and E. Tomita, editors, Algorithmic
Learning Theory: 16th International Conference, ALT
2005, volume 3734 of Lecture Notes in Computer Sci-
ence, pages 283?296, Berlin Heidelberg, Germany.
Springer-Verlag.
A. Clark and R. Eyraud. 2007. Polynomial identification
in the limit of substitutable context-free languages.
Journal of Machine Learning Research, 8:1725?1745.
A. Clark and F. Thollard. 2004. Pac-learnability of prob-
abilistic deterministic finite state automata. Journal of
Machine Learning Research, 5:473?497.
A. Clark. 2006. PAC-learning unambiguous NTS lan-
guages. In Y. Sakakibara, S. Kobayashi, K. Sato,
T. Nishino, and E. Tomita, editors, Eighth Interna-
tional Colloquium on Grammatical Inference, (ICGI);
Tokyo, Japan, number 4201 in Lecture Notes in AI,
pages 59?71, Berlin Heidelberg, Germany. Springer-
Verlag.
A. Clark. 2010. Efficient, correct, unsupervised learn-
ing of context-sensitive languages. In CoNLL ?10:
Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 28?37,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
C. de la Higuera and J. Oncina. 2004. Learning proba-
bilistic finite automata. In G. Paliouras and Y. Sakak-
ibara, editors, Grammatical Inference: Algorithms and
Applications, Proceedings of ICGI ?04, volume 3264
of LNAI, pages 175?186. Springer-Verlag.
C. de la Higuera and F. Thollard. 2000. Identification in
the limit with probability one of stochastic determinis-
tic finite automata. In A.L. de Oliveira, editor, Gram-
matical Inference: Algorithms and Applications, Pro-
ceedings of ICGI ?00, volume 1891 of Lecture Notes
in Computer Science, pages 15?24. Springer-Verlag.
C. de la Higuera, J.-C. Janodet, and F. Tantini. 2008.
Learning languages from bounded resources: the case
of the DFA and the balls of strings. In A. Clark,
F. Coste, and L. Miclet, editors, Grammatical In-
ference: Algorithms and Applications, Proceedings
of ICGI ?08, volume 5278 of LNCS, pages 43?56.
Springer-Verlag.
C. de la Higuera. 1997. Characteristic sets for polyno-
mial grammatical inference. Machine Learning Jour-
nal, 27:125?138.
C. de la Higuera. 2010. Grammatical inference: learn-
ing automata and grammars. Cambridge University
Press, Cambridge, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel
Smith, Molly Visscher, and David Wellcome. 2008.
Deciding strictly local (SL) languages. In Jon Breit-
enbucher, editor, Proceedings of the Midstates Con-
ference for Undergraduate Research in Computer Sci-
ence and Mathematics, pages 66?73.
Jie Fu, J. Heinz, and Herbert Tanner. 2011. An alge-
braic characterization of strictly piecewise languages.
In The 8th Annual Conference on Theory and Applica-
tions of Models of Computation, volume 6648 of Lec-
ture Notes in Computer Science. Springer-Verlag.
P. Garc??a and J. Ruiz. 2004. Learning k-testable
and k-piecewise testable languages from positive data.
Grammars, 7:125?140.
P. Garcia and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12:920?925.
P. Garcia, E. Vidal, and J. Oncina. 1990. Learning lo-
cally testable languages in the strict sense. In Proceed-
ings of the Workshop on Algorithmic Learning Theory,
pages 325?338.
G.Clements and J. Keyser. 1983. CV phonology: a gen-
erative theory of the syllable. Cambridge, MA: MIT
Press.
J. Geertzen and M. van Zaanen. 2004. Grammati-
cal inference using suffix trees. In G. Paliouras and
Y. Sakakibara, editors, Grammatical Inference: Algo-
rithms and Applications: Seventh International Collo-
quium, (ICGI); Athens, Greece, volume 3264 of Lec-
ture Notes in AI, pages 163?174, Berlin Heidelberg,
Germany, October 11?13. Springer-Verlag.
D. Gildea and D. Jurafsky. 1996. Learning bias and
phonological-rule induction. Computational Linguis-
tics, 24(4).
L. Gleitman. 1990. The structural sources of verb mean-
ings. Language Acquisition, 1(1):3?55.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10(5):447?474.
E. M. Gold. 1978. Complexity of automaton identi-
fication from given data. Information and Control,
37:302?320.
K. C. Hansen and L. E. Hansen. 1969. Pintupi phonol-
ogy. Oceanic Linguistics, 8:153?170.
Z. S. Harris. 1951. Structural Linguistics. University of
Chicago Press, Chicago:IL, USA and London, UK, 7th
(1966) edition. Formerly Entitled: Methods in Struc-
tural Linguistics.
B. Hayes. 1995. Metrical Stress Theory. Chicago Uni-
versity Press.
J. Heinz and J. Rogers. 2010. Estimating strictly piece-
wise distributions. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 886?896, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
J. Heinz. 2008. Left-to-right and right-to-left iterative
languages. In Alexander Clark, Franc?ois Coste, and
Lauren Miclet, editors, Grammatical Inference: Al-
gorithms and Applications, 9th International Collo-
quium, volume 5278 of Lecture Notes in Computer
Science, pages 84?97. Springer.
J. Heinz. 2009. On the role of locality in learning stress
patterns. Phonology, 26(2):303?351.
J. Heinz. 2010. String extension learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 897?906, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
R. M. A. C. Huybrechts. 1984. The weak adequacy
of context-free phrase structure grammar. In G. J.
de Haan, M. Trommelen, and W. Zonneveld, editors,
Van periferie naar kern, pages 81?99. Foris, Dor-
drecht, the Netherlands.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632, December.
A. Kasprzik and T. Ko?tzing. 2010. String extension
learning using lattices. In Henning Fernau Adrian-
Horia Dediu and Carlos Mart??n-Vide, editors, Pro-
ceedings of the 4th International Conference on Lan-
guage and Automata Theory and Applications (LATA
2010), volume 6031 of Lecture Notes in Computer Sci-
ence, pages 380?391, Trier, Germany. Springer.
M. Kearns and L. Valiant. 1989. Cryptographic lim-
itations on learning boolean formulae and finite au-
tomata. In 21st ACM Symposium on Theory of Com-
puting, pages 433?444.
M. J. Kearns and U. Vazirani. 1994. An Introduction to
Computational Learning Theory. MIT press.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In 40th Annual Meeting of the Association
for Computational Linguistics; Philadelphia:PA, USA,
pages 128?135. Association for Computational Lin-
guistics, July. yes.
D. Klein. 2004. Corpus-based induction of syntactic
structure: Models of dependency and constituency. In
42th Annual Meeting of the Association for Computa-
tional Linguistics; Barcelona, Spain, pages 479?486.
G. Kobele. 2006. Generating Copies: An Investigation
into Structural Identity in Language and Grammar.
Ph.D. thesis, University of California, Los Angeles.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4(35?56).
R. McNaughton and S. Papert. 1971. Counter-Free Au-
tomata. MIT Press.
M. Mohri. 1997. Finite-state transducers in language
and speech processing. Computational Linguistics,
23(2):269?311.
S. Muggleton. 1990. Inductive Acquisition of Expert
Knowledge. Addison-Wesley.
J. Oncina and P. Garc??a. 1992. Identifying regular lan-
guages in polynomial time. In H. Bunke, editor, Ad-
vances in Structural and Syntactic Pattern Recogni-
tion, volume 5 of Series in Machine Perception and
Artificial Intelligence, pages 99?108. World Scientific.
J. Oncina, P. Garc??a, and E. Vidal. 1993. Learning sub-
sequential transducers for pattern recognition tasks.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15:448?458, May.
L. Pitt. 1985. Probabilistic Inductive Inference. Ph.D.
thesis, Yale University. Computer Science Depart-
ment, TR-400.
L. Pitt. 1989. Inductive inference, DFA?s, and compu-
tational complexity. In Analogical and Inductive In-
ference, number 397 in LNAI, pages 18?44. Springer-
Verlag.
J. Rogers and G. Pullum. to appear. Aural pattern recog-
nition experiments and the subregular hierarchy. Jour-
nal of Logic, Language and Information.
J. Rogers, J. Heinz, Gil Bailey, Matt Edlefsen, Molly
Visscher, David Wellcome, and Sean Wibel. 2010.
On languages piecewise testable in the strict sense. In
Christian Ebert, Gerhard Ja?ger, and Jens Michaelis,
editors, The Mathematics of Language, volume 6149
of Lecture Notes in Artifical Intelligence, pages 255?
265. Springer.
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
I. Simon. 1975. Piecewise testable events. In Automata
Theory and Formal Languages, pages 214?222.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. Proceed-
ings of the National Academy of Sciences of the United
States of America, 102(33):11629?11634, August.
A. Stolcke. 1994. Bayesian Learning of Probabilistic
Language Models. Ph.D. thesis, University of Califor-
nia, Berkeley.
I. Tellier. 2008. How to split recursive automata. In
ICGI, pages 200?212.
E. Ukkonen. 1995. On-line construction of suffix trees.
Algorithmica, 14:249?260.
L. G. Valiant. 1984. A theory of the learnable. Commu-
nications of the Association for Computing Machinery,
27(11):1134?1142.
M. van Zaanen and P. W. Adriaans. 2001. Alignment-
Based Learning versus EMILE: A comparison. In
Proceedings of the Belgian-Dutch Conference on Ar-
tificial Intelligence (BNAIC); Amsterdam, the Nether-
lands, pages 315?322, October.
M. van Zaanen. 2000a. ABL: Alignment-Based
Learning. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING);
Saarbru?cken, Germany, pages 961?967. Association
for Computational Linguistics, July 31?August 4.
M. van Zaanen. 2000b. Bootstrapping syntax and recur-
sion using Alignment-Based Learning. In P. Langley,
editor, Proceedings of the Seventeenth International
Conference on Machine Learning; Stanford:CA, USA,
pages 1063?1070, June 29?July 2.
M. van Zaanen. 2002. Bootstrapping Structure into Lan-
guage: Alignment-Based Learning. Ph.D. thesis, Uni-
versity of Leeds, Leeds, UK, January.
Marco R. Vervoort. 2000. Games, Walks and Grammars.
Ph.D. thesis, University of Amsterdam, Amsterdam,
the Netherlands, September.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005a. Probabilistic finite-state
machines-part I. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1013?1025.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005b. Probabilistic finite-state
machines-part II. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1026?1039.
R. A. Wagner and M. J. Fischer. 1974. The string-to-
string correction problem. Journal of the Association
for Computing Machinery, 21(1):168?173.
R. Wiehagen, R. Frievalds, and E. Kinber. 1984. On the
power of probabilistic strategies in inductive inference.
Theoretical Computer Science, 28:111?133.
T. Yokomori. 2003. Polynomial-time identification of
very simple grammars from positive data. Theoretical
Computer Science, 298(1):179?206.
R. Yoshinaka. 2009. Learning mildly context-sensitive
languages with multidimensional substitutability from
positive data. In R. Gavalda`, G. Lugosi, T. Zeugmann,
and S. Zilles, editors, Proceedings of the Workshop on
Algorithmic Learning Theory, volume 5809 of Lecture
Notes in Computer Science, pages 278?292. Springer
Berlin / Heidelberg.
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 20?30,
Dublin, Ireland, August 24 2014.
Automatic Compound Processing: Compound Splitting and Semantic
Analysis for Afrikaans and Dutch
Ben Verhoeven
CLiPS - Computational Linguistics
University of Antwerp
Antwerp, Belgium
ben.verhoeven@uantwerp.be
Walter Daelemans
CLiPS - Computational Linguistics
University of Antwerp
Antwerp, Belgium
walter.daelemans@uantwerp.be
Menno van Zaanen
TiCC, School of Humanities
Tilburg University
Tilburg, the Netherlands
mvzaanen@uvt.nl
Gerhard van Huyssteen
Centre for Text Technology (CTexT)
North-West University
Potchefstroom, South Africa
gerhard.vanhuyssteen@nwu.ac.za
Abstract
Compounding, the process of combining several simplex words into a complex whole, is a pro-
ductive process in a wide range of languages. In particular, concatenative compounding, in
which the components are ?glued? together, leads to problems, for instance, in computational
tools that rely on a predefined lexicon. Here we present the AuCoPro project, which focuses
on compounding in the closely related languages Afrikaans and Dutch. The project consists of
subprojects focusing on compound splitting (identifying the boundaries of the components) and
compound semantics (identifying semantic relations between the components). We describe the
developed datasets as well as results showing the effectiveness of the developed datasets.
1 Introduction
In many human language technology applications (e.g. machine translators and spelling checkers), many
concatenatively written compounds are processed incorrectly. One of the reasons for this is that these
applications rely on a predefined lexicon and the productive nature of the process of compound formation
automatically results in incomplete lexicons. For example, consider the novel Afrikaans (Afr.) compound
ministerskatkis ?treasury of a minister? that should be segmented as minister+skatkis minister+treasury.
Should it be incorrectly segmented as minister s+kat+kis minister LINK+cat+coffin
1
(where LINK
refers to a linking morpheme), one would get the (possible but improbable) interpretation ?coffin of a
minister?s cat?. From a technological perspective, deficiencies related to automatic compound splitting
(also known as compound segmentation) are particularly problematic, since many other technologies
(such as morphological analyzers, or semantic parsers) might rely on highly accurate compound splitting.
For more advanced natural language processing applications like information extraction, question an-
swering and machine translation systems, proper semantic analysis of compounds might also be required.
With semantic analysis of compounds we refer to the task of determining that the Dutch (Du.) compound
keuken+tafel kitchen+table construes ?table in kitchen?, while Du. baby+tafel baby+table means ?table
for a baby? (and not, fatally so, *?table in a baby?). Internationally, research on automatic compound
analysis has focused almost exclusively on English; very little work in this regard has been done for
other languages (see section 4.1).
Concatenative compounding is a highly productive process in many languages of the world, such as
West-Germanic languages (Afrikaans, Dutch, Frisian, German, and to a far lesser extent English), Nordic
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that compound boundaries are marked using a ?+? sign and the start of a linking morpheme is indicated by an ? ?
sign.
20
languages (Danish, Icelandic, Norwegian, and Swedish) and Modern Greek; our focus in this research
is only on Afrikaans and Dutch. Next to derivation, the process of right-headed, recursive compounding
is the most productive word-formation process in these two languages. While almost all parts-of-speech
categories can be found as components of compounds, noun+noun compounds are by far the most fre-
quent type, while noun+verb compounding is generally considered to be non-productive in Germanic
languages (Don, 2009, p. 378). Components of a compound sometimes need to be ?glued? together
using linking morphemes. The occurrence of linking morphemes in Afrikaans and Dutch compounds is
well-known (Neijt et al., 2010), like Afr. besigheid s+besluit business LINK+decision ?business deci-
sion?.
Besides regular compounding, one also finds, amongst others, phrasal compounds (e.g. Afr.
help-my-fris-lyk-hemp help-me-strong-look-shirt ?gym vest?), (neo)classical compounds (e.g. Afr.
neuro+wetenskap neuro+science ?neuroscience?, or Du. bio+logie bio+logy ?biology?), separable ver-
bal compounds (e.g. Du. op+bellen up+call ?to phone?), reduplicative compounds (e.g. Afr. speel -
+speel play LINK+play ?easily?), and compounding compounds (i.e. where the two left constituents
are normally a phrase, but joined in a compound through the right-most constituent, e.g. Du. on-
der+water+camera under+water+camera ?under-water camera?). Except for the latter, none of these
marginal types of compounds were considered as data for any of the systems developed in this research
project.
In section 2 we provide an overview of the automatic compound processing (AuCoPro) project, which
forms the background of this research. Sections 3 and 4 provide details of each of the subprojects relating
to compound splitting and semantic analysis, with details about related research, the development of
datasets, and our experiments. We conclude with a discussion of results and future work in section 5.
2 Overview: The AuCoPro Project
Running from 2012 to 2013, the AuCoPro project was funded by the Dutch Language Union and the
Department of Arts and Culture of the South African Government in a programme to support collab-
orative research in human language technology between Belgium, The Netherlands and South Africa.
Additional funding was provided by the South African National Research Foundation, and the European
Network on Word Structure (NetWordS). The partners involved in the project were the University of
Antwerp (Belgium), Tilburg University (The Netherlands), and North-West University (South Africa).
The primary aim of the project was to develop resources (including annotation protocols, and training
and testing data) for the development of robust compound splitters (subproject 1), and first-generation
compound analyzers (subproject 2) for Afrikaans and Dutch, through a combination of cross-language
transfer (allowing technology recycling), data pooling, and various machine learning approaches. In a
subpart of subproject 2 we also aimed to gain insight in compound semantics by unifying perspectives
from computational semantics (
?
O S?eaghdha, 2008), typological studies (Scalise and Bisetto, 2009), and
construction-based approaches to word-formation (specifically cognitive grammar (Langacker, 2008)
and construction morphology (Booij, 2010)); the results of which can be found in Van Huyssteen (2014)
and Van Huyssteen and Verhoeven (2014).
Deliverables included eight peer-reviewed publications, a technical report on annotation guidelines
for compound processing, and six datasets. All deliverables are available in the open-source domain at
https://sourceforge.net/projects/aucopro, while more information about the project
is available at http://tinyurl.com/aucopro.
3 Compound Splitting
The aim of subproject 1 was to develop datasets that can be used to build robust compound splitters
for Afrikaans and Dutch, or for a cross-lingual analysis of the use of compounds in the closely related
languages Afrikaans and Dutch. Based on existing datasets containing words that are morphologically
analyzed, we extracted (potential) compounds, removed unwanted morphological information, and re-
analysed and corrected them.
21
In the AuCoPro datasets, compounds are analyzed in a shallow manner: no deep hierarchical ordering
of components is performed. Compounds consisting of more than two elements are annotated by indicat-
ing the location of the boundaries, so for instance, Du. bloem+boll en+veld flower+bulb LINK+field
?bulb field? consists of four components, viz. bloem, boll-, -en-, and veld, without any indication of
their syntagmatic relations. The parts bloem, boll- and veld are all simplex words, which we will call
constituents. Constituents are the meaningful parts of a compound. These constituents are prototyp-
ically independent words, but in some cases affixoids (i.e. forms that are somewhere between a word
and an affix in its development) can also occur in compounds (e.g. boer in Du. krant en+boer newspa-
per LINK+farmer ?newspaper seller? does not have the literal meaning of farmer; see Booij (2010)).
In some cases a word may undergo morphophonological changes in the context of a compound. For
instance, in the bloembollenveld example, boll- is an allomorph (or allograph) of bol ?bulb?.
As mentioned above, some compounds require linking morphemes (indicated by LINK in the exam-
ples above) to ?glue? components together. Besides ordinary linking morphemes like -e-, -en-, and -s- (in
both languages), we also defined hyphens as linking morphemes. In the orthographies of Afrikaans and
Dutch in general a hyphen is used in cases of vowel collision, i.e. between compound constituents when
the left-hand constituent ends on a vowel, and the right-hand constituent begins with the same vowel, for
example Afr. see -+eend sea LINK+duck ?seaduck?.
We also mentioned above that marginal compound types such as phrasal compounds, reduplicative
compounds, separable verbal compounds, etc. were not considered as part of the datasets. Similarly, we
excluded synthetic compounds from the datasets when the right-hand element of a synthetic compound
is a non-word (e.g. in Du. blauw+ogig blue+eye-ADJR
2
?blue-eyed?, *ogig is not a valid independent
word in Dutch). However, for this subproject we accepted and annotated compounding compounds,
since they can generally be split quite easily (e.g. Afr. drie+vlak+regering three+level+government
?three-level government?).
To demonstrate the effectiveness of the developed datasets, we started building and evaluating com-
pound splitters for both Dutch and Afrikaans based on the data only. A compound splitter takes a word
as input, and provides as output the input string divided into valid compound components. Note that
these results are only to illustrate that these datasets can be used successfully as training data for such
systems. The actual results can potentially be improved, as the systems are not optimized.
3.1 Related Research
In general, the problem of splitting compounds is found in a wide range of languages. Some of these
languages show non-concatenative compound formation (i.e. compounds are written with whitespaces
between constituents), such as English. Compounds in these languages fall under the umbrella term
multiword expressions (MWEs), which also includes idioms and collocations. Ramisch et al. (2013)
show that this is a quite active research field.
Focusing on concatenative compounding (i.e. where constituents are written conjunctively so that a
compound is always written as a single string without any whitespaces), previous work on Afrikaans has
been performed in the context of the development of spelling checkers (Van Zaanen and Van Huyssteen,
2002; Van Huyssteen and Van Zaanen, 2004). Van Huyssteen and Van Zaanen (2004) describe a com-
pound splitter for Afrikaans. To our knowledge, no stand-alone compound splitter for Dutch is available.
Research done in this field is over ten years old (e.g. Pohlmann and Kraaij (1996)), uses expensive re-
sources (e.g. Ordelman et al. (2003)), does complete morphological analysis (e.g. De Pauw et al. (2004)),
and/or has not been released for re-use in the open-source domain.
3.2 Dataset Development
The datasets developed during this subproject are based on compounds taken from existing (morpho-
logically annotated) datasets. For Dutch, a few morphologically annotated datasets exist, although none
focus on compounds specifically. The development of the Dutch dataset is based on the e-Lex dataset.
3
2
Adjectiviser.
3
This dataset was extended with a compound dataset extracted from CELEX by Lieve Macken (LT3, UGent).
22
The e-Lex dataset contains words annotated with more morphological information than required for
our dataset, but it also contains morphologically annotated non-compound words. After removing non-
compound words (and removing duplicates), 71,274 potential Dutch compounds remained.
For Afrikaans, the situation is more difficult. No dataset containing compound boundary and linking
morpheme boundary information is freely available. The Afrikaans AuCoPro dataset is based on the
PUK-Protea corpus as well as the CTexT Afrikaans spelling checking lexicon (CTexT, 2005; Pilon et al.,
2008). Both corpora do not describe any morphological information. To identify potential compounds,
a longest string matching algorithm (Van Huyssteen and Van Zaanen, 2004) is applied. This algorithm
identifies compounds by searching for known (simplex) words from the left and right ends of the potential
compound, taking the possibility of the occurrence of linking morphemes into account. This algorithm
seems to identify most compounds as well as some non-compounds, which resulted in a list of 77,651
potential Afrikaans compounds.
After this automatic collection and cleanup (for Dutch) and automatic identification and annotation (for
Afrikaans), annotators checked each compound for correct linking morpheme and compound boundaries.
For Afrikaans, seven annotators together checked 25,266 compounds. For Dutch, two annotators checked
26,000 potential compounds. In the end, this resulted in 18,497 and 21,997 true compounds for Afrikaans
and Dutch respectively.
To be able to calculate inter-annotator agreement, subsets of approximately 1,000 words were an-
notated by pairs of annotators. For Dutch in total 6,000 words were used to calculate inter-annotator
agreement and for Afrikaans 12,818 words. This leads to an average Cohen?s Kappa of 98.6 and 97.6 for
Afrikaans and Dutch respectively.
The annotators had access to an annotation manual (Verhoeven et al., 2014), which was developed
specifically for this project. The manual is based on the annotation guidelines that were developed
during the CKarma project (CTexT, 2005; Pilon et al., 2008). These initial guidelines only apply to
Afrikaans, and was hence extended to handle Dutch compounds as well as more complicated cases
not foreseen in the original CKarma guidelines. During the annotation process, regular discussions
between the annotators took place, which resulted in changes in the data and (minor) modifications to
the annotation guidelines.
3.3 Experiments
One of the reasons for creating the compound splitting datasets is to show their usefulness in the de-
velopment of automatic compound splitting systems. These systems search for compound boundaries,
effectively identifying the simplex words in compounds. This information is essential, for instance, when
developing spelling correction systems or machine translation systems for languages that have productive
compound formation processes.
As a classifier, we used the algorithm developed by Liang (1983). This system, which is used as
the hyphenation method in the L
A
T
E
X typesetting system, identifies letter combinations that either allow
or disallow boundary breaks. Even though the task of compound boundary detection is different from
hyphenation (or syllabification), the tasks are similar enough to use the same method. Since the system
is trainable, instead of hyphenation breaks, compound boundaries are provided.
Since no separate annotated gold standard test set is available, we performed leave-one-out evaluation
(using all but one instance for training and the remaining instance for testing; all instances are evaluated
once) using the full dataset. This approach is preferred over, for instance, 10-fold cross validation, which
each time removes 10% of the training data for testing. Additionally, it does not depend on a ?lucky?
selection of test data from the training data, as all compounds are tested.
Evaluating the datasets using this system (which does not have any additional tuning parameters)
results in classification accuracies of 88.28% and 91.48% on the word level for Afrikaans and Dutch
respectively. We assume that further improvements are possible with alternative systems and parameter
optimization.
23
4 Compound Semantics
The automatic processing of the semantics of compounds (or other complex nominals) is a topic in
computational linguistics that, although it has been studied regularly in the past, cannot be considered
a solved problem. Although previous research was often promising, it also had an almost exclusive
focus on English noun-noun (NN) compounds. In recent years, more languages have been studied (e.g.
German (Hinrichs et al., 2013) and Italian (Celli and Nissim, 2009)), and this project added Dutch and
Afrikaans to the list.
It is worth noting that a number of different operationalizations of compound interpretation have been
studied. The most notable are semantic classification of the constituent relation according to a limited
set of semantic categories (e.g.
?
O S?eaghdha (2008)), and the generation of possible paraphrases for
the compound that express its meaning more explicitly (Hendrickx et al., 2013). Our study adopts the
classification model, in which the set of semantic relations to be predicted (the classification scheme) is
crucial.
4.1 Related Research
Several attempts have been made in the past to postulate appropriate classification schemes for noun-
noun compound semantics. These schemes are mainly inventory-based in that they present a limited list
of predefined possible classes of semantic relations a compound can manifest.
In some cases, proposed classes are abstractly represented by a paraphrasing preposition (Lauer, 1995;
Girju et al., 2005; Lapata and Keller, 2004). For example, all compounds that can be paraphrased by
putting the preposition ?of? between the constituents belong to the class OF, e.g. a car door is the
?door of a car?. Another possibility is using predicate-based classes where the relations between the
constituents are not merely described by a preposition, but by definitions or paraphrasing predicates for
each class. The class AGENT would contain compounds that could be paraphrased as ?X is performed
by Y? (Kim and Baldwin, 2005), e.g. enemy activity can be paraphrased as ?activity is performed by
the enemy?. Different schemes vary from 9 to 43 classes with Cohen?s Kappa scores for inter-annotator
agreement ranging from 52% to 62% (Barker and Szpakowicz, 1998; Girju et al., 2005; Moldovan et al.,
2004; Nakov, 2008;
?
O S?eaghdha, 2008).
With regard to the information used by the classifier to assign the classes to the compounds (the fea-
tures of a compound to be analyzed), two main approaches are available, viz. taxonomy-based methods,
or corpus-based methods.
Taxonomy-based methods (also called semantic network similarity (
?
O S?eaghdha, 2009)) base their fea-
tures on a word?s location in a taxonomy or hierarchy of terms. Most of the taxonomy-based techniques
use WordNet (Miller, 1995) for these purposes; especially the hyponym information in the hierarchy
is used. A bag of words is created of all hyponyms and the instance vector contains binary values for
each feature (the feature being whether the considered word from the bag of words is a hyponym of the
constituent or not). Kim and Baldwin (2005) reached an accuracy of 53.3% using only WordNet. Other
research was based on Wikipedia as a semantic network (Strube and Ponzetto, 2006).
Corpus-based methods use co-occurrence information of the constituents of the selected compounds
in a corpus. The underlying idea (the distributional hypothesis) is that the set of contexts in which a
word occurs, is an implicit representation of the semantics of this word (Harris, 1968). The lexical sim-
ilarity measure assumes that compounds have a similar semantic interpretation when their respective
constituents are semantically similar. Two compounds, for example flour can and corn bag will be con-
sidered similar if they have similar modifying constituents (flour and corn) and similar head constituents
(can and bag). The co-occurrences of both constituents will be combined to calculate a measure of sim-
ilarity for the entire compound. This approach implicitly uses the lexical semantic knowledge also used
in taxonomy-based methods but without the need for a taxonomy. Performances of up to 64% F-score
have been reached (
?
O S?eaghdha and Copestake, 2013).
Corpus-based and taxonomy-based methods have also been combined by several researchers. Accu-
racies of 58.35% (
?
O S?eaghdha, 2007), 73.9% (Tratz and Hovy, 2010) and even 82.47% (Nastase et al.,
2006) were reported.
24
4.2 Dataset Development
For this project, we developed datasets of semantically annotated compounds for Afrikaans and Dutch.
This section describes these new resources.
The annotation scheme and guidelines that we used as basis, were developed by
?
O S?eaghdha (2008)
for semantic annotation of English NN compounds. For purposes of our project, some adaptations were
in order, while Dutch and Afrikaans examples were added (Verhoeven et al., 2014).
?
O S?eaghdha (2008)
describes eleven classes of compounds; six of these classes are semantically specific (see Table 1).
Class Definition Example
BE
The compound can be rewritten as ?N2 which is (like) (a) N1? with N1
and N2 being the two constituents nouns.
woman doctor
HAVE
The compound denotes some sort of possession. Part-whole com-
pounds, typical one-to-many possession, compounds expressing condi-
tions or properties and meronymic compounds belong here.
car door
IN The compound denotes a location in time or place. garden party
ACTOR
The compound denotes a characteristic event or situation and one of the
constituents is a salient entity.
enemy activity
INST
The compound denotes a characteristic event and there is no salient en-
tity present.
cheese knife
ABOUT The compound describes a topical relation between its constituents. film character
Table 1: Overview of semantically specific categories in the semantics annotation scheme.
The other five categories are less specific. The MISTAG and NONCOMPOUND categories serve
to classify compounds that do not belong in the dataset. The REL class describes compounds with a
clear meaning that does not belong to any of the other classes, but of which the relation between the
constituents seems productive (e.g. sodium chloride). The LEX category is almost the same as REL,
but the relation does not seem to be productive (e.g. monkey business). The UNKNOWN category is for
correct NN compounds of which the meaning is not clear enough to annotate.
As a subpart of this subproject, we also developed an annotation protocol for nominal compounds that
do not have a noun as first constituent (XN) (Verhoeven and Van Huyssteen, 2013). Such XN compounds
had thus far mostly been neglected, despite the fact that they are fairly productive in some Germanic
languages (although far less frequent than NN compounds). Our annotation guidelines followed the
general approach of
?
O S?eaghdha (2008).
In the course of the project, several datasets were developed. For both Dutch and Afrikaans there were
two annotation rounds for NN compounds and one smaller annotation experiment for XN compounds.
An overview of the semantics data can be found in Table 2, including the average Cohen?s Kappa scores.
The Dutch NN compounds were taken from the same raw compound list of 71,274 compounds de-
scribed in section 3.2 above. Subsequent annotations were performed by students in linguistics at the
University of Antwerp, all native speakers of Dutch. The first dataset was annotated by one student, and
a subset of 500 compounds by one of the authors in order to calculate inter-annotator agreement. The
second round of data was annotated by three students, with the data divided between them in such a way
that we had two annotations for each compound. For the XN compound dataset, only 600 compounds
were annotated.
The NN compounds for the Afrikaans dataset were taken from the CKarma list of split compounds
(see section 3.2 above). The complete Afrikaans dataset was annotated by three undergraduate linguistics
students, all native speakers of Afrikaans. This resulted in three annotations for each compound. With
regard to the XN compound subpart, a large dataset of 4,553 compounds was annotated.
4.3 Experiments
The data from the first annotation rounds were used for semantic classification experiments that were
based on those conducted by
?
O S?eaghdha (2008). We used the annotations made by the main annotator
25
language annotation type # items # annotators avg. Kappa score
Afrikaans NN-Round1 1,449 3 53.4
Afrikaans NN-Round2 2,328 3 37.6
Afrikaans XN 4,553 3 33.5
Dutch NN-Round1 1,766 2 60.0
Dutch NN-Round2 2,000 3 51.0
Dutch XN 600 2 48.6
Table 2: Overview of semantics data.
for each language in order to maintain his or her consistency of annotation. What follows is a description
of our own experimental setup. In our classification experiment, classifiers trained by machine learning
methods use feature vectors arising from a combination of the distributional hypothesis (as proposed
above) with the idea of analogical reasoning. It is assumed that the semantic category of a compound
can be predicted by comparing compounds with similar meanings (
?
O S?eaghdha, 2008).
4.3.1 Vector Creation
For every compound constituent, the co-occurrence context was calculated. For this purpose, for each
instance of the constituents in the corpus, the surrounding n words (that belong to the 10,000 most
frequent words of the corpus) were held in memory. The relative frequencies of these context words (the
number of times the word appeared in the context of the constituent, divided by the frequency of the
constituent in the corpus) for each constituent were stored.
For Dutch, the Twente News Corpus (Ordelman et al., 2007) was used. This is a 340 million word
corpus of newspaper articles. For Afrikaans, we used the Taalkommissie corpus (Taalkommissie, 2011),
a 60 million word corpus that consists of a variety of text genres.
A concatenation of the constituent data was used to create the instance vector. This is a new but very
simple technique of composition whereby each instance vector thus contains the relative frequencies for
the 1,000 most frequent words for each constituent (hence 2,000 per compound). Compounds of which
one or both of the constituents did not appear in the corpus were excluded from the data.
The classification experiment dealt with those compounds that were annotated with a semantically
specific category. This means that only compounds with the category tags BE, HAVE, IN, INST, AC-
TOR and ABOUT were used for the experiments. The final vector set for Afrikaans contained 1,439
compounds, while the final vector set for Dutch had 1,447 compounds.
4.3.2 Results
As machine learning method, we used the SMO algorithm, which is WEKA?s (Witten et al., 2011)
support vector machines (SVM) implementation, in a 10-fold cross-validation setup.
Since this was the first research on both Dutch and Afrikaans (Verhoeven et al., 2012), we assumed
a majority baseline which represents the accuracy that can be obtained by always guessing the most
frequent class as the output class. For Dutch, this baseline is 29.5% (428 instances of class IN on a total
of 1,447 compounds) (Verhoeven, 2012). For Afrikaans, this baseline is 28.2% (407 instances of class
ABOUT on a total of 1,439 instances).
The outcome of these experiments showed that the semantic relation between compound constituents
in Dutch and Afrikaans can be learned using our simple new composition method of concatenating
the constituent vectors into a compound vector. F-scores of 47.8 (Dutch) and 51.1 (Afrikaans) were
achieved using the counts of three context words left and right of the constituent for computing their
semantic representation. The approach turned out to be robust for varying sizes of context (different
numbers of context words), as well as for the way corpus counts were done: on either lemmas or word
forms (Verhoeven, 2012; Verhoeven and Daelemans, 2013). Our results are a good improvement of our
baselines, and provide a baseline for future research.
26
4.3.3 WordNet-based method for Afrikaans
In another subpart of this subproject, we experimented with an alternative approach, namely to use the
Afrikaans WordNet (CTexT, 2011) to infer compound semantics of Afrikaans compounds (Botha et al.,
2013). We followed the same approach as Kim and Baldwin (2005), and achieved precision results
similar to the general approach described above. i.e. 50.49% using the Afrikaans WordNet, vs. 50.80%
reported by Verhoeven et al. (2012). However, recall was much worse: 29.27% in this approach, vs.
51.60% using the other approach. This poor recall can be attributed to the small size of the Afrikaans
WordNet, which only contains 10,045 synsets, compared to 115,424 synsets in the Princeton WordNet
(Miller, 1995). We therefore conclude that a WordNet-approach holds much promise, on the premise
that the WordNet is large enough to ensure good coverage.
5 Discussion
We described machine learning approaches to the segmentation and semantic interpretation of com-
pounds in Dutch and Afrikaans, two related languages where concatenative compounding is a highly
productive morphological process. Success of machine learning approaches to any natural language pro-
cessing task is based on the presence of sufficient high quality training data and relevant information
sources allowing the classification problem to be solved.
For compound splitting, high annotator agreement in the annotation of the training data and high
generalization accuracy could be obtained for both languages using a statistical pattern induction method
working on the orthography of the input compounds, without need for other information sources. Further
improvement can be achieved here with more and richer training data. Other methods for sequence
learning could lead to further improvements as well, although Liang?s method (1983) turns out to be a
strong algorithm for this task.
The task of compound interpretation is much more difficult, both for people (who reached relatively
low annotation agreement for both languages) and for machine learners, suggesting that crucial infor-
mation is missing in the semantic representations we used for our compound constituents. Nevertheless,
also for this task, we were able to set a standard, well above baseline, for future work in compound in-
terpretation for Dutch and Afrikaans. Further improvement can potentially be found in many directions:
more fine-grained and more learnable semantic relation types, more consistently annotated training data
(and much more of it from different domains), and better semantic representations for the constituents,
for example using deep learning (Mikolov et al., 2013).
Acknowledgments
The AuCoPro project was funded through a research grant from the Nederlandse Taalunie (Dutch Lan-
guage Union) and the South African Department of Arts and Culture (DAC), as well as grants from the
South African National Research Foundation (NRF) (grant number 81794), and the European Network
on Word Structure (NetWordS) (European Science Foundation) (Grant number: 5570). Views expressed
in this publication cannot be ascribed to any of these funding organizations.
We would also like to acknowledge the contributions of numerous students and colleagues, including
Zandr?e Botha, Roald Eiselen, Joanie Liversage, Benito Trollip, Nanette van den Bergh (North-West
University); Natasja Loyens, Maxim Baetens, Frederik Vaassen (University of Antwerp); Chris Emmery,
Suzanne Aussems (Tilburg University).
References
Ken Barker and Stan Szpakowicz. 1998. Semi-Automatic Recognition of Non- Modifier Relationships . Proceed-
ings of the 17th International Conference on Computational Linguistics, pages 96?102.
Geert Booij. 2010. Construction Morphology. Oxford University Press, Oxford.
Zandr?e Botha, Roald Eiselen, and Gerhard van Huyssteen. 2013. Automatic compound semantic analysis using
wordnets. In Proceedings of the Twenty-Fourth Annual Symposium of the Pattern Recognition Association of
South Africa, Johannesburg, South Africa.
27
Fabio Celli and Malvina Nissim. 2009. Automatic Identification of Semantic Relation in Italian complex nominals.
In Proceedings of the Eighth International Conference on Computational Semantics (IWCS-8), Tilburg, The
Netherlands.
CTexT. 2005. CKarma (C5 KompositumAnaliseerder vir Robuuste Morfologiese Analise). [C5 Compound
Analyser for Robust Morphological Analysis]. Centre for Text Technology (CTexT), North-West University,
Potchefstroom, South Africa.
CTexT. 2011. Afrikaans WordNet. Centre for Text Technology (CTexT), North-West University, Potchefstroom,
South Africa.
Guy De Pauw, Tom Laureys, Walter Daelemans, and Hugo Van Hamme. 2004. A Comparison of Two Different
Approaches to Morphological Analysis of Dutch. In Proceedings of the Workshop of the ACL Special Interest
Group on Computational Phonology (SIGPHON), Barcelona, Spain.
Jan Don. 2009. IE, Germanic: Dutch. In Rochelle Lieber and Pavol
?
Stekauer, editors, The Oxford Handbook of
Compounding, pages 370?385. Oxford University Press, Oxford, UK.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Zellig Harris. 1968. Mathematical structures of language. Interscience, New York.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O S?eaghdha, Stan Szpakowicz, and Tony Veale. 2013.
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, USA. Association for
Computational Linguistics.
Erhard Hinrichs, Verena Henrich, and Reinhild Barkey. 2013. Using Part-Whole Relations for Automatic Deduc-
tion of Compound-internal Relations in GermaNet. Language Resources and Evaluation, 24(3):363?372.
Su Nam Kim and Timothy Baldwin. 2005. Automatic Interpretation of Noun Compounds Using WordNet Simi-
larity. Wall Street Journal, pages 945?956.
Ronald Langacker. 2008. Cognitive Grammar: A Basic Introduction. Oxford University Press, New York.
Mirella Lapata and Frank Keller. 2004. The Web as a Baseline: Evaluating the Performance of Unsupervised
Web-based Models for a Range of NLP Tasks. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics, pages 121?128. Association
for Computational Linguistics, Boston.
Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis,
Macquarie University.
Franklin Mark Liang. 1983. Word Hy-phen-a-tion by Com-put-er. Ph.D. thesis, Stanford University, Stanford,
USA.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT, pages 746?751.
George Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39?41.
Dan Moldovan, A Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the Semantic
Classification of Noun Compounds. In Proceedings of the HLT-NAACL Workshop on Computational Lexical
Semantics, pages 60?67. MA: Association for Computational Linguistics, Boston.
Preslav Nakov. 2008. Noun Compound Interpretation Using Paraphrasing Verbs: Feasibility Study. In Pro-
ceedings of the 13th International Conference on Artificial Intelligence: Methodology, Systems, Applications
(AIMSA08).
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova, and Stan Szpakowicz. 2006. Learning Noun-Modifier
Semantic Relations with Corpus-based and WordNet-based Features. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, pages 781?787. MA: American Association for Artificial Intelligence, Boston,
aaai-06 edition.
Anneke Neijt, Robert Schreuder, and Carel Jansen. 2010. Van boekenbonnen en fe?everhale: De tussenklank e(n)
in Nederlands en Afrikaanse samestellingen: vorm of betekenis? [The interfix e(n) in Dutch and Afrikaans
compounds: form or meaning?]. Nederlandse Taalkunde, 15(2):125?147.
28
Diarmuid
?
O S?eaghdha and Ann Copestake. 2013. Interpreting compound nouns with kernel methods. Journal of
Natural Language Engineering, Special Issue on the Semantics of Noun Compounds, 19:331?356.
Diarmuid
?
O S?eaghdha. 2008. Learning compound noun semantics. Ph.D. thesis, University of Cambridge,
Cambridge, UK.
Roeland Ordelman, Arjan Van Hessen, and Franciska De Jong. 2003. Compound decomposition in Dutch large
vocabulary speech recognition. In Proceedings of Eurospeech 2003, pages 225?228, Geneva, Switzerland.
Roeland Ordelman, Franciska de Jong, Arjan van Hessen, and Hendri Hondorp. 2007. TwNC: a Multifaceted
Dutch News Corpus. ELRA Newsletter 12, pages 3?4.
Diarmuid
?
O S?eaghdha. 2007. Annotating and Learning Compound Noun Semantics. In Proceedings of the ACL
2007 Student Research Workshop, pages 73?78. Association for Computational Linguistics, Prague.
Diarmuid
?
O S?eaghdha. 2009. Semantic classification with WordNet kernels. In Computational Linguistics,
NAACL-Short ?09, pages 237?240. Association for Computational Linguistics.
Sulene Pilon, Martin Puttkammer, and Gerhard Van Huyssteen. 2008. Die ontwikkeling van ?n woordafbreker en
kompositumanaliseerder vir Afrikaans. Literator, 29(1):21?41.
Renee Pohlmann and Wesley Kraaij. 1996. Improving the precision of a text retrieval system with compound
analysis. In Proceedings of the 7th Computational Linguistics in the Netherlands (CLIN 1996), pages 115?129,
Eindhoven, The Netherlands.
Carlos Ramisch, Aline Villavicencio, and Valia Kordoni. 2013. Introduction to the special issue on multiword
expressions: From theory to practice and use. ACM Transactions on Speech and Language Processing, 10(2):1?
10.
Sergio Scalise and Antonietta Bisetto. 2009. The classification of compounds. In Rochelle Lieber and Pavol
?
Stekauer, editors, The Oxford Handbook of Compounding, pages 34?53. Oxford University Press, Oxford.
Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! Computing semantic relatedness using Wikipedia.
In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06), Boston, MA.
Taalkommissie. 2011. Taalkommissiekorpus 1.1. Taalkommissie van die Suid-Afrikaanse Akademie vir Weten-
skap en Kuns. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.
Stephen Tratz and Ed Hovy. 2010. A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Inter-
pretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
678?687. Uppsala: Association for Computational Linguistics.
Gerhard Van Huyssteen and Menno Van Zaanen. 2004. Learning Compound Boundaries for Afrikaans Spelling
Checking. In Proceedings of First Workshop on International Proofing Tools and Language Technologies, pages
101?108, Patras.
Gerhard Van Huyssteen and Ben Verhoeven. 2014. A Taxonomy for Dutch and Afrikaans Compounds. In
Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA), Dublin,
Ireland.
Gerhard Van Huyssteen. 2014. Morfologie. In Wannie Carstens and Nerina Bosman, editors, Kontempor?ere
Afrikaanse Taalkunde, pages 171?208. Van Schaik Uitgewers, Pretoria, South Africa.
Menno Van Zaanen and Gerhard Van Huyssteen. 2002. Improving a Spelling Checker for Afrikaans. In Compu-
tational Linguistics in the Netherlands 2002-Selected Papers from the Thirteenth CLIN Meeting, page 143156,
Groningen, the Netherlands.
Ben Verhoeven and Walter Daelemans. 2013. Semantic Classification of Dutch Noun-Noun Compounds: A
Distributional Semantics Approach. CLIN Journal, 3:2?18.
Ben Verhoeven and Gerhard Van Huyssteen. 2013. More Than Only Noun-Noun Compounds: Towards an
Annotation Scheme for the Semantic Modelling of Other Noun Compound Types. In Proceedings of the 9th
Joint ISO - ACL SIGSEM Workshop on Interoperable Semantic Annotation, Potsdam, Germany.
Ben Verhoeven, Walter Daelemans, and Gerhard B. Van Huyssteen. 2012. Classification of noun-noun com-
pound semantics in Dutch and Afrikaans. In Proceedings of the Twenty-Third Annual Symposium of the Pattern
Recognition Association of South Africa (PRASA 2012), pages 121?125, Pretoria, South Africa.
29
Ben Verhoeven, Gerhard Van Huyssteen, Menno Van Zaanen, and Walter Daelemans. 2014. Annotation guidelines
for compound analysis. CLiPS Technical Report Series (CTRS), 5.
Ben Verhoeven. 2012. A computational semantic analysis of noun compounds in Dutch. Master?s thesis, Univer-
sity of Antwerp, Antwerp, Belgium.
Ian Witten, Eibe Frank, and Mark Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques
(Google eBook). Elsevier.
30
