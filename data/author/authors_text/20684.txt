Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?37,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Vague Sense Classifier for Detecting Vague Definitions in Ontologies
Panos Alexopoulos
iSOCO S.A.
Madrid, Spain
palexopoulos@isoco.com
John Pavlopoulos
Department of Informatics,
Athens University of Economics and Business
Athens, Greece
annis@aueb.gr
Abstract
Vagueness is a common human knowl-
edge and linguistic phenomenon, typi-
cally manifested by predicates that lack
clear applicability conditions and bound-
aries such as High, Expert or Bad. In the
context of ontologies and semantic data,
the usage of such predicates within ontol-
ogy element definitions (classes, relations
etc.) can hamper the latter?s quality, pri-
marily in terms of shareability and mean-
ing explicitness. With that in mind, we
present in this paper a vague word sense
classifier that may help both ontology cre-
ators and consumers to automatically de-
tect vague ontology definitions and, thus,
assess their quality better.
1 Introduction
Vagueness is a common human knowledge and
language phenomenon, typically manifested by
terms and concepts like High, Expert, Bad, Near
etc., and related to our inability to precisely de-
termine the extensions of such concepts in certain
domains and contexts. That is because vague con-
cepts have typically blurred boundaries which do
not allow for a sharp distinction between the enti-
ties that fall within their extension and those that
do not (Hyde, 2008) (Shapiro, 2006). For exam-
ple, some people are borderline tall: not clearly
?tall? and not clearly ?not tall?.
Ontologies, in turn, are formal shareable con-
ceptualizations of domains, describing the mean-
ing of domain aspects in a common, machine-
processable form by means of concepts and
their interrelations (Chandrasekaran et al., January
February 1999). As such, they are widely used
for the production and sharing of structured data
and knowledge that can be commonly understood
among human and software agents.
When building ontologies and semantic data,
engineers and domain experts often use predi-
cates that are vague. While this is not always
an intentional act, the use of such predicates in-
fluences in a negative way the comprehension of
this data by other parties and limits their value as
a reusable source of knowledge (Alexopoulos et
al., 2013). The reason is the subjective interpreta-
tion of vague definitions that can cause disagree-
ments among the people who develop, maintain or
use a vague ontology. In fact, as shown in (Alex-
opoulos et al., 2013), vagueness in ontologies can
be a source of problems in scenarios involving i)
structuring data with a vague ontology (where dis-
agreements among experts on the validity of vague
statements may occur), ii) utilizing vague facts in
ontology-based systems (where reasoning results
might not meet users? expectations) and iii) in-
tegrating vague semantic information (where the
merging of particular vague elements can lead to
data that will not be valid for all its users).
In this context, our goal in this paper is to en-
able ontology producers (engineers and domain
experts) as well as consumers (i.e., practitioners
who want to reuse ontologies and semantic data) to
detect, in an automatic way, ontology element def-
initions that are potentially vague. Such a detec-
tion will help ontology creators build more com-
prehensible and shareable ontologies (by refining,
eliminating or just documenting vague definitions)
and consumers assess, in an easier way, their us-
ability and quality before deciding to use it.
Our approach towards such a detection involves
training a classifier that may distinguish between
vague and non-vague term word senses and using
it to determine whether a given ontology element
definition is vague or not. For example, the def-
inition of the ontology class ?StrategicClient? as
?A client that has a high value for the company?
is (and should be) characterized as vague while
the definition of ?AmericanCompany? as ?A com-
33
pany that has legal status in the Unites States? is
not. The classifier is trained in a supervised way,
using vague and non-vague sense examples, care-
fully constructed from WordNet.
The structure of the rest of the paper is as fol-
lows. In the next section we briefly present related
work while in section 3 we describe in detail our
vague sense classifier, including the training data
we used and the evaluation we performed. Sec-
tion 4 describes the results of applying the classi-
fier in an a publicly available ontology, illustrating
its usefulness as an ontology evaluation tool. Fi-
nally, section 5 summarizes our work and outlines
its future directions.
2 Related Work
The phenomenon of vagueness in human language
and knowledge has been studied from a logic
and philosophical point of view in a number of
works (Hyde, 2008) (Shapiro, 2006) and differ-
ent theories and paradigms have been proposed
to accommodate it, including supervaluationism
(Keefe, 2008), many-valued logic and fuzzy logic
(Klir and Yuan, 1995). Moreover, in the context
of ontologies, one may find several works focus-
ing on acquisition, conceptualization and repre-
sentation of vague knowledge, mainly following a
fuzzy logic based approach (Bobillo and Straccia,
2011) (Stoilos et al., 2008) (Abulaish, 2009). Nev-
ertheless all these approaches rely on manual iden-
tification and analysis of vague terms and concepts
by domain experts and, to the best of our knowl-
edge, no work attempts to automate this task.
Another set of related work consists of ap-
proaches for subjectivity and polarity labeling of
word senses (Wiebe and Riloff, 2005) (Wiebe
and Mihalcea, 2006) (Wilson et al., 2005) (Su
and Markert, 2008) (Esuli and Sebastiani, 2006)
(Akkaya et al., 2011). While vagueness is related
to both phenomena (as polarized words are often
vague and vague words are typically subjective),
it is not exactly the same as these (e.g., subjective
statements do not always involve vagueness) and,
thus, requires specialized treatment. To illustrate
that, we compare in subsequent sections our vague
sense classifier with the subjective sense classifier
of (Wilson et al., 2005), showing that the former
performs significantly better than the latter.
3 Supervised Classification for Vague
Term Detection
3.1 Data
We created a dataset of 2,000 adjective senses, col-
lected from WordNet, such that 1,000 of them
had a vague definition and the the rest a non vague
definition. A sample of these senses is shown in
Table 1 while the whole dataset, which to the best
of our knowledge is the first of its kind, is publicly
available for further research
1
.
The dataset was constructed by an ontology ex-
pert. As the task of classifying a text as vague or
not can be quite subjective, we asked from two
other human judges to annotate a subset of the
dataset?s definitions (100), and we measured inter-
annotator agreement between all three. We found
mean pairwise JPA (Joint Probability of Agree-
ment) equal to 0.81 and mean pairwise K (Co-
hen?s Kappa) equal to 0.64, both of which indicate
a reasonable agreement.
Figure 1: Train and test error rate, per number of
training instances.
3.2 Training and Evaluation
We used the first 80% of the data (i.e., 800 vague
and 800 non vague instances) to train a multino-
mial Naive Bayes classifier.
2
We removed stop
words and we used the bag of words assumption
to represent each instance.
3
The remaining 20%
of the data (i.e., 200 vague and 200 non vague
instances) was used as a test set. Accuracy was
found to be 84%, which is considerably high. In
Figure 1, is shown the error rate on the test and
train data, as we increase the number of training
instances. We see that the two curves, initially,
1
http://glocal.isoco.net/datasets/VagueSynsets.zip
2
We used the implementation of Scikit-Learn found at
http://scikit-learn.org/stable/.
3
We used the list of stopwords provided by Scikit-Learn.
34
Vague Adjectives Non Vague Adjectives
Abnormal: not normal, not typical or usual or
regular or conforming to a norm
Compound: composed of more than one part
Impenitent: impervious to moral persuasion Biweekly: occurring every two weeks
Notorious: known widely and usually unfavor-
ably
Irregular: falling below the manufacturer?s
standard
Aroused: emotionally aroused Outermost: situated at the farthest possible
point from a center
Yellowish: of the color intermediate between
green and orange in the color spectrum, of
something resembling the color of an egg yolk
Unfeathered: having no feathers
Table 1: Sample Vague and Non-Vague Adjective Senses
Figure 2: Accuracy on the test data, per number of
selected features.
have a big gap between them, but this is progres-
sively reduced. However, more (or more compli-
cated) features could be beneficial; we intend to
study this further in the future.
We also examined the hypothesis of the exis-
tence of a small set of words that are often found
in vague definitions, but not in definitions which
are not vague, as then it would be very easy for
a system to use these words and discriminate be-
tween the two classes. To do this, we performed
feature selection with the chi-squared statistic for
various number of features and computed the ac-
curacy (i.e., one minus the error rate). As we show
in Figure 2, accuracy for only 5 selected features
is 50%, which is the same as if we selected class
in random. However, by increasing the number of
selected features, accuracy increases significantly.
This shows that there is not a subset of words
which could be used to discriminate between the
two classes; by contrast, most of the words play
their role. Again, this is something to be further
studied in future research.
Finally, in order to verify our intuition that
vagueness is not the same phenomenon as subjec-
tiveness (as we suggested in section 2), we used
the subjective sense classifier of (Wilson et al.,
2005) to classify the data of section 3.1 as subjec-
tive or objective, assuming that vague senses are
subjective while non-vague ones objective. The
particular classifier is part of the OpinionFinder
4
system and the results of its application in the 2000
adjective senses of our dataset were as follows.
From the 1000 vague senses, only 167 were classi-
fied as subjective while from the 1000 non-vague
ones 993. These numbers do no reflect of course
the quality of OpinionFinder as a subjectivity de-
tection system, they merely illustrate the fact that
treating vagueness in the same way as subjective-
ness is not really effective and, thus, more dedi-
cated, vagueness-specific work is needed.
4 Use Case: Detecting Vagueness in
CiTO Ontology
To evaluate the effectiveness and potential of our
classifier for detecting vague ontological defini-
tions, we considered a publicly available ontology
called CiTO
5
. CiTO is an ontology that enables
characterization of the nature or type of citations
and consists primarily of relations, many of which
are vague (e.g. the relation cito:plagiarizes). In
order to compare the experts? vague/non-vague
classification with the output of our system, we
worked as follows. We selected 44 relations from
CiTO (making sure to avoid duplications by e.g.
avoiding having both a relation and its inverse) and
we had again 3 human judges manually classify
them as vague or not. In the end we got 27 vague
4
http://mpqa.cs.pitt.edu/opinionfinder/
5
http://purl.org/spar/cito/
35
Vague Relations Non Vague Relations
plagiarizes: A property indicating that the au-
thor of the citing entity plagiarizes the cited
entity, by including textual or other elements
from the cited entity without formal acknowl-
edgement of their source.
sharesAuthorInstitutionWith: Each entity has
at least one author that shares a common insti-
tutional affiliation with an author of the other
entity.
citesAsAuthority: The citing entity cites the
cited entity as one that provides an authorita-
tive description or definition of the subject un-
der discussion.
providesDataFor: The cited entity presents
data that are used in work described in the cit-
ing entity.
speculatesOn: The citing entity speculates on
something within or related to the cited entity,
without firm evidence.
retracts: The citing entity constitutes a formal
retraction of the cited entity.
supports: The citing entity provides intellec-
tual or factual support for statements, ideas or
conclusions presented in the cited entity.
includesExcerptFrom: The citing entity in-
cludes one or more excerpts from the cited en-
tity.
refutes: The citing entity refutes statements,
ideas or conclusions presented in the cited en-
tity.
citesAsSourceDocument: The citing entity
cites the cited entity as being the entity from
which the citing entity is derived, or about
which the citing entity contains metadata.
Table 2: Sample Vague and Non-Vague Relations in CiTO
relations and 17 non-vague, a sample of which is
shown in Table 2.
Then we applied the trained vagueness classifier
of the previous section on the textual definitions of
the relations. The results of this were highly en-
couraging; 36/44 (82%) relations were correctly
classified as vague/non-vague with 74% accuracy
for vague relations and 94% for non-vague ones.
Again, for completeness, we classified the same
relations with OpinionFinder (as in the previous
section), in order to check if subjectivity classifi-
cation is applicable for vagueness. The results of
this were consistent to the ones reported in the pre-
vious section with the Wordnet data: 18/44 (40%)
overall correctly classified relations with 94% ac-
curacy for non-vague relations but only 7% for
vague ones.
5 Conclusions and Future Work
In this paper we considered the problem of auto-
matically detecting vague definitions in ontologies
and we developed a vague word sense classifier
using training data from Wordnet. Experiments
with both Wordnet word senses and real ontol-
ogy definitions, showed a considerably high accu-
racy of our system, thus verifying our intuition that
vague and non-vague senses can be separable. We
do understand that vagueness is a quite complex
phenomenon and the approach we have followed
in this paper rather simple. Yet, exactly because
of its simplicity, we believe that it can be a very
good baseline for further research in this particu-
lar area. The vague/non-vague sense dataset we
provide will be also very useful for that purpose.
Our future work comprises two main directions.
On the one hand, as we mentioned in the introduc-
tion, we intend to incorporate the current classifier
into an ontology analysis tool that will help ontol-
ogy engineers and users detect vague definitions in
ontologies and thus assess their quality better. On
the other hand, we want to further study the phe-
nomenon of vagueness as manifested in textual in-
formation, improve our classifer and see whether
it is possible to build a vague sense lexicon, similar
to lexicons that have already been built for subjec-
tivity and sentiment analysis.
Acknowledgments
The research leading to this results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s 7th Frame-
work Programme P7/2007-2013 under REA grant
agreement n
o
286348.
36
References
M. Abulaish. 2009. An ontology enhancement frame-
work to accommodate imprecise concepts and rela-
tions. Journal of Emerging Technologies in Web In-
telligence, 1(1).
C. Akkaya, J. Wiebe, A. Conrad, and R. Mihal-
cea. 2011. Improving the impact of subjectivity
word sense disambiguation on contextual opinion
analysis. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ?11, pages 87?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
P. Alexopoulos, B. Villazon-Terrazas, and Pan J.Z. Pan.
2013. Towards vagueness-aware semantic data. In
Fernando Bobillo, Rommel N. Carvalho, Paulo Ce-
sar G. da Costa, Claudia d?Amato, Nicola Fanizzi,
Kathryn B. Laskey, Kenneth J. Laskey, Thomas
Lukasiewicz, Trevor Martin, Matthias Nickles, and
Michael Pool, editors, URSW, volume 1073 of
CEURWorkshop Proceedings, pages 40?45. CEUR-
WS.org.
F. Bobillo and U. Straccia. 2011. Fuzzy ontology rep-
resentation using owl 2. International Journal of
Approximate Reasoning, 52(7):1073?1094, October.
B. Chandrasekaran, J. Josephson, and R. Benjamins.
January - February 1999. What are ontologies and
why do we need them? IEEE Intelligent Systems,
14(1):Page 20?26.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation (LREC06, pages
417?422.
D. Hyde. 2008. Vagueness, Logic and Ontology. Ash-
gate New Critical Thinking in Philosophy.
R. Keefe. 2008. Vagueness: Supervaluationism. Phi-
losophy Compass, 3:315?324.
G. Klir and B. Yuan. 1995. Fuzzy Sets and Fuzzy
Logic, Theory and Applications. Prentice Hall.
S. Shapiro. 2006. Vagueness in Context. Oxford Uni-
versity Press.
G. Stoilos, G. Stamou, J.Z. Pan, N. Simou, and
V. Tzouvaras. 2008. Reasoning with the Fuzzy De-
scription Logic f-SHIN: Theory, Practice and Appli-
cations. pages 262?281.
F. Su and K. Markert. 2008. From words to senses: A
case study of subjectivity recognition. In Proceed-
ings of the 22Nd International Conference on Com-
putational Linguistics - Volume 1, COLING ?08,
pages 825?832, Stroudsburg, PA, USA. Association
for Computational Linguistics.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated
texts. In In CICLing2005, pages 486?497.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and
S. Patwardhan. 2005. Opinionfinder: A sys-
tem for subjectivity analysis. In Proceedings of
HLT/EMNLP on Interactive Demonstrations, HLT-
Demo ?05, pages 34?35, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
37
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
