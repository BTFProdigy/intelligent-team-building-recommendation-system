Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499?504,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generating Recommendation Dialogs by Extracting Information from
User Reviews
Kevin Reschke, Adam Vogel, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{kreschke,acvogel,jurafsky}@stanford.edu
Abstract
Recommendation dialog systems help
users navigate e-commerce listings by ask-
ing questions about users? preferences to-
ward relevant domain attributes. We
present a framework for generating and
ranking fine-grained, highly relevant ques-
tions from user-generated reviews. We
demonstrate our approach on a new dataset
just released by Yelp, and release a new
sentiment lexicon with 1329 adjectives for
the restaurant domain.
1 Introduction
Recommendation dialog systems have been devel-
oped for a number of tasks ranging from product
search to restaurant recommendation (Chai et al,
2002; Thompson et al, 2004; Bridge et al, 2005;
Young et al, 2010). These systems learn user re-
quirements through spoken or text-based dialog,
asking questions about particular attributes to fil-
ter the space of relevant documents.
Traditionally, these systems draw questions
from a small, fixed set of attributes, such as cuisine
or price in the restaurant domain. However, these
systems overlook an important element in users?
interactions with online product listings: user-
generated reviews. Huang et al (2012) show that
information extracted from user reviews greatly
improves user experience in visual search inter-
faces. In this paper, we present a dialog-based in-
terface that takes advantage of review texts. We
demonstrate our system on a new challenge cor-
pus of 11,537 businesses and 229,907 user reviews
released by the popular review website Yelp1, fo-
cusing on the dataset?s 4724 restaurants and bars
(164,106 reviews).
This paper makes two main contributions. First,
we describe and qualitatively evaluate a frame-
1https://www.yelp.com/dataset_challenge/
work for generating new, highly-relevant ques-
tions from user review texts. The framework
makes use of techniques from topic modeling and
sentiment-based aspect extraction to identify fine-
grained attributes for each business. These at-
tributes form the basis of a new set of questions
that the system can ask the user.
Second, we use a method based on information-
gain for dynamically ranking candidate questions
during dialog production. This allows our system
to select the most informative question at each di-
alog step. An evaluation based on simulated di-
alogs shows that both the ranking method and the
automatically generated questions improve recall.
2 Generating Questions from Reviews
2.1 Subcategory Questions
Yelp provides each business with category labels
for top-level cuisine types like Japanese, Coffee
& Tea, and Vegetarian. Many of these top-level
categories have natural subcategories (e.g., ramen
vs. sushi). By identifying these subcategories, we
enable questions which probe one step deeper than
the top-level category label.
To identify these subcategories, we run Latent
Dirichlet Analysis (LDA) (Blei et al, 2003) on
the reviews of each set of businesses in the twenty
most common top-level categories, using 10 top-
ics and concatenating all of a business?s reviews
into one document.2 Several researchers have used
sentence-level documents to model topics in re-
views, but these tend to generate topics about fine-
grained aspects of the sort we discuss in Section
2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010).
We then manually labeled the topics, discarding
junk topics and merging similar topics. Table 1
displays sample extracted subcategories.
Using these topic models, we assign a business
2We use the Topic Modeling Toolkit implementation:
http://nlp.stanford.edu/software/tmt
499
Category Topic Label Top Words
pizza crust sauce pizza garlic sausage slice salad
Italian traditional pasta sauce delicious ravioli veal dishes gnocchi
bistro bruschetta patio salad valet delicious brie panini
deli sandwich deli salad pasta delicious grocery meatball
brew pub beers peaks ale brewery patio ipa brew
grill steak salad delicious sliders ribs tots drinks
bar drinks vig bartender patio uptown dive karaoke
American (New) bistro drinks pretzel salad fondue patio sanwich windsor
brunch sandwich brunch salad delicious pancakes patio
burger burger fries sauce beef potato sandwich delicious
mediterranean pita hummus jungle salad delicious mediterranean wrap
italian deli sandwich meats cannoli cheeses authentic sausage
new york deli beef sandwich pastrami corned fries waitress
Delis bagels bagel sandwiches toasted lox delicious donuts yummy
mediterranean pita lemonade falafel hummus delicious salad bakery
sandwiches sandwich subs sauce beef tasty meats delicious
sushi sushi kyoto zen rolls tuna sashimi spicy
Japanese teppanyaki sapporo chef teppanyaki sushi drinks shrimp fried
teriyaki teriyaki sauce beef bowls veggies spicy grill
ramen noodles udon dishes blossom delicious soup ramen
Table 1: A sample of subcategory topics with hand-labels and top words.
to a subcategory based on the topic with high-
est probability in that business?s topic distribution.
Finally, we use these subcategory topics to gen-
erate questions for our recommender dialog sys-
tem. Each top-level category corresponds to a sin-
gle question whose potential answers are the set of
subcategories: e.g., ?What type of Japanese cui-
sine do you want??
2.2 Questions from Fine-Grained Aspects
Our second source for questions is based on as-
pect extraction in sentiment summarization (Blair-
Goldensohn et al, 2008; Brody and Elhadad,
2010). We define an aspect as any noun-phrase
which is targeted by a sentiment predicate. For
example, from the sentence ?The place had great
atmosphere, but the service was slow.? we ex-
tract two aspects: +atmosphere and ?service.
Our aspect extraction system has two steps.
First we develop a domain specific sentiment lex-
icon. Second, we apply syntactic patterns to iden-
tify NPs targeted by these sentiment predicates.
2.2.1 Sentiment Lexicon
Coordination Graph We generate a list of
domain-specific sentiment adjectives using graph
propagation. We begin with a seed set combin-
ing PARADIGM+ (Jo and Oh, 2011) with ?strongly
subjective? adjectives from the OpinionFinder lex-
icon (Wilson et al, 2005), yielding 1342 seeds.
Like Brody and Elhadad (2010), we then construct
a coordination graph that links adjectives modify-
ing the same noun, but to increase precision we
require that the adjectives also be conjoined by
and (Hatzivassiloglou and McKeown, 1997). This
reduces problems like propagating positive sen-
timent to orange in good orange chicken. We
marked adjectives that follow too or lie in the
scope of negation with special prefixes and treated
them as distinct lexical entries.
Sentiment Propagation Negative and positive
seeds are assigned values of 0 and 1 respectively.
All other adjectives begin at 0.5. Then a stan-
dard propagation update is computed iteratively
(see Eq. 3 of Brody and Elhadad (2010)).
In Brody and Elhadad?s implementation of this
propagation method, seed sentiment values are
fixed, and the update step is repeated until the non-
seed values converge. We found that three modifi-
cations significantly improved precision. First, we
omit candidate nodes that don?t link to at least two
positive or two negative seeds. This eliminated
spurious propagation caused by one-off parsing er-
rors. Second, we run the propagation algorithm for
fewer iterations (two iterations for negative terms
and one for positive terms). We found that addi-
tional iterations led to significant error propaga-
tion when neutral (italian) or ambiguous (thick)
terms were assigned sentiment.3 Third, we update
both non-seed and seed adjectives. This allows us
to learn, for example, that the negative seed deca-
dent is positive in the restaurant domain.
Table 2 shows a sample of sentiment adjectives
3Our results are consistent with the recent finding of Whit-
ney and Sarkar (2012) that cautious systems are better when
bootstrapping from seeds.
500
Negative Sentiment
institutional, underwhelming, not nice, burn-
tish, unidentifiable, inefficient, not attentive,
grotesque, confused, trashy, insufferable,
grandiose, not pleasant, timid, degrading,
laughable, under-seasoned, dismayed, torn
Positive Sentiment
decadent, satisfied, lovely, stupendous,
sizable, nutritious, intense, peaceful,
not expensive, elegant, rustic, fast, affordable,
efficient, congenial, rich, not too heavy,
wholesome, bustling, lush
Table 2: Sample of Learned Sentiment Adjectives
derived by this graph propagation method. The
final lexicon has 1329 adjectives4, including 853
terms not in the original seed set. The lexicon is
available for download.5
Evaluative Verbs In addition to this adjective
lexicon, we take 56 evaluative verbs such as love
and hate from admire-class VerbNet predicates
(Kipper-Schuler, 2005).
2.2.2 Extraction Patterns
To identify noun-phrases which are targeted by
predicates in our sentiment lexicon, we develop
hand-crafted extraction patterns defined over syn-
tactic dependency parses (Blair-Goldensohn et al,
2008; Somasundaran and Wiebe, 2009) generated
by the Stanford parser (Klein and Manning, 2003).
Table 3 shows a sample of the aspects generated by
these methods.
Adj + NP It is common practice to extract any
NP modified by a sentiment adjective. However,
this simple extraction rule suffers from precision
problems. First, reviews often contain sentiment
toward irrelevant, non-business targets (Wayne is
the target of excellent job in (1)). Second, hypo-
thetical contexts lead to spurious extractions. In
(2), the extraction +service is clearly wrong?in
fact, the opposite sentiment is being expressed.
(1) Wayne did an excellent job addressing our
needs and giving us our options.
(2) Nice and airy atmosphere, but service could be
more attentive at times.
4We manually removed 26 spurious terms which were
caused by parsing errors or propagation to a neutral term.
5http://nlp.stanford.edu/projects/
yelp.shtml
We address these problems by filtering out sen-
tences in hypothetical contexts cued by if, should,
could, or a question mark, and by adopting the fol-
lowing, more conservative extractions rules:
i) [BIZ + have + adj. + NP] Sentiment adjec-
tive modifies NP, main verb is have, subject
is business name, it, they, place, or absent.
(E.g., This place has some really great yogurt
and toppings).
ii) [NP + be + adj.] Sentiment adjective linked
to NP by be?e.g., Our pizza was much too
jalapeno-y.
?Good For? + NP Next, we extract aspects us-
ing the pattern BIZ + positive adj. + for + NP, as in
It?s perfect for a date night. Examples of extracted
aspects include +lunch, +large groups, +drinks,
and +quick lunch.
Verb + NP Finally, we extract NPs that appear
as direct object to one of our evaluative verbs (e.g.,
We loved the fried chicken).
2.2.3 Aspects as Questions
We generate questions from these extracted as-
pects using simple templates. For example, the as-
pect +burritos yields the question: Do you want a
place with good burritos?
3 Question Selection for Dialog
To utilize the questions generated from reviews in
recommendation dialogs, we first formalize the di-
alog optimization task and then offer a solution.
3.1 Problem Statement
We consider a version of the Information Retrieval
Dialog task introduced by Kopec?ek (1999). Busi-
nesses b ? B have associated attributes, coming
from a set Att. These attributes are a combination
of Yelp categories and our automatically extracted
aspects described in Section 2. Attributes att ? Att
take values in a finite domain dom(att). We denote
the subset of businesses with an attribute att tak-
ing value val ? dom(att), as B|att=val. Attributes
are functions from businesses to subsets of values:
att : B ? P(dom(att)). We model a user in-
formation need I as a set of attribute/value pairs:
I = {(att1, val1), . . . , (att|I|, val|I|)}.
Given a set of businesses and attributes, a rec-
ommendation agent pi selects an attribute to ask
501
Chinese: Mexican:
+beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole
+noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos
+dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillas
Japanese: American (New)
+rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch
+chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area
+oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese plates
Table 3: Sample of the most frequent positive aspects extracted from review texts.
Input: Information need I
Set of businesses B
Set of attributes Att
Recommendation agent pi
Dialog length K
Output: Dialog history H
Recommended businesses B
Initialize dialog history H = ?
for step = 0; step < K; step++ do
Select an attribute: att = pi(B,H)
Query user for the answer: val = I(att)
Restrict set of businesses: B = B|att=val
Append answer: H = H ? {(att, val)}
end
Return (H,B)
Algorithm 1: Procedure for evaluating a recom-
mendation agent
the user about, then uses the answer value to nar-
row the set of businesses to those with the de-
sired attribute value, and selects another query.
Algorithm 1 presents this process more formally.
The recommendation agent can use both the set of
businesses B and the history of question and an-
swers H from the user to select the next query.
Thus, formally a recommendation agent is a func-
tion pi : B ? H ? Att. The dialog ends after a
fixed number of queries K.
3.2 Information Gain Agent
The information gain recommendation agent
chooses questions to ask the user by selecting
question attributes that maximize the entropy of
the resulting document set, in a manner similar to
decision tree learning (Mitchell, 1997). Formally,
we define a function infogain : Att? P(B)? R:
infogain(att, B) =
?
?
vals?P(dom(att))
|Batt=vals|
|B| log
|Batt=vals|
|B|
The agent then selects questions att ? Att that
maximize the information gain with respect to the
set of businesses satisfying the dialog history H:
pi(B,H) = argmax
att?Att
infogain(att, B|H)
4 Evaluation
4.1 Experimental Setup
We follow the standard approach of using the at-
tributes of an individual business as a simulation
of a user?s preferences (Chung, 2004; Young et al,
2010). For every business b ? B we form an in-
formation need composed of all of b?s attributes:
Ib =
?
{att?Att|att(b)6=?}
(att, att(b))
To evaluate a recommendation agent, we use
the recall metric, which measures how well an in-
formation need is satisfied. For each information
need I , let BI be the set of businesses that satisfy
the questions of an agent. We define the recall of
the set of businesses with respect to the informa-
tion need as
recall(BI , I) =
?
b?BI
?
(att,val)?I 1[val ? att(b)]
|BI ||I|
We average recall across all information needs,
yielding average recall.
We compare against a random agent baseline
that selects attributes att ? Att uniformly at ran-
dom at each time step. Other recommendation di-
alog systems such as Young et al (2010) select
questions from a small fixed hierarchy, which is
not applicable to our large set of attributes.
4.2 Results
Figure 1 shows the average recall for the ran-
dom agent versus the information gain agent with
varying sets of attributes. ?Top-level? repeatedly
queries the user?s top-level category preferences,
?Subtopic? additionally uses our topic modeling
subcategories, and ?All? uses these plus the as-
pects extracted from reviews. We see that for suf-
ficiently long dialogs, ?All? outperforms the other
systems. The ?Subtopic? and ?Top-level? systems
plateau after a few dialog steps once they?ve asked
502
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10
Ave
rag
e R
eca
ll
Dialog Length
Average Recall by Agent
RandomTop-levelSubtopicAll
Figure 1: Average recall for each agent.
all useful questions. For instance, most businesses
only have one or two top-level categories, so af-
ter the system has identified the top-level cate-
gory that the user is interested in, it has no more
good questions to ask. Note that the information
gain agent starts dialogs with the top-level and ap-
propriate subcategory questions, so it is only for
longer dialogs that the fine-grained aspects boost
performance.
Below we show a few sample output dialogs
from our ?All? information gain agent.
Q: What kind of place do you want?
A: American (New)
Q: What kind of American (New) do you want:
bar, bistro, standard, burgers, brew pub, or
brunch?
A: bistro
Q: Do you want a place with a good patio?
A: Yes
Q: What kind of place do you want?
A: Chinese
Q: What kind of Chinese place do you want:
buffet, dim sum, noodles, pan Asian, Panda
Express, sit down, or veggie?
A: sit down
Q: Do you want a place with a good lunch
special?
A: Yes
Q: What kind of place do you want?
A: Mexican
Q: What kind of Mexican place do you want:
dinner, taqueria, margarita bar, or tortas?
A: Margarita bar
Q: Do you want a place with a good patio?
A: Yes
5 Conclusion
We presented a system for extracting large sets
of attributes from user reviews and selecting rel-
evant attributes to ask questions about. Using
topic models to discover subtypes of businesses, a
domain-specific sentiment lexicon, and a number
of new techniques for increasing precision in sen-
timent aspect extraction yields attributes that give
a rich representation of the restaurant domain. We
have made this 1329-term sentiment lexicon for
the restaurant domain available as useful resource
to the community. Our information gain recom-
mendation agent gives a principled way to dynam-
ically combine these diverse attributes to ask rele-
vant questions in a coherent dialog. Our approach
thus offers a new way to integrate the advantages
of the curated hand-build attributes used in statisti-
cal slot and filler dialog systems, and the distribu-
tionally induced, highly relevant categories built
by sentiment aspect extraction systems.
6 Acknowledgments
Thanks to the anonymous reviewers and the Stan-
ford NLP group for helpful suggestions. The au-
thors also gratefully acknowledge the support of
the Nuance Foundation, the Defense Advanced
Research Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040, ONR grants
N00014-10-1-0109 and N00014-13-1-0287 and
ARO grant W911NF-07-1-0216, and the Center
for Advanced Study in the Behavioral Sciences.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. The Journal of
Machine Learning Research, 3:993?1022.
Derek Bridge, Mehmet H. Go?ker, Lorraine McGinty,
and Barry Smyth. 2005. Case-based recom-
mender systems. Knowledge Engineering Review,
20(3):315?320.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
503
In Proceedings of HLT NAACL 2010, pages 804?
812.
Joyce Chai, Veronika Horvath, Nicolas Nicolov, Margo
Stys, A Kambhatla, Wlodek Zadrozny, and Prem
Melville. 2002. Natural language assistant - a di-
alog system for online product recommendation. AI
Magazine, 23:63?75.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proceedings of ACL
2004, pages 63?70.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of EACL 1997, pages 174?
181.
Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin
Clark, and Christian Lee. 2012. Revminer: An ex-
tractive interface for navigating reviews on a smart-
phone. In Proceedings of UIST 2012.
Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the Fourth ACM International Confer-
ence on Web Search and Data Mining, pages 815?
824.
Karin Kipper-Schuler. 2005. Verbnet: A broad-
coverage, comprehensive verb lexicon.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings ACL
2003, pages 423?430.
I. Kopec?ek. 1999. Modeling of the information re-
trieval dialogue systems. In Proceedings of the
Workshop on Text, Speech and Dialogue-TSD 99,
Lectures Notes in Artificial Intelligence 1692, pages
302?307. Springer-Verlag.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL 2009, pages 226?234.
Cynthia A. Thompson, Mehmet H. Goeker, and Pat
Langley. 2004. A personalized system for conver-
sational recommendations. Journal of Artificial In-
telligence Research (JAIR), 21:393?428.
Max Whitney and Anoop Sarkar. 2012. Bootstrapping
via graph propagation. In Proceedings of the ACL
2012, pages 620?628, Jeju Island, Korea.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. Opinionfinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 on
Interactive Demonstrations, pages 34?35.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174, April.
504
Extracting Contextual Evaluativity
Kevin Reschke
University of California, Santa Cruz
kreschke@ucsc.edu
Pranav Anand
University of California, Santa Cruz
panand@ucsc.edu
Abstract
Recent work on evaluativity or sentiment in the language sciences has focused on the contri-
butions that lexical items provide. In this paper, we discuss contextual evaluativity, stance that is
inferred from lexical meaning and pragmatic environments. Focusing on assessor-grounding claims
like We liked him because he so clearly disliked Margaret Thatcher, we build a corpus and construct a
system employing compositional principles of evaluativity calculation to derive that we dislikes Mar-
garet Thatcher. The resulting system has an F-score of 0.90 on our dataset, outperforming reasonable
baselines, and indicating the viability of inferencing in the evaluative domain.
1 Contextual Evaluativity
A central aim of contemporary research on sentiment or evaluative language is the extraction of eval-
uative triples: ?evaluator, target, evaluation?. To date, both formal (e.g., Martin and White 2005, Potts
2005) and computational approaches (e.g., Pang and Lee 2008) have focused on how such triples are
lexically encoded (e.g., the negative affect of scoundrel or dislike). While lexical properties are a key
source of evaluative information, word-based considerations alone can miss pragmatic inferences result-
ing from context. (1), for example, communicates that the referent of we bears not only positive stance
towards the referent of him, but also negative stance towards Margaret Thatcher:
(1) We liked him because he so clearly disliked Margaret Thatcher.
LEXICAL EVALUATIVITY: ?we, him, +?; ?he, M.T., -?
CONTEXTUAL EVALUATIVITY: ?we, M.T., -?
This paper argues for a compositional approach to contextual evaluativity similar to the compositional
methods adopted for lexical evaluativity in Moilanen and Pulman (2007) and Nasukawa and Yi (2003).
At the the heart of the approach is the treatment of verbal predicates (dislike in (1)) as evaluativity
functors which relate argument/entity-level evaluativity to event-level evaluativity.
As discussed in ?2, the utitlity of such a model surfaces in cases where the event-level evaluativity
is known from context, and thus new information about the contextual evaluativity of the event partic-
ipants (e.g. Margaret Thatcher) can be inferred. Consequently, the empirical focus of this paper is on
structures like (1), where the second clause provides grounds for the sentiment encoded in the first, and
hence has a predictable event-level evaluation from the first clause?s evaluator. In ?3 we describe the
collection and annotation of a corpus of such assessment-grounding configurations from large-scale web
data. This annotated corpus serves as a test bed for experimental evaluation of various implementations
of the proposed compositional approach. The results of these experiments (?4) strongly support a com-
positional approach to contextual evaluativity inference. A simple compositional algorithm based on
a small, manually created evaluativity functor lexicon demonstrated significantly better precision than
non-compositional baselines. Moreover, a method for automatically expanding coverage to novel predi-
cates based on similarity with the manually created lexicon is shown to increase recall dramatically with
modest reduction in precision.
2 A Framework For Inferring Contextual Polarity
Evaluativity is concerned with determining private states (e.g., judgment or emotion) that a particular
evaluator bears towards a target entity, event, or proposition. This may be represented as a three place
370
Table 1: Evaluativity functors for verbs of having, withholding, disliking, and liking
x y Ehave Elack Ewthhld Edprv Espr Edislike Elike
+ + + - - - # - +
+ - - + + # + + -
- + - + + + # - +
- - + - - # - + -
x have/lack y a withhold/deprive/spare x of y x dislike/like y
relation, R ? De ?D??DE , where ? is of variable type and E is the type of evaluative stance, assumed
here to be binary. Lexical approaches to evaluativity (see Pang and Lee 2008 for a review) have focused
on those relations that are determinable from word-internal meaning alone. For example, describing
an event e as coddling gives rise to two triples: ?AGENT(e), PATIENT(e),+? and ?SPEAKER, e,??.1
These lexical inferences then become part of the feature set for classifying phrasal stance (e.g., the
author?s overall evaluativity in a sentence). A contrasting line of research (Moilanen and Pulman 2007,
Nasukawa and Yi 2003) analyzes phrasal stance as a compositional product of the polarities toward event
participants. For example, the evaluative polarity of the speaker toward the event in (2a) is positively
correlated with the polarity toward the subject, and negatively so in (2b).
(2) a. My {ally, enemy} was deprived shelter.
b. My {ally, enemy} was spared a dangerous mission.
Compositional proposals rely on mapping each n-ary predicate P an n-ary evaluativity functor EP :
DEn ? DE . Anand and Reschke (2011) argue that evaluativity functors largely group into classes,
depending on whether the predicates in question entail final states of possession and/or affectedness. For
example, the functors for predicates of withholding, including deprive and spare, are partial cases of the
functor for lack (partiality reflects lexical idiosyncracies about e.g., deprivation and positive objects), as
shown in Table 1.
While compositional systems are designed to compute phrasal stances bottom-up, their calculi straight-
forwardly allow inference to participant polarities as well, assuming knowledge of the event polarity and
all but one participant. Consider the sentence He disliked Margaret Thatcher. By the evaluativity con-
ditions in Table 1, Edislike is positive iff the evaluator has negative evaluation of Thatcher. Thus, given
knowledge of the event polarity, we can infer the evaluator?s stance with respect to Thatcher. In (1), this
information is provided by the preceding assessing clause (+, from Elike ). As the second clause serves
as grounds for the assessment in the first clause, the event described in the second clause is predictably
also assessed as + by the evaluator we. In our experiments we exploited this construction in particular,
but the general procedure does not require it (thus, for example, evaluative adverbs such as fortunately
and regrettably could provide an additional construction type). This procedure is sketched for (1) below:
(3) We likedelike him because he so clearly dislikededislike Margaret Thatcher.
LEXICAL EVALUATIVITY: ?we, him, +?; ?he, M.T., -?
PRAGMATIC INFERENCE: ?we, edislike, +? (edislike justifies ?we, him, +?)
COMPOSITIONAL INFERENCE: Edislike(+, y) = + iff y = +
therefore, y is regarded as +, or ?we, M.T., -?
Note that for this application, we may simplify the compositional picture and treat functors as either
preservers or reversers of the polarity of the object of interest, as is done in Moilanen and Pulman (2007)
and Nasukawa and Yi (2003): preservers (such as verbs of liking) match the object polarity with the
event polarity, and reverses negate it.
When the assessing clause evaluator is not affiliated with the speaker, this procedure can produce
markedly different results from lexical markers (which often show speaker evaluativity). Thus, in (4),
the speaker?s assessment of Obama?s cuts (indicated by the lexical much-needed) stands in sharp contrast
with NASA?s (determined by inference):
1Here, we simplify regarding potential evaluators outside of the speaker.
371
(4) NASA got angry at Obama because he imposed some much-needed cuts.
LEXICAL EVALUATIVITY: ?NASA, Obama, -?; ?SPEAKER, some much needed cuts, +?
CONTEXTUAL EVALUATIVITY: ?NASA, some much needed cuts, -?
The assessment-grounding configuration in (1) and (4) is highly productive. Behaviorally, implicit
causality predicates (including predicates of assessment, as well as praise and scolding) are frequently
understood by experimental subjects as describing an event involving the assessment target, especially
when followed by because (Garvey and Carmazza, 1974; Koornneef and van Berkum, 2006). In addition,
Somasundaran and Weibe (2009) exploited a similar construction to gather reasons for people?s product
assessments from online reviews. These together suggest that such constructions could be simultaneously
high-precision sources for evaluativity inference and easily obtainable from large corpora.
3 Data Gathering and Annotation
We developed a corpus of assessment-grounding excerpts from documents across the web to evaluate
the potential of the framework in ?2. 73 positive and 120 negative assessment predicates (like, adore,
hate, loathe, etc.) were selected from the MPQA subjectivity lexicon (Wilson et al, 2005). These were
expanded accross inflectional variants to produce 826 assessment templates, half with explicit because,
half without (e.g. terrified by X because he). These templates were filled with personal pronouns and
the names of 26 prominent political figures and issued as websearch queries to the Yahoo! Search
API.2 A total of 440,000 webdocument results were downloaded and processed using an 1152 core Sun
Microsystems blade cluster. The relevant sentences from each document were extracted, and those under
80 characters in length were parsed using the Stanford Dependency Parser.3
This produced 60,000 parsed assessment-grounding sentences, 6,000 of which (excluding duplicates)
passed the additional criterion that the grounding clause should contain a verb with a direct object. This
restriction ensured that each item in our corpus had a target for contextual polarity inference. An addi-
tional 3,300 cases were excluded because the target in the grounding clause shared possible coreference
with the experiencer (subject) of the assessment clause. We avoided these coreferring cases because,
from the perspective of a potential application, inferences about an experiencer?s stance towards himself
are less valuable than inferences about his stance towards others. Finally, the list was manually short-
ened to include only those sentences marked as assessment-grounding configurations according to two
annotators (? = 0.82); the classification task of whether this pragmatic connection occurs is beyond the
scope of this paper. 57% of the data was removed in this pass, 14% from tokens with because and 43%
from tokens without. Implicit causality verbs not followed by because have been shown experimentally
to give rise to a much weaker preference for justification (Au, 1986), and this is confirmed in our corpus
search. The result of this procedure was a final corpus size of 1,160.
The corpus was annotated for inferred contextual polarity. One of the authors and another annotator
coded sentences for evaluator stance toward the object (+,-, unknown); agreement was high: ? = 0.90.
The 48 unresolved cases were adjudicated by a third annotator. 27 cases were uniformly judged un-
known, involving predicates of change, disclosure (reveal, expose), and understanding (know). These
were removed from the corpus, leaving 1,133 sentences for training and testing.
4 System and Experimental Results
Restricting ourselves to the assessment-grounding configuration discussed above, we treat contextual
polarity inference as a binary classification problem with two inputs: the INPUT EVENT event-level
polarity (derived from the assessment clause) and the main verb of the grounding clause (henceforth
FUNCTOR VERB). The goal of the classifier is to correctly predict the polarity of the target NP (direct
object to the functor verb) given these inputs.
2http://developer.yahoo.com/search/
3http://nlp.stanford.edu/software/lex-parser.shtml
372
Table 2: Examples of verbs marked as
preserver/reverser and their sources
EXAMPLE CLASS SOURCE
reward preserver MPQA subj. lex.
hamper reverser MPQA subj. lex.
tutor preserver (benefit) FrameNet
batter reverser (injury) FrameNet
Table 3: Performance of systems and baselines
for contextual evaluativity classification
SYSTEM PREC. RECALL F-SCORE
B-Functor 0.39 0.24 0.30
B-Input 0.69 1.0 0.82
B-MLE 0.75 1.0 0.86
SYS 0.88 0.57 0.69
SYS-MPQA 0.88 0.24 0.38
SYS-Frame 0.89 0.41 0.56
SYS+Maj 0.82 1.0 0.90
SYS+Sim 0.84 0.97 0.90
As mentioned in ?2, we may categorize the functor verbs in our lexicon into preservers and reversers.
Two sources populate our lexicon. First, positively subjective verbs from the MPQA subjectivity lexicon
were marked as preservers and negatively subjective verbs were marked as reversers (1249 verbs total).
For example, Edislike is a reverser. Second, 487 verbs were culled from FrameNet (Ruppenhofer et al,
2005) based on their membership in six entailment classes: verbs of injury, destruction, lacking, benefit,
creation, and having. Class membership was determined by identifying 124 FrameNet frames aligning
with one or more classes, then manually selecting from these frames verbs whose class membership
was unambiguous. Verbs of benefit, creation, and having were marked as preservers. Verbs of injury,
destruction, and lacking were marked as reversers (Table 2). Our system (SYS) classifies objects in
context as follows: If the functor verb is a preserver, the target NP is assigned the same polarity as the
input event polarity. If the functor verb is a reverser, the target NP is assigned the opposite of the input
event polarity. This procedure is modulated by the presence of negation, as detected by a neg relation in
the dependency parse. Under negation, a preserver acts like a reverser, and vice versa.
We tested the performance of this system (SYS) on our annotated corpus against three baselines.
The first baseline (B-Functor) attempts to determine the importance of the input event to the calculation.
It thus ignores the preceding context, and attempts to classify the target object from the functor verb
directly, based on the verb?s polarity in the MPQA subjectivity lexicon. It has poor precision and recall,4
reflecting both the importance of the assessment context for object polarity and the fact that the functor
verbs are often not lexically sentiment bearing (e.g., predicates of possession). The second baseline
(B-Input), conversely, ignores the functor verb and uses the input event polarity as listed in the MPQA
lexicon (modulo negation) for object classification. The purpose of this baseline is to approximate a
classifier that predicts target polarity solely from the global/contextual polarity of the preceding clause.
This has sharply increased precision, indicating contextual information?s importance. The third baseline
(B-MLE) picked the majority object class (+), and had the highest precision, indicating the general bias
in our corpus for positive objects. Table 3 shows the performance (precision vs. recall) of our system
compared to the three baselines. Its precision is significantly higher, but its F-score is limited by the lower
coverage of our manually constructed lexicon. SYS-MPQA and SYS-Frame show the performance of
the system when the functor lexicon is limited to the MPQA and Framenet predicates, respectively. Both
are high precision sources of functor prediction, and pick out somewhat distinct predicates (given the
recall gain of combining them). SYS+Maj and SYS+Sim are attempts to handle the low recall of SYS
caused by functor verbs in the test data which aren?t in the system?s lexicon. SYS+Maj simply assigns
these out-of-vocabulary verbs to the majority class: preservers. SYS+Sim classifies out-of-vocabulary
verbs as preservers or reversers based on their relative similarity to the known preservers and reversers
selected from FrameNet ? an unknown verb is categorized as a preserver if its average similarity to
preservers is greater than its average similarity to reversers. Similarity was determined according to the
Jiang-Conrath distance measure (Jiang and Conrath, 1997), which based on links in WordNet (Fellbaum,
1998). (Note: this process cannot occur for words not found in WordNet ? e.g. misspellings ? hence the
4Low recall occurs when items are left unclassified due to out-of-vocabulary functor verbs. Low precision occurs when a +
item is classified as ? or vice versa.
373
less than perfect recall). These two systems outperform all baselines, but have indistinguishable F-scores
(if misspellings are excluded, SYS+Sim has a Recall of 0.99 and F-score of 0.91).
Most of the precision errors incurred by our systems were syntactic: incorrect parsing, incorrect
extraction of the object, or faulty negation handling (e.g., negative quantifiers or verbs). 26% of errors
are due to word-sense disambiguation. The verbs spoil and own each have positive and negative uses
(own can mean defeat), but only one sense was registered in the lexicon, leading to errors. The lion?s
share of these errors (22%) were due to the use of hate and similar expressions to convey jealousy (e.g.
I was mad at him because he had both Boardwalk and Park Place). In these scenarios, although the
assessment is negative, the event-level polarity of the grounding clause event type is positive (because it
is desired), a fact which our current system cannot handle. One way forward would be to apply WSD
techniques to distinguish jealous from non-jealous uses of predicates of dislike.
5 Conclusion
We have described a system for the extraction of what we termed contextual evaluativity ? evaluations
of objects that arise from the understanding of pragmatic inferences. This system, once we incorporate
procedures to automatically infer evaluativity functor class, significantly outperforms reasonable base-
lines on a corpus of assessor-grounding extracts from web documents. The system operates by running
a compositional approach to phrasal evaluativity in reverse, and is thus an instance of the potential com-
putational value of such treatments of evaluativity.
References
Anand, P. and K. Reschke (2011). Verb classes as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. to appear.
Au, T. K. (1986). A verb is worth a thousand words. Journal of Memory and Language 25, 104?122.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Garvey, C. and A. Carmazza (1974). Implict causality in verbs. Linguistic Inquiry 5, 459?484.
Jiang, J. and C. Conrath (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research in Computational Linguistics.
Koornneef, A. W. and J. J. A. van Berkum (2006). On the use of verb-based implicit causality in sentence
comprehension. Journal of Memory and Language 54, 445?465.
Martin, J. R. and P. R. R. White (2005). Language of Evaluation: Appraisal in English. Palgrave
Macmillan.
Moilanen, K. and S. Pulman (2007). Sentiment composition. In Proceedings of RANLP 2007.
Nasukawa, T. and J. Yi (2003). Sentiment analysis: Capturing favorability using natural language pro-
cessing. In Proceedings of the 2nd international conference on Knowledge capture.
Pang, B. and L. Lee (2008). Opinion mining and sentiment analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2), 1?135.
Potts, C. (2005). The Logic of Conventional Implicature. Oxford University Press.
Ruppenhofer, J., M. Ellsworth, M. R. L. Petruck, and C. R. Johnson (2005). Framenet ii: Extended
theory and practice. Technical report, ICSI Technical Report.
Somasundaran, S. and J. Weibe (2009). Recognizing stances in online debates. In Proceedings of ACL-
47, pp. 226?234.
Wilson, T., J. Weibe, and P. Hoffman (2005). Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of EMNLP-05.
374
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 84?88,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
POLITICAL-ADS: An annotated corpus of event-level evaluativity
Kevin Reschke
Department of Computer Science
Stanford University
Palo Alto, CA 94305 USA
reschkek@gmail.com
Pranav Anand
Department of Linguistics
University of California, Santa Cruz
Santa Cruz, CA 95064 USA
panand@ucsc.edu
Abstract
This paper presents a corpus targeting eval-
uative meaning as it pertains to descriptions
of events. The corpus, POLITICAL-ADS is
drawn from 141 television ads from the 2008
U.S. presidential race and contains 3945 NPs
and 1549 VPs annotated for scalar sentiment
from three different perspectives: the narra-
tor, the annotator, and general society. We
show that annotators can distinguish these per-
spectives reliably and that correlation between
the annotator?s own perspective and that of a
generic individual is higher than those with
the narrator. Finally, as a sample application,
we demonstrate that a simple compositional
model built off of lexical resources outper-
forms a lexical baseline.
1 Introduction
In the past decade, the semantics of evaluative lan-
guage has received renewed attention in both formal
and computational linguistics (Martin and White,
2005; Potts, 2005; Pang and Lee, 2008; Jackend-
off, 2007). This work has focused on evaluativity
at either the lexical level or the phrasal/event level
stance, without bridging between the two. A par-
allel tradition of compositional event polarity ((Na-
sukawa and Yi, 2003; Moilanen and Pulman, 2007;
Choi and Cardie, 2008; Neviarouskaya et al, 2010))
has grown up analogous to approaches to composi-
tionality in formal semantics: event predicates are
not of constant polarity, but provide functions from
the polarities of their arguments to event polarities.
Little work exists assessing the relative advantages
of a compositional account, in part because no re-
source annotating both NP level polarity and event-
level polarity in context exists. This paper intro-
duces such a corpus, POLITICAL-ADS, a collec-
tion of 2008 U.S. presidential race television ads
with scalar sentiment annotations at the NP and VP
level. After describing the corpus creation and char-
acteristics in sections 3 and 4, in section 5, we show
that a compositional system achieves an accuracy of
84.2%, above a lexical baseline of 65.1%.
2 Background
While many sentiment models handle negation
quasi-compositionally (Pang and Lee, 2008; Polanyi
and Zaenen, 2005), Nasukawa & Yi (Nasukawa and
Yi, 2003) first noted that predicates like prevent
are ?flippers?, conveying that their subject and ob-
ject have opposite polarity ? since trouble is nega-
tive, something that prevents trouble is good. Re-
cent work has expanded that idea into a fully com-
positional system (Moilanen and Pulman, 2007;
Neviarouskaya et al, 2010). Moilanen and Pulman
construct a system of compositional rules that builds
polarityin terms of a hand-built lexicon of predicates
as flippers or preservers. However, this system con-
flates two different assessment perspectives, that of
the Narrator and of some mentioned NP (NP-to-NP
perspective). The latter include psychological pred-
icates such as love and hate, and those of admira-
tion or censure (e.g., admonish, praise). Thus, they
would mark John dislikes scary movies as negative, a
correct NP-to-NP claim, but not necessarily correct
for the Narrator. Recognizing this, Neviarouskaya
et al (Neviarouskaya et al, 2010) develop a pair of
84
Announcer: In tough times, who will help Michigan?s
auto industry? Barack Obama favors loan guarantees to
help Detroit retool and revitalize. But John McCain re-
fused to support loan guarantees for the auto industry.
Now he?s just paying lip service. Not talking straight.
And McCain voted repeatedly for tax breaks for compa-
nies that ship jobs overseas, selling out American anno-
tators. We just can?t afford more of the same.
Figure 1: Transcript of POLITICAL-ADS ad #57
 14 
 Figure 5: Snapshot of Mechanical Turk form for Transcript #57 (Dem.)  
  Figure 6: Instruction for completing annotation form.  Our Goals: The purpose of this HIT is to help us document the words people use to persuade others. Overview: We ask you to read transcripts of political ads from the 2008 US presidential campaign (you can watch videos of the ads as well). Then you will answer questions about different highlighted portions of the ad. The questions are designed to determine how different pieces of text contribute to the overall message of the ad. You will answer the same four questions for each highlighted portion: 1. How does the narrator want you to feel about the highlighted expression? 2. How do you to feel about the highlighted expression? 3. In your opinion, how controversial is the highlighted expression in American society? 
Figure 2: POLITICAL-ADS annotation interface
compositional rules over both perspectives. Impor-
tantly, neither of these appr ache have been vali-
dated against a suffici ntly nuanced dataset. M ila-
nen and Pulman test against the SemEval-07 Head-
lines Corpus, which asks annotators to give an over-
all impression of sentiment. This approach allows a
headline such as Outcry in N Korea ?nuclear test?
to be arked negative, even though outcry over
military provocations is arguably good. Similarly,
Neviarouskaya et al evaluate only against NP-to-
NP data as well. While the MPQA corpus (Wiebe
et al, 2005), which annotates the source of each
sentiment annotation, separates these two sentiment
sources, work trained on it has not (Choi and Cardie,
2008; Moilanen et al, 2010). In addition, existing
annotation schemes are not designed to tease apart
perspectival differences. For example, MPQA in-
cludes a notion of Narrator-oriented evaluativity, but
it does not include the perspectives of you and the
general public.
3 The corpus
POLITICAL-ADS, is drawn from politics, a rich
and recently evolving domain for evaluativity re-
search that we hypothesized would involve a high
volume of sentiment claims subject to perspecti-
val differences. POLITICAL-ADS is a collec-
tion of 141 television ads that ran during the 2008
U.S. presidential race between Democratic candi-
date Barack Obama and Republican candidate John
McCain. The collection consists of 81 ads from
Democratic side and 60 ads from Republican side.
Figure 1 provides a sample transcript.
Each transcript was parsed using the Stanford
Parser and all NPs and VPs excluding those headed
by auxiliaries were extracted. VP annotations were
assumed to represent phrasal/event-level polarity
and NP ones argument-level polarity. The annota-
tion interface is shown in Figure 2. Annotators were
shown a transcript and a movie clip, and navigated
through the NPs and VPs within the document. At
each point they were asked to rate their response
on a [-1,1] scale for the following four questions
about the highlighted expression: 1) how the nar-
rator wants them to feel; 2) how they feel; 3) how
people in general feel; 4) how controversial the is-
sue is (included to test the whether sense of contro-
versy yields sharper differences between the various
assessment perspectives). Finally, because phrases
were not prefiltered, a ?Doesn?t Make Sense? button
was provided for each question.
206 annotators on Mechanical Turk completed
985 transcripts at $0.40 per transcript; each tran-
script was annotated by an average of 4.8 different
annotators living in the U.S. We then filtered anno-
tators by 200 phrases we deemed relatively uncon-
troversial in 20 randomly selected transcripts. To do
this, we scored each annotator in terms of the ab-
solute difference between their mean response and
the median (each annotator?s scores were first nor-
malized by mean absolute value) in the Narrator
question. We found when we thresholded annota-
tors at a score above 0.5, agreement with our gold
standard was 83.5% and dropped substantially after-
wards. This threshold excluded 74 annotators, leav-
ing 132 high-quality, or HQ, annotators (the full data
is available in the corpus).
The corpus consists of 5494 phrases (1549 VPs
and 3945 NPs) annotated 6.3 times on average, for
a total of 34, 692 annotations (9800 VP and 24892
NP). Each phrase was annotated by at least 3 HQ
annotators (average 3.9 annotators), and such an-
notators contributed 5960 VP and 15238 NP an-
85
notations. Of these, 12.1% HQ NP and 5.4% of
HQ VP responses were marked as ?Doesn?t Make
Sense? (DMS) for the narrator question. In general,
controversy and narrator questions had the highest
and lowest rates of DMS, respectively; NPs showed
higher response rates than VPs; and HQ annotators
had higher rates of button presses.1 In sections 4 and
5, we will ignore the DMS responses.
4 Corpus Findings
Table 1 provides summary statistics for the corpus.
Across the board, the three perspective questions av-
eraged close to 0, and in general HQ annotators are
closer to 0 (non-HQ annotators tended to provide
positive responses). VPs had slightly higher vari-
ance than NPs, at marginal probability (p < .04),
suggesting that VP responses were more extreme
than NP ones. You and Generic assessments are
highly correlated (Pearson?s ? = 0.85), but Narra-
tor is less so (? = .76/.74). All three are weakly
correlated with Controversy (? = .25/.26/.29 for
Narr., You, Gen., respectively). Narrator has the
highest standard deviations for the raw data, but the
lowest for the normed data. In the raw data, many
annotators recognized the narrators intensely parti-
san views and rated accordingly (|x| > 0.8), but
were more tempered when providing their perspec-
tive (|x| ? 0.35), leading to lower ?. This intensity
difference is factored out in normalization, yielding
the opposite pattern.
The response data was collected from our anno-
tators in scalar form, but applications (e.g., evalu-
ative polarity classification) it is the polarity of the
response that matters. Ignoring magnitude, Table 3
shows the polarity breakdown for all HQ phrasal an-
notations. Positive responses are the dominant class
across the board. Neutral responses are less frequent
for Narrator than for the other types. NPs have fewer
negatives and more neutrals than VPs.
Table 2 shows average standard deviations (i.e.,
agreement) by worker, question, and XP type. Note
both that NPs show less variance than VPs and that
non-HQ annotators less than HQ annotators (non-
HQ annotators gave more 0 responses).
1In a QUESTION + PHRASE TYPE + QUESTION + ANNOTA-
TOR TYPE linear model with annotator as a random effect, all
of the above effects are significant. This was the simplest model
COND ALL HQ ONLY
RAW RAW NORMED
Narr. .10 (.45) .05 (.62) .08 (.87)
You .10 (.34) .06 (.46) .09 (.85)
Gen. .10 (.33) .05 (.45) .08 (.86)
Contr. .17 (.22) .13 (.30) .17 (.60)
Table 1: Mean response by category and worker type
COND HQ ANNOTATORS
RAW NORMED
ALL VP NP ALL VP NP
Narr. .69 .75 .67 .96 1.06 .93
You .57 .63 .55 .99 1.12 .94
Gen. .53 .58 .51 .99 1.13 .94
Contr. .53 .58 .51 1.01 1.15 .96
ALL ANNOTATORS
ALL VP NP
Narr. .63 .68 .62
You .54 .59 .53
Gen. .52 .56 .51
Contr. .54 .56
Table 2: Average Standard Deviations For HQ and all
annotators
5 Comparing lexical and compositional
treatments
While compositional models of event-level evalua-
tivity are logically defensible, the extent to which
these models apply in the wild is an open ques-
tion. Because other compositional lexicons are not
freely available, we used the system described in
(Reschke and Anand, 2011), which induces flippers
and preservers from the MPQA subjectivity lexi-
con and FrameNet (Ruppenhofer et al, 2005). The
MPQA lexicon is a collection of over 8,000 words
marked for polarity. Our functor lexicon uses the
following heuristic: verbs marked positive in MPQA
are preservers; verbs marked negative are flippers.
For example, dislike has negative MPQA polarity;
therefore, it is marked as a flipper in our lexicon.
This gives us 1249 predicates: 869 flippers and 380
preservers. 329 additional verbs were added from
FrameNet according to their membership in five en-
according to ?w model comparison.
86
COND POL VP NP
Narr. + 2874 (51%) 6877 (51%)
- 2654 (47%) 5590 (42%)
0 111 (2%) 932 (7%)
You + 2714 (49%) 6573 (50%)
- 2466 (45%) 4967 (38%)
0 337 (6%) 1575 (12%)
Gen. + 2615 (48%) 6350 (49%)
- 2541 (48%) 5125 (39%)
0 332 (6%) 1558 (12%)
Contr. + 3095 (57%) 6522 (51%)
- 1755 (32%) 4159 (33%)
0 558 (10%) 2051 (16%)
Table 3: Polarity breakdowns for HQ annotations
tailment classes (Reschke and Anand, 2011): verbs
of injury/destruction, lacking, benefit, creation, and
having. 124 frames across these classes were identi-
fied, and then verbs of benefit, creation, and having
(aid, generate, have) were marked as preservers and
the complement set (forget, arrest, lack) as flippers.
As a lexical baseline, the MPQA polarity of each
verb was used ? flippers correspond to baseline neg-
ative events and preservers to positive ones.
A 635 VP test subset of POLITICAL-ADS was
constructed by omitting intransitive VPs and VPs
with non-NP complements. Gold standard labels
were determined from average normed HQ annota-
tor data. This yielded 329 positive, 284 negative,
and 2 neutral events. NPs, determined similarly, di-
vided into 393 positive, 230 negative, and 12 neutral.
Of the 635 VPs in the test set, only 272 (43.5%)
are in our FrameNet/MPQA lexicon and we hence
compare the two systems on this subset. On this
subset, the compositional system has an accuracy of
84.2%, while the lexical baseline has an accuracy
of 65.1%; there were 72 instances where the com-
positional model outperformed the lexical baseline
and 22 where the lexical outperformed the composi-
tional. Typical examples where the compositional
system won involve MPQA negatives like break,
cut, and hate and positives like want and trust. The
lexical model marks VPs like breaks the grip of for-
eign oil and want a massive government as negative
and positive, respectively ? because the NPs in ques-
tion are negative, the answers should be reversed. In
contrast, the lexical model wins on cases like grow
the economy and reform Wall Street correct. These
exemplify a robust pattern in the errors: cases where
the event is marked positive while the NP is marked
negative. In examples like grow Washington, the
idea that grow is a preserver is reasonable. However,
in grow the economy, the negativity of the economy
is arguably measuring the state of some constant en-
tity. While reform is marked positive in MPQA, it
is arguably a reverser; this shows the problems with
our lexicon induction.
At an intuitive level, we expect agent evalu-
ativity to mirror event-level evaluativity because
positive/negative entities tend to commit posi-
tive/negative acts, and this is borne out. For flip-
pers or preservers, the average VP evaluativity is
correlated with the average subject evaluativity. For
flippers the correlation is 0.57; for preservers it is
0.52. Although our model ignored subject evalua-
tivity, we performed a generalized linear regression
with subject and object evaluativity as predictors
and event-level evaluativity as outcome. For flip-
pers the regression coefficients were 0.52 for subject
(p < 4e? 4) and?0.52 for object (p < 1e? 5). For
preservers the coefficients were 0.27 (p < 1e?5) for
subject and 0.93 for object (p < 2e? 7). Thus, sub-
ject polarity is an important factor for flipper events
(e.g., the hero/villain defeated the enemy, but less so
for preservers (e.g. the hero/villain helped the en-
emy.).
6 Conclusion
In this paper we have presented POLITICAL-ADS,
a new resource for investigating the relationships be-
tween NP sentiment and VP sentiment systemati-
cally. We have demonstrated that annotators can re-
liably annotate political data with sentiment at the
phrasal level from multiple perspectives. We have
also shown that in the present data set that self-
reporting and judging generic positions are highly
correlated, while correlation with narrators is ap-
preciably weaker, as narrators are seen as more ex-
treme. We have also shown that the controversy of a
phrase does not correlate with annotators? disagree-
ments with the narrator. Finally, as a sample appli-
cation, we demonstrated that a simple compositional
model built off of lexical resources outperforms a
purely lexical baseline.
87
References
Y. Choi and C Cardie. 2008. Learning with compo-
sitional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of EMNLP
2008.
Ray Jackendoff. 2007. Language, consciousness, cul-
ture. MIT Press.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP 2007.
K. Moilanen, S. Pulman, and Y Zhang. 2010. Packed
feelings and ordered sentiments: Sentiment pars-
ing with quasi-compositional polarity sequencing and
compression. In Proceedings of WASSA 2010, EACI
2010.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings of the 2nd international conference on
Knowledge capture.
A. Neviarouskaya, H. Prendinger, , and M. Ishizuka.
2010. Recognition of affect, judgment, and appreci-
ation in text. In Proceedings of COLING 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
L. Polanyi and A. Zaenen. 2005. Contextual valence
shifters. in computing attitude and affect in text. In
Janyce Wiebe James G. Shanahan, Yan Qu, editor,
Computing Attitude and Affect in Text: Theory and
Application. Springer Verlag, Dordrecht, The Nether-
lands.
Chris Potts. 2005. The Logic of Conventional Implica-
ture. Oxford University Press.
K. Reschke and P. Anand. 2011. Extracting contextual
evaluativity. In Proceedings of ICWS 2011.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, and Christopher R. Johnson. 2005. Framenet
ii: Extended theory and practice. Technical report,
ICSI Technical Report.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Proceedings of LREC 2005.
88
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
