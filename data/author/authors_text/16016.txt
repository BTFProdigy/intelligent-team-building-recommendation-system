Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747?756,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Midge: Generating Image Descriptions From Computer Vision
Detections
Margaret Mitchell?
Xufeng Han?
Jesse Dodge??
Alyssa Mensch??
Amit Goyal??
Alex Berg?
Kota Yamaguchi?
Tamara Berg?
Karl Stratos?
Hal Daume? III??
?U. of Aberdeen and Oregon Health and Science University, m.mitchell@abdn.ac.uk
? Stony Brook University, {aberg,tlberg,xufhan,kyamagu}@cs.stonybrook.edu
??U. of Maryland, {hal,amit}@umiacs.umd.edu
?Columbia University, stratos@cs.columbia.edu
??U. of Washington, dodgejesse@gmail.com, ??MIT, acmensch@mit.edu
Abstract
This paper introduces a novel generation
system that composes humanlike descrip-
tions of images from computer vision de-
tections. By leveraging syntactically in-
formed word co-occurrence statistics, the
generator filters and constrains the noisy
detections output from a vision system to
generate syntactic trees that detail what
the computer vision system sees. Results
show that the generation system outper-
forms state-of-the-art systems, automati-
cally generating some of the most natural
image descriptions to date.
1 Introduction
It is becoming a real possibility for intelligent sys-
tems to talk about the visual world. New ways of
mapping computer vision to generated language
have emerged in the past few years, with a fo-
cus on pairing detections in an image to words
(Farhadi et al 2010; Li et al 2011; Kulkarni et
al., 2011; Yang et al 2011). The goal in connect-
ing vision to language has varied: systems have
started producing language that is descriptive and
poetic (Li et al 2011), summaries that add con-
tent where the computer vision system does not
(Yang et al 2011), and captions copied directly
from other images that are globally (Farhadi et al
2010) and locally similar (Ordonez et al 2011).
A commonality between all of these ap-
proaches is that they aim to produce natural-
sounding descriptions from computer vision de-
tections. This commonality is our starting point:
We aim to design a system capable of producing
natural-sounding descriptions from computer vi-
sion detections that are flexible enough to become
more descriptive and poetic, or include likely in-
The bus by the road with a clear blue sky
Figure 1: Example image with generated description.
formation from a language model, or to be short
and simple, but as true to the image as possible.
Rather than using a fixed template capable of
generating one kind of utterance, our approach
therefore lies in generating syntactic trees. We
use a tree-generating process (Section 4.3) simi-
lar to a Tree Substitution Grammar, but preserv-
ing some of the idiosyncrasies of the Penn Tree-
bank syntax (Marcus et al 1995) on which most
statistical parsers are developed. This allows us
to automatically parse and train on an unlimited
amount of text, creating data-driven models that
flesh out descriptions around detected objects in a
principled way, based on what is both likely and
syntactically well-formed.
An example generated description is given in
Figure 1, and example vision output/natural lan-
guage generation (NLG) input is given in Fig-
ure 2. The system (?Midge?) generates descrip-
tions in present-tense, declarative phrases, as a
na??ve viewer without prior knowledge of the pho-
tograph?s content.1
Midge is built using the following approach:
An image processed by computer vision algo-
rithms can be characterized as a triple <Ai, Bi,
Ci>, where:
1Midge is available to try online at:
http://recognition.cs.stonybrook.edu:8080/?mitchema/midge/.
747
stuff: sky .999
id: 1
atts: clear:0.432, blue:0.945
grey:0.853, white:0.501 ...
b. box: (1,1 440,141)
stuff: road .908
id: 2
atts: wooden:0.722 clear:0.020 ...
b. box: (1,236 188,94)
object: bus .307
id: 3
atts: black:0.872, red:0.244 ...
b. box: (38,38 366,293)
preps: id 1, id 2: by id 1, id 3: by id 2, id 3: below
Figure 2: Example computer vision output and natu-
ral language generation input. Values correspond to
scores from the vision detections.
? Ai is the set of object/stuff detections with
bounding boxes and associated ?attribute?
detections within those bounding boxes.
? Bi is the set of action or pose detections as-
sociated to each ai ? Ai.
? Ci is the set of spatial relationships that hold
between the bounding boxes of each pair
ai, aj ? Ai.
Similarly, a description of an image can be char-
acterized as a triple <Ad, Bd, Cd> where:
? Ad is the set of nouns in the description with
associated modifiers.
? Bd is the set of verbs associated to each ad ?
Ad.
? Cd is the set of prepositions that hold be-
tween each pair of ad, ae ? Ad.
With this representation, mapping <Ai, Bi, Ci>
to <Ad, Bd, Cd> is trivial. The problem then
becomes: (1) How to filter out detections that
are wrong; (2) how to order the objects so that
they are mentioned in a natural way; (3) how to
connect these ordered objects within a syntacti-
cally/semantically well-formed tree; and (4) how
to add further descriptive information from lan-
guage modeling alone, if required.
Our solution lies in usingAi andAd as descrip-
tion anchors. In computer vision, object detec-
tions form the basis of action/pose, attribute, and
spatial relationship detections; therefore, in our
approach to language generation, nouns for the
object detections are used as the basis for the de-
scription. Likelihood estimates of syntactic struc-
ture and word co-occurrence are conditioned on
object nouns, and this enables each noun head in
a description to select for the kinds of structures it
tends to appear in (syntactic constraints) and the
other words it tends to occur with (semantic con-
straints). This is a data-driven way to generate
likely adjectives, prepositions, determiners, etc.,
taking the intersection of what the vision system
predicts and how the object noun tends to be de-
scribed.
2 Background
Our approach to describing images starts with
a system from Kulkarni et al(2011) that com-
poses novel captions for images in the PASCAL
sentence data set,2 introduced in Rashtchian et
al. (2010). This provides multiple object detec-
tions based on Felzenszwalb?s mixtures of multi-
scale deformable parts models (Felzenszwalb et
al., 2008), and stuff detections (roughly, mass
nouns, things like sky and grass) based on linear
SVMs for low level region features.
Appearance characteristics are predicted using
trained detectors for colors, shapes, textures, and
materials, an idea originally introduced in Farhadi
et al(2009). Local texture, Histograms of Ori-
ented Gradients (HOG) (Dalal and Triggs, 2005),
edge, and color descriptors inside the bounding
box of a recognized object are binned into his-
tograms for a vision system to learn to recognize
when an object is rectangular, wooden, metal,
etc. Finally, simple preposition functions are used
to compute the spatial relations between objects
based on their bounding boxes.
The original Kulkarni et al(2011) system gen-
erates descriptions with a template, filling in slots
by combining computer vision outputs with text
based statistics in a conditional random field to
predict the most likely image labeling. Template-
based generation is also used in the recent Yang et
al. (2011) system, which fills in likely verbs and
prepositions by dependency parsing the human-
written UIUC Pascal-VOC dataset (Farhadi et al
2010) and selecting the dependent/head relation
with the highest log likelihood ratio.
Template-based generation is useful for auto-
matically generating consistent sentences, how-
ever, if the goal is to vary or add to the text pro-
duced, it may be suboptimal (cf. Reiter and Dale
(1997)). Work that does not use template-based
generation includes Yao et al(2010), who gener-
ate syntactic trees, similar to the approach in this
2http://vision.cs.uiuc.edu/pascal-sentences/
748
Kulkarni et al This is a pic-
ture of three persons, one bot-
tle and one diningtable. The
first rusty person is beside the
second person. The rusty bot-
tle is near the first rusty per-
son, and within the colorful
diningtable. The second per-
son is by the third rusty per-
son. The colorful diningtable
is near the first rusty person,
and near the second person,
and near the third rusty person.
Kulkarni et al This is
a picture of two potted-
plants, one dog and one
person. The black dog is
by the black person, and
near the second feathered
pottedplant.
Yang et al Three people
are showing the bottle on the
street
Yang et al The person is
sitting in the chair in the
room
Midge: people with a bottle at
the table
Midge: a person in black
with a black dog by potted
plants
Figure 3: Descriptions generated by Midge, Kulkarni
et al(2011) and Yang et al(2011) on the same images.
Midge uses the Kulkarni et al(2011) front-end, and so
outputs are directly comparable.
paper. However, their system is not automatic, re-
quiring extensive hand-coded semantic and syn-
tactic details. Another approach is provided in
Li et al(2011), who use image detections to se-
lect and combine web-scale n-grams (Brants and
Franz, 2006). This automatically generates de-
scriptions that are either poetic or strange (e.g.,
?tree snowing black train?).
A different line of work transfers captions of
similar images directly to a query image. Farhadi
et al(2010) use <object,action,scene> triples
predicted from the visual characteristics of the
image to find potential captions. Ordonez et al
(2011) use global image matching with local re-
ordering from a much larger set of captioned pho-
tographs. These transfer-based approaches result
in natural captions (they are written by humans)
that may not actually be true of the image.
This work learns and builds from these ap-
proaches. Following Kulkarni et aland Li et al
the system uses large-scale text corpora to esti-
mate likely words around object detections. Fol-
lowing Yang et al the system can hallucinate
likely words using word co-occurrence statistics
alone. And following Yao et al the system aims
black, blue, brown, colorful, golden, gray,
green, orange, pink, red, silver, white, yel-
low, bare, clear, cute, dirty, feathered, flying,
furry, pine, plastic, rectangular, rusty, shiny,
spotted, striped, wooden
Table 1: Modifiers used to extract training corpus.
for naturally varied but well-formed text, generat-
ing syntactic trees rather than filling in a template.
In addition to these tasks, Midge automatically
decides what the subject and objects of the de-
scription will be, leverages the collected word co-
occurrence statistics to filter possible incorrect de-
tections, and offers the flexibility to be as de-
scriptive or as terse as possible, specified by the
user at run-time. The end result is a fully au-
tomatic vision-to-language system that is begin-
ning to generate syntactically and semantically
well-formed descriptions with naturalistic varia-
tion. Example descriptions are given in Figures 4
and 5, and descriptions from other recent systems
are given in Figure 3.
The results are promising, but it is important to
note that Midge is a first-pass system through the
steps necessary to connect vision to language at
a deep syntactic/semantic level. As such, it uses
basic solutions at each stage of the process, which
may be improved: Midge serves as an illustration
of the types of issues that should be handled to
automatically generate syntactic trees from vision
detections, and offers some possible solutions. It
is evaluated against the Kulkarni et alsystem, the
Yang et alsystem, and human-written descrip-
tions on the same set of images in Section 5, and
is found to significantly outperform the automatic
systems.
3 Learning from Descriptive Text
To train our system on how people describe im-
ages, we use 700,000 (Flickr, 2011) images with
associated descriptions from the dataset in Or-
donez et al(2011). This is separate from our
evaluation image set, consisting of 840 PASCAL
images. The Flickr data is messier than datasets
created specifically for vision training, but pro-
vides the largest corpus of natural descriptions of
images to date.
We normalize the text by removing emoticons
and mark-up language, and parse each caption
using the Berkeley parser (Petrov, 2010). Once
parsed, we can extract syntactic information for
individual (word, tag) pairs.
749
a cow with sheep with a gray sky people with boats a brown cow people at
green grass by the road a wooden table
Figure 4: Example generated outputs.
Awkward Prepositions Incorrect Detections
a person boats under a black bicycle at the sky a yellow bus cows by black sheep
on the dog the sky a green potted plant with people by the road
Figure 5: Example generated outputs: Not quite right
We compute the probabilities for different
prenominal modifiers (shiny, clear, glowing, ...)
and determiners (a/an, the, None, ...) given a
head noun in a noun phrase (NP), as well as the
probabilities for each head noun in larger con-
structions, listed in Section 4.3. Probabilities are
conditioned only on open-class words, specifi-
cally, nouns and verbs. This means that a closed-
class word (such as a preposition) is never used to
generate an open-class word.
In addition to co-occurrence statistics, the
parsed Flickr data adds to our understanding of
the basic characteristics of visually descriptive
text. Using WordNet (Miller, 1995) to automati-
cally determine whether a head noun is a physical
object or not, we find that 92% of the sentences
have no more than 3 physical objects. This in-
forms generation by placing a cap on how many
objects are mentioned in each descriptive sen-
tence: When more than 3 objects are detected,
the system splits the description over several sen-
tences. We also find that many of the descriptions
are not sentences as well (tagged as S, 58% of the
data), but quite commonly noun phrases (tagged
as NP, 28% of the data), and expect that the num-
ber of noun phrases that form descriptions will be
much higher with domain adaptation. This also
informs generation, and the system is capable of
generating both sentences (contains a main verb)
and noun phrases (no main verb) in the final im-
age description. We use the term ?sentence? in the
rest of this paper to refer to both kinds of complex
phrases.
4 Generation
Following Penn Treebank parsing guidelines
(Marcus et al 1995), the relationship between
two head nouns in a sentence can usually be char-
acterized among the following:
1. prepositional (a boy on the table)
2. verbal (a boy cleans the table)
3. verb with preposition (a boy sits on the table)
4. verb with particle (a boy cleans up the table)
5. verb with S or SBAR complement (a boy
sees that the table is clean)
The generation system focuses on the first three
kinds of relationships, which capture a wide range
of utterances. The process of generation is ap-
proached as a problem of generating a semanti-
cally and syntactically well-formed tree based on
object nouns. These serve as head noun anchors
in a lexicalized syntactic derivation process that
we call tree growth.
Vision detections are associated to a {tag
word} pair, and the model fleshes out the tree de-
tails around head noun anchors by utilizing syn-
tactic dependencies between words learned from
the Flickr data discussed in Section 3. The anal-
ogy of growing a tree is quite appropriate here,
where nouns are bundles of constraints akin to
seeds, giving rise to the rest of the tree based on
the lexicalized subtrees in which the nouns are
likely to occur. An example generated tree struc-
ture is shown in Figure 6, with noun anchors in
bold.
750
NP
PP
NP
NN
table
DT
the
IN
at
NP
PP
NP
NN
bottle
DT
a
IN
with
NP
NN
people
DT
-
Figure 6: Tree generated from tree growth process.
Midge was developed using detections run on
Flickr images, incorporating action/pose detec-
tions for verbs as well as object detections for
nouns. In testing, we generate descriptions for
the PASCAL images, which have been used in
earlier work on the vision-to-language connection
(Kulkarni et al 2011; Yang et al 2011), and al-
lows us to compare systems directly. Action and
pose detection for this data set still does not work
well, and so the system does not receive these de-
tections from the vision front-end. However, the
system can still generate verbs when action and
pose detectors have been run, and this framework
allows the system to ?hallucinate? likely verbal
constructions between objects if specified at run-
time. A similar approach was taken in Yang et al
(2011). Some examples are given in Figure 7.
We follow a three-tiered generation process
(Reiter and Dale, 2000), utilizing content determi-
nation to first cluster and order the object nouns,
create their local subtrees, and filter incorrect de-
tections; microplanning to construct full syntactic
trees around the noun clusters, and surface real-
ization to order selected modifiers, realize them as
postnominal or prenominal, and select final out-
puts. The system follows an overgenerate-and-
select approach (Langkilde and Knight, 1998),
which allows different final trees to be selected
with different settings.
4.1 Knowledge Base
Midge uses a knowledge base that stores models
for different tasks during generation. These mod-
els are primarily data-driven, but we also include
a hand-built component to handle a small set of
rules. The data-driven component provides the
syntactically informed word co-occurrence statis-
tics learned from the Flickr data, a model for or-
dering the selected nouns in a sentence, and a
model to change computer vision attributes to at-
tribute:value pairs. Below, we discuss the three
main data-driven models within the generation
Unordered Ordered
bottle, table, person ? person, bottle, table
road, sky, cow ? cow, road, sky
Figure 8: Example nominal orderings.
pipeline. The hand-built component contains plu-
ral forms of singular nouns, the list of possible
spatial relations shown in Table 3, and a map-
ping between attribute values and modifier sur-
face forms (e.g., a green detection for person is to
be realized as the postnominal modifier in green).
4.2 Content Determination
4.2.1 Step 1: Group the Nouns
An initial set of object detections must first be
split into clusters that give rise to different sen-
tences. If more than 3 objects are detected in the
image, the system begins splitting these into dif-
ferent noun groups. In future work, we aim to
compare principled approaches to this task, e.g.,
using mutual information to cluster similar nouns
together. The current system randomizes which
nouns appear in the same group.
4.2.2 Step 2: Order the Nouns
Each group of nouns are then ordered to deter-
mine when they are mentioned in a sentence. Be-
cause the system generates declarative sentences,
this automatically determines the subject and ob-
jects. This is a novel contribution for a general
problem in NLG, and initial evaluation (Section
5) suggests it works reasonably well.
To build the nominal ordering model, we use
WordNet to associate all head nouns in the Flickr
data to all of their hypernyms. A description is
represented as an ordered set [a1...an] where each
ap is a noun with position p in the set of head
nouns in the sentence. For the position pi of each
hypernym ha in each sentence with n head nouns,
we estimate p(pi|n, ha).
During generation, the system greedily maxi-
mizes p(pi|n, ha) until all nouns have been or-
dered. Example orderings are shown in Figure 8.
This model automatically places animate objects
near the beginning of a sentence, which follows
psycholinguistic work in object naming (Branigan
et al 2007).
4.2.3 Step 3: Filter Incorrect Attributes
For the system to be able to extend coverage as
new computer vision attribute detections become
available, we develop a method to automatically
751
A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog
Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong.
COLOR purple blue green red white ...
MATERIAL plastic wooden silver ...
SURFACE furry fluffy hard soft ...
QUALITY shiny rust dirty broken ...
Table 2: Example attribute classes and values.
group adjectives into broader attribute classes,3
and the generation system uses these classes when
deciding how to describe objects. To group adjec-
tives, we use a bootstrapping technique (Kozareva
et al 2008) that learns which adjectives tend to
co-occur, and groups these together to form an at-
tribute class. Co-occurrence is computed using
cosine (distributional) similarity between adjec-
tives, considering adjacent nouns as context (i.e.,
JJ NN constructions). Contexts (nouns) for adjec-
tives are weighted using Pointwise Mutual Infor-
mation and only the top 1000 nouns are selected
for every adjective. Some of the learned attribute
classes are given in Table 2.
In the Flickr corpus, we find that each attribute
(COLOR, SIZE, etc.), rarely has more than a single
value in the final description, with the most com-
mon (COLOR) co-occurring less than 2% of the
time. Midge enforces this idea to select the most
likely word v for each attribute from the detec-
tions. In a noun phrase headed by an object noun,
NP{NN noun}, the prenominal adjective (JJ v) for
each attribute is selected using maximum likeli-
hood.
4.2.4 Step 4: Group Plurals
How to generate natural-sounding spatial rela-
tions and modifiers for a set of objects, as opposed
to a single object, is still an open problem (Fu-
nakoshi et al 2004; Gatt, 2006). In this work, we
use a simple method to group all same-type ob-
jects together, associate them to the plural form
listed in the KB, discard the modifiers, and re-
turn spatial relations based on the first recognized
3What in computer vision are called attributes are called
values in NLG. A value like red belongs to a COLOR at-
tribute, and we use this distinction in the system.
member of the group.
4.2.5 Step 5: Gather Local Subtrees Around
Object Nouns
1 2
NP
NN
n
JJ* ?DT{0,1} ? S
VP{VBZ} ?NP{NN n}
3 4
NP
VP{VB(G|N)} ?NP{NN n}
NP
PP{IN} ?NP{NN n}
5 6
PP
NP{NN n}IN ?
VP
PP{IN} ?VB(G|N|Z) ?
7
VP
NP{NN n}VB(G|N|Z) ?
Figure 9: Initial subtree frames for generation, present-
tense declarative phrases. ? marks a substitution site,
* marks ? 0 sister nodes of this type permitted, {0,1}
marks that this node can be included of excluded.
Input: set of ordered nouns, Output: trees preserving
nominal ordering.
Possible actions/poses and spatial relationships
between objects nouns, represented by verbs and
prepositions, are selected using the subtree frames
listed in Figure 9. Each head noun selects for its
likely local subtrees, some of which are not fully
formed until the Microplanning stage. As an ex-
ample of how this process works, see Figure 10,
which illustrates the combination of Trees 4 and
5. For simplicity, we do not include the selection
of further subtrees. The subject noun duck se-
lects for prepositional phrases headed by different
prepositions, and the object noun grass selects
for prepositions that head the prepositional phrase
in which it is embedded. Full PP subtrees are cre-
ated during Microplanning by taking the intersec-
tion of both.
The leftmost noun in the sequence is given a
rightward directionality constraint, placing it as
the subject of the sentence, and so it will only se-
752
a over b a above b b below a b beneath a a by b b by a a on b b under a
b underneath a a upon b a over b
a by b a against b b against a b around a a around b a at b b at a a beside b
b beside a a by b b by a a near b b near a b with a a with b
a in b a in b b outside a a within b a by b b by a
Table 3: Possible prepositions from bounding boxes.
Subtree frames:
NP
PP{IN} ?NP{NN n1}
PP
NP{NN n2}IN ?
Generated subtrees:
NP
PP
IN
above, on, by
NP
NN
duck
PP
NP
NN
grass
IN
on, by, over
Combined trees:
NP
PP
NP
NN
grass
IN
on
NP
NN
duck
NP
PP
NP
NN
grass
IN
by
NP
NN
duck
Figure 10: Example derivation.
lect for trees that expand to the right. The right-
most noun is given a leftward directionality con-
straint, placing it as an object, and so it will only
select for trees that expand to its left. The noun in
the middle, if there is one, selects for all its local
subtrees, combining first with a noun to its right
or to its left. We now walk through the deriva-
tion process for each of the listed subtree frames.
Because we are following an overgenerate-and-
select approach, all combinations above a proba-
bility threshold ? and an observation cutoff ? are
created.
Tree 1:
Collect all NP? (DT det) (JJ adj)* (NN noun)
and NP? (JJ adj)* (NN noun) subtrees, where:
? p((JJ adj)|(NN noun)) > ? for each adj
? p((DT det)|JJ, (NN noun)) > ?, and the proba-
bility of a determiner for the head noun is higher
than the probability of no determiner.
Any number of adjectives (including none) may
be generated, and we include the presence or ab-
sence of an adjective when calculating which de-
terminer to include.
The reasoning behind the generation of these
subtrees is to automatically learn whether to treat
a given noun as a mass or count noun (not taking a
determiner or taking a determiner, respectively) or
as a given or new noun (phrases like a sky sound
unnatural because sky is given knowledge, requir-
ing the definite article the). The selection of de-
terminer is not independent of the selection of ad-
jective; a sky may sound unnatural, but a blue sky
is fine. These trees take the dependency between
determiner and adjective into account.
Trees 2 and 3:
Collect beginnings of VP subtrees headed by
(VBZ verb), (VBG verb), and (VBN verb), no-
tated here as VP{VBX verb}, where:
? p(VP{VBX verb}|NP{NN noun}=SUBJ) > ?
Tree 4:
Collect beginnings of PP subtrees headed by (IN
prep), where:
? p(PP{IN prep}|NP{NN noun}=SUBJ) > ?
Tree 5:
Collect PP subtrees headed by (IN prep) with
NP complements (OBJ) headed by (NN noun),
where:
? p(PP{IN prep}|NP{NN noun}=OBJ) > ?
Tree 6:
Collect VP subtrees headed by (VBX verb) with
embedded PP complements, where:
? p(PP{IN prep}|VP{VBX verb}=SUBJ) > ?
Tree 7:
Collect VP subtrees headed by (VBX verb) with
embedded NP objects, where:
? p(VP{VBX verb}|NP{NN noun}=OBJ) > ?
4.3 Microplanning
4.3.1 Step 6: Create Full Trees
In Microplanning, full trees are created by tak-
ing the intersection of the subtrees created in Con-
tent Determination. Because the nouns are or-
dered, it is straightforward to combine the sub-
trees surrounding a noun in position 1 with sub-
trees surrounding a noun in position 2. Two
753
VP
VP* ?
NP
NP ?CC
and
NP ?
Figure 11: Auxiliary trees for generation.
further trees are necessary to allow the subtrees
gathered to combine within the Penn Treebank
syntax. These are given in Figure 11. If two
nouns in a proposed sentence cannot be combined
with prepositions or verbs, we backoff to combine
them using (CC and).
Stepping through this process, all nouns will
have a set of subtrees selected by Tree 1. Prepo-
sitional relationships between nouns are created
by substituting Tree 1 subtrees into the NP nodes
of Trees 4 and 5, as shown in Figure 10. Verbal
relationships between nouns are created by substi-
tuting Tree 1 subtrees into Trees 2, 3, and 7. Verb
with preposition relationships are created between
nouns by substituting the VBX node in Tree 6
with the corresponding node in Trees 2 and 3 to
grow the tree to the right, and the PP node in Tree
6 with the corresponding node in Tree 5 to grow
the tree to the left. Generation of a full tree stops
when all nouns in a group are dominated by the
same node, either an S or NP.
4.4 Surface Realization
In the surface realization stage, the system se-
lects a single tree from the generated set of pos-
sible trees and removes mark-up to produce a fi-
nal string. This is also the stage where punctua-
tion may be added. Different strings may be gen-
erated depending on different specifications from
the user, as discussed at the beginning of Section
4 and shown in the online demo. To evaluate the
system against other systems, we specify that the
system should (1) not hallucinate likely verbs; and
(2) return the longest string possible.
4.4.1 Step 7: Get Final Tree, Clear Mark-Up
We explored two methods for selecting a final
string. In one method, a trigram language model
built using the Europarl (Koehn, 2005) data with
start/end symbols returns the highest-scoring de-
scription (normalizing for length). In the second
method, we limit the generation system to select
the most likely closed-class words (determiners,
prepositions) while building the subtrees, over-
generating all possible adjective combinations.
The final string is then the one with the most
words. We find that the second method produces
descriptions that seem more natural and varied
than the n-gram ranking method for our develop-
ment set, and so use the longest string method in
evaluation.
4.4.2 Step 8: Prenominal Modifier Ordering
To order sets of selected adjectives, we use the
top-scoring prenominal modifier ordering model
discussed in Mitchell et al(2011). This is an n-
gram model constructed over noun phrases that
were extracted from an automatically parsed ver-
sion of the New York Times portion of the Giga-
word corpus (Graff and Cieri, 2003). With this
in place, blue clear sky becomes clear blue sky,
wooden brown table becomes brown wooden ta-
ble, etc.
5 Evaluation
Each set of sentences is generated with ? (likeli-
hood cutoff) set to .01 and ? (observation count
cutoff) set to 3. We compare the system against
human-written descriptions and two state-of-the-
art vision-to-language systems, the Kulkarni et al
(2011) and Yang et al(2011) systems.
Human judgments were collected using Ama-
zon?s Mechanical Turk (Amazon, 2011). We
follow recommended practices for evaluating an
NLG system (Reiter and Belz, 2009) and for run-
ning a study on Mechanical Turk (Callison-Burch
and Dredze, 2010), using a balanced design with
each subject rating 3 descriptions from each sys-
tem. Subjects rated their level of agreement on
a 5-point Likert scale including a neutral mid-
dle position, and since quality ratings are ordinal
(points are not necessarily equidistant), we evalu-
ate responses using a non-parametric test. Partici-
pants that took less than 3 minutes to answer all 60
questions and did not include a humanlike rating
for at least 1 of the 3 human-written descriptions
were removed and replaced. It is important to note
that this evaluation compares full generation sys-
tems; many factors are at play in each system that
may also influence participants? perception, e.g.,
sentence length (Napoles et al 2011) and punc-
tuation decisions.
The systems are evaluated on a set of 840
images evaluated in the original Kulkarni et al
(2011) system. Participants were asked to judge
the statements given in Figure 12, from Strongly
Disagree to Strongly Agree.
754
Grammaticality Main Aspects Correctness Order Humanlikeness
Human 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96)
Midge 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17)
Kulkarni et al2011 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23)
Yang et al2011 3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23)
Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the
rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.
GRAMMATICALITY:
This description is grammatically correct.
MAIN ASPECTS:
This description describes the main aspects of this
image.
CORRECTNESS:
This description does not include extraneous or in-
correct information.
ORDER:
The objects described are mentioned in a reasonable
order.
HUMANLIKENESS:
It sounds like a person wrote this description.
Figure 12: Mechanical Turk prompts.
We report the scores for the systems in Table
4. Results are analyzed using the non-parametric
Wilcoxon Signed-Rank test, which uses median
values to compare the different systems. Midge
outperforms all recent automatic approaches on
CORRECTNESS and ORDER, and Yang et alad-
ditionally on HUMANLIKENESS and MAIN AS-
PECTS. Differences between Midge and Kulkarni
et alare significant at p< .01; Midge and Yang et
al. at p< .001. For all metrics, human-written de-
scriptions still outperform automatic approaches
(p < .001).
These findings are striking, particularly be-
cause Midge uses the same input as the Kulka-
rni et alsystem. Using syntactically informed
word co-occurrence statistics from a large corpus
of descriptive text improves over state-of-the-art,
allowing syntactic trees to be generated that cap-
ture the variation of natural language.
6 Discussion
Midge automatically generates language that is as
good as or better than template-based systems,
tying vision to language at a syntactic/semantic
level to produce natural language descriptions.
Results are promising, but, there is more work to
be done: Evaluators can still tell a difference be-
tween human-written descriptions and automati-
cally generated descriptions.
Improvements to the generated language are
possible at both the vision side and the language
side. On the computer vision side, incorrect ob-
jects are often detected and salient objects are of-
ten missed. Midge does not yet screen out un-
likely objects or add likely objects, and so pro-
vides no filter for this. On the language side, like-
lihood is estimated directly, and the system pri-
marily uses simple maximum likelihood estima-
tions to combine subtrees. The descriptive cor-
pus that informs the system is not parsed with
a domain-adapted parser; with this in place, the
syntactic constructions that Midge learns will bet-
ter reflect the constructions that people use.
In future work, we hope to address these issues
as well as advance the syntactic derivation pro-
cess, providing an adjunction operation (for ex-
ample, to add likely adjectives or adverbs based
on language alone). We would also like to incor-
porate meta-data ? even when no vision detection
fires for an image, the system may be able to gen-
erate descriptions of the time and place where an
image was taken based on the image file alone.
7 Conclusion
We have introduced a generation system that uses
a new approach to generating language, tying a
syntactic model to computer vision detections.
Midge generates a well-formed description of an
image by filtering attribute detections that are un-
likely and placing objects into an ordered syntac-
tic structure. Humans judge Midge?s output to be
the most natural descriptions of images generated
thus far. The methods described here are promis-
ing for generating natural language descriptions
of the visual world, and we hope to expand and
refine the system to capture further linguistic phe-
nomena.
8 Acknowledgements
Thanks to the Johns Hopkins CLSP summer
workshop 2011 for making this system possible,
and to reviewers for helpful comments. This
work is supported in part by Michael Collins and
by NSF Faculty Early Career Development (CA-
REER) Award #1054133.
755
References
Amazon. 2011. Amazon mechanical turk: Artificial
artificial intelligence.
Holly P. Branigan, Martin J. Pickering, and Mikihiro
Tanaka. 2007. Contributions of animacy to gram-
matical function assignment and word order during
production. Lingua, 118(2):172?189.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon?s Me-
chanical Turk. NAACL 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detections. Proceed-
ings of CVPR 2005.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. Proceedings of CVPR 2009.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
Proceedings of ECCV 2010.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
maman. 2008. A discriminatively trained, mul-
tiscale, deformable part model. Proceedings of
CVPR 2008.
Flickr. 2011. http://www.flickr.com. Accessed
1.Sep.11.
Kotaro Funakoshi, Satoru Watanabe, Naoko
Kuriyama, and Takenobu Tokunaga. 2004.
Generating referring expressions using perceptual
groups. Proceedings of the 3rd INLG.
Albert Gatt. 2006. Generating collective spatial refer-
ences. Proceedings of the 28th CogSci.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
PA. LDC Catalog No. LDC2003T05.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. MT Summit.
http://www.statmt.org/europarl/.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C. Berg, and Tamara
Berg. 2011. Baby talk: Understanding and gener-
ating image descriptions. Proceedings of the 24th
CVPR.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. Proceedings of the 36th ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
Proceedings of CoNLL 2011.
Mitchell Marcus, Ann Bies, Constance Cooper, Mark
Ferguson, and Alyson Littman. 1995. Treebank II
bracketing guide.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011. Semi-supervised modeling for prenomi-
nal modifier ordering. Proceedings of the 49th
ACL:HLT.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. ACL-
HLT Workshop on Monolingual Text-To-Text Gen-
eration.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. Proceedings of NIPS 2011.
Slav Petrov. 2010. Berkeley parser. GNU General
Public License v.2.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazon?s mechanical turk. Proceed-
ings of the NAACL HLT 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. Journal
of Natural Language Engineering, pages 57?87.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and
Yiannis Aloimonos. 2011. Corpus-guided sen-
tence generation of natural images. Proceedings of
EMNLP 2011.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2T: Image pars-
ing to text description. Proceedings of IEEE 2010,
98(8):1485?1508.
756
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detecting Visual Text
Jesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6
Kota Yamaguchi3, Yejin Choi3, Hal Daume? III2, Alexander C. Berg3 and Tamara L. Berg3
1University of Washington, 2University of Maryland, 3Stony Brook University
4MIT, 5Oregon Health & Science University, 6Columbia University
dodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.edu
acmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edu
kyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edu
me@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.edu
Abstract
When people describe a scene, they often in-
clude information that is not visually apparent;
sometimes based on background knowledge,
sometimes to tell a story. We aim to sepa-
rate visual text?descriptions of what is being
seen?from non-visual text in natural images
and their descriptions. To do so, we first con-
cretely define what it means to be visual, an-
notate visual text and then develop algorithms
to automatically classify noun phrases as vi-
sual or non-visual. We find that using text
alone, we are able to achieve high accuracies
at this task, and that incorporating features
derived from computer vision algorithms im-
proves performance. Finally, we show that we
can reliably mine visual nouns and adjectives
from large corpora and that we can use these
effectively in the classification task.
1 Introduction
People use language to describe the visual world.
Our goal is to: formalize what ?visual text? is (Sec-
tion 2.2); analyze naturally occurring written lan-
guage for occurrences of visual text (Section 2); and
build models that can detect visual descriptions from
raw text or from image/text pairs (Section 3). This
is a challenging problem. One challenge is demon-
strated in Figure 1, which contains two images that
contain the noun ?car? in their human-written cap-
tions. In one case (the top image), there actually is a
car in the image; in the other case, there is not: the
car refers to the state of the speaker.
The ability to automatically identify visual text is
practically useful in a number of scenarios. One can
Another dream car to
add to the list, this one
spotted in Hanbury St.
Shot out my car win-
dow while stuck in traf-
fic because people in
Cincinnati can?t drive in
the rain.
Figure 1: Two image/caption pairs, both containing the
noun ?car? but only the top one in a visual context.
imagine automatically mining image/caption data
(like that in Figure 1) to train object recognition sys-
tems. However, in order to do so reliably, one must
know whether the ?car? actually appears or not.
When building image search engines, it is common
to use text near an image as features; this is more
useful when this text is actually visual. Or when
training systems to automatically generate captions
of images (e.g., for visually impaired users), we
need good language models for visual text.
One of our goals is to define what it means for a
bit of text to be visual. As inspiration, we consider
image/description pairs automatically crawled from
Flickr (Ordonez et al, 2011). A first pass attempt
might be to say ?a phrase in the description of an
image is visual if you can see it in the corresponding
image.? Unfortunately, this is too vague to be useful;
the biggest issues are discussed in Section 2.2.
762
Based on our analysis, we settled on the follow-
ing definition: A piece of text is visual (with re-
spect to a corresponding image) if you can cut out
a part of that image, paste it into any other image,
and a third party could describe that cut-out part in
the same way. In the car example, the claim is that I
could cut out the car, put it in the middle of any other
image, and someone else might still refer to that car
as ?dream car.? The car in the bottom image in Fig-
ure 1 is not visual because there?s nothing you could
cut out that would retain car-ness.
2 Data Analysis
Before embarking on the road to building models of
visual text, it is useful to obtain a better understand-
ing of what visual text is like, and how it compares to
the more standard corpora that we are used to work-
ing with. We describe the two large data sets that we
use (one visual, one non-visual), then describe the
quantitative differences between them, and finally
discuss our annotation effort for labeling visual text.
2.1 Data sets
We use the SBU Captioned Photo Dataset (Ordonez
et al, 2011) as our primary source of image/caption
data. This dataset contains 1 million images with
user associated captions, collected in the wild by in-
telligent filtering of a huge number of Flickr pho-
tos. Past work has made use of this dataset to re-
trieve whole captions for association with a query
image (Ordonez et al, 2011). Their method first
used global image descriptors to retrieve an initial
matched set, and then applied more local estimates
of content to re-rank this (relatively small) set (Or-
donez et al, 2011). This means that content based
matching was relatively constrained by the bottle-
neck of global descriptors, and local content (e.g.,
objects) had relatively small effect on accuracy.
As an auxiliary source of information for (largely)
non-visual text, we consider a large corpus of text
obtained by concatenating ukWaC1 and the New
York Times Newswire Service (NYT) section of the
Gigaword (Graff, 2003) Corpus. The Web-derived
ukWaC is already tokenized and POS-tagged with
the TreeTagger (Schmid, 1995). NYT is tokenized,
1ukWaC is a freely available Wikipedia-derived corpus from
2009; see http://wacky.sslmit.unibo.it/doku.php.
and POS-tagged using TagChunk (Daume? III and
Marcu, 2005). This consists of 171 million sen-
tences (4 billion words). We refer to this generic
text corpus as Large-Data.
2.2 Formalizing visual text
We begin our analysis by revisiting the definition
of visual text from the introduction, and justifying
this particular definition. In order to arrive at a suf-
ficiently specific definition of ?visual text,? we fo-
cused on the applications of visual text that we care
about. As discussed in the introduction, these are:
training object detectors, building image search en-
gines and automatically generating captions for im-
ages. Our definition is based on access to image/text
pairs, but later we discuss how to talk about it purely
based on text. To make things concrete, consider an
image/text pair like that in the top of Figure 1. And
then consider a phrase in the text, like ?dream car.?
The question is: is ?dream car? visual or not?
One of the challenges in arriving at such a defi-
nition is that the description of an image in Flickr
is almost always written by the photographer of that
image. This means the descriptions often contain in-
formation that is not actually pictured in the image,
or contain references that are only relevant to the
photographer (referring to a person/pet by name).
One might think that this is an artifact of this par-
ticular dataset, but it appears to be generic to all cap-
tions, even those written by a viewer (rather than the
photographer). Figure 2 shows an image from the
Pascal dataset (Everingham et al, 2010), together
with captions written by random people collected
via crowd-sourcing (Rashtchian et al, 2010). There
is much in this caption that is clearly made-up by the
author, presumably to make the caption more inter-
esting (e.g., meta-references like ?the camera? or ?A
photo? as well as ?guesses? about the image, such as
?garage? and ?venison?).
Second, there is a question of how much inference
you are allowed to do when you say that you ?see?
something. For example, in the top image in Fig-
ure 1, the street is pictured, but does that mean that
?Hanbury St.? is visual? What if there were a street
sign that clearly read ?Hanbury St.? in the image?
This problem comes up all the time, when people
say things like ?in London? or ?in France? in their
captions. If it?s just a portrait of people ?in France,?
763
1. A distorted photo of a man cutting up a large cut of meat in a garage.
2. A man smiling at the camera while carving up meat.
3. A man smiling while he cuts up a piece of meat.
4. A smiling man is standing next to a table dressing a piece of venison.
5. The man is smiling into the camera as he cuts meat.
Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing. Measurements on the
SMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest are
non-visual (underlined). See Section 2.4 for details.
it?s hard to say that this is visual. If you see the Eif-
fel tower in the background, this is perhaps better
(though it could be Las Vegas!), but how does this
compare to a photo taken out of an airplane window
in which you actually do see France-the-country?
This problem becomes even more challenging
when you consider things other than nouns. For in-
stance, when is a verb visual? For instance, the most
common non-copula verb in our data is ?sitting,?
which appears in roughly two usages: (1) ?Took this
shot, sitting in a bar and enjoying a Portugese beer.?
and (2) ?Lexy sitting in a basket on top of her cat
tree.? The first one is clearly not visual; the second
probably is. A more nuanced case is for ?playing,?
as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-
ing in Nannie?s yard.? The corresponding image for
the latter description shows a sleeping cat.
Our final definition, based on cutting out the po-
tentially visual part of the image, allows us to say
that: (1) ?venison? is not visual (because you cannot
actually tell); (2) ?Hanbury St.? and ?Lexy? are not
visual (you can infer them, in the first case because
there is only one street and in the second case be-
cause there is only one cat); (3) that seeing the real
Eiffel tower in the background does not mean that
?France? is visual (but again, may be inferred); etc.
2.3 Most Pronounced Differences
To get an intuitive sense of how Flickr captions (ex-
pected to be predominantly visual) and generic text
(expected not to be so) differ, we computed some
simple statistics on sentences from these. In gen-
eral, the generic text had twice as many main verbs
as the Flickr data, four times as many auxiliaries or
light verbs, and about 50% more prepositions.
Flickr captions tended to have far more references
to physical objects (versus abstract objects) than the
generic text, according to the WordNet hierarchy.
Approximately 64% of the objects in Flickr were
physical (about 22% abstract and 14% unknown).
Whereas in the generic text, only 30% of the objects
were physical, 53% were abstract (17% unknown).
A third major difference between the corpora is
in terms of noun modifiers. In both corpora, nouns
tend not to have any modifiers, but modifiers are still
more prevalent in Flickr than in generic text. In par-
ticular, 60% of nouns in Flickr have zero modifiers,
but 70% of nouns in generic text have zero modi-
fiers. In Flickr, 30% of nouns have exactly one mod-
ifier, as compared to only 22% for generic text.
The breakdown of what those modifiers look like
is even more pronounced, even when restricted just
to physical objects (modifier types are obtained
through the bootstrapping process discussed in Sec-
tion 3.1). Almost 50% of nominal modifiers in the
Flickr data are color modifiers, whereas color ac-
counts for less than 5% of nominal modifiers in
generic text. In Flickr, 10% of modifiers talk about
beauty, in comparison to less than 5% in generic
text. On the other hand, less than 3% of modifiers
in Flickr reference ethnicity, as compared to almost
20% in generic text; and 20% of Flickr modifiers
reference size, versus 50% in generic text.
2.4 Annotating Visual Text
In order to obtain ground truth data, we rely on
crowdsourcing (via Amazon?s Mechanical Turk).
Each instance is an image, a paired caption, and a
highlighted noun phrase in that caption. The anno-
tation for this instance is a label of ?visual,? ?non-
visual? or ?error,? where the error category is re-
764
served for cases where the noun phrase segmenta-
tion was erroneous. Each worker is given five in-
stances to label and paid one cent per annotation.2
For a small amount of data (803 images contain-
ing 2339 instances), we obtained annotations from
three separate workers per instance to obtain higher
quality data. For a large amount of data (48k im-
ages), we obtained annotations from only a sin-
gle worker. Subsequently, we will refer to these
two data sets as the SMALL and LARGE data sets.
In both data sets, approximately 70% of the noun
phrases were visual, 28% were non-visual and 2%
were erroneous. For simplicity, we group erroneous
and non-visual for all learning and evaluation.
In the SMALL data set, the rate of disagreement
between annotators was relatively low. In 74% of the
annotations, there was no disagreement at all. We
reconciled the annotations using the quality manage-
ment technique of Ipeirotis et al (2010); only 14%
of the annotations need to be changed in order to ob-
tain a gold standard.
One immediate question raised in this process is
whether one needs to actually see the image to per-
form the annotation. In particular, if we expect an
NLP system to be able to classify noun phrases as
visual or non-visual, we need to know whether peo-
ple can do this task sans image. We therefore per-
formed the same annotation on the SMALL data set,
but where the workers were not shown the image.
Their task was to imagine an image for this caption
and then annotate the noun phrase based on whether
they thought it would be pictured or not. We ob-
tained three annotations as before and reconciled
them (Ipeirotis et al, 2010). The accuracy of this
reconciled version against the gold standard (pro-
duced by people who did see the image) was 91%.
This suggests that while people are able to do this
task with some reliability, seeing the image is very
important (recall that always guessing ?visual? leads
to an accuracy of 70%).
3 Visual Features from Raw Text
Our first goal is to attempt to obtain relatively large
knowledge bases of terms that are (predominantly)
visual. This is potentially useful in its own right
2Data available at http://hal3.name/dvt/, with direct links
back to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determine
which query terms are likely to be pictured). We
have explored two techniques for performing this
task, the first based on bootstrapping (Section 3.1)
and the second based on label propagation (Sec-
tion 3.2). We then use these lists to generate features
for a classifier that predicts whether a noun phrase?
in context?is visual or not (Section 4).
In addition, we consider the task of separating ad-
jectives into different visual categories (Section 3.3).
We have already used the results of this in Sec-
tion 2.3 to understand the differences between our
two corpora. It is also potentially useful for the
purpose of building new object detection systems or
even attribute detection systems, to get a vocabulary
of target detections.
3.1 Bootstrapping for Visual Text
In this section, we learn visual and non-visual nouns
and adjectives automatically based on bootstrapping
techniques. First, we construct a graph between ad-
jectives by computing distributional similarity (Tur-
ney and Pantel, 2010) between them. For comput-
ing distributional similarity between adjectives, each
target adjective is defined as a vector of nouns which
are modified by the target adjective. To be exact, we
use only those adjectives as modifiers which appear
adjacent to a noun (that is, in a JJ NN construction).
For example, in ?small red apple,? we consider only
red as a modifier for noun. We use Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1989)
to weight the contexts, and select the top 1000 PMI
contexts for each adjective.3
Next, we apply cosine similarity to find the top
10 distributionally similar adjectives with respect to
each target adjective based on our large generic cor-
pus (Large-Data from Section 2.1). This creates a
graph with adjectives as nodes and cosine similarity
as weight on the edges. Analogously, we construct a
graph with nouns as nodes (here, adjectives are used
as contexts for nouns).
We then apply bootstrapping (Kozareva et al,
2008) on the noun and adjective graphs by select-
ing 10 seeds for visual and non-visual nouns and
adjectives (see Table 1). We use in-degree (sum of
weights of incoming edges) to compute the score for
3We are interested in descriptive adjectives, which ?typi-
cally ascribe to a noun a value of an attribute? (Miller, 1998).
765
Visual car house tree horse animal
nouns man table bottle
seeds woman computer
Non-visual idea bravery deceit trust
nouns dedication anger humour luck
seeds inflation honesty
Visual brown green wooden striped
adjectives orange rectangular furry
seeds shiny rusty feathered
Non-visual public original whole righteous
adjectives political personal intrinsic
seeds individual initial total
Table 1: Example seeds for bootstrapping.
each node that has connections with known (seeds)
or automatically labeled nodes, previously exploited
to learn hyponymy relations from the web (Kozareva
et al, 2008). Intuitively, in-degree captures the pop-
ularity of new instances among instances that have
already been identified as good instances. We learn
visual and non-visual words together (known as the
mutual exclusion principle in bootstrapping (The-
len and Riloff, 2002; McIntosh and Curran, 2008)):
each word (node) is assigned to only one class.
Moreover, after each iteration, we harmonically de-
crease the weight of the in-degree associated with
instances learned in later iterations. We added 25
new instances at each iteration and ran 500 iterations
of bootstrapping, yielding 11955 visual and 11978
non-visual nouns, and 7746 visual and 7464 non-
visual adjectives.
Based on manual inspection, the learned visual
and non-visual lists look great. In the future, we
would like to do a Mechanical Turk evaluation to
directly evaluate the visual and non-visual nouns
and adjectives. For now, we show the coverage of
these classes in the Flickr data-set: Visual nouns:
53.71%; Non-visual nouns: 14.25%; Visual ad-
jectives: 51.79%; Non-visual adjectives: 14.40%.
Overall, we find more visual nouns and adjectives
are covered in the Flickr data-set, which makes
sense, since the Flickr data-set is largely visual.
Second, we show the coverage of these classes
on the large text corpora (Large-Data from Sec-
tion 2.1): Visual nouns: 26.05%; Non-visual nouns:
41.16%; Visual adjectives: 20.02%; Non-visual ad-
Visual: attend, buy, clean, comb, cook, drink, eat,
fry, pack, paint, photograph, smash, spill, steal,
taste, tie, touch, watch, wear, wipe
Non-visual: achieve, admire, admit, advocate, al-
leviate, appreciate, arrange, criticize, eradicate,
induce, investigate, minimize, overcome, pro-
mote, protest, relieve, resolve, review, support,
tolerate
Table 2: Predicates that are visual and non-visual.
Visual: water, cotton, food, pumpkin, chicken,
ring, hair, mouth, meeting, kind, filter, game, oil,
show, tear, online, face, class, car
Non-visual: problem, poverty, pain, issue, use,
symptom, goal, effect, thought, government,
share, stress, work, risk, impact, concern, obsta-
cle, change, disease, dispute
Table 3: Learned visual/non-visual nouns.
jectives: 40.00%. Overall, more non-visual nouns
and adjectives cover text data, since Large-Data is
a non-visual data-set.
3.2 Label Propagation for Visual Text
To propagate visual labels, we construct a bipartite
graph between visually descriptive predicates and
their arguments. Let VP be the set of nodes that cor-
responds to predicates, and let VA be the set of nodes
that corresponds to arguments. To learn the visually
descriptive words, we set VP to 20 visually descrip-
tive predicates shown in the top of Table 2, and VA
to all nouns that appear in the object argument posi-
tion with respect to the seed predicates. We approx-
imate this by taking nouns on the right hand side
of the predicates within a window of 4 words using
the Web 1T Google N-gram data (Brants and Franz.,
2006). For edge weights, we use conditional prob-
abilities between predicates and arguments so that
w(p? a) := pr(a|p) and w(a? p) := pr(p|a).
In order to collectively induce the visually de-
scriptive words from this graph, we apply the graph
propagation algorithm of Velikovich et al (2010),
a variant of label propagation algorithms (Zhu and
Ghahramani, 2002) that has been shown to be ef-
fective for inducing a web-scale polarity lexicon
based on word co-occurrence statistics. This algo-
766
Color purple blue maroon beige green
Material plastic cotton wooden metallic silver
Shape circular square round rectangular triangular
Size small big tiny tall huge
Surface coarse smooth furry fluffy rough
Direction sideways north upward left down
Pattern striped dotted checked plaid quilted
Quality shiny rusty dirty burned glittery
Beauty beautiful cute pretty gorgeous lovely
Age young mature immature older senior
Ethnicity french asian american greek hispanic
Table 4: Attribute Classes with their seed values
rithm iteratively updates the semantic distance be-
tween each pair of nodes in the graph, then produces
a score for each node that represents how visually
descriptive each word is. To learn the words that
are not visually descriptive, we use the predicates
shown in the bottom of Table 2 as VP instead. Ta-
ble 3 shows the top ranked nouns that are visually
descriptive and not visually descriptive.
3.3 Bootstrapping Visual Adjectives
Our goal in this section is to automatically gener-
ate comprehensive lists of adjectives for different at-
tributes, such as color, material, shape, etc. To our
knowledge, this is the first significant effort of this
type for adjectives: most bootstrapping techniques
focus exclusively on nouns, although Almuhareb
and Poesio (2005) populated lists of attributes us-
ing web-based similarity measures. We found that
in some ways adjectives are easier than nouns, but
require slightly different representations.
One might conjecture that listing attributes by
hand is difficult. Colors names are well known to
be quite varied. For instance, our bootstrapping
approach is able to discover colors like ?grayish,?
?chestnut,? ?emerald,? and ?rufous? that would be
hard to list manually (the last is a reddish-brown
color, somewhat like rust). Although perhaps not
easy to create, the Wikipedia list of colors (http:
//en.wikipedia.org/wiki/List of colors) includes all of these
except ?grayish?. On the other hand, it includes
color terms that might be difficult to make use of as
colors, such as ?bisque,? ?bone? and ?bubbles? (the
last is a very light cyan), which might over-generate
hits. For shape, we find ?oblong,? ?hemispherical,?
?quadrangular? and, our favorite, ?convex?.
We use essentially the same bootstrapping process
as described earlier in Section 3.1, but on a slightly
different data representation. The only difference is
that instead of linking adjectives to their 10 most
similar neighbors, we link them only to 25 neigh-
bors to attempt to improve recall.
We begin with seeds for each attribute class from
Table 4. We conduct a manual evaluation to di-
rectly measure the quality of attribute classes. We
recruited 3 annotators and developed annotation
guidelines that instructed each recruiter to judge
whether a learned value belongs to an attribute class
or not. The annotators assigned ?1? if a learned
value belongs to a class, otherwise ?0?.
We conduct an Information Retrieval (IR) Style
human evaluation. Analogous to an IR evaluation,
here the total number of relevant values for attribute
classes can not be computed. Therefore, we assume
the correct output of several systems as the total re-
call which can be produced by any system. Now,
with the help of our 3 manual annotators, we obtain
the correct output of several systems from the total
output produced by these systems.
First, we measured the agreement on whether
each learned value belongs to a semantic class or
not. We computed ? to measure inter-annotator
agreement for each pair of annotators. We focus
our evaluation on 4 classes: age, beauty, color, and
direction; between Human 2 and Human 3 and be-
tween Human 1 and Human 3, the ? value was 0.48;
between Human 1 and Human 2 it was 0.45. These
numbers are somewhat lower than we would like,
but not terrible. If we evaluate the classes individu-
ally, we find that age has the lowest ?. If we remove
?age,? the pairwise ?s rise to 0.59, 0.57 and 0.55.
Second, we compute Precision (Pr), Recall (Rec)
and F-measure (F1) for different bootstrapping sys-
tems (based on the number of iterations and the
number of new words added in each iteration).
Two parameter settings performed consistently bet-
ter than others (10 iterations with 25 items, and 5 it-
erations with 50 items). The former system achieves
a precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-
man 2; the latter achieves scores of 0.54, 0.72, 0.62.
4 Recognizing Visual Text
We train a logistic regression (aka maximum en-
tropy) model (Daume? III, 2004) to classify text as
visual or non-visual. The features we use fall into
767
the following categories: WORDS (the actual lexi-
cal items and stems); BIGRAMS (lexical bigrams);
SPELL (lexical features such as capitalization pat-
tern, and word prefixes and suffixes); WORDNET
(set of hypernyms according to WordNet); and
BOOTSTRAP (features derived from bootstrapping
or label propagation).
For each of these feature categories, we compute
features inside the phrase being categorized (e.g.,
?the car?), before the phrase (two words to the left)
and after the phrase (two words to the right). We
additionally add a feature that computes the num-
ber of words in a phrase, and a feature that com-
putes the position of the phrase in the caption (first
fifth through last fifth of the description). This leads
to seventeen feature templates that are computed for
each example. In the SMALL data set, there are 25k
features (10k non-singletons); in the LARGE data
set, there are 191k features (79k non-singletons).
To train models on the SMALL data set, we use
1500 instances as training, 200 as development and
the remaining 639 as test data. To train models on
the LARGE data set, we use 45000 instances as train-
ing and the remaining 4401 as development. We
always test on the 639 instances from the SMALL
data, since it has been redundantly annotated. The
development data is used only to choose the regular-
ization parameter for a Gaussian prior on the logis-
tic regression model; this parameter is chosen in the
range {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.
Because of the imbalanced data problem, evalu-
ating according to accuracy is not appropriate for
this task. Even evaluating by precision/recall is not
appropriate, because a baseline system that guesses
that everything is visual obtains 100% recall and
70% precision. Due to these issues, we instead
evaluate according to the area under the ROC curve
(AUC). To check statistical significance, we com-
pute standard deviations using bootstrap resampling,
and consider there to be a significant difference if a
result falls outside of two standard deviations of the
baseline (95% confidence).
Figure 3 shows learning curves for the two data
sets. The SMALL data achieves an AUC score of
71.3 in the full data setting (1700 examples); the
LARGE data needs 12k examples to achieve similar
accuracy due to noise. However, with 49k examples,
we are able to achieve a AUC score of 75.3 using the
101 102 103 104 105
0.55
0.6
0.65
0.7
0.75
0.8
Figure 3: Learning curves for training on SMALL data
(blue solid) and LARGE data (black dashed). X-axis (in
log-scale) is number of training examples; Y-axis is AUC.
large data set. By pooling the data (and weighting
the small data), this boosts results to 76.1. The con-
fidence range on these data is approximately ?1.9,
meaning that this boost is likely not significant.
4.1 Using Image Features
As discussed previously, humans are only able to
achieve 90% accuracy on the visual/non-visual task
when they are not allowed to view the image.
This potentially upper-bounds the performance of a
learned system that can only look at text. In order to
attempt to overcome this, we augment our basic sys-
tem with a number of features computed from the
corresponding images. These features are derived
from the output of state of the art vision algorithms
to detect 121 different objects, stuff and scenes.
As our object detectors, we use standard state
of the art deformable part-based models (Felzen-
szwalb et al, 2010) for 89 common object cate-
gories, including: the original 20 objects from Pas-
cal, 49 objects from Object Bank (Li-Jia Li and Fei-
Fei, 2010), and 20 from Im2Text (Ordonez et al,
2011). We additionally use coarse image parsing
to estimate background elements in each database
image. Six possible background (stuff) categories
are considered: sky, water, grass, road, tree, and
building. For this we use detectors (Ordonez et
al., 2011) which compute color, texton, HoG (Dalal
and Triggs, 2005) and Geometric Context (Hoiem
et al, 2005) as input features to a sliding win-
dow based SVM classifier. These detectors are run
on all database images, creating a large pool of
background elements for retrieval. Finally, we ob-
768
Figure 4: (Left) Highest confidence flower detected in an
image; (Right) All detections in the same image.
tain scene descriptors for each image by comput-
ing scene classification scores for 26 common scene
categories, using the features, methods and training
data from the SUN dataset (Xiao et al, 2010).
Figure 4 shows an example image on which sev-
eral detectors have been run. From each image, we
extract the following features: which object detec-
tors fired; how many times they fired; the confidence
of the most-likely firing; the percentage of the image
(in pixels) that the bounding box corresponding to
this object occupies; and the percentage of the width
(and height) of the image that it occupies.
Unfortunately, object detection is a highly noisy
process. The right image in Figure 4 shows all de-
tections for that image, which includes, for instance,
a chair detection that spans nearly the entire image,
and a person detection in the bottom-right corner.
For an average image, if a single detector (e.g., the
flower detector) fires once, it actually fires 40 times
(?? = 1.8). Moreover, of the 120 detectors, on
an average image over 22 (?? = 5.6) of them fire
at least once (though certainly in an average image
only a few objects are actually present). Exacerbat-
ing this problem, although the confidence scores for
a single detector can be compared, the scores be-
tween different detectors are not at all comparable.
In order to attenuate this problem, we include dupli-
cate copies of all the above features restricted to the
most confident object for each object type.
On the SMALL data set, this adds 400 new fea-
CATEGORY POSITION AUC
Bootstrap Phrase 65.2
+ Spell Phrase 68.6
+ Image - 69.2
+ Words Phrase 70.0
+ Length - 69.8
+ Wordnet Phrase 70.4
+ Wordnet Before 70.6
+ Spell Before 71.8
+ Words Before 72.2
+ Bootstrap Before 72.4
+ Spell After 71.5
Table 5: Results of feature ablation on SMALL data set.
Best result is in bold; results that are not statistically sig-
nificantly worse are italicized.
tures (300 of which are non-singletons4); on the
LARGE data set, this adds 500 new features (480
non-singletons). Overall, the AUC scores trained on
the small data set increase from 71.3 to 73.9 (a sig-
nificant improvement). On the large data set, the in-
crease is only from 76.1 to 76.8, which is not likely
to be significant. In general, the improvement ob-
tained by adding image features is most pronounced
in the setting of small training data, perhaps because
these features are more generic than the highly lexi-
calized features used in the textual model. But once
there is a substantial amount of text data, the noisy
image features become less useful.
4.2 Feature Ablations
In order to ascertain the degree to which each feature
template is useful, we perform an ablation study. We
first perform feature selection at the template level
using the information gain criteria, and then train
models using the corresponding subset of features.
The results on the SMALL data set are shown in
Table 5. Here, the bootstrapping features computed
on words within the phrase to be classified were
judged as the most useful, followed by spelling fea-
tures. Image features were judged third most use-
ful. In general, features in the phrase were most use-
ful (not surprisingly), and then features before the
phrase (presumably to give context, for instance as
in ?out of the window?). Features from after the
phrase were not useful.
4Non-singleton features appear more than once in the data.
769
CATEGORY POSITION AUC
Words Phrase 74.7
+ Image - 74.4
+ Bootstrap Phrase 74.3
+ Spell Phrase 75.3
+ Length - 74.7
+ Words Before 76.2
+ Wordnet Phrase 76.1
+ Spell After 76.0
+ Spell Before 76.8
+ Wordnet Before 77.0
+ Wordnet After 75.6
Table 6: Results of feature ablation on LARGE data set.
Corresponding results on the LARGE data set are
shown in Table 6. Note that the order of features
selected is different because the training data is dif-
ferent. Here, the most useful features are simply the
words in the phrase to be classified, which alone al-
ready gives an AUC score of 74.7, only a few points
off from the best performance of 77.0 once image
features, bootstrap features and spelling features are
added. As before, these features are rated as very
useful for classification performance.
Finally, we consider the effect of using Bootstrap-
based features or label-propagation-based features.
In all the above experiments, the features used
are based on the union of word lists created by
these two techniques. We perform three experi-
ments. Beginning with the system that contains all
features (SMALL=73.9, LARGE=76.8), we first re-
move the bootstrap-based features (SMALL?71.8,
LARGE?75.5) or remove the label-propagation-
based features (SMALL?71.2, LARGE?74.9) or
remove both (SMALL?70.7, LARGE?74.2). From
these results, we can see that these techniques are
useful, but somewhat redundant: if you had to
choose one, you should choose label-propagation.
5 Discussion
As connections between language and vision be-
come stronger, for instance in the contexts of ob-
ject detection (Hou and Zhang, 2007; Kim and Tor-
ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;
Gu et al, 2009), attribute detection (Ferrari and Zis-
serman, 2007; Farhadi et al, 2009; Kumar et al,
2009; Berg et al, 2010), visual phrases (Farhadi and
Sadeghi, 2011), and automatic caption generation
(Farhadi et al, 2010; Feng and Lapata, 2010; Or-
donez et al, 2011; Kulkarni et al, 2011; Yang et
al., 2011; Li et al, 2011; Mitchell et al, 2012), it
becomes increasingly important to understand, and
to be able to detect, text that actually refers to ob-
served phenomena. Our results suggest that while
this is a hard problem, it is possible to leverage large
text resources and state-of-the-art computer vision
algorithms to address it with high accuracy.
Acknowledgments
T.L. Berg and K. Yamaguchi were supported in part
by NSF Faculty Early Career Development (CA-
REER) Award #1054133; A.C. Berg and Y. Choi
were partially supported by the Stony Brook Uni-
versity Office of the Vice President for Research; H.
Daume? III and A. Goyal were partially supported by
NSF Award IIS-1139909; all authors were partially
supported by a 2011 JHU Summer Workshop.
References
B. Alexe, T. Deselaers, and V. Ferrari. 2010. What is an
object? In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pages 73 ?80.
A. Almuhareb and M. Poesio. 2005. Finding concept at-
tributes in the web. In Corpus Linguistics Conference.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteriza-
tion from noisy web data. In European Conference on
Computer Vision (ECCV).
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In CVPR.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/#daume04cg-bfgs, implementation
available at http://hal3.name/megam/, August.
770
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2010. The PASCAL
Visual Object Classes Challenge 2010 (VOC2010)
Results. http://www.pascal-network.org/challenges/VOC/
voc2010/workshop/index.html.
Ali Farhadi and Amin Sadeghi. 2011. Recognition us-
ing visual phrases. In Computer Vision and Pattern
Recognition (CVPR).
A. Farhadi, I. Endres, D. Hoiem, and D.A. Forsyth. 2009.
Describing objects by their attributes. In Computer
Vision and Pattern Recognition (CVPR).
A. Farhadi, M. Hejrati, M.A. Sadeghi, P. Young,
C. Rashtchian1, J. Hockenmaier, and D.A. Forsyth.
2010. Every picture tells a story: Generating sentences
from images. In ECCV.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
2010. Discriminatively trained deformable part
models, release 4. http://people.cs.uchicago.edu/?pff/
latent-release4/.
Y. Feng and M. Lapata. 2010. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. In Advances in Neural Information Process-
ing Systems (NIPS).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik. 2009.
Recognition using regions. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Confer-
ence on, pages 1030 ?1037.
Derek Hoiem, Alexei A. Efros, and Martial Hebert.
2005. Geometric context from a single image. In
ICCV.
Xiaodi Hou and Liqing Zhang. 2007. Saliency detection:
A spectral residual approach. In Computer Vision and
Pattern Recognition, 2007. CVPR ?07. IEEE Confer-
ence on, pages 1 ?8.
P. Ipeirotis, F. Provost, and J. Wang. 2010. Quality man-
agement on amazon mechanical turk. In Proceedings
of the Second Human Computation Workshop (KDD-
HCOMP).
Gunhee Kim and Antonio Torralba. 2009. Unsupervised
Detection of Regions of Interest using Iterative Link
Analysis. In Annual Conference on Neural Informa-
tion Processing Systems (NIPS 2009).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C
Berg, and T. L Berg. 2011. Babytalk: Understanding
and generating simple image descriptions. In CVPR.
N. Kumar, A.C. Berg, P. Belhumeur, and S.K. Nayar.
2009. Attribute and simile classifiers for face verifi-
cation. In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CONLL.
Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei. 2010. Ob-
ject bank: A high-level image representation for scene
classification and semantic feature sparsification. In
NIPS.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
K.J. Miller. 1998. Modifiers in WordNet. In C. Fell-
baum, editor, WordNet, chapter 2. MIT Press.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon?s mechanical turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements in part?of?speech tag-
ging with an application to german. In Proceedings of
the EACL SIGDAT Workshop.
J. Sivic, B.C. Russell, A. Zisserman, W.T. Freeman, and
A.A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research (JAIR),
37:141.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
771
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.
2010. Sun database: Large-scale scene recognition
from abbey to zoo. In CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In EMNLP.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
772
Proceedings of NAACL-HLT 2013, pages 148?157,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Experiments with Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
Latent-variable PCFGs (L-PCFGs) are a
highly successful model for natural language
parsing. Recent work (Cohen et al, 2012)
has introduced a spectral algorithm for param-
eter estimation of L-PCFGs, which?unlike
the EM algorithm?is guaranteed to give con-
sistent parameter estimates (it has PAC-style
guarantees of sample complexity). This paper
describes experiments using the spectral algo-
rithm. We show that the algorithm provides
models with the same accuracy as EM, but is
an order of magnitude more efficient. We de-
scribe a number of key steps used to obtain
this level of performance; these should be rel-
evant to other work on the application of spec-
tral learning algorithms. We view our results
as strong empirical evidence for the viability
of spectral methods as an alternative to EM.
1 Introduction
Latent-variable PCFGS (L-PCFGs) are a highly suc-
cessful model for natural language parsing (Mat-
suzaki et al, 2005; Petrov et al, 2006). Recent
work (Cohen et al, 2012) has introduced a spectral
learning algorithm for L-PCFGs. A crucial prop-
erty of the algorithm is that it is guaranteed to pro-
vide consistent parameter estimates?in fact it has
PAC-style guarantees of sample complexity.1 This
is in contrast to the EM algorithm, the usual method
for parameter estimation in L-PCFGs, which has the
weaker guarantee of reaching a local maximum of
the likelihood function. The spectral algorithm is
relatively simple and efficient, relying on a singular
value decomposition of the training examples, fol-
lowed by a single pass over the data where parame-
ter values are calculated.
Cohen et al (2012) describe the algorithm, and
the theory behind it, but as yet no experimental re-
sults have been reported for the method. This paper
1under assumptions on certain singular values in the model;
see section 2.3.1.
describes experiments on natural language parsing
using the spectral algorithm for parameter estima-
tion. The algorithm provides models with slightly
higher accuracy than EM (88.05% F-measure on test
data for the spectral algorithm, vs 87.76% for EM),
but is an order of magnitude more efficient (9h52m
for training, compared to 187h12m, a speed-up of
19 times).
We describe a number of key steps in obtain-
ing this level of performance. A simple backed-off
smoothing method is used to estimate the large num-
ber of parameters in the model. The spectral algo-
rithm requires functions mapping inside and outside
trees to feature vectors?we make use of features
corresponding to single level rules, and larger tree
fragments composed of two or three levels of rules.
We show that it is important to scale features by their
inverse variance, in a manner that is closely related
to methods used in canonical correlation analysis.
Negative values can cause issues in spectral algo-
rithms, but we describe a solution to these problems.
In recent work there has been a series of results in
spectral learning algorithms for latent-variable mod-
els (Vempala and Wang, 2004; Hsu et al, 2009;
Bailly et al, 2010; Siddiqi et al, 2010; Parikh et
al., 2011; Balle et al, 2011; Arora et al, 2012;
Dhillon et al, 2012; Anandkumar et al, 2012). Most
of these results are theoretical (although see Luque
et al (2012) for empirical results of spectral learn-
ing for dependency parsing). While the focus of
our experiments is on parsing, our findings should
be relevant to the application of spectral methods to
other latent-variable models. We view our results as
strong empirical evidence for the viability of spec-
tral methods as an alternative to EM.
2 Background
In this section we first give basic definitions for L-
PCFGs, and then describe the spectral learning algo-
rithm of Cohen et al (2012).
148
2.1 L-PCFGs: Basic Definitions
We follow the definition in Cohen et al (2012)
of L-PCFGs. An L-PCFG is an 8-tuple
(N , I,P,m, n, pi, t, q) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume thatN = I?P , and I?P = ?. Hence we
have partitioned the set of non-terminals into
two subsets.
? [m] is the set of possible hidden states.2
? [n] is the set of possible words.
? For all a ? I, b, c ? N , h1, h2, h3 ?
[m], we have a context-free rule a(h1) ?
b(h2) c(h3). The rule has an associated pa-
rameter t(a? b c, h2, h3|a, h1).
? For all a ? P , h ? [m], x ? [n], we have a
context-free rule a(h) ? x. The rule has an
associated parameter q(a? x|a, h).
? For all a ? I, h ? [m], pi(a, h) is a parameter
specifying the probability of a(h) being at the
root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r1 . . . rN where each ri is either of the form a? b c
or a? x. The rule sequence forms a top-down, left-
most derivation under a CFG with skeletal rules.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
For a given skeletal tree r1 . . . rN , define ai to be
the non-terminal on the left-hand-side of rule ri. For
any i ? [N ] such that ri is of the form a? b c, de-
fine h(2)i and h
(3)
i as the hidden state value of the left
and right child respectively. The model then defines
a probability mass function (PMF) as
p(r1 . . . rN , h1 . . . hN ) =
pi(a1, h1)
?
i:ai?I
t(ri, h
(2)
i , h
(3)
i |ai, hi)
?
i:ai?P
q(ri|ai, hi)
The PMF over skeletal trees is p(r1 . . . rN ) =?
h1...hN
p(r1 . . . rN , h1 . . . hN ).
2For any integer n, we use [n] to denote the set {1, 2, . . . n}.
The parsing problem is to take a sentence as in-
put, and produce a skeletal tree as output. A stan-
dard method for parsing with L-PCFGs is as follows.
First, for a given input sentence x1 . . . xn, for any
triple (a, i, j) such that a ? N and 1 ? i ? j ? n,
the marginal ?(a, i, j) is defined as
?(a, i, j) =
?
t:(a,i,j)?t
p(t) (1)
where the sum is over all skeletal trees t for
x1 . . . xn that include non-terminal a spanning
words xi . . . xj . A variant of the inside-outside
algorithm can be used to calculate marginals.
Once marginals have been computed, Good-
man?s algorithm (Goodman, 1996) is used to find
arg maxt
?
(a,i,j)?t ?(a, i, j).
3
2.2 The Spectral Learning Algorithm
We now give a sketch of the spectral learning algo-
rithm. The training data for the algorithm is a set
of skeletal trees. The output from the algorithm is a
set of parameter estimates for t, q and pi (more pre-
cisely, the estimates are estimates of linearly trans-
formed parameters; see Cohen et al (2012) and sec-
tion 2.3.1 for more details).
The algorithm takes two inputs in addition to the
set of skeletal trees. The first is an integer m, speci-
fying the number of latent state values in the model.
Typically m is a relatively small number; in our ex-
periments we test values such as m = 8, 16 or 32.
The second is a pair of functions ? and ?, that re-
spectively map inside and outside trees to feature
vectors in Rd and Rd
?
, where d and d? are integers.
Each non-terminal in a skeletal tree has an associ-
ated inside and outside tree. The inside tree for a
node contains the entire subtree below that node; the
outside tree contains everything in the tree excluding
the inside tree. We will refer to the node above the
inside tree that has been removed as the ?foot? of the
outside tree. See figure 1 for an example.
Section 3.1 gives definitions of ?(t) and ?(o)
used in our experiments. The definitions of ?(t) and
3In fact, in our implementation we calculate marginals
?(a? b c, i, k, j) for a, b, c ? N and 1 ? i ? k < j, and
?(a, i, i) for a ? N , 1 ? i ? n, then apply the CKY algorithm
to find the parse tree that maximizes the sum of the marginals.
For simplicity of presentation we will refer to marginals of the
form ?(a, i, j) in the remainder of this paper.
149
VP
V
saw
NP
D
the
N
dog
S
NP
D
the
N
cat
VP
Figure 1: The inside tree (shown left) and out-
side tree (shown right) for the non-terminal VP
in the parse tree [S [NP [D the ] [N cat]]
[VP [V saw] [NP [D the] [N dog]]]]
?(o) are typically high-dimensional, sparse feature
vectors, similar to those in log-linear models. For
example ? might track the rule immediately below
the root of the inside tree, or larger tree fragments;
? might include similar features tracking rules or
larger rule fragments above the relevant node.
The spectral learning algorithm proceeds in two
steps. In step 1, we learn an m-dimensional rep-
resentation of inside and outside trees, using the
functions ? and ? in combination with a projection
step defined through singular value decomposition
(SVD). In step 2, we derive parameter estimates di-
rectly from training examples.
2.2.1 Step 1: An SVD-Based Projection
For a given non-terminal a ? N , each instance of
a in the training data has an associated outside tree,
and an associated inside tree. We define Oa to be
the set of pairs of inside/outside trees seen with a in
the training data: each member of Oa is a pair (o, t)
where o is an outside tree, and t is an inside tree.
Step 1 of the algorithm is then as follows:
1. For each a ? N calculate ??a ? Rd?d
?
as
[??a]i,j =
1
|Oa|
?
(o,t)?Oa
?i(t)?j(o)
2. Perform an SVD on ??a. Define Ua ? Rd?m
(V a ? Rd
??m) to be a matrix containing the
m left (right) singular vectors corresponding
to the m largest singular values; define ?a ?
Rm?m to be the diagonal matrix with the m
largest singular values on its diagonal.
3. For each inside tree in the corpus with root la-
bel a, define
Y (t) = (Ua)>?(t)
For each outside tree with a foot node labeled
a, define
Z(o) = (?a)?1(V a)>?(o)
Note that Y (t) and Z(o) are both m-dimensional
vectors; thus we have used SVD to project inside
and outside trees to m-dimensional vectors.
2.3 Step 2: Parameter Estimation
We now describe how the functions Y (t) and Z(o)
are used in estimating parameters of the model.
First, consider the t(a? b c, h2, h3|a, h1) parame-
ters. Each instance of a given rule a? b c in the
training corpus has an outside tree o associated with
the parent labeled a, and inside trees t2 and t3 as-
sociated with the children labeled b and c. For any
rule a? b cwe defineQa?b c to be the set of triples
(o, t(2), t(3)) occurring with that rule in the corpus.
The parameter estimate is then
c?(a? b c, j, k|a, i) =
count(a? b c)
count(a)
? Ea?b ci,j,k
(2)
where
Ea?b ci,j,k =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))? Yk(t(3))
|Qa?b c|
Here we use count(a? b c) and count(a) to refer
to the count of the rule a? b c and the non-terminal
a in the corpus. Note that once the SVD step has
been used to compute representations Y (t) andZ(o)
for each inside and outside tree in the corpus, calcu-
lating the parameter value c?(a? b c, j, k|a, i) is a
very simple operation.
Similarly, for any rule a ? x, define Qa?x to
be the set of outside trees seen with that rule in the
training corpus. The parameter estimate is then
c?(a? x|a, i) =
count(a? x)
count(a)
? Ea?xi (3)
where Ea?xi =
?
o?Qa?x Zi(o)/|Q
a?x|.
A similar method is used for estimating parame-
ters c?(a, i) that play the role of the pi parameters (de-
tails omitted for brevity; see Cohen et al (2012)).
2.3.1 Guarantees for the Algorithm
Once the c?(a? b c, j, k|a, i), c?(a? x|a, i) and
c?(a, i) parameters have been estimated from the
150
training corpus, they can be used in place of the t,
q and pi parameters in the inside-outside algorithm
for computing marginals (see Eq. 1). Call the re-
sulting marginals ??(a, i, j). The guarantees for the
parameter estimation method are as follows:
? Define ?a = E[?(T )(?(O))>|A = a] where
A,O, T are random variables corresponding to
the non-terminal label at a node, the outside
tree, and the inside tree (see Cohen et al (2012)
for a precise definition). Note that ??a, as de-
fined above, is an estimate of ?a. Then if ?a
has rank m, the marginals ?? will converge to
the true values ? as the number of training ex-
amples goes to infinity, assuming that the train-
ing samples are i.i.d. samples from an L-PCFG.
? Define ? to be the m?th largest singular value
of ?a. Then the number of samples required
for ?? to be -close to ? with probability at least
1? ? is polynomial in 1/, 1/?, and 1/?.
Under the first assumption, (Cohen et al,
2012) show that the c? parameters converge to
values that are linear transforms of the orig-
inal parameters in the L-PCFG. For example,
define c(a? b c, j, k|a, i) to be the value that
c?(a? b c, j, k|a, i) converges to in the limit of infi-
nite data. Then there exist invertible matrices Ga ?
Rm?m for all a ? N such that for any a? b c, for
any h1, h2, h3 ? Rm,
t(a? b c, h2, h3|a, h1) =
?
i,j,k
[Ga]i,h1 [(G
b)?1]j,h2 [(G
c)?1]k,h3c(a? b c, j, k|a, i)
The transforms defined by the Ga matrices are be-
nign, in that they cancel in the inside-outside algo-
rithm when marginals ?(a, i, j) are calculated. Sim-
ilar relationships hold for the pi and q parameters.
3 Implementation of the Algorithm
Cohen et al (2012) introduced the spectral learning
algorithm, but did not perform experiments, leaving
several choices open in how the algorithm is imple-
mented in practice. This section describes a number
of key choices made in our implementation of the
algorithm. In brief, they are as follows:
The choice of functions ? and ?. We will de-
scribe basic features used in ? and ? (single-level
rules, larger tree fragments, etc.). We will also de-
scribe a method for scaling different features in ?
and ? by their variance, which turns out to be im-
portant for empirical results.
Estimation ofEa?b ci,j,k andE
a?x
i . There are a very
large number of parameters in the model, lead-
ing to challenges in estimation. The estimates in
Eqs. 2 and 3 are unsmoothed. We describe a simple
backed-off smoothing method that leads to signifi-
cant improvements in performance of the method.
Handling positive and negative values. As de-
fined, the c? parameters may be positive or negative;
as a result, the ?? values may also be positive or neg-
ative. We find that negative values can be a signif-
icant problem if not handled correctly; but with a
very simple fix to the algorithm, it performs well.
We now turn to these three issues in more detail.
Section 4 will describe experiments measuring the
impact of the different choices.
3.1 The Choice of Functions ? and ?
Cohen et al (2012) show that the choice of feature
definitions ? and ? is crucial in two respects. First,
for all non-terminals a ? N , the matrix ?a must
be of rank m: otherwise the parameter-estimation
algorithm will not be consistent. Second, the num-
ber of samples required for learning is polynomial
in 1/?, where ? = mina?N ?m(?a), and ?m(?a)
is the m?th smallest singular value of ?a. (Note that
the second condition is stronger than the first; ? > 0
implies that ?a is of rank m for all a.) The choice
of ? and ? has a direct impact on the value for ?:
roughly speaking, the value for ? can be thought of
as a measure of how informative the functions ? and
? are about the hidden state values.
With this in mind, our goal is to define a rel-
atively simple set of features, which nevertheless
provide significant information about hidden-state
values, and hence provide high accuracy under the
model. The inside-tree feature function ?(t) makes
use of the following indicator features (throughout
these definitions assume that a? b c is at the root
of the inside tree t):
? The pair of nonterminals (a, b). E.g., for the in-
side tree in figure 1 this would be the pair (VP, V).
151
? The pair (a, c). E.g., (VP, NP).
? The rule a? b c. E.g., VP ? V NP.
? The rule a? b c paired with the rule at the
root of t(i,2). E.g., for the inside tree in fig-
ure 1 this would correspond to the tree fragment
(VP (V saw) NP).
? The rule a? b c paired with the rule at
the root of t(i,3). E.g., the tree fragment
(VP V (NP D N)).
? The head part-of-speech of t(i,1) paired with a.4
E.g., the pair (VP, V).
? The number of words dominated by t(i,1) paired
with a (this is an integer valued feature).
In the case of an inside tree consisting of a single
rule a? x the feature vector simply indicates the
identity of that rule.
To illustrate the function ?, it will be useful to
make use of the following example outside tree:
S
NP
D
the
N
cat
VP
V
saw
NP
D N
dog
Note that in this example the foot node of the out-
side tree is labeled D. The features are as follows:
? The rule above the foot node. We take care
to mark which non-terminal is the foot, using a
* symbol. In the above example this feature is
NP ? D? N.
? The two-level and three-level rule fragments
above the foot node. In the above example these fea-
tures would be
VP
V NP
D? N
S
NP VP
V NP
D? N
? The label of the foot node, together with the
label of its parent. In the above example this is
(D, NP).
? The label of the foot node, together with the la-
bel of its parent and grandparent. In the above ex-
ample this is (D, NP, VP).
? The part of speech of the first head word along
the path from the foot of the outside tree to the root
of the tree which is different from the head node of
4We use the English head rules from the Stanford parser
(Klein and Manning, 2003).
the foot node. In the above example this is N.
? The width of the span to the left of the foot node,
paired with the label of the foot node.
? The width of the span to the right of the foot
node, paired with the label of the foot node.
Scaling of features. The features defined above
are almost all binary valued features. We scale the
features in the following way. For each feature ?i(t),
define count(i) to be the number of times the feature
is equal to 1, and M to be the number of training
examples. The feature is then redefined to be
?i(t)?
?
M
count(i) + ?
where ? is a smoothing term (the method is rela-
tively insensitive to the choice of ?; we set ? = 5 in
our experiments). A similar process is applied to the
? features. The method has the effect of decreasing
the importance of more frequent features in the SVD
step of the algorithm.
The SVD-based step of the algorithm is very
closely related to previous work on CCA (Hotelling,
1936; Hardoon et al, 2004; Kakade and Foster,
2009); and the scaling step is derived from previ-
ous work on CCA (Dhillon et al, 2011). In CCA
the ? and ? vectors are ?whitened? in a preprocess-
ing step, before an SVD is applied. This whiten-
ing process involves calculating covariance matrices
Cx = E[??>] and Cy = E[??>], and replacing ?
by (Cx)?1/2? and ? by (Cy)?1/2?. The exact cal-
culation of (Cx)?1/2 and (Cy)?1/2 is challenging in
high dimensions, however, as these matrices will not
be sparse; the transformation described above can
be considered an approximation where off-diagonal
members of Cx and Cy are set to zero. We will see
that empirically this scaling gives much improved
accuracy.
3.2 Estimation of Ea?b ci,j,k and E
a?x
i
The number of Ea?b ci,j,k parameters is very large,
and the estimation method described in Eqs. 2?3 is
unsmoothed. We have found significant improve-
ments in performance using a relatively simple back-
off smoothing method. The intuition behind this
method is as follows: given two random variablesX
and Y , under the assumption that the random vari-
ables are independent, E[XY ] = E[X] ? E[Y ]. It
152
makes sense to define ?backed off? estimates which
make increasingly strong independence assumptions
of this form.
Smoothing of binary rules For any rule a? b c
and indices i, j ? [m] we can define a second-order
moment as follows:
Ea?b ci,j,? =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))
|Qa?b c|
The definitions ofEa?b ci,?,k andE
a?b c
?,j,k are analogous.
We can define a first-order estimate as follows:
Ea?b c?,?,k =
?
(o,t(2),t(3))
?Qa?b c
Yk(t(3))
|Qa?b c|
Again, we have analogous definitions of Ea?b ci,?,? and
Ea?b c?,j,? . Different levels of smoothed estimate can
be derived from these different terms. The first is
E2,a?b ci,j,k =
Ea?b ci,j,? ? E
a?b c
?,?,k + E
a?b c
i,?,k ? E
a?b c
?,j,? + E
a?b c
?,j,k ? E
a?b c
i,?,?
3
Note that we give an equal weight of 1/3 to each of
the three backed-off estimates seen in the numerator.
A second smoothed estimate is
E3,a?b ci,j,k = E
a?b c
i,?,? ? E
a?b c
?,j,? ? E
a?b c
?,?,k
Using the definition of Oa given in section 2.2.1, we
also define
F ai =
?
(o,t)?Oa Yi(t)
|Oa|
Hai =
?
(o,t)?Oa Zi(o)
|Oa|
and our next smoothed estimate asE4,a?b ci,j,k = H
a
i ?
F bj ? F
c
k .
Our final estimate is
?Ea?b ci,j,k + (1? ?)
(
?E2,a?b ci,j,k + (1? ?)K
a?b c
i,j,k
)
where Ka?b ci,j,k = ?E
3,a?b c
i,j,k + (1? ?)E
4,a?b c
i,j,k .
Here ? ? [0, 1] is a smoothing parameter, set to?
|Qa?b c|/(C +
?
|Qa?b c|) in our experiments,
where C is a parameter that is chosen by optimiza-
tion of accuracy on a held-out set of data.
Smoothing lexical rules We define a similar
method for the Ea?xi parameters. Define
Eai =
?
x?
?
o?Qa?x? Zi(o)
?
x? |Q
a?x? |
hence Eai ignores the identity of x in making its es-
timate. The smoothed estimate is then defined as
?Ea?xi +(1??)E
a
i . Here, ? is a value in [0, 1] which
is tuned on a development set. We only smooth lex-
ical rules which appear in the data less than a fixed
number of times. Unlike binary rules, for which the
estimation depends on a high order moment (third
moment), the lexical rules use first-order moments,
and therefore it is not required to smooth rules with
a relatively high count. The maximal count for this
kind of smoothing is set using a development set.
3.3 Handling Positive and Negative Values
As described before, the parameter estimates may
be positive or negative, and as a result the
marginals computed by the algorithm may in some
cases themselves be negative. In early exper-
iments we found this to be a signficant prob-
lem, with some parses having a very large num-
ber of negatives, and being extremely poor in qual-
ity. Our fix is to define the output of the parser
to be arg maxt
?
(a,i,j)?t |?(a, i, j)| rather than
arg maxt
?
(a,i,j)?t ?(a, i, j) as defined in Good-
man?s algorithm. Thus if a marginal value ?(a, i, j)
is negative, we simply replace it with its absolute
value. This step was derived after inspection of the
parsing charts for bad parses, where we saw evi-
dence that in these cases the entire set of marginal
values had been negated (and hence decoding under
Eq. 1 actually leads to the lowest probability parse
being output under the model). We suspect that this
is because in some cases a dominant parameter has
had its sign flipped due to sampling error; more the-
oretical and empirical work is required in fully un-
derstanding this issue.
4 Experiments
In this section we describe parsing experiments us-
ing the L-PCFG estimation method. We give com-
parisons to the EM algorithm, considering both
speed of training, and accuracy of the resulting
model; we also give experiments investigating the
various choices described in the previous section.
153
We use the Penn WSJ treebank (Marcus et al,
1993) for our experiments. Sections 2?21 were
used as training data, and sections 0 and 22 were
used as development data. Section 23 is used as
the final test set. We binarize the trees in train-
ing data using the same method as that described in
Petrov et al (2006). For example, the non-binary
rule VP ? V NP PP SBAR would be converted
to the structure [VP [@VP [@VP V NP] PP]
SBAR] where @VP is a new symbol in the grammar.
Unary rules are removed by collapsing non-terminal
chains: for example the unary rule S ? VP would
be replaced by a single non-terminal S|VP.
For the EM algorithm we use the initialization
method described in Matsuzaki et al (2005). For ef-
ficiency, we use a coarse-to-fine algorithm for pars-
ing with either the EM or spectral derived gram-
mar: a PCFG without latent states is used to calcu-
late marginals, and dynamic programming items are
removed if their marginal probability is lower than
some threshold (0.00005 in our experiments).
For simplicity the parser takes part-of-speech
tagged sentences as input. We use automatically
tagged data from Turbo Tagger (Martins et al,
2010). The tagger is used to tag both the devel-
opment data and the test data. The tagger was re-
trained on sections 2?21. We use the F1 measure
according to the Parseval metric (Black et al, 1991).
For the spectral algorithm, we tuned the smoothing
parameters using section 0 of the treebank.
4.1 Comparison to EM: Accuracy
We compare models trained using EM and the spec-
tral algorithm using values form in {8, 16, 24, 32}.5
For EM, we found that it was important to use de-
velopment data to choose the number of iterations
of training. We train the models for 100 iterations,
then test accuracy of the model on section 22 (devel-
opment data) at different iteration numbers. Table 1
shows that a peak level of accuracy is reached for all
values of m, other than m = 8, at iteration 20?30,
with sometimes substantial overtraining beyond that
point.
The performance of a regular PCFG model, esti-
mated using maximum likelihood and with no latent
5Lower values of m, such as 2 or 4, lead to substantially
lower performance for both models.
section 22 section 23
EM spectral EM spectral
m = 8 86.87 85.60 ? ?
m = 16 88.32 87.77 ? ?
m = 24 88.35 88.53 ? ?
m = 32 88.56 88.82 87.76 88.05
Table 2: Results on the development data (section 22,
with machine-generated POS tags) and test data (section
23, with machine-generated POS tags).
states, is 68.62%.
Table 2 gives results for the EM-trained models
and spectral-trained models. The spectral models
give very similar accuracy to the EM-trained model
on the test set. Results on the development set with
varying m show that the EM-based models perform
better for m = 8, but that the spectral algorithm
quickly catches up as m increases.
4.2 Comparison to EM: Training Speed
Table 3 gives training times for the EM algorithm
and the spectral algorithm for m ? {8, 16, 24, 32}.
All timing experiments were done on a single Intel
Xeon 2.67GHz CPU. The implementations for the
EM algorithm and the spectral algorithm were writ-
ten in Java. The spectral algorithm also made use
of Matlab for several matrix calculations such as the
SVD calculation.
For EM we show the time to train a single iter-
ation, and also the time to train the optimal model
(time for 30 iterations of training for m = 8, 16, 24,
and time for 20 iterations for m = 32). Note that
this latter time is optimistic, as it assumes an oracle
specifying exactly when it is possible to terminate
EM training with no loss in performance. The spec-
tral method is considerably faster than EM: for ex-
ample, for m = 32 the time for training the spectral
model is just under 10 hours, compared to 187 hours
for EM, a factor of almost 19 times faster.6
The reason for these speed ups is as follows.
Step 1 of the spectral algorithm (feature calculation,
transfer + scaling, and SVD) is not required by EM,
but takes a relatively small amount of time (about
1.2 hours for all values of m). Once step 1 has been
completed, step 2 of the spectral algorithm takes a
6In practice, in order to overcome the speed issue with EM
training, we parallelized the E-step on multiple cores. The spec-
tral algorithm can be similarly parallelized, computing statistics
and parameters for each nonterminal separately.
154
10 20 30 40 50 60 70 80 90 100
m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83
m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24
m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80
m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57
Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used. Best results in each row
are in boldface.
single EM spectral algorithm
EM iter. best model total feature transfer + scaling SVD a? b c a? x
m = 8 6m 3h 3h32m
22m 49m
36m 1h34m 10m
m = 16 52m 26h6m 5h19m 34m 3h13m 19m
m = 24 3h7m 93h36m 7h15m 36m 4h54m 28m
m = 32 9h21m 187h12m 9h52m 35m 7h16m 41m
Table 3: Running time for the EM algorithm and the various stages in the spectral algorithm. For EM we show the
time for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,
time for 20 iterations of training for m = 32). For the spectral method we show the following: ?total? is the total
training time; ?feature? is the time to compute the ? and ? vectors for all data points; ?transfer + scaling? is time
to transfer the data from Java to Matlab, combined with the time for scaling of the features; ?SVD? is the time for
the SVD computation; a? b c is the time to compute the c?(a? b c, h2, h3|a, h1) parameters; a? x is the time to
compute the c?(a? x, h|a, h) parameters. Note that ?feature? and ?transfer + scaling? are the same step for all values
of m, so we quote a single runtime for these steps.
single pass over the data: in contrast, EM requires
a few tens of passes (certainly more than 10 passes,
from the results in table 1). The computations per-
formed by the spectral algorithm in its single pass
are relatively cheap. In contrast to EM, the inside-
outside algorithm is not required; however various
operations such as calculating smoothing terms in
the spectral method add some overhead. The net re-
sult is that form = 32 the time for training the spec-
tral method takes a very similar amount of time to a
single pass of the EM algorithm.
4.3 Smoothing, Features, and Negatives
We now describe experiments demonstrating the im-
pact of various components described in section 3.
The effect of smoothing (section 3.2) Without
smoothing, results on section 22 are 85.05% (m =
8, ?1.82), 86.84% (m = 16, ?1.48), 86.47%
(m = 24, ?1.88), 86.47% (m = 32, ?2.09) (in
each case we show the decrease in performance from
the results in table 2). Smoothing is clearly impor-
tant.
Scaling of features (section 3.1) Without scaling
of features, the accuracy on section 22 with m = 32
is 84.40%, a very significant drop from the 88.82%
accuracy achieved with scaling.
Handling negative values (section 3.3) Replac-
ing marginal values ?(a, i, j) with their absolute
values is also important: without this step, accu-
racy on section 22 decreases to 80.61% (m = 32).
319 sentences out of 1700 examples have different
parses when this step is implemented, implying that
the problem with negative values described in sec-
tion 3.3 occurs on around 18% of all sentences.
The effect of feature functions To test the effect
of features on accuracy, we experimented with a
simpler set of features than those described in sec-
tion 3.1. This simple set just includes an indicator
for the rule below a nonterminal (for inside trees)
and the rule above a nonterminal (for outside trees).
Even this simpler set of features achieves relatively
high accuracy (m = 8: 86.44 , m = 16: 86.86,
m = 24: 87.24 , m = 32: 88.07 ).
This set of features is reminiscent of a PCFG
model where the nonterminals are augmented their
parents (vertical Markovization of order 2) and bina-
rization is done while retaining sibling information
(horizontal Markovization of order 1). See Klein
and Manning (2003) for more information. The per-
155
formance of this Markovized PCFG model lags be-
hind the spectral model: it is 82.59%. This is prob-
ably due to the complexity of the grammar which
causes ovefitting. Condensing the sibling and parent
information using latent states as done in the spectral
model leads to better generalization.
It is important to note that the results for both
EM and the spectral algorithm are comparable to
state of the art, but there are other results previ-
ously reported in the literature which are higher.
For example, Hiroyuki et al (2012) report an ac-
curacy of 92.4 F1 on section 23 of the Penn WSJ
treebank using a Bayesian tree substitution gram-
mar; Charniak and Johnson (2005) report accuracy
of 91.4 using a discriminative reranking model; Car-
reras et al (2008) report 91.1 F1 accuracy for a dis-
criminative, perceptron-trained model; Petrov and
Klein (2007) report an accuracy of 90.1 F1, using
L-PCFGs, but with a split-merge training procedure.
Collins (2003) reports an accuracy of 88.2 F1, which
is comparable to the results in this paper.
5 Conclusion
The spectral learning algorithm gives the same level
of accuracy as EM in our experiments, but has sig-
nificantly faster training times. There are several ar-
eas for future work. There are a large number of pa-
rameters in the model, and we suspect that more so-
phisticated regularization methods than the smooth-
ing method we have described may improve perfor-
mance. Future work should also investigate other
choices for the functions ? and ?. There are natu-
ral ways to extend the approach to semi-supervised
learning; for example the SVD step, where repre-
sentations of outside and inside trees are learned,
could be applied to unlabeled data parsed by a first-
pass parser. Finally, the methods we have described
should be applicable to spectral learning for other
latent variable models.
Acknowledgements
Columbia University gratefully acknowledges the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA,
AFRL, or the US government. Shay Cohen was
supported by the National Science Foundation un-
der Grant #1136996 to the Computing Research As-
sociation for the CIFellows Project. Dean Foster
was supported by National Science Foundation grant
1106743.
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for learn-
ing latent-variable models. arXiv:1210.7559.
S. Arora, R. Se, and A. Moitra. 2012. Learning topic
models - going beyond SVD. In Proceedings of
FOCS.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In Proceedings of CoNLL, pages
9?16.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In Pro-
ceedings of NIPS.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with latent
variables. In Proceedings of EMNLP.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
156
D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004.
Canonical correlation analysis: An overview with ap-
plication to learning methods. Neural Computation,
16(12):2639?2664.
S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.
2012. Bayesian symbol-refined tree substitution gram-
mars for syntactic parsing. In Proceedings of ACL,
pages 440?448.
H. Hotelling. 1936. Relations between two sets of vari-
ants. Biometrika, 28:321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
S. M. Kakade and D. P. Foster. 2009. Multi-view regres-
sion via canonical correlation analysis. In COLT.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. T.
Figueiredo, and M. Q. Aguiar. 2010. TurboParsers:
Dependency parsing by approximate variational infer-
ence. In Proceedings of EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. JMLR, 9:741?748.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
157
Tutorials, NAACL-HLT 2013, pages 13?15,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Spectral Learning Algorithms for
Natural Language Processing
Shay Cohen?, Michael Collins?, Dean P. Foster?, Karl Stratos?, Lyle Ungar?
?Columbia University
?University of Pennsylvania
scohen,mcollins,stratos@cs.columbia.edu
dean@foster.net
ungar@cis.upenn.edu
1 Introduction
Recent work in machine learning and NLP has developed spectral algorithms for
many learning tasks involving latent variables. Spectral algorithms rely on sin-
gular value decomposition as a basic operation, usually followed by some simple
estimation method based on the method of moments. From a theoretical point
of view, these methods are appealing in that they offer consistent estimators (and
PAC-style guarantees of sample complexity) for several important latent-variable
models. This is in contrast to the EM algorithm, which is an extremely success-
ful approach, but which only has guarantees of reaching a local maximum of the
likelihood function.
From a practical point of view, the methods (unlike EM) have no need for
careful initialization, and have recently been shown to be highly efficient (as one
example, in work under submission by the authors on learning of latent-variable
PCFGs, a spectral algorithm performs at identical accuracy to EM, but is around
20 times faster).
2 Outline
In this tutorial we will aim to give a broad overview of spectral methods, describing
theoretical guarantees, as well as practical issues. We will start by covering the
basics of singular value decomposition and describe efficient methods for doing
singular value decomposition. The SVD operation is at the core of most spectral
algorithms that have been developed.
13
We will then continue to cover canonical correlation analysis (CCA). CCA is an
early method from statistics for dimensionality reduction. With CCA, two or more
views of the data are created, and they are all projected into a lower dimensional
space which maximizes the correlation between the views. We will review the
basic algorithms underlying CCA, give some formal results giving guarantees for
latent-variable models and also describe how they have been applied recently to
learning lexical representations from large quantities of unlabeled data. This idea
of learning lexical representations can be extended further, where unlabeled data is
used to learn underlying representations which are subsequently used as additional
information for supervised training.
We will also cover how spectral algorithms can be used for structured predic-
tion problems with sequences and parse trees. A striking recent result by Hsu,
Kakade and Zhang (2009) shows that HMMs can be learned efficiently using a
spectral algorithm. HMMs are widely used in NLP and speech, and previous al-
gorithms (typically based on EM) were guaranteed to only reach a local maximum
of the likelihood function, so this is a crucial result. We will review the basic me-
chanics of the HMM learning algorithm, describe its formal guarantees, and also
cover practical issues.
Last, we will cover work about spectral algorithms in the context of natural
language parsing. We will show how spectral algorithms can be used to estimate
the parameter models of latent-variable PCFGs, a model which serves as the base
for state-of-the-art parsing models such as the one of Petrov et al (2007). We will
show what are the practical steps that are needed to be taken in order to make spec-
tral algorithms for L-PCFGs (or other models in general) practical and comparable
to state of the art.
3 Speaker Bios
Shay Cohen1 is a postdoctoral research scientist in the Department of Computer
Science at Columbia University. He is a computing innovation fellow. His re-
search interests span a range of topics in natural language processing and machine
learning. He is especially interested in developing efficient and scalable parsing
algorithms as well as learning algorithms for probabilistic grammars.
Michael Collins2 is the Vikram S. Pandit Professor of computer science at
Columbia University. His research is focused on topics including statistical pars-
ing, structured prediction problems in machine learning, and applications including
machine translation, dialog systems, and speech recognition. His awards include a
1http://www.cs.columbia.edu/?scohen/
2http://www.cs.columbia.edu/?mcollins/
14
Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,
2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.
Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-
tics at the Wharton School of the University of Pennsylvania. His current research
interests are machine learning, stepwise regression and computational linguistics.
He has been searching for new methods of finding useful features in big data sets.
His current set of hammers revolve around fast matrix methods (which decompose
2nd moments) and tensor methods for decomposing 3rd moments.
Karl Stratos4 is a Ph.D. student in the Department of Computer Science at
Columbia. His research is focused on machine learning and natural language pro-
cessing. His current research efforts are focused on spectral learning of latent-
variable models, or more generally, uncovering latent structure from data.
Lyle Ungar5 is a professor at the Computer and Information Science Depart-
ment at the University of Pennsylvania. His research group develops scalable ma-
chine learning and text mining methods, including clustering, feature selection,
and semi-supervised and multi-task learning for natural language, psychology, and
medical research. Example projects include spectral learning of language models,
multi-view learning for gene expression and MRI data, and mining social media to
better understand personality and well-being.
3http://gosset.wharton.upenn.edu/?foster/index.pl
4http://www.cs.columbia.edu/?stratos/
5http://www.cis.upenn.edu/?ungar/
15
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?231,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al, 2006).
Under a separability (singular value) condi-
tion, we prove that the method provides con-
sistent parameter estimates.
1 Introduction
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other fields. The EM algorithm is
a remarkably successful method for parameter esti-
mation within these models: it is simple, it is often
relatively efficient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of finding the global optimum of
the likelihood function. From a theoretical perspec-
tive, this means that the EM algorithm is not guar-
anteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difficult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation meth-
ods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov mod-
els (Hsu et al, 2009). These algorithms use spec-
tral methods: that is, algorithms based on eigen-
vector decompositions of linear systems, in particu-
lar singular value decomposition (SVD). In the gen-
eral case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods finesse
the problem of intractibility by assuming separabil-
ity conditions. For example, the algorithm of Hsu
et al (2009) has a sample complexity that is polyno-
mial in 1/?, where ? is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al, 2006; Matsuzaki et al, 2005). Our
method involves a significant extension of the tech-
niques from Hsu et al (2009). L-PCFGs have been
shown to be a very effective model for natural lan-
guage parsing. Under a separation (singular value)
condition, our algorithm provides consistent param-
eter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter es-
timation, with the usual problems of local optima.
The parameter estimation algorithm (see figure 4)
is simple and efficient. The first step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a low-
dimensional space. In a second step, empirical av-
erages are calculated on the training example, fol-
lowed by standard matrix operations. On test ex-
amples, simple (tensor-based) variants of the inside-
outside algorithm (figures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
? Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calcu-
lates inside and outside terms correctly.
? Observable representations. Section 6 shows
that under a singular-value condition, there is an ob-
servable form for the tensors required by the inside-
outside algorithm. By an observable form, we fol-
low the terminology of Hsu et al (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the ob-
servable form satisfy the conditions of theorem 1.
? Estimating the model. Section 7 gives an al-
gorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
sample complexity result, showing that the estimates
converge to the true distribution at a rate of 1/
?
M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-
223
veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for exam-
ple alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the inside-
outside algorithm gives a new view of basic calcula-
tions in PCFGs, and may itself lead to new models.
2 Related Work
For work on L-PCFGs using the EM algorithm, see
Petrov et al (2006), Matsuzaki et al (2005), Pereira
and Schabes (1992). Our work builds on meth-
ods for learning of HMMs (Hsu et al, 2009; Fos-
ter et al, 2012; Jaeger, 2000), but involves sev-
eral extensions: in particular in the tensor form of
the inside-outside algorithm, and observable repre-
sentations for the tensor form. Balle et al (2011)
consider spectral learning of finite-state transducers;
Lugue et al (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al
(2011) consider spectral learning algorithms of tree-
structured directed bayes nets.
3 Notation
Given a matrix A or a vector v, we write A? or v?
for the associated transpose. For any integer n ? 1,
we use [n] to denote the set {1, 2, . . . n}. For any
row or column vector y ? Rm, we use diag(y) to
refer to the (m?m) matrix with diagonal elements
equal to yh for h = 1 . . . m, and off-diagonal ele-
ments equal to 0. For any statement ?, we use [[?]]
to refer to the indicator function that is 1 if ? is true,
and 0 if ? is false. For a random variable X, we use
E[X] to denote its expected value.
We will make (quite limited) use of tensors:
Definition 1 A tensor C ? R(m?m?m) is a set of
m3 parameters Ci,j,k for i, j, k ? [m]. Given a ten-
sor C , and a vector y ? Rm, we define C(y) to be
the (m ? m) matrix with components [C(y)]i,j =
?
k?[m]Ci,j,kyk. Hence C can be interpreted as a
function C : Rm ? R(m?m) that maps a vector
y ? Rm to a matrix C(y) of dimension (m?m).
In addition, we define the tensor C? ? R(m?m?m)
for any tensor C ? R(m?m?m) to have values
[C?]i,j,k = Ck,j,i
Finally, for vectors x, y, z ? Rm, xy?z? is the
tensor D ? Rm?m?m where Dj,k,l = xjykzl (this
is analogous to the outer product: [xy?]j,k = xjyk).
4 L-PCFGs: Basic Definitions
This section gives a definition of the L-PCFG for-
malism used in this paper. An L-PCFG is a 5-tuple
(N ,I,P,m, n) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We assume
that N = I ? P, and I ? P = ?. Hence we have
partitioned the set of non-terminals into two subsets.
? [m] is the set of possible hidden states.
? [n] is the set of possible words.
? For all a ? I , b ? N , c ? N , h1, h2, h3 ? [m],
we have a context-free rule a(h1) ? b(h2) c(h3).
? For all a ? P, h ? [m], x ? [n], we have a
context-free rule a(h) ? x.
Hence each in-terminal a ? I is always the left-
hand-side of a binary rule a ? b c; and each pre-
terminal a ? P is always the left-hand-side of a
rule a ? x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We define the set of possible ?skeletal rules? as
R = {a ? b c : a ? I, b ? N , c ? N}. The
parameters of the model are as follows:
? For each a? b c ? R, and h ? [m], we have
a parameter q(a ? b c|h, a). For each a ? P,
x ? [n], and h ? [m], we have a parameter
q(a ? x|h, a). For each a ? b c ? R, and
h, h? ? [m], we have parameters s(h?|h, a ? b c)
and t(h?|h, a? b c).
These definitions give a PCFG, with rule proba-
bilities
p(a(h1) ? b(h2) c(h3)|a(h1)) =
q(a? b c|h1, a)? s(h2|h1, a? b c)? t(h3|h1, a? b c)
and p(a(h) ? x|a(h)) = q(a? x|h, a).
In addition, for each a ? I , for each h ? [m], we
have a parameter ?(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG defines a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 . . . rN where each ri is either of the form
a ? b c or a ? x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See figure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
224
S1
NP2
D3
the
N4
dog
VP5
V6
saw
P7
him
r1 = S ? NP VP
r2 = NP ? D N
r3 = D ? the
r4 = N ? dog
r5 = VP ? V P
r6 = V ? saw
r7 = P ? him
Figure 1: An s-tree, and its sequence of rules. (For con-
venience we have numbered the nodes in the tree.)
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
Define ai to be the non-terminal on the left-hand-
side of rule ri. For any i ? {2 . . . N} define pa(i)
to be the index of the rule above node i in the tree.
Define L ? [N ] to be the set of nodes in the tree
which are the left-child of some parent, and R ?
[N ] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
p(r1 . . . rN , h1 . . . hN ) = ?(a1, h1)
?
N
?
i=1
q(ri|hi, ai)?
?
i?L
s(hi|hpa(i), rpa(i))
?
?
i?R
t(hi|hpa(i), rpa(i)) (1)
The PMF over s-trees is p(r1 . . . rN ) =
?
h1...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper, we make use of ma-
trix form of parameters of an L-PCFG, as follows:
? For each a? b c ? R, we define Qa?b c ?
Rm?m to be the matrix with values q(a ? b c|h, a)
for h = 1, 2, . . . m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a ? P,
x ? [n], we define Qa?x ? Rm?m to be the matrix
with values q(a ? x|h, a) for h = 1, 2, . . . m on its
diagonal, and 0 values for its off-diagonal elements.
? For each a ? b c ? R, we define Sa?b c ?
Rm?m where [Sa?b c]h?,h = s(h?|h, a? b c).
? For each a ? b c ? R, we define T a?b c ?
Rm?m where [T a?b c]h?,h = t(h?|h, a? b c).
? For each a ? I , we define the vector ?a ? Rm
where [?a]h = ?(a, h).
5 Tensor Form of the Inside-Outside
Algorithm
Given an L-PCFG, two calculations are central:
Inputs: s-tree r1 . . . rN , L-PCFG (N , I,P ,m, n), parameters
? Ca?b c ? R(m?m?m) for all a? b c ? R
? c?a?x ? R(1?m) for all a ? P , x ? [n]
? c1a ? R(m?1) for all a ? I.
Algorithm: (calculate the f i terms bottom-up in the tree)
? For all i ? [N ] such that ai ? P , f i = c?ri
? For all i ? [N ] such that ai ? I, f i = f?Cri(f?) where
? is the index of the left child of node i in the tree, and ?
is the index of the right child.
Return: f1c1a1 = p(r1 . . . rN)
Figure 2: The tensor form for calculation of p(r1 . . . rN ).
1. For a given s-tree r1 . . . rN , calculate
p(r1 . . . rN ).
2. For a given input sentence x = x1 . . . xN , cal-
culate the marginal probabilities
?(a, i, j) =
?
??T (x):(a,i,j)??
p(?)
for each non-terminal a ? N , for each (i, j)
such that 1 ? i ? j ? N .
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) ? ? if non-
terminal a spans words xi . . . xj in the parse tree ? .
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 . . . xN , the parsing algorithm of Goodman (1996)
can be used to find
arg max
??T (x)
?
(a,i,j)??
?(a, i, j)
This is the parsing algorithm used by Petrov et al
(2006), for example. In addition, we can calcu-
late the probability for an input sentence, p(x) =
?
??T (x) p(?), as p(x) =
?
a?I ?(a, 1, N).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the first step in deriving the spectral estimation
method.
The algorithms are shown in figures 2 and 3. Each
algorithm takes the following inputs:
1. A tensor Ca?b c ? R(m?m?m) for each rule
a? b c.
2. A vector c?a?x ? R(1?m) for each rule a? x.
225
3. A vector c1a ? R(m?1) for each a ? I .
The following theorem gives conditions under
which the algorithms are correct:
Theorem 1 Assume that we have an L-PCFG with
parameters Qa?x, Qa?b c, T a?b c, Sa?b c, ?a, and
that there exist matrices Ga ? R(m?m) for all a ?
N such that each Ga is invertible, and such that:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1
3. For all a ? I , c1a = Ga?a
Then: 1) The algorithm in figure 2 correctly com-
putes p(r1 . . . rN ) under the L-PCFG. 2) The algo-
rithm in figure 3 correctly computes the marginals
?(a, i, j) under the L-PCFG.
Proof: See section 9.1.
6 Estimating the Tensor Model
A crucial result is that it is possible to directly esti-
mate parameters Ca?b c, c?a?x and c1a that satisfy the
conditions in theorem 1, from a training sample con-
sisting of s-trees (i.e., trees where hidden variables
are unobserved). We first describe random variables
underlying the approach, then describe observable
representations based on these random variables.
6.1 Random Variables Underlying the Approach
Each s-tree with N rules r1 . . . rN has N nodes. We
will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in figure 1 has the rule NP? D N.
If the rule at a node is of the form a? b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule pro-
duction, of the form a ? x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
S
NP VP
V
saw
P
him
Inputs: Sentence x1 . . . xN , L-PCFG (N , I,P ,m, n), param-
eters Ca?b c ? R(m?m?m) for all a? b c ? R, c?a?x ?
R(1?m) for all a ? P , x ? [n], c1a ? R(m?1) for all a ? I.
Data structures:
? Each ?a,i,j ? R1?m for a ? N , 1 ? i ? j ? N is a
row vector of inside terms.
? Each ?a,i,j ? Rm?1 for a ? N , 1 ? i ? j ? N is a
column vector of outside terms.
? Each ?(a, i, j) ? R for a ? N , 1 ? i ? j ? N is a
marginal probability.
Algorithm:
(Inside base case) ?a ? P , i ? [N ], ?a,i,i = c?a?xi
(Inside recursion) ?a ? I, 1 ? i < j ? N,
?a,i,j =
j?1
?
k=i
?
a?b c
?c,k+1,jCa?b c(?b,i,k)
(Outside base case) ?a ? I, ?a,1,n = c1a
(Outside recursion) ?a ? N , 1 ? i ? j ? N,
?a,i,j =
i?1
?
k=1
?
b?c a
Cb?c a(?c,k,i?1)?b,k,j
+
N
?
k=j+1
?
b?a c
Cb?a c? (?c,j+1,k)?b,i,k
(Marginals) ?a ? N , 1 ? i ? j ? N,
?(a, i, j) = ?a,i,j?a,i,j =
?
h?[m]
?a,i,jh ?
a,i,j
h
Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
The outside tree contains everything in the s-tree
r1 . . . rN , excluding the subtree below node i.
Our random variables are defined as follows.
First, we select a random internal node, from a ran-
dom tree, as follows:
? Sample an s-tree r1 . . . rN from the PMF
p(r1 . . . rN ). Choose a node i uniformly at ran-
dom from [N ].
If the rule ri for the node i is of the form a? b c,
we define random variables as follows:
? R1 is equal to the rule ri (e.g., NP ? D N).
? T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
? H1,H2,H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.
226
? A1, A2, A3 are the labels for node i, the left
child of node i, and the right child of node i respec-
tively. (E.g., A1 = NP, A2 = D, A3 = N.)
? O is the outside tree at node i.
? B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is of
the form a ? x, we have random vari-
ables R1, T1,H1, A1, O,B as defined above, but
H2,H3, T2, T3, A2, and A3 are not defined.
We assume a function ? that maps outside trees o
to feature vectors ?(o) ? Rd? . For example, the fea-
ture vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function ?
that maps inside trees t to feature vectors ?(t) ? Rd.
As one example, the function ? might be an indica-
tor function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good definitions of ?(o) of ?(t). One
requirement is that d? ? m and d ? m.
In tandem with these definitions, we assume pro-
jection matices Ua ? R(d?m) and V a ? R(d??m)
for all a ? N . We then define additional random
variables Y1, Y2, Y3, Z as
Y1 = (Ua1)??(T1) Z = (V a1)??(O)
Y2 = (Ua2)??(T2) Y3 = (Ua3)??(T3)
where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
6.2 Observable Representations
Given the definitions in the previous section, our
representation is based on the following matrix, ten-
sor and vector quantities, defined for all a ? N , for
all rules of the form a? b c, and for all rules of the
form a? x respectively:
?a = E[Y1Z?|A1 = a]
Da?b c = E
[
[[R1 = a? b c]]Y3Z?Y ?2 |A1 = a
]
d?a?x = E
[
[[R1 = a? x]]Z?|A1 = a
]
Assuming access to functions ? and ?, and projec-
tion matrices Ua and V a, these quantities can be es-
timated directly from training data consisting of a
set of s-trees (see section 7).
Our observable representation then consists of:
Ca?b c(y) = Da?b c(y)(?a)?1 (2)
c?a?x = d?a?x(?a)?1 (3)
c1a = E [[[A1 = a]]Y1|B = 1] (4)
We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following definition will be important:
Definition 2 For all a ? N , we define the matrices
Ia ? R(d?m) and Ja ? R(d??m) as
[Ia]i,h = E[?i(T1) | H1 = h,A1 = a]
[Ja]i,h = E[?i(O) | H1 = h,A1 = a]
In addition, for any a ? N , we use ?a ? Rm to
denote the vector with ?ah = P (H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisfied (these are
parallel to conditions 1 and 2 in Hsu et al (2009)):
Condition 1 ?a ? N , the matrices Ia and Ja are
of full rank (i.e., they have rank m). For all a ? N ,
for all h ? [m], ?ah > 0.
Condition 2 ?a ? N , the matrices Ua ? R(d?m)
and V a ? R(d??m) are such that the matrices Ga =
(Ua)?Ia and Ka = (V a)?Ja are invertible.
The following lemma justifies the use of an SVD
calculation as one method for finding values for Ua
and V a that satisfy condition 2:
Lemma 1 Assume that condition 1 holds, and for
all a ? N define
?a = E[?(T1) (?(O))? |A1 = a] (5)
Then if Ua is a matrix of the m left singular vec-
tors of ?a corresponding to non-zero singular val-
ues, and V a is a matrix of the m right singular vec-
tors of ?a corresponding to non-zero singular val-
ues, then condition 2 is satisfied.
Proof sketch: It can be shown that ?a =
Iadiag(?a)(Ja)?. The remainder is similar to the
proof of lemma 2 in Hsu et al (2009).
The matrices ?a can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions ? and ?.
We can now state the following theorem:
227
Theorem 2 Assume conditions 1 and 2 are satisfied.
For all a ? N , define Ga = (Ua)?Ia. Then under
the definitions in Eqs. 2-4:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1.
3. For all a ? N , c1a = Ga?a
Proof: The following identities hold (see sec-
tion 9.2):
Da?b c(y) = (6)
GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka)?
d?a?x = 1?Qa?xdiag(?a)(Ka)? (7)
?a = Gadiag(?a)(Ka)? (8)
c1a = Gapia (9)
Under conditions 1 and 2, ?a is invertible, and
(?a)?1 = ((Ka)?)?1(diag(?a))?1(Ga)?1. The
identities in the theorem follow immediately.
7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives esti-
mates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ? [M ].
These tuples can be derived from a training set
consisting of s-trees ?1 . . . ?M as follows:
? ?i ? [M ], choose a single node ji uniformly at
random from the nodes in ?i. Define r(i,1) to be the
rule at node ji. t(i,1) is the inside tree rooted at node
ji. If r(i,1) is of the form a? b c, then t(i,2) is the
inside tree under the left child of node ji, and t(i,3)
is the inside tree under the right child of node ji. If
r(i,1) is of the form a ? x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji. b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
?1 . . . ?M are i.i.d. draws from the distribution
p(?) over s-trees under an L-PCFG, the tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d. draws
from the joint distribution over the random variables
R1, T1, T2, T3, O,B defined in the previous section.
The algorithm first computes estimates of the pro-
jection matrices Ua and V a: following lemma 1,
this is done by first deriving estimates of ?a,
and then taking SVDs of each ?a. The matrices
are then used to project inside and outside trees
t(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-
tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used to
derive the estimates of Ca?b c, c?a?x, and c1a.
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of definitions:
? ? is the minimum absolute value of any element
of the vectors/matrices/tensors c1a, d?a?x, Da?b c,
(?a)?1. (Note that ? is a function of the projec-
tion matrices Ua and V a as well as the underlying
L-PCFG.)
? For each a ? N , ?a is the value of the m?th
largest singular value of ?a. Define ? = mina ?a.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in figure 4 are i.i.d. draws from the joint distribution
over the random variables R1, T1, T2, T3, O,B, un-
der an L-PCFG with distribution p(r1 . . . rN ) over
s-trees. Define m to be the number of latent states
in the L-PCFG. Assume that the algorithm in fig-
ure 4 has projection matrices U?a and V? a derived as
left and right singular vectors of ?a, as defined in
Eq. 5. Assume that the L-PCFG, together with U?a
and V? a, has coefficients ? > 0 and ? > 0. In addi-
tion, assume that all elements in c1a, d?a?x, Da?b c,
and ?a are in [?1,+1]. For any s-tree r1 . . . rN de-
fine p?(r1 . . . rN ) to be the value calculated by the
algorithm in figure 3 with inputs c?1a, c??a?x, C?a?b c
derived from the algorithm in figure 4. Define R to
be the total number of rules in the grammar of the
form a? b c or a ? x. Define Ma to be the num-
ber of training examples in the input to the algorithm
in figure 4 where ri,1 has non-terminal a on its left-
hand-side. Under these assumptions, if for all a
Ma ?
128m2
( 2N+1?1 + ?? 1
)2 ?2?4
log
(2mR
?
)
Then
1? ? ?
?
?
?
?
p?(r1 . . . rN )
p(r1 . . . rN )
?
?
?
?
? 1 + ?
A similar theorem (omitted for space) states that
1? ? ?
?
?
?
??(a,i,j)
?(a,i,j)
?
?
?
? 1 + ? for the marginals.
The condition that U?a and V? a are derived from
?a, as opposed to the sample estimate ??a, follows
Foster et al (2012). As these authors note, similar
techniques to those of Hsu et al (2009) should be
228
applicable in deriving results for the case where ??a
is used in place of ?a.
Proof sketch: The proof is similar to that of Foster
et al (2012). The basic idea is to first show that
under the assumptions of the theorem, the estimates
c?1a, d??a?x, D?a?b c, ??a are all close to the underlying
values being estimated. The second step is to show
that this ensures that p?(r1...rN? )p(r1...rN? ) is close to 1.
The method described of selecting a single tuple
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-
sures that the samples are i.i.d., and simplifies the
analysis underlying theorem 3. In practice, an im-
plementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p? to p.
8 Discussion
There are several potential applications of the
method. The most obvious is parsing with L-
PCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for ex-
ample in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as spe-
cial case of our approach, by converting tagged se-
quences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of ?a; these singular
values are a measure of how well correlated ? and
? are with the unobserved hidden variable H1. Ex-
perimental work is required to find a good choice of
values for ? and ? for parsing.
9 Proofs
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; in-
stead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
9.1 Proof of Theorem 1
First, the following lemma leads directly to the cor-
rectness of the algorithm in figure 2:
1Parameters can be estimated using the algorithm in
figure 4; for a test sentence x1 . . . xN we can first
use the algorithm in figure 3 to calculate marginals
?(a, i, j), then use the algorithm of Goodman (1996) to find
argmax??T (x)
?
(a,i,j)?? ?(a, i, j).
Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i ? {1 . . .M}, where r(i,1) is a context free rule; t(i,1),
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
? that maps inside trees t to feature-vectors ?(t) ? Rd. A func-
tion ? that maps outside trees o to feature-vectors ?(o) ? Rd? .
Algorithm:
Define ai to be the non-terminal on the left-hand side of rule
r(i,1). If r(i,1) is of the form a? b c, define bi to be the non-
terminal for the left-child of r(i,1), and ci to be the non-terminal
for the right-child.
(Step 0: Singular Value Decompositions)
? Use the algorithm in figure 5 to calculate matrices U?a ?
R(d?m) and V? a ? R(d??m) for each a ? N .
(Step 1: Projection)
? For all i ? [M ], compute y(i,1) = (U?ai)??(t(i,1)).
? For all i ? [M ] such that r(i,1) is of the form
a? b c, compute y(i,2) = (U?bi)??(t(i,2)) and y(i,3) =
(U?ci)??(t(i,3)).
? For all i ? [M ], compute z(i) = (V? ai)??(o(i)).
(Step 2: Calculate Correlations)
? For each a ? N , define ?a = 1/
?M
i=1[[ai = a]]
? For each rule a? b c, compute D?a?b c = ?a ?
?M
i=1[[r(i,1) = a? b c]]y(i,3)(z(i))?(y(i,2))?
? For each rule a ? x, compute d??a?x = ?a ?
?M
i=1[[r(i,1) = a? x]](z(i))?
? For each a ? N , compute ??a = ?a ?
?M
i=1[[ai = a]]y(i,1)(z(i))?
(Step 3: Compute Final Parameters)
? For all a? b c, C?a?b c(y) = D?a?b c(y)(??a)?1
? For all a? x, c??a?x = d??a?x(??a)?1
? For all a ? I, c?1a =
?M
i=1[[ai=a and b(i)=1]]y(i,1)
?M
i=1[[b(i)=1]]
Figure 4: The spectral learning algorithm.
Inputs: Identical to algorithm in figure 4.
Algorithm:
? For each a ? N , compute ??a ? R(d??d) as
??a =
?M
i=1[[ai = a]]?(t(i,1))(?(o(i)))?
?M
i=1[[ai = a]]
and calculate a singular value decomposition of ??a.
? For each a ? N , define U?a ? Rm?d to be a matrix of the left
singular vectors of ??a corresponding to the m largest singular
values. Define V? a ? Rm?d? to be a matrix of the right singular
vectors of ??a corresponding to the m largest singular values.
Figure 5: Singular value decompositions.
229
Lemma 2 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 2 is an s-tree r1 . . . rN . Define ai for i ? [N ]
to be the non-terminal on the left-hand-side of rule
ri, and ti for i ? [N ] to be the s-tree with rule ri
at its root. Finally, for all i ? [N ], define the row
vector bi ? R(1?m) to have components
bih = P (Ti = ti|Hi = h,Ai = ai)
for h ? [m]. Then for all i ? [N ], f i = bi(G(ai))?1.
It follows immediately that
f1c1a1 = b
1(G(a1))?1Ga1?a1 = p(r1 . . . rN )
This lemma shows a direct link between the vec-
tors f i calculated in the algorithm, and the terms bih,
which are terms calculated by the conventional in-
side algorithm: each f i is a linear transformation
(through Gai) of the corresponding vector bi.
Proof: The proof is by induction.
First consider the base case. For any leaf?i.e., for
any i such that ai ? P?we have bih = q(ri|h, ai),
and it is easily verified that f i = bi(G(ai))?1.
The inductive case is as follows. For all i ? [N ]
such that ai ? I , by the definition in the algorithm,
f i = f?Cri(f?)
= f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1
Assuming by induction that f? = b?(G(a? ))?1 and
f? = b?(G(a?))?1, this simplifies to
f i = ?rdiag(?l)Qri(Gai)?1 (10)
where ?r = b?T ri , and ?l = b?Sri . ?r is a row
vector with components ?rh =
?
h??[m] b
?
h?T
ri
h?,h =
?
h??[m] b
?
h?t(h?|h, ri). Similarly, ?l is a row vector
with components equal to ?lh =
?
h??[m] b
?
h?S
ri
h?,h =
?
h??[m] b
?
h?s(h?|h, ri). It can then be verified that
?rdiag(?l)Qri is a row vector with components
equal to ?rh?lhq(ri|h, ai).
But bih = q(ri|h, ai)?
(
?
h??[m] b
?
h?t(h?|h, ri)
)
?
(
?
h??[m] b
?
h?s(h?|h, ri)
)
= q(ri|h, ai)?rh?lh, hence
?rdiag(?l)Qri = bi and the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in figure 3:
Lemma 3 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 3 is a sentence x1 . . . xN . For any a ? N , for
any 1 ? i ? j ? N , define ??a,i,j ? R(1?m) to have
components ??a,i,jh = p(xi . . . xj|h, a) for h ? [m].
In addition, define ??a,i,j ? R(m?1) to have compo-
nents ??a,i,jh = p(x1 . . . xi?1, a(h), xj+1 . . . xN ) for
h ? [m]. Then for all i ? [N ], ?a,i,j = ??a,i,j(Ga)?1
and ?a,i,j = Ga??a,i,j . It follows that for all (a, i, j),
?(a, i, j) = ??a,i,j(Ga)?1Ga??a,i,j = ??a,i,j ??a,i,j
=
?
h
??a,i,jh ??
a,i,j
h =
?
??T (x):(a,i,j)??
p(?)
Thus the vectors ?a,i,j and ?a,i,j are linearly re-
lated to the vectors ??a,i,j and ??a,i,j , which are the
inside and outside terms calculated by the conven-
tional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.
9.2 Proof of the Identity in Eq. 6
We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be verified:
P (R1 = a? b c|H1 = h,A1 = a) = q(a? b c|h, a)
E [Y3,j|H1 = h,R1 = a? b c] = Ea?b cj,h
E [Zk|H1 = h,R1 = a? b c] = Kak,h
E [Y2,l|H1 = h,R1 = a? b c] = F a?b cl,h
where Ea?b c = GcT a?b c, F a?b c = GbSa?b c.
Y3, Z and Y2 are independent when conditioned
on H1, R1 (this follows from the independence as-
sumptions in the L-PCFG), hence
E [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
= q(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h
Hence (recall that ?ah = P (H1 = h|A1 = a)),
Da?b cj,k,l = E [[[R1 = a? b c]]Y3,jZkY2,l | A1 = a]
=
?
h
?ahE [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
=
?
h
?ahq(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h (11)
from which Eq. 6 follows.
230
Acknowledgements: Columbia University gratefully ac-
knowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows Project.
Dean Foster was supported by National Science Founda-
tion grant 1106743.
References
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings of FOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Associ-
ation for Computational Linguistics, pages 177?183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 75?82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128?135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algo-
rithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artificial Intelligence, pages
261?268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
231
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56?64,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Spectral Learning of Refinement HMMs
Karl Stratos1 Alexander M. Rush2 Shay B. Cohen1 Michael Collins1
1Department of Computer Science, Columbia University, New-York, NY 10027, USA
2MIT CSAIL, Cambridge, MA, 02139, USA
{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu
Abstract
We derive a spectral algorithm for learn-
ing the parameters of a refinement HMM.
This method is simple, efficient, and can
be applied to a wide range of supervised
sequence labeling tasks. Like other spec-
tral methods, it avoids the problem of lo-
cal optima and provides a consistent esti-
mate of the parameters. Our experiments
on a phoneme recognition task show that
when equipped with informative feature
functions, it performs significantly better
than a supervised HMM and competitively
with EM.
1 Introduction
Consider the task of supervised sequence label-
ing. We are given a training set where the j?th
training example consists of a sequence of ob-
servations x(j)1 ...x(j)N paired with a sequence of
labels a(j)1 ...a(j)N and asked to predict the cor-
rect labels on a test set of observations. A
common approach is to learn a joint distribu-
tion over sequences p(a1 . . . aN , x1 . . . xN ) as a
hidden Markov model (HMM). The downside of
HMMs is that they assume each label ai is inde-
pendent of labels before the previous label ai?1.
This independence assumption can be limiting,
particularly when the label space is small. To re-
lax this assumption we can refine each label ai
with a hidden state hi, which is not observed in
the training data, and model the joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This re-
finement HMM (R-HMM), illustrated in figure 1,
is able to propagate information forward through
the hidden state as well as the label.
Unfortunately, estimating the parameters of an
R-HMM is complicated by the unobserved hid-
den variables. A standard approach is to use the
expectation-maximization (EM) algorithm which
a1, h1 a2, h2 aN , hN
x1 x2 xN
(a)
a1 a2 aN
h1 h2 hN
x1 x2 xN
(b)
Figure 1: (a) An R-HMM chain. (b) An equivalent
representation where labels and hidden states are
intertwined.
has no guarantee of finding the global optimum of
its objective function. The problem of local op-
tima prevents EM from yielding statistically con-
sistent parameter estimates: even with very large
amounts of data, EM is not guaranteed to estimate
parameters which are close to the ?correct? model
parameters.
In this paper, we derive a spectral algorithm for
learning the parameters of R-HMMs. Unlike EM,
this technique is guaranteed to find the true param-
eters of the underlying model under mild condi-
tions on the singular values of the model. The al-
gorithm we derive is simple and efficient, relying
on singular value decomposition followed by stan-
dard matrix operations.
We also describe the connection of R-HMMs
to L-PCFGs. Cohen et al (2012) present a spec-
tral algorithm for L-PCFG estimation, but the
na??ve transformation of the L-PCFG model and
its spectral algorithm to R-HMMs is awkward and
opaque. We therefore work through the non-trivial
derivation the spectral algorithm for R-HMMs.
We note that much of the prior work on spec-
tral algorithms for discrete structures in NLP has
shown limited experimental success for this fam-
ily of algorithms (see, for example, Luque et al,
2012). Our experiments demonstrate empirical
56
success for the R-HMM spectral algorithm. The
spectral algorithm performs competitively with
EM on a phoneme recognition task, and is more
stable with respect to the number of hidden states.
Cohen et al (2013) present experiments with a
parsing algorithm and also demonstrate it is com-
petitive with EM. Our set of experiments comes as
an additional piece of evidence that spectral algo-
rithms can function as a viable, efficient and more
principled alternative to the EM algorithm.
2 Related Work
Recently, there has been a surge of interest in spec-
tral methods for learning HMMs (Hsu et al, 2012;
Foster et al, 2012; Jaeger, 2000; Siddiqi et al,
2010; Song et al, 2010). Like these previous
works, our method produces consistent parameter
estimates; however, we estimate parameters for a
supervised learning task. Balle et al (2011) also
consider a supervised problem, but our model is
quite different since we estimate a joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed
to a conditional distribution and use feature func-
tions over both the labels and observations of the
training data. These feature functions also go be-
yond those previously employed in other spectral
work (Siddiqi et al, 2010; Song et al, 2010). Ex-
periments show that features of this type are cru-
cial for performance.
Spectral learning has been applied to related
models beyond HMMs including: head automata
for dependency parsing (Luque et al, 2012),
tree-structured directed Bayes nets (Parikh et al,
2011), finite-state transducers (Balle et al, 2011),
and mixture models (Anandkumar et al, 2012a;
Anandkumar et al, 2012b).
Of special interest is Cohen et al (2012), who
describe a derivation for a spectral algorithm for
L-PCFGs. This derivation is the main driving
force behind the derivation of our R-HMM spec-
tral algorithm. For work on L-PCFGs estimated
with EM, see Petrov et al (2006), Matsuzaki et al
(2005), and Pereira and Schabes (1992). Petrov
et al (2007) proposes a split-merge EM procedure
for phoneme recognition analogous to that used in
latent-variable parsing.
3 The R-HMM Model
We decribe in this section the notation used
throughout the paper and the formal details of R-
HMMs.
3.1 Notation
We distinguish row vectors from column vectors
when such distinction is necessary. We use a
superscript > to denote the transpose operation.
We write [n] to denote the set {1, 2, . . . , n} for
any integer n ? 1. For any vector v ? Rm,
diag(v) ? Rm?m is a diagonal matrix with en-
tries v1 . . . vm. For any statement S , we use [[S]]
to refer to the indicator function that returns 1 if S
is true and 0 otherwise. For a random variable X ,
we use E[X] to denote its expected value.
A tensor C ? Rm?m?m is a set of m3 val-
ues Ci,j,k for i, j, k ? [m]. Given a vector v ?
Rm, we define C(v) to be the m ? m matrix
with [C(v)]i,j = ?k?[m]Ci,j,kvk. Given vectors
x, y, z ? Rm, C = xy>z> is anm?m?m tensor
with [C]i,j,k = xiyjzk.
3.2 Definition of an R-HMM
An R-HMM is a 7-tuple ?l,m, n, pi, o, t, f? for in-
tegers l,m, n ? 1 and functions pi, o, t, f where
? [l] is a set of labels.
? [m] is a set of hidden states.
? [n] is a set of observations.
? pi(a, h) is the probability of generating a ?
[l] and h ? [m] in the first position in the
labeled sequence.
? o(x|a, h) is the probability of generating x ?
[n], given a ? [l] and h ? [m].
? t(b, h?|a, h) is the probability of generating
b ? [l] and h? ? [m], given a ? [l] and
h ? [m].
? f(?|a, h) is the probability of generating the
stop symbol ?, given a ? [l] and h ? [m].
See figure 1(b) for an illustration. At any time step
of a sequence, a label a is associated with a hidden
state h. By convention, the end of an R-HMM
sequence is signaled by the symbol ?.
For the subsequent illustration, let N be the
length of the sequence we consider. A full se-
quence consists of labels a1 . . . aN , observations
x1 . . . xN , and hidden states h1 . . . hN . The model
assumes
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = pi(a1, h1)?
N?
i=1
o(xi|ai, hi)?
N?1?
i=1
t(ai+1, hi+1|ai, hi)? f(?|aN , hN )
57
Input: a sequence of observations x1 . . . xN ; operators?
Cb|a, C?|a, c1a, cax
?
Output: ?(a, i) for all a ? [l] and i ? [N ]
[Forward case]
? ?1a ? c1a for all a ? [l].
? For i = 1 . . . N ? 1
?i+1b ?
?
a?[l]
Cb|a(caxi)? ?
i
a for all b ? [l]
[Backward case]
? ?N+1a ? C?|a(caxN ) for all a ? [l]
? For i = N . . . 1
?ia ?
?
b?[l]
?i+1b ? Cb|a(caxi) for all a ? [l]
[Marginals]
? ?(a, i)? ?ia ? ?ia for all a ? [l], i ? [N ]
Figure 2: The forward-backward algorithm
A skeletal sequence consists of labels a1 . . . aN
and observations x1 . . . xN without hidden states.
Under the model, it has probability
p(a1 . . . aN , x1 . . . xN )
=
?
h1...hN
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
An equivalent definition of an R-HMM is
given by organizing the parameters in matrix
form. Specifically, an R-HMM has parameters?
pia, oax, T b|a, fa
? where pia ? Rm is a column
vector, oax is a row vector, T b|a ? Rm?m is a ma-
trix, and fa ? Rm is a row vector, defined for all
a, b ? [l] and x ? [n]. Their entries are set to
? [pia]h = pi(a, h) for h ? [m]
? [oax]h = o(x|a, h) for h ? [m]
? [T b|a]h?,h = t(b, h?|a, h) for h, h? ? [m]
? [fa]h = f(?|a, h) for h ? [m]
4 The Forward-Backward Algorithm
Given an observation sequence x1 . . . xN , we want
to infer the associated sequence of labels under
an R-HMM. This can be done by computing the
marginals of x1 . . . xN
?(a, i) =
?
a1...aN : ai=a
p(a1 . . . aN , x1 . . . xN )
for all labels a ? [l] and positions i ? [N ]. Then
the most likely label at each position i is given by
a?i = arg maxa?[l] ?(a, i)
The marginals can be computed using a tensor
variant of the forward-backward algorithm, shown
in figure 2. The algorithm takes additional quanti-
ties ?Cb|a, C?|a, c1a, cax
? called the operators:
? Tensors Cb|a ? Rm?m?m for a, b ? [l]
? Tensors C?|a ? R1?m?m for a ? [l]
? Column vectors c1a ? Rm for a ? [l]
? Row vectors cax ? Rm for a ? [l] and x ? [n]
The following proposition states that these opera-
tors can be defined in terms of the R-HMM param-
eters to guarantee the correctness of the algorithm.
Proposition 4.1. Given an R-HMM with param-
eters ?pia, oax, T b|a, fa
?, for any vector v ? Rm
define the operators:
Cb|a(v) = T b|adiag(v) c1a = pia
C?|a(v) = fadiag(v) cax = oax
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is an algebraic verification and deferred
to the appendix. Note that the running time of the
algorithm as written is O(l2m3N).1
Proposition 4.1 can be generalized to the fol-
lowing theorem. This theorem implies that the op-
erators can be linearly transformed by some invert-
ible matrices as long as the transformation leaves
the embedded R-HMM parameters intact. This
observation is central to the derivation of the spec-
tral algorithm which estimates the linearly trans-
formed operators but not the actual R-HMM pa-
rameters.
Theorem 4.1. Given an R-HMM with parameters?
pia, oax, T b|a, fa
?, assume that for each a ? [l] we
have invertible m ?m matrices Ga and Ha. For
any vector v ? Rm define the operators:
Cb|a(v) = GbT b|adiag(vHa)(Ga)?1 c1a = Gapia
C?|a(v) = fadiag(vHa)(Ga)?1 cax = oax(Ha)?1
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is similar to that of Cohen et al (2012).
1We can reduce the complexity to O(l2m2N) by pre-
computing the matricesCb|a(cax) for all a, b ? [l] and x ? [n]
after parameter estimation.
58
5 Spectral Estimation of R-HMMs
In this section, we derive a consistent estimator for
the operators ?Cb|a, C?|a, c1a, cax
? in theorem 4.1
through the use of singular-value decomposition
(SVD) followed by the method of moments.
Section 5.1 describes the decomposition of the
R-HMM model into random variables which are
used in the final algorithm. Section 5.2 can be
skimmed through on the first reading, especially
if the reader is familiar with other spectral algo-
rithms. It includes a detailed account of the deriva-
tion of the R-HMM algorithm.
For a first reading, note that an R-HMM se-
quence can be seen as a right-branching L-PCFG
tree. Thus, in principle, one can convert a se-
quence into a tree and run the inside-outside algo-
rithm of Cohen et al (2012) to learn the parame-
ters of an R-HMM. However, projecting this trans-
formation into the spectral algorithm for L-PCFGs
is cumbersome and unintuitive. This is analo-
gous to the case of the Baum-Welch algorithm for
HMMs (Rabiner, 1989), which is a special case of
the inside-outside algorithm for PCFGs (Lari and
Young, 1990).
5.1 Random Variables
We first introduce the random variables un-
derlying the approach then describe the opera-
tors based on these random variables. From
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an R-
HMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN )
and choose a time step i uniformly at random from
[N ]. The random variables are then defined as
X = xi
A1 = ai and A2 = ai+1 (if i = N , A2 = ?)
H1 = hi and H2 = hi+1
F1 = (ai . . . aN , xi . . . xN ) (future)
F2 = (ai+1 . . . aN , xi+1 . . . xN ) (skip-future)
P = (a1 . . . ai, x1 . . . xi?1) (past)
R = (ai, xi) (present)
D = (a1 . . . aN , x1 . . . xi?1, xi+1 . . . xN ) (destiny)
B = [[i = 1]]
Figure 3 shows the relationship between the ran-
dom variables. They are defined in such a way
that the future is independent of the past and the
present is independent of the destiny conditioning
on the current node?s label and hidden state.
Next, we require a set of feature functions over
the random variables.
? ? maps F1, F2 to ?(F1), ?(F2) ? Rd1 .
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
P
F1
F2
(a)
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
D R
(b)
Figure 3: Given an R-HMM sequence, we define
random variables over observed quantities so that
conditioning on the current node, (a) the future F1
is independent of the past P and (b) the present R
is independent of the density D.
? ? maps P to ?(P ) ? Rd2 .
? ? maps R to ?(R) ? Rd3 .
? ? maps D to ?(D) ? Rd4 .
We will see that the feature functions should be
chosen to capture the influence of the hidden
states. For instance, they might track the next la-
bel, the previous observation, or important combi-
nations of labels and observations.
Finally, we require projection matrices
?a ? Rm?d1 ?a ? Rm?d2
?a ? Rm?d3 ?a ? Rm?d4
defined for all labels a ? [l]. These matrices
will project the feature vectors of ?, ?, ?, and ?
from (d1, d2, d3, d4)-dimensional spaces to an m-
dimensional space. We refer to this reduced di-
mensional representation by the following random
variables:
F 1 = ?A1?(F1) (projected future)
F 2 = ?A2?(F2) (projected skip-future: if i = N , F 2 = 1)
P = ?A1?(P ) (projected past)
R = ?A1?(R) (projected present)
D = ?A1?(D) (projected destiny)
Note that they are all vectors in Rm.
59
5.2 Estimation of the Operators
Since F 1, F 2, P , R, and D do not involve hid-
den variables, the following quantities can be di-
rectly estimated from the training data of skeletal
sequences. For this reason, they are called observ-
able blocks:
?a = E[F 1P>|A1 = a] ?a ? [l]
?a = E[R D>|A1 = a] ?a ? [l]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a] ?a, b ? [l]
dax = E[[[X = x]]D>|A1 = a] ?a ? [l], x ? [n]
The main result of this paper is that under cer-
tain conditions, matrices ?a and ?a are invert-
ible and the operators ?Cb|a, C?|a, c1a, cax
? in the-
orem 4.1 can be expressed in terms of these ob-
servable blocks.
Cb|a(v) = Db|a(v)(?a)?1 (1)
C?|a(v) = D?|a(v)(?a)?1 (2)
cax = dax(?a)?1 (3)
c1a = E[[[A1 = a]]F 1|B = 1] (4)
To derive this result, we use the following defini-
tion to help specify the conditions on the expecta-
tions of the feature functions.
Definition. For each a ? [l], define matrices
Ia ? Rd1?m, Ja ? Rd2?m, Ka ? Rd3?m,W a ?
Rd4?m by
[Ia]k,h = E[[?(F1)]k|A1 = a,H1 = h]
[Ja]k,h = E[[?(P )]k|A1 = a,H1 = h]
[Ka]k,h = E[[?(R)]k|A1 = a,H1 = h]
[W a]k,h = E[[?(D)]k|A1 = a,H1 = h]
In addition, let ?a ? Rm?m be a diagonal matrix
with [?a]h,h = P (H1 = h|A1 = a).
We now state the conditions for the correctness of
Eq. (1-4). For each label a ? [l], we require that
Condition 6.1 Ia, Ja,Ka,W a have rank m.
Condition 6.2 [?a]h,h > 0 for all h ? [m].
The conditions lead to the following proposition.
Proposition 5.1. Assume Condition 6.1 and 6.2
hold. For all a ? [l], define matrices
?a1 = E[?(F1)?(P )>|A1 = a] ? Rd1?d2
?a2 = E[?(R)?(D)>|A1 = a] ? Rd3?d4
Let ua1 . . . uam ? Rd1 and va1 . . . vam ? Rd2 be the
top m left and right singular vectors of ?a. Sim-
ilarly, let la1 . . . lam ? Rd3 and ra1 . . . ram ? Rd4 be
the top m left and right singular vectors of ?a.
Define projection matrices
?a = [ua1 . . . uam]> ?a = [va1 . . . vam]>
?a = [la1 . . . lam]> ?a = [ra1 . . . ram]>
Then the following m?m matrices
Ga = ?aIa Ga = ?aJa
Ha = ?aKa Ha = ?aW a
are invertible.
The proof resembles that of lemma 2 of Hsu et al
(2012). Finally, we state the main result that shows?
Cb|a, C?|a, c1a, cax
? in Eq. (1-4) using the projec-
tions from proposition 5.1 satisfy theorem 4.1. A
sketch of the proof is deferred to the appendix.
Theorem 5.1. Assume conditions 6.1 and 6.2
hold. Let ??a,?a,?a,?a? be the projection ma-
trices from proposition 5.1. Then the operators in
Eq. (1-4) satisfy theorem 4.1.
In summary, these results show that with the
proper selection of feature functions, we can con-
struct projection matrices ??a,?a,?a,?a? to ob-
tain operators ?Cb|a, C?|a, c1a, cax
? which satisfy
the conditions of theorem 4.1.
6 The Spectral Estimation Algorithm
In this section, we give an algorithm to estimate
the operators ?Cb|a, C?|a, c1a, cax
? from samples of
skeletal sequences. Suppose the training set con-
sists of M skeletal sequences (a(j), x(j)) for j ?
[M ]. ThenM samples of the random variables can
be derived from this training set as follows
? At each j ? [M ], choose a position
ij uniformly at random from the positions
in (a(j), x(j)). Sample the random vari-
ables (X,A1, A2, F1, F2, P,R,D,B) using
the procedure defined in section 5.1.
This process yields M samples
(x(j), a(j)1 , a
(j)
2 , f
(j)
1 , f
(j)
2 , p(j), r(j), d(j), b(j)) for j ? [M ]
Assuming (a(j), x(j)) are i.i.d. draws from
the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal se-
quences under an R-HMM, the tuples obtained
through this process are i.i.d. draws from the joint
PMF over (X,A1, A2, F1, F2, P,R,D,B).
60
Input: samples of (X,A1, A2, F1, F2, P,R,D,B); feature
functions ?, ?, ?, and ?; number of hidden states m
Output: estimates
?
C?b|a, C??|a, c?1a, c?ax
?
of the operators
used in algorithm 2
[Singular Value Decomposition]
? For each label a ? [l], compute empirical estimates of
?a1 = E[?(F1)?(P )>|A1 = a]
?a2 = E[?(R)?(D)>|A1 = a]
and obtain their singular vectors via an SVD. Use
the top m singular vectors to construct projections?
??a, ??a, ??a, ??a
?
.
[Sample Projection]
? Project (d1, d2, d3, d4)-dimensional samples of
(?(F1), ?(F2), ?(P ), ?(R), ?(D))
with matrices
?
??a, ??a, ??a, ??a
?
to obtain m-
dimensional samples of
(F 1, F 2, P ,R,D)
[Method of Moments]
? For each a, b ? [l] and x ? [n], compute empirical
estimates
?
??a, ??a, D?b|a, d?ax
?
of the observable blocks
?a = E[F 1P>|A1 = a]
?a = E[R D>|A1 = a]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a]
dax = E[[[X = x]]D>|A1 = a]
and also c?1a = E[[[A1 = a]]F 1|B = 1]. Finally, set
C?b|a(v)? D?b|a(v)(??a)?1
C??|a(v)? D??|a(v)(??a)?1
c?ax ? d?ax(??a)?1
Figure 4: The spectral estimation algorithm
The algorithm in figure 4 shows how to derive
estimates of the observable representations from
these samples. It first computes the projection
matrices ??a,?a,?a,?a? for each label a ? [l]
by computing empirical estimates of ?a1 and ?a2
in proposition 5.1, calculating their singular vec-
tors via an SVD, and setting the projections in
terms of these singular vectors. These projection
matrices are then used to project (d1, d2, d3, d4)-
0 5 10 15 20 25 30hidden states (m)54.0
54.555.0
55.556.0
56.557.0
57.5
accur
acy
SpectralEM
Figure 5: Accuracy of the spectral algorithm and
EM on TIMIT development data for varying num-
bers of hidden states m. For EM, the highest scor-
ing iteration is shown.
dimensional feature vectors
(
?(f (j)1 ), ?(f
(j)
2 ), ?(p(j)), ?(r(j)), ?(d(j))
)
down to m-dimensional vectors
(
f (j)1 , f
(j)
2 , p
(j), r(j), d(j)
)
for all j ? [M ]. It then computes correlation
between these vectors in this lower dimensional
space to estimate the observable blocks which are
used to obtain the operators as in Eq. (1-4). These
operators can be used in algorithm 2 to compute
marginals.
As in other spectral methods, this estimation al-
gorithm is consistent, i.e., the marginals ??(a, i)
computed with the estimated operators approach
the true marginal values given more data. For
details, see Cohen et al (2012) and Foster et al
(2012).
7 Experiments
We apply the spectral algorithm for learning
R-HMMs to the task of phoneme recognition.
The goal is to predict the correct sequence of
phonemes a1 . . . aN for a given a set of speech
frames x1 . . . xN . Phoneme recognition is often
modeled with a fixed-structure HMM trained with
EM, which makes it a natural application for spec-
tral training.
We train and test on the TIMIT corpus of spoken
language utterances (Garofolo and others, 1988).
The label set consists of l = 39 English phonemes
following a standard phoneme set (Lee and Hon,
1989). For training, we use the sx and si utter-
ances of the TIMIT training section made up of
61
?(F1) ai+1 ? xi, ai+1, xi, np(ai . . . aN )
?(P ) (ai?1, xi?1), ai?1, xi?1, pp(a1 . . . ai)
?(R) xi
?(D) ai?1 ? xi?1, ai?1, xi?1, pp(a1 . . . ai),
pos(a1 . . . aN )
iy r r r r r r ow . . .. . .
pp b m e np
Figure 6: The feature templates for phoneme
recognition. The simplest features look only at the
current label and observation. Other features in-
dicate the previous phoneme type used before ai
(pp), the next phoneme type used after ai (np),
and the relative position (beginning, middle, or
end) of ai within the current phoneme (pos). The
figure gives a typical segment of the phoneme se-
quence a1 . . . aN
M = 3696 utterances. The parameter estimate is
smoothed using the method of Cohen et al (2013).
Each utterance consists of a speech signal
aligned with phoneme labels. As preprocessing,
we divide the signal into a sequence of N over-
lapping frames, 25ms in length with a 10ms step
size. Each frame is converted to a feature repre-
sentation using MFCC with its first and second
derivatives for a total of 39 continuous features.
To discretize the problem, we apply vector quanti-
zation using euclidean k-means to map each frame
into n = 10000 observation classes. After pre-
processing, we have 3696 skeletal sequence with
a1 . . . aN as the frame-aligned phoneme labels and
x1 . . . xN as the observation classes.
For testing, we use the core test portion of
TIMIT, consisting of 192 utterances, and for de-
velopment we use 200 additional utterances. Ac-
curacy is measured by the percentage of frames
labeled with the correct phoneme. During infer-
ence, we calculate marginals ? for each label at
each position i and choose the one with the highest
marginal probability, a?i = arg maxa?[l] ?(a, i).
The spectral method requires defining feature
functions ?, ?, ?, and ?. We use binary-valued
feature vectors which we specify through features
templates, for instance the template ai ? xi corre-
sponds to binary values for each possible label and
output pair (ln binary dimensions).
Figure 6 gives the full set of templates. These
feature functions are specially for the phoneme
labeling task. We note that the HTK baseline
explicitly models the position within the current
Method Accuracy
EM(4) 56.80
EM(24) 56.23
SPECTRAL(24), no np, pp, pos 55.45
SPECTRAL(24), no pos 56.56
SPECTRAL(24) 56.94
Figure 7: Feature ablation experiments on TIMIT
development data for the best spectral model (m =
24) with comparisons to the best EM model (m =
4) and EM with m = 24.
Method Accuracy
UNIGRAM 48.04
HMM 54.08
EM(4) 55.49
SPECTRAL(24) 55.82
HTK 55.70
Figure 8: Performance of baselines and spectral
R-HMM on TIMIT test data. Number of hidden
states m optimized on development data (see fig-
ure 5). The improvement of the spectral method
over the EM baseline is significant at the p ? 0.05
level (and very close to significant at p ? 0.01,
with a precise value of p ? 0.0104).
phoneme as part of the HMM structure. The spec-
tral method is able to encode similar information
naturally through the feature functions.
We implement several baseline for phoneme
recognition: UNIGRAM chooses the most likely
label, arg maxa?[l] p(a|xi), at each position;
HMM is a standard HMM trained with maximum-
likelihood estimation; EM(m) is an R-HMM
with m hidden states estimated using EM; and
SPECTRAL(m) is an R-HMM with m hidden
states estimated with the spectral method de-
scribed in this paper. We also compare to HTK,
a fixed-structure HMM with three segments per
phoneme estimated using EM with the HTK
speech toolkit. See Young et al (2006) for more
details on this method.
An important consideration for both EM and the
spectral method is the number of hidden states m
in the R-HMM. More states allow for greater label
refinement, with the downside of possible overfit-
ting and, in the case of EM, more local optima.
To determine the best number of hidden states, we
optimize both methods on the development set for
a range of m values between 1 to 32. For EM,
62
we run 200 training iterations on each value of m
and choose the iteration that scores best on the de-
velopment set. As the spectral algorithm is non-
iterative, we only need to evaluate the develop-
ment set once per m value. Figure 5 shows the
development accuracy of the two method as we
adjust the value of m. EM accuracy peaks at 4
hidden states and then starts degrading, whereas
the spectral method continues to improve until 24
hidden states.
Another important consideration for the spectral
method is the feature functions. The analysis sug-
gests that the best feature functions are highly in-
formative of the underlying hidden states. To test
this empirically we run spectral estimation with a
reduced set of features by ablating the templates
indicating adjacent phonemes and relative posi-
tion. Figure 7 shows that removing these features
does have a significant effect on development ac-
curacy. Without either type of feature, develop-
ment accuracy drops by 1.5%.
We can interpret the effect of the features in
a more principled manner. Informative features
yield greater singular values for the matrices ?a1
and ?a2, and these singular values directly affect
the sample complexity of the algorithm; see Cohen
et al (2012) for details. In sum, good feature func-
tions lead to well-conditioned ?a1 and ?a2, which in
turn require fewer samples for convergence.
Figure 8 gives the final performance for the
baselines and the spectral method on the TIMIT
test set. For EM and the spectral method, we
use best performing model from the develop-
ment data, 4 hidden states for EM and 24 for
the spectral method. The experiments show that
R-HMM models score significantly better than a
standard HMM and comparatively to the fixed-
structure HMM. In training the R-HMM models,
the spectral method performs competitively with
EM while avoiding the problems of local optima.
8 Conclusion
This paper derives a spectral algorithm for the
task of supervised sequence labeling using an R-
HMM. Unlike EM, the spectral method is guar-
anteed to provide a consistent estimate of the pa-
rameters of the model. In addition, the algorithm
is simple to implement, requiring only an SVD
of the observed counts and other standard ma-
trix operations. We show empirically that when
equipped with informative feature functions, the
spectral method performs competitively with EM
on the task of phoneme recognition.
Appendix
Proof of proposition 4.1. At any time step i ? [N ] in the al-
gorithm in figure 2, for all label a ? [l] we have a column
vector ?ia ? Rm and a row vector ?ia ? Rm. The value of
these vectors at each index h ? [m] can be verified as
[?ia]h =
?
a1...ai,h1...hi:
ai=a,hi=h
p(a1 . . . ai, x1 . . . xi?1, h1 . . . hi)
[?ia]h =?
ai...aN ,hi...hN :
ai=a,hi=h
p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai, hi)
Thus ?ia?ia is a scalar equal to
?
a1...aN ,h1...hN :ai=a
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
which is the value of the marginal ?(a, i).
Proof of theorem 5.1. It can be verified that c1a = Gapia. For
the others, under the conditional independence illustrated in
figure 3 we can decompose the observable blocks in terms of
the R-HMM parameters and invertible matrices
?a = Ga?a(Ga)> ?a = Ha?a(Ha)>
Db|a(v) = GbT b|adiag(vHa)?a(Ga)>
D?|a(v) = fadiag(vHa)?a(Ga)> dax = oax?a(Ha)>
using techniques similar to those sketched in Cohen et al
(2012). By proposition 5.1, ?a and ?a are invertible, and
these observable blocks yield the operators that satisfy theo-
rem 4.1 when placed in Eq. (1-3).
References
A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K.
Liu. 2012a. Two svds suffice: Spectral decompositions
for probabilistic topic modeling and latent dirichlet alo-
cation. Arxiv preprint arXiv:1204.6703.
A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A
method of moments for mixture models and hidden
markov models. Arxiv preprint arXiv:1203.0683.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral
learning algorithm for finite state transducers. Machine
Learning and Knowledge Discovery in Databases, pages
156?171.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2012. Spectral learning of latent-variable PCFGs. In
Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics. Association for Computa-
tional Linguistics.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2013. Experiments with spectral learning of latent-
variable pcfgs. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies.
63
D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spec-
tral dimensionality reduction for hmms. Arxiv preprint
arXiv:1203.6130.
J. S. Garofolo et al 1988. Getting started with the darpa
timit cd-rom: An acoustic phonetic continuous speech
database. National Institute of Standards and Technology
(NIST), Gaithersburgh, MD, 107.
D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral al-
gorithm for learning hidden markov models. Journal of
Computer and System Sciences.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371?
1398.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer speech & language, 4(1):35?56.
K.F. Lee and H.W. Hon. 1989. Speaker-independent phone
recognition using hidden markov models. Acoustics,
Speech and Signal Processing, IEEE Transactions on,
37(11):1641?1648.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012.
Spectral learning for non-deterministic dependency pars-
ing. In EACL, pages 409?419.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg
with latent annotations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Linguis-
tics, pages 75?82. Association for Computational Linguis-
tics.
A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algo-
rithm for latent tree graphical models. In Proceedings of
the 28th International Conference on Machine Learning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In Proceedings
of the 30th annual meeting on Association for Computa-
tional Linguistics, pages 128?135. Association for Com-
putational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages
433?440. Association for Computational Linguistics.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learn-
ing structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Proceed-
ings of the IEEE, 77(2):257?286.
S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reduced-
rank hidden Markov models. In Proceedings of the Thir-
teenth International Conference on Artificial Intelligence
and Statistics (AISTATS-2010).
L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola.
2010. Hilbert space embeddings of hidden markov mod-
els. In Proceedings of the 27th International Conference
on Machine Learning. Citeseer.
S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,
XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al
2006. The htk book (for htk version 3.4).
64
