Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17?21,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Temporal Text Ranking and Automatic Dating of Texts
Vlad Niculae
1
, Marcos Zampieri
2
, Liviu P. Dinu
3
, Alina Maria Ciobanu
3
Max Planck Institute for Software Systems, Germany
1
Saarland University, Germany
2
Center for Computational Linguistics, University of Bucharest, Romania
3
vniculae@mpi-sws.org, marcos.zampieri@uni-saarland.de,
ldinu@fmi.unibuc.ro, alina.ciobanu@my.fmi.unibuc.ro
Abstract
This paper presents a novel approach to
the task of temporal text classification
combining text ranking and probability for
the automatic dating of historical texts.
The method was applied to three histor-
ical corpora: an English, a Portuguese
and a Romanian corpus. It obtained per-
formance ranging from 83% to 93% ac-
curacy, using a fully automated approach
with very basic features.
1 Introduction
Temporal text classification is an underexplored
problem in NLP, which has been tackled as a
multi-class problem, with classes defined as time
intervals such as months, years, decades or cen-
turies. This approach has the drawback of having
to arbitrarily delimit the intervals, and often leads
to a model that is not informative for texts written
within such a window. If the predefined window is
too large, the output is not useful for most systems;
if the window is too small, learning is impractical
because of the large number of classes. Particu-
larly for the problem of historical datasets (as the
one we propose here), learning a year-level classi-
fier would not work, because each class would be
represented by a single document.
Our paper explores a solution to this drawback
by using a ranking approach. Ranking amounts to
ordering a set of inputs with respect to some mea-
sure. For example, a search engine ranks returned
documents by relevance. We use a formalization
of ranking that comes from ordinal regression, the
class of problems where samples belong to inher-
ently ordered classes.
This study is of interest to scholars who deal
with text classification and NLP in general; his-
torical linguists and philologists who investigate
language change; and finally scholars in the dig-
ital humanities who often deal with historical
manuscripts and might take advantage of temporal
text classification applications in their research.
2 Related Work
Modelling temporal information in text is a rele-
vant task for a number of NLP tasks. For example,
in Information Retrieval (IR) research has been
concentrated on investigating time-sensitivity doc-
ument ranking (Dakka and Gravana, 2010). Even
so, as stated before, temporal text classification
methods were not substantially explored as other
text classification tasks.
One of the first studies to model temporal infor-
mation for the automatic dating of documents is
the work of de Jong et al. (2005). In these exper-
iments, authors used unigram language models to
classify Dutch texts spanning from January 1999
to February 2005 using normalised log-likelihood
ratio (NLLR) (Kraaij, 2004). As to the features
used, a number of approaches proposed to auto-
matic date take into account lexical features (Dalli
and Wilks, 2006; Abe and Tsumoto, 2010; Ku-
mar et al., 2011) and a few use external linguistic
knowledge (Kanhabua and N?rv?ag, 2009).
A couple of approaches try to classify texts not
only regarding the time span in which the texts
were written, but also their geographical location
such as (Mokhov, 2010) for French and, more re-
cently, (Trieschnigg et al., 2012) for Dutch. At the
word level, two studies aim to model and under-
stand how word usage and meaning change over
time (Wijaya and Yeniterzi, 2011), (Mihalcea and
Nastase, 2012).
The most recent studies in temporal text classifi-
cation to our knowledge are (Ciobanu et al., 2013)
for Romanian using lexical features and (
?
Stajner
and Zampieri, 2013) for Portuguese using stylistic
and readability features.
17
3 Methods
3.1 Corpora
To evaluate the method proposed here we used
three historical corpora. An English historical
corpus entitled Corpus of Late Modern English
Texts (CLMET)
1
(de Smet, 2005), a Portuguese
historical corpus entitled Colonia
2
(Zampieri and
Becker, 2013) and a Romanian historical corpus
(Ciobanu et al., 2013).
CLMET is a collection of English texts derived
from the Project Gutenberg and from the Oxford
Text Archive. It contains around 10 million to-
kens, divided over three sub-periods of 70 years.
The corpus is available for download as raw text
or annotated with POS annotation.
For Portuguese, the aforementioned Colonia
(Zampieri and Becker, 2013) is a diachronic col-
lection containing a total of 5.1 million tokens and
100 texts ranging from the 16
th
to the early 20
th
century. The texts in Colonia are balanced be-
tween European and Brazilian Portuguese (it con-
tains 52 Brazilian texts and 48 European texts) and
the corpus is annotated with lemma and POS in-
formation. According to the authors, some texts
presented edited orthography prior to their com-
pilation but systematic spelling normalisation was
not carried out.
The Romanian corpus was compiled to portrait
different stages in the evolution of the Romanian
language, from the 16
th
to the 20
th
century in a
total of 26 complete texts. The methodology be-
hind corpus compilation and the date assignment
are described in (Ciobanu et al., 2013).
3.2 Temporal classification as ranking
We propose a temporal model that learns a linear
function g(x) = w ? x to preserve the temporal or-
dering of the texts, i.e. if document
3
x
i
predates
document x
j
, which we will henceforth denote as
x
i
? x
j
, then g(x
i
) < g(x
j
). Such a problem is
often called ranking or learning to rank. When the
goal is to recover contiguous intervals that corre-
spond to ordered classes, the problem is known as
ordinal regression.
We use a pairwise approach to ranking that re-
duces the problem to binary classification using a
1
https://perswww.kuleuven.be/
?
u0044428/clmet
2
http://corporavm.uni-koeln.de/
colonia/
3
For brevity, we use x
i
to denote both the document itself
and its representation as a feature vector.
linear model. The method is to convert a dataset
of the form D = {(x, y) : x ? R
d
, y ? Y} into a
pairwise dataset:
D
p
= {((x
i
, x
j
), I[y
i
< y
j
]) :
(x
i
, y
i
), (x
j
, y
j
) ? D}
Since the ordinal classes only induce a partial or-
dering, as elements from the same class are not
comparable, D
p
will only consist of the compara-
ble pairs.
The problem can be turned into a linear classifi-
cation problem by noting that:
w ? x
i
< w ? x
j
?? w ? (x
i
? x
j
) < 0
In order to obtain probability values for the or-
dering, we use logistic regression as the linear
model. It therefore holds that:
P(x
i
? x
j
;w) =
1
1 + exp(?w ? (x
i
? x
j
))
While logistic regression usually fits an inter-
cept term, in our case, because the samples consist
of differences of points, the model operates in an
affine space and therefore gains an extra effective
degree of freedom. The intercept is therefore not
needed.
The relationship between pairwise ranking and
predicting the class from an ordered set {r
1
, ...r
k
}
is given by assigning to a document x the class r
i
such that
?(r
i?1
) ? g(x) < ?(r
i
) (1)
where ? is an increasing function that does not
need to be linear. (Pedregosa et al., 2012), who
used the pairwise approach to ordinal regression
on neuroimaging prediction tasks, showed using
artificial data that ? can be accurately recovered
using non-parametric regression. In this work, we
use a parametric estimation of ? that can be used
in a probabilistic interpretation to identify the most
likely period when a text was written, as described
in section 3.3.
3.3 Probabilistic dating of uncertain texts
The ranking model described in the previous sec-
tion learns a direction along which the temporal
order of texts is preserved as much as possible.
This direction is connected to the chronological
axis through the ? function. For the years t for
18
which we have an unique attested document x
t
,
we have that
x ? x
t
?? g(x) < g(x
t
) < ?(t)
This can be explained by seeing that equation 2
gives ?(t) as an upper bound for the projections of
all texts written in year t, and by transitivity for all
previous texts as well.
Assuming we can estimate the function ? with
another function
?
?, the cumulative densitiy func-
tion of the distribution of the time when an unseen
document was written can be expressed.
P (x ? t) ?
1
1 + exp(w ? x?
?
?(t))
(2)
Setting the probability to
1
2
provides a point es-
timate of the time when x was written, and confi-
dence intervals can be found by setting it to p and
1? p.
3.4 Features
Our ranking and estimation model can work with
any kind of numerical features. For simplicity
we used lexical and naive morphological features,
pruned using ?
2
feature selection with tunable
granularity.
The lexical features are occurrence counts of all
words that appear in at least p
lex
documents. The
morphological features are counts of character n-
grams of length up to w
mph
in final positions of
words, filtered to occur in at least n
mph
documents.
Subsequently, a non-linear transformation ? is
optionally applied to the numerical features. This
is one of ?
sqrt
(z) =
?
z, ?
log
(z) = log(z) or
?
id
(z) = z (no transformation).
The feature selection step is applied before gen-
erating the pairs for classification, in order for the
?
2
scoring to be applicable. The raw target val-
ues used are year labels, but to avoid separating
almost every document in its own class, we in-
troduce a granularity level that transforms the la-
bels into groups of n
gran
years. For example, if
n
gran
= 10 then the features will be scored ac-
cording to how well they predict the decade a doc-
ument was written in. The features in the top p
fsel
percentile are kept. Finally, C is the regulariza-
tion parameter of the logistic regression classifier,
as defined in liblinear (Fan et al., 2008).
0.2 0.4 0.6 0.8 1.00.72
0.74
0.76
0.78
0.80
0.82
0.84
RidgeRanking
0.6 0.7 0.8 0.9 1.00.78
0.79
0.80
0.81
0.82
0.83
RidgeRanking
Figure 1: Learning curves for English (top) and
Portuguese (bottom). Proportion of training set
used versus score.
4 Results
Each corpus is split randomly into training and test
sets with equal number of documents. The best
feature set is chosen by 3-fold cross-validated ran-
dom search over a large grid of possible configu-
rations. We use random search to allow for a more
efficient exploration of the parameter space, given
that some parameters have much less impact to the
final score than others.
The evaluation metric we used is the percentage
of non-inverted (correctly ordered) pairs, follow-
ing (Pedregosa et al., 2012).
We compare the pairwise logistic approach to
a ridge regression on the same feature set, and
two multiclass SVMs, at century and decade level.
While the results are comparable with a slight ad-
vantage in favour of ranking, the pairwise ranking
system has several advantages. On the one hand, it
provides the probabilistic interpretation described
in section 3.3. On the other hand, the model can
naturally handle noisy, uncertain or wide-range la-
bels, because annotating whether a text was writ-
ten before another can be done even when the texts
do not correspond to punctual moments in time.
While we do not exploit this advantage, it can lead
to more robust models of temporal evolution. The
learning curves in Figure 1 further show that the
pairwise approach can better exploit more data and
nonlinearity.
The implementation is based on the scikit-learn
machine learning library for Python (Pedregosa et
al., 2011) with logistic regression solver from (Fan
et al., 2008). The source code will be available.
4.1 Uncertain texts
We present an example of using the method from
Section 3.3 to estimate the date of uncertain, held-
out texts of historical interest. Figure 2 shows the
process used for estimating ? as a linear, and in
the case of Portuguese, quadratic function. The
19
size p
lex
n
mph
w
mph
? n
gran
p
fsel
C score ridge century decade MAE
en 293 0.9 0 3 ?
log
100 0.15 2
9
0.838 0.837 0.751 0.813 22.8
pt 87 0.9 25 4 ?
sqrt
5 0.25 2
?5
0.829 0.819 0.712 0.620 58.7
ro 42 0.8 0 4 ?
log
5 0.10 2
28
0.929 0.924 0.855 0.792 28.8
Table 1: Test results of the system on the three datasets. The score is the proportion of pairs of docu-
ments ranked correctly. The column ridge is a linear regression model used for ranking, while century
and decade are linear SVMs used to predict the century and the decade of each text, but scored as pair-
wise ranking, for comparability. Chance level is 0.5. MAE is the mean absolute error in years. The
hyperparameters are described in section 3.4.
1650 1700 1750 1800 1850 1900 1950Year
300
200
100
0
100
200
w?x
Linear (33.54)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
40
20
0
20
40
60
w?x
Linear (17.27)Quadratic (15.44)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
100
50
0
50
100
w?x
Linear (1.87)TrainTest
Figure 2: Estimating the function ? that defines the relationship between years and projections of docu-
ments to the direction of the model, for English, Portuguese and Romanian (left to right). In parantheses,
the normalized residual of the least squares fit is reported on the test set.
154
0
156
0
158
0
160
0
162
0
164
0
166
0
168
0
170
0
172
0
174
0
176
0
178
0
180
0
182
0
184
0
186
0
188
0
190
0
192
0
194
0
196
0
198
0
200
0
202
00.2
0.00.2
0.40.6
0.81.0
1.2
Figure 3: Visualisation of the probability esti-
mation for the dating of C. Cantacuzino?s Isto-
ria T
,
?arii Rum?anes
,
ti. The horizontal axis is the
time, the points are known texts with a height
equal to the probability predicted by the classifier.
The dashed line is the estimated probability from
Equation 2.
estimation is refit on all certain documents prior to
plugging into the probability estimation.
The document we use to demonstrate the pro-
cess is Romanian nobleman and historian Con-
stantin Cantacuzino?s Istoria T
,
?arii Rum?anes
,
ti.
The work is believed to be written in 1716, the
year of the author?s death, and published in sev-
eral editions over a century later (Stahl, 2001).
This is an example of the system being reasonably
close to the hypothesis, thus providing linguistic
support to it. Our system gives an estimated dat-
ing of 1744.7 with a 90% confidence interval of
1736.2 ? 1753.2. As publications were signifi-
cantly later, the lexical pull towards the end of 18
th
century that can be observed in Figure 3 could be
driven by possible editing of the original text.
5 Conclusion
We propose a ranking approach to temporal mod-
elling of historical texts. We show how the model
can be used to produce reasonable probabilistic
estimates of the linguistic age of a text, using a
very basic, fully-automatic feature extraction step
and no linguistic or historical knowledge injected,
apart from the labels, which are possibly noisy.
Label noise can be atenuated by replacing un-
certain dates with intervals that are more certain,
and only generating training pairs out of non-
overlapping intervals. This can lead to a more
robust model and can use more data than would
be possible with a regression or classification ap-
proach. The problem of potential edits that a text
has suffered still remains open.
Finally, better engineered and linguistically-
motivated features, such as syntactic, morphologi-
cal or phonetic patterns that are known or believed
to mark epochs in the evolution of a language, can
be plugged in with no change to the fundamental
method.
20
References
H. Abe and S. Tsumoto. 2010. Text categorization
with considering temporal patterns of term usages.
In Proceedings of ICDM Workshops, pages 800?
807. IEEE.
A. Ciobanu, A. Dinu, L. Dinu, V. Niculae, and
O. Sulea. 2013. Temporal text classification for
romanian novels set in the past. In Proceedings of
RANLP2013, Hissar, Bulgaria.
W. Dakka and C. Gravana. 2010. Answering gen-
eral time-sensitive queries. IEEE Transactions on
Knowledge and Data Engineering.
A. Dalli and Y. Wilks. 2006. Automatic dating of doc-
uments and temporal text classification. In Proceed-
ings of the Workshop on Annotating and Reasoning
about Time and Events, pages 17?22, Sidney, Aus-
tralia.
F. de Jong, H. Rode, and D. Hiemstra. 2005. Temporal
language models for the disclosure of historical text.
In Proceedings of AHC 2005 (History and Comput-
ing).
H. de Smet. 2005. A corpus of late modern english.
ICAME-Journal.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
N. Kanhabua and P. N?rv?ag. 2009. Using tem-
poral language models for document dating. In
ECML/PKDD, pages 738?741.
W. Kraaij. 2004. Variations on language modeling
for information retrieval. Ph.D. thesis, University
of Twente.
A. Kumar, M. Lease, and J. Baldridge. 2011. Super-
vised language modelling for temporal resolution of
texts. In Proceedings of CIKM11 of the 20th ACM
international conference on Information and knowl-
edge management, pages 2069?2072.
R. Mihalcea and V. Nastase. 2012. Word epoch dis-
ambiguation: Finding how words change over time.
In Proceedings of ACL, pages 259?263. Association
for Computational Linguistics.
S. Mokhov. 2010. A marf approach to deft2010. In
Proceedings of TALN2010, Montreal, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Fabian Pedregosa, Alexandre Gramfort, Ga?el Varo-
quaux, Elodie Cauvet, Christophe Pallier, and
Bertrand Thirion. 2012. Learning to rank from
medical imaging data. CoRR, abs/1207.3598.
H.H. Stahl. 2001. G?anditori s?i curente de istorie
social?a rom?aneasc?a. Biblioteca Institutului Social
Rom?an. Ed. Univ. din Bucures?ti.
S.
?
Stajner and M. Zampieri. 2013. Stylistic changes
for temporal text classification. In Proceedings of
the 16th International Conference on Text Speech
and Dialogue (TSD2013), Lecture Notes in Artificial
Intelligence (LNAI), pages 519?526, Pilsen, Czech
Republic. Springer.
D. Trieschnigg, D. Hiemstra, M. Theune, F. de Jong,
and T. Meder. 2012. An exploration of lan-
guage identification techniques for the dutch folktale
database. In Proceedings of LREC2012.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In Proc. of
the Workshop on Detecting and Exploiting Cultural
Diversity on the Social Web (DETECT).
M. Zampieri and M. Becker. 2013. Colonia: Corpus of
historical portuguese. ZSM Studien, Special Volume
on Non-Standard Data Sources in Corpus-Based Re-
search, 5.
21
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216?223,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving Native Language Identification with TF-IDF Weighting
Binyam Gebrekidan Gebre1, Marcos Zampieri2, Peter Wittenburg1, Tom Heskes3
1Max Planck Institute for Psycholinguistics
2University of Cologne
3Radboud University
bingeb@mpi.nl,mzampier@uni-koeln.de,
peter.wittenburg@mpi.nl,t.heskes@science.ru.nl
Abstract
This paper presents a Native Language Iden-
tification (NLI) system based on TF-IDF
weighting schemes and using linear classi-
fiers - support vector machines, logistic re-
gressions and perceptrons. The system was
one of the participants of the 2013 NLI Shared
Task in the closed-training track, achieving
0.814 overall accuracy for a set of 11 native
languages. This accuracy was only 2.2 per-
centage points lower than the winner?s perfor-
mance. Furthermore, with subsequent evalua-
tions using 10-fold cross-validation (as given
by the organizers) on the combined training
and development data, the best average accu-
racy obtained is 0.8455 and the features that
contributed to this accuracy are the TF-IDF of
the combined unigrams and bigrams of words.
1 Introduction
Native Language Identification (NLI) is the task of
automatically identifying the native language of a
writer based on the writer?s foreign language pro-
duction. The task is modeled as a classification task
in which automatic methods have to assign class la-
bels (native languages) to objects (texts). NLI is by
no means trivial and it is based on the assumption
that the mother tongue influences Second Language
Acquisition (SLA) and production (Lado, 1957).
When an English native speaker hears someone
speaking English, it is not difficult for him/her to
identify if this person is a native speaker or not.
Moreover, it is, to some extent, possible to assert
the mother tongue of non-native speakers by his/hers
pronunciation patterns, regardless of their language
proficiency. In NLI, the same principle that seems
intuitive for spoken language, is applied to text. If
it is true that the mother tongue of an individual in-
fluences speech production, it should be possible to
identify these traits in written language as well.
NLI methods are particularly relevant for lan-
guages with a significant number of foreign speak-
ers, most notably, English. It is estimated that
the number of non-native speakers of English out-
numbers the number of native speakers by two to
one (Lewis et al, 2013). The written production
of non-native speakers is abundant on the Internet,
academia, and other contexts where English is used
as lingua franca.
This study presents the system that participated in
the 2013 NLI Shared Task (Tetreault et al, 2013)
under the name Cologne-Nijmegen. The novel as-
pect of the system is the use of TF-IDF weighting
schemes. For this study, we experimented with a
number of algorithms and features. Linear SVM and
logistic regression achieved the best accuracies on
the combined features of unigrams and bigrams of
words. The rest of the paper will explain in detail
the features, methods and results achieved.
2 Motivation
There are two main reasons to study NLI. On one
hand, there is a strong linguistic motivation, particu-
larly in the field of SLA and on the other hand, there
is the practical relevance of the task and its integra-
tion to a number of computational applications.
The linguistic motivation of NLI is the possibil-
ity of using classification methods to study the inter-
216
play between native and foreign language acquisi-
tion and performance (Wong and Dras, 2009). One
of the SLA theories that investigate these phenom-
ena is contrastive analysis, which is used to explain
why some structures of L2 are more difficult to ac-
quire than others (Lado, 1957).
Contrastive analysis postulates that the difficulty
in mastering L2 depends on the differences between
L1 and L2. In the process of acquiring L2, lan-
guage transfer (also known as L1 interference) oc-
curs and speakers apply knowledge from their na-
tive language to a second language, taking advan-
tage of their similarities. Computational methods
applied to L2 written production can function as a
corpus-driven method to level out these differences
and serve as a source of information for SLA re-
searchers. It can also be used to provide more tar-
geted feedback to language learners about their er-
rors.
NLI is also a relevant task in computational lin-
guistics and researchers have turned their attention
to it in the last few years. The task is often regarded
as a part of a broader task of authorship profiling,
which consists of the application of automatic meth-
ods to assert information about the writer of a given
text, such as age, gender as well native language.
Authorship profiling is particularly useful for foren-
sic linguistics.
Automatic methods of NLI may be integrated in
NLP applications such as spam detection or machine
translation. NLP tasks such as POS tagging and
parsing might also benefit from NLI, as these re-
sources are trained on standard language written by
native speakers. These tools can be more accurate to
tag non-native speaker?s text if trained with L2 cor-
pora.
3 Related Work
In the last years, a couple of attempts at identifying
native language have been described in the literature.
Tomokiyo and Jones (2001) uses a Naive Bayes al-
gorithm to classify transcribed data from three native
languages: Chinese, Japanese and English. The al-
gorithm reached 96% accuracy when distinguishing
native from non-native texts and 100% when distin-
guishing English native speakers from Chinese na-
tive speakers.
Koppel et al (2005) used machine learning to
identify the native languages of non-native English
speakers with five different mother tongues (Bul-
garian, Czech, French, Russian, and Spanish), us-
ing data retrieved from the International Corpus of
Learner English (ICLE) (Granger et al, 2009). The
features used in this study were function words,
character n-grams, and part-of-speech (POS) bi-
grams.
Tsur and Rappoport (2007) investigated the influ-
ence of the phonology of a writer?s mother tongue
through native language syllables modelled by char-
acter bigrams. Estival et al (2007) addressed NLI as
part of authorship profiling. Authors aim to attribute
10 different characteristics of writers by analysing
a set of English e-mails. The study reports around
84% accuracy in distinguishing e-mails written by
English Arabic and Spanish L1 speakers.
SVM, the algorithm that achieved the best results
in our experiments, was also previously used in NLI
(Kochmar, 2011). In this study, the author identi-
fied error types that are typical for speakers of differ-
ent native languages. She compiled a set of features
based on these error types to improve the classifica-
tion?s performance.
Recently, the TOEFL11 corpus was compiled to
serve as an alternative to the ICLE corpus (Tetreault
et al, 2012). Authors argue that TOEFL11 is more
suitable to NLI than ICLE. This study also experi-
mented with different features to increase results in
NLI and reports best accuracy results of 90.1% on
ICLE and 80.9% on TOEFL11.
4 Methods
We approach the task of native language identifica-
tion as a kind of text classification. In text classifica-
tion, decisions and choices have to be made at three
levels. First, how do we use the training and devel-
opment data? Second, what features do we extract
and how do we select the most informative ones?
Third, which machine learning algorithms perform
best and which parameters can we tune under the
constraints of memory and time? In the following
subsections, we answer these questions.
217
4.1 Dataset: TOEFL11
The dataset used for the shared task is called
TOEFL11 (Blanchard et al, 2013). It consists of
12,100 English essays (about 300 to 400 words long)
from the Test of English as a Foreign Language
(TOEFL). The essays are written by 11 native lan-
guage speakers (L1). Table 1 shows the 11 na-
tive languages. Each essay is labelled with an En-
glish language proficiency level (high, medium, or
low) based on the judgments of human assessment
specialists. We used 9,900 essays for training data
and 1,100 for development (parameter tuning). The
shared task organizers kept 1,100 essays for testing.
Table 1: TOEFL11
L1 languages Arabic, Chinese,
French, German,
Hindi, Italian,
Japanese, Korean,
Spanish, Telugu,
Turkish
# of essays per L1
900 for training
100 for validating
100 for testing
4.2 Features
We explored different kinds and combinations of
features that we assumed to be different for different
L1 speakers and that are also commonly used in the
NLI literature (Koppel et al, 2005; Tetreault et al,
2012). Table 2 shows the sources of the features we
considered. Unigrams and bigrams of words are ex-
plored separately and in combination. One through
four grams of part of speech tags have also been ex-
plored. For POS tagging of the essays, we applied
the default POS tagger from NLTK (Bird, 2006).
Spelling errors have also been treated as features.
We used the collection of words in Peter Norvig?s
website1 as a reference dictionary. The collection
consists of about a million words. It is a concate-
nation of several public domain books from Project
Gutenberg and lists of most frequent words from
Wiktionary and the British National Corpus.
Character n-grams have also been explored for
both the words in the essays and for words with
1http://norvig.com/spell-correct.html
spelling errors. The maximum n-gram size consid-
ered is six.
All features, consisting of either characters or
words or part-of-speech tags or their combinations,
are mapped into normalized numbers (norm L2).
For the mapping, we use TF-IDF, a weighting tech-
nique popular in information retrieval but which is
also finding its use in text classification. Features
that occurred in less than 5 of the essays or those
that occurred in more than 50% of the essays are
removed (all characters are in lower case). These
cut-off values are experimentally selected.
Table 2: A summary of features used in our experiments
Word n-grams Unigrams and bigrams of
words present in the es-
says.
POS n-grams One up to four grams of
POS tags present in the
essays; tagging is done
using default NLTK tag-
ger (Bird, 2006).
Character n-grams One up to six grams of
characters in each essay.
Spelling errors All words that are not
found in the dictionary
of Peter Norvig?s spelling
corrector.
4.2.1 Term Frequency (TF)
Term Frequency refers to the number of times a
particular term appears in an essay. In our experi-
ments, terms are n-grams of characters, words, part-
of-speech tags or any combination of them. The
intuition is that a term that occurs more frequently
identifies/specifies the essay better than another term
that occurs less frequently. This seems a useful
heuristic but what is the relationship between the fre-
quency of a term and its importance to the essay?
From among many relationships, we selected a log-
arithmic relationship (sublinear TF scaling) (Man-
ning et al, 2008):
wft,e =
{
1 + log(tft,e) if tft,e > 0
0 otherwise
(1)
218
where wft,e refers to weight and tft,e refers to the
frequency of term t in essay e.
The wft,e weight tells us the importance of a term
in an essay based on its frequency. But not all terms
that occur more frequently in an essay are equally
important. The effective importance of a term also
depends on how infrequent the term is in other es-
says and this intuition is handled by Inverse Docu-
ment Frequency(IDF).
4.2.2 Inverse Document Frequency(IDF)
Inverse Document Frequency (IDF) quantifies the
intuition that a term which occurs in many essays
is not a good discriminator, and should be given
less weight than one which occurs in fewer essays.
In mathematical terms, IDF is the log of the in-
verse probability of a term being found in any essay
(Salton and McGill, 1984):
idf(ti) = log
N
ni
, (2)
where N is the number of essays in the corpus,
and term ti occurs in ni of them. IDF gives a new
weight when combined with TF to form TF-IDF.
4.2.3 TF?IDF
TF?IDF combines the weights of TF and IDF
by multiplying them. TF gives more weight to a
frequent term in an essay and IDF downscales the
weight if the term occurs in many essays. Equation
3 shows the final weight that each term of an essay
gets before normalization.
wi,e = (1 + log(tft,e))? log(N/ni) (3)
Essay lengths are usually different and this has an
impact on term weights. To abstract from different
essay lengths, each essay feature vector is normal-
ized to unit length. After normalization, the result-
ing essay feature vectors are fed into classifiers.
4.3 Classifiers
We experimented with three linear classifiers - lin-
ear support vector machines, logistic regression and
perceptrons - all from scikit-learn (Pedregosa et al,
2011). These algorithms are suitable for high dimen-
sional and sparse data (text data is high dimensional
and sparse). In the following paragraphs, we briefly
describe the algorithms and the parameter values we
selected.
SVMs have been explored systematically for text
categorization (Joachims, 1998). An SVM classi-
fier finds a hyperplane that separates examples into
two classes with maximal margin (Cortes and Vap-
nik, 1995) (Multi-classes are handled by multi one-
versus-rest classifiers). Examples that are not lin-
early separable in the feature space are mapped to a
higher dimension using kernels. In our experiments,
we used a linear kernel and a penalty parameter of
value 1.0.
In its various forms, logistic regression is also
used for text classification (Zhang et al, 2003;
Genkin et al, 2007; Yu et al, 2011) and native
language identification (Tetreault et al, 2012). Lo-
gistic regression classifies data by using a decision
boundary, determined by a linear function of the fea-
tures. For the implementation of the algorithm, we
used the LIBLINEAR open source library (Fan et
al., 2008) from scikit-learn (Pedregosa et al, 2011)
and we fixed the regularization parameter to 100.0.
For baseline, we used a perceptron classifier
(Rosenblatt, 1957). Perceptron (or single layer net-
work) is the simplest form of neural network. It is
designed for linear separation of data and works well
for text classification. The number of iterations of
the training algorithm is fixed to 70 and the rest of
parameters are left with their default values.
5 Results and Discussion
For each classifier, we ran ten-fold cross-validation
experiments. We divided the training and develop-
ment data into ten folds using the same fold splitting
ids as requested by the shared task organizers and
also as used in (Tetreault et al, 2012). Nine of the
folds were used for training and the tenth for test-
ing the trained model. This was repeated ten times
with each fold being held out for testing. The per-
formance of the classifiers on different features are
presented in terms of average accuracy.
Table 3 gives the average accuracies based on
the TF-IDF of word and character n-grams. Lin-
ear SVM gives the highest accuracy of 84.55% us-
ing features extracted from unigrams and bigrams
of words. Logistic regression also gives comparable
accuracy of 84.45% on the same features.
219
Table 3: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
Words
1 74.73 74.18 65.45
2 80.91 80.27 75.45
1 and 2 84.55 84.45 78.82
(1 and 2)* 83.36 83.27 78.73
* minus country and language names
Characters
1 18.45 19.27 9.09
2 43.27 40.82 10.36
3 71.36 68.00 36.91
4 80.36 79.91 59.64
5 83.09 82.64 73.91
6 84.09 84.00 76.45
The size of the feature vector of unigrams and bi-
grams of words is 73,6262. For each essay, only a
few of the features have non-zero values. Which
features are active and most discriminating in the
classifiers? Table 4 shows the ten most informative
features for the 10th run in the cross-validation (as
picked up linear SVM).
Table 4: Ten most informative features for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI in china / hold / china / time on / may / taiwan / just /still / , the / . take
FRE french / conclude , / even if / in france / france / toconclude / indeed , / ... / . indeed / indeed
GER special / furthermore / might / germany / , because /have to / . but / - / often / , that
HIN which / and concept / various / hence / generation / &/ towards / then / its / as compared
ITA in italy / , for / infact / that a / italy / i think / in fact /italian / think that / :
JPN , and / i disagree / is because / . it / . if / i think /japan , / japanese / in japan / japan
KOR . however / however , / even though / however / thesedays / various / korea , / korean / in korea / korea
SPA an specific / because is / moment / , etc / going to / ,is / necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / with out / when compared / i conclude /
and also
TUR ages / istanbul / addition to / conditions / enough / inturkey / the life / ; / . because / turkey
The ten most informative features include coun-
2features that occur less than 5 times or that occur in more
than 50% of the essays are removed from the vocabulary
try and language names. For example, for Japanese
and Korean L1s, four of the ten top features include
Korea or Korean in the unigrams or bigrams. How
would the classification accuracy decrease if we re-
moved mentions of country or language names?
We made a list of the 11 L1 language names and
the countries where they are mainly spoken (for ex-
ample, German, Germany, French, France, etc.). We
considered this list as stop words (i.e. removed them
from corpus) and ran the whole classification exper-
iments. The new best accuracy is 83.36% ( a loss of
just 1.2% ). Table 3 shows the new accuracies for all
classifiers. The new top ten features mostly consist
of function words and some spelling errors. Table 5
shows all of the new top ten features.
The spelling errors seem to have been influenced
by the L1 languages, especially for French and
Spanish languages. The English words example
and developed have similar sounding/looking equiv-
alents in French (exemple and de?veloppe?) . Simi-
larly, the English words necessary and different have
similar sounding/looking words in Spanish (nece-
sario and diferente). These spelling errors made it
to the top ten features. But how discriminating are
they on their own?
Table 5: Ten most informative features (minus country
and language names) for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI and more / hold / more and / time on / taiwan / may /just / still / . take / , the
FRE conclude / exemple / developped / conclude , / evenif / to conclude / indeed , / ... / . indeed / indeed
GER has to / special / furthermore / might / , because /have to / . but / - / often / , that
HIN and concept / which / various / hence / generation / &/ towards / then / its / as compared
ITA possibility / probably / particular / , for / infact / thata / i think / in fact / think that / &
JPN i agree / the opinion / tokyo / two reasons / is because/ , and / i disagree / . it / . if / i think
KOR creative / , many / ?s / . also / . however / even though/ however , / various / however / these days
SPA activities / an specific / moment / , etc / going to / , is/ necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / when compared / with out / i conclude /
and also
TUR enjoyable / being / ages / addition to / istanbul /enough / conditions / the life / ; / . because
We ran experiments with features extracted from
220
Table 6: Confusion matrix: Best accuracy is for German (95%) and the worst is for Hindi (72%)
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
ARA 83 1 4 1 1 3 1 2 3 1 0
CHI 0 88 2 0 2 0 2 5 1 0 0
FRE 3 0 88 2 1 2 0 1 2 0 1
GER 2 0 1 95 0 0 0 0 1 0 1
HIN 2 1 1 1 72 0 0 0 2 18 3
ITA 0 0 6 3 0 84 0 0 6 0 1
JPN 1 2 0 1 1 0 84 10 0 0 1
KOR 0 3 0 2 3 0 8 81 1 1 1
SPA 6 2 5 2 0 4 0 0 79 0 2
TEL 0 0 0 0 16 0 1 0 0 83 0
TUR 1 1 0 1 3 0 0 0 1 0 93
only spelling errors. For comparison, we also ran
experiments with POS tags with and without their
words. None of these experiments beat the best ac-
curacy obtained using unigram and bigram of words
- not even the unigram and bigram of POS tagged
words. See table 7 for the obtained results.
Table 7: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
POS
1 17.00 17.09 9.09
2 43.45 40.00 11.18
3 55.27 53.55 35.36
4 56.09 56.18 48.64
POS + Word
1 75.09 74.18 64.09
2 80.45 80.64 76.18
1 and 2 83.00 83.36 79.09
Spelling errors - characters
1 20.36 21.00 9.09
2 34.09 32.64 9.73
3 47.00 44.64 26.82
4 50.82 48.09 41.64
1?4 51.82 48.27 34.18
words 42.73 39.45 28.73
All our reported results so far have been global
classification results. Table 6 shows the confusion
matrix for each L1. The best accuracy is 95% for
German and the worst is for Hindi (72%). Hindi
is classified as Telugu (18%) of the times and Tel-
ugu is classified as Hindi 16% of the times and
only one Telugu essay is classified as any other than
Hindi. More generally, the confusion matrix seems
to suggest that geographically closer countries are
more confused with each other: Hindi and Telugu,
Japanese and Korean, Chinese and Korean.
The best accuracy (84.55%) obtained in our ex-
periments is higher than the state-of-the-art accuracy
reported in (Tetreault et al, 2012) (80.9%). But the
features we used are not different from those com-
monly used in the literature (Koppel et al, 2005;
Tetreault et al, 2012) (n-grams of characters or
words). The novel aspect of our system is the use
of TF-IDF weighting on all of the features including
on unigrams and bigrams of words.
TF-IDF weighting has already been used in na-
tive language identification (Kochmar, 2011; Ahn,
2011). But its importance has not been fully ex-
plored. Experiments in Kochmar (2011) were lim-
ited to character grams and in a binary classifica-
tion scenario. Experiments in Ahn (2011) applied
TF-IDF weighting to identify content words and
showed how their removal decreased performance
(Ahn, 2011). By contrast, in this paper, we applied
TF-IDF weighting consistently to all features - same
type features (e.g. unigrams) or combined features
(e.g. unigram and bigrams).
How would the best accuracy change if TF-IDF
weighting is not applied? Table 8 shows the changes
to the best average accuracies with and without
TF/IDF weighting for the three classifiers.
Table 8: The importance of TF-IDF weighting
TF IDF SVM LR Perceptron
Yes Yes 84.55 84.45 78.82
Yes No 80.82 80.73 63.18
No Yes 82.36 82.27 78.82
No No 79.18 78.55 56.36
221
6 Conclusions
This paper has presented the system that participated
in the 2013 NLI Shared Task in the closed-training
track. Cross-validation testing on the TOEFL11 cor-
pus showed that the system could achieve an accu-
racy of about 84.55% in categorizing unseen essays
into one of the eleven L1 languages.
The novel aspect of the system is the use
of TF-IDF weighting schemes on features ?
which could be any or combination of n-gram
words/characters/POS tags. The feature combina-
tion that gave the best accuracy is the TF-IDF of
unigrams and bigrams of words. The next best fea-
ture class is the TF-IDF of 6-gram characters , which
achieved 84.09%, very close to 84.55%. Both lin-
ear support vector machines and logistic regression
classifiers have performed almost equally.
To improve performance in NLI, future work
should examine new features that can classify ge-
ographically or typologically related languages such
as Hindi and Telugu. Future work should also ana-
lyze the information obtained in NLI experiments to
quantify and investigate differences in the usage of
foreign language lexicon or grammar according to
the individual?s mother tongue.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Commissions
7th Framework Program under grant agreement no
238405 (CLARA). The authors would like to thank
the organizers of the NLI Shared Task 2013 for pro-
viding prompt reply to all our inquiries and for coor-
dinating a very interesting and fruitful shared task.
References
Charles S. Ahn. 2011. Automatically detecting authors?
native language. Ph.D. thesis, Monterey, California.
Naval Postgraduate School.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, pages 69?72. Association for
Computational Linguistics.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author Profiling
for English Emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Alexander Genkin, David D Lewis, and David Madigan.
2007. Large-scale bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Sylviane Granger, Estelle Dagneaux, and Fanny Meu-
nier. 2009. International Corpus of Learner English.
Presses Universitaires de Louvain, Louvain-la-Neuve.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. Springer.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge, United Kingdom.
Moshe Koppel, Jonathan Schler, and Kfir Zigon. 2005.
Automatically determining an anonymous author?s na-
tive language. Lecture Notes in Computer Science,
3495:209?217.
Robert Lado. 1957. Applied Linguistics for Language
Teachers. University of Michigan Press.
Paul Lewis, Gary Simons, and Charles Fennig. 2013.
Ethnologue: Languages of the World, Seventeeth Edi-
tion. SIL International.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval, volume 1. Cambridge University Press Cam-
bridge.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and E?douard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825?2830.
Frank Rosenblatt. 1957. The perceptron, a perceiv-
ing and recognizing automaton Project Para. Cornell
Aeronautical Laboratory.
Gerard Salton and Michael McGill. 1984. Introduction
to Modern Information Retrieval. McGraw-Hill Book
Company.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
222
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
Summary report on the first shared task on native lan-
guage identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you?: Naive bayes
detection of non-native utterance text. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies (NAACL ?01).
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop, pages 53?61. Citeseer.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41?75.
Jian Zhang, Rong Jin, Yiming Yang, and Alexander G.
Hauptmann. 2003. Modified logistic regression: An
approximation to svm and its applications in large-
scale text categorization. In ICML, pages 888?895.
223
Workshop on Humans and Computer-assisted Translation, pages 93?98,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Quantifying the Influence of MT Output in the Translators? Performance:
A Case Study in Technical Translation
Marcos Zampieri
Saarland University
Saarbr?ucken, Germany
mzampier@uni-koeln.de
Mihaela Vela
Saarland University
Saarbr?ucken, Germany
m.vela@mx.uni-saarland.de
Abstract
This paper presents experiments on the use
of machine translation output for technical
translation. MT output was used to pro-
duced translation memories that were used
with a commercial CAT tool. Our exper-
iments investigate the impact of the use
of different translation memories contain-
ing MT output in translations? quality and
speed compared to the same task without
the use of translation memory. We evalu-
ated the performance of 15 novice transla-
tors translating technical English texts into
German. Results suggest that translators
are on average over 28% faster when us-
ing TM.
1 Introduction
Professional translators use a number of tools to
increase the consistency, quality and speed of their
work. Some of these tools include spell checkers,
text processing software, terminological databases
and others. Among all tools used by professional
translators the most important of them nowadays
are translation memory (TM) software. TM soft-
ware use parallel corpora of previously translated
examples to serve as models for new transla-
tions. Translators then validate or correct previ-
ously translated segments and translate new ones
increasing the size of the memory after each new
translated segment.
One of the great issues in working with TMs is
to produce the TM itself. This can be time con-
suming and the memory should ideally contain a
good amount of translated segments to be consid-
ered useful and accurate. For this reason, many
novice translators do not see the benefits of the
use of TM right at the beginning, although it is
consensual that on the long run the use of TMs in-
crease the quality and speed of their work. To cope
with this limitation, more TM software have pro-
vided interface to machine translation (MT) soft-
ware. MT output can be used to suggest new seg-
ments that were not previously translated by a hu-
man translator but generated automatically from
an MT software. But how helpful are these trans-
lations?
To answer this question, the experiments pro-
posed in this paper focus on the translator?s per-
formance when using TMs produced by MT out-
put within a commercial CAT tool interface. We
evaluate the quality of the translation output as
well as the time and effort taken to accomplish
each task. The impact of MT and TM in trans-
lators? performance has been explored and quan-
tified in different settings (Bowker, 2005; Guer-
berof, 2009; Guerberof, 2012; Morado Vazquez
et al., 2013). We believe this paper constitutes
another interesting contribution to the interface
between the study of the performance of human
translators, CAT tools and machine translation.
2 Related Work
CAT tools have become very popular in the last
20 years. They are used by freelance transla-
tors as well as by companies and language ser-
vice providers to increase translation?s quality and
speed (Somers and Diaz, 2004; Lagoudaki, 2008).
The use of CAT tools is part of the core curricu-
lum of most translation studies degrees and a rea-
sonable level of proficiency in the use of these
tools is expected from all graduates. With the im-
provement of state-of-the-art MT software, a re-
cent trend in CAT research is its integration with
machine translation tools as for example the Mate-
Cat
1
project (Cettolo et al., 2013).
There is considerable amount of studies on MT
post-editing published in the last years (Specia,
2011; Green et al., 2013). Due to the scope of our
1
www.matecat.com
93
paper (and space limitation) we will deliberately
not discuss the findings of these experiments and
instead focus on those that involve the use of trans-
lation memories. Post-editing tools are substan-
tially different than commercial CAT tools (such
as the one used here) and even though the TMs
used in our experiments were produced using MT
output, we believe that our experiment setting has
more in common with similar studies that investi-
gate TMs than MT post-editing.
The study by Bowker (2005) was one of the
first to quantify the influence of TM in transla-
tors work. The experiment divided translators in
three groups: A, B and C. Translators in Group
A did not use a TM, translators in Group B used
an unmodified TM and finally translators in group
C used a TM that had been deliberately mod-
ified with a number of translation errors. The
study concluded that when faced with time pres-
sure, translators using TMs tend not to be criti-
cal enough about the suggestions presented by the
software.
Another similar experiment (Guerberof, 2009)
compared productivity and quality of human trans-
lations using MT and TM output. The experiment
was conducted starting with the hypothesis that the
time invested in post-editing one string of machine
translated text will correspond to the same time in-
vested in editing a fuzzy matched string located in
the 80-90 percent range. This study quantified the
performance of 8 translators using a post-editing
tool. According to the author, the results indicate
that using a TM with 80 to 90 fuzzy matches pro-
duces more errors than using MT segments or hu-
man translation.
The aforementioned recent work by Morado
Vazquez et al. (2013) investigates the performance
of twelve human translators (students) using the
ACCEPT post-editing tool. Researchers provided
MT and TM output and compared time, quality
and keystroke effort. Findings of this study indi-
cate that the use of a specific MT has a great im-
pact in the translation activity in all three aspects.
In the context of software localization, productiv-
ity was also tested by Plitt and Masselot (2010)
combining MT output and a post-editing tool. An-
other study compared the performance of human
translators in a scenario using TMs and a com-
mercial CAT tool (Across) with a second scenario
using post-editing (L?aubli et al., 2013).
As to our study, we used instead of a post-
editing tool, a commercial CAT tool, the SDL Tra-
dos Studio 2014 version. A similar setting to ours
was explored by Federico et al. (2012) using SDL
Trados Studio integrating a commercial MT soft-
ware. We took the decision of working a commer-
cial CAT tool for two reasons: first, because this
is the real-world scenario faced by translators in
most companies and language service providers
2
and second, because it allows us to explore a dif-
ferent variable that the aforementioned studies did
not substantially explore, namely: MT output as
TM segments.
3 Setting the Experiment
In our experiments we provided short texts from
the domain of software development containing up
to 343 tokens each to 15 beginner translators. The
average length of these texts ranges between 210
tokens in experiment 1 to 264 tokens in experi-
ment 3 divided in 15 to 17 segments (average) (see
table 2). Translators were given English texts and
were asked to translate them into German, their
mother tongue. One important remark is that all
15 participants were not aware that the TMs we
made available were produced using MT output.
The 15 translators who participated in these
experiments are all 3
rd
semester master degree
students who have completed a bachelors degree
in translation studies and are familiar with CAT
tools. All of them attended at least 20 class
hours about TM software and related technologies.
Translators who participated in this study were all
proficient in English and they have studied it as a
foreign language at bachelor level.
As previously mentioned, the CAT tool used in
these experiments is the most recent version of
SDL Trados, the Studio 2014
3
version. Transla-
tors were given three different short texts to be
translated in three different scenarios:
1. Using no translation memory.
2. Using a translation memory collected with
modified MT examples.
3. Using translation memory collected with un-
modified MT examples.
In experiment number two we performed a
number of modifications in the TM segments. As
2
Although the use of MT and post-editing software has
been growing, commercial TM software is still the most pop-
ular alternative.
3
http://www.sdl.com/campaign/lt/sdl-trados-studio-2014/
94
can be seen in table 1, these modifications were
sufficient to alter the coverage of the TM, but did
not introduce translation errors to the memory.
4
The alterations we performed along with an exam-
ple of each of them can be summarized as follows:
? Deletion: ?To paste the text currently in the
clipboard, use the Edit Paste menu item.? -
?To paste the text, use the Edit Paste menu
item.?
? Modification: ?Persistent Selection is dis-
abled by default.? - ?Persistent Selection is
enabled by default.?
? Substitution: ?The editor is composed of the
following components:? - ?The editor is com-
posed of the following elements:?
Three texts were available per scenario, each of
them with different TM coverage scores (see table
1). Students were asked to translate the texts at
their own pace without time limitation and were
allowed to use external linguistic resources such
as dictionaries, lexica, parallel concordancers, etc.
3.1 Corpus and TM
The corpus used for these experiments is the KDE
corpus obtained from the Opus
5
repository (Tiede-
mann, 2012). The corpus contains texts from the
domain of software engineering, hence the title: ?a
case study in technical translation?. We are con-
vinced that technical translation contains a sub-
stantial amount of fixed expressions and techni-
cal terms different from, for example, news texts.
This makes technical translation, to our under-
standing, an interesting domain for the use of TM
by professional translators and for experiments of
this kind.
In scenarios 1, 2 and 3 we measured different
aspects of translation such as time and edited seg-
ments. One known shortcoming of our experiment
design is that unlike most post-editing software
the reports available in CAT tools are quite poor
(e.g. no information about keystrokes is provided).
Even so, we stick to our decision of using a TM
software and tried to compensate this shortcoming
by a careful qualitative and quantitative data anal-
ysis after the experiments.
4
Modifications were carried out in the source and target
languages
5
http://opus.lingfil.uu.se/
Table number 1 presents the coverage scores for
the different TMs and texts used in the experi-
ments. Coverage scores were calculated based on
the information provided by SDL Trados Studio.
We provided 9 different texts to be translated to
German (3 for each scenario), the 6 texts provided
for experiments 2 and 3 are presented next.
Text Experiment TM Coverage
Text D 2 61.23%
Text E 2 78.16%
Text F 2 59.15%
Average 2 66,18%
Text G 3 88.27%
Text H 3 59.92%
Text I 3 65.16%
Average 3 71,12%
Table 1: TM Coverage
We provided different texts and levels of coverage
to investigate the impact of this variable. We as-
sured an equal distribution of texts among trans-
lators: each text was translated by 5 translators.
This allowed us to calculate average results and
to consider the average TM coverage difference of
4,93% between experiment 2 and 3.
4 Results
We observed performance gain when using any of
the two TMs, which was expectable. The results
varied according to the coverage of the TM. In
experiment number 3, texts contained on average
over 7 segments with 100% matches
6
and exper-
iment number 2 only 2.68. This allowed transla-
tors to finish the task faster in experiment number
3. The average results obtained in the different ex-
periments are presented in table number 2.
7
Criteria Exp. 1 Exp. 2 Exp. 3
Number of Segments 15.85 15.47 17.29
Number of Tokens 209.86 202.89 264.53
Context Matches 6.58 6.06
Repetitions 0.18
100% 2.68 7.18
95% to 99% 0.42 0.12
85% to 94% 0.21
75% to 84% 2.11 0.18
50% to 75% 0.19
New Segments 15.86 5.89 3.24
Time Elapsed (mins.) 37m45s 26m3s 19m21s
Table 2: Average Scores
6
Translators were allowed to modify 100% and context
matches.
7
According to the Trados Studio documentation, a repeti-
tion occurs every time the tool finds the exact same segment
in another (or the same) file the user is translating
95
As to the time spent per segment, experiments
indicate a performance gain of over 52% in ex-
periment number 3 and over 28% in experiment
number 2.
Criteria Exp.1 Exp. 2 Exp. 3
Time Segment (mins.) 2m22s 1m41s 1m07s
Average gain to 1 +28.87% +52.82%
Average gain to 2 +33.77%
Table 3: Time per Segment
Apart from the expectable performance gain when
using TM, we also found a considerable difference
between the use of the modified and unmodified
TM. Translators completed segments in experi-
ment number 3, on average, 33.77% faster than
experiment two. The difference of coverage be-
tween the two TMs was 4,93%, which suggests
that a few percentage points of TM coverage re-
sults on a greater performance boost.
We also have to acknowledge that the experi-
ments were carried out by translators in the same
order in which they are presented in this paper.
This may, of course, influence performance in all
three experiments as translators were more used
to the task towards the end of the experiment. One
hypothesis is that the poor performance in exper-
iment 1, could be improved if this task was done
for last and conversely, the performance boost ob-
served in experiment 3, could be a bit lower if
this experiment was done first. This variable was
not explored in similar productivity studies such
as those presented in section two and, to our un-
derstanding, inverting the order of tasks could be
an interesting variable to be tested in future exper-
iments.
As a general remark, although all translators
had experience with the 2014 version of Trados
Studio, we observed a great difficulty in perform-
ing simple tasks with Windows for at least half of
the group. Simple operations such as copying, re-
naming and moving files or creating folders in the
file system were very time consuming. Trados in-
terface also posed difficulties to translators. For
example, the generation of reports through batch
tasks in a different window was for most transla-
tors confusing. These operations could be simpli-
fied as it is in other CAT tools such as memoQ.
8
8
http://kilgray.com/products/memoq
4.1 A Glance at Quality Estimation
One of the future directions that this work will take
is to investigate the quality of human translations.
Our initial hypothesis is that it is possible to apply
state-of-the-art metrics such as BLEU (Papineni
et al., 2002) or METEOR (Denkowski and Lavie,
2011) to estimate the quality of these translations
regardless of how they are produced.
For machine translation output, quality nowa-
days is measured by automatic evaluation met-
rics such as the aforementioned IBM BLEU (Pap-
ineni et al., 2002), NIST (Doddington, 2002), ME-
TEOR (Denkowski and Lavie, 2011), the Leven-
sthein (1966) distance based WER (word error-
rate) metric, the position-independent error rate
metric PER (Tillmann et al., 1997) and the trans-
lation error rate metric TER (Snover et al., 2006)
with its newer version TERp (Snover et al., 2009).
The most frequently used one is IBM
BLEU (Papineni et al., 2002). It is easy to
use, language-independent, fast and requires
only the candidate and reference translation.
IBM BLEU is based on the n-gram precision by
matching the machine translation output against
one or more reference translations. It accounts
for adequacy and fluency through word precision,
respectively the n-gram precision, by calculating
the geometric mean. Instead of recall, in IBM
BLEU the brevity penalty (BP) was introduced.
Different from IBM BLEU, METEOR evalu-
ates a candidate translation by calculating the pre-
cision and recall on unigram level and combining
them in a parametrized harmonic mean. The result
from the harmonic mean is than scaled by a frag-
mentation penalty which penalizes gaps and dif-
ferences in word order.
For our investigation we applied METEOR on
the human translated text. Our intention is to test
whether we can reproduce the observations from
the experiments: is the experiment setting 3 bet-
ter than the setting of experiment 2? Therefore,
METEOR is used here to investigate whether we
can correlate it with our experiments and not to
evaluate the produced translations. Table number
4 presents the scores obtained with METEOR.
Exp. 2 Exp. 3
Average Score (mean) 0.14 0.41
Best Result 0.35 0.58
Worst Result 0.11 0.25
Table 4: METEOR Scores
96
In experiment number 3 we have previously ob-
served that the translators? performance was sig-
nificantly better and that translators could translate
each segment on average 33.77% faster than ex-
periment 2 and 52.82% faster than experiment 1.
By applying METEOR scores we can also observe
that experiment 3 achieved higher scores which
seems to indicate more suitable translations than
experiment number 2. Quality estimation is one
of the aspects we would like to explore in future
work.
5 Conclusion
This paper is a first step towards the comparison
of different TMs produced with MT output and
their direct impact in human translation. Our study
shows a substantial improvement in performance
with the use of translation memories containing
MT output used trough commercial CAT software.
To our knowledge this experiment setting was not
tested in similar studies, which makes our paper a
new contribution in the study of translators? per-
formance. Although the performance gain seems
intuitive, the quantification of these aspects within
a controlled experiment was not substantially ex-
plored.
We opted for the use of a state-of-the-art com-
mercial CAT tool as this is the real-world scenario
that most translators face everyday. In compari-
son to translating without TM, translators were on
average 28.87% faster using a modified TM and
52.82% using an unmodified one. Between the
two TMs we observed that translators were on av-
erage 33.77% faster when using the unmodified
TM. As previously mentioned, the order in which
this tasks were carried out should be also taken
into account. The performance boost of 33.77%
when using a TM that is only 4,93% better is also
an interesting outcome of our experiments that
should be looked at in more detail.
Finally, in this paper we used METEOR scores
to assess whether it is possible to correlate trans-
lations? speed, quality and TM coverage. The av-
erage score for experiment number 2 was 0.14 and
for experiment number 3 was 0.41. Our initial
analysis suggests that a relation between the two
variables exists for our dataset. Whether this rela-
tion can be found in other scenarios is still an open
question and we wish to investigate this variable
more carefully in future work.
5.1 Future Work
We consider these experiments as a pilot study that
was carried out to provide us a set of variables that
we wish to investigate further. There are a number
of aspects that we wish to look in more detail in
future work.
Future experiments include the aforementioned
quality estimation analysis by applying state-of-
the-art metrics used in machine translation. Using
these metrics we would like to explore the extent
to which it is possible to use automatic methods
to study the interplay between quality and perfor-
mance in computer assisted translation. Further-
more, we would like to perform a qualitative anal-
ysis of the produced translations using human an-
notators and inter annotator agreement (Carletta,
1996).
The performance boost observed between sce-
narios 2 and 3 should be looked in more detail
in future experiments. We would like to replicate
these experiments using other different TMs and
explore this variable more carefully. Another as-
pect that we would like to explore in the future is
the direct impact of the use of different CAT tools.
Does the same TM combined with different CAT
tools produce different results? When conducting
these experiments, we observed that a simplified
interface may speed up translators? work consid-
erably.
Other directions that our work will take include
controlling other variables not taken into account
in this pilot study such as: the use of termino-
logical databases, spelling correctors, etc. How
and to which extent do they influence performance
and quality? Finally, we would also like to use
eye-tracking to analyse the focus of attention of
translators as it was done in previous experiments
(O?brien, 2006).
Acknowledgments
We thank the students who participated in these
experiments for their time. We would also like
to thank the detailed feedback provided by the
anonymous reviewers who helped us to increase
the quality of this paper.
References
Lynne Bowker. 2005. Productivity vs quality? a pilot
study on the impact of translation memory systems.
Localisation Reader, pages 133?140.
97
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi,
Marcello Federico, Loic Barrault, and Holger
Schwenk. 2013. Issues in incremental adaptation of
statistical mt from human post-edits. In Proceedings
of the MT Summit XIV Workshop on Post-editing
Technology and Practice (WPTP-2), Nice, France.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT 2002, pages 138?145,
San Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.
Marcello Federico, Alessandro Cattelan, and Marco
Trombetti. 2012. Measuring user productivity
in machine translation enhanced computer assisted
translation. In Proceedings of Conference of the As-
sociation for Machine Translation in the Americas
(AMTA).
Spence Green, Jeffrey Heer, and Christopher D Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems.
Ana Guerberof. 2009. Productivity and quality in
the post-editing of outputs from translation memo-
ries and machine translation. Localisation Focus,
7(1):133?140.
Ana Guerberof. 2012. Productivity and Quality in the
Post-Edition of Outputs from Translation Memories
and Machine Translation. Ph.D. thesis, Rovira and
Virgili University Tarragona.
Elina Lagoudaki. 2008. The value of machine transla-
tion for the professional translator. In Proceedings
of the 8th Conference of the Association for Ma-
chine Translation in the Americas, pages 262?269,
Waikiki, Hawaii.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing post-editing efficiency in a realistic translation
environment. In Proceedings of MT Summit XIV
Workshop on Post-editing Technology and Practice.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, (8):707?710, Febru-
ary.
Lucia Morado Vazquez, Silvia Rodriguez Vazquez, and
Pierrette Bouillon. 2013. Comparing forum data
post-editing performance using translation memory
and machine translation output: a pilot study. In
Proceedings of the Machine Translation Summit
XIV), Nice, France.
Sharon O?brien. 2006. Eye-tracking and translation
memory matches. Perspectives: Studies in Transla-
tology, 14:185?204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. The Prague Bul-
letin of Mathematical Linguistics, 93:7?16.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, AMTA.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter? exploring different human judgments with a
tunable mt metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2009.
Harold Somers and Gabriela Fernandez Diaz. 2004.
Translation memory vs. example-based mt: What is
the difference? International Journal of Transla-
tion, 16(2):5?33.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th Conference of the European
Association for Machine Translation, pages 73?80,
Leuven, Belgium.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey. European Lan-
guage Resources Association (ELRA).
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conference on Speech Communication
and Technology, EUROSPEECH 1977, pages 2667?
2670.
98
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58?67,
Dublin, Ireland, August 23 2014.
A Report on the DSL Shared Task 2014
Marcos Zampieri
1
, Liling Tan
2
, Nikola Ljube
?
si
?
c
3
, J
?
org Tiedemann
4
Saarland University, Germany
1,2
University of Zagreb, Croatia
3
Uppsala University, Sweden
4
marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.de
jorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hr
Abstract
This paper summarizes the methods, results and findings of the Discriminating between Similar
Languages (DSL) shared task 2014. The shared task provided data from 13 different languages
and varieties divided into 6 groups. Participants were required to train their systems to discrimi-
nate between languages on a training and development set containing 20,000 sentences from each
language (closed submission) and/or any other dataset (open submission). One month later, a test
set containing 1,000 unidentified instances per language was released for evaluation. The DSL
shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7%
average accuracy.
1 Introduction
Discriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-
tification systems. Although in recent years systems have been trained to discriminate between more
languages
1
, they still struggle to discriminate between similar languages such as Croatian and Serbian or
Malay and Indonesian.
From an NLP point of view, the difficulty systems face when discriminating between closely related
languages is similar to the problem of discriminating between standard national language varieties (e.g.
American English and British English or Brazilian Portuguese and European Portuguese), henceforth
varieties. Recent studies show that language varieties can be discriminated automatically using words or
characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance
limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from
different national varieties, modelling pluricentric languages as unique classes.
To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we
decided to organize the Discriminating between Similar Languages (DSL)
2
shared task. This shared task
was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial) in the 2014 edition of COLING.
The motivation behind the DSL shared task is two-fold. Firstly, we have observed an increase of
interest in the topic. This is reflected by a number of papers that have been published about this task in
recent years starting with Ranaivo-Malanc?on (2006) for Malay and Indonesian and Ljube?si?c et al. (2007)
for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of
data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,
Malay, Portuguese and Spanish.
The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared
task focusing on the discrimination of similar languages has been organized previously. The most sim-
ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were
required to classify French journalistic texts with respect to their geographical location as well as the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Brown (2013) reports results on a system trained to recognize more than 1,100 languages
2
http://corporavm.uni-koeln.de/vardial/sharedtask.html
58
decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual
language identification shared task, a general-purpose language identification task containing data from
74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task
(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11
different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the
native language of the writer of each text.
2 Related Work
Among the first studies to investigate the question of discriminating between similar languages is the
study published by Ranaivo-Malanc?on (2006). The author presents a semi-supervised model to dis-
tinguish between Indonesian and Malay, two closely related languages from the Austronesian family
represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived
from the most frequent words in each language, lists of exclusive words, and the format of numbers
(Malay uses decimal points whereas Indonesian uses commas). The author compares the performance
of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).
Ljube?si?c et al. (2007) proposed a computational model for the identification of Croatian texts in
comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The
approach includes a ?black list?, which increases the performance of the algorithm. Tiedemann and
Ljube?si?c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study
reports significantly higher performance compared to general purpose language identification methods.
The methods applied to discriminate between texts from different language varieties and dialects are
similar to those applied to similar languages
3
. One of the methods proposed to identify language varieties
is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from
the mainland and Taiwan with results of up to 92% accuracy.
Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).
In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to
identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in
a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The
algorithm was later adapted to classify Spanish texts using not only the classical word and character
n-grams but also POS distribution (Zampieri et al., 2013).
The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate
between texts from three different English varieties (Canadian, Australian and British) across different
domains. The authors state that the results obtained suggest that each variety contains characteristics that
are consistent across multiple domains, which enables algorithms to distinguish them regardless of the
data source.
Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic
language varieties
4
using character and word n-grams. The authors built their own dataset using crowd-
sourcing and investigated annotators? behaviour, agreement and performance when manually tagging
instances with the correct label (variety).
3 Methods
In the following subsections we will describe the methodology adopted for the DSL shared task. Due to
the lack of comparable resources, the first decision we had to take was to create a dataset that could be
used in the shared task and also redistributed to be used in other experiments. We opted for the creation
of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).
Groups interested in participating in the DSL shared task had to register themselves in the shared
task website to receive the training and test data. Each group could participate in one or two types of
3
In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More
on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).
4
Zaidan and Callison-Burch (2013) use the terms ?varieties? and ?dialects? interchangeably whereas Lui and Cook (2013)
use the term ?national dialect? to refer to what previous work describes as ?national variety?.
59
submission as follows:
? Closed Submission: Using only the DSL corpus collection for training.
? Open Submission: Using any other dataset including or not the DSL collection for training.
In the open submission we did not make any distinction between systems using the DSL corpus col-
lection and those that did not. This is different from the types of submissions for the NLI shared task
2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any
dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2
- any dataset excluding TOEFL11.
For each of these submission types, participants were allowed to submit up to three runs, resulting in
a maximum of six runs in total (three closed submissions and three open submissions).
3.1 Data
As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was
entitled DSL corpus collection and its compilation was motivated by the absence of a resource that
allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation
of this collection and the preliminary baseline experiments are described in Tan et al. (2014).
The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-
ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens
5
each.
The languages are presented in table 1 with their ISO 639-1 language codes
6
. For language varieties the
country code is appended to the ISO code (e.g. en-GB refers to the British variety of English).
Group Language/Variety Code
Bosnian bs
A Croatian hr
Serbian sr
Indonesian id
B Malay my
Czech cz
C Slovak sk
Brazilian Portuguese pt-BR
D European Portuguese pt-PT
Argentine Spanish es-AR
E Castilian Spanish es-ES
British English en-GB
F American English en-US
Table 1: Language Groups - DSL 2014 Shared Task
For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were
selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard
language, which is an important factor to be considered when working with language varieties. Other data
sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore
not suitable for the purpose of the shared task. A number of studies mentioned in the related work section
use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,
2012)
Given what has been said in this section, we consider the collection to be a suitable comparable corpora
from this task, which was compiled to avoid bias in classification towards source, register and topics. The
5
We considered a token as orthographic units delimited by white spaces.
6
http://www.loc.gov/standards/iso639-2/php/English_list.php
60
DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in
the language/variety, the second column states its group and the last column refers to its language code.
7
3.1.1 Problems with Group F
There are no major problems to report regarding the organization of the shared task nor with the com-
pilation of the DSL corpus collection apart from some issues in the Group F data. The organizers and
a couple of teams participating in the shared task observed very poor performance when distinguishing
instances from group F (British English - American British). For example, the baseline experiments
described in Tan et al. (2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84
for Group E (the second lowest score). Some of the teams asked human annotators to try to distinguish
the sentences manually and they concluded that some instances were probably misclassified.
We decided to look more carefully at the data and noticed that the instances were originally tagged
based on the websites (newspapers) that they were retrieved from and not the country of the original
publication. There are, however, many cases of cross citation and republication of texts that the original
data sources did not take into account (e.g. British texts that were later republished by an American
website). As the DSL is a corpus collection and manually checking all 20,000 training and development
instances per language was not feasible, we assumed that the original sources
8
from which the texts were
retrieved provided the correct country of origin. The assumption was correct for all language groups but
English.
To illustrate the issues above we present next some misclassified examples. Two particular cases raised
by the UMich team are the following:
(1) I think they can afford to give North another innings and some time in Shield cricket and take
another middle order batsman. (en-US)
(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday night
when it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who is
making his first Triple-A start after coming down from Milwaukee. (en-GB)
Example number one was tagged as American English because it was retrieved from the online edition
of The New York Times but it was in fact first published in Australia. The second example is a text
published by Associated Press describing an event that took place in Ohio, United States, but it was
tagged as British English because it was retrieved by the UK Yahoo! sports section.
Our solution was to exclude the language group F from the final scores and perform a manual check in
all its 1,000 test instances
9
, thus giving the chance to participants to train their algorithms on other data
sources (open submission).
3.2 Schedule
The DSL shared task spanned from March 20
th
when the training set was released, to June 6
th
when
participants could submit a paper (up to 10 pages) describing their system. We provided one month
between the release of the training and the test set. The schedule of the DSL shared task 2014 can be
seen below.
Event Date
Training Set Release March 20
th
, 2014
Test Set Release April 21
st
, 2014
Submissions Due April 23
rd
, 2014
Results Announced April 30
th
, 2014
Paper Submission June 6
th
, 2014
Table 2: DSL 2014 Shared Task Schedule
7
To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask2014
8
See Tan et al. (2014) for a complete description of the data sources of the DSL corpus collection.
9
Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.
61
4 Results
This section summarises the results obtained by all participants of the shared task who submitted final
results.
10
The DSL shared task included 22 enrolled teams from different countries (e.g. Australia,
Estonia, Holland, Germany, United Kingdom and United States). From the 22 enrolled teams, eight of
them submitted their final results. Most of the groups opted to exclusively use the DSL corpus collection
and therefore participated solely in the closed submission track. Two of them compiled comparable
datasets and also participated in the open submission.
Given that the dataset contained misclassified instances, group F (English) was not taken into account
to compute the final shared task scores. In the next subsections we report results in terms of macro-
average F-measure and accuracy.
4.1 Closed Submission
Table 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted their
results for the closed submission track ordered by accuracy.
Team Macro-avg F-score Overall Accuracy
NRC-CNRC 0.957 0.957
RAE 0.947 0.947
UMich 0.932 0.932
UniMelb-NLP 0.918 0.918
QMUL 0.925 0.906
LIRA 0.721 0.766
UDE 0.657 0.681
CLCG 0.405 0.453
Table 3: Open Submission - Results
In the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-
sults over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for
discriminating similar languages and language varieties (Tiedemann and Ljube?si?c, 2012; Lui and Cook,
2013). These five teams submitted system descriptions that allowed us to look in more detail at successful
approaches for this task. System descriptions will be discussed in section 5.
Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These
three groups unfortunately did not submit system description papers. From our point of view, this would
create an interesting opportunity to look more carefully at the weaknesses of approaches that did not
obtain good results in this task.
4.2 Open Submission
Only two systems submitted results for the open submission track and their F-measure and Accuracy
results are presented in table 3.
Team Macro-avg F-score Overall Accuracy
UniMelb-NLP 0.878 0.880
UMich 0.858 0.859
Table 4: Closed Submission - Results
The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-
ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-
ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British
English.
10
Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html
for more detail on the shared task results or at the aforementioned DSL shared task website.
62
Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-
mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in
the open submission they scored 2.1% better than UMich. This difference can be explained by investi-
gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of
the method to obtain correct predictions across different datasets and domains as previously discussed
by Lui and Cook (2013) for English varieties.
4.3 Accuracy per Language Group
In this subsection we look more carefully at the performance of systems in discriminating each class
within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team
sorted alphabetically. The best score per group is displayed in bold.
CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP
A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915
B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972
C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000
D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896
E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807
Table 5: Language Groups A to E - Accuracy Results
The top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)
and C (Czech and Slovak). Four out of eight systems obtained perfect performance when discriminating
Czech and Slovak texts. Perfect performance was not achieved by any of the systems when distinguishing
Malay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracy
obtained by the NRC-CNRC group. The perfect results obtained by four groups when distinguishing
texts from group C suggest that Czech and Slovak texts are not as similar as we assumed before the
shared task, and that they therefore possess strong systemic and/or orthographic differences that allow
well-trained classifiers to perform perfectly. Figure 1 presents the accuracy results of the top 5 groups.
Figure 1: Language Groups A to E Accuracy - Top 5 Systems
Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-
taining 3 languages, proved to be a challenging task as discussed in previous research (Ljube?si?c et al.,
2007; Tiedemann and Ljube?si?c, 2012). The best result was again obtained by the NRC-CNRC group
with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Por-
tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These
results also corroborate the findings of previous studies (Zampieri et al., 2013).
63
The QMUL system that was the 5
th
best system in the closed submission track did not outperform any
of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts
from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on
two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as
?a simple baseline? (Purver, 2014) may be an explanation for the regular performance across different
language groups.
4.4 Results Group F
To document the problems in the group F (British and American English) dataset we included the results
of both the open and closed submissions for this language group in table 6. As previously mentioned,
submitting group F results was optional and we did not include these results in the final shared task
results. Six out of eight systems decided to submit their predictions as closed submissions and the two
groups participating in the open submission track also submitted their group F results.
Team F-score Accuracy Type
UMich 0.639 0.639 Open
UniMelb- NLP 0.581 0.583 Open
NRC-CNRC 0.522 0.524 Closed
LIRA 0.450 0.493 Closed
RAE 0.451 0.481 Closed
UMich 0.463 0.464 Closed
UDE 0.451 0.451 Closed
UniMelb-NLP 0.435 0.435 Closed
Table 6: Group F - Accuracy Results
The results confirm the problems in the DSL dataset discussed in section 3.1.1. After a careful manual
check of the 1,000 test instances, open submissions scores were still substantially lower than the other
groups: 69.9% and 58.3% accuracy. Closed submissions proved to be impossible and only one of the six
systems scored slightly above the 50% baseline.
It should be investigated more carefully in future research whether the poor results for group F reflect
only the problems in the dataset or also the actual difficulty in discriminating between these two varieties
of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well
as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present
in texts from these two varieties and these can be informative features for algorithms to discriminate
between them. Discriminating between other English varieties already proved to be a challenging yet
feasible task in previous research (Lui and Cook, 2013).
5 System Descriptions
All eight systems that submitted their final results to the shared task were invited to submit papers de-
scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-
CNRC, RAE, UMich, UniMelb-NLP and QMUL.
The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-
step approach to predict first the language group than the language of each instance. The language group
was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,
and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F
and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).
An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called
?white lists? inspired by the ?blacklist? classifier (Tiedemann and Ljube?si?c, 2012). These lists are word
lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc?on (2006)
proposed to discriminate between Malay and Indonesian.
64
Two groups used Information Gain (IG) to select the best features for classification, namely UMich
(King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit
open submissions. The UniMelb-NLP team tried different classification methods and features (including
delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf
general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has
been widely used for general-purpose language identification and its performance is regarded superior
to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hier-
archically firstly identifying the language group that a sentence belongs to and subsequently the specific
language, achieving performance comparable to the state-of-the-art, but still slightly below the other
three systems.
The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-
tures. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers?
performance. The cost parameter c is responsible for the trade-off between maximum margin and clas-
sification errors. According to the system description the optimal parameter for this task lies between
30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram
language models in comparison to methods using character n-grams. This observation corroborates the
findings of previous experiments that rely on words as important features to distinguish similar languages
and varieties (Huang and Lee, 2008; Zampieri, 2013)
The features and algorithms presented so far, as well as the system paper descriptions, are summarised
in table7.
11
Team Algorithm Features System Paper
NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014)
RAE MaxEnt Words 1-2, Char. 1-5, ?Whitelist? (Porta and Sancho, 2014)
UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014)
UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014)
QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014)
Table 7: Top 5 Systems - Features and Algorithms at a Glance
6 Conclusion
Shared tasks are an interesting way of comparing algorithms, computational methods and features using
the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled
an important gap in language identification and will allow other researchers to look in more detail at the
problem of discriminating similar languages. Accurate methods for discriminating similar languages can
help to improve performance not only in language identification but also in a number of NLP tasks and
applications such as part-of-speech-tagging, spell checking and machine translation.
The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varieties
divided into 5 groups (A to E), using only the DSL corpus collection. Systems that performed best
modelled their algorithms to perform two-step predictions: first the language group, then the actual class
and used characters and words as features. As we regard the corpus to be a balanced sample of the
news domain, the results obtained confirm the assumption that similar languages and varieties possess
systemic characteristics that can be modelled by algorithms in order to distinguish languages from other
similar languages or varieties using lexical or orthographical features.
Another lesson learned from this shared task is regarding the compilation of group F (English) data.
Researchers, including us, often rely on previously annotated meta-data which sometimes may contain
inaccurate information and errors. Corpus collection for this purpose should be thoroughly checked
(manually if possible). The issues with the group F might have discouraged some of the participants to
continue in the shared task (particularly those who were interested only in the discrimination of English
varieties).
11
UniMelb-NLP experimented different methods in their 6 runs. In this report we commented on the algorithm that achieved
the best performance.
65
6.1 Future Perspectives
The shared task was a very fruitful and positive experience for the organizers. We would like to organize
a second edition of the shared task containing, for example, new language groups for which we could
not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and
Flemish or the varieties of French and German which could not be included in the DSL shared task due
to the lack of available data.
The DSL corpus collection is freely available and can be used as a gold standard for language iden-
tification or to train algorithms for other NLP tasks involving similar languages. We would like to use
the dataset to investigate, for example, lexical variation between similar languages and varieties as pro-
posed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data
as discussed in Anstein (2013).
At present, we are investigating the influence of the length of texts in the discrimination of similar
languages. It is a well known fact that the longer texts are, the more likely they are to contain features
that allow algorithms to identify their language. However, this variable was not explored within the
scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another
direction that our work may take is the linguistic analysis of the most informative features in classification
as was done recently by Diwersy et al. (2014).
Acknowledgements
The authors would like to thank all participants of the DSL shared task for their comments and sugges-
tions throughout the organization of this shared task. We would also like to thank Joel Tetreault and
Binyam Gebrekidan Gebre for their valuable feedback on this report.
References
Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a
semi-automatic system for German. Ph.D. thesis, University of Stuttgart.
Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In
Proceedings of Australasian Language Technology Association Workshop, pages 4?7.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Educational Testing Service.
Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th In-
ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI
8082), pages 519?526, Pilsen, Czech Republic. Springer.
William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analy-
sis and Information Retrieval (SDAIR-94).
Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press.
Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study
of language variation. Linguistic Variation in Text and Speech, within and across Languages.
Cyril Goutte, Serge L?eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In
Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),
Dublin, Ireland.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr?esentation et
r?esultats du d?efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ?et?e ?ecrit? Actes du sixi`eme
D
?
Efi Fouille de Textes.
Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on top-
bag-of-word similarity. In Proceedings of PACLIC 2008, pages 404?410.
66
Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with
groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,
Varieties and Dialects (VarDial), Dublin, Ireland.
Nikola Ljube?si?c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Meeting of the ACL.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Aus-
tralasian Language Tchnology Workshop, pages 5?15.
Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods
and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools
to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16:469?491.
Jordi Porta and Jos?e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar lan-
guages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial), Dublin, Ireland.
Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop
on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Bali Ranaivo-Malanc?on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2:126?134.
Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between
European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in
Cognitive Sociolinguistics.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,
Atlanta, GA, USA, June. Association for Computational Linguistics.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619?2634, Mumbai, India.
Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37?41, Budapest, Hungary.
67
