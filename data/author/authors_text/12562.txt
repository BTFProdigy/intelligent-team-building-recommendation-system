Coling 2010: Poster Volume, pages 153?161,
Beijing, August 2010
Acquisition of Unknown Word Paradigms for Large-Scale Grammars
Kostadin Cholakov
University of Groningen
The Netherlands
k.cholakov@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
g.j.m.van.noord@rug.nl
Abstract
Unknown words are a major issue for
large-scale grammars of natural language.
We propose a machine learning based al-
gorithm for acquiring lexical entries for
all forms in the paradigm of a given un-
known word. The main advantages of our
method are the usage of word paradigms
to obtain valuable morphological knowl-
edge, the consideration of different con-
texts which the unknown word and all
members of its paradigm occur in and
the employment of a full-blown syntactic
parser and the grammar we want to im-
prove to analyse these contexts and pro-
vide elaborate syntactic constraints. We
test our algorithm on a large-scale gram-
mar of Dutch and show that its application
leads to an improved parsing accuracy.
1 Introduction
In this paper, we present an efficient machine
learning based method for automated lexical ac-
quisition (LA) which improves the performance
of large-scale computational grammars on real-
life tasks.
Our approach has three main advantages which
distinguish it from other methods applied to the
same task. First, it enables the acquisition of the
whole paradigm of a given unknown word while
other approaches are only concerned with the par-
ticular word form encountered in the data sub-
ject to LA. Second, we analyse different contexts
which the unknown word occurs in. Third, the
analysis of these contexts is provided by a full-
blown syntactic parser and the grammar we aim
to improve which gives the grammar the opportu-
nity to participate directly in the LA process.
Our method achieves an F-measure of 84.6%
on unknown words in experiments with the wide-
coverage Alpino grammar (van Noord, 2006) of
Dutch. The integration of this method in the
parser leads to a 4.2% error reduction in terms of
labelled dependencies.
To predict a lexical entry for a given unknown
word, we take into account two factors? its mor-
phology and the syntactic constraints imposed by
its context. As for the former, the acquisition of
the whole paradigm provides us with a valuable
source of morphological information. If we were
to deal with only one form of the unknown word,
this information would not be accessible.
Further, looking at different contexts of the un-
known word gives us the possibility to work with
linguistically diverse data and to incorporate more
syntactic information into the LA process. Cases
where this is particularly important include mor-
phologically ambiguous words and verbs which
subcategorize for various types of syntactic argu-
ments. We also consider contexts of the other
members of the paradigm of the unknown word
in order to increase the amount of linguistic data
our method has access to.
Finally, the usage of a full-blown syntactic
parser and the grammar we want to acquire lex-
ical entries for has two advantages. First, LA
can benefit from the high-quality analyses such
a parser produces and the elaborate syntactic in-
formation they provide. Second, this information
comes directly from the grammar, thus allowing
the LA process to make predictions based on what
the grammar considers to be best suited for it.
153
The remainder of the paper is organised as fol-
lows. Section 2 describes the basic steps in our
LA algorithm. Section 3 presents initial exper-
iments conducted with Alpino and shows that
the main problems our LA method encounters
are the acquisition of morphologically ambigu-
ous words, the learning of the proper subcate-
gorization frames for verbs and the acquisition
of particular types of adjectives. In Section 4
we make extensive use of the paradigms of the
unknown words to develop specific solutions for
these problems. Section 5 describes experiments
with our LA method applied to a set of real un-
known words. Section 6 provides a comparison
between our approach and work previously done
on LA. This section also discusses the application
of our method to other systems and languages.
2 Basic Algorithm
The Alpino wide-coverage dependency parser is
based on a large stochastic attribute value gram-
mar. The grammar takes a ?constructional? ap-
proach, with rich lexical representations stored in
the lexicon and a large number of detailed, con-
struction specific rules (about 800). Currently, the
lexicon contains about 100K lexical entries and
a list of about 200K named entities. Each word
is assigned one or more lexical types. For ex-
ample, the verb amuseert (to amuse) is assigned
two lexical types? verb(hebben,sg3,intransitive)
and verb(hebben,sg3,transitive)? because it can
be used either transitively or intransitively. The
other type features indicate that it is a present third
person singular verb and it forms perfect tense
with the auxiliary verb hebben.
The goal of our LA method is to assign the cor-
rect lexical type(s) to a given unknown word. The
method takes into account only open-class lexical
types: nouns, adjectives and verbs, under the as-
sumption that the grammar is already able to han-
dle all closed-class cases. We call the types con-
sidered by our method universal types. The adjec-
tives can be used as adverbs in Dutch and thus, we
do not consider the latter to be an open class.
We employ a ME-based classifier which, for
some unknown word, takes various morphological
and syntactic features as input and outputs lexical
types. The probability of a lexical type t, given an
unknown word and its context c is:
(1) p(t|c) = exp(
?
i ?ifi(t,c))?
t??T exp(
?
i ?ifi(t?,c))
where fi(t, c) may encode arbitrary characteris-
tics of the context and < ?1,?2, ... > can be eval-
uated by maximising the pseudo-likelihood on a
training corpus (Malouf, 2002).
Table 1 shows the features for the noun in-
spraakprocedures (consultation procedures). Row
(i) contains 4 separate features derived from the
prefix of the word and 4 other suffix features are
given in row (ii). The two features in rows (iii)
and (iv) indicate whether the word starts with a
particle and if it contains a hyphen, respectively.
Another source of morphological features is the
paradigm of the unknown word which provides
information that is otherwise inaccessible. For ex-
ample, in Dutch, neuter nouns always take the het
definite article while all other noun forms are used
with the de article. Since the article is distinguish-
able only in the singular noun form, the correct
article of a word, assigned a plural noun type, can
be determined if we know its singular form.
We adopt the method presented in Cholakov
and van Noord (2009) where a finite state mor-
phology is applied to generate the paradigm(s) of
a given word. The morphology does not have ac-
cess to any additional linguistic information and
thus, it generates all possible paradigms allowed
by the word structure. Then, the number of
search hits Yahoo returns for each form in a given
paradigm is combined with some simple heuris-
tics to determine the correct paradigm(s).
However, we make some modifications to this
method because it deals only with regular mor-
phological phenomena. Though all typical irreg-
ularities are included in the Alpino lexicon, there
are cases of irregular verbs composed with parti-
cles which are not listed there. One such example
is the irregular verb meevliegen (to fly with some-
one) for which no paradigm would be generated.
To avoid this, we use a list of common parti-
cles to strip off any particle from a given unknown
word. Once we have removed a particle, we check
if what is left from the word is listed in the lexicon
as a verb (e.g. vliegen in the case of meevliegen).
If so, we extract all members of its paradigm from
154
Features
i) i, in, ins, insp
ii) s, es, res, ures
iii) particle yes #in this case in
iv) hyphen no
v) noun?de,pl?
vi) noun(de,count,pl), tmp noun(de,count,sg)
vii) noun(de), noun(count), noun(pl), tmp noun(de)
tmp noun(count), tmp noun(sg)
Table 1: Features for inspraakprocedures
the lexicon and use them to build the paradigm of
the unknown word. All forms are validated by us-
ing the same web-based heuristics as in the origi-
nal model of Cholakov and van Noord (2009).
A single paradigm is generated for in-
spraakprocedures indicating that this word is a
plural de noun. This information is explicitly used
as a feature in the classifier which is shown in row
(v) of Table 1.
Next, we obtain syntactic features for in-
spraakprocedures by extracting a number of sen-
tences which it occurs in from large corpora or
Internet. These sentences are parsed with a differ-
ent ?mode? of Alpino where this word is assigned
all universal types, i.e. it is treated as being maxi-
mally ambiguous. For each sentence only the best
parse is preserved. Then, the lexical type that has
been assigned to inspraakprocedures in this parse
is stored. During parsing, Alpino?s POS tagger
(Prins and van Noord, 2001) keeps filtering im-
plausible type combinations. For example, if a de-
terminer occurs before the unknown word, all verb
types are typically not taken into consideration.
This heavily reduces the computational overload
and makes parsing with universal types computa-
tionally feasible. When all sentences have been
parsed, a list can be drawn up with the types that
have been used and their frequency:
(2) noun(de,count,pl) 78
tmp noun(de,count,sg) 7
tmp noun(het,count,pl) 6
proper name(pl,?PER?) 5
proper name(pl,?ORG?) 3
verb(hebben,pl,vp) 1
The lexical types assigned to inspraakprocedures
in at least 80% of the parses are used as features
in the classifier. These are the two features in row
(vi) of Table 1. Further, as illustrated in row (vii),
each attribute of the considered types is also taken
as a separate feature. By doing this, we let the
grammar decide which lexical type is best suited
for a given unknown word. This is a new and ef-
fective way to include the syntactic constraints of
the context in the LA process.
However, for the parsing method to work prop-
erly, the disambiguation model of the parser needs
to be adapted. The model heavily relies on the
lexicon and it has learnt preferences how to parse
certain phrases. For example, it has learnt a pref-
erence to parse prepositional phrases as verb com-
plements, if the verb includes such a subcatego-
rization frame. This is problematic when parsing
with universal types. If the unknown word is a
verb and it occurs together with a PP, it would al-
ways get analysed as a verb which subcategorizes
for a PP.
To avoid this, the disambiguation model is re-
trained on a specific set of sentences meant to
make it more robust to input containing many un-
known words. We have selected words with low
frequency in large corpora and removed them tem-
porarily from the Alpino lexicon. Less frequent
words are typically not listed in the lexicon and
the selected words are meant to simulate their be-
haviour. Then, all sentences from the Alpino tree-
bank which contain these words are extracted and
used to retrain the disambiguation model.
3 Initial Experiments and Evaluation
To evaluate the performance of the classifier, we
conduct an experiment with a target type inven-
tory of 611 universal types. A type is considered
universal only if it is assigned to at least 15 dis-
tinct words occurring in large Dutch newspaper
corpora (?16M sentences) automatically parsed
with Alpino.
In order to train the classifier, 2000 words are
temporarily removed from the Alpino lexicon.
The same is done for another 500 words which
are used as a test set. All words have between
50 and 100 occurrences in the corpora. This se-
lection is again meant to simulate the behaviour
of unknown words. Experiments with a minimum
lower than 50 occurrences have shown that this is
a reasonable threshold to filter out typos, words
written together, etc.
155
The classifier yields a probability score for each
predicted type. Since a given unknown word can
have more than one correct type, we want to pre-
dict multiple types. However, the least frequent
types, accounting together for less than 5% of
probability mass, are discarded.
We evaluate the results in terms of precision
and recall. Precision indicates how many types
found by the method are correct and recall indi-
cates how many of the lexical types of a given
word are actually found. The presented results are
the average precision and recall for the 500 test
words.
Additionally, there are three baseline methods:
? Naive? each unknown word is assigned
the most frequent type in the lexicon:
noun(de,count,sg)
? POS tagger? the unknown word is given the
type most frequently assigned by the Alpino
POS tagger in the parsing stage
? Alpino? the unknown word is assigned the
most frequently used type in the parsing
stage
The overall results are given in Table 2. Table 3
shows the results for each POS in our model.
Model Precision(%) Recall(%) F-measure(%)
Naive 19.60 18.77 19.17
POS tagger 30 26.21 27.98
Alpino 44.60 37.59 40.80
Our model 86.59 78.62 82.41
Table 2: Overall experiment results
POS Precision(%) Recall(%) F-measure(%)
Nouns 93.83 88.61 91.15
Adjectives 75.50 73.12 74.29
Verbs 77.32 55.37 64.53
Table 3: Detailed results for our model
Our LA method clearly improves upon the
baselines. However, as we see in Table 3, adjec-
tives and especially verbs remain difficult to pre-
dict.
The problems with the former are due to the fact
that Alpino employs a rather complicated adjec-
tive system. The classifier has difficulties distin-
guishing between 3 kinds of adjectives: i) adjec-
tives which can attach to and modify verbs and
verbal phrases (VPs) (3-a), ii) adjectives which
can attach to verbs and VPs but modify one of
the complements of the verb, typically the sub-
ject (3-b) and iii) adjectives which cannot attach
to verbs and VPs (3-c).
(3) a. De
DET
hardloper
runner
loopt
walks
mooi.
nice
?The runner runs nicely = The runner has a
good running technique?
b. Hij
he
loopt
walks
dronken
drunk
naar
to
huis.
home
?He walks home drunk = He is walking home
while being drunk?
c. *Hij
he
loopt
walks
nederlandstalig.
Dutch speaking
?He walks Dutch speaking.?
Each of these is marked by a special attribute in
the lexical type definitions? adv, padv and non-
adv, respectively. Since all three of them are seen
in ?typical? adjectival contexts where they modify
nouns, it is hard for the classifier to make a distinc-
tion. The predictions appear to be arbitrary and
there are many cases where the unknown word is
classified both as a nonadv and an adv adjective. It
is even more difficult to distinguish between padv
and adv adjectives since this is a solely semantic
distinction.
The main issue with verbs is the prediction of
the correct subcategorization frame. The classifier
tends to predict mostly transitive and intransitive
verb types. As a result, it either fails to capture in-
frequent frames which decreases the recall or, in
cases where it is very uncertain what to predict, it
assigns a lot of types that differ only in the subcat
frame, thus damaging the precision. For example,
onderschrijf (?to agree with?) has 2 correct sub-
cat frames but receives 8 predictions which differ
only in the subcat features.
One last issue is the prediction, in some rare
cases, of types of the wrong POS for morpholog-
ically ambiguous words. In most of these cases
adjectives are wrongly assigned a past partici-
ple type but also some nouns receive verb pre-
dictions. For instance, OESO-landen (?countries
of the OESO organisation?) has one correct noun
type but because landen is also the Dutch verb for
?to land? the classifier wrongly assigns a verb type
as well.
156
4 Improving LA
4.1 POS Correction
Since the vast majority of wrong POS predictions
has to do with the assignment of incorrect verb
types, we decided to explicitly use the generated
verb paradigms as a filtering mechanism. For each
word which is assigned a verb type, we check if
there is a verb paradigm generated for it. If not, all
verb types predicted for the word are discarded.
In very rare cases a word is assigned only verb
types and therefore, it ends up with no predictions.
For such words, we examine the ranked list of pre-
dicted types yielded by the classifier and the word
receives the non-verb lexical type with the high-
est probability score. If this type happens to be
an adjective one, we first check whether there is
an adjective paradigm generated for the word in
question. If not, the word gets the noun type with
the highest probability score.
The same procedure is also applied to all words
which are assigned an adjective type. However,
it is not used for words predicted to be nouns be-
cause the classifier is already very good at predict-
ing nouns. Further, the generated noun paradigms
are not reliable enough to be a filtering mechanism
because there are mass nouns with no plural forms
and thus with no paradigms generated.
Another modification we make to the classifier
output has to do with the fact that past participles
(psp) in Dutch can also be used as adjectives. This
systematic ambiguity, however, is not treated as
such in Alpino. Each psp should also have a sep-
arate adjective lexical entry but this is not always
the case. That is why, in some cases, the classifier
fails to capture the adjective type of a given psp.
To account for it, all words predicted to be past
participles but not adjectives are assigned two ad-
ditional adjective types? one with the nonadv and
one with the adv feature. For reasons explained
later on, a type with the padv feature is not added.
After the application of these techniques, all
cases of words wrongly predicted to be verbs or
adjectives have been eliminated.
4.2 Guessing Subcategorization Frames
Our next step is to guess the correct subcatego-
rization feature for verbs. Learning the proper
subcat frame is well studied (Brent, 1993; Man-
ning, 1993; Briscoe and Caroll, 1997; Kinyon and
Prolo, 2002; O?Donovan et al, 2005). Most of
the work follows the ?classical? Briscoe and Caroll
(1997) approach where the verb and the subcate-
gorized complements are extracted from the out-
put analyses of a probabilistic parser and stored as
syntactic patterns. Further, some statistical tech-
niques are applied to select the most probable
frames out of the proposed syntactic patterns.
Following the observations made in Korho-
nen et al (2000), Lapata (1999) and Messiant
(2008), we employ a maximum likelihood es-
timate (MLE) from observed relative frequen-
cies with an empirical threshold to filter out low
probability frames. For each word predicted to
be a verb, we look up the verb types assigned
to it during the parsing with universal types.
Then, the MLE for each subcat frame is deter-
mined and only frames with MLE of 0.2 and
above are considered. For example, jammert
(to moan.3SG.PRES) is assigned a single type?
verb(hebben,sg3,intransitive). However, the cor-
rect subcat features for it are intransitive and sbar.
Here is the list of all verb types assigned to jam-
mert during the parsing with universal types:
(4) verb(hebben,sg3,intransitive) 48
verb(hebben,sg3,transitive) 15
verb(hebben,past(sg),np sbar) 3
verb(hebben,past(sg),tr sbar) 3
verb(zijn,sg3,intransitive) 2
verb(hebben,past(sg),ld pp) 2
verb(hebben,sg3,sbar) 1
The MLE for the intransitive subcat feature is 0.68
and for the transitive one? 0.2. All previously pre-
dicted verb types are discarded and each consid-
ered subcat frame is used to create a new lexi-
cal type. That is how jammert gets two types at
the end? the correct verb(hebben,sg3,intransitive)
and the incorrect verb(hebben,sg3,transitive). The
sbar frame is wrongly discarded.
To avoid such cases, the generated word
paradigms are used to increase the number of con-
texts observed for a given verb. Up to 200 sen-
tences are extracted for each form in the paradigm
of a given word predicted to be a verb. These sen-
tences are again parsed with the universal types
and then, the MLE for each subcat frame is recal-
157
culated.
We evaluated the performance of our MLE-
based method on the 116 test words predicted to
be verbs. We extracted the subcat features from
their type definitions in the Alpino lexicon to cre-
ate a gold standard of subcat frames. Addition-
ally, we developed two baseline methods: i) all
frames assigned during parsing are considered and
ii) each verb is taken to be both transitive and in-
transitive. Since most verbs have both or one of
these frames, the purpose of the second baseline is
to see if there is a simpler solution to the problem
of finding the correct subcat frame. The results
are given in Table 4.
Model Precision(%) Recall(%) F-measure(%)
all frames 16.76 94.34 28.46
tr./intr. 62.29 69.17 65.55
our model 85.82 67.28 75.43
Table 4: Subcat frames guessing results
Our method significantly outperforms both
baselines. It is able to correctly identify the transi-
tive and/or the intransitive frames. Since they are
the most frequent ones in the test data, this boosts
up the precision. However, the method is also able
to capture other, less frequent subcat frames. For
example, after parsing the additional sentences for
jammert, the sbar frame had enough occurrences
to get above the threshold. The MLE for the tran-
sitive one, on the other hand, fell below 0.2 and it
was correctly discarded.
4.3 Guessing Adjective Types
We follow a similar approach for finding the cor-
rect adjective type. It should be noted that the
distinction among nonadv, adv and padv does
not exist for every adjective form. Most ad-
jectives in Dutch get an -e suffix when used
attributively? de mooie/mooiere/mooiste jongen
(the nice/nicer/nicest boy). Since these inflected
forms can only occur before nouns, the distinction
we are dealing with is not relevant for them. Thus
we are only interested in the noninflected base,
comparative and superlative adjective forms.
One of the possible output formats of Alpino
is dependency triples. Here is the output for the
sentence in (3-a):
(5) verb:loop|hd/su|noun:hardloper
noun:hardloper|hd/det|det:de
verb:loop|hd/mod|adj:mooi
verb:loop|?/?|punct:.
Each line is a single dependency triple. The line
contains three fields separated by the ?|? character.
The first field contains the root of the head word
and its POS, the second field indicates the type of
the dependency relation and the third one contains
the root of the dependent word and its POS. The
third line in (5) shows that the adjective mooi is a
modifier of the head, in this case the verb loopt.
Such a dependency relation indicates that this ad-
jective can modify a verb and therefore, it belongs
to the adv type.
As already mentioned, padv adjectives cannot
be distinguished from the ones of the adv kind.
That is why, if the classifier has decided to assign
a padv type to a given unknown word, we discard
all other adjective types assigned to it (if any) and
do not apply the technique described below to this
word.
For each of the 59 words assigned an non-
inflected adjective type after the POS correction
stage, we extract up to 200 sentences for all non-
inflected forms in its paradigm. These sentences
are parsed with Alpino and the universal types and
the output is dependency triples. All triples where
the unknown word occurs as a dependent word in
a head modifier dependency (hd/mod, as shown in
(5)) and its POS is adjective are extracted from the
parse output. We calculate the MLE of the cases
where the head word is a verb, i.e. where the un-
known word modifies a verb. If the MLE is 0.05
or larger, the word is assigned an adv lexical type.
For example, the classifier correctly identifies
the word doortimmerd (solid) as being of the ad-
jective(no e(nonadv)) type but it also predicts the
adjective(no e(adv))1 type for it. Since we have
not found enough sentences where this word mod-
ifies a verb, the latter type is correctly discarded.
Our technique produced correct results for 53 out
of the 59 adjectives processed.
1The no e type attribute denotes a noninflected base ad-
jective form.
158
4.4 Improved Results and Discussion
Table 5 presents the results obtained after apply-
ing the improvement techniques described in this
section to the output of the classifier (the ?Model
2? rows). For comparison, we also give the re-
sults from Table 3 again (the ?Model 1? rows).
The numbers for the nouns happen to remain un-
changed and that is why they are not shown in Ta-
ble 5.
POS Models Prec.(%) Rec.(%) F-meas.(%)
Adj Model 1 75.50 73.12 74.29Model 2 85.16 80.16 82.58
Verbs Model 1 77.32 55.37 64.53Model 2 80.56 56.24 66.24
Overall Model 1 86.59 78.62 82.41Model 2 89.08 80.52 84.58
Table 5: Improved results
The automatic addition of adjective types for
past participles improved significantly the recall
for adjectives and our method for choosing be-
tween adv and nonadv types caused a 10% in-
crease in precision.
However, these procedures also revealed some
incomplete lexical entries in Alpino. For example,
there are two past participles not listed as adjec-
tives in the lexicon though they should be. Thus
when our method correctly assigned them adjec-
tive types, it got punished since these types were
not in the gold standard.
We see in Table 5 that the increase in precision
for the verbs is small and recall remains practi-
cally unchanged. The unimproved recall shows
that we have not gained much from the subcat
frame heuristics. Even when the number of the
observed sentences was increased, less frequent
frames often remained unrecognisable from the
noise in the parsed data. This could be seen as
a proof that in the vast majority of cases verbs
are used transitively and/or intransitively. Since
the MLE method we employ proved to be good at
recognising these two frames and differentiating
between them, we have decided to continue using
it.
The overall F-score improved by only 2% be-
cause the modified verb and adjective predictions
are less than 30% of the total predictions made by
the classifier.
5 Experiment with Real Unknown
Words
To investigate whether the proposed LA method
is also beneficial for the parser, we observe how
parsing accuracy changes when the method is em-
ployed. Accuracy in Alpino is measured in terms
of labelled dependencies.
We have conducted an experiment with a test
set of 300 sentences which contain 188 real un-
known words. The sentences have been randomly
selected from the manually annotated LASSY
corpus (van Noord, 2009) which contains text
from various domains. The average sentence
length is 26.54 tokens.
The results are given in Table 6. The standard
Alpino model uses its guesser to assign types to
the unknown words. Model 1 employs the trained
ME-based classifier to predict lexical entries for
the unknown words offline and then uses them
during parsing. Model 2 uses lexical entries modi-
fied by applying the methods described in Section
4 to the output of the classifier (Model 1).
Model Accuracy (%) msec/sentence
Alpino 88.77 8658
Model 1 89.06 8772
Model 2 89.24 8906
Table 6: Results with real unknown words
Our LA system as a whole shows an error re-
duction rate of more than 4% with parse times re-
maining similar to those of the standard Alpino
version. It should also be noted that though much
of the unknown words are generally nouns, we see
from the results that it makes sense to also employ
the methods for improving the predictions for the
other POS types. A wrong verb or even adjec-
tive prediction can cause much more damage to
the analysis than a wrong noun one.
These results illustrate that the integration of
our method in the parser can improve its perfor-
mance on real-life data.
6 Discussion
6.1 Comparison to Previous Work
The performance of the LA method we presented
in this paper can be compared to the performance
159
of a number of other approaches previously ap-
plied to the same task.
Baldwin (2005) uses a set of binary classifiers
to learn lexical entries for a large-scale gram-
mar of English (ERG; (Copestake and Flickinger,
2000)). The main disadvantage of the method is
that it uses information obtained from secondary
language resources? POS taggers, chunkers, etc.
Therefore, the grammar takes no part in the LA
process and the method acquires lexical entries
based on incomplete linguistic information pro-
vided by the various resources. The highest F-
measure (about 65%) is achieved by using fea-
tures from a chunker but it is still 20% lower than
the results we report here. Further, no evalua-
tion is done on how the method affects the per-
formance of the ERG when the grammar is used
for parsing.
Zhang and Kordoni (2006) and Cholakov et
al. (2008), on the other hand, include features
from the grammar in a maximum entropy (ME)
classifier to predict new lexical entries for the
ERG and a large German grammar (GG; (Crys-
mann, 2003)), respectively. The development data
for this method consist of linguistically annotated
sentences from treebanks and the grammar fea-
tures used in the classifier are derived from this
annotation. However, when the method is applied
to open-text unannotated data, the grammar fea-
tures are replaced with POS tags. Therefore, the
grammar is no longer directly involved in the LA
process which affects the quality of the predic-
tions. Evaluation on sentences containing real un-
known words shows improvement of the coverage
for the GG when LA is employed but the accuracy
decreases by 2%. Such evaluation has not been
done for the ERG. The results on the development
data are not comparable with ours because evalu-
ation is done only in terms of precision while we
are also able to measure recall.
Statistical LA has previously been applied to
Alpino as well (van de Cruys, 2006). However,
his method employs less morphosyntactic features
in comparison to our approach and does not make
use of word paradigms. Further, though experi-
ments on development data are performed on a
smaller scale, the results in terms of F-measure are
10% lower than those reported in our case study.
Experiments with real unknown words have not
been performed.
Other, non-statistical LA methods also exist.
Cussens and Pulman (2000) describe a symbolic
approach which employs inductive logic program-
ming and Barg and Walther (1998) and Fouvry
(2003) follow a unification-based approach. How-
ever, the generated lexical entries might be both
too general or too specific and it is doubtful if
these methods can be used on a large scale. They
have not been applied to broad-coverage gram-
mars and no evaluation is provided.
6.2 Application to Other Systems and
Languages
We stress the fact that the experiments with
Alpino represent only a case study. The proposed
LA method can be applied to other computational
grammars and languages providing that the fol-
lowing conditions are fulfilled.
First, words have to be mapped onto some fi-
nite set of labels of which a subset of open-class
(universal) labels has to be selected. This subset
represents the labels which the ME-based classi-
fier can predict for unknown words. Second, a
(large) corpus has to be available, so that various
sentences in which a given unknown word occurs
can be extracted. This is crucial for obtaining dif-
ferent contexts in which this word is found.
Next, we need a parser to analyse the extracted
sentences which allows for the syntactic con-
straints imposed by these contexts to be included
in the prediction process.
Finally, as for the paradigm generation, the idea
of combining a finite state morphology and web
heuristics is general enough to be implemented
for different languages. It is also important to
note that the classifier allows for arbitrary com-
binations of features and therefore, a researcher is
free to include any (language-specific) features he
or she considers useful for performing LA.
We have already started investigating the appli-
cability of our LA method to large-scale gram-
mars of German and French and the initial experi-
ments and results we have obtained are promising.
160
References
Baldwin, Tim. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of
the ACL-SIGLEX 2005 Workshop on Deep Lexical
Acquisition, Ann Arbor, USA.
Barg, Petra and Markus Walther. 1998. Processing un-
known words in HPSG. In Proceedings of the 36th
Conference of the ACL, Montreal, Quebec, Canada.
Brent, Michael R. 1993. From grammar to lexicon:
unsupervised learning of lexical syntax. Computa-
tional Linguistics, 19(2):243?262.
Briscoe, Ted and John Caroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied Nat-
ural Language Processing, Washington, DC.
Cholakov, Kostadin and Gertjan van Noord. 2009.
Combining finite state and corpus-based techniques
for unknown word prediction. In Proceedings of the
7th Recent Advances in Natural Language Process-
ing (RANLP) conference, Borovets, Bulgaria.
Cholakov, Kostadin, Valia Kordoni, and Yi Zhang.
2008. Towards domain-independent deep linguistic
processing: Ensuring portability and re-usability of
lexicalised grammars. In Proceedings of COLING
2008 Workshop on Grammar Engineering Across
Frameworks (GEAF08), Manchester, UK.
Copestake, Ann and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resource and Evaluation (LREC
2000), Athens, Greece.
Crysmann, Berthold. 2003. On the efficient imple-
mentation of German verb placement in HPSG. In
Proceedings of RANLP 2003, Borovets, Bulgaria.
Cussens, James and Stephen Pulman. 2000. Incor-
porating linguistic constraints into inductive logic
programming. In Proceedings of the Fourth Con-
ference on Computational Natural Language Learn-
ing.
Fouvry, Frederik. 2003. Lexicon acquisition with a
large-coverage unification-based grammar. In Com-
panion to the 10th Conference of EACL, pages 87?
90, Budapest, Hungary.
Kinyon, Alexandra and Carlos A Prolo. 2002. Iden-
tifying verb arguments and their syntactic function
in the Penn Treebank. In Proceedings of the 3rd In-
ternational Conference on Language Resource and
Evaluation (LREC 2002), Las Palmas de Gran Ca-
naria, Spain.
Korhonen, Anna, Genevieve Gorell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcatego-
rization frame acquisition. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, Hong Kong, China.
Lapata, Mirella. 1999. Acquiring lexical generaliza-
tions from corpora. A case study for diathesis alter-
nations. In Proceedings of the 37th Annual Meeting
of ACL, Maryland, USA.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th conference on Natural Language
Learning (CoNLL-2002), pages 49?55, Taipei, Tai-
wan.
Manning, Christopher. 1993. Automatic acquisition
of a large subcategorization dictionary from cor-
pora. In Proceedings of the 31st Annual Meeting
of ACL, Columbus, OH.
Messiant, Cedric. 2008. A subcategorization acquisi-
tion system for French verbs. In Proceedings of the
ACL 2008 Student Research Workshop, Columbus,
OH.
O?Donovan, Ruth, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the Penn-II and Penn-III Treebanks. Computational
Linguistics, 31(3):329?365.
Prins, Robbert and Gertjan van Noord. 2001. Un-
supervised POS-tagging improves parcing accuracy
and parsing efficiency. In Proceedings of IWPT,
Beijing, China.
van de Cruys, Tim. 2006. Automatically extending the
lexicon for parsing. In Huitnik, Janneje and Sophia
Katrenko, editors, Proceedings of the Eleventh ESS-
LLI Student Session, pages 180?189.
van Noord, Gertjan. 2006. At last parsing is now oper-
ational. In Proceedings of TALN, Leuven, Belgium.
van Noord, Gertjan. 2009. Huge parsed corpora in
LASSY. In Proceedings of the Seventh Interna-
tional Workshop on Treebanks and Linguistic The-
ories (TLT 7), Groningen, The Netherlands.
Zhang, Yi and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open text processing.
In Proceedings of the Fifth International Confer-
ence on Language Resourses and Evaluation (LREC
2006), Genoa, Italy.
161
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 902?912,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Using Unknown Word Techniques To Learn Known Words
Kostadin Cholakov
University of Groningen
The Netherlands
k.cholakov@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
g.j.m.van.noord@rug.nl
Abstract
Unknown words are a hindrance to the perfor-
mance of hand-crafted computational gram-
mars of natural language. However, words
with incomplete and incorrect lexical entries
pose an even bigger problem because they can
be the cause of a parsing failure despite being
listed in the lexicon of the grammar. Such lex-
ical entries are hard to detect and even harder
to correct.
We employ an error miner to pinpoint words
with problematic lexical entries. An auto-
mated lexical acquisition technique is then
used to learn new entries for those words
which allows the grammar to parse previously
uncovered sentences successfully.
We test our method on a large-scale grammar
of Dutch and a set of sentences for which this
grammar fails to produce a parse. The appli-
cation of the method enables the grammar to
cover 83.76% of those sentences with an ac-
curacy of 86.15%.
1 Introduction
In this paper, we present an automated two-phase
method for treating incomplete or incorrect lexical
entries in the lexicons of large-scale computational
grammars. The performance of our approach is
tested in a case study with the wide-coverage Alpino
grammar (van Noord, 2006) of Dutch. When ap-
plied to real test sentences previously not covered
by Alpino, the method causes a parsing coverage of
83.76% and the accuracy of the delivered analyses
is 86.15%.
The main advantage of our approach is the suc-
cessful combination of efficient error mining and
lexical acquisition techniques. In the first phase, er-
ror mining pinpoints words which are listed in the
lexicon of a given grammar but which nevertheless
often lead to a parsing failure. This indicates that the
current lexical entry for such a word is either wrong
or incomplete and that one or more correct entries
for this word are missing from the lexicon. Our idea
is to treat the word as if it was unknown and, in the
second phase, to employ lexical acquisition (LA) to
learn the missing correct entries.
In the case study presented here, we employ the
iterative error miner of de Kok et al (2009). Since
it has to be run on a large parsed corpus, we have
parsed the Flemish Mediargus corpus (?1.5 billion
words) with Alpino. The reason for this choice is
the relatively large lexical difference between stan-
dard Dutch and Flemish. This increases the chance
to encounter words which are used in Flemish in a
way not handled by Alpino yet.
For example, the word afwater (to drain) is listed
as a first person singular present verb in the Alpino
lexicon. However, the error miner identifies this
word as the reason for the parsing failure of 9 sen-
tences. A manual examination reveals that the word
is used as a neuter noun in these cases? het afwater
(the drainage). Since there is no noun entry in the
lexicon, Alpino was not able to produce full-span
analyses.
After the error miner identifies afwater as a prob-
lematic word, we employ our machine learning
based LA method presented in Cholakov and van
Noord (2010) to learn new entries for this word.
This method has already been successfully applied
to the task of learning lexical entries for unknown
words and, as the error miner, it can be used ?out of
the box?. LA correctly predicts a neuter noun en-
902
try for afwater and the addition of this entry to the
lexicon enables Alpino to cover the 9 problematic
sentences from the Mediargus corpus.
It should be noted that since our approach cannot
differentiate between incomplete and incorrect en-
tries, no entry in the lexicon is modified. We simply
add the lexical entries which, according to the LA
method, are most suitable for a given problematic
word and assume that, if these entries are correct,
the grammar should be able to cover previously un-
parsable sentences in which the word occurs.
The remainder of the paper is organised as fol-
lows. Section 2 describes the error miner. Section
3 presents the Alpino grammar and parser and the
LA technique we employ. Section 4 describes an
experiment where error mining is performed on the
Mediargus corpus and then, LA is applied to learn
new lexical entries for problematic words. Section
5 discusses the effect which the addition of the new
entries to the lexicon has on the parsing coverage
and accuracy. Section 6 provides a comparison be-
tween our approach and previous work similar in na-
ture. This section also discusses the application of
our method to other systems and languages as well
as some ideas for future research.
2 Error Mining
The error miner of de Kok et al (2009) combines the
strengths of the error mining methods of van Noord
(2004) and Sagot and de la Clergerie (2006). The
idea behind these methods is that grammar errors
lead to the parsing failure of some grammatical sen-
tences. By running the grammar over a large corpus,
the corpus can be split into two subsets? the set of
sentences which received a full-span parse and the
set of sentences failed to parse. Words or n-grams
which occur in the latter set have a suspicion of be-
ing the cause of parsing failures.
van Noord (2004) defines the suspicion of a word
sequence as:
(1) S(wi...wj) =
C(wi...wj |error)
C(wi...wj)
where C(wi...wj) is the number of sentences
which the sequence wi...wj occurs in and
C(wi...wj |error) is the number of occurrences of
the sequence in unparsable sentences.
While this method performs well in identifying
words and n-grams that are unambiguously suspi-
cious, it also assigns incorrectly a high suspicion
to forms which happen to occur often in unparsable
sentences by ?bad luck?. The iterative error mining
algorithm of Sagot and de la Clergerie (2006) tackles
this problem by taking the following into account:
? If a form occurs within parsable sentences, it
becomes less likely for it to be the cause of a
parsing failure.
? The suspicion of a form depends on the suspi-
cions of the other forms in the unparsable sen-
tences it occurs in.
? A form observed in a shorter sentence is ini-
tially more suspicious than a form observed in
a longer one.
However, because of data sparseness problems, this
method is only able to handle unigrams and bigrams.
Another potential problem is the absence of criteria
to determine when to use unigrams and when bi-
grams to represent forms within a given sentence.
Consider the trigram w1, w2, w3 where w2 is the
cause of a parsing failure. In this case, the whole
trigram as well as the bigrams w1, w2 and w2, w3
will become suspicious which would prevent the un-
igram w2 from ?manifesting? itself.
To avoid this problem, de Kok et al (2009) uses
a preprocessor to the iterative miner of Sagot and
de la Clergerie (2006) which iterates through a sen-
tence of unigrams and expands unigrams to longer
n-grams when there is evidence that this is useful. A
unigram w1 is expanded to a bigram w1, w2 if this
bigram is more suspicious than both of its unigrams.
The general algorithm is that the expansion to an n-
gram i...j is allowed when the following two condi-
tions are fulfilled:
(2) S(i...j) > S(i...j ? 1) ? expFactor
S(i...j) > S(i + 1...j) ? expFactor
Within the preprocessor, suspicion is defined as
shown in (1) and the expFactor is a parameter spe-
cially designed to deal with data sparseness.
As the error mining technique of de Kok et al
(2009) successfully overcomes the problems which
903
the other error mining methods we discussed en-
counter, we have chosen to employ this technique
in our experiment.
3 Automated Lexical Acquisition
3.1 The Alpino Grammar and Parser
Since we employ Alpino for the purposes of our case
study, it is convenient to explain the LA method we
have chosen to use in the context of this system.
The Alpino wide-coverage parser is based on a
large stochastic attribute value grammar. The gram-
mar takes a ?constructional? approach, with rich
lexical representations stored in the lexicon and a
large number of detailed, construction specific rules
(about 800).
Currently, the lexicon contains over 100K lexical
entries and a list of about 200K named entities. Each
word is assigned one or more lexical types. For
example, the verb amuseert (to amuse) is assigned
two lexical types? verb(hebben,sg3,intransitive) and
verb(hebben,sg3,transitive)? because it can be used
either transitively or intransitively. The other type
features indicate that it is a present third person sin-
gular verb and it forms perfect tense with the auxil-
iary verb hebben.
3.2 Learning Algorithm
The goal of the LA method we describe Cholakov
and van Noord (2010) is to assign correct lexical
type(s) to a given unknown word.
It takes into account only open-class lexical types:
nouns, adjectives and verbs. The types considered in
the learning process are called universal types1.
For a given word, a maximum entropy (ME)
based classifier takes various morphological and
syntactic features as input and outputs a ranked list
of lexical types. The probability of a lexical type t,
given an unknown word and its context c is:
(3) p(t|c) =
exp(
?
i
?ifi(t,c))?
t??T
exp(
?
i
?ifi(t?,c))
where fi(t, c) may encode arbitrary characteristics
of the context and < ?1,?2, ... > is a weighting
parameter which maximises the entropy and can be
1The adjectives can be used as adverbs in Dutch and thus,
the latter are not considered to be an open class.
Features
i) a, af, afw, afwa
ii) r, er, ter, ater
iii) particle yes #in this case af
iv) hyphen no
v) noun?het,sg?, verb?sg1?
vi) noun(het,count,sg), noun(de,count,pl)
vii) noun(het), noun(count), noun(sg), noun(de)
noun(pl)
Table 1: Features for afwater
evaluated by maximising the pseudo-likelihood on a
training corpus (Malouf, 2002).
Table 1 shows the features for afwater, the word
we discussed in Section 1. Row (i) contains 4 sepa-
rate features derived from the prefix of the word and
4 other suffix features are given in row (ii). The two
features in rows (iii) and (iv) indicate whether the
word starts with a particle and if it contains a hy-
phen, respectively.
Further, the method we describe in Cholakov
and van Noord (2009) is applied to generate the
paradigm(s) of each word in question. This method
uses a finite state morphology to generate possible
paradigm(s) for a given word. The morphology does
not have access to any additional linguistic infor-
mation and thus, it generates all possible paradigms
allowed by the word orthography. Then, the num-
ber of search hits Yahoo returns for each form in
a given paradigm is combined with some simple
heuristics to determine the correct paradigm(s). The
web search heuristics are also able to determine the
correct definite article (de or het) for words with
noun paradigms.
One verb and one noun paradigm are generated
for afwater. In these paradigms, afwater is listed as
a first person singular present verb form and a sin-
gular het noun form, respectively. This information
is explicitly used as features in the classifier which
is shown in row (v) of Table 1.
Next, syntactic features for afwater are obtained
by extracting a number of sentences which it oc-
curs in from large corpora or Internet. These sen-
tences are parsed with a different ?mode? of Alpino
where this word is assigned all universal types, i.e. it
is treated as being maximally ambiguous. For each
sentence only the parse which is considered to be the
best by the Alpino statistical disambiguation model
904
is preserved. Then, the lexical type that has been
assigned to afwater in this parse is stored. During
parsing, Alpino?s POS tagger (Prins and van Noord,
2001) keeps filtering implausible type combinations.
For example, if a determiner occurs before the un-
known word, all verb types are typically not taken
into consideration. This heavily reduces the compu-
tational overload and makes parsing with universal
types computationally feasible.
When all sentences have been parsed, a list can
be drawn up with the types that have been used and
their frequency:
(4) noun(het,count,sg) 54
noun(de,count,pl) 7
tmp noun(het,count,sg) 4
adjective(no e(adv)) 4
proper name(sg,?ORG?) 1
The lexical types assigned to afwater in at least 80%
of the parses are used as features in the classifier.
These are the two features in row (vi) of Table 1.
Further, as illustrated in row (vii), each attribute of
the considered types is also taken as a separate fea-
ture.
After the classifier predicts lexical types for each
word, these predictions are subject to two additional
steps of processing. In the first one, the generated
word paradigms are explicitly used as a filtering
mechanism. When a word is assigned a verb or an
adjective type by the classifier but there is no verb or
adjective paradigm generated for it, all verb or ad-
jective predictions for this word are discarded.
The output of this ?filtering? is further processed
in the second step which deals with the correct
prediction of subcategorization frames for verbs.
Following the observations made in Korhonen et
al. (2000), Lapata (1999) and Messiant (2008),
Cholakov and van Noord (2010) employ a maximum
likelihood estimate (MLE) from observed relative
frequencies with an empirical threshold to filter out
low probability frames.
Since some frames could be very infrequent and
the MLE method may not capture them, the gener-
ated word paradigms are used to increase the num-
ber of contexts observed for a given verb. Addi-
tional sentences are extracted for each form in the
paradigm of a given word predicted to be a verb.
These sentences are again parsed with the universal
types. Then we look up the assigned universal verb
types, calculate the MLE for each subcategorization
frame and filter out frames with MLE below some
empirical threshold.
4 Learning New Lexical Entries
Before we start with the description of the exper-
iment, it is important to note that Alpino is very
robust? essentially, it always produces a parse. If
there is no analysis spanning the whole sentence,
the parser finds all parses for each substring and re-
turns what it considers to be the best sequence of
non-overlapping parses. However, in the context of
this experiment, a sentence will be considered suc-
cessfully parsed only if it receives a full-span anal-
ysis. For the sake of clarity, from now on we shall
use the terms coverage and cover only with regard
to such sentences. The term parsing failure shall re-
fer to a sentence for which Alpino fails to produce a
full-span analysis.
4.1 Error Mining on Mediargus
The first step in our experiment is to perform er-
ror mining on the Mediargus corpus. The corpus
consists of texts from Flemish newspapers from the
period between 1998 and 2007. It contains about
1.5 billion words (?78M sentences). The corpus
has been parsed with Alpino and the parsing results
are fed into the error miner of de Kok et al (2009).
The parser has not produced a full-span analysis for
7.28% of the sentences (?5.7M sentences).
When finished, the error miner stores the results
in a data base containing potentially problematic n-
grams. Each n-gram is linked to its suspicion score
and the sentences which it occurs in and which were
not covered by Alpino.
Before proceeding with LA, however, we should
identify the n-grams which are indicative for a prob-
lem in the lexicon. The first step in this direction
is to extract all unigrams from the data base which
have a suspicion equal to or greater than 0.7 together
with the uncovered sentences they occur in. This
resulted in a list containing 4179 unique unigrams.
Further, we select from this list only those unigrams
which have lexical entries in the Alpino lexicon and
occur in more than 5 sentences with no full-span
905
parse. Sometimes, the error miner might be wrong
about the exact word which causes the parsing fail-
ure for a given sentence. The 5 sentences empiri-
cal threshold is meant to guarantee that the selected
words are systematically causing problems for the
parser.
The result of this selection is 36 unigrams (words)
which occur in a total of 388 uncovered sentences?
an average of 10.78 sentences per word. The small
number of selected words is due to the fact that
most of the problematic 4179 unigrams represent to-
kenization errors (two or more words written as one)
and spelling mistakes which, naturally, are not listed
in the Alpino lexicon. Very few of the 4179 uni-
grams are actual unknown words. Table 2 shows
some of the problematic unigrams and their suspi-
cions.
opVorig 0.898989
GentHoewel 0.89759
Nieuwpoortl 0.897414
SportTijdens 0.897016
DirvenDe 0.896428
mistrap 0.896038
Dwoeurp 0.896013
passerde 0.89568
doorHugo 0.893901
goedkmaken 0.892407
ManneN 0.891539
toegnag 0.891523
Table 2: Problematic unigrams and their suspicions
It can be seen immediately that most of the uni-
grams presented in the table are tokenization errors.
There are also some typos. The unigram passerde
should be written as passeerde, the past singular
verb form of the verb ?to pass? and toegnag is the
misspelled noun toegang (access). The only prob-
lematic unigram with a lexical entry in the Alpino
lexicon is mistrap (misstep, to misstep).
Although the experiment setup yields a small test
set, we employ it because the words in this set repre-
sent ?clear-cut? cases. This allows us to demonstrate
better the effect of our technique.
4.2 Applying Lexical Acquisition
Our assumption is that incomplete or incorrect lex-
ical entries prevented the production of full-span
parses for the 388 sentences in which the 36 prob-
lematic words pinpointed by the error miner oc-
cur. That is why, in the second step of the exper-
iment, these words are temporarily removed from
the Alpino lexicon, i.e. they are treated as unknown
words, and we employ the LA method presented in
the previous section to learn offline new lexical en-
tries for them.
The setup for the learning process is exactly the
same as in Cholakov and van Noord (2010). The set
of universal types consists of 611 types and the ME-
based classifier has been trained on the same set of
2000 words as in Cholakov and van Noord (2010).
Those types predicted by the classifier which ac-
count together for less than 5% of probability mass
are discarded.
In order to increase the number of observed con-
texts for a given word when parsing with the univer-
sal types, up to 100 additional sentences in which the
word occurs are extracted from Internet. However,
when predicting new lexical entries for this word,
we want to take into account only sentences where
it causes a parsing failure. It is in such sentences
where a new lexical entry can be learnt through LA.
For example, the LA method would be able to pre-
dict a noun entry for afwater if it focuses only on
contexts where it has a noun reading, i.e. on sen-
tences not covered by Alpino.
That is why, the sentences we extracted from In-
ternet are first parsed with the standard Alpino con-
figuration. When averaging over the 36 sentence
sets, it turns out that Alpino has been able to cover
10.05% of the sentences. Although we cannot be
sure that the 36 words are the cause of a parsing
failure in each of the uncovered sentences, this low
coverage indicates once more that Alpino has sys-
tematic problems with sentences containing these
words.
Then, the uncovered sentences from Internet to-
gether with the 388 problematic sentences from the
Mediargus corpus are parsed with Alpino and the
universal types. For example, the list of univer-
sal types assigned to afwater in (4) contains mostly
noun types, i.e. the kind of types which are currently
not in the lexicon for this word and which we want
to learn.
The result of the LA process is the prediction of
a total of 102 lexical types, or 2.83 types per word.
This high number is due to the fact that 25 words
receive verb predictions. Since a verb can have vari-
906
ous subcategorization frames, there is one type as-
signed for each frame. For example, inscheppen
(to spoon in(to)) receives 3 types which differ only
in the subcategorization frame? verb(hebben,inf,tr.),
verb(hebben,inf,intr.) and verb(hebben,inf,np np).
However, the infinitive in Dutch is also the
form for plural present and inscheppen correctly
receives 3 more predictions? verb(hebben,pl,tr.),
verb(hebben,pl,intr.) and verb(hebben,pl,np np).
Let us examine the most frequent types of lexicon
errors for the 36 problematic words by looking at
the current Alpino lexical entries for some of these
words and the predictions they receive from the LA
method. The original Alpino entries for 19 of the
25 words predicted to be verbs are a product of a
specific lexical rule in the grammar. Consider the
following sentences:
(5) a. Ik
I
schep
spoon
de
the
soep
soup
in
in
de
the
kom.
bowl
?I spoon the soup into the bowl.?
b. dat
that
ik
I
de
the
soep
soup
de
the
kom
bowl
in
in
schep
spoon
?that I spoon the soup into the bowl?
c. dat
that
ik
I
de
the
soep
soup
de
the
kom
bowl
inschep
in spoon
?that I spoon the soup into the bowl?
We see in (5-b) that the preposition in is used as a
postposition in the relative clause. However, in such
cases, there is linguistic evidence that in behaves as
a separate verb particle. That is why, as shown in
(5-c), people sometimes write in and the verb to-
gether when they occur next to each other in the sen-
tence. To account for this, Alpino employs a special
lexical rule. This rule assigns a certain type of sub-
categorization frame to verbs like inscheppen where
a postposition can be interpreted as a separable par-
ticle. That subcategorization frame requires a noun
phrase (?the soup? in (5-c)) and a locative NP (?the
bowl? in (5-c)).
However, in some cases, the entries generated by
this lexical rule cannot account for other possible us-
ages of the verbs in question. For example,
(6) U
you
moet
must
deze
this
zelf
yourself
inscheppen.
spoon in.INF
?You have to spoon this in yourself.?
Alpino fails to parse this sentence because inschep-
pen is used without a locative NP. Now, when the
LA method has predicted a transitive verb type for
inscheppen, the parser should be able to cover the
sentence. Other such examples from our data in-
clude wegwist (to erase.3PER.SG), onderligt (to lie
under.3PER.SG), etc.
Further, there are 10 words, including afwater,
which represent cases of nominalisation currently
not accounted for in the Alpino lexicon. The
LA process correctly predicts noun types for these
words. This should enable the parser to cover sen-
tences like:
(7) Die
this
moet
must
een
a
deel
part
van
from
het
the
afwater
drainage
vervoeren.
transport/move
?This has to move a part of the drainage.?
where afwater is used as a noun.
There are also 3 words which correctly receive
adjective predictions. Currently, their lexical en-
tries are incomplete because they are assigned only
past participle types in the lexicon. However, past
participles in Dutch can also act as adjectives. For
historical reasons, this systematic ambiguity is not
treated as such in Alpino. Each participle should
also have a separate adjective lexical entry but, as
we see, this is not always the case.
5 Results
After LA is finished, we restore the original lexical
entries for the 36 words but, additionally, each word
is also assigned the types which have been predicted
for it by the LA method. The 388 problematic sen-
tences from the Mediargus corpus are then re-parsed
with Alpino. We are interested in observing:
1. how many sentences receive a full-span analy-
sis
2. how the parsing accuracy of Alpino changes
Table 3 shows that when the Alpino lexicon is ex-
tended with the lexical entries we learnt through LA,
the parser is able to cover nearly 84% of the sen-
tences, including the ones given in (6) and (7). Since
there is no suitable baseline which this result can
be compared to, we developed an additional model
which indicates what is likely to be the maximum
coverage that Alpino can achieve for those sentences
by adding new lexical entries only.
907
In this second model, for each of the 36 words, we
add to the lexicon all types which were successfully
used for the respective word during the parsing with
universal types. In this way, Alpino is free to choose
from all types it has considered suitable for a given
word, i.e. the parser is not limited by the outcome
of the LA process but rather by the overall quality of
the grammar.
The ?universal types? model performs better than
ours? it achieves 87.9% coverage. Still, the perfor-
mance of our model is close to this result, i.e. close
to what we consider to be the maximal possible cov-
erage of Alpino for these 388 sentences when only
LA is used.
Model Coverage (%)
Our model (Alpino + LA) 83.76
Universal types 87.89
Table 3: Coverage results for the re-parsed 388 problem-
atic sentences
Some of the sentences which cannot be covered
by both models are actually not proper sentences
but fragments which were wrongly identified as sen-
tences during tokenization. Many other cases in-
clude sentences like:
(8) Een
a
gele
yellow
frommel
crease
papier,
paper
Arabische
Arabic
lettertekens.
characters
?A yellow paper crease, Arabic characters.?
which is probably the caption of a photo or an illus-
tration. However, because of the absence of a verb,
Alpino splits the analysis into two parts? the part be-
fore the comma and the part after the comma.
Here is a more interesting case:
(9) Als
when
we
we
ons
us
naar
to
de
the
buffettafel
buffet
begeven,
proceed
mistrap
misstep
ik
I
me.
myself
?When we proceed to the buffet I misstep.?
The LA method does not predict a reflexive verb
type for mistrap which prevents the production of
a full-span analysis because Alpino cannot connect
the reflexive pronoun me to mistrap. In this case,
however, the universal type model outperforms ours.
A reflexive verb type is among the universal types
and thus, Alpino is able to use that type to deliver a
full-span parse. We should note though, that LA cor-
rectly predicts a noun type for mistrap which enables
Alpino to parse successfully the other 14 sentences
which this word occurs in.
Let us now look at the correctness of the deliv-
ered parses. To estimate the accuracy of the parser,
we have randomly selected 100 sentences out of the
388 sentences in the test set and we have manually
annotated them in order to create a gold standard for
evaluation.
Accuracy in Alpino is measured in terms of de-
pendency relations. The accuracy for sentences
which are not assigned a full-span analysis but a se-
quence of non-overlapping parses can still be larger
than zero because, within these parses, some cor-
rect dependency relations could have been produced.
That is why, though the coverage of Alpino for the
selected 100 sentences is zero, we can still obtain
a number for accuracy and use it as a baseline for
comparison. Clearly, this baseline is expected to per-
form worse than both our model and the universal
types one since those are able to cover most of the
sentences and thus, they are likely to produce more
correct dependency relations. However, it gives us
an idea how much extra quality is gained when cov-
erage improves.
The accuracy results for the 100 annotated sen-
tences are given in Table 4. The average sentence
length is 18.9 tokens.
Model Accuracy (%) msec/sentence
Alpino 63.35 803
Our model 86.15 718
Universal types 85.12 721
Table 4: Accuracy results for the 100 annotated sentences
Our model achieves the highest accuracy without
increasing the parse times. Further, the baseline has
a much lower result which shows that coverage is
not gained on the expense of accuracy.
Our model and the universal types one achieve the
same accuracy for most of the sentences. However,
the universal types model has an important disad-
vantage which, in some cases, leads to the produc-
tion of wrong dependency relations. The model pre-
dicts a large number of lexical types which, in turn,
leads to large lexical ambiguity. This lexical am-
biguity increases the number of possible analyses
Alpino chooses from, thus making it harder for the
908
parser to produce the correct analysis. Let us con-
sider the following example where a sentence is cov-
ered by both models but the universal types model
has lower accuracy:
(10) Dat
that
wij
we
het
it
rechttrokken,
straighten.PAST.PL.
pleit
plead
voor
for
onze
our
huidige
current
conditie.
condition
?It pleads for our condition that we straightened it.?
Here, het is the object of the verb rechttrokken.
However, although there are transitive verb types
among the universal types assigned to rechttrokken,
Alpino chooses to use a verb type which subcate-
gorizes for a measure NP. This causes for het to be
analysed not as an object but as a measure comple-
ment, i.e. the produced dependency relation is incor-
rect.
The LA method, on the other hand, is much more
restrictive but its predictions are also much more ac-
curate. Since it considers sentences containing other
forms of the paradigm of rechttrokken when predict-
ing subcategorization frames, the LA method cor-
rectly assigns only one transitive and one intransitive
verb type to this word. This allows Alpino to recog-
nize het as the object of the verb and to produce the
correct dependency relation.
The few cases where the universal types model
outperforms ours include sentences like the one
given in (9) where the application of our model
could not enable Alpino to assign a full-span analy-
sis. Sometimes, the LA method is too restrictive and
does not output some of the correct types. These
types, on the other hand, could be provided by the
universal types model and could enable Alpino to
cover a given sentence and thus, to produce more
correct dependency relations. Allowing for the LA
method to predict more types, however, has proven
to be a bad solution because, due to the increased
lexical ambiguity, this leads to lower parsing accu-
racy.
6 Discussion
6.1 Comparison to Previous Work
The performance of the technique we presented in
this paper can be compared to the performance of a
number of other approaches applied to similar tasks.
Zhang et al (2006) and Villavicencio et al (2007)
use error mining to semi-automatically detect En-
glish multiword expressions (MWEs). Then, they
employ LA to learn proper lexical entries for these
MWEs and add them to the lexicon of a large-scale
HPSG grammar of English (ERG; (Copestake and
Flickinger, 2000)). This increases parsing coverage
by 15% to 22.7% for a test set of 674 sentences
containing MWEs and parsed with the PET parser
(Callmeier, 2000). In both studies, however, the
combination of error mining and LA is applied to
a very specific task whereas our method is a general
one.
Nicolas et al (2008) employ a semi-automatic
method to improve a large-scale morphosyntactic
lexicon of French (Lefff ; (Sagot et al, 2006)).
The lexicon is used in two grammars? the FRMG
(Thomasset and de la Clergerie, 2005), a hybrid Tree
Adjoining/Tree Insertion Grammar, and the SxLFG-
FR LFG grammar (Boullier and Sagot, 2006). The
first step in this approach is also the application of an
error miner (Sagot and de la Clergerie, 2006) which
uses a parsed newspaper corpus (about 4.3M words)
to pinpoint problematic unigrams.
The crucial difference with our method is in the
second step. Nicolas et al (2008) assign underspec-
ified lexical entries to a given problematic unigram
to allow the grammar to parse the uncovered sen-
tences associated with this unigram. Then, these en-
tries are ranked based on the number of successful
parses they have been used in.
The use of underspecification, however, causes
large ambiguity and severe parse overgeneration
(observed also in Fouvry (2003)). As a consequence
of that, the ranked list of lexical entries for each un-
igram is manually validated to filter out the wrong
entries. The employment of LA in our approach, on
the other hand, makes it fully automatic. The rank-
ing of the predictions is done by the classifier and
the predicted entries are good enough to improve the
parsing coverage and accuracy without any manual
work involved. Generally, recent studies (Baldwin,
2005; Zhang and Kordoni, 2006; Cholakov et al,
2008; Cholakov and van Noord, 2010) have clearly
shown that when it comes to learning new lexical
entries, elaborate LA techniques perform better and
are more suitable for large-scale grammars than un-
909
derspecification2.
Further, the naive ranking system used in Nicolas
et al (2008) puts a correctly generated entry for an
infrequent usage of a given word (e.g., a verb with
a rare subcat frame) in the bottom of the ranked list
because of the low number of sentences in which
this entry is used. The LA method we employ is
more sensitive to rare usages of words because it
considers occurrences of the word in question out-
side the parsed corpus (very important if the corpus
is domain-specific) and it also takes into account all
forms in the paradigm(s) of the word. This increases
the chances of a rare usage of this word to ?manifest?
itself.
Nicolas et al (2008) uses the lexical entries which
remain after the manual validation to re-parse the
newspaper corpus. 254 words (mostly verbs) are
corrected and the parse coverage increases by 3.4%
and 1.7% for the FRMG and the SxLFG, respec-
tively. However, the authors do not mention how
many of the original uncovered sentences they are
able to cover and therefore, we cannot compare our
coverage result. Nothing is said about the parsing
accuracy. Even with manually validated lexical en-
tries, it is still possible for the grammar to produce
full-span but wrong analyses.
6.2 Application to Other Systems and
Languages
It is important to note that this paper should be
viewed as a case study where we illustrate the re-
sults of the application of what we believe to be a
good algorithm for dealing with incomplete or in-
correct lexical entries? namely, the combination of
error mining and LA. However, our method is gen-
eral enough to be applied to other large-scale gram-
mars and languages.
The error mining is directly usable as soon as
there is a large parsed corpus available. The LA
technique we employed is also quite general pro-
vided that certain requirements are fulfilled. First,
words have to be mapped onto some finite set of la-
bels of which a subset of open-class (universal) la-
bels has to be selected. This subset represents the
labels which can be predicted for unknown words.
2In Nicolas et al (2008) the authors also admit that an elab-
orate LA technique will produce better results.
Second, we need a parser to analyse sentences
in which a given unknown word occurs. Finally,
the ME-based classifier allows for arbitrary com-
binations of features and therefore, any (language-
specific) features considered useful can be included.
As for the paradigm generation method, the idea of
combining a finite state morphology and web heuris-
tics is general enough to be implemented for differ-
ent languages.
We have already started investigating the applica-
bility of our method to the FRMG and a large-scale
grammar of German and the initial experiment and
results we have obtained are promising.
6.3 Future Research
Currently, our algorithm handles only unigrams
(words). However, it would be useful to extend it,
so it can work with longer n-grams. For example,
a given word could have some reading which is not
yet handled in the lexicon only within a particular
bi- or trigram.
Consider the bigram ?schampte af ? which has
been identified as problematic by the error miner.
It represents the particle verb ?afschampte? (to
glance.PAST.SG). Although the lexicon contains a
verb entry for ?schampte?, there is no entry handling
the case when this verb combines with the particle
?af ?. Another example is the bigram ?de slachtoffer?
(the victim). In standard Dutch, the noun ?slachtof-
fer? goes with the ?het? definite article which is
marked in its lexical entry. However, in Flemish it is
used with the ?de? article.
Our method is currently not able to capture these
two cases since they can be identified as problem-
atic on bigram level and not when only unigrams are
considered.
Further, the definition of what the error miner
considers to be a successful parse is a rather crude
one. As we saw, even if the grammar is able to pro-
duce a full-span analysis for a given sentence, this
analysis could still not be the correct one. There-
fore, it is possible that a word could have a prob-
lematic lexical entry even if it only occurs in sen-
tences which are assigned a full-span parse. Cur-
rently, such a word will not be identified as prob-
lematic by the error miner. That is why, some (sta-
tistical) model which is capable of judging the plau-
sibility of a parse should be developed and incorpo-
910
rated in the calculation of the suspicions during error
mining.
References
Tim Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acqui-
sition, Ann Arbor, USA.
Pierre Boullier and Beno??t Sagot. 2006. Efficient parsing
of large corpora with a deep LFG parser. In Proceed-
ings of LREC?06, Genoa, Italy.
Ulrich Callmeier. 2000. PET? a platform for experimen-
tation with efficient HPSG processing techniques. In
Journal of Natural Language Engineering, volume 6,
pages 99?107. Cambridge University Press.
Kostadin Cholakov and Gertjan van Noord. 2009. Com-
bining finite state and corpus-based techniques for
unknown word prediction. In Proceedings of the
7th Recent Advances in Natural Language Processing
(RANLP) conference, Borovets, Bulgaria.
Kostadin Cholakov and Gertjan van Noord. 2010. Ac-
quisition of unknown word paradigms for large-scale
grammars. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING-
2010), Beijing, China.
Kostadin Cholakov, Valia Kordoni, and Yi Zhang. 2008.
Towards domain-independent deep linguistic process-
ing: Ensuring portability and re-usability of lexicalised
grammars. In Proceedings of COLING 2008 Work-
shop on Grammar Engineering Across Frameworks
(GEAF08), Manchester, UK.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resource and Evaluation (LREC 2000), Athens,
Greece.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In Proceedigns of the 2009 Work-
shop on Grammar Engineering Across Frameworks,
ACL-IJCNLP 2009, pages 71?79, Singapore.
Frederik Fouvry. 2003. Lexicon acquisition with a large-
coverage unification-based grammar. In Companion
to the 10th Conference of EACL, pages 87?90, Bu-
dapest, Hungary.
Anna Korhonen, Genevieve Gorell, and Diana McCarthy.
2000. Statistical filtering and subcategorization frame
acquisition. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, Hong Kong,
China.
Mirella Lapata. 1999. Acquiring lexical generalizations
from corpora. A case study for diathesis alternations.
In Proceedings of the 37th Annual Meeting of ACL,
Maryland, USA.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 6th conference on Natural Language Learn-
ing (CoNLL-2002), pages 49?55, Taipei, Taiwan.
Cedric Messiant. 2008. A subcategorization acquisition
system for French verbs. In Proceedings of the ACL
2008 Student Research Workshop, Columbus, OH.
Lionel Nicolas, Beno??t Sagot, Miguel Molinero, Jacques
Farre?, and Eric de la Clergerie. 2008. Computer aided
correction and extension of a syntactic wide-coverage
lexicon. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING-
2008), pages 633?640, Manchester, UK.
Robbert Prins and Gertjan van Noord. 2001. Unsu-
pervised POS-tagging improves parcing accuracy and
parsing efficiency. In Proceedings of IWPT, Beijing,
China.
Beno??t Sagot and Eric de la Clergerie. 2006. Error min-
ing in parsing results. In Proceedings of the 44th Meet-
ing of the Association for Computational Linguistics
(ACL?06), pages 329?336, Morristown, NJ, USA.
Beno??t Sagot, Lionel Cle?ment, Eric de la Clergerie, and
Pierre Boullier. 2006. The Lefff 2 syntactic lexicon
for French. In Proceedings of LREC?06, Genoa, Italy.
Franc?ois Thomasset and Eric de la Clergerie. 2005.
Comment obtenir plus des me?etagrammaires. In Pro-
ceedings of TALN?05, Dourdan, France.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), pages 446?453, Barcelona,
Spain.
Gertjan van Noord. 2006. At last parsing is now opera-
tional. In Proceedings of TALN, Leuven, Belgium.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1034?1043,
Prague, Czech Republic.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open text processing. In
Proceedings of the Fifth International Conference on
Language Resourses and Evaluation (LREC 2006),
Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression pre-
diction for grammar engineering. In Proceedings of
911
the ACL Workshop on Multiword Expressions: Identi-
fying and Exploiting Underlying Properties, pages 36?
44, Sydney, Australia.
912
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 196?201,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Better Statistical Machine Translation through Linguistic Treatment of
Phrasal Verbs
Kostadin Cholakov and Valia Kordoni
Humboldt-Universit?at zu Berlin, Germany
{kostadin.cholakov,kordonieva}@anglistik.hu-berlin.de
Abstract
This article describes a linguistically in-
formed method for integrating phrasal
verbs into statistical machine translation
(SMT) systems. In a case study involving
English to Bulgarian SMT, we show that
our method does not only improve trans-
lation quality but also outperforms simi-
lar methods previously applied to the same
task. We attribute this to the fact that, in
contrast to previous work on the subject,
we employ detailed linguistic information.
We found out that features which describe
phrasal verbs as idiomatic or composi-
tional contribute most to the better trans-
lation quality achieved by our method.
1 Introduction
Phrasal verbs are a type of multiword expressions
(MWEs) and as such, their meaning is not deriv-
able, or is only partially derivable, from the se-
mantics of their lexemes. This, together with the
high frequency of MWEs in every day communi-
cation (see Jackendoff (1997)), calls for a special
treatment of such expressions in natural language
processing (NLP) applications. Here, we con-
centrate on statistical machine translation (SMT)
where the word-to-word translation of MWEs of-
ten results in wrong translations (Piao et al., 2005).
Previous work has shown that the application
of dedicated methods to identify MWEs and then
integrate them in some way into the SMT pro-
cess often improves translation quality. Gener-
ally, automatically extracted lexicons of MWEs
are employed in the identification step. Further,
various integration strategies have been proposed.
The so called static strategy suggests training the
SMT system on corpora in which each MWE is
treated as a single unit, e.g. call off. This im-
proves SMT indirectly by improving the align-
ment between source and target sentences in the
training data. Various versions of this strategy are
applied in Lambert and Banchs (2005), Carpuat
and Diab (2010), and Simova and Kordoni (2013).
In all cases there is some improvement in transla-
tion quality, caused mainly by the better treatment
of separable PVs, such as in turn the light on.
Another strategy, which is referred to as dy-
namic, is to modify directly the SMT system. Ren
et al. (2009), for example, treat bilingual MWEs
pairs as parallel sentences which are then added
to training data and subsequently aligned with
GIZA++ (Och and Ney, 2003). Other approaches
perform feature mining and modify directly the
automatically extracted translation table. Ren et
al. (2009) and Simova and Kordoni (2013) employ
Moses
1
to build and train phrase-based SMT sys-
tems and then, in addition to the standard phrasal
translational probabilities, they add a binary fea-
ture which indicates whether an MWE is present
in a given source phrase or not. Carpuat and
Diab (2010) employ the same approach but the
additional feature indicates the number of MWEs
in each phrase. All studies report improvements
over a baseline system with no MWE knowledge
but these improvements are comparable to those
achieved by static methods.
In this article, we further improve the dynamic
strategy by adding features which, unlike all previ-
ous work, also encode some of the linguistic prop-
erties of MWEs. Since it is their peculiar linguistic
nature that makes those expressions problematic
for SMT, it is our thesis that providing more lin-
guistic information to the translation process will
improve it. In particular, we concentrate on a spe-
cific type of MWEs, namely phrasal verbs (PVs).
We add 4 binary features to the translation table
which indicate not only the presence of a PV but
also its transitivity, separability, and idiomaticity.
We found that PVs are very suitable for this study
since we can easily extract the necessary informa-
1
http://www.statmt.org/moses/
196
tion from various language resources.
To prove our claim, we perform a case study
with an English to Bulgarian SMT system. Bul-
garian lacks PVs in the same form they appear in
English. It is often the case that an English PV is
translated to a single Bulgarian verb. Such many-
to-one mappings cause the so called translation
asymmetries which make the translation of PVs
very problematic.
We perform automated and manual evaluations
with a number of feature combinations which
show that the addition of all 4 features proposed
above improves translation quality significantly.
Moreover, our method outperforms static and dy-
namic methods previously applied to the same test
data. A notable increase in performance is ob-
served for separable PVs where the verb and the
particle(s) were not adjacent in the input English
sentence as well as for idiomatic PVs. This clearly
demonstrates the importance of linguistic informa-
tion for the proper treatment of PVs in SMT.
We would like to point out that we view
the work presented here as a preliminary study
towards a more general linguistically informed
method for handling similar types of translation
asymmetries. The experiments with a single phe-
nomenon, namely PVs, serve as a case study the
purpose of which is to demonstrate the validity of
our approach and the crucial role of properly inte-
grated linguistic information into SMT. Our work,
however, can be immediately extended to other
phenomena, such as collocations and noun com-
pounds.
The remainder of the paper is organised as fol-
lows. Section 2 describes the asymmetries caused
by PVs in English to Bulgarian translation. Sec-
tion 3 provides details about the resources in-
volved in the experiments. Section 4 describes
our method and the experimental setup. Sec-
tion 5 presents the results and discusses the im-
provements in translation quality achieved by the
method. Sections 6 concludes the paper.
2 Translation Asymmetries
We will first illustrate the main issues which arise
when translating English PVs into Bulgarian. For
more convenience, the Bulgarian phrases are tran-
scribed with Latin letters.
An English PV is usually mapped to a single
Bulgarian verb:
(1) Toj
he
otmeni
cancelled
sreshtata.
meeting-the
?He called off the meeting.?
In the example above the PV called off has to
be mapped to the single Bulgarian verb otmeni,
i.e. there is many-to-one mapping. Other cases
require a many-to-many type of mapping. One
such case is the mapping of an English PV to
a ?da?-construction in Bulgarian. Such construc-
tions are very frequent in Bulgarian every day
communication since they denote complex verb
tenses, modal verb constructions, and subordinat-
ing conjunctions:
(2) Toj
he
trjabva
should
da skasa
break off
s
with
neja.
her
?He should break off with her.?
Here, da skasa should be mapped to the PV break
off. Other such cases include Bulgarian reflexive
verb constructions.
Note that such many-to-many mappings in the
case of Bulgarian pose an additional challenge for
the SMT system because, for a good translation, it
needs to guess whether to add a ?da? particle or not
which further complicates the treatment of PVs.
Also, Bulgarian is a language with rich morphol-
ogy and often translations with very good seman-
tic quality lack the proper morphological inflec-
tion. This affects negatively both automated and
manual evaluation of translation quality.
3 Language Resources
We employ the data used in the studies reported in
Simova and Kordoni (2013). The authors experi-
mented with both static and dynamic methods for
handling PVs in an English to Bulgarian SMT sys-
tem. This allows us to compare the performance
of our linguistically informed approach to that of
methods which do not make use of the linguistic
properties of PVs.
The data for the experiments are derived from
the SeTimes news corpus
2
which contains par-
allel news articles in English and 9 Balkan lan-
guages. The training data consist of approximately
151,000 sentences. Another 2,000 sentences are
used for the tuning. The test set consists of 800
sentences, 400 of which contain one or more in-
2
http://www.setimes.com
197
stances of PVs. There are 138 unique PVs with a
total of 403 instances in the test data. Further, a
language model for the target language is created
based on a 50 million words subset of the Bul-
garian National Reference Corpus.
3
All English
data are POS tagged and lemmatised using the
TreeTagger (Schmid, 1994). For Bulgarian, these
tasks were performed with the BTB-LPP tagger
(Savkov et al., 2011).
Simova and Kordoni (2013) create automati-
cally a lexicon containing English PVs. It is em-
ployed for the identification of such verbs in the
data used in the experiments. The lexicon is con-
structed from a number of resources: the En-
glish Phrasal Verbs section of Wiktionary,
4
the
Phrasal Verb Demon dictionary,
5
the CELEX Lex-
ical Database (Baayen et al., 1995), WordNet
(Fellbaum, 1998), the COMLEX Syntax dictio-
nary (Macleod et al., 1998), and the gold standard
data used for the experiments in McCarthy et al.
(2003) and Baldwin (2008). English PVs are iden-
tified in the data using the jMWE library (Kulkarni
and Finlayson, 2011) as well as a post-processing
module implemented in the form of a constrained
grammar (Karlsson et al., 1995) which filters out
spurious PV candidates. For the identification of
PVs, Simova and Kordoni (2013) report 91% pre-
cision (375 correct instances found) and a recall
score of 93% for the 800 test sentences.
The Moses toolkit is employed to build a fac-
tored phrase-based translation model which op-
erates on lemmas and POS tags. Given the rich
Bulgarian morphology, the use of lemma informa-
tion instead of surface word forms allows for a
better mapping between source and target transla-
tion equivalents. The parallel data are aligned with
GIZA++. Further, 2 5-gram language models are
built using the SRILM toolkit
6
on the monolingual
Bulgarian data to model lemma and POS n-gram
information. Note that the Bulgarian POS tags are
quite complex, so they can account for a variety
of morphological phenomena. Automated trans-
lation is performed by mapping English lemmas
and POS tags to their Bulgarian equivalents and
then generating the proper Bulgarian word form
by using lemma and POS tag information.
3
http://webclark.org/
4
http://en.wiktionary.org/wiki/
Category:English\_phrasal\_verbs
5
http://www.phrasalverbdemon.com/
6
http://www-speech.sri.com/projects/
srilm/
1 0
feature 1 PV present no PV
feature 2 transitive intransitive
feature 3 separable inseparable
feature 4 idiomatic (semi-)comp.
Table 1: Values for the 4 new features.
4 Addition of Linguistic Features
The resources from which the PV lexicon is con-
structed also contain various types of linguistic in-
formation. Wiktionary provides the most details
since the entries there contain information about
the valency of the verb (transitive vs intransitive)
and whether a particle can be separated from the
PV in particle verb constructions. Consider fell off
his bike and *fell his bike off vs turn the engine on
and turn on the engine.
Further, Wiktionary indicates whether a given
PV is compositional or idiomatic in nature. The
meaning of (semi-)compositional PVs can be (par-
tially) derived from the meaning of their lexemes,
e.g. carry in. The degree of compositionality af-
fects the productivity with which verbs and parti-
cles combine. Verbs with similar semantics often
combine with the same particle, e.g. bring/carry
in. This is not the case for fully idiomatic PVs, e.g.
get/*obtain over. Therefore, the notion of compo-
sitionality plays a very important role in the treat-
ment of PVs and MWEs in general. The dataset
described in McCarthy et al. (2003) also indicates
whether a PV is idiomatic or not.
We were able to acquire the PV lexicon and
we augmented it with the information obtained
from the various resources. Then, once the sys-
tem is trained, we add 4 binary features to each
entry in the automatically created translation table.
The values those features take are shown in Table
1. If a given property is not specified for some
PV in the lexicon, the value of the corresponding
feature is 0. Naturally, if no PV is identified in
a source phrase, the value of all 4 features is 0.
This is different from previous work where only
one feature is added, indicating the presence of a
PV. By adding those new features, we want to bias
the SMT system towards using phrases that do not
?split? PVs during decoding.
198
with PVs no PVs all
bleu nist bleu nist bleu nist
baseline 0.244 5.97 0.228 5.73 0.237 6.14
static 0.246 6.02 0.230 5.76 0.239 6.18
dynamic-1 0.250 5.92 0.226 5.54 0.244 6.02
dynamic-4 0.267 6.01 0.232 5.74 0.256 6.16
Table 2: Automatic evaluation of translation qual-
ity.
5 Results and Discussion
Automatic Evaluation. Table 2 presents the re-
sults from the automatic evaluation, in terms of
BLEU (Papineni et al., 2002) and NIST (Dodding-
ton, 2002) scores, of 4 system setups. The base-
line has no MWE knowledge, while the static and
the dynamic-1 system setups are reproduced from
the experiments described in Simova and Kordoni
(2013). Dynamic-1 includes only a single binary
feature which indicates the presence of a PV while
our method, dynamic-4, includes the 4 features de-
scribed in Table 1.
Our method outperforms all other setups in
terms of BLEU score, thus proving our point that
adding features describing the linguistic properties
of PVs improves SMT even further. Also, the re-
sults for the 400 sentences without PVs show that
the 4 new features do not have a negative impact
for PV-free contexts.
In terms of NIST the static strategy consistently
performs best, followed closely by our method.
NIST is a measure which weights the translated
n-grams according to their informativeness. Due
to the nature of this measure, less frequent cor-
rectly translated n-grams are given more weight
in the evaluation process because NIST considers
them ?more informative?. Such less frequent n-
grams, or in our case PVs, are likely to be cap-
tured better by the static setup. Therefore, this
setup achieves the highest NIST scores. This fact
also suggests that dynamic and static strategies in-
fluence the SMT process in different ways, with
our method tending to capture more frequent (and
thus less informative) n-grams. Interestingly, the
other dynamic method, dynamic-1, has the worst
performance of all setups in terms of NIST.
Manual evaluation. To get a better insight on
how the different setups deal with the translation
of PVs, we also performed a manual evaluation.
A native speaker of Bulgarian was asked to judge
the translations of PVs for the 375 test sentences in
good acceptable incorrect
baseline 0.21 0.41 0.38
static 0.25 0.5 0.25
dynamic-1 0.24 0.51 0.25
dynamic-4 0.3 0.5 0.2
Table 3: Manual evaluation of translation quality.
which such verbs were correctly identified during
the identification step. The human subject takes
into account the target PV and a limited context
around it and judges the translation as:
? good - correct translation of the PV, correct
verb inflection
? acceptable - correct translation of the PV but
wrong inflection, or wrongly built da- or re-
flexive construction
? incorrect - wrong translation which changes
the meaning of the sentence
Table 3 shows the results. Our method dynamic-
4 produces more good translations and less incor-
rect ones than all other setups. This illustrates fur-
ther the benefits of adding linguistic features to
the translation model. The results achieved by the
static approach are attributed to the better handling
of separable PVs in sentences where the particle
was not adjacent to the verb. The dynamic-1 ap-
proach and the baseline often interpret the particle
literally in such cases which leads to almost twice
the amount of wrong translations. Our method, on
the other hand, performs slightly lower than the
static approach in this respect but still much better
than the other 2 setups.
Compared to dynamic-1 and the baseline, the
static approach also handles better idiomatic PVs
but performs slightly worse for sentences with
compositional PVs. However, the addition of a
specific feature to encode idiomaticity in the trans-
lation model enables our method dynamic-4 to
achieve the best performance for idiomatic PVs
while still handling successfully many composi-
tional PVs. To summarise, the improved results
of our method in comparison to previous work are
attributed to the better handling of separable PVs
which occur in a split form and even more to the
improved ability to differentiate between compo-
sitional and idiomatic PVs.
Feature combinations. Our method performs
best when all 3 linguistic features described above
199
are taken into account by the SMT system. How-
ever, we also experimented with different combi-
nations of those features in order to get some in-
sight of the way each feature influences the trans-
lation quality. Adding only the feature denot-
ing verb transitiveness did not lead to any sig-
nificant improvement compared to the dynamic-
1 setup. Also, the combination which leaves out
this feature and uses the remaining ones ranks
second, achieving only a slightly worse perfor-
mance than dynamic-4, the setup in which all fea-
tures are employed. It seems that the transitive-
ness feature does not contribute much to the task
at hand. Adding only the feature denoting sepa-
rable vs inseparable PVs and adding only the one
denoting idiomaticity led to results slightly higher
than those of the dynamic-1 and static setups but
still, those results were significantly lower than the
ones presented in Tables 2 and 3.
6 Conclusion and Outlook
In this article, we showed that the addition of lin-
guistically informative features to a phrase-based
SMT model improves the translation quality of a
particular type of MWEs, namely phrasal verbs.
In a case study involving SMT from English to
Bulgarian, we showed that adding features which
encode not only the presence of a PV in a given
phrase but also its transitiveness, separability, and
idiomaticity led to better translation quality com-
pared to previous work which employs both static
and dynamic strategies.
In future research, we will extend our method
to other language pairs which exhibit the same
type of translation asymmetries when it comes to
PVs. Such language pairs include, among others,
English-Spanish and English-Portuguese.
Further, we will apply our linguistically in-
formed method to other phenomena which cause
similar issues for SMT. Immediate candidate phe-
nomena include other types of MWEs, colloca-
tions, and noun compounds. When it comes to
MWEs, we will pay special attention to the com-
positionality aspect since it seems to have con-
tributed most to the good performance achieve by
our method in the study presented here.
References
R H Baayen, R Piepenbrock, and L. Gulikers. 1995.
The CELEX lexical database (CD-ROM).
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of english verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop: Towards a Shared Task for Multiword Expres-
sions, pages 1?2, Marakesh, Morocco.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics., HLT ?10., pages
242?245, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. The MIT press.
Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. MIT Press, Cambridge, MA.
Fred Karlsson, Atro Voutilainen, Juha Heikkila, and
Arto Anttila. 1995. Constraint grammar: A
language-independent system for parsing unre-
stricted text. Natural Language Processing, 4.
Nidhi Kulkarni and Mark Alan Finlayson. 2011.
JMWE ? a Java toolkit for detecting multiword ex-
pressions. In Proceedings of the 2011 Workshop on
Multiword Expressions, pages 122?124.
Patrik Lambert and Rafael Banchs. 2005. Data in-
ferred multi-word expressions for statistical machine
translation. In Proceedings of the X Machine Trans-
lation Summit, pages 396?403.
Catherine Macleod, Adam Meyers, and Ralph Grish-
man, 1998. COMLEX Syntax Reference Manual.
New York University.
Diana McCarthy, B Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
Workshop on Multiword Expressions: analysis, ac-
quisition and treatment, Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Stroudsburg,
PA, USA.
200
Scott Songlin Piao, Paul Rayson, and and
Tony McEnery Dawn Archer. 2005. Com-
paring and combining a semantic tagger and a
statistical tool for MWE extraction. Comuter
Speech and Language, 19(4):378?397.
Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and
Yun Huang. 2009. Improving statistical machine
translation using domain bilingual multiword ex-
pressions. In Proceedings of the ACL Workshop on
Multiword Expressions: Identification, Interpreta-
tion, Disambiguation and Applications, pages 47?
54, Singapore.
Aleksandar Savkov, Laska Laskova, Petya Osenova,
Kiril Simov, and Stanislava Kancheva. 2011. A
web-based morphological tagger for Bulgarian. In
Proceedings of the Sixth International Conference
on Natural Language Processing, Multilinguality,
pages 126?137, Bratislava, Slovakia.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, UK.
Iliana Simova and Valia Kordoni. 2013. Improv-
ing English-Bulgarian statistical machine translation
by phrasal verb treatment. In Proceedings of MT
Summit XIV Workshop on Multi-word Units in Ma-
chine Translation and Translation Technology, Nice,
France.
201
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 68?77,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Automated Verb Sense Labelling Based on Linked Lexical Resources
Kostadin Cholakov
1
Judith Eckle-Kohler
2,3
Iryna Gurevych
2,3
1
Humboldt-Universit?at zu Berlin, kostadin.cholakov@anglistik.hu-berlin.de
2
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
3
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
We present a novel approach for creat-
ing sense annotated corpora automatically.
Our approach employs shallow syntactico-
semantic patterns derived from linked lex-
ical resources to automatically identify in-
stances of word senses in text corpora. We
evaluate our labelling method intrinsically
on SemCor and extrinsically by using au-
tomatically labelled corpus text to train a
classifier for verb sense disambiguation.
Testing this classifier on verbs from the
English MASC corpus and on verbs from
the Senseval-3 all-words disambiguation
task shows that it matches the performance
of a classifier which has been trained on
manually annotated data.
1 Introduction
Sense annotated corpora are important resources
in NLP as they can be used as training data (e.g.,
for word sense disambiguation (WSD) or semantic
role labelling) or as sources for the acquisition of
lexical information (e.g., selectional preference in-
formation). Typically, a particular sense inventory
from a lexical resource is used to annotate some or
all words with word senses from this sense inven-
tory. For instance, various sense-annotated cor-
pora based on WordNet (WN; (Fellbaum, 1998))
exist, such as the data from the Senseval competi-
tions,
1
or the SemCor corpus.
2
Such corpora are
usually created manually which is expensive and
time consuming. Furthermore, the corpora are of-
ten domain specific (e.g. newspaper texts) which
makes statistical systems trained on them strongly
biased.
We present a novel approach for creating sense
annotated corpora automatically. Our approach
1
http://www.senseval.org
2
http://www.cse.unt.edu/
?
rada/
downloads.html#semcor
employs shallow syntactico-semantic patterns de-
rived from linked lexical resources (LLRs) to auto-
matically identify instances of word senses in text
corpora. We significantly extend previous work on
this task by making two important contributions:
(i) we employ a large-scale LLR for automatically
creating sense annotated data and (ii) we perform
meaningful intrinsic and application-based eval-
uations of our method on large sense annotated
datasets.
LLRs are the result of integrating several
lexical-semantic resources by linking them at the
word sense level. Examples of large LLRs are
the multilingual BabelNet (Navigli and Ponzetto,
2012), an integration of wordnets and Wikipedia
3
,
or UBY, (Gurevych et al., 2012), the resource we
employ in our work here. UBY is an integration of
multiple resources, such as wordnets, Wikipedia,
Wiktionary (WKT)
4
, FrameNet (FN; (Baker et al.,
1998)) and VerbNet (VN; (Kipper et al., 2008)) for
English and German.
A distinguishing feature of LLRs is the enriched
sense representation for word senses that are in-
terlinked since different resources provide differ-
ent, often complementary information. Annotat-
ing corpora with such enriched sense representa-
tions turns them into versatile training data for sta-
tistical systems.
Our first contribution (i) also addresses a con-
siderable gap in recent research regarding auto-
mated sense labelling of verbs. Most previous
work is done on nouns. However, verbs pose a
bigger challenge due to their high polysemy and
the fact that, unlike nouns, syntax is of crucial im-
portance because it often reflects particular aspects
of verb meaning. That is why, here we focus on
verbs and present results and evaluations for this
previously neglected part-of-speech (POS). Our
method, however, can be applied to other parts-of
3
http://www.wikipedia.org
4
http://www.wiktionary.org
68
speech as well.
Regarding (ii), we are the first to perform mean-
ingful intrinsic and extrinsic evaluations of auto-
matically labelled data on a larger scale. The in-
trinsic evaluation measures the performance of our
method on the manually annotated SemCor cor-
pus. The extrinsic evaluation compares the perfor-
mance of a classifier for verb sense disambigua-
tion (VSD) which has been trained (a) on auto-
matically sense labelled data and (b) on manually
annotated data. Both settings achieve very simi-
lar results which means that competitive VSD can
be performed without the need of costly manually
created training data. This could be beneficial in
languages (e.g., German, Spanish) for which elab-
orate lexical-semantic resources exist but large,
high-quality sense annotated corpora are unavail-
able. Moreover, we experiment with various link-
ings between lexical resources in order to inves-
tigate how different resource combinations affect
the performance of automated sense labelling. We
show that combining all available resources might
not be the best option.
The remainder of the paper is organised as fol-
lows. Section 2 presents our method. Section 3 de-
scribes the data used in the experiments. Section
4 presents the results of the evaluations. Section
5 analyses in detail the differences between our
method and previous work. Section 6 concludes
the paper.
2 Automated Labelling of Verb Senses
This section describes our novel approach for au-
tomated sense labelling of verbs in a corpus, which
exploits the added value of LLRs.
2.1 Approach
Our approach to automatically label corpus in-
stances of verb senses with sense identifiers from
an LLR is based on a pattern-based representation
of verb senses. Such patterns constitute a common
format for the representation of verb senses avail-
able in LLRs and verb instances found in corpora.
The common format we developed resembles a
syntactico-semantic clause pattern which we call
a sense pattern (SP). Based on a comparison of the
derived SPs by means of a similarity metric, verb
instances in a corpus can automatically be labelled
with sense identifiers from an LLR.
SPs can be derived from corpus instances and
from information given in LLRs, in particular,
sense examples and more abstract predicate argu-
ment structure information.
2.2 Step 1: Creation of SPs from LLRs
For the creation of SPs, we employ the large-scale
LLR UBY which combines 10 lexical resources
for English and German to make use of the en-
riched verb sense representations provided by the
sense links between various resources available in
UBY. Although our method can work with any
LLR, we choose UBY because the various re-
sources are represented in a standardised format
(Eckle-Kohler et al., 2012) and sense links be-
tween them can uniformly and conveniently be ac-
cessed via the freely available UBY-API.
5
Since we evaluate our method on data annotated
with WN senses, we create SPs for enriched WN
senses (see example given in Table 1). We enrich
WN senses by aggregating lexical information that
can be accessed through links given in UBY to
corresponding verb senses in other resources.
In this setting, enrichment means that we make
use of sense examples from WN, from FN via
the WN?FN linking, and from WKT via the
WN?WKT linking. In addition, we use ab-
stract predicate-argument structure information
from VN via the WN?VN linking (see Table 1).
6
For phrasal verb senses (e.g., write up) and
other verbal multiword expressions (e.g., know
what?s going on) listed in WN, UBY rarely pro-
vides links to other resources. Therefore, we in-
duced sense links by following the one sense per
collocation assumption.
7
Based on this assump-
tion, we linked each sense of a verbal multiword
verb lemma in WN with each sense of the same
multiword lemma in FN and WKT.
From sense examples, we derive two different
kinds of SPs. Based on a fragment of a sense ex-
ample given by a window w around the target verb
lemma we create: (i) lemma SPs (LSPs) consisting
only of lemmas (including the target verb) and (ii)
abstract SPs (ASPs) consisting of the target verb
lemma and items from a fixed, linguistically mo-
tivated vocabulary. This is based on the intuition
that LSPs are important to identify relatively fixed
5
http://code.google.com/p/uby/
6
Although VN is linked to sense examples given in the
PropBank corpus, the rationale behind using just abstract
predicate-argument structure information was to explore,
which effect this type of information has on the performance
of an automated labelling algorithm.
7
It assumes that nearby words provide strong and consis-
tent clues to the sense of a target word, see Yarowsky (1995).
69
WN sense tell%2:32:00:: (let something be known) Corresponding sense patterns (SPs)
WN Tell them that you will be late LSP ? tell them that you will be
ASP ? tell PP that PP be JJ
WN?FN But an insider told TODAY : ? There was no animosity.? LSP ? but an insider tell Today : ? there be
ASP ? person tell location be feeling
WN?WKT Please tell me the time. LSP ? Please tell me the time
ASP ? tell PP event
WN?VN Agent[+animate| + organization] V ASP ? PP tell group about communication
Recipient[+animate| + organization]
about Topic[+communication]
Table 1: Examples of SPs derived from an enriched WN sense in UBY. PP, JJ, and VV are POS tags
from the Penn Treebank tagset, standing for personal pronoun, adjective and full verb.
verbal multiword expressions in a corpus, whereas
ASPs are necessary to identify productively used
verb senses that are constrained in their use only
by their syntactic behaviour and particular seman-
tic properties, such as selectional preferences on
their arguments.
The fixed vocabulary used for the creation of
ASPs consists of (i) the target verb lemma, (ii) se-
lected POS tags from the Penn Treebank Tagset
(Marcus et al., 1993), (iii) a list of particular func-
tion words that play an important role in fine-
grained subcategorisation frames of verbs (Eckle-
Kohler and Gurevych, 2012) and (iv) semantic cat-
egories of nouns given by WN semantic fields. We
selected POS tags that play an important role in
syntactic realisations of verbs, e.g. POS tags for
personal pronouns which are potential verb argu-
ments. In our experiments, we tried different sets
of function words and POS tags. For instance,
we found that some function words (e.g., reflex-
ive pronouns) and some POS tags (e.g., those for
past participles and comparative adjectives) intro-
duced too much noise in the data and therefore we
did not select them for the final vocabulary.
8
In order to create SPs from sense examples,
we apply POS tagging and lemmatisation using
the TreeTagger (Schmid, 1994) and named entity
tagging using the Stanford Named Entity Recog-
niser (Klein et al., 2003). The named entity
tags attached by the Named Entity Recogniser are
mapped to WN semantic fields.
For the generation of ASPs from sense exam-
ples, we used a window size of w = 7, while
the generation of LSPs has been performed with
w = 5 in order to put a focus on the closely neigh-
bouring lexemes in multiword verb lemmas. The
8
The vocabulary used for the creation of ASPs is available
at http://www.ukp.tu-darmstadt.de/data/.
window size was set empirically using the English
Lexical Sample task of the Senseval-2 dataset as
a development set. The same set was also used
for the development of the linguistically motivated
vocabulary for ASPs.
9
From the abstract predicate-argument struc-
ture information given in VN, we derived only
ASPs. For this, we employed the subcategori-
sation frames, as well as the semantic role and
selectional preference information from VN, and
created ASPs based on manually created map-
pings between these information types and the
controlled vocabulary used for ASPs.
2.3 Step 2: Automated Labelling
For the automated labelling of verbs in a corpus,
we first derive SPs from each corpus sentence con-
taining a target verb. SPs are derived from corpus
sentences by applying the same procedure as de-
scribed in Step 1 for the creation of SPs from sense
examples, the window size used is w = 7.
To compare two SPs, we propose a similarity
metric based on Dice?s coefficient which calcu-
lates the sum of the weighted number of their com-
mon bi-grams, tri-grams, and four-grams. For-
mally, the similarity score sim
w
? [0..1] of two
SPs p
1
, p
2
is defined as:
(1) sim
w
(p
1
, p
2
) =
4
?
n=2
|G
n
(p
1
)?G
n
(p
2
)|?n
norm
w
where w >= 1 is the size of the window around
the target verb, G
n
(p
i
), i ? {1, 2} is the set of n-
9
However, the Senseval-2 data are annotated with sense
keys of the WN pre-release version 1.7 and therefore, we had
to employ an automated mapping of WN 1.7 pre-release to
WN 3.0 sense keys provided by Rada Mihalcea. Since this
mapping turned out to be rather noisy, we did not use the
Senseval-2 data in our evaluations.
70
Automated labelling of corpus instances
for each sentence s
i
with verb v
derive LSP
i
and ASP
i
forall j = sizeOf(UBY-LSP(v))
compare LSP
i
with LSP
j
in UBY-LSP(v):
maxSim(LSP
i
) = argmax
j
score(LSP
i
, LSP
j
)
add sense(argmax
j
) to MostSimilarSenses(LSP
i
)
forall k = sizeOf(UBY-ASP(v))
compare ASP
i
with ASP
k
in UBY-ASP(v):
maxSim(ASP
i
) = argmax
k
score(ASP
i
, ASP
k
)
add sense(argmax
k
) to MostSimilarSenses(ASP
i
)
if maxSim
i,j
>= threshold t and
maxSim
i,j
>= maxSim
i,k
label(s
i
) = random(MostSimilarSenses(LSP
i
))
else if maxSim
i,k
>= threshold t
label(s
i
) = random(MostSimilarSenses(ASP
i
))
end if
end for
Table 2: Algorithm for labelling corpus instances
with WordNet senses.
grams occurring in SP p
i
, and norm
w
is the nor-
malisation factor defined by the sum of the max-
imum number of common bigrams, trigrams and
fourgrams in the window w. Similarity metrics
based on Dice?s coefficient have often been used
in Lesk-based WSD (Lesk, 1986) to calculate the
overlap of two sets (e.g., Baldwin et al. (2010)). In
our case, however, the elements of the two sets are
bigrams, trigrams and fourgrams, while in Lesk-
based algorithms typically sets of unigrams are
compared, thus not accounting for word order.
Table 2 shows the algorithm used for automated
labelling of corpus instances in pseudo-code. The
algorithm assumes that for each verb v, the corre-
sponding set of SPs derived from UBY sense ex-
amples (UBY-LSP(v) and UBY-ASP(v) in Table
2) has already been computed.
For each corpus sentence containing a target
verb v, the corresponding SPs for verb v derived
from UBY are scored by the similarity metric in
(1). The SPs with the maximum score that is above
a threshold t form the set of most similar senses.
From this set, the algorithm picks one sense ran-
domly as a label. How often this happens, depends
on the value of t: the percentage of randomly se-
lected senses ranges from about 33% for t = 0.14
to about 50% for t = 0.04.
3 Data
Web corpora. For the automated labelling of cor-
pus data with WN senses, we use two very large
web corpora: the English ukWaC corpus (Ba-
roni et al., 2009) and the article pages extracted
from the English Wikipedia using the Java-based
Wikipedia API JWPL (Zesch et al., 2008). Fur-
ther, for the evaluation of our method, we use three
manually sense annotated data sets.
SemCor. We use the SemCor 3.0 corpus which
is annotated with WN 3.0 senses.
MASC. MASC is a balanced subset of 500K
words of written texts and transcribed speech
drawn primarily from the Open American Na-
tional Corpus (OANC).
10
The texts come from 19
different genres which allows us to test our method
on real-life data from multiple sources. The cor-
pus is annotated with various types of linguistic
information, including WN 3.0 sense annotations
for instances of selected words. Therefore, MASC
is a lexical sample corpus.
We extracted instances of 16 MASC verbs
(11,997 instances) which have been sense anno-
tated. Most instances are annotated by multiple
annotators and, to create a gold standard, we took
the sense preferred by the majority of annotators
and ignored instances where there were ties.
Senseval-3. In the test corpus of the Senseval-
3 all-words disambiguation task sense annotations
are provided for each content word in a chunk
of the WSJ corpus (5,000 words of running text).
The third annotated data set for our experiment is
formed by extracting all verb instances from this
test corpus. Note that the gold standard annota-
tions in Senseval-3 were made using WN 1.7.1.
In our experiments, we use Rada Mihalcea?s con-
version of the corpus to WN 3.0.
11
However, we
found out that some verb instances were converted
to sense labels that do not exist in WN 3.0. Af-
ter removing those instances, there were 305 verbs
with 592 instances left.
4 Experiments and Evaluation
Next, we present the intrinsic and the application-
based evaluations of our method.
4.1 Intrinsic Evaluation
We intrinsically evaluate the performance of the
automated labelling algorithm for the Senseval-3
verbs which occur in the SemCor corpus. Occur-
rences of these 152 verbs in SemCor are processed
10
http://www.americannationalcorpus.
org/
11
http://www.cse.unt.edu/
?
rada/
downloads.html#sensevalsemcor
71
WN?FN?WKT WN?FN?WKT?VN
t Cov Cov Acc Cov Cov Acc
(Inst.) (Sense) (Inst.) (Sense)
0.04 0.55 0.27 0.32 0.48 0.25 0.35
0.07 0.15 0.17 0.36 0.13 0.15 0.42
0.1 0.11 0.14 0.35 0.10 0.13 0.42
0.14 0.02 0.07 0.41 0.02 0.05 0.47
Table 3: Performance of the automated labelling
algorithm evaluated for occurrences of Senseval-3
verbs in SemCor.
by the labelling algorithm with a window size
w = 7 and the automatically annotated WN 3.0
senses are compared with the gold senses available
in SemCor 3.0.
Quantitative Evaluation. We calculated the
accuracy as the percentage of correctly labelled in-
stances and the instance coverage as the percent-
age of labelled instances. The sense coverage is
calculated as the percentage of all predicted (not
annotated) senses relative to all gold verb senses
given in SemCor.
A random sense baseline yields 15% accuracy.
Note that a MFS baseline based on WN would
not be meaningful, because the WordNet MFS is
based on the frequency distribution of annotated
senses in SemCor.
Table 3 shows accuracy and coverage results
of the automated labelling algorithm for different
values of the threshold t and two combinations of
sense links from UBY. Depending on the threshold
t, 2% to 55% of the verb instances in SemCor can
automatically be labelled, and the instance cov-
erage goes largely in parallel to the coverage of
predicted WN senses. Accuracy ranges between
32% and 47% and exceeds the random sense base-
line by a large margin. Lowering the threshold in-
creases the coverage of the labelling method, but
it also leads to a decrease in accuracy of 9 percent-
age points (12 for the configuration with VN).
Adding more patterns from VN via the WN?
VN alignment, leads to a decrease in both instance
and sense coverage combined with an increase in
accuracy. Since SemCor is a rather small corpus,
the increase in instance coverage is not as clear
as for large Web corpora such as the ukWaC cor-
pus. Labelling a 1GB subset of the ukWaC cor-
pus based on patterns derived from the WN?FN?
WKT alignments resulted in 15MB of labelled
data, whereas 25MB labelled data could be created
from the same subset with the additional patterns
from the WN?VN alignment.
Qualitative Analysis. In Table 4, we show ex-
amples of the highest ranking patterns and the cor-
responding labelled SemCor instances for senses
that were correctly and falsely annotated. The ex-
amples in Table 4 show that the similarity metric
assigns the highest values to instances where func-
tion words (e.g., in, to, who) or POS tags (e.g., PP,
VV) from the ASP vocabulary occur in the im-
mediate neighbourhood of the target verb. Since
such functions words play an important role in the
ASPs derived from VN, the VN ASPs possibly
tend to dominate over the SPs derived from sense
examples, which explains the observed decrease in
coverage (see Table 3).
The falsely labelled instances turn out to be ex-
amples of WN senses where the gold sense is very
similar to the automatically attached sense as evi-
dent from the synset definition given in the right-
most column.
4.2 Extrinsic Evaluation
We extrinsically evaluate our method for auto-
mated verb sense labelling by using it for learning
a classifier for VSD in a train-test setting. We use
features which have been widely used in super-
vised WSD systems, in particular features based
on dependency parsing. While this might seem
to be in contrast to our labelling algorithm which
is based on shallow linguistic preprocessing, it is
fully justified by the purpose of our extrinsic eval-
uation: The main purpose of the extrinsic evalua-
tion is not to outperform state-of-the-art VSD sys-
tems, but to show that, when operating with rea-
sonable features, a classifier trained on the data
automatically labelled with our method performs
equally well as when this classifier is trained on
manually annotated data.
4.2.1 Features
The training and test data are parsed with the Stan-
ford parser (Klein and Manning, 2003) which pro-
vides Stanford Dependencies output (De Marneffe
et al., 2006) as well as phrase structure trees. We
employ the Stanford Named Entity Recogniser to
identify named entities. We then extract lexical,
syntactic, and semantic features from the parse re-
sults for classification.
Lexical features include the lemmas and POS
tags of the two words before and after the tar-
get verb. To extract syntactic features we select
all dependency relations from the parser output in
72
SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets)
Some of the New York Philharmonic
musicians who live in the suburbs spent
yesterday morning digging themselves
free from snow.
of group person who live
in location VVD time time
VVG
0.29 live%2:42:08:: (live%2:42:08::)
These societies can expect to face diffi-
cult times.
group expect to VV JJ
event
0.22 expect%2:31:01:: (expect%2:31:01::)
As autumn starts its annual sweep , few
Americans and Canadians realize how
fortunate they are in having the world ?s
finest fall coloring.
JJ attribute JJ person real-
ize how JJ PP be in
0.22 realize%2:31:00:: ? perceive (an idea or
situation) mentally (realize%2:31:01::
? be fully aware or cognizant of)
Dan Morgan told himself he would for-
get Ann Turner.
person person VVD PP PP
forget person location
0.16 forget%2:31:00:: ? be unable to re-
member (forget%2:31:01:: ? dismiss
from the mind; stop remembering)
Table 4: Examples of SemCor instances with high similarity scores (upper half shows correctly labelled
instances, lower half incorrectly labelled instances.
which the target verb is related to a noun, a pro-
noun, or a named entity. For each selected word,
the lemma of the word (or the named entity tag in
case of proper nouns) is combined with the type
of the dependency relation which exists between
it and the verb to form a separate feature. In a
similar feature, the lemma of the selected word is
replaced by its POS tag. The semantic features
include all synsets found in WN for nominal argu-
ments of the verb. Personal pronouns are mapped
to ?person? and the three synsets found in WN 3.0
for this word are taken as features.
4.2.2 Train and Test Data
Using exactly the same method as intrinsically
evaluated in section 4.1, we automatically labelled
occurrences of the 16 MASC verbs and the 305
Senseval-3 verbs in both web corpora with WN
senses. Only occurrences with similarity score
above 0.1 are labelled ? all other occurrences are
discarded. We refer to the resulting data as au-
tomatically labelled corpus (ALC) and use it as
training data for statistical VSD.
Instances of the test verbs found in SemCor are
also used as training data in order to compare the
performance of the classifier in a fully supervised
setting.
MASC. There are 22 senses with instances in
MASC which are not found in SemCor. For the
ALC this number is 34. However, in the latter
there are 27 senses, instances of which are un-
seen in MASC. 20 of those represent phrasal verbs
which we attribute to the special treatment of such
verbs in our method.
The classifier cannot correctly classify senses
which are not seen in the training data. The cov-
erage of the ALC is 88.05% and that of SemCor
? 94.8%. The SemCor data can mainly cover
more test instances of 3 verbs ? launch, rule, and
transfer ? the WN senses of which lack sense
examples or links to other senses in UBY. Un-
like the hand-labelled SemCor data, our automated
sense labelling method is limited to the informa-
tion found in the LLR used. However, there are
also 330 MASC instances covered by the ALC
only. Those are mostly instances of phrasal verbs,
such as rip off and show up. Note that the defini-
tion of coverage we use here makes its values the
upper bounds for the performance of the classifier.
Senseval-3. We also generated training data au-
tomatically for the 305 Senseval verbs. However,
only 152 of those verbs (442 instances) are found
in SemCor. This means we cannot train the classi-
fier for the remaining Senseval verbs. The cover-
age of the SemCor training data for the 152 verbs
which can be classified is 96.15% and that of the
ALC ? 95.25%. For all 592 Senseval test in-
stances, the coverage of the ALC is 90.38%.
4.2.3 Results and Analysis
We trained a separate logistic regression classi-
fier for each test verb in the two datasets us-
ing the WEKA data mining software (Hall et al.,
2009) with default parameters. The classifiers
were trained with features extracted from (i) the
SemCor hand-labelled data and (ii) the ALC.
MASC. The classifier achieves 50.23% accu-
racy when SemCor is used and 49% when the
ALC is employed. The difference in the results is
not statistically significant at p < 0.05. The MFS
73
baseline scores at 41.72%.
Senseval-3. The classifier achieves 43.24%
with the ALC. We assigned the MFS to each of
the 143 test verbs not found in SemCor since we
cannot train the classifier for those. The achieved
accuracy is 45.2%. We also measured accuracy
in a setup where no MFS back-off strategy was
employed for SemCor (152 test verbs with 442
instances). When trained on SemCor data, the
classifier achieves 48.64% accuracy compared to
47.51% for the ALC. All differences in the results
are not statistically significant at p < 0.05. Fi-
nally, the MFS baseline accuracy is significantly
lower at 25.34% for all 305 test verbs.
For both test datasets, the overall performance
of the classifier when trained on automatically la-
belled data is very close to the setting in which
manually created training data is employed. We
thus conclude that the quality of the data produced
by our sense labelling method is sufficient and
these data can be directly used for training a statis-
tical VSD classifier. As a reference, the state-of-
the-art supervised VSD system described in Chen
and Palmer (2009) achieves 64.8% accuracy on the
Senseval-2 fine-grained data. However, we cannot
compare to this result due to the different sense in-
ventory which the Senseval-2 data were annotated
with.
4.2.4 Sense Links
In order to investigate the effect of LLRs, we
performed experiments in which sense examples
found in WN only were used. We also experi-
mented with various combinations of the resources
available in UBY to determine the contribution of
each of those to our method. Table 5 shows the re-
sults. The setting which includes only WN has the
worst performance, thus clearly showing the ben-
efits of using LLRs. Next, the inclusion of WKT
improves both coverage and accuracy. We con-
clude that WKT plays an important role in discov-
ering additional verb senses. Finally, similarly to
the results of the intrinsic evaluation, adding VN
to the mix increases slightly the coverage but de-
creases accuracy.
5 Related Work and Discussion
Our work is related to previous research on
(i) using a combination of lexical resources for
knowledge-based WSD, (ii) using lexical re-
sources for distant supervision, and (iii) the auto-
mated acquisition of sense-annotated data.
MASC Senseval
Cov Acc Cov Acc
WN 0.6573 0.3498 0.6372 0.3209
WN?FN 0.8562 0.4810 0.8812 0.4172
WN?FN?WKT 0.8805 0.4900 0.9038 0.4324
WN?FN?WKT?VN 0.8822 0.4688 0.9139 0.4054
Table 5: Performance of the various combinations
of lexical resources.
Knowledge-based WSD. While the combina-
tion of sense-annotated data and wordnets has
been described for knowledge-based WSD before
(e.g., Navigli and Velardi (2005; Agirre and Soroa
(2009) who use graph algorithms), only recently
Ponzetto and Navigli (2010) have investigated the
impact of the combination of different lexical re-
sources on the performance of WSD. They aligned
WN senses with Wikipedia articles and employed
two simple knowledge-based algorithms, i.e., a
Lesk-based algorithm and a graph-based algo-
rithm, to evaluate the resulting LLR for WSD.
While their evaluation demonstrates that the use
of an LLR boosts the performance of knowledege-
based WSD, it is restricted to nouns only since
Wikipedia provides very few verb senses. More-
over, lexical resources that are rich in lexical-
syntactic information such as VN have not been
involved.
Miller et al. (2012) employ a Lesk-based algo-
rithm which makes use of a combination of WN
and an automatically acquired distributional the-
saurus. Lesk-based algorithms play a central role
in knowledge-based WSD. Based on the overlap
of the context of the target word and sense defi-
nitions in a given sense inventory, they assign the
sense with the highest overlap as disambiguation
result. We were kindly provided with the system
described in Miller et al. (2012) and we were able
to test its performance on our test sets. The sys-
tem achieved only 33.86% and 30.16% accuracy
for the MASC and the Senseval-3 verbs, respec-
tively, which is far below the results we presented.
This low performance is due to the fact that Lesk-
based algorithms do not account for word order.
Such information is important especially for verb
senses, as the syntactic behaviour of a verb reflects
aspects of its meaning.
Distant supervision. Distant supervision is
a learning paradigm similar to semi-supervised
learning. Unlike semi-supervised methods which
typically employ a supervised classifier and a
74
small number of seed instances to do bootstrap
learning (Yarowsky, 1995; Mihalcea, 2004; Fujita
and Fujino, 2011), in distant supervision training
data are created in a single run from scratch by
aligning corpus instances with entries in a knowl-
edge base. Distant supervision methods that have
used LLRs as knowledge bases have been previ-
ously applied in relation extraction, e.g. Freebase
(Mintz et al., 2009; Surdeanu et al., 2012) and Ba-
belNet (Krause et al., 2012; Moro et al., 2013).
However, as far as we are aware, we are the first to
apply distant supervision to the task of verb sense
disambiguation.
Acquisition of sense-annotated data. Most
previous work on using lexical resources for au-
tomatically acquiring sense-annotated data either
was mostly restricted to noun senses or, unlike
us, did not present a meaningful evaluation. Lea-
cock et al. (1998) describe the automated creation
of training data for supervised WSD on the ba-
sis of WN as a lexical resource combined with
corpus statistics, but they evaluate their approach
just on one noun, verb, and adjective, and thus
it is unclear whether their results can be gener-
alized. Cuadros and Rigau (2008) used the ap-
proach of Leacock et al. (1998) to automatically
build a large KnowNet from the Web, but they
evaluated this resource only for WSD of nouns.
However, the system based on KnowNet yields re-
sults below the SemCor-MFS baseline. Mihalcea
and Moldovan (1999) use WordNet glosses to ex-
tract sense examples from the Web via a search en-
gine and use this approach in a subsequent paper
(Mihalcea, 2002) to generate a sense tagged cor-
pus. For five randomly selected nouns, they per-
formed a comparative evaluation of a WSD classi-
fier trained on an automatically tagged corpus on
the one hand, and on the manually annotated data
from the Senseval-2 English lexical sample task
on the other hand. The results obtained for these
five nouns seem to be similar but the dataset used
is too small to draw meaningful conclusions and
moreover, it does not cover verbs. Mostow and
Duan (2011) presented a system that extracts ex-
ample contexts for nouns and apply these contexts
in (Duan and Yates, 2010) for WSD by using them
to label text and train a statistical classifier. An
evaluation of this classifier yielded results similar
to those obtained by a supervised WSD system.
K?ubler and Zhekova (2009) extract example
sentences from several English dictionaries and
various types of corpora, including web corpora.
They employ a Lesk-based algorithm to automati-
cally annotate the target word instances in the ex-
tracted example sentences with WN senses and
use them in one of their experiments as train-
ing data for a WSD classifier. However, the per-
formance of the system decreased significantly
achieving the lowest accuracy among all system
configurations. The authors provide only the over-
all accuracy score, so we do not know how disam-
biguation of verbs was affected.
Summary. We consider the ability to estab-
lish a link between the rich knowledge available in
LLRs and corpora of any kind to be the main ad-
vantage of our automated labelling method. How-
ever, to automatically label a suffcient amount
of data for supervised learning, very large cor-
pora are required. Our method can be extended
to other POS (using sense examples and possibly
other types of lexical information), as well as to
other languages where (linked) lexical resources
are available.
6 Conclusion
In this paper, we presented a novel method for cre-
ating sense labelled corpora automatically. We ex-
ploit LLRs and perform large-scale intrinsic and
application-based evaluations. The results of those
evaluations show that the quality of the sense la-
belled corpora created with our method matches
that of manually annotated corpora.
In future research, we plan to use PropBank
(Palmer et al., 2005) in order to extract sense
examples for VN as well. This might improve
the performance of lexical resource combinations
which include VN. We will also apply our method
to languages (e.g., German) for which lexical re-
sources are available but no or little sense anno-
tated corpora exist.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg- Professor-
ship Program under grant No. I/82806 and by the
German Research Foundation under grant No. GU
798/9-1. We would like to thank the anonymous
reviewers for their valuable feedback.
75
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2009), pages 33?41, Athens, Greece.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics-Volume 1, pages
86?90, Montreal, Canada.
Timothy Baldwin, Sunam Kim, Francis Bond, Sanae
Fujita, David Martinez, and Takaaki Tanaka. 2010.
A Reexamination of MRD-Based Word Sense Dis-
ambiguation. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 9(1):4:1?
4:21.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Jinying Chen and Martha Palmer. 2009. Improv-
ing English verb sense disambiguation performance
with linguistically motivated features and clear sense
distinction boundaries. Language Resources and
Evaluation, 43:181?208.
Montse Cuadros and German Rigau. 2008. Knownet:
Building a large net of knowledge from the web.
In 22nd International Conference on Computational
Linguistics (COLING), pages 161?168, Manchester,
UK.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC 2006), pages 449?454, Genoa,
Italy.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
627?635, Los Angeles, USA.
Judith Eckle-Kohler and Iryna Gurevych. 2012.
Subcat-LMF: Fleshing out a standardized format for
subcategorization frame interoperability. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 550?560, Avignon,
France.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-
mann, Michael Matuschek, and Christian M. Meyer.
2012. UBY-LMF ? A uniform format for standard-
izing heterogeneous lexical-semantic resources in
ISO-LMF. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 275?282, Istanbul, Turkey.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Sanae Fujita and Akinori Fujino. 2011. Word sense
disambiguation by combining labeled data expan-
sion and semi-supervised learning method. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 676?685,
Chiang Mai, Thailand.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 580?590.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42:21?40.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430, Sapporo,
Japan. Association for Computational Linguistics.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 180?183, Edmonton,
Canada.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International Se-
mantic Web Conference, pages 263?278, Boston,
Masachusetts, USA, 11. Springer.
Sandra K?ubler and Desislava Zhekova. 2009. Semi-
Supervised Learning for Word Sense Disambigua-
tion: Quality vs. Quantity. In Proceedings of the
International Conference RANLP-2009, pages 197?
202, Borovets, Bulgaria.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and word-
net relations for sense identification. Computational
Linguistics, 24(1):147?165.
76
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference on
Systems Documentation, pages 24?26, Toronto, On-
tario, Canada.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Rada Mihalcea and Dan Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of the American Association for Ar-
tificial Intelligence (AAAI 1999), Orlando, Florida,
USA.
Rada Mihalcea. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the Third Interna-
tional Conference of Language Resources and Eval-
uation (LREC 2002), pages 1407?1411, Las Palmas,
Canary Islands, Spain.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings
of the Conference on Computational Natural Lan-
guage Learning (CoNLL-2004), Boston, MA, USA.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of the 24th
International Conference on Computational Lin-
guistics (COLING 2012), pages 1781?1796, Mum-
bai, India.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore.
Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu,
Roberto Navigli, and Hans Uszkoreit. 2013. Se-
mantic rule filtering for web-scale relation extrac-
tion. In Proceedings of the 12th International
Semantic Web Conference, Sydney, Australia, 10.
Springer.
Jack Mostow and Weisi Duan. 2011. Generating ex-
ample contexts to illustrate a target word sense. In
Proceedings of the Sixth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 105?110, Portland, Oregon, June. Association
for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1086.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1):71?
105.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1522?1531, Uppsala,
Sweden.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465, Jeju Island, Korea.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd annual meeting on Associa-
tion for Computational Linguistics, pages 189?196,
Cambridge, Massachusetts, USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the 6th
International Conference on Language Resources
and Evaluation (LREC 2008), volume 8, pages
1646?1652, Marrakech, Morocco.
77
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 57?64
Manchester, August 2008
Towards Domain-Independent Deep Linguistic Processing:
Ensuring Portability and Re-Usability of Lexicalised Grammars
Kostadin Cholakov?, Valia Kordoni??, Yi Zhang??
? Department of Computational Linguistics, Saarland University, Germany
? LT-Lab, DFKI GmbH, Germany
{kostadin,kordoni,yzhang}@coli.uni-sb.de
Abstract
In this paper we illustrate and underline
the importance of making detailed linguis-
tic information a central part of the pro-
cess of automatic acquisition of large-scale
lexicons as a means for enhancing robust-
ness and at the same time ensuring main-
tainability and re-usability of deep lexi-
calised grammars. Using the error mining
techniques proposed in (van Noord, 2004)
we show very convincingly that the main
hindrance to portability of deep lexicalised
grammars to domains other than the ones
originally developed in, as well as to ro-
bustness of systems using such grammars
is low lexical coverage. To this effect,
we develop linguistically-driven methods
that use detailed morphosyntactic informa-
tion to automatically enhance the perfor-
mance of deep lexicalised grammars main-
taining at the same time their usually al-
ready achieved high linguistic quality.
1 Introduction
We focus on enhancing robustness and ensur-
ing maintainability and re-usability for a large-
scale deep grammar of German (GG; (Crysmann,
2003)), developed in the framework of Head-
driven Phrase Structure Grammar (HPSG). Specif-
ically, we show that the incorporation of detailed
linguistic information into the process of auto-
matic extension of the lexicon of such a language
resource enhances its performance and provides
linguistically sound and more informative predic-
tions which bring a bigger benefit for the grammar
when employed in practical real-life applications.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
In recent years, various techniques and re-
sources have been developed in order to improve
robustness of deep grammars for real-life applica-
tions in various domains. Nevertheless, low cover-
age of such grammars remains the main hindrance
to their employment in open domain natural lan-
guage processing. (Baldwin et al, 2004), as well
as (van Noord, 2004) and (Zhang and Kordoni,
2006) have clearly shown that the majority of pars-
ing failures with large-scale deep grammars are
caused by missing or wrong entries in the lexicons
accompanying grammars like the aforementioned
ones. Based on these findings, it has become clear
that it is crucial to explore and develop efficient
methods for automated (Deep) Lexical Acquisition
(henceforward (D)LA), the process of automati-
cally recovering missing entries in the lexicons of
deep grammars.
Recently, various high-quality DLA approaches
have been proposed. (Baldwin, 2005), as well
as (Zhang and Kordoni, 2006), (van de Cruys,
2006) and (Nicholson et al, 2008) describe effi-
cient methods towards the task of lexicon acqui-
sition for large-scale deep grammars for English,
Dutch and German. They treat DLA as a classi-
fication task and make use of various robust and
efficient machine learning techniques to perform
the acquisition process.
However, it is our claim that to achieve bet-
ter and more practically useful results, apart from
good learning algorithms, we also need to incorpo-
rate into the learning process fine-grained linguis-
tic information which deep grammars inherently
include and provide for. As we clearly show in
the following, it is not sufficient to only develop
and use good and complicated classification algo-
rithms. We must look at the detailed linguistic in-
formation that is already included and provided for
by the grammar itself and try to capture and make
as much use of it as possible, for this is the infor-
mation we aim at learning when performing DLA.
57
In this way, the learning process is facilitated and
at the same time it is as much as possible ensured
that its outcome be linguistically more informative
and, thus, practically more useful.
We use the GG deep grammar for the work we
present in this paper because German is a language
with rich morphology and free word order, which
exhibits a range of interesting linguistic phenom-
ena, a fair number of which are already analysed in
the GG. Thus, the grammar is a valuable linguistic
resource since it provides linguistically sound and
detailed analyses of these phenomena. Apart from
the interesting syntactic structures, though, the lex-
ical entries in the lexicon of the aforementioned
grammar also exhibit a rich and complicated struc-
ture and contain various important linguistic con-
straints. Based on our claim above, in this pa-
per we show how the information these constraints
provide can be captured and used in linguistically-
motivated DLA methods which we propose here.
We then apply our approach on real-life data and
observe the impact it has on the the grammar cov-
erage and its practical application. In this way we
try to prove our assumption that the linguistic in-
formation we incorporate into our DLA methods
is vital for the good performance of the acquisition
process and for the maintainability and re-usability
of the grammar, as well for its successful practical
application.
The remainder of the paper is organised as fol-
lows. In Section 2 we show that low (lexical) cov-
erage is a serious issue for the GG when employed
for open domain natural language processing. Sec-
tion 3 presents the types in the lexical architecture
of the GG that are considered to be relevant for the
purposes of our experiments. Section 4 describes
the extensive linguistic analysis we perform in or-
der to deal with the linguistic information these
types provide and presents the target type inven-
tory for our DLA methods. Section 5 reports on
statistical approaches towards automatic DLA and
shows the importance of a good and linguistically-
motivated feature selection. Section 6 illustrates
the practical usage of the proposed DLA methods
and their impact on grammar coverage. Section 7
concludes the paper.
2 Coverage Test with the GG
We start off adopting the automated error mining
method described in (van Noord, 2004) for iden-
tification of the major type of errors in the GG.
As an HPSG grammar, the GG is based on typed
feature structures. The GG types are strictly de-
fined within a type hierarchy. The GG also con-
tains constructional and lexical rules and a lexicon
with its entries belonging to lexical types which
are themselves defined again within the type hier-
archy. The grammar originates from (Mu?ller and
Kasper, 2000), but continued to improve after the
end of the Verbmobil project (Wahlster, 2000) and
it currently consists of 5K types, 115 rules and the
lexicon contains approximately 35K entries. These
entries belong to 386 distinct lexical types.
In the experiments we report here two corpora
of different kind and size have been used. The
first one has been extracted from the Frankfurter
Rundschau newspaper and contains about 614K
sentences that have between 5 and 20 tokens. The
second corpus is a subset of the German part of the
Wacky project (Kilgarriff and Grefenstette, 2003).
The Wacky project aims at the creation of large
corpora for different languages, including German,
from various web sources, such as online news-
papers and magazines, legal texts, internet fora,
university and science web sites, etc. The Ger-
man part, named deWaC (Web as Corpus), con-
tains about 93M sentences and 1.65 billion tokens.
The subset used in our experiments is extracted
by randomly selecting 2.57M sentences that have
between 4 and 30 tokens. These corpora have
been chosen because it is interesting to observe
the grammar performance on a relatively balanced
newspaper corpus that does not include so many
long sentences and sophisticated linguistic con-
structions and to compare it with the performance
of the grammar on a random open domain text cor-
pus.
The sentences are fed into the PET HPSG parser
(Callmeier, 2000) with the GG loaded. The parser
has been configured with a maximum edge num-
ber limit of 100K and it is running in the best-only
mode so that it does not exhaustively find all pos-
sible parses. The result of each sentence is marked
as one of the following four cases:
? P means at least one parse is found for the
sentence;
? L means the parser halted after the morpho-
logical analysis and was not able to construct
any lexical item for the input token;
? N means that the parser exhausted the search-
ing and was not able to parse the sentence;
58
? E means the parser reached the maximum
edge number limit and was still not able to
find a parse.
Table 1 shows the results of the experiments
with the two corpora. From these results it can
FR deWaC
Result #Sentences % #Sentences %
P 62,768 10.22% 109,498 4.3%
L 464,112 75.55% 2,328,490 90.5%
N 87,415 14.23% 134,917 5.2%
E 3 ? 14 ?
Total: 614,298 100% 2,572,919 100%
Table 1: Parsing results with the GG and the test
corpora
be seen that the GG has full lexical span for only
a small portion of the sentences? about 25% and
10% for the Frankfurter Rundschau and the deWaC
corpora, respectively. The output of the error min-
ing confirms our assumption that missing lexical
entries are the main problem when it comes to
robust performance of the GG and illustrates the
need for efficient DLA methods.
3 Atomic Lexical Types
Before describing the proposed DLA algorithm,
we should define what exactly is being learnt.
Most of the so called deep grammars are strongly
lexicalised. As mentioned in the previous section,
the GG employs a type inheritance system and its
lexicon has a flat structure with each lexical entry
mapped onto one type in the inheritance hierarchy.
Normally, the types assigned to the lexical entries
are maximal on the type hierarchy, i.e., they do not
have any subtypes. They provide the most specific
information available for this branch of the hierar-
chy. These maximal types which the lexical entries
are mapped onto are called atomic lexical types.
Thus, in our experiment setup, we can define the
lexicon of the grammar as being a one-to-one map-
ping from word stems to atomic lexical types. It is
this mapping which must be automatically learnt
(guessed) by the different DLA methods.
We are interested in learning open-class words,
i.e., nouns, adjectives, verbs and adverbs. We as-
sume that the close-class words are already in the
lexicon or the grammar can handle them through
various lexical rules and they are not crucial for
the grammar performance in real life applications.
Thus, for the purposes of our experiments, we con-
sider only the open-class lexical types. Moreover,
we propose an inventory of open-class lexical types
with sufficient type and token frequency. The type
frequency of a given lexical type is defined as
the number of lexical entries in the lexicon of the
grammar that belong to this type and the token fre-
quency is the number of words in some corpus that
belong to this type.
We use sentences from the Verbmobil corpus
which have been treebanked with the GG in order
to determine the token frequency and to map the
lexemes to their correct entries in the lexicon for
the purposes of the experiment. This set contains
11K sentences and about 73K tokens; this gives an
average of 6.8 words per sentence. The sentences
are taken from spoken dialogues. Hence, they are
not long and most of them do not exhibit interest-
ing linguistic properties which is a clear drawback
but currently there is no other annotated data com-
patible with the GG.
We used a type frequency threshold of 10 entries
in the lexicon and a token frequency threshold of
3 occurrences in the treebanked sentences to form
a list of relevant open-class lexical types. The re-
sulting list contains 38 atomic lexical types with a
total of 32,687 lexical entries.
4 Incorporation of Linguistic Features
However, in the case of the GG this type inventory
is not a sufficient solution. As already mentioned,
in the lexicon of the grammar much of the relevant
linguistic information is encoded not in the type
definition itself but in the form of constraints in the
feature structures of the various types. Moreover,
given that German has a rich morphology, a given
attribute may have many different values among
lexical entries of the same type and it is crucial for
the DLA process to capture all the different com-
binations. That is why we expand the identified
38 atomic lexical type definitions by including the
values of various features into them.
By doing this, we are trying to facilitate the
DLA process because, in that way, it can ?learn?
to differentiate not only the various lexical types
but also significant morphosyntactic differences
among entries that belong to the same lexical type.
That gives the DLA methods access to much more
linguistic information and they are able to apply
more linguistically fine-tuned classification crite-
ria when deciding which lexical type the unknown
word must be assigned to. Furthermore, we en-
sure that the learning process deliver linguistically
59
Feature Values Meaning
SUBJOPT (subject options)
+ in some cases the article for the noun can be omitted
- the noun always goes with an article
+ raising verb
- non-raising verb
KEYAGR (key agreement)
? case-number-gender information for nouns
c-s-n underspecified-singular-neutral
c-p-g underspecified-plural-underspecified
... ...
(O)COMPAGR ((oblique) a-n-g, d-n-g, etc. case-number-gender information
complement ? for (oblique) verb complements
agreement ? case-number-gender of the modified noun (for adjectives)
(O)COMPTOPT ((oblique) ? verbs can take a different number of complements
complement + the respective (oblique) complement is present
options - the respective (oblique) complement is absent
KEYFORM
? the auxiliary verb used for the formation of perfect tense
haben the auxiliary verb is ?haben?
sein the auxiliary verb is ?sein?
Table 2: Relevant features used for type expansion
plausible, precise and more practically useful re-
sults. The more the captured and used linguistic
information is, the better and more useful the DLA
results will be.
However, we have to avoid creating data sparse
problems. We do so by making the assumption
that not every feature could really contribute to the
classification process and by filtering out these fea-
tures that we consider irrelevant for the enhance-
ment of the DLA task. Naturally, the question
which features are to be considered relevant arises.
After performing an extensive linguistic analysis,
we have decided to take the features shown in Ta-
ble 2 into account.
We have thoroughly analysed each of these fea-
tures and selected them on the basis of their lin-
guistic meaning and their significance and contri-
bution to the DLA process. The SUBJOPT fea-
ture can be used to differentiate among nouns that
have a similar morphosyntactic behaviour but dif-
fer only in the usage of articles; 4 out of the consid-
ered 9 noun atomic lexical types do not define this
feature. Furthermore, using this feature, we can
also refine our classification within a single atomic
lexical type. For example, the entry ?adresse-n?
(address) of the type ?count-noun-le?1 has ?-? for
the SUBJOPT value, whereas the value for the en-
try ?anbindung-n? (connection) of the same type is
?+?:
(1) a. Das
det.NEUT.NOM
Hotel
hotel
hat
have.3PER.SG
gute
good
Anbindung
connection
an
to
die
det.PL.ACC
o?ffentlichen
public
1count noun lexeme; all lexical entries in the lexicon end
with le which stands for lexeme.
Verkehrsmittel.
transportation means
?The hotel has a good connection to public
transportation.?
b. Die
det.FEM.NOM
Anbindung
connection
an
to
Rom
Rome
mit
with
dem
det.MASC.DAT
Zug
train
ist
be.3PER.SG
gut.
good
?The train connection to Rome is good.?
The distinction between raising and non-raising
verbs that this feature expresses is also an impor-
tant contribution to the classification process.
The case-number-gender data the KEYAGR and
(O)COMPAGR features provide allows for a bet-
ter usage of morphosyntactic information for the
purposes of DLA. Based on this data, the classifi-
cation method is able to capture words with sim-
ilar morphosyntactic behaviour and give various
indications for their syntactic nature; for instance,
if the word is a subject, direct or indirect object.
This is especially relevant and useful for languages
with rich morphology and relatively free word or-
der such as German. The same is also valid for
the (O)COMPOPT and KEYFORM features? they
allow the DLA method to successfully learn and
classify verbs with similar syntactic properties.
The values of the features are just attached to the
old type name to form a new type definition. In this
way, we ?promote? them and these features are now
part of the type hierarchy of the grammar which
makes them accessible for the DLA process since
this operates on the type level. For example, the
original type of the entry for the noun ?abenteuer?
(adventure):
abenteuer-n := count-noun-le &
[ [ --SUBJOPT -,
60
KEYAGR c-n-n,
KEYREL "_abenteuer_n_rel",
KEYSORT situation,
MCLASS nclass-2_-u_-e ] ].
will become abenteuer-n := count-noun-le - c-n-
n when we incorporate the values of the features
SUBJOPT and KEYAGR into the original type
definition. The new expanded type inventory is
shown in Table 3.
Original Expanded
lexicon lexicon
Number of lexical types 386 485
Atomic lexical types 38 137
-nouns 9 72
-verbs 19 53
-adjectives 3 5
-adverbs 7 7
Table 3: Expanded atomic lexical types
The features we have ignored do not contribute
to the learning process and are likely to cre-
ate sparse data problems. The (O)COMPFORM
((oblique) complement form) features which de-
note dependent to verbs prepositions are not con-
sidered to be relevant. An example of OCOMP-
FORM is the lexical entry ?begru?nden mit-v? (jus-
tify with) where the feature has the preposition
?mit? (with) as its value. Though for German
prepositions can be considered as case markers, the
DLA has already a reliable access to case informa-
tion through the (O)COMPAGR features. More-
over, a given dependent preposition is distributed
across many types and it does not indicate clearly
which type the respective verb belongs to.
The same is valid for the feature VCOPMFORM
(verb complement form) that denotes the separa-
ble particle (if present) of the verb in question.
An example of this feature is the lexical entry
?abdecken-v? (to cover) where VCOMPFORM has
the separable particle ?ab? as its value. However,
treating such discontinuous verb-particle combina-
tions as a lexical unit could help for the acquisi-
tion of subcategorizational frames. For example,
anho?ren (to listen to someone/something) takes an
accusative NP as argument, zuho?ren (to listen to)
takes a dative NP and aufho?ren (to stop, to termi-
nate) takes an infinitival complement. Thus, ignor-
ing VCOMPFORM could be a hindrance for the
acquisition of some verb types2.
We have also tried to incorporate some sort of
semantic information into the expanded atomic
2We thank the anonymous reviewer who pointed this out
for us.
lexical type definitions by also attaching the
KEYSORT semantic feature to them. KEYSORT
defines a certain situation semantics category
(?anything?, ?action sit?, ?mental sit?) which the
lexical entry belongs to. However, this has caused
again a sparse data problem because the semantic
classification is too specific and, thus, the number
of possible classes is too large. Moreover, seman-
tic classification is done based on completely dif-
ferent criteria and it cannot be directly linked to the
morphosyntactic features. That is why we have fi-
nally excluded this feature, as well.
Armed with this elaborate target type inventory,
we now proceed with the DLA experiments for the
GG.
5 DLA Experiments with the GG
For our DLA experiments, we adopted the Max-
imum Entropy based model described in (Zhang
and Kordoni, 2006), which has been applied to the
ERG (Copestake and Flickinger, 2000), a wide-
coverage HPSG grammar for English. For the pro-
posed prediction model, the probability of a lexical
type t given an unknown word and its context c is:
(2) p(t|c) = exp(
?
i
?
i
f
i
(t,c))
?
t
?
?T
exp(
?
i
?
i
f
i
(t
?
,c))
where f
i
(t, c) may encode arbitrary characteristics
of the context and ?
i
is a weighting factor esti-
mated on a training corpus. Our experiments have
been performed with the feature set shown in Table
4.
Features
the prefix of the unknown word
(length is less or equal 4)
the suffix of the unknown word
(length is less or equal 4)
the 2 words before and after the unknown word
the 2 types before and after the unknown word
Table 4: Features for the DLA experiment
We have also experimented with prefix and suf-
fix lengths up to 3. To evaluate the contribution
of various features and the overall precision of the
ME-based unknown word prediction model, we
have done a 10-fold cross validation on the Verb-
mobil treebanked data. For each fold, words that
do not occur in the training partition are assumed
to be unknown and are temporarily removed from
the lexicon.
For comparison, we have also built a baseline
model that always assigns a majority type to each
61
unknown word according to its POS tag. Specifi-
cally, we tag the input sentence with a small POS
tagset. It is then mapped to a most popular lexi-
cal type for that POS. Table 5 shows the relevant
mappings.
POS Majority lexical type
noun count-noun-le - c-n-f
verb trans-nerg-str-verb-le haben-auxf
adj adj-non-prd-le
adv intersect-adv-le
Table 5: POS tags to lexical types mapping
Again for comparison, we have built another
simple baseline model using the TnT POS tagger
(Brants, 2000). TnT is a general-purpose HMM-
based trigram tagger. We have trained the tagging
models with all the lexical types as the tagset. The
tagger tags the whole sentence but only the output
tags for the unknown words are taken to generate
lexical entries and to be considered for the eval-
uation. The precisions of the different prediction
models are given in Table 6.
The baseline achieves a precision of about 38%
and the POS tagger outperforms it by nearly 10%.
These results can be explained by the nature of the
Verbmobil data. The vast majority of the adjec-
tives and the adverbs in the sentences belong to
the majority types shown in Table 5 and, thus, the
baseline model assigns the correct lexical types to
almost every adjective and adverb, which brings
up the overall precision. The short sentence length
facilitates the tagger extremely, for TnT, as an
HMM-based tagger, makes predictions based on
the whole sentence. The longer the sentences are,
the more challenging the tagging task for TnT is.
The results of these models clearly show that the
task of unknown word type prediction for deep
grammars is non-trivial.
Our ME-based models give the best results in
terms of precision. However, verbs and adverbs
remain extremely difficult for classification. The
simple morphological features we use in the ME
model are not good enough for making good pre-
dictions for verbs. Morphology cannot capture
such purely syntactic features as subcategoriza-
tional frames, for example.
While the errors for verbs are pretty random,
there is one major type of wrong predictions for
adverbs. Most of them are correctly predicted as
such but they receive the majority type for adverbs,
namely ?intersect-adv-le?. Since most of the ad-
verbs in the Verbmobil data we are using belong
to the majority adverb type, the predictor is biased
towards assigning it to the unknown words which
have been identified as adverbs.
The results in the top half of the Table 6 show
that morphological features are already very good
for predicting adjectives. In contrast with ad-
verbs, adjectives occur in pretty limited number of
contexts. Moreover, when dealing with morpho-
logically rich languages such as German, adjec-
tives are typically marked by specific affixes cor-
responding to a specific case-number-gender com-
bination. Since we have incorporated this kind of
linguistic information into our target lexical type
definitions, this significantly helps the prediction
process based on morphological features.
Surprisingly, nouns seem to be hard to learn.
Apparently, the vast majority of the wrong pre-
dictions have been made for nouns that belong to
the expanded variants of the lexical type ?count-
noun-le? which is also the most common non-
expanded lexical type for nouns in the original lex-
icon. Many nouns have been assigned the right lex-
ical type except for the gender:
(3) Betrieb (business, company, enterprise)
prediction: count-noun-le - c-n-n
correct type: count-noun-le - c-n-m
According to the strict exact-match evaluate mea-
sure we use, such cases are considered to be errors
because the predicted lexical type does not match
the type of the lexical entry in the lexicon.
The low numbers for verbs and adverbs show
clearly that we also need to incorporate some sort
of syntactic information into the prediction model.
We adopt the method described in (Zhang and Ko-
rdoni, 2006) where the disambiguation model of
the parser is used for this purpose. We also believe
that the kind of detailed morphosyntactic informa-
tion which the learning process now has access
to would facilitate the disambiguation model be-
cause the input to the model is linguistically more
fine-grained. In another DLA experiment we let
PET use the top 3 predictions provided by the lex-
ical type predictor in order to generate sentence
analyses. Then we use the disambiguation model,
trained on the Verbmobil data, to choose the best
one of these analyses and the corresponding lexical
entry is taken to be the final result of the prediction
process.
As shown in the last line of Table 6, we achieve
an increase of 19% which means that in many
cases the correct lexical type has been ranked sec-
62
Model Precision Nouns Adjectives Verbs Adverbs
Baseline 37.89% 27.03% 62.69% 33.57% 67.14%
TnT 47.53% 53.76% 74.52% 26.94% 32.68%
ME(affix length=3) 51.2% 48.25% 75.41% 44.06% 44.13%
ME(affix length=4) 54.63% 53.55% 76.79% 47.10% 43.55%
ME + disamb. 73.54% 75% 88.24% 65.98% 65.90%
Table 6: Precision of unknown word type predictors
ond or third by the predictor. This proves that
the expanded lexical types improve also the perfor-
mance of the disambiguation model and allow for
its successful application for the purposes of DLA.
It also shows, once again, the importance of the
morphology in the case of the GG and proves the
rightness of our decision to expand the type defini-
tions with detailed linguistic information.3
6 Practical Application
Since our main claim in this paper is that for
good and practically useful DLA, which at the
same time may facilitate robustness and ensure
maintainability and re-usability of deep lexicalised
grammars, we do not only need good machine
learning algorithms but also classification and fea-
ture selection that are based on an extensive lin-
guistic analysis, we apply our DLA methods to real
test data. We believe that due to our expanded lex-
ical type definitions, we provide much more lin-
guistically accurate predictions. With this type of
predictions, we anticipate a bigger improvement of
the grammar coverage and accuracy for the pre-
diction process delivers much more linguistically
relevant information which facilitates parsing with
the GG.
We have conducted experiments with PET and
the two corpora we have used for the error mining
to determine whether we can improve coverage by
using our DLA method to predict the types of un-
known words online. We have trained the predic-
tor on the whole set of treebanked sentences and
extracted a subset of 50K sentences from each cor-
pus. Since lexical types are not available for these
sentences, we have used POS tags instead as fea-
tures for our prediction model. Coverage is mea-
sured as the number of sentences that received at
least one parse and accuracy is measured as the
number of sentences that received a correct analy-
sis. The results are shown in Table 7.
The coverage for FR improves with more than
12% and the accuracy number remains almost the
3Another reason for this high result is the short average
length of the treebanked sentences which facilitates the dis-
ambiguation model of the parser.
Parsed Corpus Coverage Accuracy
FR with the vanilla version GG 8.89% 85%
FR with the GG + DLA 21.08% 83%
deWaC with the vanilla version GG 7.46% ?
deWaC with the GG + DLA 16.95% ?
Table 7: Coverage results
same. Thus, with our linguistically-oriented DLA
method, we have managed to increase parsing cov-
erage and at the same time to preserve the high
accuracy of the grammar. It is also interesting to
note the increase in coverage for the deWaC cor-
pus. It is about 10%, and given the fact that deWaC
is an open and unbalanced corpus, this is a clear
improvement. However, we do not measure ac-
curacy on the deWaC corpus because many sen-
tences are not well formed and the corpus itself
contains much ?noise?. Still, these results show
that the incorporation of detailed linguistic infor-
mation in the prediction process contributed to the
parser performance and the robustness of the gram-
mar without harming the quality of the delivered
analyses.
7 Conclusion
In this paper, we have tackled from a more
linguistically-oriented point of view the lexicon
acquisition problem for a large-scale deep gram-
mar for German, developed in HPSG. We have
shown clearly that missing lexical entries are the
main cause for parsing failures and, thus, illus-
trated the importance of increasing the lexical cov-
erage of the grammar. The target type inventory
for the learning process has been developed in a
linguistically motivated way in an attempt to cap-
ture significant morphosyntactic information and,
thus, achieve a better performance and more prac-
tically useful results.
With the proposed DLA approach and our elab-
orate target type inventory we have achieved nearly
75% precision and this way we have illustrated the
importance of fine-grained linguistic information
for the lexical prediction process. In the end, we
have shown that with our linguistically motivated
DLA methods, the parsing coverage of the afore-
63
mentioned deep grammar improves significantly
while its linguistic quality remains intact.
The conclusion, therefore, is that it is vital to
be able to capture linguistic information and suc-
cessfully incorporate it in DLA processes, for it
facilitates deep grammars and makes processing
with them much more robust for applications. At
the same time, the almost self-evident portability
to new domains and the re-usability of the gram-
mar for open domain natural language processing
is significantly enhanced.
The DLA method we propose can be used as
an external module that can help the grammar be
ported and operate on different domains. Thus,
specifically in the case of HPSG, DLA can also
be seen as a way for achieving more modular-
ity in the grammar. Moreover, in a future re-
search, the proposed kind of DLA might also be
used in order to facilitate the division and transi-
tion from a core deep grammar with a core lex-
icon towards subgrammars with domain specific
lexicons/lexical constraints in a linguistically mo-
tivated way. The use of both these divisions nat-
urally leads to a highly modular structure of the
grammar and the system using the grammar, which
at the same time helps in controlling its complex-
ity.
Our linguistically motivated approach provides
fine-grained results that can be used in a number
of different ways. It is a valuable linguistic tool
and it is up to the grammar developer to choose
how to use the many opportunities it provides.
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Proceedings of the Fourth Internation Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lisbon,
Portugal.
Baldwin, Timothy. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisi-
tion, pages 67?76, Ann Arbor, USA.
Brants, Thorsten. 2000. TnT- a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Applied
Natural Language Processing ANLP-2000, Seattle, WA,
USA.
Callmeier, Ulrich. 2000. PET- a platform for experimenta-
tion with efficient HPSG processing techniques. In Jour-
nal of Natural Language Engineering, volume 6(1), pages
99?108.
Copestake, Ann and Dan Flickinger. 2000. An open-sourse
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the Sec-
ond conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Crysmann, Berthold. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings of
RANLP 2003, pages 112?116, Borovets, Bulgaria.
Kilgarriff, Adam and G Grefenstette. 2003. Introduction to
the special issue on the web as corpus. Computational Lin-
guistics, 29:333?347.
Mu?ller, Stephan and Walter Kasper. 2000. HPSG analysis of
German. In Wahlster, Wolfgang, editor, Verbmobil: Foun-
dations of Speech-to-Speech Translation, pages 238?253.
Springer-Verlag.
Nicholson, Jeremy, Valia Kordoni, Yi Zhang, Timothy Bald-
win, and Rebecca Dridan. 2008. Evaluating and extend-
ing the coverage of HPSG grammars. In In proceedings of
LREC, Marrakesh, Marocco.
van de Cruys, Tim. 2006. Automatically extending the lexi-
con for parsing. In Huitink, Janneke and Sophia Katrenko,
editors, Proceedings of the Student Session of the Euro-
pean Summer School in Logic, Language and Information
(ESSLLI), pages 180?191, Malaga, Spain.
van Noord, Gertjan. 2004. Error mining for wide coverage
grammar engineering. In Proceedings of the 42nd Meeting
of the Assiciation for Computational Linguistics (ACL?04),
Main Volume, pages 446?453, Barcelona, Spain.
Wahlster, Wolfgang, editor. 2000. Verbmobil: Foundations
of Speech-to-Speech Translation. Artificial Intelligence.
Springer.
Zhang, Yi and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open text processing. In Proceed-
ings of the Fifth International Conference on Language
Resourses and Evaluation (LREC 2006), Genoa, Italy.
64
