Proceedings of the 43rd Annual Meeting of the ACL, pages 50?57,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Aggregation improves learning:
experiments in natural language generation for intelligent tutoring systems
Barbara Di Eugenio and Davide Fossati and Dan Yu
University of Illinois
Chicago, IL, 60607, USA
{bdieugen,dfossa1,dyu6}@uic.edu
Susan Haller
University of Wisconsin - Parkside
Kenosha, WI 53141, USA
haller@cs.uic.edu
Michael Glass
Valparaiso University
Valparaiso, IN, 46383, USA
Michael.Glass@valpo.edu
Abstract
To improve the interaction between students
and an intelligent tutoring system, we devel-
oped two Natural Language generators, that we
systematically evaluated in a three way com-
parison that included the original system as
well. We found that the generator which intu-
itively produces the best language does engen-
der the most learning. Specifically, it appears
that functional aggregation is responsible for
the improvement.
1 Introduction
The work we present in this paper addresses three
issues: evaluation of Natural Language Generation
(NLG) systems, the place of aggregation in NLG,
and NL interfaces for Intelligent Tutoring Systems.
NLG systems have been evaluated in various
ways, such as via task efficacy measures, i.e., mea-
suring how well the users of the system perform on
the task at hand (Young, 1999; Carenini and Moore,
2000; Reiter et al, 2003). We also employed task
efficacy, as we evaluated the learning that occurs
in students interacting with an Intelligent Tutoring
System (ITS) enhanced with NLG capabilities. We
focused on sentence planning, and specifically, on
aggregation. We developed two different feedback
generation engines, that we systematically evaluated
in a three way comparison that included the orig-
inal system as well. Our work is novel for NLG
evaluation in that we focus on one specific com-
ponent of the NLG process, aggregation. Aggrega-
tion pertains to combining two or more of the mes-
sages to be communicated into one sentence (Reiter
and Dale, 2000). Whereas it is considered an es-
sential task of an NLG system, its specific contri-
butions to the effectiveness of the text that is even-
tually produced have rarely been assessed (Harvey
and Carberry, 1998). We found that syntactic aggre-
gation does not improve learning, but that what we
call functional aggregation does. Further, we ran a
controlled data collection in order to provide a more
solid empirical base for aggregation rules than what
is normally found in the literature, e.g. (Dalianis,
1996; Shaw, 2002).
As regards NL interfaces for ITSs, research on the
next generation of ITSs (Evens et al, 1993; Litman
et al, 2004; Graesser et al, 2005) explores NL as
one of the keys to bridging the gap between cur-
rent ITSs and human tutors. However, it is still not
known whether the NL interaction between students
and an ITS does in fact improve learning. We are
among the first to show that this is the case.
We will first discuss DIAG, the ITS shell we are
using, and the two feedback generators that we de-
veloped, DIAG-NLP1and DIAG-NLP2 . Since the
latter is based on a corpus study, we will briefly de-
scribe that as well. We will then discuss the formal
evaluation we conducted and our results.
2 Natural Language Generation for DIAG
DIAG (Towne, 1997) is a shell to build ITSs based
on interactive graphical models that teach students to
troubleshoot complex systems such as home heating
and circuitry. A DIAG application presents a student
with a series of troubleshooting problems of increas-
ing difficulty. The student tests indicators and tries
to infer which faulty part (RU) may cause the abnor-
mal states detected via the indicator readings. RU
stands for replaceable unit, because the only course
of action for the student to fix the problem is to re-
place faulty components in the graphical simulation.
50
Figure 1: The furnace system
Fig. 1 shows the furnace, one subsystem of the home
heating system in our DIAG application. Fig. 1 in-
cludes indicators such as the gauge labeled Water
Temperature, RUs, and complex modules (e.g., the
Oil Burner) that contain indicators and RUs. Com-
plex components are zoomable.
At any point, the student can consult the tutor
via the Consult menu (cf. the Consult button in
Fig. 1). There are two main types of queries: Con-
sultInd(icator) and ConsultRU. ConsultInd queries
are used mainly when an indicator shows an ab-
normal reading, to obtain a hint regarding which
RUs may cause the problem. DIAG discusses the
RUs that should be most suspected given the symp-
toms the student has already observed. ConsultRU
queries are mainly used to obtain feedback on the di-
agnosis that a certain RU is faulty. DIAG responds
with an assessment of that diagnosis and provides
evidence for it in terms of the symptoms that have
been observed relative to that RU.
The original DIAG system (DIAG-orig) uses very
simple templates to assemble the text to present to
the student. The top parts of Figs. 2 and 3 show the
replies provided by DIAG-orig to a ConsultInd on
the Visual Combustion Check, and to a ConsultRu
on the Water Pump.
The highly repetitive feedback by DIAG-orig
screams for improvements based on aggregation
techniques. Our goal in developing DIAG-NLP1
and DIAG-NLP2 was to assess whether simple,
rapidly deployable NLG techniques would lead to
measurable improvements in the student?s learning.
Thus, in both cases it is still DIAG that performs
content determination, and provides to DIAG-NLP1
and DIAG-NLP2 a file in which the facts to be com-
municated are written ? a fact is the basic unit of
information that underlies each of the clauses in a
reply by DIAG-orig . The only way we altered the
interaction between student and system is the ac-
tual language that is presented in the output win-
dow. In DIAG-NLP1 we mostly explored using syn-
tactic aggregation to improve the feedback, whereas
DIAG-NLP2 is corpus-based and focuses on func-
tional aggregation. In both DIAG-NLP1 and DIAG-
NLP2 , we use EXEMPLARS (White and Cald-
well, 1998), an object-oriented, rule-based genera-
tor. The rules (called exemplars) are meant to cap-
ture an exemplary way of achieving a communica-
tive goal in a given context. EXEMPLARS selects
rules by traversing the exemplar specialization hi-
erarchy and evaluating the applicability conditions
associated with each exemplar.
The visual combustion check is igniting which is abnormal
(normal is combusting).
Oil Nozzle always
produces this abnormality when it fails.
Oil Supply Valve always
produces this abnormality when it fails.
Oil pump always
produces this abnormality when it fails.
Oil Filter always
produces this abnormality when it fails.
System Control Module sometimes
produces this abnormality when it fails.
Ignitor Assembly never
produces this abnormality when it fails.
Burner Motor always
produces this abnormality when it fails.
The visual combustion check indicator is igniting.
This is abnormal.
Normal is combusting.
Within the furnace system,
this is sometimes caused if
the System Control Module has failed.
Within the Oil Burner
this is never caused if
the Ignitor Assembly has failed.
In contrast, this is always caused if
the Burner Motor, Oil Filter, Oil Pump,
Oil Supply Valve, or Oil Nozzle has failed.
The combustion is abnormal.
In the oil burner, check the units along the path of the oil and
the burner motor.
Figure 2: Answers to ConsultInd by DIAG-orig ,
DIAG-NLP1and DIAG-NLP2
51
Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
Water pump sound was normal.
This normal indication never results when this unit fails.
Visual combustion check was igniting.
This abnormal indication never results when this unit fails.
Burner Motor RMP Gauge was 525.
This normal indication never results when this unit fails.
The Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
The following indicators never display normally
when this unit fails.
Within the furnace system,
the Burner Motor RMP Gauge is 525.
Within the water pump and safety cutoff valve,
the water pump sound indicator is normal.
The following indicators never display abnormally
when this unit fails.
Within the fire door sight hole,
the visual combustion check indicator is igniting.
The water pump is a poor suspect since the water pump
sound is ok.
You have seen that the combustion is abnormal.
Check the units along the path of the oil and the electrical
devices.
Figure 3: Answers to ConsultRu by DIAG-orig ,
DIAG-NLP1 and DIAG-NLP2
2.1 DIAG-NLP1 : Syntactic aggregation
DIAG-NLP1 1 (i) introduces syntactic aggregation
(Dalianis, 1996; Huang and Fiedler, 1996; Reape
and Mellish, 1998; Shaw, 2002) and what we call
structural aggregation, namely, grouping parts ac-
cording to the structure of the system; (ii) gener-
ates some referring expressions; (iii) models a few
rhetorical relations; and (iv) improves the format of
the output.
The middle parts of Figs. 2 and 3 show the revised
output produced by DIAG-NLP1 . E.g., in Fig. 2 the
RUs of interest are grouped by the system modules
that contain them (Oil Burner and Furnace System),
and by the likelihood that a certain RU causes the
observed symptoms. In contrast to the original an-
swer, the revised answer highlights that the Ignitor
Assembly cannot cause the symptom.
In DIAG-NLP1 , EXEMPLARS accesses the
SNePS Knowledge Representation and Reasoning
System (Shapiro, 2000) for static domain informa-
tion.2 SNePS makes it easy to recognize structural
1DIAG-NLP1 actually augments and refines the first feed-
back generator we created for DIAG, DIAG-NLP0 (Di Eugenio
et al, 2002). DIAG-NLP0 only covered (i) and (iv).
2In DIAG, domain knowledge is hidden and hardly acces-
similarities and use shared structures. Using SNePS,
we can examine the dimensional structure of an ag-
gregation and its values to give preference to aggre-
gations with top-level dimensions that have fewer
values, to give summary statements when a dimen-
sion has many values that are reported on, and to
introduce simple text structuring in terms of rhetor-
ical relations, inserting relations like contrast and
concession to highlight distinctions between dimen-
sional values (see Fig. 2, middle).
DIAG-NLP1 uses the GNOME algorithm (Kib-
ble and Power, 2000) to generate referential expres-
sions. Importantly, using SNePS propositions can
be treated as discourse entities, added to the dis-
course model and referred to (see This is ... caused
if ... in Fig. 2, middle). Information about lexical
realization, and choice of referring expression is en-
coded in the appropriate exemplars.
2.2 DIAG-NLP2 : functional aggregation
In the interest of rapid prototyping, DIAG-NLP1
was implemented without the benefit of a corpus
study. DIAG-NLP2 is the empirically grounded
version of the feedback generator. We collected
23 tutoring interactions between a student using the
DIAG tutor on home heating and two human tutors,
for a total of 272 tutor turns, of which 235 in re-
ply to ConsultRU and 37 in reply to ConsultInd (the
type of student query is automatically logged). The
tutor and the student are in different rooms, sharing
images of the same DIAG tutoring screen. When
the student consults DIAG, the tutor sees, in tabular
form, the information that DIAG would use in gen-
erating its advice ? the same ?fact file? that DIAG
gives to DIAG-NLP1and DIAG-NLP2? and types
a response that substitutes for DIAG?s. The tutor is
presented with this information because we wanted
to uncover empirical evidence for aggregation rules
in our domain. Although we cannot constrain the tu-
tor to mention only the facts that DIAG would have
communicated, we can analyze how the tutor uses
the information provided by DIAG.
We developed a coding scheme (Glass et al,
2002) and annotated the data. As the annotation was
performed by a single coder, we lack measures of
intercoder reliability. Thus, what follows should be
taken as observations rather than as rigorous find-
ings ? useful observations they clearly are, since
sible. Thus, in both DIAG-NLP1 and DIAG-NLP2 we had to
build a small knowledge base that contains domain knowledge.
52
DIAG-NLP2 is based on these observations and its
language fosters the most learning.
Our coding scheme focuses on four areas. Fig. 4
shows examples of some of the tags (the SCM is the
System Control Module). Each tag has from one to
five additional attributes (not shown) that need to be
annotated too.
Domain ontology. We tag objects in the domain
with their class indicator, RU and their states, de-
noted by indication and operationality, respectively.
Tutoring actions. They include (i) Judgment. The
tutor evaluates what the student did. (ii) Problem
solving. The tutor suggests the next course of ac-
tion. (iii) The tutor imparts Domain Knowledge.
Aggregation. Objects may be functional aggre-
gates, such as the oil burner, which is a system com-
ponent that includes other components; linguistic
aggregates, which include plurals and conjunctions;
or a summary over several unspecified indicators or
RUs. Functional/linguistic aggregate and summary
tags often co-occur, as shown in Fig. 4.
Relation to DIAG?s output. Contrary to all other
tags, in this case we annotate the input that DIAG
gave the tutor. We tag its portions as included / ex-
cluded / contradicted, according to how it has been
dealt with by the tutor.
Tutors provide explicit problem solving directions
in 73% of the replies, and evaluate the student?s ac-
tion in 45% of the replies (clearly, they do both in
28% of the replies, as in Fig. 4). As expected, they
are much more concise than DIAG, e.g., they never
mention RUs that cannot or are not as likely to cause
a certain problem, such as, respectively, the ignitor
assembly and the SCM in Fig. 2.
As regards aggregation, 101 out of 551 RUs, i.e.
18%, are labelled as summary; 38 out of 193 indica-
tors, i.e. 20%, are labelled as summary. These per-
centages, though seemingly low, represent a consid-
erable amount of aggregation, since in our domain
some items have very little in common with others,
and hence cannot be aggregated. Further, tutors ag-
gregate parts functionally rather than syntactically.
For example, the same assemblage of parts, i.e., oil
nozzle, supply valve, pump, filter, etc., can be de-
scribed as the other items on the fuel line or as the
path of the oil flow.
Finally, directness ? an attribute on the indica-
tor tag ? encodes whether the tutor explicitly talks
about the indicator (e.g., The water temperature
gauge reading is low), or implicitly via the object
to which the indicator refers (e.g., the water is too
cold). 110 out of 193 indicators, i.e. 57%, are
marked as implicit, 45, i.e. 41%, as explicit, and 2%
are not marked for directness (the coder was free to
leave attributes unmarked). This, and the 137 occur-
rences of indication, prompted us to refer to objects
and their states, rather than to indicators (as imple-
mented by Steps 2 in Fig. 5, and 2(b)i, 3(b)i, 3(c)i in
Fig. 6, which generate The combustion is abnormal
and The water pump sound is OK in Figs. 2 and 3).
2.3 Feedback Generation in DIAG-NLP2
In DIAG-NLP1 the fact file provided by DIAG is
directly processed by EXEMPLARS. In contrast, in
DIAG-NLP2 a planning module manipulates the in-
formation before passing it to EXEMPLARS. This
module decides which information to include ac-
cording to the type of query the system is respond-
ing to, and produces one or more Sentence Structure
objects. These are then passed to EXEMPLARS
that transforms them into Deep Syntactic Structures.
Then, a sentence realizer, RealPro (Lavoie and Ram-
bow, 1997), transforms them into English sentences.
Figs. 5 and 6 show the control flow in DIAG-
NLP2 for feedback generation for ConsultInd and
ConsultRU. Step 3a in Fig. 5 chooses, among all
the RUs that DIAG would talk about, only those
that would definitely result in the observed symp-
tom. Step 2 in the AGGREGATE procedure in Fig. 5
uses a simple heuristic to decide whether and how to
use functional aggregation. For each RU, its possi-
ble aggregators and the number n of units it covers
are listed in a table (e.g., electrical devices covers
4 RUs, ignitor, photoelectric cell, transformer and
burner motor). If a group of REL-RUs contains k
units that a certain aggregator Agg covers, if k < n2 ,
Agg will not be used; if n2 ? k < n, Agg preceded
by some of will be used; if k = n, Agg will be used.
DIAG-NLP2 does not use SNePS, but a relational
database storing relations, such as the ISA hierarchy
(e.g., burner motor IS-A RU), information about ref-
erents of indicators (e.g., room temperature gauge
REFERS-TO room), and correlations between RUs
and the indicators they affect.
3 Evaluation
Our empirical evaluation is a three group, between-
subject study: one group interacts with DIAG-orig ,
53
[judgment [replaceable?unit the ignitor] is a poor suspect] since [indication combustion is working] during startup. The problem is
that the SCM is shutting the system off during heating.
[domain?knowledge The SCM reads [summary [linguistic?aggregate input signals from sensors]] and uses the signals to determine
how to control the system.]
[problem?solving Check the sensors.]
Figure 4: Examples of a coded tutor reply
1. IND? queried indicator
2. Mention the referent of IND and its state
3. IF IND reads abnormal,
(a) REL-RUs? choose relevant RUs
(b) AGGR-RUs? AGGREGATE(REL-RUs)
(c) Suggest to check AGGR-RUs
AGGREGATE(RUs)
1. Partition REL-RUs into subsets by system structure
2. Apply functional aggregation to subsets
Figure 5: DIAG-NLP2 : Feedback generation for
ConsultInd
one with DIAG-NLP1 , one with DIAG-NLP2 . The
75 subjects (25 per group) were all science or engi-
neering majors affiliated with our university. Each
subject read some short material about home heat-
ing, went through one trial problem, then continued
through the curriculum on his/her own. The curricu-
lum consisted of three problems of increasing dif-
ficulty. As there was no time limit, every student
solved every problem. Reading materials and cur-
riculum were identical in the three conditions.
While a subject was interacting with the system,
a log was collected including, for each problem:
whether the problem was solved; total time, and time
spent reading feedback; how many and which in-
dicators and RUs the subject consults DIAG about;
how many, and which RUs the subject replaces. We
will refer to all the measures that were automatically
collected as performance measures.
At the end of the experiment, each subject was ad-
ministered a questionnaire divided into three parts.
The first part (the posttest) consists of three ques-
tions and tests what the student learned about the
domain. The second part concerns whether subjects
remember their actions, specifically, the RUs they
replaced. We quantify the subjects? recollections in
terms of precision and recall with respect to the log
that the system collects. We expect precision and re-
call of the replaced RUs to correlate with transfer,
namely, to predict how well a subject is able to ap-
ply what s/he learnt about diagnosing malfunctions
1. RU? queried RU
REL-IND? indicator associated to RU
2. IF RU warrants suspicion,
(a) state RU is a suspect
(b) IF student knows that REL-IND is abnormal
i. remind him of referent of REL-IND and
its abnormal state
ii. suggest to replace RU
(c) ELSE suggest to check REL-IND
3. ELSE
(a) state RU is not a suspect
(b) IF student knows that REL-IND is normal
i. use referent of REL-IND and its normal state
to justify judgment
(c) IF student knows of abnormal indicators OTHER-INDs
i. remind him of referents of OTHER-INDs
and their abnormal states
ii. FOR each OTHER-IND
A. REL-RUs? RUs associated with OTHER-IND
B. AGGR-RUs? AGGREGATE(REL-RUs)
? AGGR-RUs
iii. Suggest to check AGGR-RUs
Figure 6: DIAG-NLP2 : Feedback generation for
ConsultRU
to new problems. The third part concerns usability,
to be discussed below.
We found that subjects who used DIAG-NLP2
had significantly higher scores on the posttest, and
were significantly more correct (higher precision)
in remembering what they did. As regards perfor-
mance measures, there are no so clear cut results.
As regards usability, subjects prefer DIAG-NLP1 /2
to DIAG-orig , however results are mixed as regards
which of the two they actually prefer.
In the tables that follow, boldface indicates sig-
nificant differences, as determined by an analysis of
variance performed via ANOVA, followed by post-
hoc Tukey tests.
Table 1 reports learning measures, average across
the three problems. DIAG-NLP2 is significantly
better as regards PostTest score (F = 10.359, p =
0.000), and RU Precision (F = 4.719, p =
0.012). Performance on individual questions in the
54
DIAG-orig DIAG-NLP1 DIAG-NLP2
PostTest 0.72 0.69 0.90
RU Precision 0.78 0.70 0.91
RU Recall .53 .47 .40
Table 1: Learning Scores
Figure 7: Scores on PostTest questions
PostTest3 is illustrated in Fig. 7. Scores in DIAG-
NLP2 are always higher, significantly so on ques-
tions 2 and 3 (F = 8.481, p = 0.000, and F =
7.909, p = 0.001), and marginally so on question 1
(F = 2.774, p = 0.069).4
D-Orig D-NLP1 D-NLP2
Total Time 30?17? 28?34? 34?53?
RU Replacements 8.88 11.12 11.36
ConsultInd 22.16 6.92 28.16
Avg. Reading Time 8? 14? 2?
ConsultRU 63.52 45.68 52.12
Avg. Reading Time 5? 4? 5?
Table 2: Performance Measures
Table 2 reports performance measures, cumula-
tive across the three problems, other than average
reading times. Subjects don?t differ significantly in
the time they spend solving the problems, or in the
number of RU replacements they perform. DIAG?s
assumption (known to the subjects) is that there is
only one broken RU per problem, but the simula-
tion allows subjects to replace as many as they want
without any penalty before they come to the correct
solution. The trend on RU replacements is opposite
what we would have hoped for: when repairing a
real system, replacing parts that are working should
clearly be kept to a minimum, and subjects replace
3The three questions are: 1. Describe the main subsystems
of the furnace. 2. What is the purpose of (a) the oil pump (b)
the system control module? 3. Assume the photoelectric cell is
covered with enough soot that it could not detect combustion.
What impact would this have on the system?
4The PostTest was scored by one of the authors, following
written guidelines.
fewer parts in DIAG-orig .
The next four entries in Table 2 report the number
of queries that subjects ask, and the average time it
takes subjects to read the feedback. The subjects
ask significantly fewer ConsultInd in DIAG-NLP1
(F = 8.905, p = 0.000), and take significantly less
time reading ConsultInd feedback in DIAG-NLP2
(F = 15.266, p = 0.000). The latter result is
not surprising, since the feedback in DIAG-NLP2 is
much shorter than in DIAG-orig and DIAG-NLP1 .
Neither the reason not the significance of subjects
asking many fewer ConsultInd of DIAG-NLP1 are
apparent to us ? it happens for ConsultRU as well,
to a lesser, not significant degree.
We also collected usability measures. Although
these are not usually reported in ITS evaluations,
in a real setting students should be more willing to
sit down with a system that they perceive as more
friendly and usable. Subjects rate the system along
four dimensions on a five point scale: clarity, useful-
ness, repetitiveness, and whether it ever misled them
(the scale is appropriately arranged: the highest clar-
ity but the lowest repetitiveness receive 5 points).
There are no significant differences on individual
dimensions. Cumulatively, DIAG-NLP2 (at 15.08)
slightly outperforms the other two (DIAG-orig at
14.68 and DIAG-NLP1 at 14.32), however, the dif-
ference is not significant (highest possible rating is
20 points).
prefer neutral disprefer
DIAG-NLP1 to DIAG-orig 28 5 17
DIAG-NLP2 to DIAG-orig 34 1 15
DIAG-NLP2 to DIAG-NLP1 24 1 25
Table 3: User preferences among the three systems
prefer neutral disprefer
Consult Ind. 8 1 16
Consult RU 16 0 9
Table 4: DIAG-NLP2 versus DIAG-NLP1
natural concise clear contentful
DIAG-NLP1 4 8 10 23
DIAG-NLP2 16 8 11 12
Table 5: Reasons for system preference
Finally,5 on paper, subjects compare two pairs of
versions of feedback: in each pair, the first feedback
5Subjects can also add free-form comments. Only few did
55
is generated by the system they just worked with,
the second is generated by one of the other two sys-
tems. Subjects say which version they prefer, and
why (they can judge the system along one or more
of four dimensions: natural, concise, clear, content-
ful). The first two lines in Table 3 show that subjects
prefer the NLP systems to DIAG-orig (marginally
significant, ?2 = 9.49, p < 0.1). DIAG-NLP1
and DIAG-NLP2 receive the same number of pref-
erences; however, a more detailed analysis (Table 4)
shows that subjects prefer DIAG-NLP1 for feed-
back to ConsultInd, but DIAG-NLP2 for feedback
to ConsultRu (marginally significant, ?2 = 5.6, p <
0.1). Finally, subjects find DIAG-NLP2 more nat-
ural, but DIAG-NLP1 more contentful (Table 5,
?2 = 10.66, p < 0.025).
4 Discussion and conclusions
Our work touches on three issues: aggregation, eval-
uation of NLG systems, and the role of NL inter-
faces for ITSs.
In much work on aggregation (Huang and Fiedler,
1996; Horacek, 2002), aggregation rules and heuris-
tics are shown to be plausible, but are not based on
any hard evidence. Even where corpus work is used
(Dalianis, 1996; Harvey and Carberry, 1998; Shaw,
2002), the results are not completely convincing be-
cause we do not know for certain the content to be
communicated from which these texts supposedly
have been aggregated. Therefore, positing empir-
ically based rules is guesswork at best. Our data
collection attempts at providing a more solid em-
pirical base for aggregation rules; we found that tu-
tors exclude significant amounts of factual informa-
tion, and use high degrees of aggregation based on
functionality. As a consequence, while part of our
rules implement standard types of aggregation, such
as conjunction via shared participants, we also intro-
duced functional aggregation (see conceptual aggre-
gation (Reape and Mellish, 1998)).
As regards evaluation, NLG systems have been
evaluated e.g. by using human judges to assess the
quality of the texts produced (Coch, 1996; Lester
and Porter, 1997; Harvey and Carberry, 1998); by
comparing the system?s performance to that of hu-
mans (Yeh and Mellish, 1997); or through task ef-
ficacy measures, i.e., measuring how well the users
so, and the distribution of topics and of evaluations is too broad
to be telling.
of the system perform on the task at hand (Young,
1999; Carenini and Moore, 2000; Reiter et al,
2003). The latter kind of studies generally contrast
different interventions, i.e. a baseline that does not
use NLG and one or more variations obtained by pa-
rameterizing the NLG system. However, the evalu-
ation does not focus on a specific component of the
NLG process, as we did here for aggregation.
Regarding the role of NL interfaces for ITSs, only
very recently have the first few results become avail-
able, to show that first of all, students do learn when
interacting in NL with an ITS (Litman et al, 2004;
Graesser et al, 2005). However, there are very few
studies like ours, that evaluate specific features of
the NL interaction, e.g. see (Litman et al, 2004). In
our case, we did find that different features of the NL
feedback impact learning. Although we contend that
this effect is due to functional aggregation, the feed-
back in DIAG-NLP2 changed along other dimen-
sions, mainly using referents of indicators instead of
indicators, and being more strongly directive in sug-
gesting what to do next. Of course, we cannot ar-
gue that our best NL generator is equivalent to a hu-
man tutor ? e.g., dividing the number of ConsultRU
and ConsultInd reported in Sec. 2.2 by the number
of dialogues shows that students ask about 10 Con-
sultRus and 1.5 ConsultInd per dialogue when in-
teracting with a human, many fewer than those they
pose to the ITSs (cf. Table 2) (regrettably we did not
administer a PostTest to students in the human data
collection). We further discuss the implications of
our results for NL interfaces for ITSs in a compan-
ion paper (Di Eugenio et al, 2005).
The DIAG project has come to a close. We are
satisfied that we demonstrated that even not overly
sophisticated NL feedback can make a difference;
however, the fact that DIAG-NLP2 has the best lan-
guage and engenders the most learning prompts us
to explore more complex language interactions. We
are pursuing new exciting directions in a new do-
main, that of basic data structures and algorithms.
We are investigating what distinguishes expert from
novice tutors, and we will implement our findings
in an ITS that tutors in this domain.
Acknowledgments. This work is supported by the Office
of Naval Research (awards N00014-99-1-0930 and N00014-00-
1-0640), and in part by the National Science Foundation (award
IIS 0133123). We are grateful to CoGenTex Inc. for making
EXEMPLARS and RealPro available to us.
56
References
Giuseppe Carenini and Johanna D. Moore. 2000. An em-
pirical study of the influence of argument conciseness
on argument effectiveness. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, Hong Kong.
Jose? Coch. 1996. Evaluating and comparing three text-
production techniques. In COLING96, Proceedings of
the Sixteenth International Conference on Computa-
tional Linguistics, pages 249?254
Hercules Dalianis. 1996. Concise Natural Language
Generation from Formal Specifications. Ph.D. thesis,
Department of Computer and Systems Science, Sto-
cholm University. Technical Report 96-008.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural Lan-
guage Generation for Intelligent Tutoring Systems. In
INLG02, The Third International Natural Language
Generation Conference, pages 120?127.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Natural language
generation for intelligent tutoring systems: a case
study. In AIED 2005, the 12th International Confer-
ence on Artificial Intelligence in Education.
M. W. Evens, J. Spitkovsky, P. Boyle, J. A. Michael, and
A. A. Rovick. 1993. Synthesizing tutorial dialogues.
In Proceedings of the Fifteenth Annual Conference of
the Cognitive Science Society, pages 137?140.
Michael Glass, Heena Raval, Barbara Di Eugenio, and
Maarika Traat. 2002. The DIAG-NLP dialogues: cod-
ing manual. Technical Report UIC-CS 02-03, Univer-
sity of Illinois - Chicago.
A.C. Graesser, N. Person, Z. Lu, M.G. Jeon, and B. Mc-
Daniel. 2005. Learning while holding a conversation
with a computer. In L. PytlikZillig, M. Bodvarsson,
and R. Brunin, editors, Technology-based education:
Bringing researchers and practitioners together. Infor-
mation Age Publishing.
Terrence Harvey and Sandra Carberry. 1998. Inte-
grating text plans for conciseness and coherence. In
ACL/COLING 98, Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 512?518.
Helmut Horacek. 2002. Aggregation with strong regu-
larities and alternatives. In International Conference
on Natural Language Generation.
Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing
and aggregating argumentative text using text struc-
ture. In Proceedings of the 8th International Workshop
on Natural Language Generation, pages 21?30.
Rodger Kibble and Richard Power. 2000. Nominal gen-
eration in GNOME and ICONOCLAST. Technical re-
port, Information Technology Research Institute, Uni-
versity of Brighton, Brighton, UK.
Beno??t Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing.
James C. Lester and Bruce W. Porter. 1997. Developing
and empirically evaluating robust explanation genera-
tors: the KNIGHT experiments. Computational Lin-
guistics, 23(1):65?102.
D. J. Litman, C. P. Rose?, K. Forbes-Riley, K. VanLehn,
D. Bhembe, and S. Silliman. 2004. Spoken versus
typed human and computer dialogue tutoring. In Pro-
ceedings of the Seventh International Conference on
Intelligent Tutoring Systems, Maceio, Brazil.
Mike Reape and Chris Mellish. 1998. Just what is ag-
gregation anyway? In Proceedings of the European
Workshop on Natural Language Generation.
Ehud Reiter and Robert Dale. 2000. Building Natu-
ral Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
S. C. Shapiro. 2000. SNePS: A logic for natural lan-
guage understanding and commonsense reasoning. In
L. M. Iwanska and S. C. Shapiro, editors, Natural
Language Processing and Knowledge Representation.
AAAI Press/MIT Press.
James Shaw. 2002. A corpus-based analysis for the or-
dering of clause aggregation operators. In COLING02,
Proceedings of the 19th International Conference on
Computational Linguistics.
Douglas M. Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal of Artificial Intelligence in Education.
Michael White and Ted Caldwell. 1998. Exemplars: A
practical, extensible framework for dynamic text gen-
eration. In Proceedings of the Ninth International
Workshop on Natural Language Generation, pages
266?275, Niagara-on-the-Lake, Canada.
Ching-Long Yeh and Chris Mellish. 1997. An empir-
ical study on the generation of anaphora in Chinese.
Computational Linguistics, 23(1):169?190.
R. Michael Young. 1999. Using Grice?s maxim of quan-
tity to select the content of plan descriptions. Artificial
Intelligence, 115:215?256.
57
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 31?36,
Columbus, June 2008. c?2008 Association for Computational Linguistics
The role of positive feedback in Intelligent Tutoring Systems
Davide Fossati
Department of Computer Science
University of Illinois at Chicago
Chicago, IL, USA
dfossa1@uic.edu
Abstract
The focus of this study is positive feedback in
one-on-one tutoring, its computational model-
ing, and its application to the design of more
effective Intelligent Tutoring Systems. A data
collection of tutoring sessions in the domain
of basic Computer Science data structures has
been carried out. A methodology based on
multiple regression is proposed, and some pre-
liminary results are presented. A prototype In-
telligent Tutoring System on linked lists has
been developed and deployed in a college-
level Computer Science class.
1 Introduction
One-on-one tutoring has been shown to be a very
effective form of instruction (Bloom, 1984). The
research community is working on discovering the
characteristics of tutoring. One of the goals is to un-
derstand the strategies tutors use, in order to design
effective learning environments and tools to support
learning. Among the tools, particular attention is
given to Intelligent Tutoring Systems (ITSs), which
are sophisticated software systems that can provide
personalized instruction to students, in some respect
similar to one-on-one tutoring (Beck et al, 1996).
Many of these systems have been shown to be very
effective (Evens and Michael, 2006; Van Lehn et al,
2005; Di Eugenio et al, 2005; Mitrovic? et al, 2004;
Person et al, 2001). In many experiments, ITSs in-
duced learning gains higher than those measured in
a classroom environment, but lower than those ob-
tained with one-on-one interactions with human tu-
tors. The belief of the research community is that
knowing more about human tutoring would help im-
prove the design of ITSs. In particular, the effective
use of natural language might be a key element. In
most of the studies mentioned above, systems with
more sophisticated language interfaces performed
better than other experimental conditions.
An important form of student-tutor interaction is
feedback. Negative feedback can be provided by the
tutor in response to students? mistakes. An effective
use of negative feedback can help the student cor-
rect a mistake and prevent him/her from repeating
the same or a similar mistake again, effectively pro-
viding a learning opportunity to the student. Posi-
tive feedback is usually provided in response to some
correct input from the student. Positive feedback can
help students reinforce the correct knowledge they
already have, or successfully integrate new knowl-
edge, if the correct input provided by the student was
originated by a random or tentative step.
The goal of this study is to assess the relevance of
positive feedback in tutoring, and build a computa-
tional model of positive feedback that can be imple-
mented in ITSs. Even though some form of positive
feedback is present in many successful ITSs, the pre-
dominant type of feedback generated by those sys-
tems is negative feedback, as those systems are de-
signed to react to students mistakes. To date, there
is no systematic study of the role of positive feed-
back in ITSs in the literature. However, there is
an increasing amount of evidence that suggests that
positive feedback may be very important in enhanc-
ing students? learning. In a detailed study in a con-
trolled environment and domain, the letter pattern
extrapolation task, Corrigan-Halpern (2006) found
31
that subjects given positive feedback performed bet-
ter in an assessment task than subjects receiving neg-
ative feedback. In another study on the same do-
main, Lu (2007) found that the ratio of the positive
over negative messages in her corpus of expert tu-
toring dialogues is about 4 to 1, and the ratio is even
higher in the messages presented by her successful
ITS modeled after an expert tutor, being about 10
to 1. In the dataset subject of this study, which is
on a completely different domain ?Computer Sci-
ence data structures? such a high ratio of positive
over negative feedback messages still holds, in the
order of about 8 to 1. In a recent study, Barrow et al
(2008) showed that a version of their SQL-Tutor en-
riched with positive feedback generation helped stu-
dents learn faster than another version of the same
system delivering negative feedback only.
What might be the educational value of positive
feedback in ITSs? First of all, positive feedback
may be an effective motivational technique (Lepper
et al, 1997). Positive feedback can also have cog-
nitive value. In a problem solving setting, the stu-
dent can make a tentative (maybe random) step to-
wards the correct solution. At this point, positive
feedback from the tutor may be important in help-
ing the student consolidate this step and learn from
it. Some researchers outlined the importance of self-
explanation in learning (Chi, 1996; Renkl, 2002).
Positive feedback has the potential to improve self-
explanation, in terms of quantity and effectiveness.
Another issue is how students perceive and accept
feedback (Weaver, 2006), and, in the case of auto-
mated tutoring systems, whether students read feed-
back messages at all (Heift, 2001). Positive feed-
back might also make students more willing to ac-
cept help and advice from the tutor.
2 A study of human tutoring
The domain of this study is Computer Science data
structures, specifically linked lists, stacks, and bi-
nary search trees. A corpus of 54 one-on-one tutor-
ing sessions has been collected. Each individual stu-
dent participated in only one tutoring session, with
a tutor randomly assigned from a pool of two tutors.
One of the tutors is an experienced Computer Sci-
ence professor, with more than 30 years of teaching
experience. The other tutor is a senior undergrad-
Topic Tutor Avg Stdev t df P
List
Novice .09 .22 -2.00 23 .057
Expert .18 .26 -3.85 29 < .01
Both .14 .25 -4.24 53 < .01
None .01 .15 -0.56 52 ns
iList .09 .17 -3.04 32 < .01
Stack
Novice .35 .25 -6.90 23 < .01
Expert .27 .22 -6.15 23 < .01
Both .31 .24 -9.20 47 < .01
No .05 .17 -2.15 52 < .05
Tree
Novice .33 .26 -6.13 23 < .01
Expert .29 .23 -6.84 29 < .01
Both .30 .24 -9.23 53 < .01
No .04 .16 -1.78 52 ns
Table 1: Learning gains and t-test statistics
uate student in Computer Science, with only one
semester of previous tutoring experience. The tutor-
ing sessions have been videotaped and transcribed.
Student took a pre-test right before the tutoring ses-
sion, and a post-test immediately after. An addi-
tional group of 53 students (control group) took the
pre and post tests, but they did not participate in a tu-
toring session, and attended a lecture about a totally
unrelated topic instead.
Paired samples t-tests revealed that post-test
scores are significantly higher than pre-test scores
in the two tutored conditions for all the topics, ex-
cept for linked lists with the less experienced tu-
tor, where the difference is only marginally signifi-
cant. If the two tutored groups are aggregated, there
is significant difference for all the topics. Students
in the control group did not show significant learn-
ing for linked lists and binary search trees, and only
marginally significant learning for stacks. Means,
standard deviations, and t-test statistic values are re-
ported in Table 1.
There is no significant difference between the two
tutored conditions in terms of learning gain, ex-
pressed as the difference between post-score and
pre-score. This is revealed by ANOVA between
the two groups of students in the tutored condition.
For lists, F (1, 53) = 1.82, P = ns. For stacks,
F (1, 47) = 1.35, P = ns. For trees, F (1, 53) =
0.32, P = ns.
The learning gain of students that received tutor-
ing is significantly higher than the learning gain of
the students in the control group, for all the topics.
32
This is showed by ANOVA between the group of
tutored students (with both tutors) and the control
group. For lists, F (1, 106) = 11.0, P < 0.01. For
stacks, F (1, 100) = 41.4, P < 0.01. For trees,
F (1, 106) = 43.9, P < 0.01. Means and standard
deviations are reported in Table 1.
3 Regression-based analysis
The distribution of scores across sessions shows a lot
of variability (Table 1). In all the conditions, there
are sessions with very high learning gains, and ses-
sions with very low ones. This observation and the
previous results suggest a new direction for subse-
quent analysis: instead of looking at the character-
istics of a particular tutor, it is better to look at the
features that discriminate the most successful ses-
sions from the least successful ones. As advocated
in (Ohlsson et al, 2007), a sensible way to do that
is to adopt an approach based on multiple regression
of learning outcomes per tutoring session onto the
frequencies of the different features. The following
analysis has been done adopting a hierarchical, lin-
ear regression model.
Prior knowledge First of all, we want to factor out
the effect of prior knowledge, measured by the pre-
test score. A linear regression model reveals strong
effect of pre-test scores on learning gain (Table 2).
However, the R2 values show that there is a lot of
variance left to be explained, especially for lists and
stacks, although not so much for trees. Notice that
the ? weights are negative. That means students
with higher pre-test scores learn less then students
with lower pre-test scores. A possible explanation
is that students with more previous knowledge have
less learning opportunity than students with less pre-
vious knowledge.
Time on task Another variable that is recognized
as important by the educational research commu-
nity is time on task, and we can approximate it with
the length of the tutoring session. In the hierarchi-
cal regression model, session length follows pre-test
score. Surprisingly, session length has a significant
effect only on linked lists (Table 2).
Student activity Another hypothesis is that the
degree of student activity, in the sense of the amount
of student?s participation in the discussion, might
relate to learning (Lepper et al, 1997; Chi et al,
2001). To test this hypothesis, the following defi-
nition of student activity has been adopted:
student activity =
# of turns? # of short turns
session length
Turns are the sequences of uninterrupted speech of
the student. Short turns are the student turns shorter
than three words. The regression analysis revealed
no significant effect of this measure of students? ac-
tivity on learning gain.
Feedback The dataset has been manually anno-
tated for episodes where positive or negative feed-
back is delivered. All the protocols have been
annotated by one coder, and some of them have
been double-coded by a second one (intercoder
agreement: kappa = 0.67). Examples of feedback
episodes are reported in Figure 1.
The number of positive feedback episodes and the
number of negative feedback episodes have been in-
troduced in the regression model (Table 2). The
model showed a significant effect of feedback for
linked lists and stacks, but no significant effect on
trees. Interestingly, the effect of positive feedback is
positive, but the effect of negative feedback is nega-
tive, as can be seen by the sign of the ? value.
4 A tutoring system for linked lists
A new ITS in the domain of linked lists, iList, is
being developed (Figure 2).
The iList system is based on the constraint-based
design paradigm. Originally developed from a cog-
nitive theory of how people might learn from per-
formance errors (Ohlsson, 1996), constraint-based
modeling has grown into a methodology used to
build full-fledged ITSs, and an alternative to the
model tracing approach adopted by many ITSs. In a
constraint-based system, domain knowledge is mod-
eled with a set of constraints, logic units composed
of a relevance condition and a satisfaction condi-
tion. A constraint is irrelevant when the relevance
condition is not satisfied; it is satisfied when both
relevance and satisfaction conditions are satisfied; it
is violated when the relevance condition is satisfied
but the satisfaction condition is not. In the context
of tutoring, constraints are matched against student
33
T: do you see a problem?
T: I have found the node a@l, see here I found the node b@l, and
then I put g@l in after it.
Begin + T: here I have found the node a@l and now the link I have to
change is +...
S: ++ you have to link e@l <over xxx.> [>]
End + T: [<] <yeah> I have to go back to this one.
S: *mmhm
T: so I *uh once I?m here, this key is here, I can?t go backwards.
Begin - S: <so you> [>] <you won?t get the same> [//] would you get the
same point out of writing t@l close to c@l at the top?
T: oh, t@l equals c@l.
T: no because you would have a type mismatch.
End - T: t@l <is a pointer> [//] is an address, and this is contents.
Figure 1: Positive and negative feedback (T = tutor, S = student)
Topic Model Predictor ? R2 P
List
1 Pre-test -.45 .18 < .05
2
Pre-test -.40
.28
< .05
Session length .35 < .05
3
Pre-test -.35
.36
< .05
Session length .33 .05
+ feedback .46 .05
- feedback -.53 < .05
Stack
1 Pre-test -.53 .26 < .01
2
Pre-test -.52
.24
< .01
Session length .05 ns
3
Pre-test -.58
.33
< .01
Session length .01 ns
+ feedback .61 < .05
- feedback -.55 < .05
Tree
1 Pre-test -.79 .61 < .01
2
Pre-test -.78
.60
< .01
Session length .03 ns
3
Pre-test -.77
.59
< .01
Session length .04 ns
+ feedback .06 ns
- feedback -.12 ns
All
1 Pre-test -.52 .26 < .01
2
Pre-test -.54
.29
< .01
Session length .20 < .05
3
Pre-test -.57
.32
< .01
Session length .16 .06
+ feedback .30 < .05
- feedback -.23 .05
Table 2: Linear regression
Figure 2: The iList system
solutions. Satisfied constraints correspond to knowl-
edge that students have acquired, whereas violated
constraints correspond to gaps or incorrect knowl-
edge. An important feature is that there is no need
for an explicit model of students? mistakes, as op-
posed to buggy rules in model tracing. The possible
errors are implicitly specified as the possible ways
in which constraints can be violated.
The architecture of iList includes a problem
model, a constraint evaluator, a feedback manager,
and a graphical user interface. Student model and
pedagogical module, important components of a
complete ITS (Beck et al, 1996), have not been
implemented yet, and will be included in a future
version. Currently, the system provides only simple
negative feedback in response to students? mistakes,
as customary in constraint-based ITSs.
A first version of the system has been deployed
34
into a Computer Science class of a partner institu-
tion. 33 students took a pre-test before using the
system, and a post-test immediately afterwards. The
students also filled in a questionnaire about their
subjective impressions on the system. The interac-
tion of the students with the system was logged.
T-test on test scores revealed that students did
learn during the interaction with iList (Table 1). The
learning gain is somewhere in between the one ob-
served in the control condition and the one of the
tutored condition. ANOVA revealed no significant
difference between the control group and the iList
group, nor between the iList group and the tutored
group, whereas the difference between control and
tutored groups is significant.
A preliminary analysis of the questionnaires re-
vealed that students felt that iList helped them learn
linked lists to a moderate degree (on a 1 to 5 scale:
avg = 2.88, stdev = 1.18), but working with iList
was interesting to them (avg = 4.0, stdev = 1.27).
Students found the feedback provided by the sys-
tem somewhat repetitive (avg = 3.88, stdev = 1.18),
which is not surprising given the simple template-
based generation mechanism. Also, the feedback
was considered not very useful (avg = 2.31, 1.23),
but at least not too misleading (avg = 2.22, stdev
= 1.21). Interestingly, students declared that they
read the feedback provided by the system (avg =
4.25, stdev = 1.05), but the logs of the system re-
veal just the opposite. In fact, on average, students
read feedback messages for 3.56 seconds (stdev =
2.66 seconds), resulting in a reading speed of 532
words/minute (stdev = 224 words/minute). Accord-
ing to Carver?s taxonomy (Carver, 1990), such speed
indicates a quick skimming of the text, whereas
reading for learning typically has a lower speed, in
the order of 200 words/minute.
5 Future work
The main goal of this research is to build a compu-
tational model of positive feedback that can be used
in ITSs. The study of empirical data and the sys-
tem design and development will proceed in paral-
lel, helping and informing each other as new results
are obtained.
The conditions and the modalities of positive
feedback delivery by tutors will be investigated from
the human tutoring dataset. To do so, more coding
categories will be defined, and the data will be anno-
tated with these categories. The results of the statis-
tical analysis over the first few coding categories will
be used to guide the definition of more categories,
that will be in turn used to annotate the data, and
so on. An example of potential coding category is
whether the student?s action that triggered the feed-
back was prompted by the tutor or volunteered by
the student. Another example is whether the feed-
back?s content was a repetition of what the student
just said or included additional explanation.
The first experiment with iList provided a com-
prehensive log of the students? interaction with the
system. Additional analysis of this data will be im-
portant, especially because the nature of the interac-
tion of a student with a computer system differs from
the interaction with a human tutor. When working
with a computer system, most of the interaction hap-
pens through a graphical interface, instead of natu-
ral language dialogue. Also, the interaction with a
computer system is mostly student-driven, whereas
our human protocols show a clear predominance of
the tutor in the conversation. In the CS protocols,
on average, 94% of the words belong to the tutor,
and most of the tutors? discourse is some form of di-
rect instruction. On the other hand, the interaction
with the system will mostly consist of actions that
students make to solve the problems that they will
be asked to solve, with few interventions from the
system. An interesting analysis that could be done
on the logs is the discovery of sequential patterns us-
ing data mining algorithms, such as MS-GSP (Liu,
2006). Such patterns could then be regressed against
learning outcomes, in order to assess their correla-
tion with learning.
After the relevant features are discovered, a com-
putational model of positive feedback will be built
and integrated into iList. The model will en-
code knowledge extracted with machine learning ap-
proaches, and such knowledge will inform a dis-
course planner, responsible of organizing and gen-
erating appropriate positive feedback. The choiche
of the specific machine learning and discourse plan-
ning methods will require extensive empirical inves-
tigation. Specifically, among the different machine
learning methods, some are able to provide some
sort of human-readable symbolic model, which can
35
be inspected to gain some insights on how the model
works. Decision trees and association rules belong
to this category. Other methods provide a less read-
able, black-box type of models, but they may be very
useful and effective as well. Examples of such meth-
ods include Neural Networks and Markov Models.
The ultimate goal of this research is to get both an ef-
fective model and to gain insights on tutoring. Thus,
both classes of machine learning methods will be
tried, with the goal of finding a balance between
model effectiveness and model readability.
Finally, the system with enhanced feedback capa-
bilities will be deployed and evaluated.
Acknowledgments
This work is supported by award N00014-07-1-0040
from the Office of Naval Research, and additionally
by awards ALT-0536968 and IIS-0133123 from the
National Science Foundation.
References
Devon Barrow, Antonija Mitrovic?, Stellan Ohlsson, and
Michael Grimley. 2008. Assessing the impact of pos-
itive feedback in constraint-based tutors. In ITS 2008,
The 9th International Conference on Intelligent Tutor-
ing Systems, Montreal, Canada.
Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996.
Applications of AI in education. ACM crossroads.
http://www.acm.org/crossroads/xrds3-1/aied.html.
B. S. Bloom. 1984. The 2 sigma problem: The search for
methods of group instruction as effective as one-to-one
tutoring. Educational Researcher, 13:4?16.
Ronald P. Carver. 1990. Reading Rate: A Review of
Research and Theory. Academic Press, San Diego,
CA.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn Jeong,
Takashi Yamauchi, and Robert G. Hausmann. 2001.
Learning from human tutoring. Cognitive Science,
25:471?533.
Michelene T.H. Chi. 1996. Constructing self-
explanations and scaffolded explanations in tutoring.
Applied Cognitive Psychology, 10:33?49.
Andrew Corrigan-Halpern. 2006. Feedback in Complex
Learning: Considering the Relationship Between Util-
ity and Processing Demands. Ph.D. thesis, University
of Illinois at Chicago.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Aggregation im-
proves learning: Experiments in natural language gen-
eration for intelligent tutoring systems. In ACL05,
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics, Ann Arbor, MI.
Martha Evens and Joel Michael. 2006. One-on-one
Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Trude Heift. 2001. Error-specific and individualized
feedback in a web-based language tutoring system: Do
they read it? ReCALL Journal, 13(2):129?142.
M. R. Lepper, M. Drake, and T. M. O?Donnell-Johnson.
1997. Scaffolding techniques of expert human tutors.
In K. Hogan and M. Pressley, editors, Scaffolding stu-
dent learning: Instructional approaches and issues,
pages 108?144. Brookline Books, New York.
Bing Liu. 2006. Web Data Mining. Springer, Berlin.
Xin Lu. 2007. Expert Tutoring and Natural Language
Feedback in Intelligent Tutoring Systems. Ph.D. thesis,
University of Illinois at Chicago.
Antonija Mitrovic?, Pramuditha Suraweera, Brent Mar-
tin, and A. Weerasinghe. 2004. DB-suite: Ex-
periences with three intelligent, web-based database
tutors. Journal of Interactive Learning Research,
15(4):409?432.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In AIED07, 13th International Conference on
Artificial Intelligence in Education.
Stellan Ohlsson. 1996. Learning from performance er-
rors. Psychological Review, 103:241?262.
N. K. Person, A. C. Graesser, L. Bautista, E. C. Mathews,
and the Tutoring Research Group. 2001. Evaluating
student learning gains in two versions of AutoTutor.
In J. D. Moore, C. L. Redfield, and W. L. Johnson,
editors, Artificial intelligence in education: AI-ED in
the wired and wireless future, pages 286?293. Amster-
dam: IOS Press.
Alexander Renkl. 2002. Learning from worked-out ex-
amples: Instructional explanations supplement self-
explanations. Learning and Instruction, 12:529?556.
Kurt Van Lehn, Collin Lynch, Kay Schulze, Joel A.
Shapiro, Robert H. Shelby, Linwood Taylor, Don J.
Treacy, Anders Weinstein, and Mary C. Wintersgill.
2005. The Andes physics tutoring system: Five years
of evaluations. In G. I. McCalla and C. K. Looi, ed-
itors, Artificial Intelligence in Education Conference.
Amsterdam: IOS Press.
Melanie R. Weaver. 2006. Do students value feed-
back? Student perceptions of tutors? written re-
sponses. Assessment and Evaluation in Higher Edu-
cation, 31(3):379?394.
36
The problem of ontology alignment on the web: a first report
Davide Fossati and Gabriele Ghidoni and Barbara Di Eugenio
and Isabel Cruz and Huiyong Xiao and Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
dfossa1@uic.edu, red.one.999@virgilio.it, bdieugen@cs.uic.edu
ifc@cs.uic.edu, hxiao2@uic.edu, rsubba@cs.uic.edu
Abstract
This paper presents a general architec-
ture and four algorithms that use Natu-
ral Language Processing for automatic on-
tology matching. The proposed approach
is purely instance based, i.e., only the
instance documents associated with the
nodes of ontologies are taken into account.
The four algorithms have been evaluated
using real world test data, taken from the
Google and LookSmart online directories.
The results show that NLP techniques ap-
plied to instance documents help the sys-
tem achieve higher performance.
1 Introduction
Many fundamental issues about the viability and
exploitation of the web as a linguistic corpus have
not been tackled yet. The web is a massive reposi-
tory of text and multimedia data. However, there is
not a systematic way of classifying and retrieving
these documents. Computational Linguists are of
course not the only ones looking at these issues;
research on the Semantic Web focuses on pro-
viding a semantic description of all the resources
on the web, resulting into a mesh of information
linked up in such a way as to be easily process-
able by machines, on a global scale. You can think
of it as being an efficient way of representing data
on the World Wide Web, or as a globally linked
database.1 The way the vision of the Semantic
Web will be achieved, is by describing each doc-
ument using languages such as RDF Schema and
OWL, which are capable of explicitly expressing
the meaning of terms in vocabularies and the rela-
tionships between those terms.
1http://infomesh.net/2001/swintro/
The issue we are focusing on in this paper is
that these languages are used to define ontologies
as well. If ultimately a single ontology were used
to describe all the documents on the web, sys-
tems would be able to exchange information in a
transparent way for the end user. The availability
of such a standard ontology would be extremely
helpful to NLP as well, e.g., it would make it far
easier to retrieve all documents on a certain topic.
However, until this vision becomes a reality, a plu-
rality of ontologies are being used to describe doc-
uments and their content. The task of automatic
ontology alignment ormatching (Hughes and Ash-
pole, 2005) then needs to be addressed.
The task of ontology matching has been typi-
cally carried out manually or semi-automatically,
for example through the use of graphical user in-
terfaces (Noy and Musen, 2000). Previous work
has been done to provide automated support to this
time consuming task (Rahm and Bernstein, 2001;
Cruz and Rajendran, 2003; Doan et al, 2003;
Cruz et al, 2004; Subba and Masud, 2004). The
various methods can be classified into two main
categories: schema based and instance based.
Schema based approaches try to infer the seman-
tic mappings by exploiting information related to
the structure of the ontologies to be matched, like
their topological properties, the labels or descrip-
tion of their nodes, and structural constraints de-
fined on the schemas of the ontologies. These
methods do not take into account the actual data
classified by the ontologies. On the other hand,
instance based approaches look at the information
contained in the instances of each element of the
schema. These methods try to infer the relation-
ships between the nodes of the ontologies from
the analysis of their instances. Finally, hybrid
approaches combine schema and instance based
51
methods into integrated systems.
Neither instance level information, nor NLP
techniques have been extensively explored in pre-
vious work on ontology matching. For exam-
ple, (Agirre et al, 2000) exploits documents (in-
stances) on the WWW to enrich WordNet (Miller
et al, 1990), i.e., to compute ?concept signatures,?
collection of words that significantly distinguish
one sense from another, however, not directly for
ontology matching. (Liu et al, 2005) uses doc-
uments retrieved via queries augmented with, for
example, synonyms that WordNet provides to im-
prove the accuracy of the queries themselves, but
not for ontology matching. NLP techniques such
as POS tagging, or parsing, have been used for
ontology matching, but on the names and defini-
tions in the ontology itself, for example, in (Hovy,
2002), hence with a schema based methodology.
In this paper, we describe the results we ob-
tained when using some simple but effective NLP
methods to align web ontologies, using an instance
based approach. As we will see, our results show
that more sophisticated methods do not necessar-
ily lead to better results.
2 General architecture
The instance based approach we propose uses
NLP techniques to compute matching scores
based on the documents classified under the nodes
of ontologies. There is no assumption on the struc-
tural properties of the ontologies to be compared:
they can be any kind of graph representable in
OWL. The instance documents are assumed to be
text documents (plain text or HTML).
The matching process starts from a pair of on-
tologies to be aligned. The two ontologies are
traversed and, for each node having at least one
instance, the system computes a signature based
on the instance documents. Then, the signatures
associated to the nodes of the two ontologies are
compared pairwise, and a similarity score for each
pair is generated. This score could then be used
to estimate the likelihood of a match between a
pair of nodes, under the assumption that the se-
mantics of a node corresponds to the semantics of
the instance documents classified under that node.
Figure 1 shows the architecture of our system.
The two main issues to be addressed are (1)
the representation of signatures and (2) the def-
inition of a suitable comparison metric between
signatures. For a long time, the Information Re-
Ontologiesdescription(OWL)Instancedocuments(HTML orplain text) NodeSignaturesCreation SignaturesComparison SimilarityScores
FileSystem WordNet
Figure 1: Ontology aligment architecture
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion Tokengroupingand counting
Figure 2: Baseline signature creation
trieval community has succesfully adopted a ?bag
of words? approach to effectively represent and
compare text documents. We start from there to
define a general signature structure and a metric to
compare signatures.
A signature is defined as a function S : K ?
R+, mapping a finite set of keys (which can be
complex objects) to positive real values. With a
signature of that form, we can use the cosine sim-
ilarity metric to score the similarity between two
signatures:
simil(S1, S2) =
?
p S1(kp)S2(kp)
?
?
i S1(ki)
2 ?
??
j S2(kj)
2
kp ? K1 ?K2, ki ? K1, kj ? K2
The cosine similarity formula produces a value
in the range [0, 1]. The meaning of that value de-
pends on the algorithm used to build the signa-
ture. In particular, there is no predefined thresh-
old that can be used to discriminate matches from
non-matches. However, such a threshold could be
computed a-posteriori from a statistical analysis of
experimental results.
2.1 Signature generation algorithms
For our experiments, we defined and implemented
four algorithms to generate signatures. The four
algorithms make use of text and language process-
ing techniques of increasing complexity.
2.1.1 Algorithm 1: Baseline signature
The baseline algorithm performs a very simple
sequence of text processing, schematically repre-
sented in Figure 2.
52
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Noungroupingand countingPOS tagging
Figure 3: Noun signature creation
HTML tags are first removed from the in-
stance documents. Then, the texts are tokenized
and punctuation is removed. Everything is then
converted to lowercase. Finally, the tokens are
grouped and counted. The final signature has the
form of a mapping table token ? frequency.
The main problem we expected with this
method is the presence of a lot of noise. In fact,
many ?irrelevant? words, like determiners, prepo-
sitions, and so on, are added to the final signature.
2.1.2 Algorithm 2: Noun signature
To cope with the problem of excessive noise,
people in IR often use fixed lists of stop words
to be removed from the texts. Instead, we intro-
duced a syntax based filter in our chain of pro-
cessing. The main assuption is that nouns are the
words that carry most of the meaning for our kind
of document comparison. Thus, we introduced
a part-of-speech tagger right after the tokeniza-
tion module (Figure 3). The results of the tagger
are used to discard everything but nouns from the
input documents. The part-of-speech tagger we
used ?QTAG 3.1 (Tufis and Mason, 1998), readily
available on the web as a Java library? is a Hidden
Markov Model based statistical tagger.
The problems we expected with this approach
are related to the high specialization of words in
natural language. Different nouns can bear simi-
lar meaning, but our system would treat them as if
they were completely unrelated words. For exam-
ple, the words ?apple? and ?orange? are semanti-
cally closer than ?apple? and ?chair,? but a purely
syntactic approach would not make any difference
between these two pairs. Also, the current method
does not include morphological processing, so dif-
ferent inflections of the same word, such as ?ap-
ple? and ?apples,? are treated as distinct words.
In further experiments, we also considered
verbs, another syntactic category of words bearing
a lot of semantics in natural language. We com-
puted signatures with verbs only, and with verbs
and nouns together. In both cases, however, the
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionNoungroupingand counting
Figure 4: WordNet signature creation
performance of the system was worse. Thus, we
will not consider verbs in the rest of the paper.
2.1.3 Algorithm 3: WordNet signature
To address the limitations stated above, we used
the WordNet lexical resource (Miller et al, 1990).
WordNet is a dictionary where words are linked
together by semantic relationships. In Word-
Net, words are grouped into synsets, i.e., sets of
synonyms. Each synset can have links to other
synsets. These links represent semantic relation-
ships like hypernymy, hyponymy, and so on.
In our approach, after the extraction of nouns
and their grouping, each noun is looked up on
WordNet (Figure 4). The synsets to which the
noun belongs are added to the final signature in
place of the noun itself. The signature can also
be enriched with the hypernyms of these synsets,
up to a specified level. The final signature has the
form of a mapping synset ? value, where value is
a weighted sum of all the synsets found.
Two important parameters of this method are
related to the hypernym expansion process men-
tioned above. The first parameter is the maximum
level of hypernyms to be added to the signature
(hypernym level). A hypernym level value of 0
would make the algorithm add only the synsets of
a word, without any hypernym, to the signature. A
value of 1 would cause the algorithm to add also
their parents in the hypernym hierarchy to the sig-
nature. With higher values, all the ancestors up to
the specified level are added. The second parame-
ter, hypernym factor, specifies the damping of the
weight of the hypernyms in the expansion process.
Our algorithm exponentially dampens the hyper-
nyms, i.e., the weigth of a hypernym decreases ex-
ponentially as its level increases. The hypernym
factor is the base of the exponential function.
In general, a noun can have more than one
sense, e.g., ?apple? can be either a fruit or a tree.
This is reflected in WordNet by the fact that a
noun can belong to multiple synsets. With the
current approach, the system cannot decide which
53
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionWord sensedisambigua-tion
Figure 5: Disambiguated signature creation
sense is the most appropriate, so all the senses
of a word are added to the final signature, with
a weight inversely proportional to the number of
possible senses of that word. This fact poten-
tially introduces semantic noise in the signature,
because many irrelevant senses might be added to
the signature itself.
Another limitation is that a portion of the nouns
in the source texts cannot be located in WordNet
(see Figure 6). Thus, we also tried a variation (al-
gorithm 3+2) that falls back on to the bare lexi-
cal form of a noun if it cannot be found in Word-
Net. This variation, however, resulted in a slight
decrease of performance.
2.1.4 Algorithm 4: Disambiguated signature
The problem of having multiple senses for each
word calls for the adoption of word sense dis-
ambiguation techniques. Thus, we implemented
a word sense disambiguator algorithm, and we
inserted it into the signature generation pipeline
(Figure 5). For each noun in the input documents,
the disambiguator takes into account a specified
number of context words, i.e., nouns preceding
and/or following the target word. The algorithm
computes a measure of the semantic distance be-
tween the possible senses of the target word and
the senses of each of its context words, pair-
wise. A sense for the target word is chosen such
that the total distance to its context is minimized.
The semantic distance between two synsets is de-
fined here as the minimum number of hops in
the WordNet hypernym hierarchy connecting the
two synsets. This definition allows for a rela-
tively straightforward computation of the seman-
tic distance using WordNet. Other more sophisti-
cated definitions of semantic distance can be found
in (Patwardhan et al, 2003). The word sense
disambiguation algorithm we implemented is cer-
tainly simpler than others proposed in the litera-
ture, but we used it to see whether a method that is
relatively simple to implement could still help.
The overall parameters for this signature cre-
ation algorithm are the same as the WordNet sig-
nature algorithm, plus two additional parameters
for the word sense disambiguator: left context
length and right context length. They represent re-
spectively how many nouns before and after the
target should be taken into account by the dis-
ambiguator. If those two parameters are both set
to zero, then no context is provided, and the first
possible sense is chosen. Notice that even in this
case the behaviour of this signature generation al-
gorithm is different from the previous one. In
a WordNet signature, every possible sense for a
word is inserted, whereas in a WordNet disam-
biguated signature only one sense is added.
3 Experimental setting
All the algorithms described in the previous sec-
tion have been fully implemented in a coherent
and extensible framework using the Java program-
ming language, and evaluation experiments have
been run. This section describes how the experi-
ments have been conducted.
3.1 Test data
The evaluation of ontology matching approaches
is usually made difficult by the scarceness of test
ontologies readily available in the community.
This problem is even worse for instance based ap-
proaches, because the test ontologies need also to
be ?filled? with instance documents. Also, we
wanted to test our algorithms with ?real world?
data, rather than toy examples.
We were able to collect suitable test data start-
ing from the ontologies published by the Ontology
Alignment Evaluation Initiative 2005 (Euzenat et
al., 2005). A section of their data contained an
OWL representation of fragments of the Google,
Yahoo, and LookSmart web directories. We ?re-
verse engineered? some of this fragments, in or-
der to reconstruct two consistent trees, one rep-
resenting part of the Google directory structure,
the other representing part of the LookSmart hi-
erarchy. The leaf nodes of these trees were filled
with instances downloaded from the web pages
classified by the appropriate directories. With this
method, we were able to fill 7 nodes of each ontol-
ogy with 10 documents per node, for a total of 140
documents. Each document came from a distinct
web page, so there was no overlap in the data to be
compared. A graphical representation of our two
test ontologies, source and target, is shown in Fig-
54
ure 6. The darker outlined nodes are those filled
with instance documents. For the sake of readabil-
ity, the names of the nodes corresponding to real
matches are the same. Of course, this informa-
tion is not used by our algorithms, which adopt a
purely instance based approach. Figure 6 also re-
ports the size of the instance documents associated
to each node: total number of words, noun tokens,
nouns, and nouns covered by WordNet.
3.2 Parameters
The experiments have been run with several com-
binations of the relevant parameters: number of
instance documents per node (5 or 10), algorithm
(1 to 4), extracted parts of speech (nouns, verbs, or
both), hypernym level (an integer value equal or
greater than zero), hypernym factor (a real num-
ber), and context length (an integer number equal
or greater than zero). Not all of the parameters are
applicable to every algorithm. The total number of
runs was 90.
4 Results
Each run of the system with our test ontologies
produced a set of 49 values, representing the
matching score of every pair of nodes containing
instances across the two ontologies. Selected ex-
amples of these results are shown in Tables 1, 2,
3, and 4. In the experiments shown in those ta-
bles, 10 instance documents for each node were
used to compute the signatures. Nodes that ac-
tually match (identified by the same label, e.g.,
?Canada? and ?Canada?) should show high sim-
ilarity scores, whereas nodes that do not match
(e.g., ?Canada? and ?Dendrochronology?), should
have low scores. Better algorithms would have
higher scores for matching nodes, and lower score
for non-matching ones. Notice that the two nodes
?Egypt? and ?Pyramid Theories,? although intu-
itively related, have documents that take different
perspectives on the subject. So, the algorithms
correctly identify the nodes as being different.
Looking at the results in this form makes it dif-
ficult to precisely assess the quality of the algo-
rithms. To do so, a statistical analysis has to be
performed. For each table of results, let us parti-
tion the scores in two distinct sets:
A = {simil(nodei, nodej) | real match = true}
B = {simil(nodei, nodej) | real match = false}
Target node
Canada
Canada 0.95 0.89 0.89 0.91 0.87 0.86 0.92
0.90 0.97 0.91 0.90 0.88 0.87 0.92
Egypt 0.86 0.89 0.91 0.87 0.86 0.88 0.90
Megaliths 0.90 0.91 0.99 0.93 0.95 0.94 0.93
Museums 0.89 0.88 0.90 0.93 0.88 0.87 0.90
0.88 0.88 0.95 0.91 0.99 0.93 0.91
0.87 0.87 0.86 0.88 0.82 0.82 0.96
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 1: Results ? Baseline signature algorithm
Target node
Canada
Canada 0.67 0.20 0.14 0.35 0.08 0.08 0.41
0.22 0.80 0.15 0.22 0.09 0.09 0.25
Egypt 0.13 0.23 0.26 0.22 0.17 0.24 0.25
Megaliths 0.28 0.20 0.85 0.37 0.22 0.27 0.33
Museums 0.30 0.19 0.18 0.58 0.08 0.14 0.27
0.13 0.12 0.26 0.18 0.96 0.14 0.17
0.42 0.20 0.17 0.26 0.09 0.11 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 2: Results ? Noun signature algorithm
Target node
Canada
Canada 0.79 0.19 0.19 0.38 0.15 0.06 0.56
0.26 0.83 0.18 0.20 0.16 0.07 0.24
Egypt 0.17 0.24 0.32 0.21 0.31 0.30 0.27
Megaliths 0.39 0.21 0.81 0.41 0.40 0.25 0.42
Museums 0.31 0.14 0.17 0.70 0.11 0.11 0.26
0.24 0.20 0.42 0.29 0.91 0.21 0.29
0.56 0.17 0.22 0.25 0.15 0.08 0.84
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 3: Results ? WordNet signature algorithm
(hypernym level=0)
55
TopGoogle
Science
Social_Sciences
Archaeology
Egypt
Alternative
Megaliths South_America
Nazca_Lines
Methodology
Dendrochronology
Museums Publications
Europe
UnitedKingdom
Organizations
NorthAmerica
Canada
TopLookSmart
Science_&_Health
Social_Science
Archaeology
Pyramid_Theories
Topics
Megaliths Nazca_Lines
Science
Dendrochronology
Museums Publications
UnitedKingdom
Associations
Canada
Seven_Wonders_of_the_World
Gyza_Pyramids
Source Target
44307; 16001;3957; 2220 15447; 4400;1660; 1428 18754; 5574; 1725; 1576
7362; 2377; 953; 823
2872; 949;499; 441
9620; 3257; 1233; 1001 3972; 1355;603; 541 3523; 1270;617; 555
23039; 7926; 1762; 1451
13705; 3958;1484; 1303
6171; 2333;943; 844
10721; 3280; 1099; 9887841; 2486; 869; 769
17196; 5529;1792; 1486
Figure 6: Ontologies used in the experiments. The numbers below the leaves indicate the size of instance
documents: # of words; # of noun tokens; # of nouns; # of nouns in WordNet
Target node
Canada
Canada 0.68 0.18 0.13 0.33 0.12 0.05 0.44
0.23 0.79 0.15 0.20 0.14 0.07 0.23
Egypt 0.15 0.23 0.28 0.22 0.27 0.31 0.27
Megaliths 0.30 0.18 0.84 0.37 0.34 0.27 0.33
Museums 0.29 0.16 0.15 0.60 0.11 0.10 0.24
0.20 0.17 0.38 0.26 0.89 0.21 0.26
0.45 0.17 0.18 0.24 0.15 0.08 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 4: Results ? Disambiguated signature al-
gorithm (hypernym level=0, left context=1, right
context=1)
With our test data, we would have 6 values in
set A and 43 values in set B. Then, let us com-
pute average and standard deviation of the values
included in each set. The average of A represents
the expected score that the system would assign
to a match; likewise, the average of B is the ex-
pected score of a non-match. We define the fol-
lowing measure to compare the performance of
our matching algorithms, inspired by ?effect size?
from (VanLehn et al, 2005):
discrimination size =
avg(A) ? avg(B)
stdev(A) + stdev(B)
Higher discrimination values mean that the
scores assigned to matches and non-matches are
more ?far away,? making it possible to use those
scores to make more reliable decisions about the
matching degree of pairs of nodes.
Table 5 shows the values of discrimination size
(last column) out of selected results from our ex-
periments. The algorithm used is reported in the
first column, and the values of the other relevant
parameters are indicated in other columns. We can
make the following observations.
? Algorithms 2, 3, and 4 generally outperform
the baseline (algorithm 1).
? Algorithm 2 (Noun signature), which still
uses a fairly simple and purely syntactical
technique, shows a substantial improvement.
Algorithm 3 (WordNet signature), which in-
troduces some additional level of semantics,
has even better performance.
? In algorithms 3 and 4, hypernym expansion
looks detrimental to performance. In fact, the
best results are obtained with hypernym level
equal to zero (no hypernym expansion).
? The word sense disambiguator implemented
in algorithm 4 does not help. Even though
disambiguating with some limited context
(1 word before and 1 word after) provides
slightly better results than choosing the first
available sense for a word (context length
equal to zero), the overall results are worse
than adding all the possible senses to the sig-
nature (algorithm 3).
? Using only 5 documents per node signifi-
cantly degrades the performance of all the al-
gorithms (see the last 5 lines of the table).
5 Conclusions and future work
The results of our experiments point out several
research questions and directions for future work,
56
Alg Docs POS Hyp lev Hyp fac L cont R cont Avg (A) Stdev (A) Avg (B) Stdev (B) Discrimination size1 10 0.96 0.02 0.89 0.03 1.372 10 noun 0.78 0.13 0.21 0.09 2.552 10 verb 0.64 0.20 0.31 0.11 1.042 10 nn+vb 0.77 0.14 0.21 0.09 2.483 10 noun 0 0.81 0.07 0.25 0.12 3.083 10 noun 1 1 0.85 0.07 0.41 0.12 2.353 10 noun 1 2 0.84 0.07 0.34 0.12 2.643 10 noun 1 3 0.83 0.07 0.31 0.12 2.803 10 noun 2 1 0.90 0.06 0.62 0.11 1.643 10 noun 2 2 0.86 0.07 0.45 0.12 2.183 10 noun 2 3 0.84 0.07 0.36 0.12 2.563 10 noun 3 1 0.95 0.04 0.78 0.08 1.443 10 noun 3 2 0.88 0.07 0.52 0.12 1.913 10 noun 3 3 0.85 0.07 0.38 0.12 2.453+2 10 noun 0 0 0.80 0.09 0.21 0.11 2.943+2 10 noun 1 2 0.83 0.08 0.30 0.11 2.733+2 10 noun 2 2 0.85 0.08 0.39 0.11 2.404 10 noun 0 0 0 0.80 0.12 0.24 0.10 2.644 10 noun 0 1 1 0.77 0.11 0.22 0.10 2.674 10 noun 0 2 2 0.77 0.11 0.23 0.10 2.594 10 noun 1 2 0 0 0.82 0.10 0.29 0.10 2.564 10 noun 1 2 1 1 0.80 0.10 0.34 0.10 2.274 10 noun 1 2 2 2 0.80 0.10 0.35 0.10 2.22
1 5 noun 0.93 0.05 0.86 0.04 0.882 5 noun 0.66 0.23 0.17 0.08 1.613 5 noun 0 0.70 0.17 0.21 0.11 1.764 5 noun 0 0 0 0.69 0.21 0.20 0.09 1.634 5 noun 0 1 1 0.64 0.21 0.18 0.08 1.58
Table 5: Results ? Discrimination size
some more specific and some more general. As
regards the more specific issues,
? Algorithm 2 does not perform morphological
processing, whereas Algorithm 3 does. How
much of the improved effectiveness of Algo-
rithm 3 is due to this fact? To answer this
question, Algorithm 2 could be enhanced to
include a morphological processor.
? The effectiveness of Algorithms 3 and 4 may
be hindered by the fact that many words are
not yet included in theWordNet database (see
Figure 6). Falling back on to Algorithm 2
proved not to be a solution. The impact of the
incompleteness of the lexical resource should
be investigated and assessed more precisely.
Another venue of research may be to exploit
different thesauri, such as the ones automati-
cally derived as in (Curran andMoens, 2002).
? The performance of Algorithm 4 might be
improved by using more sophisticated word
sense disambiguation methods. It would also
be interesting to explore the application of
the unsupervised method described in (Mc-
Carthy et al, 2004).
As regards our long term plans, first, structural
properties of the ontologies could potentially be
exploited for the computation of node signatures.
This kind of enhancement would make our system
move from a purely instance based approach to a
combined hybrid approach based on schema and
instances.
More fundamentally, we need to address the
lack of appropriate, domain specific resources that
can support the training of algorithms and models
appropriate for the task at hand. WordNet is a very
general lexicon that does not support domain spe-
cific vocabulary, such as that used in geosciences
or in medicine or simply that contained in a sub-
ontology that users may define according to their
interests. Of course, we do not want to develop
by hand domain specific resources that we have to
change each time a new domain arises.
The crucial research issue is how to exploit ex-
tremely scarce resources to build efficient and ef-
fective models. The issue of scarce resources
makes it impossible to use methods that are suc-
cesful at discriminating documents based on the
words they contain but that need large corpora
for training, for example Latent Semantic Anal-
ysis (Landauer et al, 1998). The experiments de-
scribed in this paper could be seen as providing
57
a bootstrapped model (Riloff and Jones, 1999; Ng
and Cardie, 2003)?in ML, bootstrapping requires
to seed the classifier with a small number of well
chosen target examples. We could develop a web
spider, based on the work described on this paper,
to automatically retrieve larger amounts of train-
ing and test data, that in turn could be processed
with more sophisticated NLP techniques.
Acknowledgements
This work was partially supported by NSF Awards
IIS?0133123, IIS?0326284, IIS?0513553, and
ONR Grant N00014?00?1?0640.
References
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David
Martinez. 2000. Enriching very large ontologies
using the WWW. In ECAI Workshop on Ontology
Learning, Berlin, August.
Isabel F. Cruz and Afsheen Rajendran. 2003. Ex-
ploring a new approach to the alignment of ontolo-
gies. In Workshop on Semantic Web Technologies
for Searching and Retrieving Scientific Data, in co-
operation with the International Semantic Web Con-
ference.
Isabel F. Cruz, William Sunna, and Anjli Chaudhry.
2004. Semi-automatic ontology alignment for
geospatial data integration. GIScience, pages 51?
66.
James Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Work-
shop on Unsupervised Lexical Acquisition, pages
59?67, Philadelphia, PA, USA.
AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pe-
dro Domingos, and Alon Halevy. 2003. Learning to
match ontologies on the semantic web. VLDB Jour-
nal, 12(4):303?319.
Je?ro?me Euzenat, Heiner Stuckenschmidt, and
Mikalai Yatskevich. 2005. Introduction
to the ontology alignment evaluation 2005.
http://oaei.inrialpes.fr/2005/results/oaei2005.pdf.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontology. In R. Green, C. A. Bean, and S. H.
Myaeng, editors, Semantics of Relationships: An In-
terdisciplinary Perspective, pages 91?110. Kluwer.
T. C. Hughes and B. C. Ashpole. 2005. The semantics
of ontology alignment. Draft Paper, Lockheed Mar-
tin Advanced Technology Laboratories, Cherry Hill,
NJ. http://www.atl.lmco.com/projects/ontology/ pa-
pers/ SOA.pdf.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259?284.
Shuang Liu, Clement Yu, and Weiyi Meng. 2005.
Word sense disambiguation in queries. In ACM
Conference on Information and Knowledge Man-
agement (CIKM2005), Bremen, Germany.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In 42nd Annual Meeting of the As-
sociation for Computational Linguistics, Barcelona,
Spain.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. Introduction to wordnet: an on-
line lexical database. International Journal of Lexi-
cography, 3 (4):235?244.
Vincent Ng and Claire Cardie. 2003. Bootstrapping
coreference classifiers with multiple machine learn-
ing algorithms. In The 2003 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2003).
Natalya Fridman Noy and Mark A. Musen. 2000.
Prompt: Algorithm and tool for automated ontology
merging and alignment. In National Conference on
Artificial Intelligence (AAAI).
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using semantic relatedness for
word sense disambiguation. In Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CiCLING-03), Mexico City.
Erhard Rahm and Philip A. Bernstein. 2001. A sur-
vey of approaches to automatic schema matching.
VLDB Journal, 10(4):334?350.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, Sixteenth National Con-
ference on Artificial Intelligence.
Rajen Subba and Sadia Masud. 2004. Automatic gen-
eration of a thesaurus using wordnet as a means to
map concepts. Tech report, University of Illinois at
Chicago.
Dan Tufis and Oliver Mason. 1998. Tagging romanian
texts: a case study for qtag, a language independent
probabilistic tagger. In First International Confer-
ence on Language Resources & Evaluation (LREC),
pages 589?596, Granada, Spain.
Kurt VanLehn, Collin Lynch, Kay Schulze, Joel
Shapiro, Robert Shelby, Linwood Taylor, Donald
Treacy, Anders Weinstein, and Mary Wintersgill.
2005. The andes physics tutoring system: Five years
of evaluations. In 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
58
Simple but effective feedback generation to tutor abstract problem solving
Xin Lu, Barbara Di Eugenio, Stellan Ohlsson and Davide Fossati
University of Illinois at Chicago
Chicago, IL 60607, USA
xinlu@northsideinc.com
bdieugen,stellan,dfossa1@uic.edu
Abstract
To generate natural language feedback for an
intelligent tutoring system, we developed a
simple planning model with a distinguishing
feature: its plan operators are derived auto-
matically, on the basis of the association rules
mined from our tutorial dialog corpus. Auto-
matically mined rules are also used for real-
ization. We evaluated 5 different versions of
a system that tutors on an abstract sequence
learning task. The version that uses our plan-
ning framework is significantly more effective
than the other four versions. We compared this
version to the human tutors we employed in
our tutorial dialogs, with intriguing results.
1 Introduction
Intelligent Tutoring Systems (ITSs) are software
systems that provide individualized instruction, like
human tutors do in one-on-one tutoring sessions.
Whereas ITSs have been shown to be effective in
engendering learning gains, they still are not equiv-
alent to human tutors. Hence, many researchers
are exploring Natural Language (NL) as the key to
bridging the gap between human tutors and current
ITSs. A few results are now available, that show that
ITS with relatively sophisticated language interfaces
are more effective than some other competitive con-
dition (Graesser et al, 2004; Litman et al, 2006;
Evens and Michael, 2006; Kumar et al, 2006; Van-
Lehn et al, 2007; Di Eugenio et al, 2008). Ascer-
taining which specific features of the NL interaction
are responsible for learning still remains an open re-
search question.
In our experiments, we contrasted the richness
with which human tutors respond to student ac-
tions with poorer forms of providing feedback, e.g.
only graphical. Our study starts exploring the role
that positive feedback plays in tutoring and in ITSs.
While it has long been observed that most tutors tend
to avoid direct negative feedback, e.g. (Fox, 1993;
Moore et al, 2004), ITSs mostly provide negative
feedback, as they react to student errors.
In this paper, we will first briefly describe our tu-
torial dialog collection. We will then present the
planning architecture that underlies our feedback
generator. Even if our ITS does not currently al-
low for student input, our generation architecture is
inspired by state-of-the art tutorial dialog manage-
ment (Freedman, 2000; Jordan et al, 2001; Zinn et
al., 2002). One limitation of these approaches is that
plan operators are difficult to maintain and extend,
partly because they are manually defined and tuned.
Crucially, our plan operators are automatically de-
rived via the association rules mined from our cor-
pus. Finally, we will devote a substantial amount
of space to evaluation. Our work is among the first
to show not only that a more sophisticated language
interface results in more learning, but that it favor-
ably compares with human tutors. Full details on
our work can be found in (Lu, 2007).
2 Task and curriculum
Our domain concerns extrapolating letter patterns,
such as inferring EFMGHM, given the pattern
ABMCDM and the new initial letter E. This task is
used in cognitive science to investigate human in-
formation processing (Kotovsky and Simon, 1973;
Reed and Johnson, 1994; Nokes and Ohlsson, 2005).
The curriculum we designed consists of 13 patterns
of increasing length and difficulty; it was used un-
changed in both our human data collection and ITS
experiments. The curriculum is followed by two
104
Tutor Moves
Answer student?s questions
Evaluate student?s actions
Summarize what done so far
Prompt student into activity
Diagnose what student is doing
Instruct
Demonstrate how to solve (portions of) problem
Support ? Encourage student
Conversation ? Acknowledgments, small talk
Student Moves
Question
Explain what student said or did
Reflect ? Evaluate own understanding
Answer tutor?s question
Action Response ? Perform non-linguistic action
(e.g. write letter down)
Complete tutor?s utterance
Conversation ? Acknowledgments, small talk
Table 1: Tutor and student moves
post-test problems, each 15 letter long: subjects (all
from the psychology subject pool) have n trials to
extrapolate each pattern, always starting from a dif-
ferent letter (n = 6 in the human conditions, and
n = 10 in the ITS conditions). While the ear-
lier example was kept simple for illustrative pur-
poses, our patterns become very complex. Start-
ing e.g. from the letter L, we invite the reader to
extrapolate problem 9 in our curriculum: BDDFF-
FCCEEGGGC, or the second test problem: ACZD-
BYYDFXGEWWGI.1
3 Human dialogs
Three tutors ? one expert, one novice, and one
(the lecturer) experienced in teaching, but not in
one-on-one tutoring ? were videotaped as they in-
teracted with 11 subjects each.2 A repeated mea-
sures ANOVA, followed by post-hoc tests, revealed
that students with the expert tutor performed signifi-
cantly better than the students with the other two tu-
tors on both test problems (p < 0.05 in both cases).
36 dialog excerpts were transcribed, taken from
1The solutions are, respectively: LNNPPPMMOOQQQM,
and LNZOMYYOQXRPWWRT .
2One goal of ours was to ascertain whether expert tutors are
indeed more effective than non-expert tutors, not at all a fore-
gone conclusion since very few studies have contrasted expert
and non expert tutors, e.g. (Glass et al, 1999).
18 different subjects (6 per tutor), for a total of
about 2600 tutor utterances and 660 student ut-
terances (transcription guidelines were taken from
(MacWhinney, 2000)). For each subject, these two
dialog excerpts cover the whole interaction with the
tutor on one easy and one difficult problem (# 2 and
9 respectively). 2 groups of 2 coders each, annotated
half of the transcripts each, with dialogue moves.
Our move inventory comprises 9 tutor moves and
7 student moves, as listed in Table 1.3 Table 2
presents an annotated fragment from one of the di-
alogues with the expert tutor. Kappa measures for
intercoder agreement had values in the following
ranges, according to the scale in (Rietveld and van
Hout, 1993): for tutor moves, from moderate (0.4
for Support) to excellent (0.82 for Prompt); for stu-
dent moves, from substantial (0.64 for Explanation)
to excellent (0.82 for Question, 0.97 for ActionRe-
sponse). Whereas some of these Kappa measures
are lower than what we had strived for, we decided
to nonetheless use the move inventory in its entirety,
after the coders reconciled their codings. In fact, our
ultimate evaluation measure concerns learning, and
indeed the ITS version that uses that entire move in-
ventory engenders the most learning. Please see (Lu
et al, 2007) for a detailed analysis of these dialogs
and for a discussion of differences among the tutors
in terms of tutor and student moves.
The transcripts were further annotated by one
coder for tutor attitude (whether the tutor agrees
with the student?s response ? positive, negative, neu-
tral), for correctness of student move and for stu-
dent confidence (positive, negative, neutral). Stu-
dent hesitation time (long, medium, short) was es-
timated by the transcribers. Additionally, we an-
notated for the problem features under discussion.
Of the 8 possible relationships between letters, most
relevant to the examples discussed in this paper are
forward, backward, progression and marker. E.g. in
ABMCDM, M functions as chunk marker, and the
sequence moves forward by one step, both within
one chunk and across chunks. Within and across are
two out of 4 relationship scopes, which encode the
coverage of a particular relationship within the se-
quence.
3There is no explicit tutor question move because we focus
on the goal of a tutor?s question, either prompt or diagnose.
105
Line Utterances Annotation
38 Tutor: how?d you actually get the n in the first place? Diagnose
39 Student: from here I count from c to g and then just from n to r. Answer
40 Tutor: okay so do the c to g. Prompt
41 Tutor: do it out loud so I can hear you do it. Prompt
42 Student: c d e f. Explain
43 Student: so it?s three spaces. Answer
44 Tutor: okay so it?s three spaces in between. Summarize
Table 2: An annotated fragment from a dialogue with the expert tutor
4 Learning rules to provide feedback
Once the corpus was annotated, we mined the ex-
pert tutor portion via Classification based on Associ-
ations (CBA) (Liu et al, 1998). CBA generates un-
derstandable rules and has been effectively applied
to various domains. CBA finds all rules that exist
in the data, which is especially important for small
training sets such as ours.
To modularize what the rules should learn, we de-
composed what the tutor should do into two com-
ponents pertaining to content: letter relationship and
relationship scope; and two components pertaining
to how to deliver that content: tutor move and tutor
attitude. Hence, we derived 4 sets of tutorial rules.
Features used in the rules are those annotated on the
tutoring dialogs, plus the student?s Knowledge State
(KS) on each type of letter relationship rel, com-
puted as follows:
KS(rel) = b
p? 0.5 + w
t
? 5c (1)
p is the number of partially correct student inputs, w
is the number of wrong student inputs and t is the to-
tal number of student inputs (?inputs? here are only
those relevant to the relationship rel, as logged by
the ITS from the beginning of the session). KS(rel)
ranges from 0 to 5. The higher the value, the worse
the performance on rel. The scale of 5 was chosen
to result in just enough values for KS(rel) to be use-
ful for classification.
We ran experiments with different lengths of dia-
log history, but using only the last utterance gave us
the best results. Three of the four rule sets have ac-
curacies between 88 and 90% (results are based on
6-way cross-validation, and the cardinality of each
set of rules is in the low hundreds. ). Whereas the
tutor move rule set only has 57% accuracy, as for
some of the low Kappa values mentioned earlier, our
relation-marker = No, relation-forward = Yes,
student-move = ActionResponse
? relation-forward = Yes
(Confidence = 100%, Support = 4.396%)
correctness = wrong, scope-within = No,
KS(backward) = 0, relation-forward = Yes
? tutor-move = Summarize
(Confidence = 100%, Support = 6.983%)
correctness = wrong, relation-forward = Yes,
KS(forward) = 1, hesitation = no
? tutor-attitude = negative
(Confidence = 100%, Support = 1.130%)
Figure 1: Example Tutorial Rules
ultimate evaluation measure is that the NL feedback
based on these rules does improve learning.
Figure 1 shows three example rules, for choosing
relationship, move and attitude respectively ? we?ll
discuss two of them. The first rule predicts that the
ITS will continue focusing on the forward relation,
if it was focusing on forward and not on marker, and
the student just input something. The second rule
chooses the summarize move if the student made a
mistake, the ITS was focusing on forward but not on
relationships within chunks, and the student showed
perfect knowledge of backward.
Two strength measurements are associated with
each rule X ? y. A rule holds with confidence
conf if conf% of cases that containX also contain y;
and with support sup if sup% of cases contain X or
y. Rules are ordered, with confidence having prece-
dence over support. Ties over confidence are solved
via support; any remaining ties are solved according
to the order rules were generated.
106
For each Tut-Move-Rule TMRi,k whose Left-Hand Side LHS matches ISi do:
1. Create and Populate New Plan pi,k:
(a) preconditions = ISi; actions = tutor move from TMRi,k; strength = confidence and support from TMRi,k
(b) Fill remaining slots in pi,k:
i. contents = relationship ? scope (from highest ranked rules that match ISi from relationship and scope
rule sets);
ii. modifiers = attitude (from highest ranked rule that matches ISi from tutor attitude rule set)
2. Augment Plan: do the following n times :
(a) make copy of ISi and name it ISi+1;
(b) change agent to ?tutor?;
(c) change corresponding elements in ISi+1 to move, attitude, letter relationship and scope from pi,k;
(d) from the two rule sets for tutor move and tutor attitude, retrieve highest ranked rules that match ISi+1,
TMRi+1,j and TARi+1,j
(e) add to actions tutor move from TMRi+1,j ; add to modifiers tutor attitude from TARi+1,j
Figure 2: Plan generation
5 From rules to plans
For our task of extrapolating abstract sequences, we
built a model-tracing ITS by means of the Tutoring
Development Kit (TDK) (Koedinger et al, 2003).
Model-tracing ITSs codify cognitive skills via pro-
duction rules. The student?s solution is monitored
by rules that fire according to the underlying model.
When the student steps differ from that model, an
error is recognized. A portion of the student inter-
face of the ITS is shown in Figure 4a. It mainly in-
cludes two rows, one showing the Example Pattern,
the other for the student to input the New Pattern
extrapolated starting with the letter in the first cell.
In model-tracing ITSs, production rules provide
the capability to generate simple template-based
messages. We developed a more sophisticated NL
feedback generator consisting of three major mod-
ules: update, planning and feedback realization.
The update module maintains the context, rep-
resented by the Information State (IS) (Larsson
and Traum, 2000), which captures the overall dia-
log context and interfaces with external knowledge
sources (e.g., curriculum, tutorial rules) and the pro-
duction rule system. As the student performs a new
action, the IS is updated. The planning module gen-
erates or revises the system plan and selects the next
tutoring move based on the newly updated IS. At
last the feedback realization module transforms this
move into NL feedback.
The planning module consists of three compo-
nents, plan generation, plan selection and plan mon-
itoring. A plan includes an ordered collection of tu-
toring moves meant to help the student correctly fill
a single cell. The structure of our plans is shown in
Figure 3.
Plan generation generates a plan set which con-
tains one plan for each tutor move rule that matches
the current ISi. Each of these plans is augmented at
plan generation time by ?simulating? the next ISi+1
that would result if the move is executed but its ef-
fects are not achieved. The algorithm is sketched
in Figure 2. The planner iterates through the tutor
move rule set.4 Recall that our four rule sets are to-
tally ordered. Also, note that each rule set contains a
default rule that fires when no rule matches the cur-
rent ISi. In Step 1b, at every iteration only the rules
that have not been checked yet from those three rule
sets are considered. In Step 2, n is set to 3, i.e., each
plan contains 3 additional moves and corresponding
attitudes, which will provide hints when no response
from the student occurs. Three hints plus one orig-
inal move makes 4, which is the average number of
moves in one turn of the expert tutor.
An example plan is shown in Figure 3. It is gen-
erated in reaction to the mistake in Figure 4a, and by
4Since there is no language input, rules which include stu-
dent moves other than ActionResponse in their LHS will never
be activated. Additionally, we recast tutor answers as confirm
moves, since students cannot ask questions.
107
Preconditions (same as the IS in Figure 4b)
Effects student?s input = W
Contents relationship = forward
scope = across
Actions summarize, evaluate, prompt,
summarize
Modifiers negative, negative, neutral, neutral
Strength conf = 100%, sup = 6.983%
Figure 3: An Example Plan
firing, among others, the rules in Figure 1. The IS in
Figure 4b reflects some of the history of this interac-
tion (in the slots Relationships, Scopes and KS), and
as such corresponds to the situation depicted in Fig-
ure 4a in a specific context (this plan was generated
in one of our user experiments).
The plan selection component retrieves the high-
est ranked plan in the newly generated plan set, se-
lects a template for each tutoring move in its ?Ac-
tions? slot and puts each tutoring move onto the di-
alog move (DM) stack. Earlier we mentioned that
rules are totally ordered according to confidence,
then support and finally rule generation order. When
a plan set contains more than one plan, plans are
also totally ordered, since they inherit strength mea-
surements from the rule that engenders the first tutor
move in the Actions slot.
After the student receives the message which
realizes the top tutoring move in the DM stack,
plan monitoring checks whether its intended effects
have been obtained. If the effects have not been ob-
tained, and the student?s input is unchanged, the next
move from the DM stack will be executed to pro-
vide the student with hint messages until either the
student?s input changes or the DM stack becomes
empty. If the DM stack becomes empty, the next
plan is selected from the original plan set and the
tutoring moves within that plan are pushed onto the
DM stack. Whenever the student?s input changes,
or after every plan in the plan set has been selected,
control returns to plan generation.
The realization module. A tutor move is pushed
onto the DM stack by plan selection together with
a template to realize it. 50 templates were writ-
ten manually upon inspection of the expert tutor di-
alogs. Since several templates can realize each tutor
move, we used CBA to learn rules to choose among
templates. Features used to learn when to use each
(a) A Student Action in Problem 4
1. Agent: student (producer of current move);
2. Relationships: forward, progress in length
3. Scopes: across (for ?forward?), within (for
?progress in length?);
4. Agent?s move: action response;
5. Agent?s attitude: positive (since student shows
no hesitation before inputting letter);
6. Correctness: wrong (correct letter is W);
7. Student?s input: X;
8. Student?s selection: 4th cell in New Pattern row;
9. Hesitation time: no;
10. Student?s knowledge state (KS): 1 (on ?for-
ward?), 3 (on ?progress in length?).
(b) The corresponding IS
Figure 4: A snapshot of an ITS interaction
template also include the tutor attitude. For the first
Summarize move in the plan in Figure 3, given the
IS in Figure 4b, the rule in Fig. 5 will fire (tutor at-
titude does not affect this specific rule). As a result,
the following feedback message is generated: ?From
V to X, you are going forward 2 in the alphabet.?
6 Evaluation
To demonstrate the utility of our feedback genera-
tor, we developed five different versions of our ITS,
named according to how feedback is generated:
1. No feedback: The ITS only provides the basic
interface, so that subjects can practice solving
the 13 problems in the curriculum, but does not
provide any kind of feedback.
2. Color only: The ITS provides graphic feed-
back by turning the input green if it is correct
or red if it is wrong.
3. Negative: In addition to the color feedback, the
108
scope-within = No, relation-marker = No,
relation-forward = Yes, move= Summarize ?
template = TPL11
[where TPL11: From ?<reference-pattern>?
to ?<input>?, you are going <input-relation>
<input-number> in the alphabet.]
Figure 5: Example Realization Rule
ITS provides feedback messages when the in-
put is wrong.
4. Positive: In addition to the color feedback, the
ITS provides feedback messages when the in-
put is correct.
5. Model: In addition to the color feedback, the
ITS provides feedback messages generated by
the feedback generator just described.
Feedback is given for each input letter. Positive
and negative verbal feedback messages are given
out whenever the student?s input is correct or incor-
rect, respectively. Positive feedback messages con-
firm the correct input and explain the relationships
which this input is involved in. Negative feedback
messages flag the incorrect input and deliver hints.
The feedback messages for the ?negative? and ?pos-
itive? versions were developed earlier in the project,
to avoid repetitions and inspired by the expert tu-
tor?s language but before we performed any anno-
tation and mining. They are directly generated by
TDK production rules.
Although in reality positive and negative feedback
are both present in tutoring sessions, one study for
the letter pattern task shows that positive/negative
feedback, given independently, perform different
functions (Corrigan-Halpern and Ohlsson, 2002). In
addition, our negative condition is meant to embody
the ?classical? model-tracing ITS, that only reacts
to student errors. Hence, in our experiments, we
elected to keep these two types of feedback separate,
other than in the ?model? version of the ITS.
To evaluate the five versions of the ITS, we ran a
between-subjects study in which each group of sub-
jects interacted with one version of the system. A
group of control subjects took the post-test with no
training at all but only read a short description of the
Score
Condition Prob 1 Prob 2 Total
0 Control 36.50 32.84 69.34
1 No feedback 58.21 75.27 133.48
2 Color only 68.32 66.30 134.62
3 Negative 70.33 66.06 141.83
4 Positive 75.06 79.00 154.06
5 Model 91.95 101.76 193.71
Table 3: Average Post-test Scores of the ITS
domain.5 Subjects were trained to solve the same
13 problems in the curriculum that were used in the
human tutoring condition. They also did the same
post-test (2 problems, each pattern 15 letters long).
For each post-test problem, each subject had 10 tri-
als, where each trial started with a new letter.
6.1 Results
Table 3 reports the average post-test scores of the
six groups of subjects, corresponding to the five ver-
sions of the ITS and the control condition. Perfor-
mance on each problem is measured by the number
of correct letters out of a total of 150 letters (15 let-
ters by 10 trials); hence, cumulative post-test score,
is the number of correct letters out of 300 possible.
A note before we proceed. In ITS research it is
common to administer the same test before (pre-test)
and after treatment (post-test), but we only have the
post-test. The pre/post-test paradigm is used for two
reasons. First, for evaluation proper, to gauge learn-
ing gains. Second, to verify that the groups have the
same level of pre-tutoring ability, as shown when the
pre-tests of the different groups are statistically in-
distinguishable, and hence, that they can be rightly
compared. Even without a pre-test we can assess
this. An ANOVA on ?time spent on the first 3 prob-
lems? revealed no significant differences across the
different groups. Since time spent on the first 3 prob-
lems is highly correlated with post-test score (multi-
ple regression, p < 0.03), this provides indirect evi-
dence that all subjects before treatment have equiva-
lent ability for this task. Hence, we can trust that our
evaluation, in terms of absolute scores, does reveal
differences between conditions.
Our main findings are based on one-way
ANOVAs, followed by Tukey post-hoc tests:
5The number of subjects in each condition varies from 32 to
38. Groups differ in size because of technical problems.
109
? A main effect of ITS (p ? 0.05). Subjects who
interacted with any version of the ITS had sig-
nificantly higher total post-test scores than sub-
jects in the control condition.
? A main effect of modeled feedback (p< 0.05).
Subjects who interacted with the ?model? ver-
sion of the ITS had significantly higher total
post-test scores than control subjects, and sub-
jects with any other version of the ITS.
? No other effects. Subjects trained by the three
versions ?color only?, ?negative?, ?positive?,
did not have significantly higher total post-test
scores than subjects with the ?no feedback?
version; neither did subjects trained by the two
versions ?negative?, ?positive?, wrt subjects
with the ?color-only? version.
If we examine individual problems, the same pat-
tern of results hold, other than, interestingly, the
model and positive versions are not significantly dif-
ferent any more. As customary, we also analyze ef-
fect sizes , i.e., how much more subjects learn with
the ?model? ITS in comparison to the other condi-
tions. On the Y axis, Figure 6 shows Cohen?s d, a
common measure of effect size. Each point repre-
sents the difference between the means of the scores
in the ?model? ITS and in one of the other condi-
tion, divided by the standard deviation of either con-
dition. According to (Cohen, 1988), the effect sizes
shown in Figure 6 are large as concerns the compari-
son with the ?no feedback?, ?color only? and ?nega-
tive? conditions, and moderate as concerns the ?pos-
itive? condition.6
ITSs and Human Tutors. After we established
that, at least cumulatively, the ?model? ITS is more
effective than the other ITSs, we wanted to assess
how well the ?model? ITS fares in comparison to
the expert tutor it is modeled on. Since in the human
data each post-test problem consists of only 6 trials,
the first 6 trials per problem from the ITSs are used
to run this comparison, for a maximum total score
of 180 (15 letters by 6 trials, by 2 problems). Fig-
ure 7 shows the overall post-test performance of all
9 conditions. The error bars in the figure represent
the standard deviations.
6A very large effect size with respect to control is not shown
in Figure 6.
Figure 6: Effect sizes: how much more subjects
learn with the ?model? ITS
Figure 7: Post-test performance ? all conditions
Paired t-tests between the model ITS and each of
the human tutors show that:7
? on problem 1, the ?model? ITS is indistinguish-
able from the expert tutor, and is significantly
better than the novice and the lecturer
(p = 0.05 and p = 0.039 respectively);
? on problem 2, the model ITS is significantly
worse than the expert tutor (p = 0.020), and
is not different from the other two tutors;
? cumulatively, there are no significant differ-
ences between the ?model? ITS and any of the
three human tutors.
7A 9-way ANOVA among all conditions is not appropriate,
since in a sense we have two ?super conditions?, human and
ITS. It is better to compare the ?model? ITS to each of the human
tutors via t-tests, as a follow-up to the differences highlighted by
the separate analyses on the two ?super conditions?.
110
7 Discussion and Conclusions
Our results add to the growing body of evidence
that language feedback engenders more learning not
only than simple practice, but also, than less sophis-
ticated language feedback. Importantly, our ?model?
ITS appears intriguingly close to our expert tutor in
effectiveness: on post-test problem 1, it is as effec-
tive as the expert tutor himself, and significantly bet-
ter than the other two tutors, as the expert tutor is. It
appears our ?model? ITS does capture at least some
features of successful tutoring.
As concerns the specific language the ITS gen-
erates, we compared different ways of providing
verbal feedback. A subject receives both positive
and negative verbal feedback when interacting with
the ?model? version, while a subject receives only
one type of verbal feedback when interacting with
the ?positive? and ?negative? versions (recall that
in all these versions including the ?model? ITS the
red/green graphical feedback is provided on every
input). While we cannot draw definite conclusions
regarding the functions of positive and negative
feedback, since the ?model? version provides other
tutorial moves beyond positive / negative feedback,
we have suggestive evidence that negative feedback
by itself is not as effective. Additionally, positive
feedback appears to play an important role. First,
the ?model? and the ?positive? versions are statisti-
cally equivalent when we analyze performance on
individual problems. Further, in the ?model? ver-
sion, the ratio of positive to negative messages turns
out to be 9 to 1. In our tutoring dialogs, positive
feedback still outnumbers negative feedback, but by
a lower margin, 4 to 1. We hypothesize that convey-
ing a positive attitude in an ITS is perhaps even more
important than in human tutoring since a human has
many more ways of conveying subtle shades of ap-
proval and disapproval.
From the NLG point of view, we have presented
a simple generation architecture that turns out to be
rather effective. Among its clear limitations are the
lack of hierarchical planning, and the fact that dif-
ferent components of a plan are generated indepen-
dently one from the other. Among its strengths are
that the plan operators are derived automatically via
the rules we mined, both for content planning and,
partly, for realization.
It clearly remains to be seen whether our NLG
framework can easily be ported to other domains
? the issue is not domain dependence, but whether
a more complex domain will require some form of
hierarchical planning. We are now working in the
domain of Computer Science data structures and al-
gorithms, where we continue exploring the role of
positive feedback. We collected data with two tu-
tors in that domain, and there again, we found that
in the human data positive feedback occurs about 8
times more often than negative feedback. We are
now annotating the data to mine it as we did here,
and developing the core ITS.
Acknowledgments
This work was supported by awards N00014-00-
1-0640 and N00014-07-1-0040 from the Office of
Naval Research, by Campus Research Board S02
and S03 awards from the University of Illinois at
Chicago, and in part, by awards IIS 0133123 and
ALT 0536968 from the National Science Founda-
tion.
References
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences (2nd ed.). Hillsdale, NJ: Lawrence
Earlbaum Associates.
Andrew Corrigan-Halpern and Stellan Ohlsson. 2002.
Feedback effects in the acquisition of a hierarchical
skill. In Proceedings of the 24th Annual Conference
of the Cognitive Science Society.
Barbara Di Eugenio, Davide Fossati, Susan Haller, Dan
Yu, and Michael Glass. 2008. Be brief, and they
shall learn: Generating concise language feedback for
a computer tutor. International Journal of AI in Edu-
cation, 18(4). To appear.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Reva K. Freedman. 2000. Plan-based dialogue manage-
ment in a physics tutor. In Proceedings of the Sixth
Applied Natural Language Conference, Seattle, WA,
May.
Michael Glass, Jung Hee Kim, Martha W. Evens, Joel A.
Michael, and Allen A. Rovick. 1999. Novice vs. ex-
pert tutors: A comparison of style. In MAICS-99, Pro-
ceedings of the Tenth Midwest AI and Cognitive Sci-
ence Conference, pages 43?49, Bloomington, IN.
111
Arthur C. Graesser, S. Lu, G.T. Jackson, H. Mitchell,
M. Ventura, A. Olney, and M.M. Louwerse. 2004.
AutoTutor: A tutor with dialogue in natural language.
Behavioral Research Methods, Instruments, and Com-
puters, 36:180?193.
Pamela Jordan, Carolyn Penstein Rose?, and Kurt Van-
Lehn. 2001. Tools for authoring tutorial dialogue
knowledge. In Proceedings of AI in Education 2001
Conference.
Kenneth R. Koedinger, Vincent Aleven, and Neil T. Hef-
fernan. 2003. Toward a rapid development environ-
ment for cognitive tutors. In 12th Annual Conference
on Behavior Representation in Modeling and Simula-
tion.
K. Kotovsky and H. Simon. 1973. Empirical tests of a
theory of human acquisition of information-processing
analysis. British Journal of Psychology, 61:243?257.
Rohit Kumar, Carolyn P. Rose?, Vincent Aleven, Ana Igle-
sias, and Allen Robinson. 2006. Evaluating the Ef-
fectiveness of Tutorial Dialogue Instruction in an Ex-
ploratory Learning Context. In Proceedings of the
Seventh International Conference on Intelligent Tutor-
ing Systems, Jhongli, Taiwan, June.
Staffan Larsson and David R. Traum. 2000. Information
state and dialogue management in the trindi dialogue
move engine toolkit. Natural Language Engineering,
6(3-4):323?340.
Diane J. Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man. 2006. Spoken versus typed human and computer
dialogue tutoring. International Journal of Artificial
Intelligence in Education, 16:145?170.
Bing Liu, Wynne Hsu, and Yiming Ma. 1998. Inte-
grating classification and association rule mining. In
Knowledge Discovery and Data Mining, pages 80?86,
New York, August.
Xin Lu, Barbara Di Eugenio, Trina Kershaw, Stellan
Ohlsson, and Andrew Corrigan-Halpern. 2007. Ex-
pert vs. non-expert tutoring: Dialogue moves, in-
teraction patterns and multi-utterance turns. In CI-
CLING07, Proceedings of the 8th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 456?467. Best Student Paper
Award.
Xin Lu. 2007. Expert tutoring and natural language
feedback in intelligent tutoring systems. Ph.D. thesis,
University of Illinois - Chicago.
Brian MacWhinney. 2000. The CHILDES project. Tools
for analyzing talk: Transcription Format and Pro-
grams, volume 1. Lawrence Erlbaum, Mahwah, NJ,
third edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Timothy J. Nokes and Stellan Ohlsson. 2005. Compar-
ing multiple paths to mastery: What is learned? Cog-
nitive Science, 29:769?796.
Jonathan Reed and Peder Johnson. 1994. Assessing im-
plicit learning with indirect tests: Determining what is
learned about sequence structure. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 20(3):585?594.
Toni Rietveld and Roeland van Hout. 1993. Statistical
Techniques for the Study of Language and Language
Behaviour. Mouton de Gruyter, Berlin - New York.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial
dialogue. In ITS 2002, 6th. Intl. Conference on In-
telligent Tutoring Systems, pages 574?584, Biarritz,
France.
112
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 65?75,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exploring Effective Dialogue Act Sequences
in One-on-one Computer Science Tutoring Dialogues
Lin Chen, Barbara Di Eugenio
Computer Science
U. of Illinois at Chicago
lchen43,bdieugen@uic.edu
Davide Fossati
Computer Science
Carnegie Mellon U. in Qatar
davide@fossati.us
Stellan Ohlsson, David Cosejo
Psychology
U. of Illinois at Chicago
stellan,dcosej1@uic.edu
Abstract
We present an empirical study of one-on-
one human tutoring dialogues in the domain
of Computer Science data structures. We
are interested in discovering effective tutor-
ing strategies, that we frame as discovering
which Dialogue Act (DA) sequences corre-
late with learning. We employ multiple lin-
ear regression, to discover the strongest mod-
els that explain why students learn during
one-on-one tutoring. Importantly, we define
?flexible? DA sequence, in which extraneous
DAs can easily be discounted. Our experi-
ments reveal several cognitively plausible DA
sequences which significantly correlate with
learning outcomes.
1 Introduction
One-on-one tutoring has been shown to be a very ef-
fective form of instruction compared to other educa-
tional settings. Much research on discovering why
this is the case has focused on the analysis of the
interaction between tutor and students (Fox, 1993;
Graesser et al, 1995; Lepper et al, 1997; Chi et al,
2001). In the last fifteen years, many such analyses
have been approached from a Natural Language Pro-
cessing (NLP) perspective, with the goal of build-
ing interfaces that allow students to naturally inter-
act with Intelligent Tutoring Systems (ITSs) (Moore
et al, 2004; Cade et al, 2008; Chi et al, 2010).
There have been two main types of approaches to
the analysis of tutoring dialogues. The first kind
of approach compares groups of subjects interact-
ing with different tutors (Graesser et al, 2004; Van-
Lehn et al, 2007), in some instances contrasting the
number of occurrences of relevant features between
the groups (Evens and Michael, 2006; Chi et al,
2010). However, as we already argued in (Ohlsson
et al, 2007), this code-and-count methodology only
focuses on what a certain type of tutor (assumed to
be better according to certain criteria) does differ-
ently from another tutor, rather than on strategies
that may be effective independently from their fre-
quencies of usage by different types of tutor. Indeed
we had followed this same methodology in previous
work (Di Eugenio et al, 2006), but a key turning
point for our work was to discover that our expert
and novice tutors were equally effective (please see
below).
The other kind of approach uses linear regression
analysis to find correlations between dialogue fea-
tures and learning gains (Litman and Forbes-Riley,
2006; Di Eugenio et al, 2009). Whereas linear
regression is broadly used to analyze experimental
data, only few analyses of tutorial data or tutoring
experiments use it. In this paper, we follow
Litman and Forbes-Riley (2006) in correlating se-
quences of Dialogue Acts (DAs) with learning gains.
We extend that work in that our bigram and trigram
DAs are not limited to tutor-student DA bigrams ?
Litman and Forbes-Riley (2006) only considers bi-
grams where one DA comes from the tutor?s turn
and one from the student?s turn, in either order. Im-
portantly, we further relax constraints on how these
sequences are built, in particular, we are able to
model DA sequences that include gaps. This allows
us to discount the noise resulting from intervening
DAs that do not contribute to the effectiveness of
the specific sequence. For example, if we want to
65
explore sequences in which the tutor first provides
some knowledge to solve the problem (DPI) and
then knowledge about the problem (DDI) (DPI and
DDI will be explained later), an exchange such as
the one in Figure 1 should be taken into account
(JAC and later LOW are the tutors, students are indi-
cated with a numeric code, such as 113 in Figure 1).
However, if we just use adjacent utterances, the ok
from the student (113) interrupts the sequence, and
we could not take this example into account. By al-
lowing gaps in our sequences, we test a large number
of linear regression models, some of which result in
significant models that can be used as guidelines to
design an ITS. Specifically, these guidelines will be
used for further improvement of iList, an ITS that
provides feedback on linked list problems and that
we have developed over the last few years. Five
different versions of iList have been evaluated with
220 users (Fossati et al, 2009; Fossati et al, 2010).
iList is available at http://www.digitaltutor.net, and
has been used by more than 550 additional users at
15 different institutions.
JAC: so we would set k equal to e and then delete. [DPI]
113: ok.
JAC: so we?ve inserted this whole list in here.[DDI]
113: yeah.
Figure 1: {DPI, DDI} Sequence Excerpt
The rest of the paper is organized as follows.
In Section 2, we describe the CS-Tutoring corpus,
including data collection, transcription, and anno-
tation. In Section 3, we introduce our methodol-
ogy that combines multiple linear regression with n-
grams of DAs that allow for gaps. We discuss our
experiments and results in Section 4.
2 The CS Tutoring Corpus
2.1 Data Collection
During the time span of 3 semesters, we collected a
corpus of 54 one-on-one tutoring sessions on Com-
puter Science data structures: linked list, stack and
binary search tree. (In the following context, we
will refer them as Lists, Stacks and Trees). Each stu-
dent only participated in one session, and was ran-
domly assigned to one of two tutors: LOW, an expe-
rienced Computer Science professor, with more than
30 years of teaching experience; or JAC, a senior un-
dergraduate student in Computer Science, with only
one semester of previous tutoring experience. In the
end 30 students interacted with LOW and 24 with
JAC.
Students took a pre-test right before the tutoring
session, and an identical post-test immediately after.
The test had two problems on Lists, two problems on
Stacks, and four problems on Trees. Each problem
was graded out of 5 points, for a possible maximum
score of 10 points each for Lists and Stacks, and 20
points for Trees. Pre and post-test scores for each
topic were later normalized to the [0..1] interval, and
learning gains were computed.
Table 1 includes information on session length.
Note that for each topic, the number of sessions is
lower than 54. The tutor was free to tutor on what
he felt was more appropriate, after he was given an
informal assessment of the student?s performance on
the pre-test (tutors were not shown pre-tests to avoid
that they?d tutor to the pre-test only). Hence, not
every student was tutored on every topic.
Topic N Session length (minutes)Min Max Total ? ?
Lists 52 3.4 41.4 750.4 14.4 5.8
Stacks 46 0.3 9.4 264.5 5.8 1.8
Trees 53 9.1 40.0 1017.6 19.2 6.6
Sessions 54 12.8 61.1 2032.5 37.6 6.1
Table 1: CS Tutoring Corpus - Descriptives
Each tutoring session was videotaped. The cam-
era was pointing at the sheets of paper on which tu-
tors and students were writing during the session.
The videos were all transcribed. The transcripts
were produced according to the rules and conven-
tions described in the transcription manual of the
CHILDES project (MacWhinney, 2000). Dialogue
excerpts included in this paper show some of the
transcription conventions. For example, ?+...?
denotes trailing, ?xxx? unintelligible speech and
?#? a short pause (see Figure 2). The CHILDES
transcription manual also provides directions on ut-
terance segmentation.
An additional group of 53 students (control
group) took the pre- and post-tests, but instead of
participating in a tutoring session they attended a
40 minute lecture about an unrelated CS topic. The
rationale for such a control condition was to assess
66
LOW: what?s the if? [Prompt]
LOW: well of course, don?t do this if t two is null so if t
two isn?t null we can do that and xxx properly # thinking
I put it in here. [DPI]
LOW: or else if t two is null that?s telling us that this is
the +. . . [Prompt,FB]
Figure 2: {Prompt,DPI,FB} sequence excerpt
whether by simply taking the pre-test students would
learn about data-structures, and hence, to tease out
whether any learning we would see in the tutored
conditions would be indeed due to tutoring.
The learning gain, expressed as the difference
between post-score and pre-score, of students that
received tutoring was significantly higher than the
learning gain of the students in the control group, for
all the topics. This was showed by ANOVA between
the aggregated group of tutored students and the
control group, and was significant at the p < 0.01
for each topic. There was no significant difference
between the two tutored conditions in terms of learn-
ing gain. The fact that students did not learn more
with the experienced tutor was an important finding
that led us to question the approach of comparing
and contrasting more and less experienced tutors.
Please refer to (Di Eugenio et al, 2009) for further
descriptive measurements of the corpus.
2.2 Dialogue Act Annotation
Many theories have been proposed as concerns DAs,
and there are many plausible inventories of DAs, in-
cluding for tutorial dialogue (Evens and Michael,
2006; Litman and Forbes-Riley, 2006; Boyer et al,
2010). We start from a minimalist point of view,
postulating that, according to current theories of
skill acquisition (Anderson, 1986; Sun et al, 2005;
Ohlsson, 2008), at least the following types of tuto-
rial intervention can be explained in terms of why
and how they might support learning:
1. A tutor can tell the student how to perform the
task.
2. A tutor can state declarative information about
the domain.
3. A tutor can provide feedback:
(a) positive, to confirm that a correct but tentative
step is in fact correct;
(b) negative, to help a student detect and correct an
error.
We first read through the entire corpus and exam-
ined it for impressions and trends, as suggested by
(Chi, 1997). Our informal assessment convinced us
that our minimalist set of tutoring moves was an ap-
propriate starting point. For example, contrary to
much that has been written about an idealized so-
cratic type of tutoring where students build knowl-
edge by themselves (Chi et al, 1994), our tutors
are rather directive in style, namely, they do a lot
of telling and stating. Indeed our tutors talk a lot,
to the tune of producing 93.5% of the total words!
We translated the four types above into the follow-
ing DAs: Direct Procedural Instruction (DPI), Di-
rect Declarative Instruction (DDI), Positive Feed-
back (+FB), and Negative Feedback (-FB). Besides
those 4 categories, we additionally annotated the
corpus for Prompt (PT), since our tutors did explic-
itly invite students to be active in the interaction.
We also annotated for Student Initiative (SI), to cap-
ture active participation on the part of the student?s.
SI occurs when the student proactively produces a
meaningful utterance, by providing unsolicited ex-
planation (see Figures 6 and 4), or by asking ques-
tions. As we had expected, SIs are not as frequent as
other moves (see below). However, this is precisely
the kind of move that a regression analysis would
tease out from others, if it correlates with learning,
even if it occurs relatively infrequently. This indeed
happens in two models, see Table 8.
Direct Procedural Instruction(DPI) occurs when
the tutor directly tells the student what task to per-
form. More specifically:
? Utterances containing correct steps that lead to
the solution of a problem, e.g. see Figure 1.
? Utterances containing high-level steps or sub-
goals (it wants us to put the new node that con-
tains G in it, after the node that contains B).
? Utterances containing tactics and strategies (so
with these kinds of problems, the first thing I
have to say is always draw pictures).
? Utterances where the tutor talked in the first-
person but in reality the tutor instructed the stu-
dent on what to do (So I?m pushing this value
onto a stack. So I?m pushing G back on).
Direct Declarative Instruction (DDI) occurred
when the tutor provided facts about the domain or
67
a specific problem. The key to determine if an ut-
terance is DDI is that the tutor is telling the student
something that he or she ostensibly does not already
know. Common sense knowledge is not DDI ( ten
is less than eleven ). Utterances annotated as DDI
include:
? Providing general knowledge about data struc-
tures (the standard format is right child is al-
ways greater than the parent, left child is al-
ways less than the parent).
? Telling the student information about a specific
problem (this is not a binary search tree).
? Conveying the results of a given action (so now
since we?ve eliminated nine, it?s gone).
? Describing pictures of data structures (and then
there is a link to the next node).
Prompts (PT) occur when the tutor attempts to
elicit a meaningful contribution from the student.
We code for six types of tutor prompts, including:
? Specific prompt: An attempt to get a specific
response from the student (that?s not b so what
do we want to do?).
? Diagnosing: The tutor attempts to determine
the student?s knowledge state (why did you put
a D there?).
? Confirm-OK: The tutor attempts to determine if
the student understood or if the student is pay-
ing attention (okay, got that idea?).
? Fill-in-the-blank: The tutor does not complete
an utterance thereby inviting the student to
complete the utterance, e.g. see Figure 2.
Up to now we have discussed annotations for ut-
terances that do not explicitly address what the stu-
dent has said or done. However, many tutoring
moves concern providing feedback to the student.
Indeed as already known but not often acted upon in
ITS interfaces, tutors do not just point out mistakes,
but also confirm that the student is making correct
steps. While the DAs discussed so far label single
utterances, our positive and negative feedback (+FB
and -FB) annotations comprise a sequence of con-
secutive utterances, that starts where the tutor starts
providing feedback. We opted for a sequence of ut-
terances rather than for labeling one single utterance
because we found it very difficult to pick one single
utterance as the one providing feedback, when the
tutor may include e.g. an explanation that we con-
sider to be part of feedback. Positive feedback oc-
curs when the student says or does something cor-
rect, either spontaneously or after being prompted
by the tutor. The tutor acknowledges the correctness
of the student?s utterance, and possibly elaborates on
it with further explanation. Negative feedback oc-
curs when the student says or does something incor-
rect, either spontaneously or after being prompted
by the tutor. The tutor reacts to the mistake and pos-
sibly provides some form of explanation.
After developing a first version of the coding
manual, we refined it iteratively. During each itera-
tion, two human annotators independently annotated
several dialogues for one DA at a time, compared
outcomes, discussed disagreements, and fine-tuned
the scheme accordingly. This process was repeated
until a sufficiently high inter-coder agreement was
reached. The Kappa values we obtained in the fi-
nal iteration of this process are listed in Table 2
(Di Eugenio and Glass, 2004; Artstein and Poesio,
2008). In Table 2, the ?Double Coded*? column
refers to the sessions that we double coded to cal-
culate the inter-coder agreement. This number does
not include the sessions which were double coded
when coders were developing the coding manual.
The numbers of double-coded sessions differ by DA
since it depends on the frequency on the particular
DA (recall that we coded for one DA at a time).
For example, since Student Initiatives (SI) are not as
frequent, we needed to double code more sessions
to find a number of SI?s high enough to compute a
meaningful Kappa (in our whole corpus, there are
1157 SIs but e.g. 4957 Prompts).
Category Double Coded* Kappa
DPI 10 .7133
Feedback 5 .6747
DDI 10 .8018
SI 14 .8686
Prompt 8 .9490
Table 2: Inter-Coder Agreement in Corpus
The remainder of the corpus was then indepen-
dently annotated by the two annotators. For our
final corpus, for the double coded sessions we did
not come to a consensus label when disagreements
arose; rather, we set up a priority order based on
68
topic and coder (e.g., during development of the
coding scheme, when coders came to consensus
coding, which coder?s interpretation was chosen
more often), and we chose the annotation by a cer-
tain coder based on that order.
As a final important note, given our coding
scheme some utterances have more than one label
(see Figures 2 and 4), whereas others are not la-
belled at all. Specifically, most student utterances,
and some tutor utterances, are not labelled (see Fig-
ures 1 and 4).
3 Method
3.1 Linear Regression Models
In this work, we adopt a multiple regression model,
because it can tell us how much variation in learning
outcomes is explained by the variation of individual
features in the data. The features we use include pre-
test score, the length of the tutoring sessions, and
DAs, both the single DAs we annotated for and DA
n-grams, i.e. DA sequences of length n. Pre-test
score is always included since the effect of previ-
ous knowledge on learning is well established, and
confirmed in our data (see all Models 1 in Table 4);
indeed multiple linear regression allows us to factor
out the effect of previous knowledge on learning, by
quantifying the predictive power of features that are
added beyond pre-test score.
3.2 n-gram Dialogue Act Model
n-grams (sequences of n units, such as words, POS
tags, dialogue acts) have been used to derive lan-
guage models in computational linguistics for a long
time, and have proven effective in tasks like part-of-
speech tagging, spell checking.
Our innovation with regard to using DA n-grams
is to allow gaps in the sequence. This allows us
to extract the sequences that are really effective,
and to eliminate noise. Note that from the point
of view of an effective sequence, noise is anything
that does not contribute to the sequence. For ex-
ample, a tutor?s turn may be interrupted by a stu-
dent?s acknowledgments, such as ?OK? or ?Uh-hah?
(see Figure 1). Whereas these acknowledgments
perform fundamental functions in conversation such
as grounding (Clark, 1992), they may not directly
correlate with learning (a hypothesis to test). If we
counted them in the sequence, they would contribute
two utterances, transforming a 3 DA sequence into a
5 DA sequence. As well known, the higher the n, the
sparser the data becomes, i.e., the fewer sequences
of length n we find, making the task of discover-
ing significant correlations all the harder. Note that
some of the bigrams in (Litman and Forbes-Riley,
2006) could be considered to have gaps, since they
pair one student move (say SI) with each tutor move
contained in the next tutor turn (eg, in our Figure 6
they would derive two bigrams [SI, FB], and [SI,
Prompt]). However, this does not result in a system-
atic exploration of all possible sequences of a certain
length n, with all possible gaps of length up to m, as
we do here.
The tool that allows us to leave gaps in sequences
is part of Apache Lucene,1 an open source full text
search library. It provides strong capabilities to
match and count efficiently. Our counting method
is based on two important features provided by
Lucene, that we already used in other work (Chen
and Di Eugenio, 2010) to detect uncertainty in dif-
ferent types of corpora.
? Synonym matching: We can specify several
different tokens at the same position in a field
of a document, so that each of them can be used
to match the query.
? Precise gaps: With Lucene, we can precisely
specify the gap between the matched query and
the indexed documents (sequences of DAs in
our case) using a special type of query called
SpanNearQuery.
To take advantage of Lucene as described above,
we use the following algorithm to index our corpus.
1. For each Tutor-Topic session, we generate n-
gram utterance sequences ? note that these are
sequences of utterances at this point, not of
DAs.
2. We prune utterance sequences where either 0
or only 1 utterance is annotated with a DA, be-
cause we are mining sequences with at least 2
DAs. Recall that given our annotation, some ut-
terances are not annotated (see e.g. Figure 1).
3. After pruning, for each utterance sequence, we
generate a Lucene document: each DA label on
an utterance will be treated as a token, multiple
1http://lucene.apache.org/
69
labels on the same utterance will be treated as
?synonyms?.
By indexing annotations as just described, we
avoid the problem of generating too many combina-
tions of labels. After indexing, we can use SpanN-
earQuery to query the index. SpanNearQuery allows
us to specify the position distance allowed between
each term in the query.
Figure 3 is the field of the generated Lucene doc-
ument corresponding to the utterance sequences in
Figure 4. We can see that each utterance of the tu-
tor is tagged with 2 DAs. Those 2 DAs produce 2
tokens, which are put into the same position. The
tokens in the same position act as synonyms to each
other during the query.
Figure 3: Lucene Document Example for DAs
258: okay.
JAC: its right child is eight. [DDI, FB]
258: uh no it has to be greater than ten. [SI]
JAC: right so it?s not a binary search tree # it?s not a b s t,
right? [DDI,Prompt]
Figure 4: {FB, SI, DDI} is most effective in Trees
4 Experiments and Results
Here we build on our previous results reported
in (Di Eugenio et al, 2009). There we had shown
that, for lists and stacks, models that include positive
and negative feedback are significant and explain
more of the variance with respect to models that only
include pre-test score, or include pre-test score and
session length. Table 4 still follows the same ap-
proach, but adds to the regression models the addi-
tional DAs, DPI, DDI, Prompt and SI that had not
been included in that earlier work. The column M
refers to three types of models, Model 1 only in-
cludes Pre-test, Model 2 adds session length to Pre-
test, and Model 3 adds to Pre-test all the DAs. As ev-
idenced by the table, only DPI provides a marginally
significant contribution, and only for lists. Note that
length is not included in Model 3?s. We did run all
the equivalent models to Model 3?s including length.
The R2?s stay the same (literally, to the second dec-
imal digit), or minimally decrease. However, in all
these Model 3+?s that include length no DA is sig-
nificant, hence we consider them as less explana-
tory than the Model 3?s in Table 4: finding that a
longer dialogue positively affects learning does not
tell us what happens during that dialogue which is
conducive to learning.
Note that the ? weights on the pre-test are al-
ways negative in every model, namely, students with
higher pre-test scores learn less than students with
lower pre-test scores. This is an example of the well-
known ceiling effect: students with more previous
knowledge have less learning opportunity. Also no-
ticeable is that the R2 for the Trees models are much
higher than for Lists and Stacks, and that for Trees
no DA is significant (although there will be signifi-
cant trigram models that involve DAs for Trees). We
have observed that Lists are in general more diffi-
cult than Stacks and Trees (well, at least than binary
search trees) for students.
Topic Pre-Test ? Gain ?
Lists .40 .27 .14 .25
Stacks .29 .30 .31 .24
Trees .50 .26 .30 .24
Table 3: Learning gains and t-test statistics
Indeed Table 3 shows that in the CS-tutoring cor-
pus the average learning gain is only .14 for Lists,
but .31 for Stacks and .30 for Trees; whereas stu-
dents have the lowest pre-test score on Stacks, and
hence they have more opportunities for learning,
they learn as much for Trees, but not for Lists.
We now examine whether DA sequences help us
explain why student learn. We have run 24 sets of
linear regression experiments, which are grouped as
the following 6 types of models.
? With DA bigrams (DA sequences of length 2):
? Gain ? DA Bigram
? Gain ? DA Bigram + Pre-test Score
? Gain ? DA Bigram + Pre-test Score +
Session Length
? With DA trigrams (DA sequences of length 3):
? Gain ? DA Trigram
? Gain ? DA Trigram + Pre-test Score
? Gain ? DA Trigram + Pre-test Score +
Session Length
For each type of model:
70
Topic M Predictor ? R2 P
Lists
1 Pre-test ?.47 .20 < .001
2
Pre-test ?.43 .29 < .001Length .01 < .001
3
Pre-test ?.500
.377
< .001
+FB .020 < .01
-FB .039 ns
DPI .004 < .1
DDI .001 ns
SI .005 ns
Prompt .001 ns
Stacks
1 Pre-test ?.46 .296 < .001
2 Pre-test ?.46 .280 < .001Length ?.002 ns
3
Pre-test ?.465
.275
< .001
+FB ?.017 < .01
-FB ?.045 ns
DPI .007 ns
DDI .001 ns
SI .008 ns
Prompt ?.006 ns
Trees
1 Pre-test ?.739 .676 < .001
2 Pre-test ?.733 .670 < .001Length .001 ns
3
Pre-test ?.712
.667
< .001
+FB ?.002 ns
-FB ?.018 ns
DPI ?.001 ns
DDI ?.001 ns
SI ?.001 ns
Prompt ?.001 ns
All
1 Pre-test ?.505 .305 < .001
2 Pre-test ?.528 .338 < .001Length .06 < .001
3
Pre-test ?.573
.382
< .001
+FB .009 < .001
-FB ?.024 ns
DPI .001 ns
DDI .001 ns
SI .001 ns
Prompt .001 ns
Table 4: Linear Regression ? Human Tutoring
1. We index the corpus according to the length of
the sequence (2 or 3) using the method we in-
troduced in section 3.2.
2. We generate all the permutations of all the DAs
we annotated for within the specified length;
count the number of occurrences of each per-
mutation using Lucene?s SpanNearQuery al-
lowing for gaps of specified length. Gaps can
span from 0 to 3 utterances; for example, the
excerpt in Figure 1 will be counted as a {DPI,
DDI} bigram with a gap of length 1. Gaps can
be discontinuous.
3. We run linear regressions2 on the six types of
models listed above, generating actual models
by replacing a generic DA bi- or tri-gram with
each possible DA sequence we generated in
step 2.
4. We output those regression results, in which the
whole model and every predictor are at least
marginally significant (p < 0.1).
The number of generated significant models is
shown in Figure 5. In the legend of the Figure,
B stands for Bigram DA sequence, T stands for
Trigram DA sequence, L stands for session Length,
P stands for Pre-test score. Not surprisingly, Fig-
ure 5 shows that, as the allowed gap increases in
length, the number of significant models increases
too, which give us more models to analyze.
0
10
20
30
40
50
60
Gap Allowed
N
um
be
r o
f S
ig
ni
fic
an
t M
od
el
s
0 1 2 3
?
?
?
??
?
?
?
?
?
Predictors
T
T+P
T+P+L
B
B+P
B+P+L
Figure 5: Gaps Allowed vs. Significant Models
Figure 5 shows that there are a high number of
significant models. In what follows we will present
first of all those that improve on the models that
do not use sequences of DAs, as presented in Ta-
ble 4. Improvement here means not only that the
R2 is higher, but that the model is more appropriate
as an approximation of a tutor strategy, and hence,
constitutes a better guideline for an ITS. For exam-
ple, take model 3 for Lists in Table 4. It tells us
that positive feedback (+FB) and direct procedural
instruction (DPI) positively correlate with learning
2We used rJava, http://www.rforge.net/rJava/
71
gains. However, this obviously cannot mean that our
ITS should only produce +FB and DPI. The ITS is
interacting with the student, and it needs to tune its
strategies according to what happens in the interac-
tion; model 3 doesn?t even tell us if +FB and DPI
should be used together or independently. Models
that include sequences of DAs will be more useful
for the design of an ITS, since they point out what
sequences of DAs the ITS may use, even if they still
don?t answer the question, when should the ITS en-
gage in a particular sequence ? we have addressed
related issues in our work on iList (Fossati et al,
2009; Fossati et al, 2010).
4.1 Bigram Models
{DPI, Feedback} Model Indeed the first signifi-
cant models that include a DA bigram include the
{DPI, Feedback} DA sequence. Note that we distin-
guish between models that employ Feedback (FB)
without distinguishing between positive and nega-
tive feedback; and models where the type of feed-
back is taken into account (+FB, -FB). Table 5 shows
that for Lists, a sequence that includes DPI followed
by any type of feedback (Feedback, +FB, -FB) pro-
duces significant models when the model includes
pre-test. Table 5 and all tables that follow include
the column Gap that indicates the length of the gap
within the DA sequence with which that model was
obtained. When, as in Table 5, multiple numbers
appear in the Gap column, this indicates that the
model is significant with all those gap settings. We
only show the ?, R2 and P values for the gap length
which generates the highest R2 for a model, and the
corresponding gap length is in bold font: for exam-
ple, the first model for Lists in Table 5 is obtained
with a gap length = 2. For Lists, these models are not
as predictive as Model 3 in Table 4, however we be-
lieve they are more useful from an ITS design point
of view: they tell us that when the tutor gives direct
instruction on how to solve the problem, within a
short span of dialogue the tutor produces feedback,
since (presumably) the student will have tried to ap-
ply that DPI. For Stacks, a {DPI, -FB} model (with-
out taking pre-test into account) significantly corre-
lates (p < 0.05) with learning gain, and marginally
significantly correlates with learning gain when the
model also includes pre-test score. This latter model
is actually more predictive than Model 3 for Stacks
in Table 4 that includes +FB but not DPI. We can
see the ? weight is negative for the sequence {DPI,
-FB} in the Stacks model. No models including the
bigram {DPI, -FB} are significant for Trees.
Topic Predictor ? R2 P Gap
Lists
DPI, -FB .039 .235 <.001 2, 3Pre-test ?.513 < .001
DPI, +FB .019
.339
<.001
0, 1, 2, 3Pre-test ?.492 < .001
Length .011 < 0.05
DPI, FB .016
.333
<.05
0, 1, 2, 3Pre-test ?.489 < .001
Length .011 < 0.05
Stacks
DPI, -FB ?.290 .136 <.05 0, 1, 2, 3
DPI, -FB ?.187 .342 <.1 0, 1, 2, 3Pre-test ?.401 < .001
Table 5: DPI, Feedback Model
{FB, DDI} Model A natural question arises:
since Feedback following DPI results in significant
models, are there any significant models which in-
clude sequences whose first component is a Feed-
back move? We found only two that are signif-
icant, when Feedback is followed by DDI (Direct
Declarative Instruction). Note that here we are not
distinguishing between negative and positive feed-
back. Those models are shown in Table 6. The
Lists model is not more effective than the original
Model 3 for Lists in Table 4, but the model for Trees
is slightly more explanatory than the best model
for Trees in that same table, and includes a bigram
model, whereas in Table 4, only pre-test is signifi-
cant for Trees.
Topic Predictor ? R2 P Gap
Lists
FB, DDI .1478
.321
<.1
1Pre-test ?.470 < .001
Length .011 < .05
Trees FB, DDI .0709 .6953 <.05 0Pre-test ?.7409 < .001
Table 6: {FB, DDI} Model
4.2 Trigram Models
{DPI, FB, DDI} Model Given our significant bi-
gram models for DPI followed by FB, and FB fol-
lowed by DDI, it is natural to ask whether the com-
bined trigram model {DPI, FB, DDI} results in a
significant model. It does for the topic List, as
shown in table 7, however again the R2 is lower than
72
that of Model 3 in Table 4. This suggests that an ef-
fective tutoring sequence is to provide instruction on
how to solve the problem (DPI), then Feedback on
what the student does, and finally some declarative
instruction (DDI).
Topic Predictor ? R2 P Gap
Lists
DPI, FB, DDI .156
.371
<.01
1Pre-test ?.528 < .001
Length .012 < .05
Table 7: {DPI, FB, DDI} Model
More effective trigram models include Prompt
and SI. Up to now, only one model including se-
quences of DAs was superior to the simpler models
in Table 4. Interestingly, different trigrams that still
include some form of Feedback, DPI or DDI, and
then either Prompt or SI (Student Initiative) result in
models that exhibit slightly higher R2; additionally
in all these models the trigram predictor is highly
significant. These models are listed in table 8 (note
that the two Trees models differ because in one FB is
generic Feedback, irregardless of orientation, in the
other it?s +FB, i.e., positive feedback). In detail, im-
provements in R2 are 0.0382 in topic Lists, 0.12 in
topic Stacks and 0.0563 in topic Trees. The highest
improvement is in Stacks.
Topic Predictor ? R2 P Gap
Lists
PT,DPI,FB .266
.415
<.01
0Pre-test ?.463 < .001
Length .011 < .05
Stacks DDI,FB,PT ?.06 .416 <.01 1Pre-test ?.52 < .001
Trees +FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Trees FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Table 8: Highest R2 Models
It is interesting to note that the model for Lists add
Prompt at the beginning to a bigram that had already
been found to contribute to a significant model. For
Trees, likewise, we add another DA to the bigram
{FB,DDI} that had been found to be significant; this
time, it is Student Initiative (SI) and it occurs in
the middle. This indicates that, after the tutor pro-
vides feedback, the student takes the initiative, and
the tutor responds with one piece of information the
student didn?t know (DDI). Of course, the role of
Prompts and SI is not surprising, although interest-
ingly they are significant only in association with
certain tutor moves. It is well known that students
learn more when they build knowledge by them-
selves, either by taking the initiative (SI), or after
the tutor prompts them to do so (Chi et al, 1994;
Chi et al, 2001).
LOW: it?s backwards # it?s got four elements, but they
are backwards. [DDI]
234: so we have do it again. [SI]
LOW: so do it again. [FB]
LOW: do what again? [Prompt]
Figure 6: {DDI, FB, PT} is most effective in Stacks
4.3 Other models
We found other significant models, specifically,
{DDI,DPI} for all three topics, and {-FB,SI} for
Lists. However, their R2 are very low, and much
lower than any of the other models presented so
far. Besides models that include only one DA se-
quence and pre-test score to predict learning gain,
we also ran experiments to see if adding multiple
DA sequences to pre-test score will lead to signifi-
cant models ? namely, we experimented with mod-
els which include two sequences as predictors, say,
the two bigrams {-FB,SI} and {FB,DDI}. However,
no significant models were found.
5 Conclusions
In this paper, we explored effective tutoring strate-
gies expressed as sequence of DAs. We first pre-
sented the CS-Tutoring corpus. By relaxing the DA
n-gram definition via the fuzzy matching provided
by Apache Lucene, we managed to discover several
DA sequences that significantly correlate with learn-
ing gain. Further, we discovered models with higher
R2 than models which include only one single DA,
which are also more informative from the point of
view of the design of interfaces to ITSs.
6 Acknowledgments
This work was mainly supported by ONR (N00014-
00-1-0640), and by the UIC Graduate College
(2008/2009 Dean?s Scholar Award). Partial sup-
port is also provided by NSF (ALT-0536968, IIS-
0905593).
73
References
John R. Anderson. 1986. Knowledge compilation: The
general learning mechanism. Machine learning: An
artificial intelligence approach, 2:289?310.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. Survey Article.
Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2010. Characterizing the effectiveness
of tutorial dialogue with Hidden Markov Models. In
Intelligent Tutoring Systems, pages 55?64. Springer.
Whitney L. Cade, Jessica L. Copeland, Natalie K. Per-
son, and Sidney K. D?Mello. 2008. Dialogue modes
in expert tutoring. In Intelligent Tutoring Systems,
volume 5091 of Lecture Notes in Computer Science,
pages 470?479. Springer Berlin / Heidelberg.
Lin Chen and Barbara Di Eugenio. 2010. A lucene
and maximum entropy model based hedge detection
system. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 114?119, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Michelene T. H. Chi, Stephanie A. Siler, Takashi Ya-
mauchi, and Robert G. Hausmann. 2001. Learning
from human tutoring. Cognitive Science, 25:471?533.
Min Chi, Kurt VanLehn, and Diane Litman. 2010. The
more the merrier? Examining three interaction hy-
potheses. In Proceedings of the 32nd Annual Confer-
ence of the Cognitive Science Society (CogSci2010),
Portland,OR.
Michelene T.H. Chi. 1997. Quantifying qualitative anal-
yses of verbal data: A practical guide. Journal of the
Learning Sciences, 6(3):271?315.
Herbert H. Clark. 1992. Arenas of Language Use. The
University of Chicago Press, Chicago, IL.
Barbara Di Eugenio and Michael Glass. 2004. The
Kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101. Squib.
Barbara Di Eugenio, Trina C. Kershaw, Xin Lu, Andrew
Corrigan-Halpern, and Stellan Ohlsson. 2006. To-
ward a computational model of expert tutoring: a first
report. In FLAIRS06, the 19th International Florida
AI Research Symposium, Melbourne Beach, FL.
Barbara Di Eugenio, Davide Fossati, Stellan Ohlsson,
and David Cosejo. 2009. Towards explaining effec-
tive tutorial dialogues. In Annual Meeting of the Cog-
nitive Science Society, pages 1430?1435, Amsterdam,
July.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Davide Fossati, Barbara Di Eugenio, Christopher Brown,
Stellan Ohlsson, David Cosejo, and Lin Chen. 2009.
Supporting Computer Science curriculum: Exploring
and learning linked lists with iList. IEEE Transac-
tions on Learning Technologies, Special Issue on Real-
World Applications of Intelligent Tutoring Systems,
2(2):107?120, April-June.
Davide Fossati, Barbara Di Eugenio, Stellan Ohlsson,
Christopher Brown, and Lin Chen. 2010. Generat-
ing proactive feedback to help students stay on track.
In ITS 2010, 10th International Conference on Intelli-
gent Tutoring Systems. Poster.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Arthur C. Graesser, Natalie K. Person, and Joseph P.
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive
Psychology, 9:495?522.
Arthur C. Graesser, Shulan Lu, George Tanner Jack-
son, Heather Hite Mitchell, Mathew Ventura, Andrew
Olney, and Max M. Louwerse. 2004. AutoTutor:
A tutor with dialogue in natural language. Behav-
ioral Research Methods, Instruments, and Computers,
36:180?193.
Mark R. Lepper, Michael F. Drake, and Teresa
O?Donnell-Johnson. 1997. Scaffolding techniques of
expert human tutors. In K. Hogan and M. Pressley, ed-
itors, Scaffolding student learning: Instructional ap-
proaches and issues. Cambridge, MA: Brookline.
Diane Litman and Kate Forbes-Riley. 2006. Correla-
tions between dialogue acts and learning in spoken
tutoring dialogues. Natural Language Engineering,
12(02):161?176.
Brian MacWhinney. 2000. The Childes Project: Tools
for Analyzing Talk: Transcription format and pro-
grams, volume 1. Psychology Press, 3 edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In Proceedings of the 13th International Con-
ference on Artificial Intelligence in Education, pages
349?356, Los Angeles, CA, July. IOS Press.
Stellan Ohlsson. 2008. Computational models of skill
acquisition. The Cambridge handbook of computa-
tional psychology, pages 359?395.
74
Ron Sun, Paul Slusarz, and Chris Terry. 2005. The Inter-
action of the Explicit and the Implicit in Skill Learn-
ing: A Dual-Process Approach. Psychological Re-
view, 112:159?192.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
75
