A Mixture-of-Experts Framework for Text Classification
Andrew Estabrooks Nathalie Japkowicz
IBM Toronto Lab, Office 1B28B, SITE, University of Ottawa,
1150 Eglinton Avenue East, 150 Louis Pasteur, P.O. Box 450 Stn. A,
North York, Ontario, Canada, M3C 1H7 Ottawa, Ontario, Canada K1N 6N5
aestabro@ca.ibm.com nat@site.uottawa.ca
Abstract
One of the particular characteristics
of text classification tasks is that they
present large class imbalances. Such a
problem can easily be tackled using re-
sampling methods. However, although
these approaches are very simple to im-
plement, tuning them most effectively
is not an easy task. In particular,
it is unclear whether oversampling is
more effective than undersampling and
which oversampling or undersampling
rate should be used. This paper presents
a method for combining different ex-
pressions of the re-sampling approach
in a mixture of experts framework. The
proposed combination scheme is evalu-
ated on a very imbalanced subset of the
REUTERS-21578 text collection and is
shown to be very effective on this do-
main.
1 Introduction
A typical use of Machine Learning methods in
the context of Natural Language Processing is in
the domain of text classification. Unfortunately,
several characteristics specific to text data make
its classification a difficult problem to handle. In
particular, the data is typically highly dimensional
and it presents a large class imbalance, i.e., there,
typically, are very few documents on the topic of
interest while texts on unrelated subjects abound.
Furthermore, although large amounts of texts are
available on line, little of them are labeled. Be-
cause the class imbalance problem is known to
negatively affect typical classifiers and because
unlabeled data have no place in conventional su-
pervised learning, using off-the-shelf supervised
classifiers is likely not to be very successful in
the context of text data. It is, instead, recom-
mended to devise a classification method specifi-
cally tuned to the text classification problem.
The purpose of this study is to target some of
the characteristics of text data in the hope of im-
proving the effectiveness of the classification pro-
cess. The topics of finding a good representa-
tion for text data and dealing with its high di-
mensionality have been investigated previously
with, for example, the use of Wordnet [e.g., (Scott
& Matwin, 1999)] and Support Vector Machines
[e.g., (Joachims, 1998)], respectively. We will not
be addressing these problems here. The question
that we will tackle in this paper, instead, is that
of dealing with the class imbalance, and, in the
process of doing so, that of finding a way to take
advantage of the extra, albeit, unlabeled data that
are often left unused in classification studies.1
Several approaches have previously been pro-
posed to deal with the class imbalance problem
including a simple and yet quite effective method:
re-sampling [e.g., (Lewis & Gale, 1994), (Kubat
& Matwin, 1997), (Domingos, 1999)]. This paper
deals with the two different types of re-sampling
approaches: methods that oversample the small
class in order to make it reach a size close to that
of the larger class and methods that undersample
the large class in order to make it reach a size
close to that of the smaller class. Because it is
unclear whether oversampling is more effective
than undersampling and which oversampling or
undersampling rate should be used, we propose a
1Note, however, that unlabeled data is not always left un-
used as in the work on co-learning of (Blum & Mitchell,
1998). As discussed below, however, our approach will
make use of the unlabeled data in a different way.
method for combining a number of classifiers that
oversample and undersample the data at differ-
ent rates in a mixture of experts framework. The
mixture-of-experts is constructed in the context of
a decision tree induction system: C5.0, and all re-
sampling is done randomly. This proposed com-
bination scheme is, subsequently, evaluated on a
a subset of the REUTERS-21578 text collection
and is shown to be very effective in this case.
The remainder of this paper is divided into
four sections. Section 2 describes an experi-
mental study on a series of artificial data sets
to explore the effect of oversampling and under-
sampling and oversampling or undersampling at
different rates. This study suggests a mixture-
of-experts scheme which is described in Section
3. Section 4 discusses the experiment conducted
with that mixture-of-experts scheme on a series
of text-classification tasks and discusses their re-
sults. Section 5 is the conclusion.
2 Experimental Study
We begin this work by studying the effects of
oversampling versus undersampling and over-
sampling or undersampling at different rates.2 All
the experiments in this part of the paper are con-
ducted over artificial data sets defined over the do-
main of 4 x 7 DNF expressions, where the first
number represents the number of literals present
in each disjunct and the second number represents
the number of disjuncts in each concept.3 We
used an alphabet of size 50. For each concept,
we created a training set containing 240 positive
and 6000 negative examples. In other words, we
2Throughout this work, we consider a fixed imbalance
ratio, a fixed number of training examples and a fixed degree
of concept complexity. A thorough study relating different
degrees of imbalance ratios, training set sizes and concept
difficulty was previously reported in (Japkowicz, 2000).
3DNF expressions were specifically chosen because of
their simplicity as well as their similarity to text data whose
classification accuracy we are ultimately interested in im-
proving. In particular, like in the case of text-classification,
DNF concepts of interest are, generally, represented by
much fewer examples than there are counter-examples of
these concepts, especially when 1) the concept at hand is
fairly specific; 2) the number of disjuncts and literals per dis-
junct grows larger; and 3) the values assumed by the literals
are drawn from a large alphabet. Furthermore, an impor-
tant aspect of concept complexity can be expressed in sim-
ilar ways in DNF and textual concepts since adding a new
subtopic to a textual concept corresponds to adding a new
disjunct to a DNF concept.
0
10
20
30
40
50
60
Error over Positive Data          Error over Negative Data
Er
ro
r R
at
e 
(%
)
Imbalanced 
Re?Sampling
Down?Sizing
Figure 1: Re-Sampling versus Downsizing
created an imbalance ratio of 1:25 in favor of the
negative class.
2.1 Re-Sampling versus Downsizing
In this part of our study, three sets of experiments
were conducted. First, we trained and tested C5.0
on the 4x7 DNF 1:25 imbalanced data sets just
mentioned.4 Second, we randomly oversampled
the positive class, until its size reached the size
of the negative class, i.e., 6000 examples. The
added examples were straight copies of the data
in the original positive class, with no noise added.
Finally, we undersampled the negative class by
randomly eliminating data points from the neg-
ative class until it reached the size of the positive
class or, 240 data points. Here again, we used
a straightforward random approach for selecting
the points to be eliminated. Each experiment was
repeated 50 times on different 4x7 DNF concepts
and using different oversampled or removed ex-
amples. After each training session, C5.0 was
tested on separate testing sets containing 1,200
positive and 1,200 negative examples. The aver-
age accuracy results are reported in Figure 1. The
left side of Figure 1 shows the results obtained on
the positive testing set while its right side shows
the results obtained on the negative testing set.
As can be expected, the results show that the
number of false negatives (results over the pos-
4(Estabrooks, 2000) reports results on 4 other concept
sizes. An imbalanced ratio of 1:5 was also tried in prelim-
inary experiments and caused a loss of accuracy about as
large as the 1:25 ratio. Imbalanced ratios greater than 1:25
were not tried on this particular problem since we did not
want to confuse the imbalance problem with the small sam-
ple problem.
itive class) is a lot higher than the number of
false positives (results over the negative class). As
well, the results suggest that both naive oversam-
pling and undersampling are helpful for reducing
the error caused by the class imbalance on this
problem although oversampling appears more ac-
curate than undersampling.5
2.2. Re-Sampling and Down-Sizing at various
Rates
In order to find out what happens when different
sampling rates are used, we continued using the
imbalanced data sets of the previous section, but
rather than simply oversampling and undersam-
pling them by equalizing the size of the positive
and the negative set, we oversampled and under-
sampled them at different rates. In particular, we
divided the difference between the size of the pos-
itive and negative training sets by 10 and used this
value as an increment in our oversampling and un-
dersampling experiments. We chose to make the
100% oversampling rate correspond to the fully
oversampled data sets of the previous section but
to make the 90% undersampled rate correspond to
the fully undersampled data sets of the previous
section.6 For example, data sets with a 10% over-
sampling rate contain  
	 
