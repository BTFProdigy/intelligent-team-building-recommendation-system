Jurilinguistic Engineering in Cantonese Chinese: 
An N-gram-based Speech to Text Transcription System 
B K T'sou, K K Sin, S W K Chan, T B Y Lai, C Lun, K T Ko, G K K Chan, L Y L Cheung 
Language hfformation Sciences Research Centre 
City University of Itong Kong 
Tat Chee Avenue, Kowloon 
Hong Kong SAR, China 
Email: rlbtsou @nxmail.cityu.edu.hk 
Abstract  
A Cantonese Chinese transcription system to 
automatically convert stenograph code to 
Chinese characters ix reported. The major 
challenge in developing such a system is the 
critical homocode problem because of 
homonymy. The statistical N-gram model is 
used to compute the best combination of 
characters. Supplemented with a 0.85 million 
character corpus of donmin-specific training 
data and enhancement measures, the bigram 
and trigrmn implementations achieve 95% 
and 96% accuracy respectively, as compared 
with 78% accuracy in the baseline model. The 
system perforlnance is comparable with other 
adwmced Chinese Speech-to-Text input 
applications under development. The system 
meets an urgent need o1' the .ludiciary ot: post- 
1997 Hong Kong. 
Keyword: Speech to Text, Statistical 
Modelling, Cantonese, Chinese, Language 
Engineering 
1. Introduct ion 
British rule in Hong Kong lnade English the only 
official language in the legal domain for over a 
Century. After the reversion of Hong Kong 
sovereignty to China in 1997, legal bilingualism 
has brought on an urgent need to create a 
Computer-Aided Transcription (CAT) system for 
Cantonese Chinese to produce and maintain the 
massive legally tenable records of court 
proceedings conducted in the local majority 
language (T'sou, 1993, Sin and T'sou, 1994, Lun 
et al, 1995). With the support fl'om the Hong 
Kong Judiciary, we have developed a 
transcription system for converting stenograph 
code to Chinese characters. 
CAT has been widely used for English for 
many years and awlilable R~r Mandarin Chinese, 
but none has existed for Cantonese. Althongh 
Cantonese is a Chinese dialect, Cantonese and 
Mandarin differ considerably in terms of 
phonological struclure, phouotactics, word 
morphology, vocabulary and orthogral)hy. Mutual 
intelligibility between the two dialects is generally 
very low. For example, while Cantonese has lnole 
than 700 distinct syllables, Mandarin has only 
about 400. Cantonese has 6 tone contours and 
Mandarin only 4. As for vocabulary, 16.5% of the 
words in a 1 million character corpus of court 
proceedings in Canlonese cannot be found in a 
corlms consisting of 30 million character 
newspaper texts in Modern Written Chinese 
(T'sou el al, 1997). For orthography, Mainhmd 
China uses the Simplified Chinese character set, 
and Hong Kong uses the Traditional set l~lus 
4,702 special local Cantonese Chinese characters 
(Hong Kong Government, 1999). Such 
differences between Cantonese and Mandarin 
necessitate the Jtnilinguistic Engineering 
undertaking to develop an independent Cantonese 
CAT system for the local language nvironment. 
The major challenge in developing a 
Cantonese CAT system lies in the conversion of 
phonologically-based stenograph code into 
Chinese text. Chinese is a logographic language. 
Each character or logograph represents a syllable. 
While the total inventory of Cantonese syllable 
types is about 720, them am at least 14,000 
Chinese character types. The limited syllabary 
creates many homophones in the language (T'sou, 
1976). In a one million character corlms of court 
proceedings, 565 distinct syllable types were 
found, representing 2,922 distinct character types. 
Of the 565 syllable types, 470 have 2 or morn 
homophonous characters. In the extreme case, zi 
represents 35 homophonous character types. 
1121 
Coverage ofl lomophonous and Non-homophous Characters 
100.00% ?'o o~ 4:~ ~ ~os 4:e 
80.00% 
6 
60.00% 
# 40.00% 
c~ 20.00% 
0.00% 
? No. of High Freq. Characters %~ t~ p 
\[EJ HoInophonous Charactcrs \[\] Non-honaophonous chaliactcrs\] 
Figure I. Covcragc of Homonymous Characters 
These 470 syllables represent 2,810 homophonous 
character types which account for 94.7% of the 
text, as shown in Figure \]. The homocode 
problem nmst be properly resolved to ensure 
successful conversion. 
2. Computer-Aided Transcription (CAT) 
~dJ l  
t 
? I 
co 2F2L . 
Stcnoma~h code I i 
Slzg?2 E 
I Transcription \[ | 
/ ! EIt?illC ; | 
h Trigranl ,, Big,an, i ~ '9 
=, E ) 
! 
Proofreading 
\[ i 
' I 
' Bigram / Trip, ram 
i Statistical Dala 
Chinese Text i l l :~!  
Figure 2. Automatic Transcription Process 
Figure 2 outlines the transcription process in the 
Cantonese CAT system. Following typical 
courtroom CAT systems, our process is divided 
into three major stages. In Stage 1, simultaneous 
to a litigant speaking, a stenographer inputs 
speech, i.e. a sequence of transcribed syllables or 
stenograph codes, via a stenograph code generator. 
Each stenograph code basically stands for a 
syllable. In Stage 2, the transcription software 
converts the sequence of stenograph codes \[Sl . . . . .  
s,,} into the original character text {q . . . . .  c,,}. 
This procedure requires the conversion 
component to be tightly bound to the phonology 
and orthography of a specific language. To 
specifically address homonymy in Cantonese, the 
conversion procedure in our system is supported 
by bigram and trigram statistical data derived 
from domain-specific training. In Stage 3, manual 
editing of the transcribed texts corrects errors 
from typing mistakes or his-transcription. 
3. System Architecture 
3.1 Statistical Formulation 
To resolve massive ambiguity in speech to text 
conversion, the N-gram model is used to 
determine the most probable character sequence 
{q . . . . .  ck} given the input stenograph code 
sequence {s~ . . . . .  Sk}. The conditional probability 
(1) is to be maximized. 
(1) P(q . . . . .  c~l sl . . . . .  sk) 
where {q . . . . .  c~} stands for a sequence of N 
characters, and {sl . . . . .  sk} for a sequence of k 
input stenograph codes. 
The co-occurrence frequencies necessary for 
computation are acquired through training. 
However, a huge amount of data is needed to 
generate reliable statistical estimates for (1) if 
N > 3. Consequently, N-gram probability is 
approximated by bigram or trigram estimates. 
First, rewrite (1) as (2) using Bayes' rule. 
P(c, ..... c )xP(s, ..... s lc, ..... c , )  
(2) 
P(s, ..... s k ) 
As the value of P(s I . . . . .  st) remains unchanged 
for any choice of {q . . . . .  ct}, one needs only to 
maximize the numerator in (2), i.e. (3). 
(3) P(cl ..... Ck) X P(sl ..... s,\[cl ..... ck) 
(3) can then be approximated by (4) or (5) using 
bigram and trigram models respectively. 
(4) FL=,...., (P(c,lq_,) x P(s,.Iq)) 
(5) ?P(silci)) 
The transcription program is to compute the best 
sequence of {q . . . . .  c,} so as to maximize (4) or 
(5). The advantage of the approximations in (4) 
and (5) is that P(s,lc,), P(c,lc,.,) and P(c,lc,_2c,_,) 
can be readily estimated using a training corpus of 
manageable size. 
3.2 Viterbi Algorithm 
The Viterbi algorithm (Viterbi, 1967) is 
implemented toefficiently compute the maxinmm 
value of (4) and (5) for different choices of 
1122 
character sequences. Instead o1' exhaustively 
computing tile values for all possible character 
sequences, the algorithm only keeps track of the 
probability of the best character sequence 
terminating in each possible character candidate 
for a stenograph code. 
In the trigram implelnentatiou, size limitation 
in the training cortms makes it impossible to 
estimate all possible P(c i lc i_2ci . i )  because some 
{ci_2, ci_l, q} may never occur there. Following 
Jelinek (1990), P(cil ci.2ci_ i ) is approximated by 
the summation of weighted lrigram, bigram and 
unigram estimates in (6). 
(6) p(c i I ci_ 2 ci_ 1) 
f(ci_ 2 ci- I ci ) f(ci_ 1 c i ) f(c i ) 
= w 3 X + w 2 X + w 1 X -  
f(ci_ 2 ci_ 1 ) f(ci_ I ) Z f(cj ) 
where (i) w,, w2, w-s _> 0 are weights, (ii) 
wl-l-w2-{-H; 3 = 1, and (iii) Z f(q) is the stun of 
frequencies of all characters. Typically lhe best 
results can be obtained if w:~, the weight for 
trigram, is significantly greater than the olher two 
weights so that the trigram probability has 
dominant effect in the probability expression. In 
our tests, we sot wl=0.01, w2=0.09, aud u;3=0.9. 
The Viterbi algorithm substantially reduces the 
computational complexity flom O(m") to O(m.-~n) 
and O(nr~n) using bigram and trigram estimation 
rc:spectively where n is the number of stenograph 
code tokens in a sentence, and m is tile upper 
bound of the number of homophonous characters 
for a stenograph code. 
To maximize the transcription accuracy, we 
also refine the training corpus to ensure that the 
bigram and trigram statistical models reflect the 
comtroom lauguage closely. This is done by 
enlarging tile size of tile training corpus and by 
compiling domain-specific text corpora. 
3.,3 Special Encoding 
After some initial trial tests, error analysis was 
conducted to investigate the causes of the mis- 
transcribed characters. It showed that a noticeable 
amount of errors were due to high failure rate in 
the mtriewtl of seine characters in the 
transcription. The main reason is that high 
fiequency characters are more likely to interfere 
with the correct retrieval of other relatively lower 
frequency homophouous characters. For example, 
Cantonese, hal ('to be') and hal ('at') are 
homophouous in terms of seglnental makeup. 
Their absolute fiequcucies in our training corpus 
are 8,695 and 1,614 respectively. Because of the 
large fi'equency discrepancy, the latter was mis- 
transcribed as tile former 44% of the times in a 
trial test. 32 such high fi'equency characters were 
found to contribute to about 25% of all 
transcription errors. To minimize the interference, 
special encoding, which resulted flom shallow 
linguistic processing, is applied to the 32 
characters o that each of them is assigned a 
unique stenograph code. This was readily 
accepted by the court stenographers. 
4. hnplementation and Results 
4.1 Compilation of Corpora 
In our expreriments, authentic Chinese court 
proceedings from the Hong Kong Judiciary were 
used fox tile compilation of the training and 
testing corpora for the CAT prototypes. To ensure 
that tile training data is comparable with tile data 
to be transcribed, the training corpus should be 
large enough to obtain reliable estimates for 
P(silc,.), P(cilci j) and P(cilci_2ci_l).  in our trials, 
we quickly approached the point of diminishing 
return when the size of the training corpus reaches 
about 0.85 million characters. (See Section 4.2.2.) 
To further enhance training, the system also 
exploited stylistic and lexical variations across 
different legal domains, e.g. tra\[.'fic, assauh,  and 
f raud  offences. Since different case types show 
distinct domain-specific legal vocabulary or usage, 
simply integrating all texts in a single training 
corpus may obscure the characteristics o1' specific 
language domains, thus degrading the modelling. 
Hence domain-specific training corpora were also 
compiled to enhance performance. 
Two sets of data were created for testing and 
comparison: Gener ic  Coqms (GC) and Domain -  
,specific Cmpus  (DC). Whereas GC consists of 
texts representing various legal case types, DC is 
restricted to traffic offence cases. Each set 
consists of a training corpus of 0.85 million 
characters and a testing corpus of 0.2 million 
characters. The training corpus consists of 
Chinese characters along with the corresponding 
stenograph codes, and tile testing corpus consists 
solely of stenograph codes of the Chinese texts. 
4.2 Experimental Results 
For ewfluation, several prototypes were set up to 
1123 
test how different factors affected transcription 
accuracy. They included (i) use of bigram vs. 
trigram models, (ii) the size of the training 
corpora, (iii) domain-specific training, and (iv) 
special encoding. To measure conversion 
accuracy, the output text was compared with the 
original Chinese text in each test on a character by 
character basis, and the percentage of correctly 
transcribed characters was computed. Five sets of 
experiments are reported below. 
4.2.1 Bigram vs. Trigram 
Three prototypes were developed: the Bigram 
Prototype, CA Tva2, the Trigram Prototype, CA Tva.~, 
and the Baseline Prototype, CATo. CATva2 and 
CATvA.~ implelnent he conversion engines using 
the bigram and trigram Viterbi algorithm 
respectively. CA7o, was set up to serve as an 
experimental control. Instead of implementing the 
N-gram model, conversion is accomplished by 
selecting the highest fiequency item out of the 
homophonous character set for each stenograph 
code. GC was used throughout the three 
experiments. The training and testing data sets are 
0.85 and 0.20 million characters respectively. The 
results are summarized in Table 1. 
Corpus GC GC GC 
Accuracy 78.0% 92.4% 93.6% 
Table 1. Different N-gram Models 
The application of the bigram and trigram models 
offers about 14% and 15% improvement in 
accuracy over Control Prototype, CATo. 
4.2.2 Size of Training Corpora 
In this set of tests, the size of the training corpora 
was varied to determine the impact of the training 
corpus size on accuracy. The sizes tested are 0.20, 
0.35, 0.50, 0.63, 0.73 and 0.85 million characters. 
Each corpus is a proper subset of the immediately 
larger corpus so as to ensure the comparability of 
he trainin texts. CATvA 2 was used in the tests. 
Training Corpus GC GC GC 
Accurac~ 89.5% 91.2% 91.8% 
Training Corpus GC GC GC 
Accuracy 92.1% 92.3 % 92.4 % 
Table 2. Variable Training Data Size 
The results in Table 2 show that increasing the 
size of the training corpus enhances the accuracy 
incrementally. However, the point of diminishing 
return is reached when the size reaches 0.85 
million characters. We also tried doubling the 
corpus size to 1.50 million characters. It only 
yields 0.8% gain over the 0.85 million character 
corpus. 
4.2.3 Use of Domain-specific Training 
This set of tests evaluates the effectiveness of 
domain-specific training. Data fi'oln the two 
corpora, GC and DC, are utilized in the training of 
the bigram and trigram prototypes. The size of 
each training set is 0.85 million characters. The 
same set of 0.2 million character testing data from 
DC is used in all four conversion tests. Without 
increasing the size of the training data, setups with 
domain-specific training consistently ield about 
2% improvement. A more comprehensive set of 
corpora including Tra.lfic, Assault, and Robbeo~ is
bein )iled and will be re )ortcd in future. 
Prototypes 
Training Data 
CATvA; CATvA3 CATvaz CATvA3 
GC GC DC DC 
Testing Data DC DC DC DC 
Accuracy 92.6% 92.8% 94.7% 94.8% 
Table 3. Application of Domain-Specificity 
4.2.4 Special Encoding 
Following shallow linguistic processing, special 
encoding assigns unique codes to 32 characters to 
reduce confusion with other characters. Another 
round of tests was repeated, identical to the 
CATvA2 and CATvA 3 tests in Section 4.2.1, except 
for the use of special encoding. The use of 
training and testing corpora have 0.85 and 0.20 
million characters respective 
S ~ i ~  I;:~ :::NOfA~I~Ii~: 
Prototypes CATw~ CATvA~ CATw2 CATvA3 
Corpus GC GC GC GC 
Accuracy 92.4% 93.6% 94.7% 95.6% 
Table 4. Application of Special Encoding 
Table 4 shows that the addition of special 
encoding consistently offers about 2% increase in 
accuracy. Special encoding and hence shallow 
linguistic processing provide the most significant 
improvement in accuracy. 
4.2.5 Incorporation of Domain-Specificity and 
Special Eneoding 
As discussed above, both domain-specific training 
and special encoding raise the accuracy of 
transcription. The last set of tests deals with the 
integration of the two features. Special encoding 
1124 
is utilized in the training and testing data of DC 
which have 0.85 and 0.20 million characters 
respectively. 
~raining/Testing, Data DC DC 
I S. Encoding Applied Applied / zcuracy 95.4 % 96.2 % 
Table 5. Integration of D. Specificity and S. Encoding 
Recall that Domain-Specificity and Special 
Encoding each offers 2% improvelnent. Table 5 
shows that combining BOTH features offer about 
3% improvement over tests without them. (See 
non-domain-specific tests in Section 4.2.3) 
The 96.2% accuracy achieved by CATvA 3 
represents the best performance of our system. 
The result is conaparable with other relevant 
advanced systems for speech to text conversion. 
For example, Lee (1999) reported 94% accuracy 
in a Chinese speech to text transcription system 
under developlnent with very large training 
corpus. 
5. Conclusion 
We have created a Cantonese Chinese CAT 
system which uses the phonologically-based 
stenograph machine. The system delivers 
encouragingly accurate transcription in a language 
which has many hon\]ol~honous characters. To 
resolve problematic ambiguity in the conversion 
fi'on-i a I)honologically-based code to the 
logograt)hic Chinese characters, we made use of 
lhe N-gram statistical model. The Viterbi 
algorithm has enabled us to identify the most 
probable sequence of characters from the sels of 
possible homophonous characters. With the 
additional use of special encoding and domain- 
specific training, the Cantonese CAT system has 
attained 96% transcription accuracy. The success 
of the Jurilinguistic Engineering project can 
further enhance the efforts by the Hong Kong 
Judiciary to conduct trials in the language of the 
majority population. Further improvement to the 
system will include (i) more domain-specific 
training and testing across different case types, (2) 
firm-tuning for the optimal weights in the trigram 
formula, and (3) optilnizing the balance between 
training corpus size and shallow linguistic 
processing. 
Acknowledgement  
Support for the research reported here is provided 
mainly through the Research Grants Council of 
Hong Kong under Competitive Earmarked 
Research Grant (CERG) No. 9040326. 
References 
Hong Kong Govennnent. 1999. Hong Kong 
Szq~plemenmry Character Set. hfformation 
Technology Services Department & Official 
Languages Agency. 
Jelinek, F. 1990. "Self-organized Language Modeling 
lbr Speech Recognition." In A. Waibel and K.F. 
Lee, (eds.). Readings in Speech Recognition. San 
Mateo: CA: Morgan Kaufmann Publishers. 
Lee, K. F. 1999. "Towards a Multimedia, Multimodal, 
Multilingual Computer." Paper presented on behall' 
of Microsoft Research Institute, China in the 5th 
Natural Language Processing Pacil'ic Rim 
Symposium held in Belling, China, November 5-7, 
1999. 
Lun, S., K. K. Sin, B. K. T'sou and T. A. Cheng. 1997. 
"l)iannao Fuzhu Yueyu Suii Fangan." (The 
Cantonese Shorthand System for Computer-Aided 
Transcription) (in Chinese) Proceedings o.f the 5th 
lntermttional Confere,ce on Cantonese and Other 
Yue Dialects. B. H. Zhan (ed). Guangzhou: Jinan 
University Press. pp. 217--227. 
Sin, K. K. and B. K. T'sou. 1994. "Hong Kong 
Courtroon\] Language: Some Issues on Linguistics 
and Language Technology." Paper presented at lhe 
Third International Conference on Chinese 
Linguistics. Hong Kong. 
T'sou, B. K. 1976. "Homophony and Internal Change 
in Chinese." Computational Analyses of Asian and 
African Lauguages 3, 67--86. 
T'sou, t3. K. 1993. "Some Issues on Law and Language 
in the ltong Kong Special Administrative Region 
(HKSAR) of China." Language, Law attd Equality: 
Proceedings of the 3rd International Conference of 
the International Acaden G of Language Law (IALL). 
K. Prinsloo et al Pretoria (cds.): University of South 
Africa. pp. 314-331. 
T'sou, B. K., H. L. Lin, G. Liu, T. Chan, J. Hu, C. H. 
Chew, and J. K. P. Tse. 1997. "A Synchronous 
Chinese Language Corpus fi'om Different Speech 
Communities: Construction and Applications." 
Computational Linguistics and Chinese Language 
Ptwcessing 2:91-- 104. 
Viterbi, A. J. 1967. "Error Bounds for Convolution 
Codes and an Asymptotically Optimal l)ecoding 
Algorithm." IEEE 7'ransactions on htformation 
TheoG 13: 260--269. 
1125 
Morpheme-based Derivation of  
Bipolar Semantic Orientation of Chinese Words  
Raymond W.M. Yuen, Terence Y.W. Chan, Tom B.Y. Lai, O.Y. Kwong, Benjamin K.Y. T'sou  
Language Information Sciences Research Centre, the City University of Hong Kong 
83 Tat Chee Avenue, Hong Kong  
{ wmyuen, dcywchan, cttomlai, rlolivia, rlbtsou}@cityu.edu.hk 
 
Abstract 
The evaluative character of a word is called its 
semantic orientation (SO). A positive SO 
indicates desirability (e.g. Good, Honest) and 
a negative SO indicates undesirability (e.g., 
Bad, Ugly). This paper presents a method, 
based on Turney (2003), for inferring the SO 
of a word from its statistical association with 
strongly-polarized words and morphemes in 
Chinese. It is noted that morphemes are much 
less numerous than words, and that also a 
small number of fundamental morphemes may 
be used in the modified system to great 
advantage. The algorithm was tested on 1,249 
words (604 positive and 645 negative) in a 
corpus of 34 million words, and was run with 
20 and 40 polarized words respectively, giving 
a high precision (79.96% to 81.05%), but a 
low recall (45.56% to 59.57%). The algorithm 
was then run with 20 polarized morphemes, or 
single characters, in the same corpus, giving a 
high precision of 80.23% and a high recall of 
85.03%. We concluded that morphemes in 
Chinese, as in any language, constitute a dis-
tinct sub-lexical unit which, though small in 
number, has greater linguistic significance 
than words, as seen by the significant en-
hancement of results with a much smaller 
corpus than that required by Turney. 
1. Introduction 
The semantic orientation (SO) of a word indicates 
the direction in which the word deviates from the 
norm for its semantic group or lexical field (Lehrer, 
1974). Words that encode a desirable state (e.g., 
beautiful) have a positive SO, while words that 
represent undesirable states (e.g. absurd) have a 
negative SO (Hatzivassiloglou and Wiebe, 2000). 
Hatzivassiloglou and Mckeown (1997) used the 
words ?and?, ?or?, and ?but? as linguistic cues to 
extract adjective pairs. Turney (2003) assessed the 
SO of words using their occurrences near strongly-
polarized words like ?excellent? and ?poor? with 
accuracy from 61% to 82%, subject to corpus size. 
Turney?s algorithm requires a colossal corpus 
(hundred billion words) indexed by the AltaVista 
search engine in his experiment. Undoubtedly, 
internet texts have formed a very large and easily-
accessible corpus. However, Chinese texts in 
internet are not segmented so it is not cost-
effective to use them. 
This paper presents a general strategy for 
inferring SO for Chinese words from their 
association with some strongly-polarized 
morphemes. The modified system of using 
morphemes was proved to be more effective than  
strongly-polarized words in a much smaller corpus.  
Related work and potential applications of SO 
are discussed in section 2. 
Section 3 illustrates one of the methods of 
Turney?s model for inferring SO, namely, 
Pointwise Mutual Information (PMI), based on the 
hypothesis that the SO of a word tends to 
correspond to the SO of its neighbours. 
The experiment with polarized words is 
presented in section 4. The test set includes 1,249 
words (604 positive and 645 negative). In a corpus 
of 34 million word tokens, 410k word types, the 
algorithm is run with 20 and 40 polarized words, 
giving a precision of 79.96% and 81.05%, and a 
recall  of 45.56% and 59.57%, respectively. 
The system is further modified by using 
polarized morphemes in section 5. We first 
evaluate the distinction of Chinese morphemes to 
justify why the modification can probably give 
simpler and better results, and then introduce a 
more scientific selection of polarized morphemes. 
A high precision of 80.23% and a greatly increased 
recall of 85.03% are yielded. 
In section 6, the algorithm is run with 14, 10 and 
6 morphemes, giving a precision of 79.15%, 
79.89% and 75.65%, and a recall of 79.50%, 
73.26% and 66.29% respectively. It shows that the 
algorithm can be also effectively run with 6 to 10 
polarized morphemes in a smaller corpus. 
The conclusion and future work are discussed in 
section 7. 
2. Related Work and Applications 
Hatzivassiloglou and Mckeown (1997) presented a 
method for automatically assigning a + or ? 
orientation label to adjectives known to have some 
SO by the linguistic constraints on the use of 
adjectives in conjunctions. For example, ?and? 
links adjectives that have the same SO, while ?but? 
links adjectives that have opposite SO. They 
devised an algorithm based on such constraints to 
evaluate 1,336 manually-labeled adjectives (657 
positive and 679 negative) with 97% accuracy in a 
corpus of 21 million words. 
Turney (2003) introduced a method for 
automatically inferring the direction and intensity 
of the SO of a word from its statistical association 
with a set of positive and negative paradigm words, 
i.e., strongly-polarized words. The algorithm was 
evaluated on 3,596 words (1,614 positive and 
1,982 negative) including adjectives, adverbs, 
nouns, and verbs. An accuracy of 82.8% was 
attained in a corpus of hundred billion words. 
SO can be used to classify reviews (e.g., movie 
reviews) as positive or negative (Turney, 2002), 
and applied to subjectivity analysis such as 
recognizing hostile messages, classifying emails, 
mining reviews (Wiebe et al, 2001). The first step 
of those applications is to recognize that the text is 
subjective and then the second step, naturally, is to 
determine the SO of the subjective text. Also, it 
can be used to summarize argumentative articles 
like editorials of news media. A summarization 
system would benefit from distinguishing 
sentences intended to present factual materials 
from those intended to present opinions, since 
many summaries are meant to include only facts. 
3. SO from Association-PMI 
Turney (2003) examined SO-PMI (Pointwise 
Mutual Information) and SO-LSA (Latent 
Semantic Analysis). SO-PMI will be our focus in 
the following parts. PMI is defined as:  
 
PMI(word1, word2)=log2( )()(
)&(
21
21
wordpwordp
wordwordp ) 
 
where p(word1 & word2) is the probability that 
word1 and word2 co-occur. If the words are 
statistically independent, the probability that they 
co-occur is given by the product p(word1) p(word2). 
The ratio between p(word1 & word2) and p(word1) 
p(word2) is a measure of the degree of statistical 
dependence between the words. The SO of a given 
word is calculated from the strength of its 
association with a set of positive words, minus the 
strength of its association with a set of negative 
words. Thus the SO of a word, word, is calculated 
by SO-PMI as follows: 
SO-PMI(word) = 

?Pwordspword
pwordwordPMI ),(  - 
?Nwordsnword
nwordwordPMI ),(  
 
where Pwords is a set of 7 positive paradigm 
words (good, nice, excellent, positive, fortunate, 
correct, and superior) and Nwords is a set of 7 
negative paradigm words (bad, nasty, poor, 
negative, unfortunate, wrong, and inferior). Those 
14 words were chosen by intuition and based on 
opposing pairs (good/bad, excellent/poor, etc.). 
The words are rather insensitive to context, i.e., 
?excellent? is positive in almost all contexts. 
A word, word, is classified as having a positive 
SO when SO-PMI(word) is positive and a negative 
SO when SO-PMI(word) is negative.  
Turney (2003) used the Alta Vista Advanced 
search engine with a NEAR operator, which 
constrains the search to documents that contain the 
words within ten words of one another, in either 
order. Three corpora were tested. AV-ENG is the 
largest corpus covering 350 million web pages 
(English only) indexed by Alta Vista. The medium 
corpus is a 2% subset of AV-ENG corpus called 
AV-CA (Canadian domain only). The smallest 
corpus TASA is about 0.5% of AV-CA and 
contains various short documents. 
One of the lexicons used in Turney?s experiment 
is the GI lexicon (Stone et al, 1966), which 
consists of 3,596 adjectives, adverbs, nouns, and 
verbs, 1,614 positive and 1,982 negative. 
Table 1 shows the precision of SO-PMI with the 
GI lexicon in the three corpora. 
Precision Percent of 
full test set 
Size of 
test set AV-ENG AV-CA TASA 
100% 3596 82.84% 76.06% 61.26% 
75% 2697 90.66% 81.76% 63.92% 
50% 1798 95.49% 87.26% 47.33% 
25% 899 97.11% 89.88% 68.74% 
Approx. no. of 
words 1x10
11
 2x109 1x107 
Table 1: The precision of SO-PMI with the GI 
lexicon  
 
The strength (absolute value) of the SO was 
used as a measure of confidence that the words 
will be correctly classified. Test set words were 
sorted in descending order of the absolute value of 
their SO and the top ranked words (the highest 
confidence words) were then classified. For 
example, the second row (starting with 75%) in 
table 1 shows the precision when the top 75% were 
classified and the last 25% (with lowest confidence) 
were ignored. We will employ this measure of 
confidence in the following experiments.  
Turney concluded that SO-PMI requires a large 
corpus (hundred billion words), but it is simple, 
easy to implement, unsupervised, and it is not 
restricted to adjectives.  
4. Experiment with Chinese Words 
In the following experiments, we applied Turney?s 
method to Chinese. The algorithm was run with 20 
and then 40 paradigm words for comparison. The 
experiment details include: 
NEAR Operator: it was applied to constrain 
the search to documents that contain the words 
within ten words of one another, in either order. 
Corpus: the LIVAC synchronous corpus (Tsou 
et al, 2000, http://www.livac.org) was used. It 
covers 9-year news reports of Chinese 
communities including Hong Kong, Beijing and 
Taiwan, and we used a sub-corpus with about 34 
million word tokens and 410k word types.  
Test Set Words: a combined set of two 
dictionaries of polarized words (Guo, 1999, Wang, 
2001) was used to evaluate the results. While 
LIVAC is an enormous Chinese corpus, its size is 
still far from the hundred-billion-word corpus used 
by Turney. It is likely that some words in the 
combined set are not used in the 9-year corpus. To 
avoid a skewed recall, the number of test set words 
used in the corpus is given in table 2. In other 
words, the recall can be calculated by the total 
number of words used in the corpus, but not by 
that recorded in the dictionaries. The difference 
between two numbers is just 100. 
Polarity Total no. of the 
test set words 
Words used in 
the 9-year corpus 
Positive 629 604 
Negative 721 645 
Total 1350 1249 
Table 2: Number of the test set words  
 
Paradigm words: The paradigm words were 
chosen using intuition and based on opposing pairs, 
as Turney (2003) did. The first experiment was 
conducted with 10 positive and 10 negative 
paradigm words, as follows,  
Pwords: (honest), (clever), (sufficient), 
(lucky), (right), (excellent), 
(prosperous), (kind), (brave), (humble) 
Nwords: (hypocritical), (foolish), 
(deficient), (unlucky), (wrong), (adverse), 
(unsuccessful), (violent), (cowardly), 
(arrogant) 
The experiment was then repeated by increasing 
the number of paradigm words to 40. The 
paradigm words added are: 
Pwords: (mild), (favourable), 
(successful), (positive), (active), 
(optimistic), (benign), (attentive), 
(promising), (incorrupt) 
Nwords: (radical), (unfavourable), 
(failed), (negative), (passive), 
(pessimistic), (malignant), (inattentive), 
(indifferent), (corrupt) 
4.1 Results 
Tables 3 and 4 show the precision and recall of 
SO-PMI by two sets of paradigm words.  
% of test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312 
Extracted Set 569 427 285 142 
Precision 79.96% 86.17% 86.99% 90.16% 
Recall 45.56% 
Table 3: Precision and Recall of the SO-PMI of the 
20 paradigm word test set 
% of  test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312 
Extracted Set 744 558 372 186 
Precision 81.05% 86.02% 88.71% 94.09% 
Recall 59.57% 
Table 4: Precision and Recall of the SO-PMI of the 
40 paradigm word test set 
 
The results of both sets gave a satisfactory 
precision of 80% even in 100% confidence. 
However, the recall was just 45.56% under the 20-
word condition, and rose to 59.57% under the 40-
word condition. The 15% rise was noted. 
To further improve the recall performance, we 
experimented with a modified algorithm based on 
the distinct features of Chinese morphemes.  
5. Experiment with Chinese Morphemes 
Taking morphemes to be smallest linguistic 
meaningful unit, Chinese morphemes are mostly 
monosyllabic and single characters, although there 
are some exceptional poly-syllabic morphemes like 
 (grape),  (coffee), which are mostly 
loanwords. In the following discussion, we 
consider morphemes to be monosyllabic and 
represented by single characters. 
It is observed that many poly-syllabic words 
with the same SO incorporate a common set of 
morphemes. The fact suggests the possibility of 
using paradigm morphemes instead of words.  
Unlike English, the constituent morphemes of a 
Chinese word are often free-standing monosyllabic 
words. It is note-worthy that words in ancient 
Chinese were much more mono-morphemic than 
modern Chinese. The evolution from monosyllabic 
word to disyllabic word may have its origin in the 
phonological simplification which has given rise to 
homophony, and which has affected the efficacy of 
communication. To compensate for this, many 
more related disyllabic words have appeared in 
modern Chinese (Tsou, 1976). There are three 
basic constructions for deriving disyllabic words in 
Chinese, including:  
(1) combination of synonyms or near 
synonyms ( , warm, genial, =warm, mild, 
=warm, genial) 
(2) combination of semantically related 
morphemes ( , =affair, =circumstances) 
(3) The affixation of minor suffixes which 
serve no primary grammatical function ( , 
=village, =zi, suffix) 
The three processes for deriving disyllabic 
morphemes in Chinese outlined here should be 
viewed as historical processes. The extent to which 
such processes may be realized by native speakers 
to be productive synchronically bears further 
exploration. Of the three processes, the first two, 
i.e., synonym and near-synonym compounding, are 
used frequently by speakers for purposes of 
disambiguation. In view of this development, the 
evolution from monosyllabic words in ancient 
Chinese to disyllabic words in modern Chinese 
does not change the inherent meaning of the 
morphemes (words in ancient Chinese) in many 
cases. The SO of a word often conforms to that of 
its morphemes.  
In English, there are affixal morphemes like dis-, 
un- (negation prefix), or ?less (suffix meaning 
short-age), -ful (suffix meaning ?to have a property 
of?), we can say ?careful? or ?careless? to expand 
the meaning of ?care?. However, it is impossible to 
construct a word like ?*ful-care?, ?*less-care?. 
However, in Chinese, the position of a morpheme 
in many disyllabic words is far more flexible in the 
formation of synonym and near-synonym 
compound words. For instance, ? ?(honor) is a 
part of two similar word ? ? (honor-bright) and 
? ?(outstanding-honor). Morphemes in Chinese 
are like a ?zipped file? of the same file types. When 
it unzips, all the words released have the same SO. 
5.1 Probability of Constituent Morphemes 
of Words with the Same SO 
Most morphemes can contribute to positive or 
negative words, regardless of their inherent 
meaning. For example, ? ? (luck) has inherently a 
positive meaning, but it can construct both positive 
word ? ? (lucky) or a negative word ? ? 
(unlucky). Thus it is not easy to define the 
paradigm set simply by intuition. But we can 
assign a probability value for a morpheme in 
forming polarized words on the basis of corpus 
data. 
The first step is to come up with possible 
paradigm morphemes by intuition in a large set of 
polarized words. With the LIVAC synchronous 
corpus, the types and tokens of the words 
constructed by the selected morphemes can easily 
be extracted. The word types, excluding proper 
nouns, are then manually-labeled as negative, 
neutral or positive. Then to obtain the probability 
that a polar morpheme generates words with the 
same SO, the tokens of the polarized word types 
carrying the morpheme are divided by the tokens 
of all word types carrying the morpheme. For 
example, given a negative morpheme, m1, the 
probability that it appears in negative words in 
token, P(m1, -ve) is given by: 
 
1m Carrying  WordtypesAll of Tokens
1m Carrying rdtypesNegativeWo of Tokens
 
 
Positive morphemes can be done likewise. Ten 
negative morphemes and ten positive morphemes 
were chosen as in table 5. Their values of 
P(morpheme, orientation) are all above 0.95. 
 +ve Morpheme -ve Morpheme 
1  (gift) (hurt) 
2  (win)  
3  (good) (doubt) 
4  (secure) (difficult) 
5  (rich) (rush) 
6  (health)  
7  (happy) (explode) 
8  (honor) (ban) 
9 (hardworking) (collapse) 
10 (smooth) (reject) 
Derived Types 7383 2048 
Tokens 247249 166335 
Table 5: Selected positive and negative 
morphemes 
 
Those morphemes were extracted from a 5-year 
subset of the LIVAC corpus. A morpheme, free to 
construct new words, may construct hundreds of 
words but those words with extremely low 
frequency can be regarded as ?noise?. The ?noise? 
may be ?creative use? or even incorrect use. Thus, 
the number of ready-to-label word types formed 
from a particular morpheme was limited to 50, but 
it must cover 80% of the tokens of all word types 
carrying the morpheme in the corpus (i.e., 80% 
dominance). For example, if the morpheme m1 
constructs 120 word types with 10,000 tokens, and 
the first 50 high-frequency words can reach 8,000 
tokens, then the remaining 70 low-frequency word 
types, or noise, are discarded. Otherwise, the 
number of sampled words would be expanded to a 
number (over 50) fulfilling 80% dominance. 
5.2 Results and Evaluation 
In table 6, the precision of 80.23% is slightly better 
than 79.96% of the 20-word condition, and just 1% 
lower than that of the 40-word condition. However, 
the recall drastically increases from 45.56%, or 
59.57% under the 40-word condition, to 85.03%. 
In other words, the algorithm run with 20 Chinese 
paradigm morphemes resulted not only in high 
precision but also much higher recall than Chinese 
paradigm words in the same corpus. 
% of test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312
Extracted Set 1062 797 531 266
Precision 80.23% 85.44% 90.96% 96.61%
Recall 85.03% 
Table 6: Precision and Recall of SO-PMI of the 20 
paradigm morpheme test set 
 
Since the morphemes were chosen from a subset 
of the corpus for evaluation, we repeated the 
experiment in a separate 1-year corpus (2001-
2002). The results in table 7 reflect a similar 
pattern in the two corpora ? both words and 
morphemes can get high precision, but morphemes 
can double the recall of words. 
 40 Words 20 Morphemes 
Size of test set 1065 
Extracted Set 333 671 
Precision (Full Set) 75.38% 73.62% 
Recall 31.27% 63.00% 
Table 7: Precision (full test set only) and Recall of 
SO-PMI of 40 paradigm words and 20 paradigm 
morphemes in 1-year corpus 
 
It is assumed that a smaller corpus easily leads 
to the algorithm?s low recall because many low-
frequency words in the test set barely associate 
with the paradigm words. To examine the 
assumption, the results were further analyzed with 
the frequency of the test set words. First, the 
occurrence of the test set words in the 9-year 
corpus was counted, then the median of the 
frequency, 44 in this case, was taken. The results 
were divided into two sections from the median 
value, and the recall of two sections was calculated 
respectively, as in table 8.  
?
Table 8: Morpheme-based and word-based recall 
of high-frequency and low-frequency words  
 
The results showed that high-frequency words 
could be largely extracted by the algorithm with 
both morphemes (99.80% recall) and words 
(89.45% recall). However, paradigm words gave 
26.55% recall of low-frequency words, whereas 
paradigm morphemes gave 67.66%. They showed 
that morphemes outperform words in the retrieval 
of low-frequency words. 
Colossal corpora like Turney?s hundred-billion-
word corpus can compensate for the low 
performance of paradigm words in low-frequency 
words. Such a large corpus has been easily-
accessible since the emergence of internet, but it is 
not cost-effective to use the Chinese texts from the 
internet because those texts are not segmented. 
Another way of compensation is the expansion of 
paradigm words, but doubling the number of 
paradigm words just raised the recall from 45.56% 
to 59.57%, as shown in section 4. The supervised 
cost is not reasonable if the number of paradigm 
words is further expanded. 
Morphemes, or single characters in Chinese, 
naturally occur more frequently than words in an 
article, so 20 morphemes can be more discretely-
distributed over texts than 20 or even 40 words. 
The results show that some morphemes always 
retain their inherent SO when becoming 
constituents in other derived words. Such 
morphemes are like a zipped file of the same SO, 
when the algorithm is run with 20 paradigm 
morphemes, it is actually run by thousands of 
paradigm words. Consequently, the recall could 
double while the high precision was not affected.  
It may be argued that the labour cost of defining 
the SO of 20 morphemes is not sufficiently low 
either. The following experiments will demonstrate 
that decreasing the number of morphemes can also 
give satisfactory results. 
6. Experiment with different number of 
morphemes 
The following experiments were done respectively 
by decreasing the number of morphemes, i.e., 14 
and 10 morphemes, chosen from table 5. The 
algorithm was then run with 3 groups of 6 different 
morphemes, in which the morphemes were 
different, and the combination of morphemes in 
each group was random. The morphemes in each 
group are shown in table 9. Other conditions for 
the experiments were unchanged. 
6.1 Results and Evaluation 
Table 10 shows the results with different number 
of morphemes, and table 11 shows those for 
different groups of 6 morphemes. For convenient 
comparison, the tables only show the results of the 
full test set, i.e., no threshold filtering. 
It is shown that the recall falls as the number of 
morphemes is reduced. However, even the average 
recall 66.29% under the 6-morpheme condition is 
still higher than that under the 40-word condition 
(59.57%). In section 5, it was evaluated that low 
recall could be attributed to the low frequency of 
test set words. Therefore, 6 to 10 morphemes are 
already ideal for deducing the SO of high-
frequency words.  
 Number of morphemes used 
Morpheme 20 14 10 6 (Gp1) 
6 
(Gp2) 
6 
(Gp3)
P 
 (gift) 1   1   
P 
 (good) 1 1 1 1   
P 
 (happy) 1 1  1   
P 
 (rich) 1 1   1  
P 
 (honor) 1 1 1  1  
P (smooth) 1 1 1  1  
P 
 (win) 1     1 
P 
 (secure) 1     1 
P 
 (health) 1 1 1   1 
P 
 (hardworking) 1 1 1    
N (doubt) 1 1 1 1   
N (explode) 1 1  1   
N (ban) 1 1 1 1   
N (rash) 1 1 1  1  
N (greedy) 1 1 1  1  
N (difficult) 1 1 1  1  
N (hurt) 1 1    1 
N (rush) 1     1 
N (collapse) 1     1 
N (reject) 1      
Table 9: Morphemes selected for different 
experimental sets, P=+ve, N=-ve, 1=?selected?, 
Gp= Group 
 Number of morphemes used 
No of morphemes 20 14 10 
Size of test set 1249 1249 1249 
Extracted Set 1062 993 915 
Precision (%) 80.23 79.15 79.89 
Recall (%) 85.03 79.50 73.26 
Table 10: Precision and Recall of SO-PMI of the 
test set words with different no. of morphemes 
Group of 
Morphemes 
Group 1 Group 2 Group 3 Average 
Size of test set 1249 1249 1249 1249 
Extracted Set 837 776 871 828
Precision (%) 79.69 78.48 68.77 75.65
Recall (%) 67.01 62.13 69.74 66.29
Table 11: Precision and Recall of SO-PMI of the 
test set words with 3 different groups of 6 
morphemes 
 
The precision remains high from 20 morphemes 
to 6 morphemes, but from table 10 the precision 
varies with different sets of morphemes. Group 3 
gave the lowest precision of 68.77%, whereas 
other groups gave a high precision close to 80%. 
The limited space of this paper cannot allow a 
detailed investigation into the reasons for this 
result, only some suggestions can be made. 
The precision may be related to the dominant 
lexical types of the words constructed by the 
morphemes and those of the test set words. Lexical 
types should be carefully considered in the 
algorithm for Chinese because Chinese is an 
isolating language - no form change. For example, 
the word ? ? (recover) can appear in different 
positions of a sentence, such as the following 
examples extracted from the corpus:  
(1)? ? (...American 
economy is gradually recovering?) 
(2) ?
(?most people is now pessimistic about the 
economy recovery) 
(3) ?
(?decelerates the recovery, but also makes the 
future unpredictable.) 
English allows different forms of ?recovery, like 
?recovery?, ?recovering?, ?recovered? but Chinese 
does not. Lexical types are thus an important factor 
for the precision performance. Another way of 
solving the problems of lexical types is the 
automatic extraction of meaningful units 
(Danielsson, 2003). Simply, meaningful units are 
some frequently-used patterns which consist of two 
or more words. It is useful to automatically extract 
the meaningful units with SO in future. 
Syntactic markers like negation, and creative 
uses like ironical expression of adding quotation 
marks can also affect the precision. Here is an 
example from the corpus: 
(?HONEST BUSINESSMAN?). The quotation 
mark (? ? in English) is to actually express the 
opposite meaning of words within the mark, i.e., 
HONEST means DISHONEST in this case. Such 
markers should further be handled, just as with the 
use of ?so-called?. 
6 Conclusion and Future Work 
This paper presents an algorithm based on 
Turney?s model (2003) for inferring SO of Chinese 
words from their association with strongly-
polarized Chinese morphemes. The algorithm was 
run with 20 and 40 strongly-polarized Chinese 
words respectively in a corpus of 34 million words, 
giving a high precision of 79.96% and 81.05%, but 
a low recall of 45.56% and 59.57%. The algorithm 
was then run with 20 Chinese polarized 
morphemes, or single characters, in the same 
corpus, giving a high precision of 80.23% and an 
even high recall of 85.03%. The algorithm was 
further run with just 14, 10 and 6 morphemes, 
giving a precision of 79.15%, 79.89% and 75.65%, 
and a recall of 79.50%, 73.26% and 66.29% 
respectively.  
Thus, conveniently defined morphemes in 
Chinese enhance the effectiveness of the algorithm 
by simplifying processing and yielding better 
results even in a smaller corpus compared with 
what Turney (2003) used. Just 6 to 10 morphemes 
can give satisfactory results in a smaller corpus. 
The efficient application of Turney?s algorithm 
with help of colossal corpus like hundred-billion-
word corpus is matched by the ready availability of 
internet texts. However, the same convenience is 
not available to Chinese because of the heavy cost 
of word segmentation. 
The efficient application of Turney?s algorithm 
with help of colossal corpus like hundred-billion-
word corpus is matched by the ready availability of 
internet texts. However, the same convenience is 
not available to Chinese because of the heavy cost 
of word segmentation. 
In our experiment, all syntactic markers are 
ignored. Better results can be expected if syntactic 
markers are taken into consideration. An obvious 
example is negation (not, never) which can 
counteract the polarity of a word. In future, we will 
try to handle negation and other syntactic markers. 
The lists of the probability of morphemes 
forming polarized words in section 5.2 can be 
handled by the concept of decision list (Yarowsky, 
2000) which has not been applied in this paper for 
simplification. In the future, decision lists can be 
employed to systematically include the loaded 
features of morphemes. 
The experiment can be conducted with different 
sets of paradigm morphemes, and on corpora of 
different sizes. With the LIVAC synchronous 
corpus (Tsou et al, 2000), it should be possible to 
compare the SO of some words in different 
communities like Beijing, Hong Kong and Taipei. 
The data would be valuable for cultural studies if 
the SO of some words fluctuates in different 
communities.  
SO from association can be also applied to the 
judgment of news articles like editorials on 
celebrities. Given a celebrity name or organization 
name, we can calculate, using SO-PMI, the 
strength of SO of the ?given word?, i.e., the name. 
Then we would be able to tell whether the news 
about the target is positive or negative. For 
example, we tried to calculate the SO-PMI of the 
name ?George W Bush?, the U.S. President, with 
thousands of polarized Chinese words in the 
corpus, it was found that the SO-PMI of ?Bush? 
was about -200 from January to February, 2003, 
and plunged to -500 from March to April, 2003, 
when U.S. launched an ?unauthorized war? against 
Iraq. Such useful applications will be further 
investigated in future. 
References  
DANIELSSON, P. 2003. Automatic Extraction of 
Meaningful Units from Corpora. International 
Journal of Corpus Linguistics, 8(1), 109-127. 
GUO XIAN-ZHEN, ZHANG WEI, LIU JIN, WANG 
LING-LING. 1999. ChangYong BaoBianYi CiYu 
XiangJie CiDian ( ). 
Commercial Press, Beijing.  
HATZIVASSILOGLOU, V., AND MCKEOWN, K.R. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of the 35th Annual Meeting 
of the Association for Computational Linguistics and 
the 8th Conference of the European Chapter of the 
ACL, Madrid, Spain, 174-181. 
HATZIVASSILOGLOU, V. AND WIEBE, J.M. 2000. 
Effects of Adjective Orientation and Grad-ability on 
Sentence Subjectivity. Proceedings of 18th 
International Conference on Computational 
Linguistics (Coling?00), Saarbr?cken, Germany. 
LEHRER, A. 1974. Semantic Fields and Lexical 
Structure. North Holland, Amsterdam and New York. 
STONE, P.J., DUNPHY, D. C., SMITH, M. S., AND 
OGILVIE, D. M. 1966. The General Inquirer: A 
Computer Approach to Content Analysis. MIT Press, 
Cambridge, MA. 
TSOU, B.K. 1976. Homophony and Internal Change in 
Chinese. Computational Analyses of Asian and 
African Languages 3, 67-86. 
TSOU, B.K., TSOI, W.F., LAI, T.B.Y., HU, J. AND 
CHAN, S.W.K. 2000. LIVAC, A Chinese 
Synchronous Corpus, and Some Applications. 
Proceedings of the ICCLC International Conference 
on Chinese Language Computing. Chicago, 233-238. 
TURNEY, P.D. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 
Association for Computational Linguistics 40th 
Anniversary Meeting, University of Pennsylvania, 
Philadelphia, PA, USA. 
TURNEY, P.D. & LITTMAN, M.L. 2003. Measuring 
Praise and Criticism: Inference of Semantic 
Orientation from Association. ACM Transactions on 
Information System (TOIS), 21(4), pp315-346. 
WANG GUO-ZHANG. 2001. A Dictionary of Chinese 
Praise and Blame Words (
). Sinolingua, Beijing.  
WIEBE, J.M., BRUCE, R., BELL, M. MARTIN, M., 
AND WILSON, T. 2001. A Corpus Study of 
Evaluative and Speculative Language. Proceedings 
of the Second ACL SIG on Dialogue Work-shop on 
Discourse and Dialogue. Aalborg, Denmark.  
YAROWSKY, D. 2000. Hierarchical Decision Lists for 
Word Sense Disambiguation. Computers and the 
Humanities, 34(1-2). 
 
Mining Discourse Markers for Chinese Textual Summarization 
Samuel W. K. Chan I, Tom B. Y. Lai 2, W. J. Gao 3, Benjamin K. T'sou 4 
J 24Languag e Information Sciences Research Centre 
City Un!versity of Hong Kong 
Tat CheeAvenue, Kowloon Tong, 
Hong Kong SAR, China 
3 North Eastern University, China 
Iswkchan@cs.cityu.edu.hk, {2cttomlai, 4rlbtsou} @cpccux0.cityu.edu.hk, 3wjgao@ramm.neu.edu.cn 
Abstract 
Discourse markers foreshadow the message 
thrust of texts and saliently guide their 
rhetorical structure which are important for 
content filtering and text abstraction. This 
paper reports on efforts to automatically 
identify and classify discourse markers in 
Chinese texts using heuristic-based and 
corpus-based ata-mining methods, as an 
integral part of automatic text 
summarization via rhetorical structure and 
Discourse Markers. Encouraging results are 
reported. 
1 Introduction 
Discourse is understood to refer to any form of 
language-based communication involving multiple 
sentences or utterances. The most important forms 
of discourse of interest o computerized natural 
.language processing are text and dialogue. While 
discourse such as written text normally appears to 
? be a linear sequence of clauses and sentences, it 
has "long been recognized by linguists that these 
clauses and sentences tend to cluster together into 
units, called discourse segments, that are related 
pragmatically toform a hierarchical structure. 
Discourse analysis goes beyond the levels of 
syntactic and semantic analysis, which typically 
treats each sentence as an isolated, independent 
unit. The function of discourse analysis is to 
divide a text into discourse segments, and to 
recognize and re-construct the discourse structure 
of the text as intended by its author. Results of 
discourse analysis can be used to solve many 
important NLP problems such as anaphoric 
reference (Hirst 1981), tense and aspect analysis 
(Hwang and Schubert 1992), intention recognition 
(Grosz and Sidner 1986; Litman and Allen 1990), 
or'can be directly applied to computational NLP 
applications uch as text abstraction (Ono et al 
1994; T'sou et al 1996) and text generation 
(McKeown 1985; Lin et al 1991). 
Automatic text abstraction has received 
considerable attention (see Paice (1990) for a 
comprehensive r view). While some statistical 
approaches have had some success in extracting 
one or more sentences which can serve as a 
summary (Brandow et al 1995; Kupiec et al 1995; 
Salton et al 1997), summarization i  general has 
remained an elusive task. McKeown and Radev 
(1995) develop a system SUMMONS to 
summarize full text input using templates 
produced by the message understanding systems, 
developed under ARPA human language 
technology. Unlike previous approaches, their 
system summarizes a series of news articles on the 
same event, producing a paragraph consisting of 
one or more sentences. Endres-Niggemeyer et al 
(1995) uses a blackboard system architecture with 
co-operating object-oriented agents and a dynamic 
text representation which borrows its conceptual 
relations from Rhetorical Structure Theory (RST) 
(Mann and Thompson 1986). Furthermore, 
connectionist models of discourse summarization 
have also attracted a lot of attention (Aretoulaki et 
al. 1998). The main underlying principles are the 
distributed encoding of concepts and the 
simulation of human association with a large 
amount of processing nodes. What is crucial in 
this approach is to provide a subconceptual l yer 
in the linguistic reasoning. 
As in Paice (1990), summarization 
techniques in text analysis are severely impaired 
by the absence of a generally accepted iscourse 
11 
model and the use of superstructural schemes is 
promising for abstracting text. Johnson et al (1993) 
describes a text processing system that can 
identify anaphors o that they may be utilized to 
enhance sentence selection. It is based on the 
assumption that sentences which contain non- 
anaphoric noun phrases and introduce key 
concepts into the text are worthy of inclusion in an 
abstract. Ono et al (1994), T'sou et al (1992) and 
Marcu (1997) focus on discourse structure in 
summarization using the Rhetorical Structure 
Theory (RST). The theory has been exploited in a. 
number of computational systems (e.g. Hovy 
1993). The main idea is to build a discourse tree 
where each node of the tree represents a RST 
relation. Summarization is achieved by trimming 
unimportant sentences on the basis of the relative 
saliency or rhetorical relations. On the other hand, 
cohesion can also provide context o aid in the 
resolution of ambiguity as well as in text 
summarization (Halliday and Hasan 1976; Morris 
and Hirst 1991; Hearst 1997). Mani et al (1998) 
describes a method based on text coherence which 
models text in terms of macro-level relations 
between clauses or sentences tohelp determine the 
overall argumentative structure of the text. They 
examine the extent to which cohesion and 
coherence can each be used to establish saliency of 
textual units. 
The SIFAS (S,yntactic Marker based Eull- 
Text Abstration System) system has been designed 
and implemented to use discourse markers in the 
automatic summarization of Chinese. Section 2 
provides an introduction to discourse markers in 
Chinese. An overview of SIFAS is presented in 
Section 3. In Section 4, we describe a coding 
scheme for tagging every discourse marker 
appearing in the SIFAS corpus. In Section 5, we 
introduce a heuristic-based algorithm for 
automatic tagging of discourse markers. In Section 
6, we describe the application of the C4.5 
algorithm to the same task. In Section 7, we 
present he evaluation results of applying the two 
algorithms to corpus tagging, followed by a 
conclusion. 
2 Chinese Discourse Markers 
Among all kinds of information that may be found 
in a piece of discourse, discourse markers (also 
known as discourse connectives, clue words 
(Reichman 1978; Siegel et al 1994) or cue phrases 
(Grosz et al 1986; Litman 1996) are regarded as 
the major linguistic deviceavailable for a writer to 
structure a discourse. Discourse markers are 
expressions which signal a sequential relationship 
between the current basic message and the 
previous discourse. Schiffrin (1987) is concerned 
with elements which mark sequentially dependent 
units of discourse. She examines discourse 
markers in interview data, looking specifically at 
their distribution and their particular 
interpretation(s). She proposes that these markers 
typically serve three functions: (i) they index 
adjacent utterances to the speaker, the hearer, or 
both; (ii) they index adjacent utterances to prior 
and/or subsequent discourse; (iii) they work as 
contextual coordinates for utterances by locating 
them on one or more planes of her discourse 
model. 
Discourse markers also figure prominently in
Chinese which has a tendency to delay topic 
introduction (Kaplan 1996; Kirkpatrick 1993). 
Hinds (1982) and Kong (1998) also maintain that 
the Chinese tendency of delayed topic introduction 
is heavily influenced by the qi cheng zhuan he 
canonical structure (a Chinese rhetorical pattern). 
In a study examining rhetorical structure in 
Chinese, Kirkpatrick (1993) found that several 
major patterns, favored and considered to be good 
style by native Chinese writers, are hinted at by 
Chinese discourse markers. Although the effect of 
discourse markers in other languages might not be 
too prominent, here is a great necessity to study 
discourse markers in Chinese in order to capture 
the major associated rhetorical patterns in Chinese 
texts. While the full semantic understanding in
Chinese texts is obviously much more difficult to 
accomplish, the approach using text mining 
techniques in identifying discourse markers and 
associated rhetorical structures in a sizeable 
Chinese corpus will be certainly beneficial to any 
language processing, such as summarization and 
knowledge xtraction i  Chinese. 
In Chinese, two distinct classes of discourse 
markers are useful for identification and 
interpretation of the discourse structure of a 
Chinese text: pr imary discourse markers and 
secondary discourse markers (T'sou et al 1999). 
Discourse markers can be either words or phrases. 
Table 1 provides a sample listing of various 
12 
I 
I 
I 
I 
i 
I 
I 
rhetorical relations and examples considered in 
this research. 
\[Discourse Type 
Sufficiency 
Necessity 
Causality 
Deduction 
?\[dversativity 
Concession 
Conjunction 
Disjunction 
Progression 
Table 
Discourse 
Primary Marker 
ruguo 'if', name 'then' 
zhiyou 'only if', cai 'only \[hen' 
?inwei 'because', suoyi 'therefore' 
iiran 'given that', name 'then' 
suiran 'although', danshi 'but' 
"ishi 'even if', rengran 'still' 
chule 'except', j ianzhi 'also' 
huozhe 'or', huozhe 'or' 
~udan 'not only', erqie 'but also' 
/ 
Examples of Discourse Markers 
Markers 
Discourse Type 
Summary 
Contrast 
fflustration 
Specification 
Generalization 
Digression 
rtemization 
Paraphrasing 
Equivalence 
Enquiry 
ludgment 
Secondary Marker  
zong er yan zhi 'in one word' 
~hishi shang 'in fact' 
liru 'for example' 
tebie shi 'in particular' 
dati er yan 'in general' 
wulun ruhe 'anyway'  
shouxian 'first', qici "next" 
huan ju hua shuo 'in other words' 
zhengru 'just as' 
nandao ('does it mean... ')  
kexi 'unfortunately' 
and Associated Rhetorical Relations in Chinese 
It may be noted that our analysis of Chinese 
has yielded about 150 discourse markers, and that 
on the average, argumentative t xt (e.g. editorials) 
in Chinese shows more than one third of the 
discourse segments to contain discourse markers. 
While primary discourse markers can be paired 
discontinuous constituents, with each marker 
attached to one of the two utterances or 
propositions, the socondary discourse markers 
tend to be unitary constituents only. In the case of 
primary discourse markers, it is quite common that 
one member of the pair is deleted, unless for 
emphasis. The deletion of both discourse markers 
ts also possible. The recovery process therefore 
faces considerable challenge ven when concerned 
? with the deletion of only one member of the paired 
discourse markers. Since these discourse markers 
'have no unique lexical realization, there is also the 
need for disambiguation i  a homocode problem. 
Moreover, primary discourse markers can 
also be classified as simple adverbials, as is the 
case in English: 
(I) Even though a child, John is so tall that 
he has problem getting half-fare. 
(2) Even though a child, (because) John is 
tall, so he has problem getting half-fare. 
In (1), so is usually classified as an adverb 
within a sentence, but in (2) so is recognized as 
marking a change in message thrust at the 
discourse level. 
In the deeper linguistic analysis the two so's 
may be related, for they refer to a situation 
involving excessive height with implied 
consequence which may or may not be stated. In 
terms of the surface syntactic structure, so in (1) 
can occur in a simple (exclamatory) sentence (e.g. 
"John is so tall!"), but so in (2) must occur in the 
context of complex sentences. Our concern in this 
project is to identify so in the discourse sense as in 
(2) in contrast to so used as an adverb in the 
sentential sense as in (1). Similar difficulties are 
found in Chinese, as discussed in Section 7. 
3 SIFAS System Architecture 
From the perspective of discourse analysis, the 
study of discourse markers basically involves four 
distinct but fundamental issues: 1) the occurrence 
and the frequency of occurrence of discourse 
markers (Moser and Moore 1995), 2) determining 
whether a candidate linguistic item is a discourse 
marker (identification / disambiguation) 
(Hirschberg and Litman 1993; Siegel and 
McKeown 1994), 3) determination or selection of 
the discourse function of an identified discourse 
marker (Moser and Moore 1995), and 4) the 
coverage capabilities (in terms of levels of 
embedding) among rhetorical relations, as well as 
among individual discourse markers. Discussion 
of these problems for Chinese compound 
sentences can be found in Wang et al (1994). 
Previous attempts to address the above 
problems in Chinese text have usually been based 
on the investigators' intuition and knowledge, or 
on a small number of constructed examples. In our 
current research, we adopt heuristics-based 
13 
corpus-based 
learning to discover the correlation between 
various linguistic features and different aspects of 
approaches, and use machine discourse marker usage. Our research framework 
i 
Statistical Analysis i 
Discourse Analysis 
Text Abstraction i 
i Natural Language 1 
\[ Understanding 
\[ 
! 
I 
Analysis 
& 
Application 
is shown in Figure I. 
Raw Corpus . 
...... (Editorials) 
...... ~ ' " !  ....... ~,~uto Tagging & 
Proofreading 
Segmented 
Corpus " 
~'~-~ ~ J  Word 
~.-~.._~.: . . . .  Segmentation 
Discourse 
Marker & 
Rhetorical 
Relation Tagged 
Corpus 
Feature 
.. ~ ~. .  Extractiov 
Feature 
Database 
; i i 1 
f f  ~ i 
j 
i Dicti?naries \] ,~ ,, 
Heuristics I / .......... ;-
i Induced Rules ! \ ........... ~ i 
? "- '~- ~ ML & 
Evaluation 
xq 
=_. 
OQ 
K 
?-) 
- i  
?tl 
t-' ?D 
,< 
Figure 1 Framework for Corpus-based Study of Discourse Marker Usage in Chinese Text 
Data in the segmented corpus are divided 
into two sets of texts, namely, the training set and 
:the test set, each of which includes 40 editorials in 
:our present research. Texts in the training set are 
. manually and semi-automatically tagged to reflect where, 
the  properties of every Candidate Discourse DMi: 
Marker (CDM). Texts in the test set are 
automatically tagged and proofread. Different 
algorithms, depending on the features being RRi: 
investigated, are derived to automatically extract 
the interesting features to form a feature database. RPi: 
Machine learning algorithms are then applied to 
the feature database to generate linguistic rules 
(decision trees) reflecting the characteristics of 
various discourse markers and the relevant CT~: 
rhetorical relations. For every induced rule (or a 
combination of them), its performance is evaluated 
by tagging the discourse markers appearing in th- 
test set of the corpus. 
4 A Framework for Tagging MN~: 
Discourse Markers 
The following coding scheme is designed to 
encode all and only Real Discourse Markers RN~: 
(RDM) appearing in the SIFAS corpus. We 
describe the i th discourse marker with a 7-tuple 
RDMi, 
RDMi=< DMI, RR/,  RPI, CTi, MNi ,  RNI ,  
> 
the lexical item of the Discourse Marker, 
or the value 'NULL'.  
the Rhetorical Relation in which DMi is 
one of the constituting markers. 
the Relative Position of DM;. The value 
of RPi can be either 'Front' or 'Back' 
denoting the relative posit ion of the 
marker in the rhetorical relation RRi. 
the Connection Type of RRi. The value 
of CT~ can be either 'Inter" or ' Intra', 
which indicates that the DM~ functions as 
a discourse marker in an inter-sentence 
relation or an Intra-sentence relation. 
the Discourse Marker Sequence Number. 
The value of MNi is assigned 
sequentially from the beginning of the 
processed text to the end. 
the Rhetorical Relation Sequence 
Number. The value of RNi is assigned 
14 
I 
I 
I 
I 
I 
l 
sequentially to the corresponding 
rhetorical relation RR; in the text. 
OTi: the Order Type of RR;. The value of OTi 
can be 1, -1 or 0, denoting respectively 
the normal order, reverse order or 
irrelevance of the premise-consequence 
ordering of RRI. 
For Apparent Discourse Markers (ADM) that do 
not function as real discourse markers in a text, a 
different 3-tuple coding scheme is used to encode 
them: 
ADM~ = < LIi, *, SNi > where, 
LIi: the Lexical Item of the ADM. 
SNi: the Sequence Number of the ADM. 
To illustrate the above coding scheme 
consider the following examples of encoded 
sentences where every CDM has been tagged to be 
either a 7-tuple or a 3-tuple. 
Example 1 
<vouvu ('because').Causalitv. Front. lntra. 2. 2. 
/> Zhu Pei ('Jospin') zhengfu ('government') 
taidu ('attitude') qiangying ('adamant'), chaoye 
('government-public') duikang ('confrontation') 
yue-yan-yue ('more-develop-more') -lie 
('strong'), <NULL. Causality. Back. Intra, O. 2. 
/> gongchao ('labour unrest') <vi ('with'). * 
:1> liaoyuan ('bum-plain') zhi ('gen') shi 
'tendency' xunshu 'quick' poji 'spread to' ge 
('every') hang ('profession') ge ('every') ye 
, ('trade'). 
'As a result of the adamant attitude of the 
Jospin administration, confrontation between 
the government and the public is becoming 
w.orse and worse. Labour unrest has spread 
quickly to all industrial sectors.' 
From the above tagging, we can immediately 
obtain the discourse structure that the two clauses 
encapsulated by the two discourse markers youyu 
(with sequence number 2) and NULL (with 
sequence number 0). They have formed a causality 
relation (with sequence number 2). We denote this 
as a binary relation 
Causality(FrontClause(2), BaekClause(2)) 
where FrontClause(n) denotes the discourse 
segment that is encapsulated by the Front 
discourse marker of the corresponding rhetorical 
relation whose sequence number is n. 
15 
BackClause(n) can be defined similarly. Note that 
although yi is a CDM, it does not function as a 
discourse indicator in this sentence. Therefore, it is " 
encoded as an apparent discourse marker. 
Example 2 
<dan ('however'). Adversativitv. Back. Inter. 
17. 14. 1> <ruguo 'if'. Su_~ciencv. Front. Inter, 
18. 15. 1> Zhu Pei ('Jospin') zhengfu 
('government') cici ('this time') zai ('at') 
gongchao ('labour unrest') mianqian ('in the 
face of') tuique ('back down'), <NULL. 
Su.~ciencv. Back. Inter. O. 15. 1> houguo 
('result') <geng.('more'). *. 3> shi bukan ('is 
unbearable') shexian ('imagine'). 
'However, if the Jospin administration backs 
down in the face of the labour unrest, the result 
will be terrible.' 
From the above tagging, we can obtain the 
following discourse structure with embedding 
relations: 
A dversativity ( &F (14 ), 
Sufficiency(F rontClause(15), 
BackClause(15))) 
where &F(n) denotes the Front discourse segment 
of an inter-sentence rhetorical relation whose 
sequence number is n. We can define &B(n) 
similarly. 
5 Heuristic-based Tagging of 
Discourse Markers 
In the previous section, we have introduced a
coding, scheme for CDMs, and have explained 
how to automatically derive the discourse 
structure from sentences with tagged discourse 
markers. Now, the problem we have to resolve is: 
Is there an algorithm that will tag the markers 
according to the above encoding scheme? 
To derive such an algorithm,-even an 
imperfect one, it is necessary that we have 
knowledge of the usage patterns and statistics of 
discourse markers in unrestricted texts. This is 
exactly what project SIFAS intends to achieve as 
explained in Section 3. Instead of completely 
relying on a human encoder to encode all the 
training texts in the SIFAS corpus, we have 
experimented with a simple algorithm using a 
small number of heuristic rules to automatically 
encode the CDMs. The algorithm is a 
straightforward matching algorithm for rhetorical 
relations based recognition of their constituent 
discourse markers as specified in the Rhetorical 
Relation Dictionary (T'sou et al 1999). The 
following principles are adopted by the heuristic- 
based algorithm to resolve ambiguous ituations 
encountered in the process of matching discourse 
markers: 
(1) Principle of Greediness: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the first matched relation 
from the left. 
(2) Principle of Locality: When matching a pair 
of CDMs for a rhetorical relation, priority is 
given to the relation where the distance 
between its constituent CDMs is shortest. 
(3) Principle of Explicitness: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the relation that has both 
CDMs explicitly present. 
(4) Principle of Superiority: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the inter-sentence relation 
whose back discourse marker matches the 
first CDM of a sentence. 
(5) Principle of Back-Marker Preference: this 
principle is applicable only to rhetorical 
relations where either the front or the back 
marker is absent. In such cases, priority is 
given to the relation with the back marker 
present. 
' Application of the above principles to 
process a text is in the order shown, with the 
? exception that the principle of greediness is 
applied whenever none of the other principles can 
be, used to resolve an ambiguous ituation. The 
following pseudo code realizes principles 1, 2 and 
3: 
I := l ;  
whi le I < NumberOfCDMsInTheSentence  do 
beg in  
for J := l  to NumberOfCDMsInTheSentencen  - 
I do 
i f  ((not CDMs\ [ J \ ] .Tagged)  and (not 
CDMs\ [ J+ I \ ] .Tagged)}  then 
Match ing(CDMs\ [ J \ ] ,  CDMs\ [ J+ I \ ] )  ; 
I := I + 1 ; 
end ; 
The following code realizes principles 1,4 and 5: 
16 
for I :=l  to NumberOfCDMs lnTheSentence  do 
begin 
if (not CDMs\ [ I \ ] .Tagged)  then  
Match ing(NULL ,  CDMs\[ I \ ] )  ; 
i f  (not CDMs\ [ I \ ] .Tagged)  then  
Match ing(CDMs\ [ I \ ] ,  NULL) ; 
end ; 
In the above pseudo codes, CDMs\[\] denotes 
the array holding the candidate discourse markers, 
and the Boolean variable Tagged is used to 
indicate whether a CDM has been tagged. 
Furthermore, the procedure Matching0 is to 
examine whether the first word or phrase 
appearing in a sentence is an inter-sentence 
CDMs\[I\]. 
6 Mining Discourse Marker Using 
Machine Learning 
Data mining techniques constitute a field 
dedicated to the development of computational 
methods underlying learning processes and they 
have been applied in various disciplines in text 
processing, such as finding associations in a 
collection of texts (Feldman and Hirsh 1997) and 
mining online text (Knight 1999). In this section, 
we focus on the problem of discourse marker 
disambiguation using decision trees obtained by 
machine learning techniques. Our novel approach 
in mining Chinese discourse markers attempts to 
apply the C4.5 learning algorithm, as introduced 
by Quinlan (1993), in the context of non-tabular, 
unstructured ata. A decision tree consists of 
nodes and branches connecting the nodes. The 
nodes located at the bottom of the tree are called 
leaves, and indicate classes. The top node in the 
tree is called the root, and contains all the training 
examples that are to be divided into classes. In 
order to minimize the branches in the tree, the best 
attribute is selected and used in the test at the root 
node of the tree. A descendant of the root node is 
then created for each possible value of this 
attribute, and the training examples are sorted to 
the appropriate descendant node. The entire 
process is then repeated using the training 
examples associated with each descendant ode to 
select he best attribute for testing at that point in 
the tree. A statistical property, called information 
gain, is used to measure how well a given attribute 
differentiates the training examples according to 
their target classificatory scheme and to select he 
I 
I 
I 
I 
| 
I 
I 
I 
! 
I 
I 
I 
I 
I 
II. 
I 
I- 
II 
I 
most suitable candidate attribute at each step while 
expanding the tree. 
The attributes we use in this research include 
the candidate discourse marker itself, two words 
immediately to the left of the CDM, and two 
words immediately to the right of the CDM. The 
attribute names are F2, F1, CDM, B1,  B2, 
respectively. All these five attributes are discrete. 
The following are two examples: 
? ",", dan 'but', youyu 'since', Xianggang 
'Hong Kong', de 'of', T. 
? zhe 'this', yi 'also', zhishi 'is only', 
Xianggang 'Hong Kong', de 'of', F. 
where "T" denotes the CDM youyu as a discourse 
marker in the given context, and "F" denotes that 
zhishi is not a discourse marker. 
In building up a decision-tree in our 
application of C4.5 to the mining of discourse 
markers, entropy, first of all, is used to measure 
the homogeneity of the examples. For any possible 
candidate A chosen as an attribute in classifying 
the training data S, Gain(S, A) information gain, 
relative to a data set S is defined. This information 
gain measures the expected reduction in entropy 
and defines one branch for the possible subset Si 
of the training examples. For each subset Si, a new 
test is then chosen for any further split. If Si 
satisfies a stopping criterion, such as all the 
element in S~ belong to one class, the decision tree 
is formed with all the leaf nodes associated with 
the most frequent class in S. C4.5 uses arg 
max(Gain(S, A)) or arg max(Gain Ratio(S, A)) as 
defined in the following to construct the minimal 
decision tree. 
c 
Entropy(S) = -~_ -  p, log 2 p~ (Eqn. I) 
i=1 
Gain(S,A) = Entropy(S)- ~ -~'Entropy(S~) isl 
(Eqn. 2) 
Gain Ratio - Gain(S,A) (Eqn. 3) 
Splitlnformation( S, A) 
? ? j is, t.  s,i where Splitlnformation=-2./--,Jog 2 ~, Si is 
!S! iS! 
subset of S for which A has value vt 
In our text mining, according to the number 
of times a CDM occurs in the 80 tagged editorials, 
we select 75 CDMs with more than 10 occurrences. 
To avoid decision trees being over-fitted or trivial, 
for F2, F1, B1 and B2, only values of attributes 
with frequency more than 15 in the corpus are 
used in building the decision trees. We denote all 
values of attributes with frequency less than 15 as 
'Other'.  If a CDM is the first, the second or the 
last word of a sentence, values of F2, F1, or B2 
will be null, we denote a null-value as "*". The 
following are two other examples: 
? "*", "*", zheyang 'thus', ",", Other, T. 
? "*", "*", zheyang 'thus', Other, de 'of', F. 
7 Evaluation 
7.1 Evaluation of Heuristic-based 
Algorithm 
In order to evaluate the effectiveness of the 
heuristic-based algorithm, we randomly selected 
40 editorials from Ming Pao, a Chinese newspaper 
of Hong Kong, to form our test data. Only 
editorials are chosen because they are mainly 
argumentative texts and their lengths are relatively 
uniform. 
The steps of evaluation consist of: 1) tagging 
all of the test data using the heuristic-based 
algorithm, and 2) proofreading, correcting and 
recording all the tagging errors by a human 
encoder. The resulting statistics include, for each 
editorial in the test data, the number of lexical 
items (#Lltms), the number of sentences (#Sens), 
the number of discourse markers (#Mrkrs), and the 
number of sentences containing at least one 
discourse marker (#CSens). Table 2 shows the 
minimum, maximum and average values of these 
characteristics. The ratio of the average number of 
discourse markers to the average number of lexical 
items is 4.37%, and the ratio of the average 
number of sentences 
discourse marker to 
sentences i  62.66%. 
#Lltms 
MIN 466 
MAX 1082 
AVERAGE 676.25 
containing at least one 
the average number of 
#Mrkrs #Sens #CSens 
14 11 6 
52 45 26 
29.58 22.15 13.88 
Table 2 Characteristics of the Test Data 
Our evaluation is based on counting the 
number of discourse markers that are correctly 
17 
tagged. For incorrectly tagged iscourse markers, 
we classify them according to the types of errors 
that we have introduced in T'sou et al (1999). We 
define two evaluation metrics as follows: Gross 
Accuracy (GA) is defined to be the percentage of
correctly tagged discourse markers to the total 
number of discourse markers while Relation- 
Matching Accuracy (RMA) is defined to be the 
percentage of correctly tagged discourse markers 
to the total number of discourse markers minus 
those errors caused by non-markers and 
unrecorded markers. The results for our testing. 
data have GA = 68.89% and RMA = 95.07%. 
Since the heuristic-based algorithm does 
not assume any knowledge of the statistics and 
behavioral patterns of discourse markers, our GA 
demonstrates the usefulness of the algorithm in 
alleviating the burden of human encoders in 
developing a sufficiently large-corpus for the 
purpose of studying the usage of discourse 
markers. 
In our experiment, most errors come from 
tagging non-discourse markers as discourse 
markers (T'sou et al 1999). This is due to the fact 
that, similar to the question of cue phrase 
polysemy (Hirschberg and Litman 1993), many 
Chinese discourse markers have both discourse 
senses and alternate sentential senses in different 
;utterances. For example: 
? ... Zhe ('this') buguo shi ('only is') yi ('one') 
ge ('classifier') wanxiao ('joke') 
...('This is only a joke'.) (sentential sense) 
? ...Buguo ('however'), wo (T)  bu ('neg') 
zheyang ('thus') renwei ('consider') 
? . . ( 'But  I don't think so.') (discourse sense) 
7.2 Evaluation of Decision Tree 
Algorithm (with C4.5) 
In Section 6, we discuss how machine learning 
techniques have been applied to the problem of 
discourse marker disambiguation in Chinese. 
In our experiment, there are a total of 2627 
cases. In our decision tree construction, we use 75 
percent of the total cases as a training set, and the 
remaining 25 percent of cases as a test set. Many 
decision trees can be generated by adjusting the 
parameters in the learning algorithm. Many 
decision trees generated in our experiment have an 
accuracy around 80% for both the training set and 
the test set. Figure 2 shows one of the possible 
decision trees in our experiment. The last branch 
of the decision tree 
F1 = danshi 'but' 
I CDM in {ru 'if', reng 'still', geng 'even more', que 
'however' }:F (6/0) 
I CDM in {chule 'except', youyu 'since', ruo 'if"} : T 
(4/0) 
can be explained as: 
if (F1 = danshi 'but') then 
if (CDM in {ru 'if', reng 'still', geng "even more', que 
'however' }) then dassify as F 
else 
if (CDM in {chule 'except',youyu 'since', ru0 'if '  }) 
then classify as T 
Decision Tree: (Size = 38, Items = 1971, Errors = 282) 
F1 in {di, ye, yi} : F (25/5) 
F i  in (,shi, ;} : T (712/131) 
F1 = Other: 
F I  = danshi :
I CDM in {ru, reng, geng, que} : F (7/10) 
I CDM in {chule, youyu, ruo} : T (4/0) 
Evaluation on trainine data from Data. Data (1971 cases~: 
Classified results: 
T F I <" Classified 
937 125 \] C lass :T  
157 752 C.lass : F Errors : 282 (14.3%) 
Evaluation on testin~ data from Data. Test (656 cases): 
T F ~ f i e d  
293 62 \[Class : T 
68 233 IClass : F Errors : 130 (19.8%) 
1 
Figure 2 An Example of Decision Trees 
The two numbers in the brackets denote the 
number of cases covered by the branch and the 
number of cases being misclassified respectively: 
The results of our experiment will be elaborated 
on in future, when we shall also explore the 
application of machine learning techniques to 
recognizing rhetorical relations on the basis of 
discourse markers, and extracting important 
sentences from Chinese text. 
8. Conclusion 
We discuss in this paper the use of discourse 
markers in Chinese text summarization. Discourse 
structure trees with nodes representing RST 
(Rhetorical Structure Theory) relations are built 
and summarization is achieved by trimming 
18 
unimportant sentences on the basis of the relative 
saliency or rhetorical relations. In order to study 
discourse markers for use in the automatic 
summarization f Chinese, we have designed and 
implemented the SIFAS system. We investigate 
the relationships between various linguistic 
features and different aspects of discourse marker 
usage on naturally occurring text. An encoding 
scheme that captures the essential features of 
discourse marker usage is introduced. A heuristic- 
based algorithm for automatic tagging of discourse 
markers is designed to alleviate the burden of a 
human encoder in developing a large corpus of 
encoded texts and to discover potential problems 
in automatic discourse marker tagging. A study on 
applying machine learning techniques todiscourse 
marker disambiguation is also conducted. C4.5 is 
used to generate decision tree classifiers. Our 
results indicate that machine learning is a 
promising approach to improving the accuracy of 
discourse marker tagging. 
9 Acknowledgement  , 
Support for the research reported here is 
provided through the Research Grants Council 
of Hong Kong under Competitive Earmarked 
Research Grants (CERG) No. 9040067, 
9040233 and 9040326. 
10 References 
~Aretoulaki M., Scheler G. and Brauer W. (1998) 
"Connectionist Modeling of Human Event 
Memorization Processes with Application to 
Automatic Text Summarization." In 
Proceedings of AAAI Spring Symposium on 
Intelligent Text Summarization, Stanford, pp. 
148-150. 
Brandow R., Mitze K. and Rau L. F. (1995) 
"Automatic Condensation of Electronic 
Publications by Sentence Selection." 
Information Processing and Management, 
31(5): 675-685. 
Endres-Niggemeyer B., Maier E. and Sigel A. 
(1995) "How to Implement a Naturalistic 
Model of Abstracting: Four Core Working 
Steps of an Expert Abstractor." Information 
Processing and Management, 31(5): 631-674. 
19 
Feldman R. and Hirsh H. (1997). "Finding 
associations in collections of text." In R.S. 
Michalski I. Bratko and Kubat M. (Eds.)," 
Machine Learning and Data Mining: Methods 
and Applications, pp. 224-240. Wiley. 
Grosz B.J. and Sidner C. (1986) "Attention, 
Intention, and the Structure of Discourse," 
Computational Linguistics 12(3): 175-204. 
Halliday M. A. K. and Hasan R. (1976) Cohesion 
in English, Longman. 
Hearst M. A. (1997) "Texttiling: Segmenting Text 
into Multi-paragraph Subtopic Passages." 
Computational Linguistics, 23(1):33-64. 
Hinds J. (1982) "Inductive, deductive, quasi- 
.inductive: Expository writing in Japanese, 
Korean, Chinese, and Thai." In U. Connor and 
A.M. Johns (Eds.). Coherence in Writing, pp. 
89-109. TESOL publisher. 
Hirschberg J. and Litman D. (1993) "Empirical 
Studies on the Disambiguation f Cue Phrases." 
Computational Linguistics 19(3): 501-530. 
Hirst G. (1981) "Discourse Oriented Anaphoral 
Resolution in Natural Language Understanding: 
A Review." Computational Linguistics 7(2): 
85-98. 
Hovy E. (1993) "Automated Discourse Generation 
using Discourse Structure Relations." Artificial 
Intelligence 63: 341-385. 
Hwang C. H. and Schubert L. K. (1992) "Tense 
Trees as the 'Fine Structure' of Discourse." In 
Proc. 30 th Annual Meeting, Assoc. for 
Computational Linguistics, pp. 232-240. 
Johnson F. C., Paice C. D., Black W. J. and Neal 
A. P. (1993) "'The Application of Linguistic 
Processing to Automatic Abstract Generation." 
Journal of Document and Text Management I:
215-241. 
Kaplan R. B. (1996) "Cultural though patterns in 
intercultural education." Language Learning, 
l&2: 1-20. 
Kirkpatrick A. (1993) "Information sequencing in
modem standard Chinese in a genre of extended 
spoken discourse." Text 13(3): 423-453. 
Kong K.C.C. (1998) "Are simple business request 
letters really simple? A comparison of Chinese 
and English business request letters." Text 18(1 ) :  
103-141. 
Knight K. (1999) "Mining online text." 
Communications of the A CM 42(11): 58-61. 
Kupiec J., Pedersen J., and Chen F. (1995) "A 
Trainable Document Summarizer." In 
Proceedings of the lff h Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval, Seattle, 
pp. 68-73. 
Lin H. L., T'sou B. K., H. C. Ho, Lai T., Lun C., 
C. K. Choi and C.Y. Kit. (1991) "Automatic 
Chinese Text Generation Based on Inference 
Trees.'" In Proc. of ROCL1NG Computational 
Linguistic Conference IV, Taipei, pp. 215-236. 
Litman D. J. and Allen J. (1990) "Discourse 
Processing and Commonsense Plans." In Cohen 
et al(ed.) Intentions in Communications, pp. 
365-388. 
Litman D. J. (1996) "Cue Phrase Classification 
Using Machine Learning." Journal of Artificial 
Intelligence Research 5: 53-94. 
Mani I., Bloedorn E. and B. Gates (1998) "Using 
Cohesion and Coherence Models for Text 
Summarization." In Proceedings of AAAI 
Spring Symposium on Intelligent Text 
Summarization, Stanford, pp. 69-76. 
Mann W. C. and Thompson S. A (1988) 
"Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization." Text 
8(3): 243-281. 
Marcu D. (1997) "From Discourse Structures to 
Text Summaries." In Proceedings of the 
A CL/EA CL '97 Workshop on Intelligent 
: Scalable Text Summarization, Spain, pp. 82-88. 
McKeown K. and Radev D. (1995) "Summaries of 
Multiple News Articles." In Proceedings of the 
18 'h Annual International A CM S1GIR 
Conference on Research and Development in
Information Retrieval, Seattle, pp. 74-82. 
McKeown K. R. (1985) "Discourse Strategies for 
Generating Natural-Language T xt." Artificial 
Intelligence 27(1): 1-41. 
Morris J. and Hirst G. (1991) "Lexical Cohesion 
Computed by Thesaural Relations as an 
Indicator of the Structure of Text." 
Computational Linguistics 17(1): 21-48. 
Moser M. and Moore J. D. (1995) "Investigating 
Cue Selection and Placement in Tutorial 
Discourse.'" In Proceedings of ACL'95, pp. 
130-135. 
Ono K., Sumita K. and S. Miike. (1994) "Abstract 
Generation based on Rhetorical Structure 
Extraction." In Proceedings of International 
Conference on Computational Linguistics, 
Japan, pp. 344-348. 
Paice C. D. (1990) "Constructing Literature 
Abstracts by Computer: Techniques and 
Prospects." Information Processing and" 
Management 26(1): 171-186. 
Quinlan J. Ross (1993)"C4.5 Programs for 
Machine Learning." San Mateo, CA: Morgan 
Kaufmann. 
Reichman R. (1978) "Conversational Coherence." 
Cognitive Science 2(4): 283-328. 
Salton G., Singhal A., Mitra M. and Buckley C. 
(1997) "Automatic Text Structuring and 
Summarization." Information Processing and 
Management 33(2): 193-207. 
Schiffrin D. (1987) Discourse Markers. 
' Cambridge: Cambridge University Press. 
Siegel E. V. and McKeown K. R. (1994) 
"Emergent Linguistic Rules from Inducing 
Decision Trees: Disambiguating Discourse Clue 
Words." In Proceedings of AAAI, pp. 820-826. 
T'sou B. K., Ho H. C., Lai B. Y., Lun C. and Lin 
H. L. (1992) "A Knowledge-based Machine- 
aided System for Chinese Text Abstraction." In 
Proceedings of International Conference on 
Computational Linguistics, France, pp, 1039- 
1042. 
T'sou B. K., Gao W. J., Lin H. L., Lai T. B. Y. 
and Ho H. C. (1999) "Tagging Discourse 
Markers: Towards a Corpus based Study of 
Discourse Marker Usage in Chinese Text" In 
Proceedings of the 18th International 
Conference on Computer Processing of 
Oriental Languages, March 1999, Japan, pp. 
391-396. 
T'sou B. K., Lin H. L., Ho H. C., Lai T. and Chan 
T. (1996) "Automated Chinese Full-text 
Abstraction Based on Rhetorical Structure 
Analysis." Computer Processing of Oriental 
Languages 10(2): 225-238. 
Wang W. X., Zhang X. C., Lu M. Y. and Cheng H. 
Y. (1994) "Xian Dai Han Yu Fu Ju Xian Jie (A 
New Analysis of Complex Sentences in Modern 
Standard Chinese)", Hua Dong S.hi Fan Da Xue 
Chu Ban She, 1994. 
20 
! 
I 
i 
i 
! 
I 
I 
I 
I 
I 
i 
I 
I 
i 
i 
I 
I 
I 
I 
Enhancement of a Chinese Discourse Marker Tagger with C4.5 
Benjamin K. T'sou l, Torn B. Y. Lai 2, Samuel W. K. Chan 3, Weijun Gao 4, Xuegang Zhan 5
23Languag e Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon 
Hong Kong SAR, China 
Northeastern U iversity, China 
{ ~rlbtsou, 2ettomlai} @uxmail.cityu.edu.hk, 3swkchan@cs.cityu.edu.hk, 
4wj gao@mail.neu.edu.cn, Szxg@ics.cs.neu.edu.cn 
Abstract 
Discourse markers are complex 
discontinuous linguistic expressions which 
are used to explicitly signal the discourse 
structure of a text. This paper describes 
efforts to improve an automatic tagging 
system which identifies and classifies 
discourse markers in Chinese texts by 
applying machine learning (ML) to the 
disambiguation f discourse markers, as an 
integral part of automatic text summarization 
via rhetorical structure. Encouraging results 
are reported. 
Keywords: discourse marker, Chinese 
corpus, rhetorical relation, automatic tagging, 
machine learning 
1 Introduction 
Discourse refers to any form of 
language-based communication involving 
multiple sentences or utterances. The most 
important forms of discourse of interest o 
Natural Language Processing (NLP) are text 
and dialogue. The function of discourse 
analysis is to divide a text into discourse 
segments, and to recognize and re-construct 
the discourse structure of the text as intended 
by its author. 
Automatic text abstraction has received 
considerable attention (Paice 1990). Various 
systems have been developed (Chan et al 
2000). Ono et al (1994), T'sou et al (1992) 
and Marcu (1997) focus on discourse 
structure in summarization using the 
Rhetorical Structure Theory (RST, Mann and 
Thompson 1986). The theory has been 
exploited in a number of computational 
systems (e.g. Hovy 1993). The main idea is 
to build a discourse tree where each node of 
the tree represents an RST relation. 
Summarization is achieved by trimming 
lmimportant sentences on the basis of the 
relative saliency or rhetorical relations. 
The SIFAS (Syntactic Marker based 
Full-Text Abstraction System) system has 
been implemented to use discourse markers 
in the automatic summarization of Chinese 
(T'sou et al 1999). In this paper, we report 
our efforts to improve the SIFAS tagging 
system by applying machine learning 
techniques to disambiguation of discourse 
markers. C4.5 (Quirdan, 1993) is used in our 
system. 
2 Manual Tagging Process 
To tag the discourse markers, the 
following coding scheme is designed to 
encode Real Discourse Markers (RDM) 
appearing in the SIFAS corpus (T'sou et al 
1998). We describe the z ~h discourse marker 
with a 7-tuple RDM; 
RDMi=< DM i, RRi, RPi, CTi, MNi, 
RNi, OT i >, where 
38 
DMi 
RP i : 
: 
MN~ : 
RNi : 
OT~ : 
the lexical item of the 
Discourse Marker, or the 
value'NULL'. 
the Rhetorical Relation in 
which DIVI~ is a constituent 
marker. 
the Relative Position of DMi. 
the Connection Type of RRi. 
the Discourse Marker 
Sequence Number. 
the Rhetorical Relation 
Sequence Number. 
the Order Type of RR~. The 
value of OTi can be 1, -1 or 0, 
denoting respectively the 
normal order, reverse order or 
irrelevance of the premise- 
consequence ordering of RR i . 
For apparent discourse markers that do 
not function as a real discourse marker in a 
text, a different coding scheme is used to 
encode them. We describe the i th apparent 
discourse marker using a 3-Tuple ADM~: 
ADMi =< LIi, *, SNi >, where 
LIi : the Lexical Item of the 
apparent discourse marker. 
SNi : the Sequence Number of the 
apparent discourse marker. 
In Chinese, discourse markers can be 
either words or phrases. To tag the SIFAS 
corpus, all discourse markers are organized 
into a discourse marker pair-rhetorical 
relation correspondence table. Part of the 
table is shown Table 1. 
To construct an automatic tagging 
system, let us first examine the sequential 
steps in the tagging process of a human 
tagger. 
S1. Written Chinese consists of rurming texts 
without word delimiters; the first step is 
is to segment the text into Chinese word 
sequences. 
$2. On the basis of a discourse marker list, 
we identify those words in the text 
which appear on the list as Candidate 
Discourse Markers (CDMs). 
$3. To winnow Real Discourse Markers 
(RDMs) and Apparent Discourse 
Markers (ADMs) from the CDMs, and 
encode the ADMs with a 3-tuple. 
$4. To encode the RDM with a 7-tuple 
according to a Discourse Marker Pair- 
Rhetorical Relation correspondence 
table. 
Relat- 
ion 
Adver- 
sativity 
Adver- 
sativity 
Causa- 
nty 
Causa- 
lity 
Front Back Con- 
nection 
Type 
Inter 
Intra 
Intra 1 
Intra -1 
Table 1 Discourse Marker Pair- 
Rhetorical Relation Table 
Order 
Type 
3 Automat ic  Tagg ing  Process  
The identification of candidate discourse 
markers is based on a discourse marker list, 
which now contains 306 discourse markers 
plus a NULL marker. The markers are 
extracted from newspaper editorials of Hong 
Kong, Mainland China, Taiwan and 
Singapore. These markers constitute 480 
distinct discontinuous pairs that correspond 
to 25 rhetorical relations. In actual usage, 
some discourse marker pairs designate 
multiple rhetorical relations according to 
context. Some pairs can represent both 
INTER-sentence and INTRA-sentence 
relations. Thus the correspondence b tween 
the discourse marker pairs and the rhetorical 
relations is not single-valued. Some 
discourse marker pairs correspond to more 
than one rhetorical relation or connection 
type. We have 504 correspondences between 
the discourse marker pairs and the rhetorical 
relations. 
39 
In practice, one discontinuous 
constituent member of a marker pair is often 
omitted. We use the NULL marker to 
indicate the omission. In the 504 
correspondences, 244 of them are double 
constituent marker pairs, 260 are single 
constituent markers (i.e. One of the markers 
is NULL). And in the 244 double constituent 
markers, only 3 are not single-valued 
correspondences (one of" which is an 
INTER/INTRA relation, and can easily be 
distinguished.). Thus the tagging of the 244 
double constituent markers is basically a 
table searching process. But for the 260 
single constituent markers, the identity of the 
NULL marker is often difficult o determine. 
The SIFAS tagging system works in two 
modes: automatic and interactive (semi- 
automatic). The automatic tagging procedure 
is as follows: 
1. Data preparation: Input data files are 
modified according to the required 
format. 
2. Word segmentation: Because there are 
no delimiters between Chinese words in 
a text, words have to be extracted 
through asegmentation process. 
3. CDM identification 
4. Full-Marker RDM recognition 
5. ADM identification (first pass, 
deterministic) 
6. CDM feature xtraction 
7. ADM identification (2nd pass, via ML) 
8. Tagging NuLL-marker CDM pairs (via 
ML) 
9.ADM and RDM sequencing, proof- 
reading, training data generation, and 
statistics 
The following principles are adopted by 
the tagging algorithm to resolve ambiguity in 
the process of matching discontinuous 
discourse markers: 
1. the principle of greediness: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the first matched relation from the left. 
2.the principle of locality: When 
matching apair of discourse markers for 
a rhetorical relation, priority is given to 
the relation where the distance between 
its constituent markers is shortest. 
3.the principle of explicitness: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the relation where both markers are 
explicitly presented. 
4. the principle of superiority: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the inter-sentence r lation whose back 
discourse marker matched with the first 
word of a sentence. 
5. the principle of Back-marker 
preference: This is applicable only to 
rhetorical relations where either the 
front or the back marker is absent, or to 
a NULL marker. In such cases, priority 
is given to the relation with the back 
marker present. 
Steps 1 to 6 and the five principles 
underlie the original naive tagger of the 
SIFAS system (T'sou et al 1998), which also 
contains the system framework. 
4 Improvement 
4.1 Problems 
Many Chinese discourse markers have 
both discourse senses and alternate sentential 
senses in different context. For a human 
tagger, steps $3 and $4 in section 2 are not 
difficult because he/she can identify an 
ADM/RDM based on his/her text 
comprehension. However, for an automatic 
process, it is quite difficult o distinguish an 
ADM from an RDM if no syntactic/semantic 
information is available. 
Another problem is the location of 
NULL-Marker described above. Our earlier 
statistics howed some characteristics in the 
distance measured by punctuation marks. 
Statistics from 80 tagged editorials how that 
most of the relations are INTRA-Sentence 
relations (about 93%), about 70% of the 
INTRA RDM pairs have NULL markers. 
Most of these RDM pairs are separated by 
ONE comma (62%). These statistics how 
40 
the importance of the problems of 
positioning the NULL markers. 
The naive tagger partially solved the 
CDM discrimination and NULL marker 
location problems. Our experiment shows 
that about 45% of the ADMs can be 
correctly identified, and about 60% of the 
NULL markers can be correctly located one 
comma/period away from the current RDM. 
This leaves much room for improvement. 
One solution is to add a few rules 
according to previous tatistics. The original 
naive tagger did not assume any knowledge 
of the statistics and behavioral patterns of 
discourse markers. From the error analysis, 
we extracted some additional rules to guide 
the classification and matching of the 
discourse markers. For example, one of the 
rules we extracted is: 
"A matching pair must be separated by 
at least two words or by punctuation 
marks". Using this rule, the following 
full marker matching error is avoided. 
< ~ ~ >< ~ ~ >< ~ x ~ >< 
./~ ,conjunction,Front, Intra,5,5,1>< ~ >< ~,  
conjunction, Back, Intra, 6,5,1><~>, <~t~><7~ 
?~x~><~><t$ i~>,  <~,* ,7xf f~>< 
X><~x<~><~x~><~x~_~ 
><~>0 
Another solution is to use 
? syntactic/semantic information through 
machine learning. 
4.2 C4.5 
Most empirical learning systems are 
given a set of pre-classified cases, each 
described by a vector of attribute values, and 
construct from them a mapping from 
attribute values to classes. C4.5 is one such 
system that learns decision-tree classifiers. It
uses a divide-and-conquer approach to 
growing decision trees. The current version 
of C4.5 is C5.0 for Unix and See5 for 
Windows. 
Let attributes be denoted A={a~, a2, ..., 
a,,J, cases be denoted D={d 1, d2, ..., d J ,  and 
classes be denoted C={c, c 2, ..., cJ. For a 
set of cases D, a test 1q is a split of D based 
on attribute at. It splits D into mutually 
exclusive subsets D~, D 2, ..., D r These 
subsets of cases are single-class collections 
of cases. 
If a test T is chosen, the decision tree 
for D consists of a node identifying the test 
T ,  and one branch for each possible subset 
D~. For each subset D~, a new test is then 
chosen for further split. If D~ satisfies a 
stopping criterion, the tree for Dr is a leaf 
associated with the most frequent class in D~. 
One reason for stopping is that cases in D~ 
belong to one class. 
C4.5 uses arg max(gain(D,1)) or arg 
max(gain ratio(D,T)) to choose tests for 
split: 
k 
Info(D) = -~p(c , ,D)  * log2(p(c,,D)) 
i=I 
Split(D,T) = _L ID ,  I .  log2(~-~) 
i=l IDI 
Gain(D,T) = Info(D)- "J"'~.~'. Di I.  Info(Di) 
i=l I DI 
Gain ratio(D, T) = gain(D, T) / Split(D, T) 
where, p(c~,D) denotes the proportion of 
cases in D that belong to the i th class. 
4.3 Application of C4.5 
Since using semantic information 
requires a comprehensive thesaurus, which is 
unavailable at present, we only use syntactic 
information through machine learning. 
The attributes used in the original 
SIFAS system include the candidate 
discourse marker itself, two words 
immediately to the left of the CDM, and two 
words immediately to the right of the CDM. 
The attribute names are F2, F1, CDM, B1, 
B2, respectively (T'sou et al 1999). SIFAS 
only uses the Part Of Speech attribute of the 
neighboring words. This reflects to some 
degree the syntactic characteristics of the 
CDM. 
To reflect the distance characteristics, 
we add two other attributes: the number of 
discourse delimiters (commas, semicolons 
for INTRA-sentence relation, periods and 
41 
exclamation marks for INTER-sentence 
relation) before and after the current CDM, 
denoted Fcom and Boom, respectively. For 
the location of the NULL marker, we still 
add an actual number of delirniters Acorn. 
The order of these attributes is: CDM, 
F1, F2, B1, B2, Fcom, Boom Acorn for Null 
marker location, and CDM, F1, F2, B1, B2, 
Fcom, Bcom, IsRDM for CDM classification, 
where IsRDM is a Boolean value. 
The following are two examples of 
cases: 
9~: _N. ,?,q,a,a,7,1,1 for NULL marker 
location 
N~,d,?,u,?,l ,0,F for CDM classificati 
on 
where "?" denotes that no corresponding 
word is at the position (beginning or end of 
sentence); a, d, q, and u are part-of-speech 
symbols in our segmentation dictionary, 
representing adjective, adverb, classifier, and 
auxiliary, respectively. 
The following are two examples of the 
rules generated by the C4.5. The first is a 
CDM classification rule, and the other is a 
NULL marker location rule. 
Rule 5: (11/1, lift 2.2) 
CDM = 
B1 =v 
Fcom > 0 
class T \[0.846\] 
which can be explained as: if the word after 
the CDM "~:" is a verb, and there is one 
comma in the sentence, before "~J:~:", then 
"~:" is an RDM. 
Rule 22: (1, lift 3.4) 
B2 = p 
Fcom > 1 
class 2 \[0.667\] 
which can be explained as: if the second 
word after the RDM is a preposition, and 
there is more then one commas before the 
current RDM, then the location of the NULL 
marker is two commas away from the RDM. 
4.4 Objects in the SIFAS system 
The objects in the new SIFAS tagging 
system are listed below. 
1. Dictionary Editor: for the update of 
word segmentation dictionary and the 
rhetorical relation table. 
2. Data Manager: for the modification of 
the input data (editorial texts) to 
conform with the required format. 
3. Word Segmenter: for the segmentation 
of the original texts, and the recognition 
of CDMs. 
4. RDM Tagger: The initial identification 
of RDMs is a table searching process. 
All those full-marker pairs are identified 
as rhetorical relations according to the 
principles described above. For those 
Null-marker pairs, the location of the 
Null maker is left to the rule interpreter. 
5. ADM Tagger: The identification of 
ADMs is also a table searching process, 
because, without other 
syntactic/semantic information, the only 
way to identify ADMs from the CDMs 
is to find out that the CDM cannot form 
a valid pair with any other CDMs 
(including the NULL marker) to 
correspond to a rhetorical relation. 
6. CDM Feature Extractor: For those 
untagged CDMs, the classification is 
carried out through C4.5. The Feature 
Extractor extracts yntactic information 
about he current CDM and send it to the 
Rule Interpreter (see below). 
7. Rule Interpreter: C4.5 takes feature data 
file as the input to construct a classifier, 
and the rules formed are stored in an 
output file. The rule interpreter eads 
this output file and applies the rules to 
classify the CDMs. In our system, The 
Rule Interpreter functions as a NULL 
Marker Locator and a CDM classifier. 
8. Sequencer: for the rearrangement of
RDM and ADM order number. In the 
rearranging process, the Sequencer also 
extracts statistical information for 
analysis. 
9. Interaction Recorder: for the recording 
of user interaction information for 
42 
statistics use. 
10. Data Retriever: for data retrieval and 
browsing. 
5 Evaluation 
In order to evaluate the effectiveness of 
the tagging system in terms of the percentage 
of discourse markers that can be tagged 
correctly, we have chosen 80 tagged 
editorials from Ming Pao, a Chinese 
newspaper of Hong Kong, in the duration 
from December 1995 to January 1996 to 
form a training data set. Then we randomly 
selected 20 editorials from Mainland China 
and Hong Kong newspapers for the system 
to tag automatically, and then manually 
checked the results. 
The total CDMs in the training data set 
is 4764, in which 2116 are RDMs and 2648 
are ADMs. The distribution of INTER- 
sentence r lations, INTRA-sentence r lations, 
and NULL marker pairs is shown below. 
Total 
Relations 
Inter- 
Sentence 
Relations 
Intra- 
Sentence 
Relations 
Relations 
with 
NULL 
marker 
pair 
1589 98 1491 1062 
100% 6.17% 93.83% 66.83% 
Table 2 Distribution of INTER-/INTRA- 
sentence relations, 
and NULL marker pairs 
Our evaluation is based on counting the 
number of discourse markers that are 
correctly and incorrectly tagged. 
The total CDMs in the test data set is 
1134, in which 563 are RDMs and 571 are 
ADMs. The distribution of INTER-sentence 
relations, INTRA-sentence relations, and 
NULL marker pairs in the test data set is 
shown in Table 3. 
Total 
Relations 
Inter- 
Sentence 
Relations 
Intra- 
Sentence 
Relations 
Relations 
with 
NULL 
marker 
pair 
424 23 401 285 
100% 5.42% 94.58% 67.22% 
Table 3 Distribution of INTER-/INTRA- 
sentence relations, and NULL marker 
pairs in testing data set 
451 399 11 1 65 3 
Table 4 Test Results 
From the test results shown in Table 4, 
we can see that most of the errors are caused 
by the misclassification f the CDMs. An 
example of Other errors is shown below. 
The following sentence is from an editorial 
of People's Daily. 
< ~ ~17 >< ~ ~.~ >< ~ ~ > , 
<NULL,sufficienc y, Front, Intra, O,81,1x -- ~ ~ff \[\] 
><~ ><~. ~lJ><~.~.>< \[\] ~>,  <~><~iA><-- 
+~ \[ \ ]><~><)~lJ>, <~><~iA><~>< 
~,*,80><~ \[\]><1~><--~52">, <~~>< 
~,sufticiency, Back,Intra,81,81,1 < ~ ~ x :~ x 
~.><~><~>,  <~ ~><~ :~ ><~,*,SZ><~ l~J, 
><~>? 
In the above sentence, the first "R"  is 
matched with the NULL marker, but the 
second "R" is left as an ADM. This causes 
an "Other error" and an "ADM/RDM 
classification error". 
The Gross Accuracy (GA) as defined in 
T'sou et al (1999) is: 
GA = correctly tagged discourse 
markers / total number of discourse markers 
= 95.38% 
This greatly improves the performance 
compared with the original GA = 68.89%. 
The overgeneration problem (tagged 415, 
actual 424) is caused by the mismatch of 
CDMs as RDM pairs, or by the 
43 
misclassification of CDMs as RDMs. 
Following are two examples. 
< ~\[I ~ ,sufficieney, Front, Intra,54,54,1x ~ ~1"\] >_< 
~\[1- ~><~,* ,56x~xf f  ~ >, <~A.x~x~ :t: 
>< ~ > , < ~1~. ,sufficieney, Back,Intra,57,54,1>_< 
,*,58><~ x~ ~><~><:~ ~x~ ~x 
><~I l~ . l><~x(~ ~ x~,* ,59><~ A.x~ ~ 
><:t:~><--~>? 
In this example, "~tl ~"  could have 
matched <:~,*.55>, < ~,*,56>, or<~,*,58>. 
Only the <:~,*,55> and the <~,*,58> can be 
eliminated from the candidates according to 
the "simple rules" mentioned in section 4.1. 
The system has to choose from <~,*,56> and 
<}J~,*,57> to match with "~zn~'. Luckily, 
the system has given a right choice here. 
< --  ~" ~ \[\] >< ~ ,conjunction,Front, Intra,46, 
46,1><~~><~><~r~> , <NULL, 
conjunction,Front, Intra,0,49,1 ><-- f" ~ \[\] ><)~ 
>< ~,~ ,conjunction,Back, Intra,47,46,1>< 
~i~,*,48>< ~ ~><1~ \]\]~><~E ~><~><~ >< 
~ \[\] A .><~><~ ~ ><th~> , < 
,eonjunction,Baek,Intra,49,49,1>< ~ ~tJ ~ 
><?x~ ~><\[ \ ]  ~><~n><~,*,50><l~# 
\[\]><~ I x~x\ [ \ ]  g,~><~ ~>< ~ ><Lib>. 
The two "~" are misclassified as RDMs, 
and causes a mismatch of RDM pair. Such 
errors are difficult to avoid for an automatic 
system. Without further syntactic/semantic 
analysis, we can only hope for the ML 
algorithm to give us a solution from more 
training data. 
6 Conc lus ion  
In order to study discourse markers for 
use in the automatic summarization of 
Chinese text, we have designed and 
implemented the SIFAS system. In this 
paper, we have focused on the problems of 
NULL marker location and the classification 
of RDMs and ADMs. A study on applying 
machine learning techniques to discourse 
marker disambiguation is conducted. C4.5 is 
used to generate decision tree classifiers. Our 
results indicate that machine learning is an 
effective approach to improving the accuracy 
of discourse marker tagging. For interactive 
use of the system, if we set a threshold for 
the rule precision and only display those low 
precision rules for interactive selection, we 
can greatly speed up the semi-automatic 
tagging process. 
7 References  
Chart S., Lai T., Gao W. J. and T'sou B. K. 
(2000) "Mining Discourse Markers for 
Chinese Textual Summarization." In 
Proceedings of the Sixth Applied Natural 
Language Processing Conference and the 
North American Chapter of the 
Association for Computational Linguistics. 
Workshop on Automatic Summarization, 
Seattle, Washington, 29 April to 3 May, 
2000. 
Grosz B.J. and Sidner C. (1986) "Attention, 
Intention, and the Structure of Discourse," 
Computational Linguistics 12(3): 175-204. 
Hirst G. (1981) "Discourse Oriented 
Anaphoral Resolution in Natural Language 
Understanding: A Review." Computational 
Linguistics 7(2): 85-98. 
Hovy E. (1993) "Automated Discourse 
Generation using Discourse Structure 
Relations." Artificial Intelligence 63: 341- 
385. 
Hwang C. H. and Schubert L. K. (1992) 
"Tense Trees as the 'Fine Structure' of 
Discourse." In Proc. 30th Annual Meeting, 
Assoc. for Computational Linguistics, pp. 
232-240. 
Lin H. L., T'sou B. K., H. C. Ho, Lai T., Lun 
C., C. K. Choi and C.Y. Kit. (1991) 
"Automatic Chinese Text Generation 
Based on Inference Trees." In Proe. of 
ROCLING Computational Linguistic 
Conference IV, Taipei, pp. 215-236. 
Litman D. J. and Allen J. (1990) "Discourse 
Processing and Commonsense Plans." In 
Cohen et al(ed.) Intentions in 
Communications, pp. 365-388. 
Mann W. C. and Thompson S. A (1988) 
"Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization." 
44 
? Text 8(3): 243-281. 
Marcu D. (1997) "From Discourse Structures 
to Text Summaries." In Proceedings of the 
ACL/EACL'97 Workshop on Intelligent 
Scalable Text Summarization, Spain, pp. 
82-88. 
McKeown K. and Radev D. (1995) 
"Summaries of Multiple News Articles." 
In Proceedings of the 18th Annual 
International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, Seattle, pp. 74-82. 
Ono K., Surnita K. and S. Miike. (1994) 
"Abstract Generation based on Rhetorical 
Structure Extraction." In Proceedings of 
International Conference on 
Computational Linguistics, Japan, pp. 344- 
348. 
Paice C. D. (1990) "Constructing Literature 
Abstracts by Computer: Techniques and 
Prospects." Information Processing and 
Management 26(1): 171-186. 
Qulnlan J. Ross (1993) "C4.5 Programs for 
Machine Learning." San Mateo, CA: 
Morgan Kaufmann. 
T'sou B. K., Ho H. C., Lai B. ?., Lun C. and 
Lin H. L. (1992) "A Knowledge-based 
Machine-aided System for Chinese Text 
Abstraction." In Proceedings of 
International Conference on 
Computational Linguistics, France, pp. 
1039-1042. 
T'sou B. K., Gao W. J., Lin H. L., Lai T. B. 
Y. and Ho H. C. (1999) "Tagging 
Discourse Markers: Towards a Corpus 
based Study of Discourse Marker Usage in 
Chinese Text" In Proceedings of the 18th 
International Conference on Computer 
Processing of Oriental Languages, March 
1999, Japan, pp. 391-396. 
T'sou B. K., Lin H. L., Ho H. C., Lai T. and 
Chan T. (1996) "Automated Chinese Full- 
text Abstraction Based on Rhetorical 
Structure Analysis." Computer Processing 
of Oriental Languages 10(2): 225-238. 
Tsou, B.K., et al, 1998: ~1~,  ~ ,  
i ~ ~ 1 - ~ 3 - ~ ~  ~ ~ " ,  
ICCIP'98, Beijing, Nov. 18-20, 1998. 
45 
 	
 Some Considerations on Guidelines for  
Bilingual Alignment and Terminology Extraction 
 
Lawrence Cheung, Tom Lai, Robert Luk?, Oi Yee Kwong, King Kui Sin, Benjamin K. Tsou 
 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong  
{rlylc, cttomlai, rlolivia, ctsinkk, rlbtsou}@cityu.edu.hk  
?Department of Computing 
Hong Kong Polytechnic University  
Hung Hom, Kowloon, Hong Kong 
csrluk@comp.polyu.edu.hk 
 
Abstract  
Despite progress in the development of 
computational means, human input is still 
critical in the production of consistent and 
useable aligned corpora and term banks. This 
is especially true for specialized corpora and 
term banks whose end-users are often 
professionals with very stringent 
requirements for accuracy, consistency and 
coverage. In the compilation of a high quality 
Chinese-English legal glossary for ELDoS 
project, we have identified a number of issues 
that make the role human input critical for 
term alignment and extraction. They include 
the identification of low frequency terms, 
paraphrastic expressions, discontinuous units, 
and maintaining consistent term granularity, 
etc. Although manual intervention can more 
satisfactorily address these issues, steps must 
also be taken to address intra- and 
inter-annotator inconsistency.  
 
Keyword: legal terminology, bilingual 
terminology, bilingual alignment, 
corpus-based linguistics 
1. Introduction 
Multilingual terminology is an important 
language resource for a range of natural language 
processing tasks such as machine translation and 
cross-lingual information retrieval. The 
compilation of multilingual terminology is often 
time-consuming and involves much manual 
labour to be of practical use. Aligning texts of 
typologically different languages such as Chinese 
and English is even more challenging because of 
the significant differences in lexicon, syntax, 
semantics and styles. The discussion in the paper 
is based on issues arising from the extraction of 
bilingual legal terms from aligned 
Chinese-English legal corpus in the 
implementation of a bilingual a text retrieval 
system for the Judiciary of the Hong Kong Special 
Administrative Region (HKSAR) Government.  
 Much attention in computational 
terminology has been directed to the development 
of algorithms for extraction from parallel texts. 
For example, Chinese-English (Wu and Xia 1995), 
Swedish-English-Polish (Borin 2000), and 
Chinese-Korean (Huang and Choi 2000). Despite 
considerable progress, bilingual terminology so 
generated is often not ready for immediate and 
practical use. Machine extraction is often the first 
step of terminology extraction and must be used in 
conjunction with rigorous and well-managed 
manual efforts which are critical for the 
production of consistent and useable multilingual 
terminology. However, there has been relatively 
little discussion on the significance of human 
intervention. The process is far from being 
straightforward because of the different purposes 
of alignment, the requirements of target users and 
the corpus type. Indeed, there remain many 
problematical issues that will not be easy to be 
resolved satisfactorily by computational means in 
the near future, especially when typologically 
different languages are involved, and must require 
considerable manual intervention. Unfortunately, 
such critical manual input has often been treated as 
an obscure process. As with other human cognitive 
process (T?sou et al 1998), manual terminology 
markup is not a straightforward task and many 
issues deserve closer investigation. 
 In this paper, we will present some 
significant issues for Chinese-English alignment 
 and term extraction for the construction of a 
bilingual legal glossary. Section 2 describes the 
background of the associated bilingual alignment 
project. Section 3 discusses the necessity of 
manual input in bilingual alignment, and some 
principles adopted in the project to address these 
issues. Section 4 provides an outline for further 
works to improve terminology management, 
followed by a conclusion in Section 5. 
2. High Quality Terminology Alignment 
and Extraction 
2.1 Bilingual Legal Terminology in Hong 
Kong 
The implementation of a bilingual legal system in 
Hong Kong as a result of the return of 
sovereignty to China in 1997 has given rise to a 
need for the creation and standardization of 
Chinese legal terminology of the Common Law 
on par with the English one. The standardization 
of legal terminology will not only facilitate the 
mandated wider use of Chinese among legal 
professionals in various legal practices such as 
trials and production of legal documentation 
involving bilingual laws and judgments, but also 
promote greater consistency of semantic 
reference of terminology to minimize ambiguity 
and to avoid confusion of interpretation in legal 
argumentation.  
 In the early 90?s, Hong Kong law drafters 
and legal translation experts undertook the 
unprecedented task of translating Hong Kong 
Laws, which are based on the Common Law 
system, from English into Chinese. In the 
process, many new Chinese legal terms for the 
Common Law were introduced. On this basis, an 
English-Chinese Glossary of legal terms and a 
Chinese-English Glossary were published in 1995 
and 1999 respectively. The legal terminology was 
vetted by the high level Bilingual Laws Advisory 
Committee (BLAC) of Hong Kong. The 
glossaries which contain about 30,000 basic 
entries have become an important reference for 
Chinese legal terms in Hong Kong. The Bilingual 
Legal Information System (BLIS) developed by 
the Department of Justice, HKSAR provides 
simple keyword search for the glossaries and 
laws that are available in both Chinese and 
English. Nevertheless, the glossaries are far from 
being adequate for many different types of legal 
documentation, e.g. contracts, court judgments, 
etc. One major limitation of the BLIS glossary is 
its restricted coverage of legal terminology in the 
Laws of Hong Kong, within a basically 
prescriptive context as when the laws were studied 
at the time of its promulgation. There are other 
important bilingual references (Li and Poon 1998, 
Yiu and Au-Yeung 1992, Yiu and Cheung 1996) 
which focus more on the translation of Common 
Law concepts. These are almost exclusively 
nominal expressions. 
 In 2000, the City University of Hong 
Kong, in cooperation with the Judiciary, HKSAR, 
initiated a research project to develop a bilingual 
text retrieval system, Electronic Legal 
Documentation/Corpus System (ELDoS), which is 
supported by a bilingually aligned corpus of 
judgments. The purpose of the on-going project is 
twofold. First, the aligned legal corpus enables the 
retrieval of legal terms used in authentic contexts 
where the essence and spirit of the laws are tested 
(and contested) in reality, explicated and 
elaborated on, as an integral part of the evolving 
and defining body of important precedent cases 
unique to the Common Law tradition. Second, the 
corpus covers judgment texts involving 
interpretation of different language styles and 
vocabulary from Hong Kong laws. The alignment 
markup also serves as the basis for the compilation 
of a high-quality bilingual legal term bank. To 
complete the task within the tight timeframe, a 
team of annotators highly trained in law and 
language are involved in alignment markup and 
related editing. 
2.2 Need for Human Input 
The legal professionals which are the target users 
of ELDoS have very stringent demands on 
terminology in terms of accuracy, coverage and 
consistency. Aligned texts and extracted terms 
must therefore be carefully and thoroughly 
verified manually to minimize errors. 
Furthermore, many studies on terminology 
alignment and extraction deal predominantly with 
nominal expressions. Since the project aims to 
provide comprehensive information on the 
manifestations of legal vocabulary in Chinese and 
English texts, the retrieval system should not 
restrict users to nominal expressions but should 
also provide reference to many other phenomena 
such as alternation of part-of-speech (POS) (e.g. 
noun-verb alternation) inherent in bilingual texts, 
as will be seen in Section 3.  
 The availability of bilingual corpora has 
made it possible to construct representative term 
 banks. Nonetheless, current alignment and term 
extraction technology are still considered 
insufficient to meet the requirements for high 
quality terminology extraction. In ELDoS project, 
many issues are difficult to be handled 
satisfactorily by the computer in the foreseeable 
future. Although human input is essential for high 
quality term bank construction, the practice of 
manual intervention is not straightforward. 
Indeed, the manual efforts to correct the errors 
can be substantial, and the associated cost should 
not be underestimated. The annotator must first 
go through the entire texts to spot the errors and 
terms left out by the machines. In this process, 
both the source and target materials have to be 
consulted. The annotator must also ensure the 
consistency of the output. As a result, guidelines 
should be set up to streamline the process. 
3. Aspects of Terminology Alignment 
The approach adopted for the manual annotation 
of alignment markup and the maintenance of term 
bank in the ELDoS project will be described. 
Additional caution has been taken in the 
coordination of a team of annotators.  
3.1 Term Frequency 
An important reason for manual intervention in 
bilingual term alignment is the relatively poor 
recall rate for low frequency terms. Many 
extraction algorithms make use of statistical 
techniques to identify multi-word strings that 
frequently co-occur (Wu and Xia 1995; Kwong 
and Tsou 2001). These methods are less effective 
for locating low frequency terms. Of the 16,000 
terms extracted from ELDoS bilingual corpora, 
about 62% occur only once in about 80 
judgments. For high quality alignment and 
extraction, failure to include these low frequency 
terms would be totally unacceptable.  
3.2 Correspondence of Aligned Units 
Because of the different grammatical requirement 
and language style, a term in the source language 
often differs in different ways from the 
corresponding manifestations in the target 
language. These differences could be alternation 
of POS and the use of paraphrastic expressions. 
Although many term banks avoid such variations 
and focus primarily on equivalent nominals or 
verbs, the correspondence of terms between two 
typologically different languages is often more 
complicated. For example, the English nominal 
(?fulfilment?) is more naturally translated into 
Chinese as a verb (?l??, ?????, ????). 
More examples can be found in Table 1. 
 
Alternation of POS  
English  Chinese POS alternation 
The accused 
o+ 
det + adj ~ noun 
hold 
*? 
verb ~ noun 
fulfillment  
?
 
noun ~ verb 
administration  
?D 
noun ~ verb 
repudiation  
l? 
noun ~ neg + verb 
Table 1. Alternation of POS  
 
In some cases, there are simply no equivalent 
words in the target language. Paraphrasing or 
circumlocution may be necessary. Such 
correspondence is far less consistent and obvious 
to be identified by the computer.  
 
Paraphrasing/Circumlocution 
English Chinese 
The judge entered judgment in 
favour of the respondents in 
respect of their claim for arrears 
of wages, and severance payment. 
t?o31
?2??KDI
??9? 
In our view,? z??a?
 
?evidenced by the Defendant's 
letter ? 
?7:+3?}
???1$Ym
??S??? 
Table 2.  Examples of paraphrasing 
 
Because of language differences, legal terms can 
be contextually realized as anaphors in the target 
language. Examples of such correspondence 
would be useful for legal drafting and translation. 
Again, such anaphoric relations are more 
accurately handled by humans. 
 
Anaphoric Relation 
English Chinese 
He was subsequently 
charged?  
9?3??s? 
Liu JA dealt with that 
application on 14 March 
1996 and dismissed it. 
B9?t?W?
?1996?3?14?A?
 ?-??9?? 
Enforcement of a 
Convention award may 
also be refused if the 
award is in respect of a 
matter which is not capable 
of settlement by arbitration.
???*????1
??"lh?X*
?????M???
N+ 	
	

