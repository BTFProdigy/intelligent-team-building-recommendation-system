Proceedings of the 12th Conference of the European Chapter of the ACL, pages 211?219,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Inference Rules and their Application to Recognizing Textual Entailment
Georgiana Dinu
Saarland University
Campus, D-66123 Saarbru?cken
dinu@coli.uni-sb.de
Rui Wang
Saarland University
Campus, D-66123 Saarbru?cken
rwang@coli.uni-sb.de
Abstract
In this paper, we explore ways of improv-
ing an inference rule collection and its ap-
plication to the task of recognizing textual
entailment. For this purpose, we start with
an automatically acquired collection and
we propose methods to refine it and ob-
tain more rules using a hand-crafted lex-
ical resource. Following this, we derive
a dependency-based structure representa-
tion from texts, which aims to provide a
proper base for the inference rule appli-
cation. The evaluation of our approach
on the recognizing textual entailment data
shows promising results on precision and
the error analysis suggests possible im-
provements.
1 Introduction
Textual inference plays an important role in many
natural language processing (NLP) tasks. In recent
years, the recognizing textual entailment (RTE)
(Dagan et al, 2006) challenge, which focuses on
detecting semantic inference, has attracted a lot of
attention. Given a text T (several sentences) and a
hypothesis H (one sentence), the goal is to detect
if H can be inferred from T.
Studies such as (Clark et al, 2007) attest that
lexical substitution (e.g. synonyms, antonyms) or
simple syntactic variation account for the entail-
ment only in a small number of pairs. Thus, one
essential issue is to identify more complex expres-
sions which, in appropriate contexts, convey the
same (or similar) meaning. However, more gener-
ally, we are also interested in pairs of expressions
in which only a uni-directional inference relation
holds1.
1We will use the term inference rule to stand for such con-
cept; the two expressions can be actual paraphrases if the re-
lation is bi-directional
A typical example is the following RTE pair in
which accelerate to in H is used as an alternative
formulation for reach speed of in T.
T: The high-speed train, scheduled for a trial run on Tues-
day, is able to reach a maximum speed of up to 430 kilome-
ters per hour, or 119 meters per second.
H: The train accelerates to 430 kilometers per hour.
One way to deal with textual inference is
through rule representation, for example X wrote
Y ? X is author of Y. However, manually building
collections of inference rules is time-consuming
and it is unlikely that humans can exhaustively
enumerate all the rules encoding the knowledge
needed in reasoning with natural language. In-
stead, an alternative is to acquire these rules au-
tomatically from large corpora. Given such a rule
collection, the next step to focus on is how to suc-
cessfully use it in NLP applications. This paper
tackles both aspects, acquiring inference rules and
using them for the task of recognizing textual en-
tailment.
For the first aspect, we extend and refine an ex-
isting collection of inference rules acquired based
on the Distributional Hypothesis (DH). One of the
main advantages of using the DH is that the only
input needed is a large corpus of (parsed) text2.
For the extension and refinement, a hand-crafted
lexical resource is used for augmenting the origi-
nal inference rule collection and exclude some of
the incorrect rules.
For the second aspect, we focus on applying
these rules to the RTE task. In particular, we use
a structure representation derived from the depen-
dency parse trees of T and H, which aims to cap-
ture the essential information they convey.
The rest of the paper is organized as follows:
Section 2 introduces the inference rule collection
2Another line of work on acquiring paraphrases uses com-
parable corpora, for instance (Barzilay and McKeown, 2001),
(Pang et al, 2003)
211
we use, based on the Discovery of Inference Rules
from Text (henceforth DIRT) algorithm and dis-
cusses previous work on applying it to the RTE
task. Section 3 focuses on the rule collection it-
self and on the methods in which we use an exter-
nal lexical resource to extend and refine it. Sec-
tion 4 discusses the application of the rules for the
RTE data, describing the structure representation
we use to identify the appropriate context for the
rule application. The experimental results will be
presented in Section 5, followed by an error analy-
sis and discussions in Section 6. Finally Section 7
will conclude the paper and point out future work
directions.
2 Background
A number of automatically acquired inference
rule/paraphrase collections are available, such as
(Szpektor et al, 2004), (Sekine, 2005). In our
work we use the DIRT collection because it is the
largest one available and it has a relatively good
accuracy (in the 50% range for top generated para-
phrases, (Szpektor et al, 2007)). In this section,
we describe the DIRT algorithm for acquiring in-
ference rules. Following that, we will overview
the RTE systems which take DIRT as an external
knowledge resource.
2.1 Discovery of Inference Rules from Text
The DIRT algorithm has been introduced by (Lin
and Pantel, 2001) and it is based on what is called
the Extended Distributional Hypothesis. The orig-
inal DH states that words occurring in similar
contexts have similar meaning, whereas the ex-
tended version hypothesizes that phrases occur-
ring in similar contexts are similar.
An inference rule in DIRT is a pair of binary
relations ? pattern1(X,Y ), pattern2(X,Y ) ?
which stand in an inference relation. pattern1 and
pattern2 are chains in dependency trees3 while X
and Y are placeholders for nouns at the end of this
chain. The two patterns will constitute a candi-
date paraphrase if the sets of X and Y values ex-
hibit relevant overlap. In the following example,
the two patterns are prevent and provide protection
against.
X
subj
???? prevent
obj
??? Y
X
subj
???? provide
obj
??? protection
mod
???? against
pcomp
????? Y
3obtained with the Minipar parser (Lin, 1998)
X put emphasis on Y
? X pay attention to Y
? X attach importance to Y
? X increase spending on Y
? X place emphasis on Y
? Y priority of X
? X focus on Y
Table 1: Example of DIRT algorithm output. Most
confident paraphrases of X put emphasis on Y
Such rules can be informally defined (Szpek-
tor et al, 2007) as directional relations between
two text patterns with variables. The left-hand-
side pattern is assumed to entail the right-hand-
side pattern in certain contexts, under the same
variable instantiation. The definition relaxes the
intuition of inference, as we only require the en-
tailment to hold in some and not all contexts, mo-
tivated by the fact that such inferences occur often
in natural text.
The algorithm does not extract directional in-
ference rules, it can only identify candidate para-
phrases; many of the rules are however uni-
directional. Besides syntactic rewriting or lexi-
cal rules, rules in which the patterns are rather
complex phrases are also extracted. Some of the
rules encode lexical relations which can also be
found in resources such as WordNet while oth-
ers are lexical-syntactic variations that are unlikely
to occur in hand-crafted resources (Lin and Pan-
tel, 2001). Table 1 gives a few examples of rules
present in DIRT4.
Current work on inference rules focuses on
making such resources more precise. (Basili et
al., 2007) and (Szpektor et al, 2008) propose at-
taching selectional preferences to inference rules.
These are semantic classes which correspond to
the anchor values of an inference rule and have
the role of making precise the context in which the
rule can be applied 5. This aspect is very impor-
tant and we plan to address it in our future work.
However in this paper we investigate the first and
more basic issue: how to successfully use rules in
their current form.
4For simplification, in the rest of the paper we will omit
giving the dependency relations in a pattern.
5For example X won Y entails X played Y only when Y
refers to some sort of competition, but not if Y refers to a
musical instrument.
212
2.2 Related Work
Intuitively such inference rules should be effective
for recognizing textual entailment. However, only
a small number of systems have used DIRT as a re-
source in the RTE-3 challenge, and the experimen-
tal results have not fully shown it has an important
contribution.
In (Clark et al, 2007)?s approach, semantic
parsing to clause representation is performed and
true entailment is decided only if every clause
in the semantic representation of T semantically
matches some clause in H. The only variation al-
lowed consists of rewritings derived from Word-
Net and DIRT. Given the preliminary stage of this
system, the overall results show very low improve-
ment over a random classification baseline.
(Bar-Haim et al, 2007) implement a proof
system using rules for generic linguistic struc-
tures, lexical-based rules, and lexical-syntactic
rules (these obtained with a DIRT-like algorithm
on the first CD of the Reuters RCV1 corpus). The
entailment considers not only the strict notion of
proof but also an approximate one. Given premise
p and hypothesis h, the lexical-syntactic compo-
nent marks all lexical noun alignments. For ev-
ery pair of alignment, the paths between the two
nouns are extracted, and the DIRT algorithm is
applied to obtain a similarity score. If the score
is above a threshold the rule is applied. However
these lexical-syntactic rules are only used in about
3% of the attempted proofs and in most cases there
is no lexical variation.
(Iftene and Balahur-Dobrescu, 2007) use DIRT
in a more relaxed manner. A DIRT rule is em-
ployed in the system if at least one of the anchors
match in T and H, i.e. they use them as unary
rules. However, the detailed analysis of the sys-
tem that they provide shows that the DIRT com-
ponent is the least relevant one (adding 0.4% of
precision).
In (Marsi et al, 2007), the focus is on the use-
fulness of DIRT. In their system a paraphrase sub-
stitution step is added on top of a system based on
a tree alignment algorithm. The basic paraphrase
substitution method follows three steps. Initially,
the two patterns of a rule are matched in T and
H (instantiations of the anchors X , Y do not have
to match). The text tree is transformed by apply-
ing the paraphrase substitution. Following this,
the transformed text tree and hypothesis trees are
aligned. The coverage (proportion of aligned con-
X write Y ?X author Y
X, founded in Y ?X, opened in Y
X launch Y ? X produce Y
X represent Z ? X work for Y
death relieved X? X died
X faces menace from Y ? X endangered by Y
X, peace agreement for Y
? X is formulated to end war in Y
Table 2: Example of inference rules needed in
RTE
tent words) is computed and if above some thresh-
old, entailment is true. The paraphrase compo-
nent adds 1.0% to development set results and only
0.5% to test sets, but a more detailed analysis on
the results of the interaction with the other system
components is not given.
3 Extending and refining DIRT
Based on observations of using the inference rule
collection on the real data, we discover that 1)
some of the needed rules still lack even in a very
large collection such as DIRT and 2) some system-
atic errors in the collection can be excluded. On
both aspects, we use WordNet as additional lexi-
cal resource.
Missing Rules
A closer look into the RTE data reveals that
DIRT lacks many of the rules that entailment pairs
require.
Table 2 lists a selection of such rules. The
first rows contain rules which are structurally very
simple. These, however, are missing from DIRT
and most of them also from other hand-crafted re-
sources such as WordNet (i.e. there is no short
path connecting the two verbs). This is to be ex-
pected as they are rules which hold in specific con-
texts, but difficult to be captured by a sense dis-
tinction of the lexical items involved.
The more complex rules are even more difficult
to capture with a DIRT-like algorithm. Some of
these do not occur frequently enough even in large
amounts of text to permit acquiring them via the
DH.
Combining WordNet and DIRT
In order to address the issue of missing rules,
we investigate the effects of combining DIRT with
an exact hand-coded lexical resource in order to
create new rules.
For this we extended the DIRT rules by adding
213
X face threat of Y
? X at risk of Y
face
? confront, front, look, face up
threat
? menace, terror, scourge
risk
? danger, hazard, jeopardy,
endangerment, peril
Table 3: Lexical variations creating new rules
based on DIRT rule X face threat of Y ? X at risk
of Y
rules in which any of the lexical items involved
in the patterns can be replaced by WordNet syn-
onyms. In the example above, we consider the
DIRT rule X face threat of Y ? X, at risk of Y
(Table 3).
Of course at this moment due to the lack of
sense disambiguation, our method introduces lots
of rules that are not correct. As one can see, ex-
pressions such as front scourge do not make any
sense, therefore any rules containing this will be
incorrect. However some of the new rules created
in this example, such as X face threat of Y ? X,
at danger of Y are reasonable ones and the rules
which are incorrect often contain patterns that are
very unlikely to occur in natural text.
The idea behind this is that a combination of
various lexical resources is needed in order to
cover the vast variety of phrases which humans
can judge to be in an inference relation.
The method just described allows us to identify
the first four rules listed in Table 2. We also ac-
quire the rule X face menace of Y ? X endangered
by Y (via X face threat of Y ? X threatened by Y,
menace ? threat, threaten ? endanger).
Our extension is application-oriented therefore
it is not intended to be evaluated as an independent
rule collection, but in an application scenario such
as RTE (Section 6).
In our experiments we also made a step towards
removing the most systematic errors present in
DIRT. DH algorithms have the main disadvantage
that not only phrases with the same meaning are
extracted but also phrases with opposite meaning.
In order to overcome this problem and since
such errors are relatively easy to detect, we ap-
plied a filter to the DIRT rules. This eliminates
inference rules which contain WordNet antonyms.
For such a rule to be eliminated the two patterns
have to be identical (with respect to edge labels
and content words) except from the antonymous
words; an example of a rule eliminated this way is
X have confidence in Y ? X lack confidence in Y.
As pointed out by (Szpektor et al, 2007) a thor-
ough evaluation of a rule collection is not a trivial
task; however due to our methodology we can as-
sume that the percentage of rules eliminated this
way that are indeed contradictions gets close to
100%.
4 Applying DIRT on RTE
In this section we point out two issues that are en-
countered when applying inference rules for tex-
tual entailment. The first issue is concerned with
correctly identifying the pairs in which the knowl-
edge encoded in these rules is needed. Follow-
ing this, another non-trivial task is to determine
the way this knowledge interacts with the rest of
information conveyed in an entailment pair. In or-
der to further investigate these issues, we apply the
rule collection on a dependency-based representa-
tion of text and hypothesis, namely Tree Skeleton.
4.1 Observations
A straightforward experiment can reveal the num-
ber of pairs in the RTE data which contain rules
present in DIRT. For all the experiments in this pa-
per, we use the DIRT collection provided by (Lin
and Pantel, 2001), derived from the DIRT algo-
rithm applied on 1GB of news text. The results
we report here use only the most confident rules
amounting to more than 4 million rules (top 40 fol-
lowing (Lin and Pantel, 2001)).6
Following the definition of an entail-
ment rule, we identify RTE pairs in which
pattern1(w1, w2) and pattern2(w1, w2) are
matched one in T and the other one in H and
?pattern1(X,Y ), pattern2(X,Y )? is an infer-
ence rule. The pair bellow is an example of this.
T: The sale was made to pay Yukos US$ 27.5 billion tax
bill, Yuganskneftegaz was originally sold for US$ 9.4 bil-
lion to a little known company Baikalfinansgroup which was
later bought by the Russian state-owned oil company Ros-
neft.
H: Baikalfinansgroup was sold to Rosneft.
6Another set of experiments showed that for this particu-
lar task, using the entire collection instead of a subset gave
similar results.
214
On average, only 2% of the pairs in the RTE
data is subject to the application of such inference
rules. Out of these, approximately 50% are lexical
rules (one verb entailing the other). Out of these
lexical rules, around 50% are present in WordNet
in a synonym, hypernym or sister relation. At a
manual analysis, close to 80% of these are correct
rules; this is higher than the estimated accuracy of
DIRT, probably due to the bias of the data which
consists of pairs which are entailment candidates.
However, given the small number of inference
rules identified this way, we performed another
analysis. This aims at determining an upper
bound of the number of pairs featuring entailment
phrases present in a collection. Given DIRT and
the RTE data, we compute in how many pairs
the two patterns of a paraphrase can be matched
irrespective of their anchor values. An example is
the following pair,
T: Libya?s case against Britain and the US concerns the
dispute over their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie in 1988.
H: One case involved the extradition of Libyan suspects
in the Pan Am Lockerbie bombing.
This is a case in which the rule is correct and
the entailment is positive. In order to determine
this, a system will have to know that Libya?s case
against Britain and the US in T entails one case
in H. Similarly, in this context, the dispute over
their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie in
1988 in T can be replaced with the extradition of
Libyan suspects in the Pan Am Lockerbie bombing
preserving the meaning.
Altogether in around 20% of the pairs, patterns
of a rule can be found this way, many times with
more than one rule found in a pair. However, in
many of these pairs, finding the patterns of an in-
ference rule does not imply that the rule is truly
present in that pair.
Considering a system is capable of correctly
identifying the cases in which an inference rule
is needed, subsequent issues arise from the way
these fragments of text interact with the surround-
ing context. Assuming we have a correct rule
present in an entailment pair, the cases in which
the pair is still not a positive case of entailment
can be summarized as follows:
? The entailment rule is present in parts of the
text which are not relevant to the entailment
value of the pair.
? The rule is relevant, however the sentences
in which the patterns are embedded block the
entailment (e.g. through negative markers,
modifiers, embedding verbs not preserving
entailment)7
? The rule is correct in a limited number of con-
texts, but the current context is not the correct
one.
To sum up, making use of the knowledge en-
coded with such rules is not a trivial task. If rules
are used strictly in concordance with their defini-
tion, their utility is limited to a very small number
of entailment pairs. For this reason, 1) instead of
forcing the anchor values to be identical as most
previous work, we allow more flexible rule match-
ing (similar to (Marsi et al, 2007)) and 2) fur-
thermore, we control the rule application process
using a text representation based on dependency
structure.
4.2 Tree Skeleton
The Tree Skeleton (TS) structure was proposed by
(Wang and Neumann, 2007), and can be viewed
as an extended version of the predicate-argument
structure. Since it contains not only the predi-
cate and its arguments, but also the dependency
paths in-between, it captures the essential part of
the sentence.
Following their algorithm, we first preprocess
the data using a dependency parser8 and then
select overlapping topic words (i.e. nouns) in T
and H. By doing so, we use fuzzy match at the
substring level instead of full match. Starting
with these nouns, we traverse the dependency
tree to identify the lowest common ancestor node
(named as root node). This sub-tree without the
inner yield is defined as a Tree Skeleton. Figure
1 shows the TS of T of the following positive
example,
T For their discovery of ulcer-causing bacteria, Aus-
tralian doctors Robin Warren and Barry Marshall have re-
ceived the 2005 Nobel Prize in Physiology or Medicine.
H Robin Warren was awarded a Nobel Prize.
Notice that, in order to match the inference rules
with two anchors, the number of the dependency
7See (Nairn et al, 2006) for a detailed analysis of these
aspects.
8Here we also use Minipar for the reason of consistence
215
Figure 1: Dependency structure of text. Tree
skeleton in bold
paths contained in a TS should also be two. In
practice, among all the 800 T-H pairs of the RTE-
2 test set, we successfully extracted tree skeletons
in 296 text pairs, i.e., 37% of the test data is cov-
ered by this step and results on other data sets are
similar.
Applying DIRT on a TS
Dependency representations like the tree skele-
ton have been explored by many researchers, e.g.
(Zanzotto and Moschitti, 2006) have utilized a tree
kernel method to calculate the similarity between
T and H, and (Wang and Neumann, 2007) chose
subsequence kernel to reduce the computational
complexity. However, the focus of this paper is to
evaluate the application of inference rules on RTE,
instead of exploring methods of tackling the task
itself. Therefore, we performed a straightforward
matching algorithm to apply the inference rules
on top of the tree skeleton structure. Given tree
skeletons of T and H, we check if the two left de-
pendency paths, the two right ones or the two root
nodes contain the patterns of a rule.
In the example above, the rule X obj???
receive
subj
???? Y ? X
obj2
???? award
obj1
???? Y satisfies
this criterion, as it is matched at the root nodes.
Notice that the rule is correct only in restricted
contexts, in which the object of receive is some-
thing which is conferred on the basis of merit.
However in this pair, the context is indeed the cor-
rect one.
5 Experiments
Our experiments consist in predicting positive en-
tailment in a very straightforward rule-based man-
ner (Table 4 summarizes the results using three
different rule collections). For each collection we
select the RTE pairs in which we find a tree skele-
ton and match an inference rule. The first number
in our table entries represents how many of such
pairs we have identified, out the 1600 of devel-
opment and test pairs. For these pairs we simply
predict positive entailment and the second entry
represents what percentage of these pairs are in-
deed positive entailment. Our work does not fo-
cus on building a complete RTE system; however,
we also combine our method with a bag of words
baseline to see the effects on the whole data set.
5.1 Results on a subset of the data
In the first two columns (DirtTS and Dirt+WNTS)
we consider DIRT in its original state and DIRT
with rules generated with WordNet as described
in Section 3; all precisions are higher than 67%9.
After adding WordNet, approximately in twice as
many pairs, tree skeletons and rules are matched,
while the precision is not harmed. This may in-
dicate that our method of adding rules does not
decrease precision of an RTE system.
In the third column we report the results of us-
ing a set of rules containing only the trivial iden-
tity ones (IdTS). For our current system, this can
be seen as a precision upper bound for all the
other collections, in concordance with the fact that
identical rules are nothing but inference rules of
highest possible confidence. The fourth column
(Dirt+Id+WNTS) contains what can be consid-
ered our best setting. In this setting considerably
more pairs are covered using a collection contain-
ing DIRT and identity rules with WordNet exten-
sion.
Although the precision results with this setting
are encouraging (65% for RTE2 data and 72% for
RTE3 data), the coverage is still low, 8% for RTE2
and 6% for RTE3. This aspect together with an er-
ror analysis we performed are the focus of Section
7.
The last column (Dirt+Id+WN) gives the preci-
sion we obtain if we simply decide a pair is true
entailment if we have an inference rule matched in
it (irrespective of the values of the anchors or of
the existence of tree skeletons). As expected, only
identifying the patterns of a rule in a pair irrespec-
tive of tree skeletons does not give any indication
of the entailment value of the pair.
9The RTE task is considered to be difficult. The aver-
age accuracy of the systems in the RTE-3 challenge is around
61% (Giampiccolo et al, 2007)
216
RTE Set DirtTS Dirt + WNTS IdTS Dirt + Id + WNTS Dirt + Id + WN
RTE2 49/69.38 94/67.02 45/66.66 130/65.38 673/50.07
RTE3 42/69.04 70/70.00 29/79.31 93/72.05 661/55.06
Table 4: Coverage/precision with various rule collections
RTE Set BoW Main
RTE2 (85 pairs) 51.76% 60.00%
RTE3 (64 pairs) 54.68% 62.50%
Table 5: Precision on the covered RTE data
RTE Set (800 pairs) BoW Main & BoW
RTE2 56.87% 57.75%
RTE3 61.12% 61.75%
Table 6: Precision on full RTE data
5.2 Results on the entire data
At last, we also integrate our method with a bag
of words baseline, which calculates the ratio of
overlapping words in T and H. For the pairs that
our method covers, we overrule the baseline?s de-
cision. The results are shown in Table 6 (Main
stands for the Dirt + Id + WNTS configuration).
On the full data set, the improvement is still small
due to the low coverage of our method, however
on the pairs that are covered by our method (Ta-
ble 5), there is a significant improvement over the
overlap baseline.
6 Discussion
In this section we take a closer look at the data in
order to better understand how does our method
of combining tree skeletons and inference rules
work. We will first perform error analysis on what
we have considered our best setting so far. Fol-
lowing this, we analyze data to identify the main
reasons which cause the low coverage.
For error analysis we consider the pairs incor-
rectly classified in the RTE3 test data set, consist-
ing of a total of 25 pairs. We classify the errors
into three main categories: rule application errors,
inference rule errors, and other errors (Table 7).
In the first category, the tree skeleton fails to
match the corresponding anchors of the inference
rules. For instance, if someone founded the Insti-
tute of Mathematics (Instituto di Matematica) at
the University of Milan, it does not follow that they
founded The University of Milan. The Institute of
Mathematics should be aligned with the Univer-
sity of Milan, which should avoid applying the in-
ference rule for this pair.
A rather small portion of the errors (16%) are
caused by incorrect inference rules. Out of these,
two are correct in some contexts but not in the en-
tailment pairs in which they are found. For exam-
ple, the following rule X generate Y ? X earn Y is
used incorrectly, however in the restricted context
of money or income, the two verbs have similar
meaning. An example of an incorrect rule is X is-
sue Y ? X hit Y since it is difficult to find a context
in which this holds.
The last category contains all the other errors.
In all these cases, the additional information con-
veyed by the text or the hypothesis which cannot
be captured by our current approach, affects the
entailment. For example an imitation diamond is
not a diamond, and more than 1,000 members
of the Russian and foreign media does not entail
more than 1,000 members from Russia; these are
not trivial, since lexical semantics and fine-grained
analysis of the restrictors are needed.
For the second part of our analysis we discuss
the coverage issue, based on an analysis of uncov-
ered pairs. A main factor in failing to detect pairs
in which entailment rules should be applied is the
fact that the tree skeleton does not find the corre-
sponding lexical items of two rule patterns.
Issues will occur even if the tree skeleton struc-
ture is modified to align all the corresponding frag-
ments together. Consider cases such as threaten to
boycott and boycott or similar constructions with
other embedding verbs such as manage, forget, at-
tempt. Our method can detect if the two embedded
verbs convey a similar meaning, however not how
the embedding verbs affect the implication.
Independent of the shortcomings of our tree
skeleton structure, a second factor in failing to de-
tect true entailment still lies in lack of rules. For
instance, the last two examples in Table 2 are en-
tailment pair fragments which can be formulated
as inference rules, but it is not straightforward to
acquire them via the DH.
217
Source of error % pairs
Incorrect rule application 32%
Incorrect inference rules 16%
Other errors 52%
Table 7: Error analysis
7 Conclusion
Throughout the paper we have identified impor-
tant issues encountered in using inference rules for
textual entailment and proposed methods to solve
them. We explored the possibility of combin-
ing a collection obtained in a statistical, unsuper-
vised manner, DIRT, with a hand-crafted lexical
resource in order to make inference rules have a
larger contribution to applications. We also inves-
tigated ways of effectively applying these rules.
The experiment results show that although cover-
age is still not satisfying, the precision is promis-
ing. Therefore our method has the potential to be
successfully integrated in a larger entailment de-
tection framework.
The error analysis points out several possible
future directions. The tree skeleton representation
we used needs to be enhanced in order to capture
more accurately the relevant fragments of the text.
A different issue remains the fact that a lot of rules
we could use for textual entailment detection are
still lacking. A proper study of the limitations of
the DH as well as a classification of the knowledge
we want to encode as inference rules would be a
step forward towards solving this problem.
Furthermore, although all the inference rules we
used aim at recognizing positive entailment cases,
it is natural to use them for detecting negative
cases of entailment as well. In general, we can
identify pairs in which the patterns of an inference
rule are present but the anchors are mismatched, or
they are not the correct hypernym/hyponym rela-
tion. This can be the base of a principled method
for detecting structural contradictions (de Marn-
effe et al, 2008).
8 Acknowledgments
We thank Dekang Lin and Patrick Pantel for
providing the DIRT collection and to Grzegorz
Chrupa?a, Alexander Koller, Manfred Pinkal and
Stefan Thater for very useful discussions. Geor-
giana Dinu and Rui Wang are funded by the IRTG
and PIRE PhD scholarship programs.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-
tor, and Moshe Friedman. 2007. Semantic inference
at the lexical-syntactic level for textual entailment
recognition. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 131?136, Prague, June. Association for Com-
putational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of 39th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 50?57,
Toulouse, France, July. Association for Computa-
tional Linguistics.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
In Proceedings of RANLP, Borovets, Bulgaria.
Peter Clark, Phil Harrison, John Thompson, William
Murray, Jerry Hobbs, and Christiane Fellbaum.
2007. On the role of lexical and world knowledge
in rte3. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
54?59, Prague, June. Association for Computational
Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
Vol. 3944, Springer, pages 177?190. Quionero-
Candela, J.; Dagan, I.; Magnini, B.; d?Alch-Buc, F.
Machine Learning Challenges.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047, Columbus, Ohio, June. Association for
Computational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 1?9, Prague, June. Association
for Computational Linguistics.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, pages 125?130,
Prague, June. Association for Computational Lin-
guistics.
Dekang Lin and Patrick Pantel. 2001. Dirt. discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328, New York, NY, USA. ACM.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems, Granada.
218
Erwin Marsi, Emiel Krahmer, and Wauter Bosma.
2007. Dependency-based paraphrasing for recog-
nizing textual entailment. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 83?88, Prague, June. Associa-
tion for Computational Linguistics.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5 (Inference in Com-
putational Semantics, Buxton, UK.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In HLT-NAACL, pages 102?109.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In Proceedings of International Workshop on Para-
phrase, pages 80?87, Jeju Island, Korea.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In In Proceedings of
EMNLP, pages 41?48.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT, pages 683?691, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Rui Wang and Gu?nter Neumann. 2007. Recognizing
textual entailment using sentence similarity based on
dependency tree skeletons. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 36?41, Prague, June. Associa-
tion for Computational Linguistics.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 401?408, Morristown, NJ, USA. Association
for Computational Linguistics.
219
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 44?47,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Ranking Paraphrases in Context
Stefan Thater
Universit?t des Saarlandes
stth@coli.uni-sb.de
Georgiana Dinu
Universit?t des Saarlandes
dinu@coli.uni-sb.de
Manfred Pinkal
Universit?t des Saarlandes
pinkal@coli.uni-sb.de
Abstract
We present a vector space model that sup-
ports the computation of appropriate vec-
tor representations for words in context,
and apply it to a paraphrase ranking task.
An evaluation on the SemEval 2007 lexical
substitution task data shows promising re-
sults: the model significantly outperforms
a current state of the art model, and our
treatment of context is effective.
1 Introduction
Knowledge about paraphrases is of central impor-
tance to textual inference modeling. Systems which
support automatic extraction of large repositories
of paraphrase or inference rules like Lin and Pantel
(2001) or Szpektor et al (2004) thus form first-class
candidate resources to be leveraged for NLP tasks
like question answering, information extraction, or
summarization, and the meta-task of recognizing
textual entailment.
Existing knowledge bases still suffer a number
of limitations, making their use in applications
challenging. One of the most serious problems
is insensitivity to context. Natural-language infer-
ence is highly context-sensitive, the applicability
of inference rules depending on word sense and
even finer grained contextual distinctions in us-
age (Szpektor et al, 2007). Application of a rule
like ?X shed Y ? X throw Y? is appropriate in a
sentence like ?a mouse study sheds light on the
mixed results,? but not in sentences like ?the econ-
omy seems to be shedding fewer jobs? or ?cats
do not shed the virus to other cats.? Systems like
the above-mentioned ones base the extraction of
inference rules on distributional similarity of words
rather than word senses, and apply unconditionally
whenever one side of the rule matches on the word
level, which may lead to considerable precision
problems (Geffet and Dagan, 2005) .
Some approaches address the problem of con-
text sensitivity by deriving inference rules whose
argument slots bear selectional preference infor-
mation (Pantel et al, 2007; Basili et al, 2007). A
different line of accounting for contextual variation
has been taken by Mitchell and Lapata (2008), who
propose a compositional approach, ?contextualiz-
ing? the vector-space meaning representation of
predicates by combining the distributional proper-
ties of the predicate with those of its arguments.
A related approach has been proposed by Erk and
Pad? (2008), who integrate selectional preferences
into the compositional picture. In this paper, we
propose a context-sensitive vector-space approach
which draws some important ideas from Erk and
Pado?s paper (?E&P? in the following), but imple-
ments them in a different, more effective way: An
evaluation on the SemEval 2007 lexical substitu-
tion task data shows that our model significantly
outperforms E&P in terms of average precision.
Plan of the paper. Section 2 presents our model
and briefly relates it to previous work. Section 3
describes the evaluation of our model on the lexical
substitution task data. Section 4 concludes.
2 A model for meaning in context
We propose a dependency-based model whose di-
mensions reflect dependency relations, and distin-
guish two kinds or layers of lexical meaning: ar-
gument meaning and predicate meaning. The argu-
ment meaning of a word w is a vector representing
frequencies of all pairs (w
?
,r
?
) of predicate expres-
sions w
?
and dependency relations r
?
such that w
?
stands in relation r
?
to w. Intuitively, argument
meaning is similar to E&P?s ?inverse selectional
preferences.? Argument meanings are used for two
purposes in our model: (i) to construct predicate
meanings, and (ii) to contextually constrain them.
For technical convenience, we will use a defini-
tional variant of argument meaning, by indexing
it with an ?incoming? relation, which allows pred-
icate and argument meaning to be treated techni-
cally as vectors of the same type. Assuming a set
44
R of role labels and a set W of words, we represent
both predicate and argument meaning as vectors
in a vector space V with a basis {e
i
}
i?R?R?W
, i.e.,
a vector space whose dimensions correspond to
triples of two role labels and a word. The argument
meaning v
r
(w) of a word w is defined as follows:
v
r
(w) =
?
w
?
?W,r
?
?R
f (w
?
,r
?
,w) ? e
(r,r
?
,w
?
)
, (1)
where r is the ?incoming? relation, and f (w
?
,r
?
,w)
denotes the frequency of w occurring in relation r
?
to w
?
in a collection of dependency trees. To obtain
predicate meaning v
P
(w), we count the occurrences
of argument words w
?
standing in relation r to w,
and compute the predicate meaning as the sum of
the argument meanings v
r
(w
?
), weighted by these
co-occurrence frequencies:
v
P
(w) =
?
r?R,w
?
?W
f (w,r,w
?
) ? v
r
(w
?
) (2)
That is, the meaning of a predicate is modelled by a
vector representing ?second order? co-coccurrence
frequencies with other predicates.
In general, words have both a ?downward look-
ing? predicate meaning and an ?upward looking?
argument meaning. In our study, only one of them
will be relevant, since we will restrict ourselves
to local predicate-argument structures with verbal
heads and nominal arguments.
Computing meaning in context. Vectors repre-
senting predicate meaning are derived by collecting
co-occurrence frequencies for all uses of the pred-
icate, possibly resulting in vector representations
in which different meanings of the predicate are
combined. Given an instance of a predicate w that
has arguments w
1
, . . . ,w
k
, we can now contextually
constrain the predicate meaning of w by the argu-
ment meanings of its arguments. Here, we propose
to simple ?restrict? the predicate meaning to those
dimensions that have a non-zero value in at least
one of its argument meanings. More formally, we
write v
|v
?
to denote a vector that is identical to v
for all components that have a non-zero value in v
?
,
zero otherwise. We compute predicate meaning in
context as follows:
v
P
(w)
|
?
1?i?k
v
r
i
(w
i
)
, (3)
where r
i
is the argument position filled by w
i
.
Parameters. To reduce the effect of noise and
provide a more fine-grained control over the ef-
fect of context, we can choose different thresholds
target subject object paraphrases
shed study light throw 3, reveal 2, shine 1
shed cat virus spread 2, pass 2, emit 1, transmit 2
shed you blood lose 3, spill 1, give 1
Table 1: Lexical substitution task data set
for function f in the computation of predicate and
argument meaning. In Section 3, we obtain best
results if we consider only dependency relations
that occur at least 6 times in the British National
Corpus (BNC) for the computation of predicate
meaning, and relations occurring at least 15 times
for the computation of argument meanings when
predicate meaning is contextually constrained.
Related work. Our model is similar to the struc-
tured vector space model proposed by Erk and Pad?
(2008) in that the representation of predicate mean-
ing is based on dependency relations, and that ?in-
verse selectional preferences? play an important
role. However, inverse selectional preferences are
used in E&P?s model mainly to compute mean-
ing in context, while they are directly ?built into?
the vectors representing predicate meaning in our
model.
3 Evaluation
We evaluate our model on a paraphrase ranking
task on a subset of the SemEval 2007 lexical substi-
tution task (McCarthy and Navigli, 2007) data, and
compare it to a random baseline and E&P?s state
of the art model.
Dataset. The lexical substitution task dataset con-
tains 10 instances for 44 target verbs in different
sentential contexts. Systems that participated in
the task had to generate paraphrases for each of
these instances, which are evaluated against a gold
standard containing up to 9 possible paraphrases
for individual instances. Following Erk and Pad?
(2008), we use the data in a different fashion: we
pool paraphrases for all instances of a verb in all
contexts, and use the models to rank these para-
phrase candidates in specific contexts.
Table 1 shows three instances of the target verb
shed together with its paraphrases in the gold stan-
dard as an expample. The paraphrases are attached
with weights, which correspond to the number of
times they have been given by different annotators.
To allow for a comparision with E&P?s model,
we follow Erk and Pad? (2008) and extract only
sentences from the dataset containing target verbs
45
with overtly realized subject and object, and re-
move instances from the dataset for which the tar-
get verb or one of its arguments is not in the BNC.
We obtain a set of 162 instances for 34 different
verbs. We also remove paraphrases that are not
in the BNC. On average, target verbs have 20.5
paraphrase candidates, 3.9 of which are correct in
specific contexts.
Experimental setup. We parse the BNC using
MiniPar (Lin, 1993) and extract co-occurrence fre-
quencies, considering only dependency relations
for the most frequent 2000 verbs. We don?t use raw
frequency counts directly but reweight the vectors
by pointwise mutual information.
To rank paraphrases in context, we compute con-
textually constrained vectors for the verb in the
input sentence and all its paraphrase candidates
by taking the corresponding predicate vectors and
restricting them to the argument meanings of the
argument head nouns in the input sentence. The
restricted vectors for the paraphrase candidates are
then ranked by comparing them to the restricted
vector of the input verb using cosine similarity.
In order to compare our model with state of the
art, we reimplement E&P?s structured vector space
model. We filter stop words, and compute lexical
vectors in a ?syntactic? space using the most fre-
quent 2000 words from the BNC as basis. We also
consider a variant in which the basis corresponds
to words indexed by their grammatical roles. We
choose parameters that Erk and Pad? (2009) report
to perform best, and use the method described in
Erk and Pad? (2009) to compute vectors in context.
Evaluation metrics. As scoring methods, we
use both ?precision out of ten? (P
oot
), which was
originally used in the lexical substitution task and
also used by E&P, and generalized average preci-
sion (Kishida, 2005), a variant of average precision
which is frequently used in information extraction
tasks and has also been used in the PASCAL RTE
challenges (Dagan et al, 2006).
P
oot
can be defined as follows:
P
oot
=
?
s?M
?
G
f (s)
?
s?G
f (s)
,
where M is the list of 10 paraphrase candidates
top-ranked by the model, G is the corresponding
annotated gold data, and f (s) is the weight of the
individual paraphrases. Here, P
oot
is computed for
each target instance separately; below, we report
the average over all instances.
Model P
oot
GAP
Random baseline 54.25 26.03
E&P (target only) 64.61 (63.31) 29.95 (32.02)
E&P (add, object only) 66.20 (62.90) 29.93 (31.54)
E&P (min, both) 64.86 (59.62) 32.22 (31.28)
TDP 63.32 36.54
TDP (target only) 62.60 33.04
Table 2: Results
Generalized average precision (GAP) is a more
precise measure than P
oot
: Applied to a ranking
task with about 20 candidates, P
oot
just gives the
percentage of good candidates found in the upper
half of the proposed ranking. Average precision
is sensitive to the relative position of correct and
incorrect candidates in the ranking, GAP moreover
rewards the correct order of positive cases w.r.t.
their gold standard weight.
We define average precision first:
AP =
?
n
i=1
x
i
p
i
R
p
i
=
?
i
k=1
x
k
i
where x
i
is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n the number of paraphrase candidates to be
ranked. If we take x
i
to be the gold standard weight
of the ith item or zero if it is not in the gold standard,
we can define generalized average precision as
follows:
GAP =
?
n
i=1
I(x
i
) p
i
R
?
R
?
= ?
R
i=1
I(y
i
)y
i
where I(x
i
) = 1 if x
i
is larger than zero, zero oth-
erwise, and y
i
is the average weight of the ideal
ranked list y
1
, . . . ,y
i
of paraphrases in the gold stan-
dard.
Results and discussion. Table 2 shows the re-
sults of our experiments for two variants of our
model (?TDP?), and compares them to a random
baseline and three instantiations (in two variants) of
E&P?s model. The ?target only? models don?t use
context information, i.e., paraphrases are ranked by
cosine similarity of predicate meaning only. The
other models take context into account. The ?min?
E&P model takes the component-wise minimum to
combine a lexical vector with context vectors and
considers both subject and object as context; it is
the best performing model in Erk and Pad? (2009).
The ?add? model uses vector addition and consid-
ers only objects as context; it is the best-performing
46
020
40
60
80
100
1 2 3 4 5 6 7 8 9 10
p
r
e
s
i
s
i
o
n
 
o
u
t
 
o
f
 
n
E&P (add, object only) 
present paper
baseline
upper bound
Figure 1: ?Precision out of n? for 1? n? 10.
model (in terms of P
oot
) for our dataset. The num-
bers in brackets refer to variants of the E&P models
in which the basis corresponds to words indexed
by their syntactic roles. Note that the results for the
E&P models are better than the results published
in Erk and Pad? (2009), which might be due to
slightly different datasets or lists of stop-words.
As can be seen, our model performs > 10% bet-
ter than the random baseline. It performs > 4%
better than the ?min? E&P model and > 6% better
then the ?add? model in terms of GAP if we use a
vectors space with words as basis. For the variants
of the E&P models in which the basis corresponds
to words indexed by their syntactic role, we ob-
tain different results, but our model is still > 4%
better than these variants. We can also see that
our treatment of context is effective, leading to a
> 3% increase of GAP. A stratified shuffling-based
randomization test (Yeh, 2000) shows that the dif-
ferences are statistically significant (p < 0.05).
In terms of P
oot
, the ?add? E&P model performs
better than our model, which might look surprising,
given its low GAP score. Fig. 1 gives a more fine-
grained comparison between the two models. It
displays the ?precision out of n? of the two models
for varying n. As can be seen, our model performs
better for all n< 10, and much better than the base-
line and E&P for n? 4.
4 Conclusion
In this paper, we have proposed a dependency-
based context-sensitive vector-space approach that
supports the computation of adequate vector-based
representations of predicate meaning in context.
An evaluation on a paraphrase ranking task using
a subset of the SemEval 2007 lexical substitution
task data shows promising results: our model per-
forms significantly better than a current state of the
art system (Erk and Pad?, 2008), and our treatment
of context is effective.
Since the dataset we used for the evaluation is
relatively small, there is a potential danger for over-
fitting, and it remains to be seen whether the results
carry over to larger datasets. First experiments
indicate that this is actually the case.
We expect that our approach can be generalized
to arrive at a general compositional model, which
would allow to compute contextually appropriate
meaning representations for complex relational ex-
pressions rather than single lexical predicates.
Acknowledgements. We thank Katrin Erk and
Sebastian Pad? for help and critical comments.
References
R. Basili, D. De Cao, P. Marocco, and M. Pennacchiotti. 2007.
Learning selectional preferences for entailment or para-
phrasing rules. In Proc. of RANLP 2007.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Machine
Learning Challenges, volume 3944. Springer.
K. Erk and S. Pad?. 2008. A structured vector space model
for word meaning in context. In Proc. of EMNLP.
K. Erk and S. Pad?. 2009. Paraphrase assessment in struc-
tured vector space: Exploring parameters and datasets. In
Proc. of the Workshop on Geometrical Models of Natural
Language Semantics, Athens.
M. Geffet and I. Dagan. 2005. The distributional inclusion
hypotheses and lexical entailment. In Proc. of the ACL.
K. Kishida. 2005. Property of average precision and its
generalization: An examination of evaluation indicator for
information retrieval experiments. NII Technical Report.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Inference
Rules from Text. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining, San Francisco.
D. Lin. 1993. Principle-based parsing without overgeneration.
In Proc. of ACL, Columbus.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proc. of SemEval,
Prague.
J. Mitchell and M. Lapata. 2008. Vector-based models of se-
mantic composition. In Proc. of ACL-08: HLT, Columbus.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and E. Hovy.
2007. ISP: Learning inferential selectional preferences. In
Human Language Technologies 2007, Rochester.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scal-
ing web-based acquisition of entailment relations. In Proc.
of EMNLP, Barcellona.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-based
evaluation of entailment rule acquisition. In Proc. of ACL.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proc. of COLING.
47
Proceedings of the 8th International Conference on Computational Semantics, pages 90?103,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Inference Rules for Recognizing Textual Entailment
Georgiana Dinu
Saarland University
dinu@coli.uni-sb.de
Rui Wang
Saarland University
rwang@coli.uni-sb.de
Abstract
In this paper, we explore the application of inference rules for rec-
ognizing textual entailment (RTE). We start with an automatically
acquired collection and then propose methods to refine it and obtain
more rules using a hand-crafted lexical resource. Following this, we
derive a dependency-based representation from texts, which aims to
provide a proper base for the inference rule application. The evalu-
ation of our approach on the RTE data shows promising results on
precision and the error analysis suggests future improvements.
1 Introduction
Textual inference plays an important role in many natural language pro-
cessing (NLP) tasks, such as question answering [7]. In recent years, the
recognizing textual entailment (RTE) [4] challenge, which focuses on de-
tecting semantic inference, has attracted a lot of attention. Given a text T
(several sentences) and a hypothesis H (one sentence), the goal is to detect
if H can be inferred from T.
Studies such as [3] attest that lexical substitution (e.g. synonyms, anto-
nyms) or simple syntactic variation accounts for the entailment only in a
small number of pairs. Thus, one essential issue is to identify more complex
expressions which, in appropriate contexts, convey the same (or similar)
meaning. More generally, we are also interested in pairs of expressions in
which only a uni-directional inference relation holds
1
.
A typical example is the following RTE pair in which accelerate to in H
is used as an alternative formulation for reach speed of in T.
1
We will use the term inference rule to stand for such concept; the two expressions can
be actual paraphrases if the relation is bi-directional
90
T: The high-speed train, scheduled for a trial run on Tuesday, is able to reach
a maximum speed of up to 430 kilometers per hour, or 119 meters per second.
H: The train accelerates to 430 kilometers per hour.
One way to deal with textual inference is through rule representation,
such as X wrote Y ? X is author of Y. However, manually building col-
lections of inference rules is time-consuming and it is unlikely that humans
can exhaustively enumerate all the rules encoding the knowledge needed in
reasoning with natural languages. Instead, an alternative is to acquire these
rules automatically from large corpora. Furthermore, given such a rule col-
lection, how to successfully use it in NLP applications is the next step to be
focused on.
For the first aspect, we extend and refine an existing collection of infer-
ence rules acquired based on the Distributional Hypothesis (DH). One of the
main advantages of using DH is that the only input needed is a large corpus
of (parsed) text
2
. For this purpose, a hand-crafted lexical resource is used
for augmenting the original inference rule collection and excluding some of
the incorrect rules.
For the second aspect, we focus on applying these rules to the RTE task.
In particular, we use a structure representation derived from the dependency
parse trees of T and H, which aims to capture the essential information they
convey.
The rest of the paper is organized as follows: Section 2 introduces the
inference rule collection we use, based on the Discovery of Inference Rules
from Text (henceforth DIRT) algorithm; we also discuss previous work on
applying it to the RTE task. Section 3 presents our analyses on the RTE
data and discusses two issues: the lack of rules and the difficulty of finding
proper ways of applying them. Section 4 proposes methods to extend and
refine the rule collection aiming at the former issue. To address the latter
issue, Section 5 describes the structure representation we use to identify the
appropriate context for the rule application. The experiments will be pre-
sented in Section 6, followed by an error analysis and discussions in Section
7. Finally, Section 8 will conclude the paper and point out some future
work.
2
Another line of work on acquiring paraphrases uses comparable corpora, for instance
[2], [12]
91
2 Background
A number of automatically acquired inference rule/paraphrase collections
are available, such as [14]. In our work we use the DIRT collection because
it is the largest one and it has a relatively good accuracy (in the 50% range,
[13]). In this section, we describe the DIRT algorithm for acquiring inference
rules. Following that, we will overview the RTE systems which take DIRT
as an external knowledge resource.
2.1 Discovery of Inference Rules from Text
The DIRT algorithm has been introduced by [10] and it is based on what
is called the Extended Distributional Hypothesis. The original DH states
that words occurring in similar contexts have similar meaning, whereas the
extended version hypothesizes that phrases occurring in similar contexts are
similar.
An inference rule in DIRT is a pair of binary relations ? pattern
1
(X,Y ),
pattern
2
(X,Y ) ? which stand in an inference relation. pattern
1
and pattern
2
are chains in Minipar [9] dependency trees while X and Y are placeholders for
nouns at the end of the chains. The two patterns will constitute a candidate
paraphrase if the sets of X and Y values exhibit relevant overlap. An example
is the pair (X
subj
??? prevent
obj
??? Y, X
subj
??? provide
obj
??? protection
mod
???
against
pcomp
????? Y).
Such rules can be defined [13] as directional relations between two text
patterns with variables. The left-hand-side pattern is assumed to entail
the right-hand-side pattern in certain contexts, under the same variable
instantiation. The definition relaxes the intuition of inference, as we only
require the entailment to hold in some but not all contexts, motivated by
the fact that such inferences occur often in natural text.
2.2 Related Work
Intuitively such inference rules should be effective for recognizing textual
entailment. However, only a small number of systems have used DIRT as
a resource in the RTE-3 challenge, and the experimental results have not
shown its great contribution.
In [3]?s approach, semantic parsing in clause representation is performed
and true entailment is decided only if every clause in the semantic repre-
sentation of T semantically matches some clause in H. The only variation
allowed consists of rewritings derived from WordNet and DIRT. Given the
92
preliminary stage of this system, the overall results show very low improve-
ment over a random classification baseline.
[1] implement a proof system using rules for generic linguistic structures,
lexical-based rules, and lexical-syntactic rules (which were obtained with the
DIRT algorithm applied to the first CD of the Reuters RCV1 corpus). Given
a premise p and a hypothesis h, the lexical-syntactic component marks all
lexical noun alignments. For every pair of alignments, the paths between
the two nouns are extracted, and the DIRT algorithm is applied to obtain
a similarity score. If the score is above a threshold, the rule will be ap-
plied. However, these lexical-syntactic rules are only used in about 3% of
the attempted proofs and for most cases there is no lexical variation.
[8] use DIRT in a more relaxed manner. A DIRT rule is employed in the
system if at least one of the anchors match in T and H, i.e. they use them
as unary rules. However, the analysis of the system shows that the DIRT
component is the least relevant one (adding 0.4% to the precision).
In [11]?s system, a paraphrase substitution step is added on top of a sys-
tem based on a tree alignment algorithm. The basic paraphrase substitution
method follows several steps. Initially, the two patterns of a rule are matched
in T and H (instantiations of the anchors X, Y do not have to match). The
T tree is transformed by applying the paraphrase substitution. Following
that, the transformed T tree and H tree are aligned. The coverage (pro-
portion of aligned content words) is computed and if above some threshold,
the entailment holds. The paraphrase component adds 1.0% to the result
on the development set and only 0.5% to the test set, but a more detailed
analysis on the interaction of this component with other components of the
system is not given.
3 Inference Rules for RTE
In this section our goal is to investigate the causes for which a resource
such as DIRT fails to bring clear improvements to RTE. The issues we have
encountered can be divided into two categories. Firstly, given a collection
of correct inference rules, making full use of the knowledge encoded in it is
not a trivial task. Secondly, some of the needed rules still lack even in a
very large collection such as DIRT. Section 4 will tackle the latter issue first
while Section 5 will focus on the former one.
93
3.1 DIRT Rules Found in the RTE Data
To Address this first issue, we begin with a straightforward experiment to
discover the number of pairs in the RTE data which contain rules present
in DIRT
3
.
Following the definition of an entailment rule, we identify RTE pairs in
which pattern
1
(w1, w2) and pattern
2
(w1, w2) are matched, one in T and
the other one in H, and thus, ?pattern
1
(X,Y ), pattern2(X,Y )? is an infer-
ence rule. The pair below is an example of this.
T: The sale was made to pay Yukos US$ 27.5 billion tax bill, Yuganskneftegaz
was originally sold for US$ 9.4 billion to a little known company Baikalfinans-
group which was later bought by the Russian state-owned oil company Rosneft.
H: Baikalfinansgroup was sold to Rosneft.
On average, only 2% of the pairs in the RTE data are subject to such
inference rules. Out of these, approximately 50% are lexical rules (one verb
entailing the other) and in the rest, around 50% are present in WordNet as
a synonym, hypernym or sister relation.
However, given the small number of inference rules identified this way,
we performed another analysis. This aims at determining an upper bound
of the number of pairs featuring entailment phrases present in a collection.
Given DIRT and the RTE data, we compute that in how many pairs two
patterns of a paraphrase can be matched irrespectively of their anchor val-
ues. An example is the following pair,
T: Libyas case against Britain and the US concerns the dispute over their
demand for extradition of Libyans charged with blowing up a Pan Am jet over
Lockerbie in 1988.
H: One case involved the extradition of Libyan suspects in the Pan Am Locker-
bie bombing.
This is a case in which the rule is correct and the entailment is positive.
In order to determine this, a system will have to know that Libya?s case
against Britain and the US in T entails one case in H. Similarly, in this
context, the dispute over their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie can be replaced with the
extradition of Libyan suspects in the Pan Am Lockerbie bombing. Altogether
3
For all the experiments in this paper, we use the DIRT collection provided by [10],
derived from the DIRT algorithm applied on 1GB of newstext.
94
X, founded in Y ? X, opened in Y
X launch Y ? X produce Y
X represent Z ? X work for Y
X faces menace from Y ? X endangered by Y
X, peace agreement for Y ? X is formulated to end war in Y
Table 1: Example of inference rules needed in RTE
in around 25% of the pairs, patterns of a rule can be found in this way, and
many times more than one rule in a pair. However, in many of these pairs,
finding out the patterns of an inference rule does not imply that the rule is
truly present in that pair.
Making use of the knowledge encoded with such rules is therefore, not
a trivial task. If rules are used strictly in concordance with their definition,
their utility is limited to a very small number of pairs. For this reason, 1)
instead of forcing the anchor values to be identical as most previous works,
we allow flexible rule matching (similar to [11]) and 2) furthermore, we
control the rule application process using a structure representation derived
from the dependency tree (Section 5).
3.2 Missing Rules
Apart from the issues underlined in the previous section, looking at the
data, we find it quite clear that DIRT lacks rules that many entailment
pairs require.
Table 1 gives a selection of rules that are needed in some entailment pairs.
The first three rows contain rules which are not structurally complex. These,
however, are missing from both DIRT and also other hand-crafted resources
such as WordNet (i.e. there is no short path connecting them). This is
to be expected as they are rules which hold in some specific contexts, but
difficult to be captured by a sense distinction of the lexical items involved.
The more complex rules are even more difficult to be captured by a DIRT-
like algorithm. Some of these do not occur frequently enough even in large
amounts of text to permit the acquirement of them via DH.
4 Extending and Refining DIRT
In order to address the issue of missing rules, we investigate the effects of
combining DIRT with an exact hand-coded lexical resource in order to create
new rules.
95
X face threat of Y X at risk of Y
face ? confront, front, look, face up risk ? danger, hazard, jeopardy
threat ? menace, terror, scourge
endangerment, peril
Table 2: Lexical variations creating new rules based on DIRT rule X face
threat of Y ? X at risk of Y
For this we extended the DIRT rules by adding rules in which any of the
lexical items involved in the patterns can be replaced by WordNet synonyms.
The idea behind this is that a combination of various lexical resources is
needed in order to cover the vast variety of phrases which humans can judge
to be in an inference relation.
In the example above, we consider the DIRT rule X face threat of Y
? X, at risk of Y (Table 2). Of course at this moment due to the lack
of sense disambiguation, our method introduces lots of rules that are not
correct. As one can see, expressions such as front scourge do not make any
sense, therefore any rules containing this will be incorrect. However some
of the new rules created in this example, such as X face threat of Y ? X,
at danger of Y are reasonable ones and the rules which are incorrect often
contain patterns that are very unlikely to occur in natural text.
The method just described allows us to identify the first three rules listed
in Table 1. We also acquire the rule X face menace of Y ? X endangered
by Y (via X face threat of Y ? X threatened by Y, menace ? threat,
threaten ? endanger). However the entailment pair requires a slightly
different version of the rule, involving the phrase face menace from.
Our extension is application-oriented therefore it is not intended to be
evaluated as an independent rule collection, but in an application scenario
such as RTE (Section 6).
Another issue that we address is the one of removing the most systematic
errors present in DIRT. DH algorithms have the main disadvantage that not
only phrases with the same meaning are extracted but also phrases with
opposite meaning.
In order to overcome this problem and since such errors are relatively
easy to detect, we applied a filter to the DIRT rules. This eliminates in-
ference rules which contain WordNet antonyms. To evaluate the precision
of our method, we randomly selected 200 examples of rules eliminated from
DIRT (irrespective of the textual entailment data) and a human evaluator
decided if they are indeed incorrect inference rules. Out of these 92% turned
96
out to be incorrect rules, such as X right about Y ? X wrong about Y. How-
ever, there are also cases of correct rules being eliminated, such as X have
second thoughts about Y ? X lack confidence about Y.
5 Inference Rules on Tree Skeletons
In order to address the issues described in Section 3.1, we choose to apply
the rule collection on a dependency-based representation of T and H. We
will first introduce this representation and the algorithm to derive it, and
following that we will describe how we applied the inference rules on this
structure.
Tree Skeletons
The Tree Skeleton (TS) structure was proposed by [15], and can be
viewed as an extended version of the predicate-argument structure. Since it
contains not only the predicate and its arguments, but also the dependency
paths in-between, it captures the essential part of the sentence.
Following their algorithm, we first preprocess the data using the Minipar
dependency parser and then select overlapping topic words (i.e. nouns) in
T and H (we use fuzzy match at the substring level instead of full match).
Starting with these nouns, we traverse the dependency tree to identify the
lowest common ancestor node (named as root node). This sub-tree without
the inner yield is defined as a Tree Skeleton. Figure 1 shows the TS of T in
the pair:
T For their discovery of ulcer-causing bacteria, Australian doctors Robin War-
ren and Barry Marshall have received the 2005 Nobel Prize in Physiology or Medicine.
H Robin Warren was awarded a Nobel Prize.
Notice that, in order to match the inference rules with two anchors, the
number of the dependency paths from the nouns to the root node should
also be two. In practice, tree skeletons can be extracted from approximately
30% of the T-H pairs.
Applying DIRT on a TS
After extracting the TS, the next step is to find the inference rules which
match the two tree skeletons of a T-H pair. This is done in a rather straight-
forward manner. Given tree skeletons of T and H, we check if the two left
dependency paths, the two right ones or the two root nodes contain the
patterns of a rule.
In the example above, the rule X
obj
??? receive
subj
???? Y ?X
obj2
???? award
obj1
????
97
Figure 1: Dependency structure of text. Tree skeleton in bold
Y satisfies this criterion, as it is matched at the root nodes. Notice that the
rule is correct only in restricted contexts, in which the object of receive is
something which is conferred on the basis of merit.
6 Experiments
Our experiments consist in predicting positive entailment in a very straight-
forward rule-based manner. For each collection we select the RTE pairs in
which we find a tree skeleton and match an inference rule. The first number
in our table entries represents how many of such pairs we have identified, out
of 1600 development and test pairs. For these pairs we simply predict pos-
itive entailment and the second entry represents what percentage of these
pairs are indeed true entailment. Our work does not focus on building a
complete RTE system but we also combine our method with a bag of words
baseline to see the effects on the entire data set.
In the first two columns (Table 3: Dirt
TS
and Dirt+WN
TS
) we consider
DIRT in its original state and DIRT with rules generated with WordNet as
described in Section 4; all precisions are higher than 63%
4
. After adding
WordNet, tree skeletons and rules are matched in approximately twice as
many pairs, while the precision is not harmed. This may indicate that our
method of adding rules does not decrease precision of an RTE system.
In the third column we report the results of using a set of rules containing
only the trivial identity ones (Id
TS
). For our current system, this can be
seen as a precision upper bound for all the other collections, in concordance
4
The RTE task is considered to be difficult. The average accuracy of the systems in
the RTE-3 challenge is around 61% [6]
98
RTE Set Dirt
TS
Dirt+WN
TS
Id
TS
Dirt+Id Dirt+Id
+WN
?
TS
+WN
?
TS
RTE2 55/0.63 103/0.65 45/0.66 136/0.65 90/0.67
RTE3 48/0.66 79/0.65 29/0.79 101/0.69 74/0.71
Table 3: Results on tree skeletons with various rule collections
with the fact that identical rules are nothing but inference rules of highest
possible confidence. The fourth column (Dirt+Id+WN
TS
) contains what
can be considered our best setting. In this setting three times as many
pairs are covered using a collection containing DIRT and identity rules with
WordNet extension. Although the precision results with this setting are
encouraging (65% for RTE2 data and 69% for RTE3 data), the coverage is
still low, 8% for RTE2 and 6% for RTE3. This aspect together with an error
analysis we performed are the focus of Section 7.
Another experiment aimed at improving the precision of our predictions.
For this we further restrict our method: we have a true entailment only if
applying the inference rule to a TS leaves no unmatched lexical items in the
fragment of the dependency path where it has been identified. The more re-
stricted method (Dirt+Id+WN
?
TS
) gives, as expected, better precision with
an approximately 30% loss in coverage.
At last, we also integrate our method with a bag of words baseline,
which calculates the ratio of overlapping words in T and H. For the pairs
that our method covers, we overrule the baseline?s decision. The results are
shown in Table 4. On the full data set, the improvement is still small due
to the low coverage of our method, however on the pairs that are covered by
our method, there is a significant improvement over the overlap baseline.
RTE Test(# pairs) BoW BoW&Main
RTE2 (89) 52.80% 60.67%
RTE2 (800) 56.87% 57.75%
RTE3 (71) 52.11% 59.15%
RTE3 (800) 61.12% 61.75%
Table 4: Results on RTE test data. Covered set and full set.
99
Source of error # pairs % pairs
TS structure 7 23%
Incorrect rules 9 30%
Other 14 47%
Table 5: Error analysis
7 Discussion
In this section we take a closer look at the data in order to better understand
how does our method of combining tree skeletons and inference rules work.
For error analysis we consider the pairs incorrectly classified in the RTE3
data, consisting of a total of 30 pairs. We classify the errors into three main
categories: tree skeleton structure errors, inference rule errors, and other
errors (Table 5).
In the first category, seven T-H pairs are incorrect. In those cases the
tree skeleton fails to match the corresponding anchors of the inference rules.
For instance, if someone founded the Institute of Mathematics (Instituto di
Matematica) at the University of Milan, it does not follow that they founded
The University of Milan.
Approximately 30% of the errors are caused by incorrect inference rules.
Out of these, two are correct in some contexts but not in the entailment
pairs in which they are found. For example, the following rule X generate Y
? X earn Y is used incorrectly, however in the restricted context of money
or income, the two verbs have similar meaning. An example of an incorrect
rule is X issue Y ? X hit Y since it is difficult to find a context in which
this holds.
The last category contains all the other errors. In all these cases, the
additional information conveyed by the text or the hypothesis which cannot
be captured by our current approach, affects the entailment. For example
an imitation diamond is not a diamond, and more than 1,000 members of
the Russian and foreign media does not entail more than 1,000 members
from Russia; these are not trivial, since lexical semantics and fine-grained
analysis of the restrictors are needed.
In a second part of our analysis we discuss the coverage issue, based on
an analysis of uncovered pairs. A main factor in failing to detect pairs in
which entailment rules should be applied is the fact that the tree skeleton
does not find the corresponding lexical items of two rule patterns. In one of
the pairs 78% increase in X entails X rose by 78%. This rule is available,
however the tree skeletons capture reach and rise as key verbal nodes. In
100
another example, information such as the fact that rains are creating flood-
ing and devastating are all necessary to conclude that floods are ravaging
Europe. However a structure like tree skeleton cannot capture all these el-
ements. Issues will occur even if the tree skeleton structure is modified to
align all the corresponding fragments together. Consider constructions with
embedding verbs such as manage, forget, attempt. Our method can detect
if the two embedded verbs convey a similar meaning, however not how the
embedding verbs affect the entailment. Independent of the shortcomings of
our tree skeleton structure, a second factor in failing to detect true entail-
ment still lies in lack of rules (e.g. the last two examples in Table 1 are
entailment pair fragments which can be formulated as inference rules, but
are not straightforward to acquire).
8 Conclusion
Throughout the paper we have identified important issues encountered in
using inference rules for recognizing textual entailment and proposed meth-
ods to solve them. We explored the possibility of combining a collection
obtained in a statistical, unsupervised manner, DIRT, with a hand-crafted
lexical resource in order to make inference rules have a larger contribution to
applications. We also investigated ways of effectively applying these rules.
The experiment results show that although coverage is still not satisfying,
the precision is promising. Therefore our method has the potential to be
successfully integrated into a larger entailment detection framework.
The error analysis points out several possible future directions. The tree
skeleton representation we used needs to be enhanced in order to capture
more accurately the relevant fragments of the text. A different issue remains
the fact that a lot of rules we could use for RTE are still lacking. A proper
study of the limitations of the DH as well as a classification of the knowl-
edge we want to encode as inference rules would be a step forward towards
solving this problem. Furthermore, although all the inference rules we used
aim at recognizing positive entailment cases, it is natural to use them for
detecting negative cases of entailment as well. In general, we can identify
pairs in which the patterns of an inference rule are present but the anchors
are missmatched, or they are not in the correct hypernym/hyponym rela-
tion. This can be the base of a principled method for detecting structural
contradictions [5].
101
References
[1] Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpektor, and Moshe Fried-
man. Semantic inference at the lexical-syntactic level for textual entailment
recognition. In Proceedings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 131?136, Prague, June 2007.
[2] Regina Barzilay and Kathleen R. McKeown. Extracting paraphrases from a
parallel corpus. In Proceedings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 50?57, Toulouse, France, July 2001.
[3] Peter Clark, Phil Harrison, John Thompson, William Murray, Jerry Hobbs,
and Christiane Fellbaum. On the role of lexical and world knowledge in rte3.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 54?59, June 2007.
[4] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising
textual entailment challenge. In Lecture Notes in Computer Science, Vol.
3944, Springer, pages 177?190. Quionero-Candela, J.; Dagan, I.; Magnini, B.;
d?Alch-Buc, F. Machine Learning Challenges, 2006.
[5] Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning.
Finding contradictions in text. In Proceedings of ACL-08: HLT, pages 1039?
1047, Columbus, Ohio, June 2008.
[6] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The
third pascal recognizing textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1?
9, Prague, June 2007.
[7] Sanda Harabagiu and Andrew Hickl. Methods for using textual entailment in
open-domain question answering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual Meeting of the ACL,
pages 905?912, Sydney, Australia, July 2006.
[8] Adrian Iftene and Alexandra Balahur-Dobrescu. Hypothesis transformation
and semantic variability rules used in recognizing textual entailment. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 125?130, Prague, June 2007.
[9] Dekang Lin. Dependency-based evaluation of minipar. In Proc. Workshop on
the Evaluation of Parsing Systems, Granada, 1998.
[10] Dekang Lin and Patrick Pantel. Dirt. discovery of inference rules from text. In
KDD ?01: Proceedings of the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 323?328, New York, USA,
2001.
102
[11] Erwin Marsi, Emiel Krahmer, and Wauter Bosma. Dependency-based para-
phrasing for recognizing textual entailment. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Paraphrasing, pages 83?88,
Prague, June 2007.
[12] Bo Pang, Kevin Knight, and Daniel Marcu. Syntax-based alignment of mul-
tiple translations: Extracting paraphrases and generating new sentences. In
HLT-NAACL, pages 102?109, 2003.
[13] Idan Szpektor, Eyal Shnarch, and Ido Dagan. Instance-based evaluation of
entailment rule acquisition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages 456?463, Prague, Czech
Republic, June 2007.
[14] Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. Scaling
web-based acquisition of entailment relations. In Proceedings of EMNLP, pages
41?48, 2004.
[15] Rui Wang and Gu?nter Neumann. Recognizing textual entailment using sen-
tence similarity based on dependency tree skeletons. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 36?
41, June 2007.
103
Coling 2010: Poster Volume, pages 250?258,
Beijing, August 2010
Topic models for meaning similarity in context
Georgiana Dinu
Dept. of Computational Linguistics
Saarland University
dinu@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Recent work on distributional methods for
similarity focuses on using the context
in which a target word occurs to derive
context-sensitive similarity computations.
In this paper we present a method for com-
puting similarity which builds vector rep-
resentations for words in context by mod-
eling senses as latent variables in a large
corpus. We apply this to the Lexical Sub-
stitution Task and we show that our model
significantly outperforms typical distribu-
tional methods.
1 Introduction
Distributional methods for word similarity ((Lan-
dauer and Dumais, 1997), (Schuetze, 1998)) are
based on co-occurrence statistics extracted from
large amounts of text. Typically, each word is
assigned a representation as a point in a high-
dimensional space, where the dimensions rep-
resent contextual features such as co-occurring
words. Following this, meaning relatedness
scores are computed by using various similarity
measures on the vector representations.
One of the major issues that all distributional
methods have to face is sense ambiguity. Since
vector representations reflect mixtures of uses ad-
ditional methods have to be employed in order to
capture specific meanings of a word in context.
Consider the occurrence of verb shed in the fol-
lowing SemEval 2007 Lexical Substitution Task
(McCarthy and Navigli, 2007) example:
Cats in the latent phase only have the virus internally ,
but feel normal and do not shed the virus to other cats and
the environment .
Human participants in this task provided words
such as transmit and spread as good substitutes
for shed in this context, however a vector space
representation of shed will not capture this infre-
quent sense.
For these reasons, recent work on distributional
methods for similarity such as (Mitchell and La-
pata, 2008) (Erk and Pado?, 2008) (Thater et al,
2009) focuses on using the context in which a tar-
get word occurs to derive context-sensitive simi-
larity computations.
In this paper we present a method for comput-
ing similarity which builds vector representations
for words in context. Most distributional methods
so far extract representations from large texts, and
only as a follow-on step they either 1) alter these
in order to reflect a disambiguated word (such
as (Erk and Pado?, 2008)) or 2) directly asses the
appropriateness of a similarity judgment, given a
specific context (such as (Pantel et al, 2007)). Our
approach differs from this as we assume ambigu-
ity of words at the, initial, acquisition step, by en-
coding senses of words as a hidden variable in the
text we process.
In this paper we focus on a particular distribu-
tional representation inspired by (Lin and Pantel,
2001a) and induce context-sensitive similarity be-
tween phrases represented as paths in dependency
graphs. It is inspired by recent work on topic mod-
els and it deals with sense-ambiguity in a natural
manner by modeling senses as latent variables in
a large corpus. We apply this to the Lexical Sub-
stitution Task and we show that our model outper-
forms the (Lin and Pantel, 2001a) method by in-
ducing context-appropriate similarity judgments.
250
2 Related work
Discovery of Inference Rules from Text (DIRT)
A popular distributional method for meaning re-
latedness is the DIRT algorithm for extracting in-
ference rules (Lin and Pantel, 2001a). In this al-
gorithm a pattern is a noun-ending path in a de-
pendency graph and the goal is to acquire pairs of
patterns for which entailment holds (in at least one
direction) such as (X solve Y, X find solution to Y).
The method can be seen a particular instance
of a vector space. Each pattern is represented by
the sets of its left hand side (X) and right hand
side (Y) noun fillers in a large corpus. Two pat-
terns are compared in the X-filler space, and cor-
respondingly in the Y-filler space by using the Lin
similarity measure:
simLin(v, w) =
?
i?I(v)?I(w)(vi + wi)?
i?I(v) vi +
?
l?I(w)wi
where values in v and w are point-wise mutual
information, and I(?) gives the indices of positive
values in a vector.
The final similarity score between two patterns
is obtained by multiplying the X and Y similarity
scores. Table 1 shows a fragment of a DIRT-like
vector space.
.. case problem ..
(X solve Y, Y) .. 6.1 4.4 ..
(X settle Y, Y) .. 5.2 5.9 ..
Table 1: DIRT-like vector representation in the Y-filler
space. The values represent mutual information.
Further on, this similarity method is used for
the task of paraphrasing. A total set of patterns
is extracted from a large corpus and each of them
can be paraphrased by returning its most similar
patterns, according to the similarity score. Al-
though relatively accurate1, it has been noted (Lin
and Pantel, 2001b) that the paraphrases extracted
this way reflect, as expected, various meanings,
and that a context-sensitive representation would
be appropriate.
1Precision is estimated to lie around 50% for the most
confident paraphrases
Context-sensitive extensions of DIRT (Pantel
et al, 2007) and (Basili et al, 2007) focus on mak-
ing DIRT rules context-sensitive by attaching ap-
propriate semantic classes to the X and Y slots of
an inference rule. For this purpose, the initial step
in their methods is to acquire an inference rule
database, using the DIRT algorithm. Following
this, given an inference rule, they identify seman-
tic classes for the X and Y fillers which make the
application of the rule appropriate. For this (Pan-
tel et al, 2007) build a set of semantic classes us-
ing WordNet in one case and CBC clustering al-
gorithm in the other; for each rule, they use the
overlap of the fillers found in the input corpus as
an indicator of the correct semantic classes. The
same idea is used in (Basili et al, 2007) where,
this time, the X and Y fillers are clustered for each
rule individually; these nouns are clustered us-
ing an LSA-vector representation extracted from
a large corpus.
(Connor and Roth, 2007) take a slightly differ-
ent approach as they attempt to classify the con-
text of a rule as appropriate or not, again using
the overlap of fillers as an indicator. They all
show improvement over DIRT by evaluating on
occurrences of rules in context which are anno-
tated as correct/incorrect by human participants.
On a common data set (Pantel et al, 2007) and
(Basili et al, 2007) achieve significant improve-
ments over DIRT at 95% confidence level when
employing the clustering methods. (Szpektor et
al., 2008) propose a general framework for these
methods and show that some of these settings ob-
tain significant (level 0.01) improvements over the
DIRT algorithm on data derived from the ACE
2005 event detection task.
Related work on topic models Topic models
have been previously used for semantic tasks.
Work such as (Cai et al, 2007) or (Boyd-Graber et
al., 2007) use the document-level topics extracted
with Latent Dirichlet Allocation (LDA) as indi-
cators of meanings for word sense disambigua-
tion. More related to our work are (Brody and
Lapata, 2009) or (Toutanova and Johnson, 2008)
who use LDA-based models which induce latent
variables from task-specific data rather than from
simple documents.
251
(Brody and Lapata, 2009) apply such a model
for word sense induction on a set of 35 target
nouns. They assume senses as latent variables and
context features as observations; unlike our model
they induce local senses specific to every target
word by estimating separate models with the final
goal of explicitly inducing word senses.
(Toutanova and Johnson, 2008) use an LDA-
based model for semi-supervised part-of-speech
tagging. They build a word context model in
which each token involves: generating a distri-
bution over tags, sampling a tag, and finally gen-
erating context words according to a tag-specific
word distribution (context words are observa-
tions). Their model achieves highest performance
when combined with a ambiguity class compo-
nent which uses a dictionary for possible tags of
target words.
Both these papers show improvements over
state-of-the-art systems for their tasks.
3 Generative model for similarity in
context
We develop a method for computing similarity of
patterns in context, i.e. patterns with instantiated
X and Y values. We do not enhance the repre-
sentation of an inference rule with sense (context-
appropriateness) information but rather focus on
the task of assigning similarity scores to such pairs
of instantiated patterns. Unlike previous work, we
do not employ any other additional resources, in-
vestigating this way whether structurally richer in-
formation can be learned from the same input co-
occurrence matrix as the original DIRT method.
Our model, as well as the DIRT algorithm,
uses context information extracted from large
corpora to learn similarities between patterns;
however ideally we would like to learn contex-
tual preferences (or, in general, some form of
sense-disambiguation) for these patterns. This is
achieved in our model by assuming an intermedi-
ate layer consisting of meanings (senses): the con-
text surrounding a pattern is indicative of mean-
ings, and preference for some meanings gives the
characterization of a pattern.
For this we use a generative model inspired
by Latent Dirichlet Allocation (Blei et al, 2003)
(Griffiths and Steyvers, 2004) which is success-
X solve Y
we-X:122, country-X:89, government-X:82,
it-X:69,..., problem-Y:1088, issue-Y:134,
crisis-Y:99, dispute-Y:78,...
Table 2: Fragments of the document associated
to X solve Y. we-X: 122 indicates that X solve Y
occurs 122 times with we as an X filler.
fully employed for modeling collections of doc-
uments and the underlying topics which occur in
them. The statistical model is characterized by the
following distributions:
wi|zi, ?zi Discrete(?zi)
?z Dirichlet(?)
zi|?p Discrete(?p)
?p Dirichlet(?)
?p is the distribution over meanings associated
to a pattern p and ?z is the distribution over words
associated to a meaning z. The occurrence of
each filler word wi with a pattern p, is then gener-
ated by sampling 1) a meaning conditioned on the
meaning distribution associated to p: zi|?p and 2)
a word conditioned on the word distribution asso-
ciated to the meaning zi: wi|zi, ?zi . ?p and ?z are
assumed to be Dirichlet distributions with param-
eters ? and ?.
The set of context words (X and Y fillers) oc-
curring with a pattern p form the document (in
LDA terms) associated to a pattern p. Table 2 lists
a fragment of the document associated to pattern
X solve Y. These are built simply by listing for
each pattern, occurrence counts with specific filler
words. Since we want our model to differentiate
between X and Y fillers, words occurring as fillers
are made disjoint by adding a corresponding suf-
fix.
The total set of such documents extracted from
a large corpus is then used for estimating the
model. We use Gibbs sampling2 and the result
is a set of samples from P (z|w) (i.e. mean-
ing assignments for each occurring filler word)
from which ?p (pattern-meaning distributions)
and ?z(meaning-word distributions) can be esti-
mated.
Our model has the advantage that, once these
2http://gibbslda.sourceforge.net/
252
distributions are estimated, given a pattern p and a
context wn, in-context vector representations can
be built in a straightforward manner.
Meaning representation in-context Let K be
the assumed number of meanings, (z1, ..., zK).
We associate to a pattern in context (p,wn), the
K-dimensional vector containing for each mean-
ing zi (i : 1..K), the probability of zi, conditioned
on pattern p and context word wn:
vec(p, wn) = (P (z1|wn, p), ..., P (zK |wn, p))
(1)
where,
P (zi|wn, p) =
P (zi, p)P (wn|zi)
?Ki=1P (zi, p)P (wn|zi)
(2)
This is the probability that wn is generated by
meaning zi conditioned on p, therefore, the proba-
bility that pattern p has meaning zi in context wn,
exactly the concept we want to model.
Meaning representation out-of-context We
can also associate to pattern p an out-of-context
vector representation: the K-dimensional vector
representing its distribution over meanings:
vec(p) = (P (z1|p), ..., P (zK |p)) (3)
This can be seen as a dimensionality reduction
method, since we bring vector representations to a
lower dimensional space over (ideally) meaning-
ful concepts.
From the generative model we obtain the de-
sired distributions P (zi|p) = ?pi and P (wn|zi) =
?zin .3
Computing similarity between patterns The
similarity between patterns occurring with X and
Y filler-words is computed following (Lin and
Pantel, 2001a) by multiplying the similarities ob-
tained separately in the X and Y spaces.:
sim((wX1, p1, wY 1)(wX2, p2, wY 2)) =
sim(vec(p1, wX1), vec(p2, wX2))?
sim(vec(p1, wY 1), vec(p2, wY 2))
(4)
3For similarity in context, we use the conditional P (zi|p)
instead of the joint P (zi, p) which is computationally equiv-
alent for the paraphrasing setting.
we subj???? make dobj????statement
we subj???? give dobj????statement good
we subj???? prepare dobj????statement bad
Table 3: Development set: good/bad substitutes
for we subj???? make dobj????statement
Out-of-context similarity is defined in a straight-
forward manner:
sim(p1, p2) = sim(vec(p1, ), vec(p2)) (5)
4 Evaluation setup
In this paper we evaluate our model on
computing similarities between pairs of the
type (X, pattern, Y ), (X, pattern?, Y ) where
two different patterns are compared in identical
contexts. For this we use the Semeval Lexical
Substitution dataset, which requires human par-
ticipants to provide substitutes for a set of target
words occurring in different contexts. This sec-
tion describes the evaluation methodology for this
data as well as the automatically generated data
set we use for development.
Development set For finding good model pa-
rameters, we use the SemCor corpus providing
text in which all content words are tagged with
WordNet 1.6 senses. We used this data in the fol-
lowing manner: We parse the text using Stanford
parser and extract occurrences of triples (X, pat-
tern, Y). Given these triples we generate good and
bad substitutes for them: the good substitutes are
generated by replacing the words occurring in the
patterns with sense-appropriate synonyms, while
bad ones are obtained by substitution with syn-
onyms corresponding to the rest of the senses (the
wrong senses). The synonyms are extracted from
WordNet 1.6 synsets using the sense annotation
present in the text.
For evaluation we feed the models pairs of in-
stantiated patterns. One of them is the original
phrase encountered in the data, and the other one
is a good/bad substitute for it. Table 3 shows an
example of the data.
We evaluate the output of a system by requir-
ing that, for each instance, every good substitute
is scored more similar to the original phrase than
253
every bad substitute. This leads to an accuracy
score which can be compared against a random
baseline of 50%.
The data set obtained is far from being a very
reliable resource for the task of lexical substitu-
tion, however this method of generating data has
the advantage of producing a large number of in-
stances which can be easily acquired from any
sense-annotated data set. In our experiments we
use the Brown2 fragment from which we extract
over 3000 instances of patterns in context.
Lexical substitution task The Lexical Substitu-
tion Task (McCarthy and Navigli, 2007) presents
5 annotators with a set of target words, each in
different context sentences. The task requires
the participants to provide appropriate substitute
words for each occurrence of the target words.
We use this data similarly to (Erk and Pado?,
2008) and (Thater et al, 2009) and for each target
word, we pool together all the substitutes given
for all context sentences. Similarly to the Sem-
Cor data, we do not use the entire sentence as a
context as we extract only patterns containing tar-
get words together with their X and Y fillers. The
models assign similarity scores to each candidate
by comparing them to the pattern occurring in the
original sentence. A ranked list of candidates is
obtained which in turn is compared with the sub-
stitutes provided by the participants. Table 4 gives
an example of this data set (for each substitute we
list the number of participants providing it).
To evaluate the performance of a model we em-
ploy two similarity measures, which capture dif-
ferent aspects of the task. Kendall ? rank coeffi-
cient measures the correlation between two ranks;
since the gold ranking is usually only a partial or-
der, we use ?b which makes adjustments for ties.
We employ a second evaluation measure: Gener-
alized Average Precision (Kishida, 2005). This is
a measure inspired from information retrieval and
has been previously used for evaluating this task
(Thater et al, 2009). It evaluates a system on its
ability to retrieve correct substitutes using the gold
ranking together with the associated confidence
scores. The confidence scores are in turn deter-
mined by the number of people providing each
substitute.
pattern human substitutes
study subj???? shed dobj????light throw 3, reveal 2,
shine 1
cat subj???? shed dobj????virus spread 2, pass 2,
transmit 2, emit 1
Table 4: Lexical substitution data set: target verb
shed
5 Experiments
5.1 Model selection
The data we use to estimate our models is ex-
tracted from a GigaWord fragment containing ap-
proximately 100 million tokens. We parse the
text with Stanford dependency parser to obtain de-
pendency graphs from which we extract paths to-
gether with counts of their left and right fillers.
We extract paths containing at most four words,
including the two noun anchors. Furthermore
we impose a frequency threshold on patterns and
words, leading us to a collection of?80 000 paths,
with filler nouns over a vocabulary of ?40 000
words.
We estimate a total number of 20 models. We
set ? = 0.01 as previous work (Wang et al, 2009)
reports good results with this value. For parame-
ter ? we test 4 settings: ?1 = 2K and ?4 = 50Kwhich are reported in the literature as good ((Por-
teous et al, 2008) and (Griffiths and Steyvers,
2004)), as well as 2 intermediate values: ?2 = 5Kand ?3 = 10K . We test a set of 5 K values:
{800, 1000, 1200, 1400, 1600}. These are chosen
to be large since they represent the global set of
meanings shared by all the patterns in the collec-
tion.
As vector similarity measure we test scalar
product (sp), which in our model is interpreted
as the probability that two patterns share a com-
mon meaning. Additionally we test cosine (cos)
similarity and inverse Jensen-Shannon (JS) diver-
gence, which is a popular measure for comparing
probability distributions:
JSD(v, w) = 12KLD(v|m) +
1
2KLD(w|m)
with m = 12(v + w) and KLD the stan-dard Kullback-Leibler divergence: KLD(v|w) =
?ivilog( viwi ).
254
We perform both in-context (using eq. (4))
as well as out-of-context computations (eq. (5)).
Similarly to previous work (Erk and Pado?, 2008),
we observe that comparing a contextualized repre-
sentation against a non-contextualized one brings
significant improvements over comparing two
representations in context. We assume this is spe-
cific to the type of data we work with, in which
two patterns are compared in an identical context,
rather than across different contexts; we therefore
compute context-sensitive similarities by contex-
tualizing just the target word.
Number of topics Although the parameters
cover relatively large ranges the models perform
surprisingly similar across different ? and K val-
ues, as well as across all three similarity measures.
For sp similarity, the accuracy scores we obtain
are in the range [56.5-59.5] with a average devi-
ation from the mean of just 0.8%; similar figures
are obtained using the other similarity measures.
Figure 1 plots the average of the accuracy scores
using sp as similarity measure, across different
number of topics. A small preference for higher
K values is observed, all models performing con-
sistently good at 1200, 1400 and 1600 topics.
Figure 1: Average accuracy across the 5 K values.
Mixture models This leads us to attempting a
very simple mixture model, which computes the
similarity score between two patterns as the aver-
age similarity obtained across a number of mod-
els. For each ? setting, we mix models across the
three best topic numbers: {1200, 1400, 1600}. In
Figure 2 we plot this mixture model together with
the three single ones, at each ? value. It can be
Figure 2: Mixture model {1200, 1400, 1600}
(bold) vs. the three individual models, across the
4 ? values.
noticed that the mixture model improves over all
three single models for three out of the four ? val-
ues.
In-context vs. out-of-context computations
Further on we compare in-context versus out-of-
context computations. The similarity measures
exhibit significant differences in regard to this as-
pect. In Figure 3 we plot in-context vs. out-of-
context computations using scalar product (left)
and JS (right) with the mixture model previously
defined, plotted at different ? values. For sp
in-context computations significantly outperform
out-of-context ones and the two intermediate al-
pha values seem to be the best. However for JS
similarity the out-of-context computations are sig-
nificantly better and a clear preference for smaller
? values can be observed.
Finally, on the test data, we use the following
models (where GMmixt/sing,sim stands for a mix-
ture or single model with similarity measure sim):
? GMmixt,sp/cos
mixt({1200, 1400, 1600}x{?2, ?3})
? GMmixt,js
mixt({1200, 1400, 1600}x{?1, ?2})
? GMsing,sp: (1600, ?2)
? GMsing,cos/js: (1200, ?1)
The mixture models are build based on the ob-
servations previously made while the single mod-
255
Model In-context Out-of-context
GMmixt,sp 59.89 58.68
GMmixt,cos 59.50 58.67
GMmixt,js 59.73 60.68
GMsing,sp 59.48 58.86
GMsing,cos 59.43 57.87
GMsing,js 58.65 59.36
Table 5: Accuracy results on development set
els are the best performing ones, for each similar-
ity measure. The accuracy scores obtained with
these models are given in Table 5. Mixture models
generally outperform single ones and in-context
computations outperform out-of-context ones for
sp and cos. The best results on the development
set are however achieved by out-of-context mod-
els using JS as similarity measure.
Figure 3: In-context (bold) vs. out-of-context
computations across the 4 ? values using scalar
product (left) and JS (right)
5.2 Results
Table 6 shows the results for the Lexical Substitu-
tion data set. We use the subset of the data con-
taining sentences in which the target word is part
of a syntactic path which is present in the total col-
lection of patterns. This leads to a set containing
165 instances of patterns in context, most of these
containing target verbs.
Since sp and cos measures perform very sim-
ilarly we only list results with cosine similarity
measure. In addition to the models with settings
determined on the development set, we also test
a very simple mixture model: GMmixt?all,sim.
This simply averages over all 20 configurations
and its purpose is to investigate the necessity of a
carefully selected mixture model.
It can be noticed that all GM mixture mod-
els outperform DIRT, which is reflected in both
Model ?b GAP
Random 0.0 34.91
DIRT 14.53 48.06
GMmixt,cos 22.35 52.04
GMmixt,js 18.17 50.80
GMmixt?all,cos 20.42 51.13
GMmixt?all,js 19.03 51.15
GMsing,cos 15.10 48.20
GMsing,js 14.17 47.97
Table 6: Results on Lexical Substitution data
similarity measures. Notably the very simple
model which averages all the configurations im-
plemented is surprisingly performant. Using ran-
domized significance testing we obtained that
GMmixt,cos is significantly better than DIRT at p
level 1e-03 on both GAP and ?b. GMmixt?all,cos
outperforms DIRT at level 0.05.
In terms of similarity measures, the observa-
tions made on the development set hold, as for
the in-context computations cos and sp outper-
form JS. However, unlike on the development
data, the single models perform much worse than
the mixture ones which can indicate that the de-
velopment set is not perfectly suited for choosing
model parameters.
Out-of-context computations for all models and
all similarity measures are significantly outper-
formed, leading to scores in ranges [11-14] ?b and
[45-48] GAP.
In Table 7 we list the rankings produced by
three models for the target word shed in con-
text virus obj??? shed prep???? to pobj???? cat. As it
can be observed, the model performing context-
sensitive computations GMmixt,cos-in-context re-
turns a better ranking in comparison to theDIRT
and GMmixt,cos-out-of-context models.
6 Conclusion
We have addressed the task of computing meaning
similarity in context using distributional methods.
The specific representation we use follows (Lin
and Pantel, 2001a): we extract patterns (paths
in dependency trees which connect two nouns)
and we use the co-occurrence with these nouns
to build high-dimensional vectors. Using this data
256
virus obj??? shed prep???? to pobj???? cat
GMmixt,cos GMmixt,cos DIRT GOLD
in-context out-of-context
lose lose drop pass 2
drop drop lose spread 2
transmit relinquish give transmit 2
spread reveal transmit
pass pass spread
relinquish throw reveal
reveal spread relinquish
throw transmit throw
give give pass
Table 7: Ranks returned for virus obj??? shed prep???? to pobj???? cat
we develop a principled method to induce context-
sensitive representations by modeling the mean-
ing of a pattern as a latent variable in the input
corpus. We apply this model to the task of Lex-
ical Substitution and we show it allows the com-
putation of context-sensitive similarities; it signif-
icantly outperforms the original method, while us-
ing the exact same input data.
In future work, we plan to use our model for
generating paraphrases for patterns occurring in
context, a scenario closer to real applications than
out-of-context paraphrasing.
Finally, a formulation of our model in a typical
bag-of-words semantic space for word similarity
can be employed in a wider range of applications
and will allow comparison with other methods for
building context-sensitive vector representations.
7 Acknowledgments
This work was partially supported by DFG (IRTG
715).
References
Basili, Roberto, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
In Proceedings of RANLP 2007, Borovets, Bulgaria.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Boyd-Graber, Jordan, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Empirical Methods in Natural Language
Processing.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL ?09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 103?111, Morristown, NJ, USA. Association
for Computational Linguistics.
Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml:improving word sense disambiguation us-
ing topic features. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Connor, Michael and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In ECML ?07: Proceedings of the 18th Euro-
pean conference on Machine Learning, pages 104?
115, Berlin, Heidelberg. Springer-Verlag.
Erk, Katrin and Sabastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP 2008, Waikiki, Honolulu,
Hawaii.
Griffiths, T. L. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1):5228?5235, April.
Kishida, Kazuaki. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent seman-
tic analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Lin, Dekang and Patrick Pantel. 2001a. DIRT ? Dis-
covery of Inference Rules from Text. In Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01), San Francisco, CA.
257
Lin, Dekang and Patrick Pantel. 2001b. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4):343?360.
McCarthy, D. and R. Navigli. 2007. SemEval-2007
Task 10: English Lexical Substitution Task. In Pro-
ceedings of SemEval, pages 48?53, Prague.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Rochester, New
York.
Porteous, Ian, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling.
2008. Fast collapsed gibbs sampling for latent
dirichlet alocation. In KDD ?08: Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 569?
577, New York, NY, USA. ACM.
Schuetze, Hinrich. 1998. Automatic word sense dis-
crimination. Journal of Computational Linguistics,
24:97?123.
Szpektor, Idan, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT, pages 683?691, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Thater, Stefan, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of TextInfer ACL 2009.
Toutanova, Kristina and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. MIT Press, Cambridge, MA.
Wang, Yi, Hongjie Bai, Matt Stanton, Wen-Yen Chen,
and Edward Y. Chang. 2009. Plda: Parallel latent
dirichlet alocation for large-scale applications. In
Proc. of 5th International Conference on Algorith-
mic Aspects in Information and Management.
258
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162?1172,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Measuring Distributional Similarity in Context
Georgiana Dinu
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
dinu@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
The computation of meaning similarity as
operationalized by vector-based models has
found widespread use in many tasks ranging
from the acquisition of synonyms and para-
phrases to word sense disambiguation and tex-
tual entailment. Vector-based models are typ-
ically directed at representing words in isola-
tion and thus best suited for measuring simi-
larity out of context. In his paper we propose
a probabilistic framework for measuring sim-
ilarity in context. Central to our approach is
the intuition that word meaning is represented
as a probability distribution over a set of la-
tent senses and is modulated by context. Ex-
perimental results on lexical substitution and
word similarity show that our algorithm out-
performs previously proposed models.
1 Introduction
The computation of meaning similarity as op-
erationalized by vector-based models has found
widespread use in many tasks within natural lan-
guage processing (NLP). These range from the ac-
quisition of synonyms (Grefenstette, 1994; Lin,
1998) and paraphrases (Lin and Pantel, 2001) to
word sense disambiguation (Schuetze, 1998), tex-
tual entailment (Clarke, 2009), and notably informa-
tion retrieval (Salton et al, 1975).
The popularity of vector-based models lies in
their unsupervised nature and ease of computation.
In their simplest incarnation, these models repre-
sent the meaning of each word as a point in a
high-dimensional space, where each component cor-
responds to some co-occurring contextual element
(Landauer and Dumais, 1997; McDonald, 2000;
Lund and Burgess, 1996). The advantage of taking
such a geometric approach is that the similarity of
word meanings can be easily quantified by measur-
ing their distance in the vector space, or the cosine
of the angle between them.
Vector-based models do not explicitly identify the
different senses of words and consequently repre-
sent their meaning invariably (i.e., irrespective of co-
occurring context). Consider for example the adjec-
tive heavy which we may associate with the gen-
eral meaning of ?dense? or ?massive?. However,
when attested in context, heavy may refer to an over-
weight person (e.g., She is short and heavy but she
has a heart of gold.) or an excessive cannabis user
(e.g., Some heavy users develop a psychological de-
pendence on cannabis.).
Recent work addresses this issue indirectly with
the development of specialized models that repre-
sent word meaning in context (Mitchell and Lap-
ata, 2008; Erk and Pado?, 2008; Thater et al, 2009).
These methods first extract typical co-occurrence
vectors representing a mixture of senses and then use
vector operations to either obtain contextualized rep-
resentations of a target word (Erk and Pado?, 2008)
or a representation for a set of words (Mitchell and
Lapata, 2009).
In this paper we propose a probabilistic frame-
work for representing word meaning and measuring
similarity in context. We model the meaning of iso-
lated words as a probability distribution over a set of
latent senses. This distribution reflects the a priori,
out-of-context likelihood of each sense. Because
sense ambiguity is taken into account directly in the
1162
vector construction process, contextualized meaning
can be modeled naturally as a change in the origi-
nal sense distribution. We evaluate our approach on
word similarity (Finkelstein et al, 2002) and lexical
substitution (McCarthy and Navigli, 2007) and show
improvements over competitive baselines.
In the remainder of this paper we give a brief
overview of related work, emphasizing vector-based
approaches that compute word meaning in context
(Section 2). Next, we present our probabilistic
framework and different instantiations thereof (Sec-
tions 3 and 4). Finally, we discuss our experimental
results (Sections 5 and 6) and conclude the paper
with future work.
2 Related work
Vector composition methods construct representa-
tions that go beyond individual words (e.g., for
phrases or sentences) and thus by default obtain
word meanings in context. Mitchell and Lapata
(2008) investigate several vector composition op-
erations for representing short sentences (consist-
ing of intransitive verbs and their subjects). They
show that models performing point-wise multiplica-
tion of component vectors outperform earlier pro-
posals based on vector addition (Landauer and Du-
mais, 1997; Kintsch, 2001). They argue that multi-
plication approximates the intersection of the mean-
ing of two vectors, whereas addition their union.
Mitchell and Lapata (2009) further show that their
models yield improvements in language modeling.
Erk and Pado? (2008) employ selectional prefer-
ences to contextualize occurrences of target words.
For example, the meaning of a verb in the presence
of its object is modeled as the multiplication of the
verb?s vector with the vector capturing the inverse
selectional preferences of the object; the latter are
computed as the centroid of the verbs that occur
with this object. Thater et al (2009) improve on this
model by representing verbs in a second order space,
while the representation for objects remains first or-
der. The meaning of a verb boils down to restricting
its vector to the features active in the argument noun
(i.e., dimensions with value larger than zero).
More recently, Reisinger and Mooney (2010)
present a method that uses clustering to pro-
duce multiple sense-specific vectors for each word.
Specifically, a word?s contexts are clustered to pro-
duce groups of similar context vectors. An aver-
age prototype vector is then computed separately
for each cluster, producing a set of vectors for each
word. These cluster vectors can be used to determine
the semantic similarity of both isolated words and
words in context. In the second case, the distance
between prototypes is weighted by the probability
that the context belongs to the prototype?s cluster.
Erk and Pado? (2010) propose an exemplar-based
model for capturing word meaning in context. In
contrast to the prototype-based approach, no cluster-
ing takes place, it is assumed that there are as many
senses as there are instances. The meaning of a word
in context is the set of exemplars most similar to it.
Unlike Reisinger and Mooney (2010) and Erk and
Pado? (2010) our model is probabilistic (we repre-
sent word meaning as a distribution over a set of la-
tent senses), which makes it easy to integrate and
combine with other systems via mixture or product
models. More importantly, our approach is concep-
tually simpler as we use a single vector representa-
tion for isolated words as well as for words in con-
text. A word?s different meanings are simply mod-
eled as changes in its sense distribution. We should
also point out that our approach is not tied to a spe-
cific sense induction method and can be used with
different variants of vector-space models.
3 Meaning Representation in Context
In this section we first describe how we represent
the meaning of individual words and then move on
to discuss our model of inducing meaning represen-
tations in context.
Observed Representations Most vector space
models in the literature perform computations on
a co-occurrence matrix where each row repre-
sents a target word, each column a document or
another neighboring word, and each entry their
co-occurrence frequency. The raw counts are typ-
ically mapped into the components of a vector in
some space using for example conditional probabil-
ity, the log-likelihood ratio or tf-idf weighting. Un-
der this representation, the similarity of word mean-
ings can be easily quantified by measuring their dis-
tance in the vector space, the cosine of the angle be-
tween them, or their scalar product.
1163
Our model assumes the same type of input data,
namely a co-occurrence matrix, where rows corre-
spond to target words and columns to context fea-
tures (e.g., co-occurring neighbors). Throughout
this paper we will use the notation ti with i : 1..I
to refer to a target word and cj with j : 1..J to refer
to context features. A cell (i, j) in the matrix rep-
resents the frequency of occurrence of target ti with
context feature cj over a corpus.
Meaning Representation over Latent Senses
We further assume that the target words ti i : 1...I
found in a corpus share a global set of meanings
or senses Z = {zk|k : 1...K}. And therefore the
meaning of individual target words can be described
as a distribution over this set of senses. More for-
mally, a target ti is represented by the following vec-
tor:
v(ti) = (P(z1|ti), ...,P(zK|ti)) (1)
where component P (z1|ti) is the probability of
sense z1 given target word ti, component P (z2|ti)
the probability of sense z2 given ti and so on.
The intuition behind such a representation is that
a target word can be described by a set of core mean-
ings and by the frequency with which these are at-
tested. Note that the representation in (1) is not
fixed but parametrized with respect to an input cor-
pus (i.e., it only reflects word usage as attested in
that corpus). The senses z1 . . . zK are latent and can
be seen as a means of reducing the dimensionality
of the original co-occurrence matrix.
Analogously, we can represent the meaning of a
target word given a context feature as:
v(ti, cj) = (P(z1|ti, cj), ...,P(zK|ti, cj)) (2)
Here, target ti is again represented as a distribution
over senses, but is now modulated by a specific con-
text cj which reflects actual word usage. This distri-
bution is more ?focused? compared to (1); the con-
text helps disambiguate the meaning of the target
word, and as a result fewer senses will share most
of the probability mass.
In order to create the context-aware representa-
tions defined in (2) we must estimate the proba-
bilities P (zk|ti, cj) which can be factorized as the
product of P (ti, zk), the joint probability of target ti
and latent sense zk, and P (cj |zk, ti), the conditional
probability of context cj given target ti and sense zk:
P (zk|ti, cj) =
P (ti, zk)P (cj |zk, ti)
?
k P (ti, zk)P (cj |zk, ti)
(3)
Problematically, the term P (cj |zk, ti) is difficult to
estimate since it implies learning a total number of
K ? I J-dimensional distributions. We will there-
fore make the simplifying assumption that target
words ti and context features cj are conditionally in-
dependent given sense zk:
P (zk|ti, cj) ?
P (zk|ti)P (cj |zk)
?
k P (zk|ti)P (cj |zk)
(4)
Although not true in general, the assumption is rela-
tively weak. We do not assume that words and con-
text features occur independently of each other, but
only that they are generated independently given an
assigned meaning. A variety of latent variable mod-
els can be used to obtain senses z1 . . . zK and es-
timate the distributions P (zk|ti) and P (cj |zk); we
give specific examples in Section 4.
Note that we abuse terminology here, as the
senses our models obtain are not lexicographic
meaning distinctions. Rather, they denote coarse-
grained senses or more generally topics attested in
the document collections our model is trained on.
Furthermore, the senses are not word-specific but
global (i.e., shared across all words) and modulated
either within or out of context probabilistically via
estimating P (zk|ti, cj) and P (zk|ti), respectively.
4 Parametrizations
The general framework outlined above can be
parametrized with respect to the input co-occurrence
matrix and the algorithm employed for inducing the
latent structure. Considerable latitude is available
when creating the co-occurrence matrix, especially
when defining its columns, i.e., the linguistic con-
texts a target word is attested with. These con-
texts can be a small number of words surrounding
the target word (Lund and Burgess, 1996; Lowe
and McDonald, 2000), entire paragraphs, documents
(Salton et al, 1975; Landauer and Dumais, 1997)
or even syntactic dependencies (Grefenstette, 1994;
Lin, 1998; Pado? and Lapata, 2007).
1164
Analogously, a number of probabilistic models
can be employed to induce the latent senses. Ex-
amples include Probabilistic Latent Semantic Anal-
ysis (PLSA, Hofmann (2001)), Probabilistic Prin-
cipal Components Analysis (Tipping and Bishop,
1999), non-negative matrix factorization (NMF, Lee
and Seung (2000)), and latent Dirichlet alocation
(LDA, Blei et al (2003)). We give a more detailed
description of the latter two models as we employ
them in our experiments.
Non-negative Matrix Factorization Non-
negative matrix factorization algorithms approx-
imate a non-negative input matrix V by two
non-negative factors W and H , under a given
loss function. W and H are reduced-dimensional
matrices and their product can be regarded as a
compressed form of the data in V :
VI,J ?WI,KHK,J (5)
where W is a basis vector matrix and H is an en-
coded matrix of the basis vectors in equation (5).
Several loss functions are possible, such as mean
squared error and Kullback-Leibler (KL) diver-
gence. In keeping with the formulation in Sec-
tion 3 we opt for a probabilistic interpretation of
NMF (Gaussier and Goutte, 2005; Ding et al, 2008)
and thus minimize the KL divergence between WH
and V .
min
?
i,j
(Vi,j log
Vi,j
WHi,j
? Vi,j +WHi,j) (6)
Specifically, we interpret matrix V as
Vij = P (ti, cj), and matrices W and H as P (ti, zk)
and P (cj |zk), respectively. We can also ob-
tain the following more detailed factorization:
P (ti, cj) =
?
k P (ti)P (zk|ti)P (cj |zk).
Le WH denote the factors in a NMF decom-
position of an input matrix V and B be a diag-
onal matrix with Bkk =
?
j Hkj . B
?1H gives a
row-normalized version of H . Similarly, given
matrix WB, we can define a diagonal matrix A,
with Aii =
?
k(WB)ik. A
?1WB row-normalizes
matrix WB. The factorization WH can now be re-
written as:
WH=AA?1WBB?1H=A(A?1WB)(B?1H)
which allows us to interpret A as P (ti), A?1WB
as P (zk|ti) and B?1H as P (cj |zk). These interpre-
tations are valid since the rows of A?1WB and of
B?1H sum to 1, matrix A is diagonal with trace 1
because elements in WH sum to 1, and all entries
are non-negative.
Latent Dirichlet Allocation LDA (Blei et al,
2003) is a probabilistic model of text generation.
Each document d is modeled as a distribution
over K topics, which are themselves characterized
by distributions over words. The individual words
in a document are generated by repeatedly sampling
a topic according to the topic distribution and then
sampling a single word from the chosen topic.
More formally, we first draw the mixing propor-
tion over topics ?d from a Dirichlet prior with pa-
rameters ?. Next, for each of the Nd words wdn in
document d, a topic zdn is first drawn from a multi-
nomial distribution with parameters ?dn. The prob-
ability of a word token w taking on value i given
that topic z = j is parametrized using a matrix ?
with bij = P (w = i|z = j). Integrating out ?d?s
and zdn?s, gives P (D|?, ?), the probability of a cor-
pus (or document collection):
M?
d=1
?
P (?d|?)
?
?
Nd?
n=1
?
zdn
P (zdn|?d)P (wdn|zdn, ?)
?
?d?d
The central computational problem in topic
modeling is to obtain the posterior distri-
bution P (?, z|w, ?, ?) of the hidden vari-
ables z = (z1, z2, . . . , zN ). given a docu-
ment w = (w1, w2, . . . , wN ). Although this
distribution is intractable in general, a variety
of approximate inference algorithms have been
proposed in the literature. We adopt the Gibbs
sampling procedure discussed in Griffiths and
Steyvers (2004). In this model, P (w = i|z = j) is
also a Dirichlet mixture (denoted ?) with symmetric
priors (denoted ?).
We use LDA to induce senses of target words
based on context words, and therefore each row ti
in the input matrix transforms into a document. The
frequency of ti occurring with context feature cj is
the number of times word cj is encountered in the
?document? associated with ti. We train the LDA
model on this data to obtain the ? and ? distribu-
1165
tions. ? gives the sense distributions of each tar-
get ti: ?ik = P (zk|ti) and ? the context-word dis-
tribution for each sense zk: ?kj = P (cj |zk).
5 Experimental Set-up
In this section we discuss the experiments we per-
formed in order to evaluate our model. We describe
the tasks on which it was applied, the corpora used
for model training and our evaluation methodology.
Tasks The probabilistic model presented in Sec-
tion 3 represents words via a set of induced senses.
We experimented with two types of semantic space
based on NMF and LDA and optimized parameters
for these models on a word similarity task. The
latter involves judging the similarity sim(ti, t?i) =
sim(v(ti), v(t?i)) of words ti and t
?
i out of context,
where v(ti) and v(t?i) are obtained from the output of
NMF or LDA, respectively. In our experiments we
used the data set of Finkelstein et al (2002). It con-
tains 353 pairs of words and their similarity scores
as perceived by human subjects.
The contextualized representations were next
evaluated on lexical substitution (McCarthy and
Navigli, 2007). The task requires systems to find
appropriate substitutes for target words occurring in
context. Typically, systems are given a set of substi-
tutes, and must produce a ranking such that appro-
priate substitutes are assigned a higher rank com-
pared to non-appropriate ones. We made use of the
SemEval 2007 Lexical Substitution Task benchmark
data set. It contains 200 target words, namely nouns,
verbs, adjectives and adverbs, each of which occurs
in 10 distinct sentential contexts. The total set con-
tains 2,000 sentences. Five human annotators were
asked to provide substitutes for these target words.
Table 1 gives an example of the adjective still and
its substitutes.
Following Erk and Pado? (2008), we pool together
the total set of substitutes for each target word.
Then, for each instance the model has to produce a
ranking for the total substitute set. We rank the can-
didate substitutes based on the similarity of the con-
textualized target and the out-of-context substitute,
sim(v(ti, cj), v(t?i)), where ti is the target word, cj a
context word and t?i a substitute. Contextualizing
just one of the words brings higher discriminative
power to the model rather than performing compar-
Sentences Substitutes
It is important to apply the
herbicide on a still day, be-
cause spray drift can kill
non-target plants.
calm (5) not-windy (1)
windless (1)
A movie is a visual docu-
ment comprised of a series
of still images.
motionless (3) unmov-
ing (2) fixed (1) sta-
tionary (1) static (1)
Table 1: Lexical substitution data example for the adjec-
tive still ; numbers in parentheses indicate the frequency
of the substitute.
isons with the target and its substitute embedded in
an identical context (see also Thater et al (2010) for
a similar observation).
Model Training All the models we experimented
with use identical input data, i.e., a bag-of-words
matrix extracted from the GigaWord collection of
news text. Rows in this matrix are target words and
columns are their co-occurring neighbors, within a
symmetric window of size 5. As context words, we
used a vocabulary of the 3,000 most frequent words
in the corpus.1
We implemented the classical NMF factorization
algorithm described in Lee and Seung (2000). The
input matrix was normalized so that all elements
summed to 1. We experimented with four dimen-
sions K: [600 ? 1000] with step size 200. We ran
the algorithm for 150 iterations to obtain factors W
and H which we further processes as described in
Section 4 to obtain the desired probability distribu-
tions. Since the only parameter of the NMF model
is the factorization dimension K, we performed two
independent runs with each K value and averaged
their predictions.
The parameters for the LDA model are the num-
ber of topicsK and Dirichlet priors ? and ?. We ex-
perimented with topics K: [600? 1400], again with
step size 200. We fixed ? to 0.01 and tested two val-
ues for ?: 2K (Porteous et al, 2008) and
50
K (Griffiths
and Steyvers, 2004). We used Gibbs sampling on
the ?document collection? obtained from the input
matrix and estimated the sense distributions as de-
scribed in Section 4. We ran the chains for 1000 iter-
1The GigaWord corpus contains 1.7B words; we scale down
all the counts by a factor of 70 to speed up the computation of
the LDA models. All models use this reduced size input data.
1166
ations and averaged over five iterations [600?1000]
at lag 100 (we observed no topic drift).
We measured similarity using the scalar prod-
uct, cosine, and inverse Jensen-Shannon (IJS) diver-
gence (see (7), (8), and (9), respectively):
sp(v, w) =< v,w >=
?
i
viwi (7)
cos(v, w) =
?v, w?
||v|| ||w||
(8)
IJS(v, w) =
1
JS(v,w)
(9)
JS(v,w) =
1
2
KL(v|m) +
1
2
KL(w|m) (10)
where m is a shorthand for 12(v + w) and
KL the Kullback-Leibler divergence, KL(v|w) =
?
i vilog(
vi
wi
).
Among the above similarity measures, the scalar
product has the most straightforward interpretation
as the probability of two targets sharing a common
meaning (i.e., the sum over all possible meanings).
The scalar product assigns 1 to a pair of identi-
cal vectors if and only if P (zi) = 1 for some i
and P (zj) = 0,?j 6= i. Thus, only fully disam-
biguated words receive a score of 1. Beyond similar-
ity, the measure also reflects how ?focused? the dis-
tributions in question are, as very ambiguous words
are unlikely to receive high scalar product values.
Given a set of context words, we contextualize the
target using one context word at a time and compute
the overall similarity score by multiplying the indi-
vidual scores.
Baselines Our baseline models for measuring sim-
ilarity out of context are Latent Semantic Analysis
(Landauer and Dumais, 1997) and a simple seman-
tic space without any dimensionality reduction.
For LSA, we computed the U?V SVD decompo-
sition of the original matrix to rank k = 1000. Any
decomposition of lower rank can be obtained from
this by setting rows and columns to 0. We evaluated
decompositions to ranks K: [200 ? 1000], at each
100 step. Similarity computations were performed
in the lower rank approximation matrix U?V , as
originally proposed in Deerwester et al (1990), and
in matrix U which maps the words into the concept
space. It is common to compute SVD decomposi-
tions on matrices to which prior weighting schemes
have been applied. We experimented with tf-idf
weighting and line normalization.
Our second baseline, the simple semantic space,
was based on the original input matrix on which
we applied several weighting schemes such as point-
wise mutual information, tf-idf, and line normaliza-
tion. Again, we measured similarity using cosine,
scalar product and inverse JS divergence. In addi-
tion, we also experimented with Lin?s (1998) simi-
larity measure:
lin(v, w) =
?
i?I(v)?I(w)(vi + wi)
?
i?I(v) vi +
?
l?I(w)wi
(11)
where the values in v and w are point-wise mutual
information, and I(?) gives the indices of positive
values in a vector.
Our baselines for contextualized similarity were
vector addition and vector multiplication which
we performed using the simple semantic space
(Mitchell and Lapata, 2008) and dimensionality
reduced representations obtained from NMF and
LDA. To create a ranking of the candidate substi-
tutes we compose the vector of the target with its
context and compare it with each substitute vector.
Given a set of context words, we contextualize the
target using each context word at a time and multi-
ply the individual scores.
Evaluation Method For the word similarity task
we used correlation analysis to examine the rela-
tionship between the human ratings and their cor-
responding vector-based similarity values. We re-
port Spearman?s ? correlations between the simi-
larity values provided by the models and the mean
participant similarity ratings in the Finkelstein et al
(2002) data set. For the lexical substitution task, we
compare the system ranking with the gold standard
ranking using Kendall?s ?b rank correlation (which is
adjusted for tied ranks). For all contextualized mod-
els we defined the context of a target word as the
words occurring within a symmetric context window
of size 5. We assess differences between models us-
ing stratified shuffling (Yeh, 2000).2
2Given two system outputs, the null hypothesis (i.e., that
the two predictions are indistinguishable) is tested by randomly
mixing the individual instances (in our case sentences) of the
two outputs. We ran a standard number of 10000 iterations.
1167
Model Spearman ?
SVS 38.35
LSA 49.43
NMF 52.99
LDA 53.39
LSAMIX 49.76
NMFMIX 51.62
LDAMIX 51.97
Table 2: Results on out of context word similarity using
a simple co-occurrence based vector space model (SVS),
latent semantic analysis, non-negative matrix factoriza-
tion and latent Dirichlet alocation as individual models
with the best parameter setting (LSA, NMF, LDA) and as
mixtures (LSAMIX, NMFMIX, LDAMIX).
6 Results
Word Similarity Our results on word similar-
ity are summarized in Table 2. The simple co-
occurrence based vector space (SVS) performed best
with tf-idf weighting and the cosine similarity mea-
sure. With regard to LSA, we obtained best re-
sults with initial line normalization of the matrix,
K = 600 dimensions, and the scalar product sim-
ilarity measure while performing computations in
matrix U . Both NMF and LDA models are generally
better with a larger number of senses. NMF yields
best performance with K = 1000 dimensions and
the scalar product similarity measure. The best LDA
model also uses the scalar product, has K = 1200
topics, and ? set to 50K .
Following Reisinger and Mooney (2010), we also
evaluated mixture models that combine the output
of models with varying parameter settings. For both
NMF and LDA we averaged the similarity scores re-
turned by all runs. For comparison, we also present
an LSA mixture model over the (best) middle in-
terval K values. As can be seen, the LSA model
improves slightly, whereas NMF and LDA perform
worse than their best individual models.3 Overall,
we observe that NMF and LDA yield significantly
(p < 0.01) better correlations than LSA and the sim-
3It is difficult to relate our results to Reisinger and Mooney
(2010), due to differences in the training data and the vector rep-
resentations it gives rise to. As a comparison, a baseline config-
uration with tf-idf weighting and the cosine similarity measure
yields a correlation of 0.38 with our data and 0.49 in Reisinger
and Mooney (2010).
Model Kendall?s ?b
SVS 11.05
Add-SVS 12.74
Add-NMF 12.85
Add-LDA 12.33
Mult-SVS 14.41
Mult-NMF 13.20
Mult-LDA 12.90
Cont-NMF 14.95
Cont-LDA 13.71
Cont-NMFMIX 16.01
Cont-LDAMIX 15.53
Table 3: Results on lexical substitution using a simple
semantic space model (SVS), additive and multiplicative
compositional models with vector representations based
on co-occurrences (Add-SVS, Mult-SVS), NMF (Add-
NMF, Mult-NMF), and LDA (Add-LDA, Mult-LDA) and
contextualized models based on NMF and LDA with the
best parameter setting (Cont-NMF, Cont-LDA) and as
mixtures (Cont-NMFMIX, Cont-LDAMIX).
ple semantic space, both as individual models and as
mixtures.
Lexical Substitution Our results on lexical sub-
stitution are shown in Table 3. As a baseline we
also report the performance of the simple semantic
space that does not use any contextual information.
This model returns the same ranking of the substi-
tute candidates for each instance, based solely on
their similarity with the target word. This is a rel-
atively competitive baseline as observed by Erk and
Pado? (2008) and Thater et al (2009).
We report results with contextualized NMF and
LDA as individual models (the best word similar-
ity settings) and as mixtures (as described above).
These are in turn compared against additive and
multiplicative compositional models. We imple-
mented an additive model with pmi weighting and
Lin?s similarity measure which is defined in an ad-
ditive fashion. The multiplicative model uses tf-
idf weighting and cosine similarity, which involves
multiplication of vector components. Other combi-
nations of weighting schemes and similarity mea-
sures delivered significantly lower results. We also
report results for these models when using the NMF
and LDA reduced representations.
1168
Model Adv Adj Noun Verb
SVS 22.47 14.38 09.52 7.98
Add-SVS 22.79 14.56 11.59 10.00
Mult-SVS 22.85 16.37 13.59 11.60
Cont-NMFMIX 26.13 17.10 15.16 14.18
Cont-LDAMIX 21.21 16.00 16.31 13.67
Table 4: Results on lexical substitution for different parts
of speech with a simple semantic space model (SVS), two
compositional models (Add-SVS, Mult-SVS), and con-
textualized mixture models with NMF and LDA (Cont-
NMFMIX, Cont-LDAMIX), using Kendall?s ?b correlation
coefficient.
All models significantly (p < 0.01) outperform
the context agnostic simple semantic space (see
SVS in Table 3). Mixture NMF and LDA mod-
els are significantly better than all variants of com-
positional models (p < 0.01); the individual mod-
els are numerically better, however the difference
is not statistically significant. We also find that the
multiplicative model using a simple semantic space
(Mult-SVS) is the best performing compositional
model, thus corroborating the results of Mitchell and
Lapata (2009). Interestingly, dimensionality compo-
sitional models. This indicates that the better results
we obtain are due to the probabilistic formulation of
our contextualized model as a whole rather than the
use of NMF or LDA. Finally, we observe that the
Cont-NMF model is slightly better than Cont-LDA,
however the difference is not statistically significant.
To allow comparison with previous results re-
ported on this data set, we also used the General-
ized Average Precision (GAP, Kishida (2005)) as an
evaluation measure. GAP takes into account the or-
der of candidates ranked correctly by a hypothetical
system, whereas average precision is only sensitive
to their relative position. The best performing mod-
els are Cont-NMFMIX and Cont-LDAMIX obtaining
a GAP of 42.7% and 42.9%, respectively. Erk and
Pado? (2010) report a GAP of 38.6% on this data set
with their best model.
Table 4 shows how the models perform across dif-
ferent parts of speech. While verbs and nouns seem
to be most difficult, we observe higher gains from
the use of contextualized models. Cont-LDAMIX
obtains approximately 7% absolute gain for nouns
and Cont-NMFMIX approximately 6% for verbs. All
Senses Word Distributions
TRAFFIC (0.18) road, traffic, highway, route, bridge
MUSIC (0.04) music, song, rock, band, dance, play
FAN (0.04) crowd, fan, people, wave, cheer, street
VEHICLE (0.04) car, truck, bus, train, driver, vehicle
Table 5: Induced senses of jam and five most likely words
given these senses using an LDA model; sense probabili-
ties are shown in parentheses.
contextualized models obtain smaller improvements
for adjectives. For adverbs most models do not im-
prove over the no-context setting, with the exception
Cont-NMFMIX.
Finally, we also qualitatively examined how the
context words influence the sense distributions of
target words using examples from the lexical sub-
stitution dataset and the output of an individual
Cont-LDA model. In many cases, a target word
starts with a distribution spread over a larger number
of senses, while a context word shifts this distribu-
tion to one majority sense. Consider, for instance,
the target noun jam in the following sentence:
(1) With their transcendent, improvisational jams
and Mayan-inspired sense of a higher, meta-
physical purpose, the band?s music delivers a
spiritual sustenance that has earned them a very
devoted core following.
Table 5 shows the out-of-context senses activated
for jam together with the five most likely words as-
sociated with them.4 Sense probabilities are also
shown in parentheses. As can be seen, initially two
traffic-related and two music-related senses are acti-
vated, however with low probabilities. In the pres-
ence of the context word band, we obtain a much
more ?focused? distribution, in which the MUSIC
sense has 0.88 probability. The system ranks riff
and gig as the most likely two substitutes for jam.
The gold annotation also lists session as a possible
substitute.
In a large number of cases, the target is only par-
tially disambiguated by a context word and this is
also reflected in the resulting distribution. An ex-
4Sense names are provided by the authors in an attempt to
best describe the clusters (i.e., topics for LDA) to which words
are assigned.
1169
ample is the word bug which initially has a distribu-
tion triggering the SOFTWARE (0.09, computer, soft-
ware, microsoft, windows) and DISEASE (0.06, dis-
ease, aids, virus, cause) senses. In the context of
client, bug remains ambiguous between the senses
SECRET-AGENCY (0.34, agent, secret, intelligence,
FBI) ) and SOFTWARE (0.29):
(2) We wanted to give our client more than just a
list of bugs and an invoice ? we wanted to
provide an audit trail of our work along with
meaningful productivity metrics.
There are also cases where the contextualized dis-
tributions are not correct, especially when senses are
domain specific. An example is the word function
occurring in its mathematical sense with the context
word distribution. However, the senses that are trig-
gered by this pair all relate to the ?service? sense of
function. This is a consequence of the newspaper
corpus we use, in which the mathematical sense of
function is rare. We also see several cases where
the target word and one of the context words are as-
signed senses that are locally correct, but invalid in
the larger context. In the following example:
(3) Check the shoulders so it hangs well, stops at
hips or below, and make sure the pants are long
enough.
The pair (check, shoulder) triggers senses IN-
JURY (0.81, injury, left, knee, shoulder) and
BALL-SPORTS (0.10, ball, shot, hit, throw). How-
ever, the sentential context ascribes a meaning that
is neither related to injury nor sports. This suggests
that our models could benefit from more principled
context feature aggregation.
Generally, verbs are not as good context words
as nouns. To give an example, we often encounter
the pair (let, know), used in the common ?inform?
meaning. The senses we obtain for this pair, are,
however, rather uninformative general verb classes:
{see, know, think, do} (0.57) and {go, say, do,
can} (0.20). This type of error can be eliminated in
a space where context features are designed to best
reflect the properties of the target words.
7 Conclusions
In this paper we have presented a general frame-
work for computing similarity in context. Key in this
framework is the representation of word meaning as
a distribution over a set of global senses where con-
textualized meaning is modeled as a change in this
distribution. The approach is conceptually simple,
the same vector representation is used for isolated
words and words in context without being tied to a
specific sense induction method or type of semantic
space.
We have illustrated two instantiations of this
framework using non-negative matrix factorization
and latent Dirichlet alocation for inducing the la-
tent structure, and shown experimentally that they
outperform previously proposed methods for mea-
suring similarity in context. Furthermore, both of
them benefit from mixing model predictions over a
set of different parameter choices, thus making pa-
rameter tuning redundant.
The directions for future work are many and var-
ied. Conceptually, we have defined our model in an
asymmetric fashion, i.e., by stipulating a difference
between target words and contextual features. How-
ever, in practice, we used vector representations that
do not distinguish the two: target words and con-
textual features are both words. This choice was
made to facilitate comparisons with the popular bag-
of-words vector space models. However, differen-
tiating target from context representations may be
beneficial particularly when the similarity compu-
tations are embedded within specific tasks such as
the acquisition of paraphrases, the recognition of en-
tailment relations, and thesaurus construction. Also
note that our model currently contextualizes target
words with respect to individual contexts. Ideally,
we would like to compute the collective influence of
several context words on the target. We plan to fur-
ther investigate how to select or to better aggregate
the entire set of features extracted from a context.
Acknowledgments The authors acknowledge the
support of the DFG (Dinu; International Re-
search Training Group ?Language Technology and
Cognitive Systems?) and EPSRC (Lapata; grant
GR/T04540/01).
1170
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112?119, Athens, Greece.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41:391?407.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Katrin Erk and Sabastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th Annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, New York, NY.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Machine Learn-
ing, 41(2):177?196.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In NIPS,
pages 556?562.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):342?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768?774, Montre?al, Canada.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd Annual Conference of the Cognitive Sci-
ence Society, pages 675?680, Philadelphia, PA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proceedings of SemEval, pages 48?53, Prague, Czech
Republic.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 430?439, Suntec, Singa-
pore.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent Dirichlet alo-
cation. In Proceeding of the 14th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 569?577, New York, NY.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 109?
117, Los Angeles, California.
G Salton, A Wang, and C Yang. 1975. A vector-space
model for information retrieval. Journal of the Ameri-
can Society for Information Science, 18:613?620.
1171
Hinrich Schuetze. 1998. Automatic word sense discrim-
ination. Journal of Computational Linguistics, 24:97?
123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Michael E. Tipping and Chris M. Bishop. 1999. Prob-
abilistic principal component analysis. Journal of the
Royal Statistical Society, Series B, 61:611?622.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics,
pages 947?953, Saarbru?cken, Germany.
1172
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434?442,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving the Lexical Function Composition Model
with Pathwise Optimized Elastic-Net Regression
Jiming Li and Marco Baroni and Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento, Italy
(jiming.li|marco.baroni|georgiana.dinu)@unitn.it
Abstract
In this paper, we show that the lexical
function model for composition of dis-
tributional semantic vectors can be im-
proved by adopting a more advanced re-
gression technique. We use the pathwise
coordinate-descent optimized elastic-net
regression method to estimate the compo-
sition parameters, and compare the result-
ing model with several recent alternative
approaches in the task of composing sim-
ple intransitive sentences, adjective-noun
phrases and determiner phrases. Experi-
mental results demonstrate that the lexical
function model estimated by elastic-net re-
gression achieves better performance, and
it provides good qualitative interpretabil-
ity through sparsity constraints on model
parameters.
1 Introduction
Vector-based distributional semantic models of
word meaning have gained increased attention in
recent years (Turney and Pantel, 2010). Differ-
ent from formal semantics, distributional seman-
tics represents word meanings as vectors in a high-
dimensional semantic space, where the dimen-
sions are given by co-occurring contextual fea-
tures. The intuition behind these models lies in
the fact that words which are similar in meaning
often occur in similar contexts, e.g., moon and
star might both occur with sky, night and bright.
This leads to convenient ways to measure similar-
ity between different words using geometric meth-
ods (e.g., the cosine of the angle between two
vectors that summarize their contextual distribu-
tion). Distributional semantic models have been
successfully applied to many tasks in linguistics
and cognitive science (Griffiths et al., 2007; Foltz
et al., 1998; Laham, 1997; McDonald and Brew,
2004). However, most of these tasks only deal
with isolated words, and there is a strong need
to construct representations for longer linguistic
structures such as phrases and sentences. In or-
der to achieve this goal, the principle of com-
positionality of linguistic structures, which states
that complex linguistic structures can be formed
through composition of simple elements, is ap-
plied to distributional vectors. Therefore, in recent
years, the problem of composition within distribu-
tional models has caught many researchers? atten-
tion (Clark, 2013; Erk, 2012).
A number of compositional frameworks have
been proposed and tested. Mitchell and Lapata
(2008) propose a set of simple component-wise
operations, such as multiplication and addition.
Later, Guevara (2010) and Baroni and Zampar-
elli (2010) proposed more elaborate methods, in
which composition is modeled as matrix-vector
multiplication operations. Particularly new to their
approach is the proposal to estimate model param-
eters by minimizing the distance of the composed
vectors to corpus-observed phrase vectors. For ex-
ample, Baroni and Zamparelli (2010) consider the
case of Adjective-Noun composition and model it
as matrix-vector multiplication: adjective matrices
are parameters to be estimated and nouns are co-
occurrence vectors. The model parameter estima-
tion procedure becomes a multiple response mul-
tivariate regression problem. This method, that,
following Dinu et al. (2013) and others, we term
the lexical function composition model, can also
be generalized to more complex structures such
as 3rd order tensors for modeling transitive verbs
(Grefenstette et al., 2013).
Socher et al. (2012) proposed a more complex
and flexible framework based on matrix-vector
representations. Each word or lexical node in a
parsing tree is assigned a vector (representing in-
herent meaning of the constituent) and a matrix
(controlling the behavior to modify the meaning of
434
Model Composition function Parameters
Add w
1
u? + w
2
v? w
1
, w
2
Mult u?
w
1
? v?
w
2
w
1
, w
2
Dil ||u?||
2
2
v? + (?? 1)?u?, v??u? ?
Fulladd W
1
u? + W
2
v? W
1
,W
2
? R
m?m
Lexfunc A
u
v? A
u
? R
m?m
Fulllex tanh([W
1
,W
2
]
[
A
u
v?
A
v
u?
]
) W
1
,W
2
,
A
u
, A
v
? R
m?m
Table 1: Composition functions of inputs (u, v).
neighbor words or phrases) simultaneously. They
use recursive neural networks to learn and con-
struct the entire model and show that it reaches
state-of-the-art performance in various evaluation
experiments.
In this paper, we focus on the simpler, linear
lexical function model proposed by Baroni and
Zamparelli (2010) (see also Coecke et al. (2010))
and show that its performance can be further im-
proved through more advanced regression tech-
niques. We use the recently introduced elastic-
net regularized linear regression method, which
is solved by the pathwise coordinate descent opti-
mization algorithm along a regularization parame-
ter path. This new regression method can rapidly
generate a sequence of solutions along the regular-
ization path. Performing cross-validation on this
parameter path should yield a much more accurate
model for prediction. Besides better prediction ac-
curacy, the elastic-net method also brings inter-
pretability to the composition procedure through
sparsity constraints on the model.
The rest of this paper is organized as follows: In
Section 2, we give details on the above-mentioned
composition models, which will be used for com-
parison in our experiments. In Section 3, we de-
scribe the pathwise optimized elastic-net regres-
sion algorithm. Experimental evaluation on three
composition tasks is provided in Section 4. In Sec-
tion 5 we conclude and suggest directions for fu-
ture work.
2 Composition Models
Mitchell and Lapata (2008; 2010) present a set of
simple but effective models in which each compo-
nent of the output vector is a function of the cor-
responding components of the inputs. Given in-
put vectors u? and v?, the weighted additive model
(Add) returns their weighted sum: p? = w
1
u? +
w
2
v?. In the dilation model (Dil), the output vector
is obtained by decomposing one of the input vec-
tors, say v?, into a vector parallel to u? and an or-
thogonal vector, and then dilating only the parallel
vector by a factor ? before re-combining (formula
in Table 1). Mitchell and Lapata also propose a
simple multiplicative model in which the output
components are obtained by component-wise mul-
tiplication of the corresponding input components.
We use its natural weighted extension (Mult), in-
troduced by Dinu et al. (2013), that takes w
1
and
w
2
powers of the components before multiplying,
such that each phrase component p
i
is given by:
p
i
= u
w
1
i
v
w
2
i
.
Guevara (2010) and Zanzotto et al. (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: p? =
W
1
u? + W
2
v?.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a regression function from nouns onto (mod-
ified) nouns. Given that linear functions can be
expressed by matrices and their application by
matrix-by-vector multiplication, a functor (such
as the adjective) is represented by a matrix A
u
to be composed with the argument vector v? (e.g.,
the noun) by multiplication, returning the lexical
function (Lexfunc) representation of the phrase:
p? = A
u
v?.
The method proposed by Socher et al. (2012)
can be seen as a combination and non-linear ex-
tension of Fulladd and Lexfunc (that Dinu and col-
leagues thus called Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (u?, A
u
) and (v?, A
v
), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products A
u
v?
and A
v
u? (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Parameter estimation of the above composition
models follows Dinu et al. (2013) by minimizing
the distance to corpus-extracted phrase vectors. In
435
Figure 1: A sketch of the composition model train-
ing and composing procedure.
the case of the Fulladd and Lexfunc models this
amounts to solving a multiple response multivari-
ate regression problem.
The whole composition model training and
phrase composition procedure is described with a
sketch in Figure 1. To illustrate with an example,
given an intransitive verb boom, we want to train
a model for this intransitive verb so that we can
use it for composition with a noun subject (e.g.,
export) to form an intransitive sentence (e.g., ex-
port boom(s)). We treat these steps as a composi-
tion model learning and predicting procedure. The
training dataset is formed with pairs of input (e.g.,
activity) and output (e.g., activity boom) vectors.
All composition models except Lexfunc also use
the functor vector (boom) in the training data. Lex-
func does not use this functor vector, but it would
rather like to encode the learning target?s vector
meaning in a different way (see experimental anal-
ysis in Section 4.3). Then, this dataset is used for
parameter estimation of models. When a model
(boom) is trained and given a new input seman-
tic vector (e.g., export), it will output another vec-
tor representing the concept for export boom. And
the concept export boom should be close to simi-
lar concepts (e.g., export prosper) in meaning un-
der some distance metric in semantic vector space.
The same training and composition scheme is ap-
plied for other types of functors (e.g., adjectives
and determiners). All the above mentioned com-
position models are evaluated within this scheme,
but note that in the case of Add, Dil, Mult and Ful-
ladd, a single set of parameters is obtained across
all functors of a certain syntactic category.
3 Pathwise Optimized Elastic-net
Algorithm
The elastic-net regression method (Zou and
Hastie, 2005) is proposed as a compromise be-
tween lasso (Tibshirani, 1996) and ridge regres-
sion (Hastie et al., 2009). Suppose there are N
observation pairs (x
i
, y
i
), here x
i
? R
p
is the ith
training sample and y
i
? R is the corresponding
response variable in the typical regression setting.
For simplicity, assume the x
ij
are standardized:
?
N
i=1
x
2
ij
= 1, for j = 1, . . . , p. The elastic-net
solves the following problem:
min
(?
0
,?)?R
p+1
[
1
N
N
?
i=1
(y
i
? ?
0
? x
T
i
?)
2
+ ?P
?
(?)
]
(1)
where
P
?
(?) = ?((1? ?)
1
2
? ? ?
2
?
2
+??
?
1
)
=
p
?
j=1
[
1
2
(1? ?)?
2
j
+ ?|?
j
|].
P is the elastic-net penalty, and it is a compro-
mise between the ridge regression penalty and the
lasso penalty. The merit of the elastic-net penalty
depends on two facts: the first is that elastic-net in-
herits lasso?s characteristic to shrink many of the
regression coefficients to zero, a property called
sparsity, which results in better interpretability of
model; the second is that elastic-net inherits ridge
regression?s property of a grouping effect, which
means important correlated features can be con-
tained in the model simultaneously, and not be
omitted as in lasso.
For these linear-type regression problem (ridge,
lasso and elastic-net), the determination of the ?
value is very important for prediction accuracy.
Efron et al. (2004) developed an efficient algo-
rithm to compute the entire regularization path
for the lasso problem in 2004. Later, Friedman
et al. (Friedman et al., 2007; Friedman et al.,
2010) proposed a coordinate descent optimization
436
method for the regularization parameter path, and
they also provided a solution for elastic-net. The
main idea of pathwise coordinate descent is to
solve the penalized regression problem along an
entire path of values for the regularization param-
eters ?, using the current estimates as warm starts.
The idea turns out to be quite efficient for elastic-
net regression. The procedure can be described as
below: firstly establish an 100 ? value sequence
in log scale, and for each of the 100 regulariza-
tion parameters, use the following coordinate-wise
updating rule to cycle around the features for es-
timating the corresponding regression coefficients
until convergence.
?
?
j
?
S
(
1
N
?
N
i=1
x
ij
(y
i
? y?
(j)
i
), ??
)
1 + ?(1? ?)
(2)
where
? y?
(j)
i
=
?
?
0
+
?
? ?=j
x
i?
?
?
?
is the fitted value ex-
cluding the contribution from x
ij
, and hence
y
i
? y?
(j)
i
the partial residual for fitting ?
j
.
? S(z, ?) is the soft-thresholding operator with
value
S(z, ?) = sign(z)(|z| ? ?)
+
=
?
?
?
z ? ? if z > 0 and ? < |z|
z + ? if z < 0 and ? < |z|
0 if ? ? |z|
Then solutions for a decreasing sequence of val-
ues for ? are computed in this way, starting at the
smallest value ?
max
for which the entire coeffi-
cient vector
?
? = 0. Then, 10-fold cross valida-
tion on this regularization path is used to deter-
mine the best model for prediction accuracy. The
? parameter controls the model sparsity (the num-
ber of coefficients equal to zero) and grouping ef-
fect (shrinking highly correlated features simulta-
neously).
In what follows, we call the elastic-net regres-
sion lexical function model EnetLex. In Sec-
tion 4, we will report the experiment results by
EnetLex with ? = 1. It equals to pathwise co-
ordinate descent optimized lasso, which favours
sparser solutions and is often a better estimator
when the number of training samples is far greater
than the number of feature dimensions, as in our
case. We also experimented with intermediate ?
values (e.g., ? = 0.5), that were, consistently, in-
ferior or equal to the lasso setting.
?2 0 2 4
200
400
600
800
log(Lambda)
Mean
?Squ
ared 
Error
50 50 50 50 50 50 50 50 50 48 29 21 12 7 4 2Model selection procedure for ?EnetLex?
Figure 2: Example of model selection procedure
for elastic-net regression (?the? model for deter-
miner phrase experiment, SVD, 50 dimensions).
Figure 2 is an example of the model selection
procedure between different regularization param-
eter ? values for determiner ?the? (experimental
details are described in section 4). When ? is
fixed, EnetLex first generates a ? sequence from
?
max
to ?
min
(?
max
is set to the smallest value
which will shrink all the regression coefficients
to zero, ?
min
= 0.0001) in log scale (rightmost
point in the plot). The red points corresponding
to each ? value in the plot represent mean cross-
validated errors and their standard errors. To esti-
mate a model corresponding to some ? value ex-
cept ?
max
, we use the solution from previous ?
value as the initial coefficients (the warm starts
mentioned before) for iteration with coordinate
descent. This will often generate a stable solu-
tion path for the whole ? sequence very fast. And
we can choose the model with minimum cross-
validation error on this path and use it for more
accurate prediction. In Figure 2, the labels on the
top are numbers of corresponding selected vari-
ables (features), the right vertical dotted line is the
largest value of lambda such that error is within 1
standard error of the minimum, and the left verti-
cal dotted line corresponds to the ? value which
gives minimum cross-validated error. In this case,
the ? value of minimum cross-validated error is
0.106, and its log is -2.244316. In all of our ex-
periments, we will select models corresponding to
minimum training-data cross-validated error.
4 Experiments
4.1 Datasets
We evaluate on the three data sets described below,
that were also used by Dinu et al. (2013), our most
437
direct point of comparison.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2010), focuses
on the composition of intransitive verbs and their
noun subjects. It contains a total of 120 sentence
pairs together with human similarity judgments on
a 7-point scale. For example, value slumps/value
declines is scored 7, skin glows/skin burns is
scored 1. On average, each pair is rated by 30
participants. Rather than evaluating against mean
scores, we use each rating as a separate data point,
as done by Mitchell and Lapata. We report Spear-
man correlations between human-assigned scores
and cosines of model-generated vector pairs.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We fo-
cus on the latter, and we frame the task as in Dinu
et al. (2013). The dataset contains 620 ANs, each
paired with a single-noun paraphrase. Examples
include: upper side/upside, false belief/fallacy and
electric refrigerator/fridge. We evaluate a model
by computing the cosine of all 20K nouns in our
semantic space with the target AN, and looking at
the rank of the correct paraphrase in this list. The
lower the rank, the better the model. We report
median rank across the test items.
Determiner phrases The third dataset, intro-
duced in Bernardi et al. (2013), focuses on a
class of determiner words. It is a multiple-
choice test where target nouns (e.g., omniscience)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., all knowl-
edge). There are 173 target nouns in total, each
paired with one correct DP response, as well as
5 foils, namely the determiner (all) and noun
(knowledge) from the correct response and three
more DPs, two of which contain the same noun as
the correct phrase (much knowledge, some knowl-
edge), the third the same determiner (all prelimi-
naries). Other examples of targets/related-phrases
are quatrain/four lines and apathy/no emotion.
The models compute cosines between target noun
and responses and are scored based on their accu-
racy at ranking the correct phrase first.
4.2 Setup
We use a concatenation of ukWaC, Wikipedia
(2009 dump) and BNC as source corpus, total-
Model Reduction Dim Correlation
Add NMF 150 0.1349
Dil NMF 300 0.1288
Mult NMF 250 0.2246
Fulladd SVD 300 0.0461
Lexfunc SVD 250 0.2673
Fulllex NMF 300 0.2682
EnetLex SVD 250 0.3239
Table 2: Best performance comparison for intran-
sitive verb sentence composition.
ing 2.8 billion tokens.
1
Word co-occurrences are
collected within sentence boundaries (with a max-
imum of a 50-words window around the target
word). Following Dinu et al. (2013), we use the
top 10K most frequent content lemmas as context
features, Pointwise Mutual Information as weight-
ing method and we reduce the dimensionality of
the data by both Non-negative Matrix Factoriza-
tion (NMF, Lee and Seung (2000)) and Singular
Value Decomposition (SVD). For both data di-
mensionality reduction techniques, we experiment
with different numbers of dimension varying from
50 to 300 with a step of 50. Since the Mult model
works very poorly when the input vectors contain
negative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
We use the DIStributional SEmantics Compo-
sition Toolkit (DISSECT)
2
which provides imple-
mentations for all models we use for comparison.
Following Dinu and colleagues, we used ordinary
least-squares to estimate Fulladd and ridge for
Lexfunc. The EnetLex model is implemented in R
with support from the glmnet package,
3
which im-
plements pathwise coordinate descent elastic-net
regression.
4.3 Experimental Results and Analysis
The experimental results are shown in Ta-
bles 2, 3, 4 and Figures 3, 4, 5. The best per-
formances from each model on the three compo-
sition tasks are shown in the tables. The over-
all result distributions across reduction techniques
and dimensionalities are displayed in the figure
1
http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
2
http://clic.cimec.unitn.it/composes/
toolkit/
3
http://cran.r-project.org/web/
packages/glmnet/
438
Model Reduction Dim Rank
Add NMF 300 113
Dil NMF 300 354.5
Mult NMF 300 146.5
Fulladd SVD 300 123
Lexfunc SVD 150 117.5
Fulllex SVD 50 394
EnetLex SVD 300 108.5
Table 3: Best performance comparison for adjec-
tive noun composition (lower ranks mean better
performance).
Model Reduction Dim Rank
Add NMF 100 0.3237
Dil NMF 100 0.3584
Mult NMF 300 0.2023
Fulladd NMF 200 0.3642
Lexfunc SVD 200 0.3699
Fulllex SVD 100 0.3699
EnetLex SVD 250 0.4046
Table 4: Best performance comparison for deter-
miner phrase composition.
boxplots (NMF and SVD results are shown sep-
arately). From Tables 2, 3, 4, we can see that
EnetLex consistently achieves the best composi-
tion performance overall, also outperforming the
standard lexical function model. In the boxplot
display, we can see that SVD is in general more
stable across dimensionalities, yielding smaller
variance in the results than NMF. We also observe,
more specifically, larger variance in EnetLex per-
formance on NMF than in Lexfunc, especially for
determiner phrase composition. The large vari-
ance with EnetLex comes from the NMF low-
dimensionality results, especially the 50 dimen-
sions condition. The main reason for this lies
in the fast-computing tricks of the coordinate de-
scent algorithm when cycling around many fea-
tures with zero values (as resulting from NMF),
which cause fast convergence at the beginning of
the regularization path, generating an inaccurate
model. A subordinate reason might lie in the un-
standardized larger values of the NMF features
(causing large gaps between adjacent parameter
values in the regularization path). Although data
standardization or other feature scaling techniques
are often adopted in statistical analysis, they are
seldom used in semantic composition tasks due to
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
0.000.05
0.100.15
0.200.25
0.30
Intransitive sentences
Figure 3: Intransitive verb sentence composition
results.
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
800
600
400
200
0 Adjective?noun phrases
Figure 4: Adjective noun phrase composition re-
sults.
the fact that they might negatively affect the se-
mantic vector space. A reasonable way out of this
problem would be to save the mean and standard
deviation parameters used for data standardization
and use them to project the composed phrase vec-
tor outputs back to the original vector space.
On the other hand, EnetLex obtained a stable
good performance in SVD space, with the best re-
sults achieved with dimensions between 200 and
300. A set of Tukey?s Honestly Significant Tests
show that EnetLex significantly outperforms the
other models across SVD settings for determiner
phrases and intransitive sentences. The difference
is not significant for most comparisons in the ad-
jective phrases task.
For the simpler models for which it was com-
putationally feasible, we repeated the experiments
without dimensionality reduction. The results ob-
tained with (unweighted) Add and Mult using full-
space representations are reported in Table 5. Due
to computational limitations, we tuned full-space
weights for Add model only, obtaining similar re-
sults to those reported in the table. The full-space
439
Add?
nmf Dil?n
mf
Mult?
nmf
Fulla
dd?n
mf
Lexfu
nc?n
mf
Fullle
x?n
mf
Enet
Lex?
nmf
Add?
svd Dil?s
vd
Mult?
svd
Fulla
dd?s
vd
Lexfu
nc?s
vd
Fullle
x?s
vd
Enet
Lex?
svd
0.15
0.20
0.25
0.30
0.35
0.40 Determiner phrases
Figure 5: Determiner phrase composition results.
model verb adjective determiner
Add 0.0259 957 0.2832
Mult 0.1796 298.5 0.0405
Table 5: Performance of Add and Mult models
without dimensionality reduction.
results confirm that dimensionality reduction is
not only a computational necessity when work-
ing with more complex models, but it is actually
improving the quality of the underlying semantic
space.
Another benefit that elastic-net has brought to
us is the sparsity in coefficient matrices. Sparsity
here means that many entries in the coefficient ma-
trix are shrunk to 0. For the above three exper-
iments, the mean adjective, verb and determiner
models? sparsity ratios are 0.66, 0.55 and 0.18 re-
spectively. Sparsity can greatly reduce the space
needed to store the lexical function model, espe-
cially when we want to use higher orders of repre-
sentation. Moreover, sparsity in the model is help-
ful to interpret the concept a specific functor word
is conveying. For example, we show how to an-
alyze the coefficient matrices for functor content
words (verbs and adjectives). The verb burst and
adjective poisonous, when estimated in the space
projected to 100 dimensions with NMF, have per-
centages of sparsity 47% and 39% respectively,
which means 47% of the entries in the burst ma-
trix and 39% of the entries in the poisonous ma-
trix are zeros.
4
Most of the (hopefully) irrelevant
dimensions were discarded during model training.
For visualization, we list the 6 most significant
4
We analyze NMF rather than the better-performing SVD
features because the presence of negative values in the latter
makes their interpretation very difficult. And NMF achieves
comparably good performance for interpretation when di-
mension exceeds 100.
columns and rows from verb burst and adjective
poisonous in Table 6. Each reduced NMF di-
mension is represented by the 3 largest original-
context entries in the corresponding row of the
NMF basis matrix. The top columns and rows
are selected by ordering sums of row entries and
sums of column entries (the 10 most common fea-
tures across trained matrices are omitted). In the
matrix-vector multiplication scenario, a larger col-
umn contributes more to all the features of the
composed output phrase vector, while one large
row corresponds to a large composition output fea-
ture. From these tables, we can see that the se-
lected top columns and rows are mostly semanti-
cally relevant to the corresponding functor words
(burst and poisonous, in the displayed examples).
A very interesting aspect of these experiments
is the role of the intercept in our regression model.
The path-wise optimization algorithm starts with
a lambda value (?
max
), which sets all the coef-
ficients exactly to 0, and at that time the inter-
cept is just the expected mean value of the train-
ing phrase vectors, which in turn is of course quite
similar to the co-occurrence vector of the cor-
responding functor word (by averaging the poi-
sonous N context distributions, we obtain a vec-
tor that approximates the poisonous distribution).
And, although the intercept also changes with dif-
ferent lambda values, it still highly correlates with
the co-occurrence vectors of the functor words
in vector space. For adjectives and verbs, we
compared the initial model?s (?
max
) intercept and
the minimum cross-validation error model inter-
cept with corpus-extracted vectors for the corre-
sponding words. That is, we used the word co-
occurrence vector for a verb or an adjective ex-
tracted from the corpus and projected onto the
reduced feature space (e.g., NMF, 100 dimen-
sions), then computed cosine similarity between
this word meaning representation and its corre-
sponding EnetLex matrix initial and minimum-
error intercepts, respectively. Most of the simi-
larities are still quite high after estimation: The
mean cosine values for adjectives are 0.82 for the
initial intercept and 0.72 for the minimum-error
one. For verbs, the corresponding values are 0.75
and 0.69, respectively. Apparently, the sparsity
constraint helps the intercept retaining information
from training phrases.
Qualitatively, often the intercept encodes the
representation of the original word meaning in
440
burst significant columns significant rows
policeman, mob, guard hurricane, earthquake, disaster
Iraqi, Lebanese, Kurdish conquer, Byzantine, conquest
jealousy, anger, guilt policeman, mob, guard
hurricane, earthquake, disaster terminus, traffic, interchange
defender, keeper, striker convict, sentence, imprisonment
volcanic, sediment, geological boost, unveil, campaigner
poisonous significant columns significant rows
bathroom, wc, shower ventilation, fluid, bacterium
ignite, emit, reactor ignite, emit, reactor
reptile, mammal, predator infectious, infect, infected
ventilation, fluid, bacterium slay, pharaoh, tribe
flowering, shrub, perennial park, lorry, pavement
sauce, onion, garlic knife, pierce, brass
Table 6: Interpretability for verbs and adjectives (exemplified by burst and poisonous).
vector space. For example, if we check the inter-
cept for poisonous, the cosine between the origi-
nal vector space representation (from corpus) and
the minimum-error solution intercept (from train-
ing phrases) is at 0.7. The NMF dimensions cor-
responding with the largest intercept entries are
rather intuitive for poisonous: ?ventilation, fluid,
bacterium?, ?racist, racism, outrage?, ?reptile,
mammal, predator?, ?flowering, shrub, perennial?,
?sceptical, accusation, credibility?, ?infectious, in-
fect, infected?.
The mathematical reason for the above facts lies
in the updating rule of the elastic-net?s intercept:
?
0
=
?
y ?
p
?
j=1
?
?
j
?
x
j
(3)
Sparsity in the regression coefficients (
?
?
j
) encour-
ages intercept ?
0
to stay as close to the mean
value of response
?
y as possible. So the elastic-
net lexical function composition model is de facto
also capturing the inherent meaning of the func-
tor word, learning it from the training word-phrase
pairs. In future research, we would like to test if
these lexical meaning representations are as good
or even better than standard co-occurrence vectors
for single-word similarity tasks.
5 Conclusion
In this paper, we have shown that the lexical func-
tion composition model can be improved by ad-
vanced regression techniques. We use pathwise
coordinate descent optimized elastic-net, testing
it on composing intransitive sentences, adjective-
noun phrases and determiner phrases in compari-
son with other composition models, including lex-
ical function estimated with ridge regression. The
elastic-net method leads to performance gains on
all three tasks. Through sparsity constraints on the
model, elastic-net also introduces interpretability
in the lexical function composition model. The
regression coefficient matrices can often be eas-
ily interpreted by looking at large row and column
sums, as many matrix entries are shrunk to zero.
The intercept of elastic-net regression also plays
an interesting role in the model. With the sparsity
constraints, the intercept of the model tends to re-
tain the inherent meaning of the word by averaging
training phrase vectors.
Our approach naturally generalizes to similar
composition tasks, in particular those involving
higher-order tensors (Grefenstette et al., 2013),
where sparseness might be crucial in producing
compact representations of very large objects. Our
results also suggest that the performance of the
lexical function composition model might be fur-
ther improved with even more advanced methods,
such as nonlinear regression. In the future, we
would also like to explore interpretability more in
depth, by looking at grouping and interaction ef-
fects between features.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES), and we
thank the reviewers for helpful feedback.
441
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), pages 53?57, Sofia, Bulgaria.
Stephen Clark. 2013. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Bradley Efron, Trevor Hastie, Iain Johnstone, and
Robert Tibshirani. 2004. Least angle regression.
The Annals of statistics, 32(2):407?499.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25:285?307.
Jerome Friedman, Trevor Hastie, Holger H?ofling, and
Robert Tibshirani. 2007. Pathwise coordinate
optimization. The Annals of Applied Statistics,
1(2):302?332.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of statisti-
cal software, 33(1):1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Darrell Laham. 1997. Latent Semantic Analysis
approaches to categorization. In Proceedings of
CogSci, page 979.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Scott McDonald and Chris Brew. 2004. A distribu-
tional model of semantic context effects in lexical
processing. In Proceedings of ACL, pages 17?24,
Barcelona, Spain.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Rob Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533?585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
Hui Zou and Trevor Hastie. 2005. Regularization
and variable selection via the elastic net. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2):301?320.
442
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611?615,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A comparison of models of word meaning in context
Georgiana Dinu
Universit?t des Saarlandes
Saarbr?cken, Germany
dinu@coli.uni-saarland.de
Stefan Thater
Universit?t des Saarlandes
Saarbr?cken, Germany
stth@coli.uni-saarland.de
S?ren Laue
Friedrich-Schiller Universit?t
Jena, Germany
soeren.laue@uni-jena.de
Abstract
This paper compares a number of recently pro-
posed models for computing context sensitive
word similarity. We clarify the connections
between these models, simplify their formula-
tion and evaluate them in a unified setting. We
show that the models are essentially equivalent
if syntactic information is ignored, and that the
substantial performance differences previously
reported disappear to a large extent when these
simplified variants are evaluated under identi-
cal conditions. Furthermore, our reformulation
allows for the design of a straightforward and
fast implementation.
1 Introduction
The computation of semantic similarity scores be-
tween words is an important sub-task for a variety
of NLP applications (Turney and Pantel, 2010). One
standard approach is to exploit the so-called distribu-
tional hypothesis that similar words tend to appear
in similar contexts: Word meaning is represented by
the contexts in which a word occurs, and semantic
similarity is computed by comparing these contexts
in a high-dimensional vector space.
Such distributional models of word meaning are
attractive because they are simple, have wide cover-
age, and can be easily acquired in an unsupervised
way. Ambiguity, however, is a fundamental problem:
when encountering a word in context, we want a dis-
tributional representation which reflects its meaning
in this specific context. For instance, while buy and
acquire are similar when we consider them in iso-
lation, they do not convey the same meaning when
acquire occurs in students acquire knowledge. This
is particularly difficult for vector space models which
compute a single type vector summing up over all
occurrences of a word. This vector mixes all of a
word?s usages and makes no distinctions between
its?potentially very diverse?senses.
Several proposals have been made in the recent
literature to address this problem. Type-based meth-
ods combine the (type) vector of the target with the
vectors of the surrounding context words to obtain
a disambiguated representation. In recent work, this
has been proposed by Mitchell and Lapata (2008),
Erk and Pad? (2008) and Thater et al (2010; 2011),
which differ in the choice of input vector representa-
tion and in the combination operation they propose.
A different approach has been taken by Erk and
Pad? (2010), Reisinger and Mooney (2010) and
Reddy et al (2011), who make use of token vectors
for individual occurrences of a word, rather than us-
ing the already mixed type vectors. Generally speak-
ing, these methods ?select? a set of token vectors
of the target, which are similar to the current con-
text, and use only these to obtain a disambiguated
representation.
Yet another approach has been taken by Dinu and
Lapata (2010), ? S?aghdha and Korhonen (2011)
and Van de Cruys et al (2011), who propose to use
latent variable models. Conceptually, this comes
close to token-based models, however their approach
is more unitary as they attempt to recover a hidden
layer which best explains the observation data.
In this paper, we focus on the first group of ap-
proaches and investigate the precise differences be-
tween the three models of Erk and Pad? and Thater et
al., out of which (Thater et al, 2011) achieves state of
the art results on a standard data set. Despite the fact
that these models exploit similar intuitions, both their
formal presentations and the results obtained vary to
a great extent. The answer given in this paper is sur-
prising: the three models are essentially equivalent if
syntactic information is ignored; in a syntactic space
the three methods implement only slightly different
611
intuitions. We clarify these connections, simplify
the syntactic variants originally proposed and reduce
them to straightforward matrix operations, and evalu-
ate them in a unified experimental setting. We obtain
significantly better results than originally reported in
the literature. Our reformulation also also supports
efficient implementations for these methods.
2 Models for meaning in context
We consider the following problem: we are given
an occurrence of a target word and want to obtain a
vector that reflects its meaning in the given context.
To simplify the presentation, we restrict ourselves to
contexts consisting of a single word, and use acquire
in context knowledge as a running example.
EP08. Erk and Pad? (2008) compute a contextu-
alized vector for acquire by combining its type vec-
tor (~w) with the inverse selectional preference vector
of knowledge (c). This is simply the centroid of the
vectors of all words that take knowledge as direct
object (r):
v(w,r,c) =
(
1
n?w?
f (w?,r,c) ? ~w?
)
?~w (1)
where f (w?,r,c) denotes the co-occurrence associa-
tion between the context word c and words w? related
to c by grammatical relation r in a training corpus;
n is the number of words w? and ? denotes a vector
composition operation. In this paper, we take? to be
point-wise multiplication, which is reported to work
best in many studies in the literature.
TFP10. Thater et al (2010) also compute contex-
tualized vectors by combing the vectors of the target
word and of its context. In contrast to EP08, however,
they use second order vectors as basic representation
for the target word.
~w = ?
r,r?,w??
(
?
w?
f (w,r,w?) ? f (w?,r?,w??)
)
~er,r?,w?? (2)
That is, the vector for a target word w has components
for all combinations of two grammatical roles r,r? and
a context word w?; the inner sum gives the value for
each component.
The contextualized vector for acquire is obtained
through pointwise multiplication with the (1st-order)
vector for knowledge (~c), which has to be ?lifted? first
to make the two vectors comparable:
v(w,r,c) = ~w?Lr(~c) (3)
~c = ?r?,w? f (c,r
?,w?)~e(r?,w?) is a first order vector
for the context word; the ?lifting map" Lr(~c) maps
this vector to ?r?,w? f (c,r
?,w?)~e(r,r?,w?) to make it com-
patible with ~w.
TFP11. Thater et al (2011) take a slightly different
perspective on contextualization. Instead of comb-
ing vector representations for the target word and its
context directly, they propose to re-weight the vector
components of the target word, based on distribu-
tional similarity with the context word:
v(w,r,c) = ?
r?,w?
?(r,c,r?,w?) ? f (w,r?,w?) ?~e(r?,w?) (4)
where ?(r,c,r?,w?) is simply cos(~c,~w?) if r and r?
denote the same grammatical function, else 0.
3 Comparison
The models presented above have a number of things
in common: they all use syntactic information and
?second order? vectors to represent word meaning in
context. Yet, their formal presentations differ substan-
tially. We now show that the models are essentially
equivalent if we ignore syntax: they component-wise
multiply the second order vector of one word (target
or context) with the first order vector of the other
word. Specifically, we obtain the following deriva-
tions, where W = {w1, ...,wn} denotes the vocabu-
lary, and V the symmetric n?n input matrix, where
Vi j = f (wi,w j) gives the co-occurrence association
between words wi and w j:
vEP08(w,c) =
1
n?w?
(
f (w?,c) ? ~w?
)
?~w
=
1
n?w?
(
f (w?,c) ? ? f (w?,w1), . . .?
)
?~w
=
1
n
??
w?
f (w?,c) ? f (w?,w1), . . .??~w
=
1
n
?<~c, ~w1>,. . . ,<~c, ~wn>??~w
=
1
n
~c V ?~w
612
vTFP10(w,c) = ?
w???W
(
?
w??W
f (w,w?) ? f (w?,w??)
)
~ew???~c
= ? ?
w??W
f (w,w?) f (w?,w1), . . .??~c
= ?<~w, ~w1>,...,<~w, ~wn>??~c
= ~w V ?~c
vTFP11(w,c) = ?
w??W
?(c,w?) ? f (w,w?) ?~ew?
= ??(w1,c) ? f (w,w1), . . .?
= ??(w1,c), . . .??~w (*)
= ?<~w1,~c>,. . . ,<~wn,~c>??~w
=~c V ?~w
where <~v,~w> denotes scalar product. In step (*), we
assume that ?(w,c) denotes the scalar product of ~w
and~c, instead of cosine similarity, as TFP11. This is
justified if we assume that all vectors are normalized,
in which case the two are identical.
As it can be observed the syntax-free variants of
EP08 and TFP11 are identical up to the choice in
normalization. TFP10 proposes an identical model to
that of TFP11, however with a different interpretation,
in which the roles of the context word and of the
target word are swapped.
4 Evaluation
We have just shown that EP08, TFP10 and TFP11
are essentially equivalent to each other if syntactic
information is ignored, hence it is a bit surprising that
performance results reported in the literature vary
to such a great extent. In this section we consider
syntactic variants of these methods and we show that
performance differences previously reported can only
partly be explained by the different ways syntactic
information is used: when we simplify these models
and evaluate them under identical conditions, the
differences between them disappear to a large extent.
To evaluate the three models, we reimplemented
them using matrix operations similar to the ones used
in Section 3, where we made few simplifications
to the TFP10 and EP08 models: we follow TFP11
and we use component-wise multiplication to com-
bine the target with one context word, and add the
resulting composed vectors when given more con-
text words1. Furthermore for TFP10, we change the
1Note that some of the parameters in the EP08 method (omit-
Model GAP ? Literature
EP08 46.6 + 14.4 (32.2)?
TFP10 48.3 + 3.9 (44.4)
TFP11 51.8 ?0.0
TFP10+11 52.1 N/A
Table 1: GAP scores LST data.
? The best available GAP score for this model (from Erk and
Pad? (2010)) is reported only on a subset of the data - this subset
is however judged by the authors to be ?easier? than the entire
data; all other methods are tested on the entire dataset.
treatment of syntax in the line of the much simpler
proposal of TFP11. Specifically:
v(w,r,c) = Lr?1(VV
T )w,:?Vc,: (TFP10)
v(w,r,c) = Vw,:?Lr(VV
T )c,: (TFP11)
where V is a I? J syntactic input matrix, i.e. the
columns are (word, relation) pairs. For simplification,
the columns of V are reordered such that syntactic
relations form continuous regions. Lr is a lifting map
similar to that of Equation (3) as it maps I- into J-
dimensional vectors: the resulting vector is equal to
the original one in the column region of relation r,
while everything else is 0. In the above equations we
use the standard Matlab notation, Vw,: denoting a row
vector in matrix V .
We evaluate these models on a paraphrase ranking
task, using the SemEval 2007 Lexical Substitution
Task (LST) dataset: the models are given a target
word in context plus a list of potential synonyms
(substitution candidates) ranging over all senses of
the target word. The models have to decide to what
extent each substitution candidate is a synonym of
the target in the given context. We omit the precise de-
scription of the evaluation setting here, as we follow
the methodology described in Thater et al (2011).
Results are shown in Table 1, where the first col-
umn gives the GAP (Generalized Average Precision)
score of the model and the second column gives
the difference to the result reported in the literature.
TFP10 and EP08 perform much better than the origi-
nal proposals, as we obtain very significant gains of
4 and 14 GAP points.
ted in the brief presentation in Section 2), which are difficult to
tune (Erk and Pad? (2009)), disappear this way.
613
We can observe that the differences between the
three methods, when simplified and tested in an uni-
fied setting, largely disappear. This is to be expected
as all three methods implement very similar, all moti-
vated intuitions: TFP11 reweights the vector of the
target acquire with the second order vector of the
context knowledge, i.e. with the vector of similarities
of knowledge to all other words in the vocabulary.
TFP10 takes a complementary approach: it reweights
the vector of knowledge with the second order vector
of acquire. In both these methods, anything outside
the object (object?1 respectively) region of the space,
is set to 0. The variant of EP08 that we implement is
very similar to TFP11, however it compares knowl-
edge to all other words in the vocabulary only using
occurrences as objects while TFP11 takes all syntac-
tic relations into account.
Note that TFP10 and TFP11 operate on comple-
mentary syntactic regions of the vectors. For this
reason the two models can be trivially combined.
The combined model (TFP10+11) achieves even bet-
ter results: the difference to TFP11 is small, however
statistically significant at level p < 0.05.
Implementation details. Straightforward imple-
mentations of the three models are computationally
expensive, as they all use ?second order? vectors to
implement contextualization of a target word. Our re-
formulation in terms of matrix operations allows for
efficient implementations, which take advantage of
the sparsity of the input matrix V : contextualization
of a target word runs in O(nnz(V )), where nnz is the
number of non-zero entries. Note that ranking not
only a small set of predefined substitution candidates,
as in the experiment above, but also ranking the en-
tire vocabulary runs in O(nnz(V )). On this task, this
overall running time is in fact identical to that of sim-
pler methods such as those of Mitchell and Lapata
(2008).
In our experiments, we use GigaWord to extract
a syntactic input matrix V of size ? 2M?7M. V is
only 4.5?10?06 dense. Note that because of the sim-
ple operations involved, we do not need to compute
or store the entire VV T matrix, which is much denser
than V (we have estimated order of 1010 entries). The
sparsity of V allows for very efficient computations
in practice: the best single model, TFP11, runs in
less than 0.2s/0.4s per LST instance, for ranking the
candidate list/entire vocabulary in a Python imple-
mentation using scipy.sparse, on a standard 1GHz
processor.
5 Conclusions
In this paper, we have compared three related vec-
tor space models of word meaning in context. We
have reformulated the models and showed that they
are in fact very similar. We also showed that the
different performances reported in the literature are
only to some extent due to the differences in the
models: We evaluated simplified variants of these
and obtained results which are (much) better than
previously reported, bringing the three models much
closer together in terms of performance. Aside from
clarifying the precise relationship between the three
models under consideration, our reformulation has
the additional benefit of allowing the design of a
straightforward and efficient implementation.
Finally, our focus on these methods is justified by
their clear advantages over other classes of models:
unlike token-based or latent variable methods, they
are much simpler and require no parameter tuning.
Furthermore, they also obtain state of the art results
on the paraphrase ranking task, outperforming other
simple type-based methods (see (Van de Cruys et
al., 2011) and (? S?aghdha and Korhonen, 2011) for
results of other methods on this data).
Acknowledgments. This work was partially sup-
ported by the Cluster of Excellence ?Multimodal
Computing and Interaction", funded by the German
Excellence Initiative.
References
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
EMNLP 2010, Cambridge, MA.
Katrin Erk and Sebastian Pad?. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of EMNLP 2008, Honolulu, HI, USA.
Katrin Erk and Sebastian Pad?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL 2010 Short Papers, Uppsala, Sweden.
614
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Diarmuid ? S?aghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP 2011.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Proc. of
IJCNLP 2011.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of NAACL 2010, Los Angeles, California.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of IJCNLP 2011.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2011. Latent vector weighting for word meaning in
context. In Proceedings of EMNLP 2011.
615
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53?57,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A relatedness benchmark to test the role of determiners
in compositional distributional semantics
Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Distributional models of semantics cap-
ture word meaning very effectively, and
they have been recently extended to ac-
count for compositionally-obtained rep-
resentations of phrases made of content
words. We explore whether compositional
distributional semantic models can also
handle a construction in which grammat-
ical terms play a crucial role, namely de-
terminer phrases (DPs). We introduce a
new publicly available dataset to test dis-
tributional representations of DPs, and we
evaluate state-of-the-art models on this set.
1 Introduction
Distributional semantics models (DSMs) approx-
imate meaning with vectors that record the dis-
tributional occurrence patterns of words in cor-
pora. DSMs have been effectively applied to in-
creasingly more sophisticated semantic tasks in
linguistics, artificial intelligence and cognitive sci-
ence, and they have been recently extended to
capture the meaning of phrases and sentences via
compositional mechanisms. However, scaling up
to larger constituents poses the issue of how to
handle grammatical words, such as determiners,
prepositions, or auxiliaries, that lack rich concep-
tual content, and operate instead as the logical
?glue? holding sentences together.
In typical DSMs, grammatical words are treated
as ?stop words? to be discarded, or at best used
as context features in the representation of content
words. Similarly, current compositional DSMs
(cDSMs) focus almost entirely on phrases made
of two or more content words (e.g., adjective-noun
or verb-noun combinations) and completely ig-
nore grammatical words, to the point that even
the test set of transitive sentences proposed by
Grefenstette and Sadrzadeh (2011) contains only
Tarzan-style statements with determiner-less sub-
jects and objects: ?table show result?, ?priest say
mass?, etc. As these examples suggest, however,
as soon as we set our sight on modeling phrases
and sentences, grammatical words are hard to
avoid. Stripping off grammatical words has more
serious consequences than making you sound like
the Lord of the Jungle. Even if we accept the
view of, e.g., Garrette et al (2013), that the log-
ical framework of language should be left to other
devices than distributional semantics, and the lat-
ter should be limited to similarity scoring, still ig-
noring grammatical elements is going to dramat-
ically distort the very similarity scores (c)DSMs
should provide. If we want to use a cDSM for
the classic similarity-based paraphrasing task, the
model shouldn?t conclude that ?The table shows
many results? is identical to ?the table shows no
results? since the two sentences contain the same
content words, or that ?to kill many rats? and ?to
kill few rats? are equally good paraphrases of ?to
exterminate rats?.
We focus here on how cDSMs handle determin-
ers and the phrases they form with nouns (deter-
miner phrases, or DPs).1 While determiners are
only a subset of grammatical words, they are a
large and important subset, constituting the natu-
ral stepping stone towards sentential distributional
semantics: Compositional methods have already
been successfully applied to simple noun-verb and
noun-verb-noun structures (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011), and de-
terminers are just what is missing to turn these
skeletal constructions into full-fledged sentences.
Moreover, determiner-noun phrases are, in super-
ficial syntactic terms, similar to the adjective-noun
phrases that have already been extensively studied
from a cDSM perspective by Baroni and Zampar-
1Some linguists refer to what we call DPs as noun phrases
or NPs. We say DPs simply to emphasize our focus on deter-
miners.
53
elli (2010), Guevara (2010) and Mitchell and Lap-
ata (2010). Thus, we can straightforwardly extend
the methods already proposed for adjective-noun
phrases to DPs.
We introduce a new task, a similarity-based
challenge, where we consider nouns that are
strongly conceptually related to certain DPs and
test whether cDSMs can pick the most appropri-
ate related DP (e.g., monarchy is more related to
one ruler than many rulers).2 We make our new
dataset publicly available, and we hope that it will
stimulate further work on the distributional seman-
tics of grammatical elements.3
2 Composition models
Interest in compositional DSMs has skyrocketed
in the last few years, particularly since the influ-
ential work of Mitchell and Lapata (2008; 2009;
2010), who proposed three simple but effective
composition models. In these models, the com-
posed vectors are obtained through component-
wise operations on the constituent vectors. Given
input vectors u and v, the multiplicative model
(mult) returns a composed vector p with: pi =
uivi. In the weighted additive model (wadd), the
composed vector is a weighted sum of the two in-
put vectors: p = ?u+?v, where ? and ? are two
scalars. Finally, in the dilation model, the output
vector is obtained by first decomposing one of the
input vectors, say v, into a vector parallel to u and
an orthogonal vector. Following this, the parallel
vector is dilated by a factor ? before re-combining.
This results in: p = (?? 1)?u,v?u+ ?u,u?v.
A more general form of the additive model
(fulladd) has been proposed by Guevara (2010)
(see also Zanzotto et al (2010)). In this approach,
the two vectors to be added are pre-multiplied by
weight matrices estimated from corpus-extracted
examples: p = Au+Bv.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application. The former model adjective-noun
phrases by treating the adjective as a function from
nouns onto modified nouns. Given that linear
functions can be expressed by matrices and their
application by matrix-by-vector multiplication, a
2Baroni et al (2012), like us, study determiner phrases
with distributional methods, but they do not model them com-
positionally.
3Dataset and code available from clic.cimec.
unitn.it/composes.
functor (such as the adjective) is represented by a
matrix U to be multiplied with the argument vec-
tor v (e.g., the noun vector): p = Uv. Adjective
matrices are estimated from corpus-extracted ex-
amples of noun vectors and corresponding output
adjective-noun phrase vectors, similarly to Gue-
vara?s approach.4
3 The noun-DP relatedness benchmark
Paraphrasing a single word with a phrase is a
natural task for models of compositionality (Tur-
ney, 2012; Zanzotto et al, 2010) and determin-
ers sometimes play a crucial role in defining the
meaning of a noun. For example a trilogy is com-
posed of three works, an assemblage includes sev-
eral things and an orchestra is made of many
musicians. These examples are particularly in-
teresting, since they point to a ?conceptual? use
of determiners, as components of the stable and
generic meaning of a content word (as opposed to
situation-dependent deictic and anaphoric usages):
for these determiners the boundary between con-
tent and grammatical word is somewhat blurred,
and they thus provide a good entry point for testing
DSM representations of DPs on a classic similarity
task. In other words, we can set up an experiment
in which having an effective representation of the
determiner is crucial in order to obtain the correct
result.
Using regular expressions over WordNet
glosses (Fellbaum, 1998) and complementing
them with definitions from various online dic-
tionaries, we constructed a list of more than 200
nouns that are strongly conceptually related to a
specific DP. We created a multiple-choice test set
by matching each noun with its associated DP
(target DP), two ?foil? DPs sharing the same noun
as the target but combined with other determiners
(same-N foils), one DP made of the target deter-
miner combined with a random noun (same-D
foil), the target determiner (D foil), and the target
noun (N foil). A few examples are shown in Table
1. After the materials were checked by all authors,
two native speakers took the multiple-choice test.
We removed the cases (32) where these subjects
provided an unexpected answer. The final set,
4Other approaches to composition in DSMs have been re-
cently proposed by Socher et al (2012) and Turney (2012).
We leave their empirical evaluation on DPs to further work,
in the first case because it is not trivial to adapt their complex
architecture to our setting; in the other because it is not clear
how Turney would extend his approach to represent DPs.
54
noun target DP same-N foil 1 same-N foil 2 same-D foil D foil N foil
duel two opponents various opponents three opponents two engineers two opponents
homeless no home too few homes one home no incision no home
polygamy several wives most wives fewer wives several negotiators several wives
opulence too many goods some goods no goods too many abductions too many goods
Table 1: Examples from the noun-DP relatedness benchmark
characterized by full subject agreement, contains
173 nouns, each matched with 6 possible answers.
The target DPs contain 23 distinct determiners.
4 Setup
Our semantic space provides distributional repre-
sentations of determiners, nouns and DPs. We
considered a set of 50 determiners that include all
those in our benchmark and range from quanti-
fying determiners (every, some. . . ) and low nu-
merals (one to four), to multi-word units analyzed
as single determiners in the literature, such as a
few, all that, too much. We picked the 20K most
frequent nouns in our source corpus considering
singular and plural forms as separate words, since
number clearly plays an important role in DP se-
mantics. Finally, for each of the target determiners
we added to the space the 2K most frequent DPs
containing that determiner and a target noun.
Co-occurrence statistics were collected from the
concatenation of ukWaC, a mid-2009 dump of the
English Wikipedia and the British National Cor-
pus,5 with a total of 2.8 billion tokens. We use
a bag-of-words approach, counting co-occurrence
with all context words in the same sentence with
a target item. We tuned a number of parameters
on the independent MEN word-relatedness bench-
mark (Bruni et al, 2012). This led us to pick the
top 20K most frequent content word lemmas as
context items, Pointwise Mutual Information as
weighting scheme, and dimensionality reduction
by Non-negative Matrix Factorization.
Except for the parameter-free mult method, pa-
rameters of the composition methods are esti-
mated by minimizing the average Euclidean dis-
tance between the model-generated and corpus-
extracted vectors of the 20K DPs we consider.6
For the lexfunc model, we assume that the deter-
miner is the functor and the noun is the argument,
5wacky.sslmit.unibo.it; www.natcorp.ox.
ac.uk
6All vectors are normalized to unit length before compo-
sition. Note that the objective function used in estimation
minimizes the distance between model-generated and corpus-
extracted vectors. We do not use labeled evaluation data to
optimize the model parameters.
method accuracy method accuracy
lexfunc 39.3 noun 17.3
fulladd 34.7 random 16.7
observed 34.1 mult 12.7
dilation 31.8 determiner 4.6
wadd 23.1
Table 2: Percentage accuracy of composition
methods on the relatedness benchmark
and estimate separate matrices representing each
determiner using the 2K DPs in the semantic space
that contain that determiner. For dilation, we treat
direction of stretching as a parameter, finding that
it is better to stretch the noun.
Similarly to the classic TOEFL synonym detec-
tion challenge (Landauer and Dumais, 1997), our
models tackle the relatedness task by measuring
cosines between each target noun and the candi-
date answers and returning the item with the high-
est cosine.
5 Results
Table 2 reports the accuracy results (mean ranks
of correct answers confirm the same trend). All
models except mult and determiner outperform the
trivial random guessing baseline, although they
are all well below the 100% accuracy of the hu-
mans who took our test. For the mult method we
observe a very strong bias for choosing a single
word as answer (>60% of the times), which in
the test set is always incorrect. This leads to its
accuracy being below the chance level. We sus-
pect that the highly ?intersective? nature of this
model (we obtain very sparse composed DP vec-
tors, only ?4% dense) leads to it not being a re-
liable method for comparing sequences of words
of different length: Shorter sequences will be con-
sidered more similar due to their higher density.
The determiner-only baseline (using the vector of
the component determiner as surrogate for the DP)
fails because D vectors tend to be far from N vec-
tors, thus the N foil is often preferred to the correct
response (that is represented, for this baseline, by
its D). In the noun-only baseline (use the vector
of the component noun as surrogate for the DP),
55
the correct response is identical to the same-N and
N foils, thus forcing a random choice between
these. Not surprisingly, this approach performs
quite badly. The observed DP vectors extracted di-
rectly from the corpus compete with the top com-
positional methods, but do not surpass them.7
The lexfunc method is the best compositional
model, indicating that its added flexibility in mod-
eling composition pays off empirically. The ful-
ladd model is not as good, but also performs well.
The wadd and especially dilation models perform
relatively well, but they are penalized by the fact
that they assign more weight to the noun vectors,
making the right answer dangerously similar to the
same-N and N foils.
Taking a closer look at the performance of the
best model (lexfunc), we observe that it is not
equally distributed across determiners. Focusing
on those determiners appearing in at least 4 cor-
rect answers, they range from those where lexfunc
performance was very significantly above chance
(p<0.001 of equal or higher chance performance):
too few, all, four, too much, less, several; to
those on which performance was still significant
but less impressively so (0.001<p< 0.05): sev-
eral, no, various, most, two, too many, many, one;
to those where performance was not significantly
better than chance at the 0.05 level: much, more,
three, another. Given that, on the one hand, per-
formance is not constant across determiners, and
on the other no obvious groupings can account
for their performance difference (compare the ex-
cellent lexfunc performance on four to the lousy
one on three!), future research should explore the
contextual properties of specific determiners that
make them more or less amenable to be captured
by compositional DSMs.
6 Conclusion
DSMs, even when applied to phrases, are typically
seen as models of content word meaning. How-
ever, to scale up compositionally beyond the sim-
plest constructions, cDSMs must deal with gram-
matical terms such as determiners. This paper
started exploring this issue by introducing a new
and publicly available set testing DP semantics in
a similarity-based task and using it to systemati-
cally evaluate, for the first time, cDSMs on a con-
7The observed method is in fact at advantage in our ex-
periment because a considerable number of DP foils are not
found in the corpus and are assigned similarity 0 with the tar-
get.
struction involving grammatical words. The most
important take-home message is that distributional
representations are rich enough to encode infor-
mation about determiners, achieving performance
well above chance on the new benchmark.
Theoretical considerations would lead one to
expect a ?functional? approach to determiner rep-
resentations along the lines of Baroni and Zampar-
elli (2010) and Coecke et al (2010) to outperform
those approaches that combine vectors separately
representing determiners and nouns. This predic-
tion was largely borne out in the results, although
the additive models, and particularly fulladd, were
competitive rivals.
We attempted to capture the distributional se-
mantics of DPs using a fairly standard, ?vanilla?
semantic space characterized by latent dimensions
that summarize patterns of co-occurrence with
content word contexts. By inspecting the con-
text words that are most associated with the var-
ious latent dimensions we obtained through Non-
negative Matrix Factorization, we notice how they
are capturing broad, ?topical? aspects of meaning
(the first dimension is represented by scripture, be-
liever, resurrection, the fourth by fever, infection,
infected, and so on). Considering the sort of se-
mantic space we used (which we took to be a rea-
sonable starting point because of its effectiveness
in a standard lexical task), it is actually surpris-
ing that we obtained the significant results we ob-
tained. Thus, a top priority in future work is to ex-
plore different contextual features, such as adverbs
and grammatical terms, that might carry informa-
tion that is more directly relevant to the semantics
of determiners.
Another important line of research pertains to
improving composition methods: Although the
best model, at 40% accuracy, is well above chance,
we are still far from the 100% performance of hu-
mans. We will try, in particular, to include non-
linear transformations in the spirit of Socher et al
(2012), and look for better ways to automatically
select training data.
Last but not least, in the near future we
would like to test if cDSMs, besides dealing with
similarity-based aspects of determiner meaning,
can also help in capturing those formal properties
of determiners, such as monotonicity or definite-
ness, that theoretical semanticists have been tradi-
tionally interested in.
56
7 Acknowledgments
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A
formal approach to linking logical form and vector-
space lexical semantics. In H. Bunt, J. Bos, and
S. Pulman, editors, Computing Meaning, Vol. 4. In
press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
57
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31?36,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DISSECT - DIStributional SEmantics Composition Toolkit
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
We introduce DISSECT, a toolkit to
build and explore computational models
of word, phrase and sentence meaning
based on the principles of distributional
semantics. The toolkit focuses in partic-
ular on compositional meaning, and im-
plements a number of composition meth-
ods that have been proposed in the litera-
ture. Furthermore, DISSECT can be use-
ful to researchers and practitioners who
need models of word meaning (without
composition) as well, as it supports var-
ious methods to construct distributional
semantic spaces, assessing similarity and
even evaluating against benchmarks, that
are independent of the composition infras-
tructure.
1 Introduction
Distributional methods for meaning similarity are
based on the observation that similar words oc-
cur in similar contexts and measure similarity
based on patterns of word occurrence in large cor-
pora (Clark, 2012; Erk, 2012; Turney and Pan-
tel, 2010). More precisely, they represent words,
or any other target linguistic elements, as high-
dimensional vectors, where the dimensions repre-
sent context features. Semantic relatedness is as-
sessed by comparing vectors, leading, for exam-
ple, to determine that car and vehicle are very sim-
ilar in meaning, since they have similar contextual
distributions. Despite the appeal of these meth-
ods, modeling words in isolation has limited ap-
plications and ideally we want to model semantics
beyond word level by representing the meaning of
phrases or sentences. These combinations are in-
finite and compositional methods are called for to
derive the meaning of a larger construction from
the meaning of its parts. For this reason, the ques-
tion of compositionality within the distributional
paradigm has received a lot of attention in recent
years and a number of compositional frameworks
have been proposed in the distributional seman-
tic literature, see, e.g., Coecke et al (2010) and
Mitchell and Lapata (2010). For example, in such
frameworks, the distributional representations of
red and car may be combined, through various op-
erations, in order to obtain a vector for red car.
The DISSECT toolkit (http://clic.
cimec.unitn.it/composes/toolkit)
is, to the best of our knowledge, the first to
provide an easy-to-use implementation of many
compositional methods proposed in the literature.
As such, we hope that it will foster further work
on compositional distributional semantics, as well
as making the relevant techniques easily available
to those interested in their many potential applica-
tions, e.g., to context-based polysemy resolution,
recognizing textual entailment or paraphrase
detection. Moreover, the DISSECT tools to
construct distributional semantic spaces from
raw co-occurrence counts, to measure similarity
and to evaluate these spaces might also be of
use to researchers who are not interested in the
compositional framework. DISSECT is freely
available under the GNU General Public License.
2 Building and composing distributional
semantic representations
The pipeline from corpora to compositional mod-
els of meaning can be roughly summarized as con-
sisting of three stages:1
1. Extraction of co-occurrence counts from cor-
pora In this stage, an input corpus is used to ex-
tract counts of target elements co-occurring with
some contextual features. The target elements
can vary from words (for lexical similarity), to
pairs of words (e.g., for relation categorization),
1See Turney and Pantel (2010) for a technical overview of
distributional methods for semantics.
31
to paths in syntactic trees (for unsupervised para-
phrasing). Context features can also vary from
shallow window-based collocates to syntactic de-
pendencies.
2. Transformation of the raw counts This
stage may involve the application of weighting
schemes such as Pointwise Mutual Information,
feature selection, dimensionality reduction meth-
ods such as Singular Value Decomposition, etc.
The goal is to eliminate the biases that typically
affect raw counts and to produce vectors which
better approximate similarity in meaning.
3. Application of composition functions
Once meaningful representations have been
constructed for the atomic target elements of
interest (typically, words), various methods, such
as vector addition or multiplication, can be used
for combining them to derive context-sensitive
representations or for constructing representations
for larger phrases or even entire sentences.
DISSECT can be used for the second and
third stages of this pipeline, as well as to measure
similarity among the resulting word or phrase vec-
tors. The first step is highly language-, task- and
corpus-annotation-dependent. We do not attempt
to implement all the corpus pre-processing and
co-occurrence extraction routines that it would
require to be of general use, and expect instead as
input a matrix of raw target-context co-occurrence
counts.2 DISSECT provides various methods to
re-weight the counts with association measures,
dimensionality reduction methods as well as the
composition functions proposed by Mitchell and
Lapata (2010) (Additive, Multiplicative and Dila-
tion), Baroni and Zamparelli (2010)/Coecke et al
(2010) (Lexfunc) and Guevara (2010)/Zanzotto et
al. (2010) (Fulladd). In DISSECT we define and
implement these in a unified framework and in a
computationally efficient manner. The focus of
DISSECT is to provide an intuitive interface for
researchers and to allow easy extension by adding
other composition methods.
3 DISSECT overview
DISSECT is written in Python. We provide many
standard functionalities through a set of power-
2These counts can be read from a text file containing two
strings (the target and context items) and a number (the corre-
sponding count) on each line (e.g., maggot food 15) or
from a matrix in format word freq1 freq2 ...
#create a semantic space from counts in
#dense format("dm"): word freq1 freq2 ..
ss = Space.build(data="counts.txt",
format="dm")
#apply transformations
ss = ss.apply(PpmiWeighting())
ss = ss.apply(Svd(300))
#retrieve the vector of a target element
print ss.get_row("car")
Figure 1: Creating a semantic space.
ful command-line tools, however users with ba-
sic Python familiarity are encouraged to use the
Python interface that DISSECT provides. This
section focuses on this interface (see the online
documentation on how to perform the same oper-
ations with the command-line tools), that consists
of the following top-level packages:
#DISSECT packages
composes.matrix
composes.semantic_space
composes.transformation
composes.similarity
composes.composition
composes.utils
Semantic spaces and transforma-
tions The concept of a semantic space
(composes.semantic space) is at the
core of the DISSECT toolkit. A semantic
space consists of co-occurrence values, stored
as a matrix, together with strings associated to
the rows of this matrix (by design, the target
linguistic elements) and a (potentially empty)
list of strings associated to the columns (the
context features). A number of transforma-
tions (composes.transformation) can
be applied to semantic spaces. We implement
weighting schemes such as positive Pointwise
Mutual Information (ppmi) and Local Mu-
tual Information, feature selection methods,
dimensionality reduction (Singular Value De-
composition (SVD) and Nonnegative Matrix
Factorization (NMF)), and new methods can
be easily added.3 Going from raw counts to a
transformed space is accomplished in just a few
lines of code (Figure 1).
3The complete list of transformations currently sup-
ported can be found at http://clic.cimec.unitn.
it/composes/toolkit/spacetrans.html#
spacetrans.
32
#load a previously saved space
ss = io_utils.load("ss.pkl")
#compute cosine similarity
print ss.get_sim("car", "book",
CosSimilarity())
#the two nearest neighbours of "car"
print ss.get_neighbours("car", 2,
CosSimilarity())
Figure 2: Similarity queries in a semantic space.
Furthermore DISSECT allows the pos-
sibility of adding new data to a seman-
tic space in an online manner (using the
semantic space.peripheral space
functionality). This can be used as a way to effi-
ciently expand a co-occurrence matrix with new
rows, without re-applying the transformations to
the entire space. In some other cases, the user may
want to represent phrases that are specialization
of words already existing in the space (e.g.,
slimy maggot and maggot), without distorting the
computation of association measures by counting
the same context twice. In this case, adding slimy
maggot as a ?peripheral? row to a semantic space
that already contains maggot implements the
desired behaviour.
Similarity queries Semantic spaces are used for
the computation of similarity scores. DISSECT
provides a series of similarity measures such as co-
sine, inverse Euclidean distance and Lin similarity,
implemented in the composes.similarity
package. Similarity of two elements can be com-
puted within one semantic space or across two
spaces that have the same dimensionality. Figure
2 exemplifies (word) similarity computations with
DISSECT.
Composition functions Composition functions
in DISSECT (composes.composition) take
as arguments a list of element pairs to be com-
posed, and one or two spaces where the elements
to be composed are represented. They return a se-
mantic space containing the distributional repre-
sentations of the composed items, which can be
further transformed, used for similarity queries, or
used as inputs to another round of composition,
thus scaling up beyond binary composition. Fig-
ure 3 shows a Multiplicative composition exam-
ple. See Table 1 for the currently available com-
position models, their definitions and parameters.
Model Composition function Parameters
Add. w1~u+ w2~v w1(= 1), w2(= 1)
Mult. ~u ~v -
Dilation ||~u||22~v + (?? 1)?~u,~v?~u ?(= 2)
Fulladd W1~u+W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Table 1: Currently implemented composition
functions of inputs (u, v) together with parame-
ters and their default values in parenthesis, where
defined. Note that in Lexfunc each functor word
corresponds to a separate matrix or tensor Au (Ba-
roni and Zamparelli, 2010).
Parameter estimation All composition models
except Multiplicative have parameters to be esti-
mated. For simple models with few parameters,
such as as Additive, the parameters can be passed
by hand. However, DISSECT supports automated
parameter estimation from training examples. In
particular, we extend to all composition methods
the idea originally proposed by Baroni and Zam-
parelli (2010) for Lexfunc and Guevara (2010) for
Fulladd, namely to use corpus-extracted example
vectors of both the input (typically, words) and
output elements (typically, phrases) in order to op-
timize the composition operation parameters. The
problem can be generally stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where U, V and P are matrices containing input
and output vectors respectively. For example U
may contain adjective vectors such as red, blue,
V noun vectors such as car, sky and P corpus-
extracted vectors for the corresponding phrases
red car, blue sky. fcomp? is a composition func-
tion and ? stands for a list of parameters that this
composition function is associated with.4 We im-
plement standard least-squares estimation meth-
ods as well as Ridge regression with the option
for generalized cross-validation, but other meth-
ods such as partial least-squares regression can be
easily added. Figure 4 exemplifies the Fulladd
model.
Composition output examples DISSECT pro-
vides functions to evaluate (compositional) distri-
butional semantic spaces against benchmarks in
the composes.utils package. However, as a
more qualitatively interesting example of what can
be done with DISSECT, Table 2 shows the nearest
4Details on the extended corpus-extracted vector estima-
tion method in DISSECT can be found in Dinu et al (2013).
33
#instantiate a multiplicative model
mult_model = Multiplicative()
#use the model to compose words from input space input_space
comp_space = mult_model.compose([("red", "book", "my_red_book"),
("red", "car", "my_red_car")],
input_space)
#compute similarity of: 1) two composed phrases and 2) a composed phrase and a word
print comp_space.get_sim("my_red_book", "my_red_car", CosSimilarity())
print comp_space.get_sim("my_red_book", "book", CosSimilarity(), input_space)
Figure 3: Creating and using Multiplicative phrase vectors.
#training data for learning an adjective-noun phrase model
train_data = [("red","book","red_book"), ("blue","car","blue_car")]
#train a fulladd model
fa_model = FullAdditive()
fa_model.train(train_data, input_space, phrase_space)
#use the model to compose a phrase from new words and retrieve its nearest neighb.
comp_space = fa_model.compose([("yellow", "table", "my_yellow_table")], input_space)
print comp_space.get_neighbours("my_yellow_table", 10, CosSimilarity())
Figure 4: Estimating a Fulladd model and using it to create phrase vectors.
Target Method Neighbours
florist Corpus Harrod, wholesaler, stockist
flora + -ist
Fulladd flora, fauna, ecologist
Lexfunc ornithologist, naturalist, botanist
Additive flora, fauna, ecosystem
Table 3: Compositional models for morphol-
ogy. Top 3 neighbours of florist using its (low-
frequency) corpus-extracted vector, and when the
vector is obtained through composition of flora
and -ist with Fulladd, Lexfunc and Additive.
neighbours of false belief obtained through com-
position with the Fulladd, Lexfunc and Additive
models. In Table 3, we exemplify a less typical ap-
plication of compositional models to derivational
morphology, namely obtaining a representation of
florist compositionally from distributional repre-
sentations of flora and -ist (Lazaridou et al, 2013).
4 Main features
Support for dense and sparse representations
Co-occurrence matrices, as extracted from text,
tend to be very sparse structures, especially when
using detailed context features which include syn-
tactic information, for example. On the other
hand, dimensionality reduction operations, which
are often used in distributional models, lead to
smaller, dense structures, for which sparse rep-
resentations are not optimal. This is our motiva-
tion for supporting both dense and sparse repre-
sentations. The choice of dense vs. sparse is ini-
tially determined by the input format, if a space
is created from co-occurrence counts. By default,
DISSECT switches to dense representations af-
ter dimensionality reduction, however the user can
freely switch from one representation to the other,
in order to optimize computations. For this pur-
pose DISSECT provides wrappers around matrix
operations, as well as around common linear alge-
bra operations, in the composes.matrix pack-
age. The underlying Python functionality is pro-
vided by numpy.array and scipy.sparse.
Efficient computations DISSECT is optimized
for speed since most operations are cast as matrix
operations, that are very efficiently implemented
in Python?s numpy and scipy modules5. Ta-
bles 4 and 5 show running times for typical DIS-
SECT operations: application of the ppmi weight-
ing scheme, nearest neighbour queries and estima-
tion of composition function parameters (on a 2.1
5For SVD on sparse structures, we use sparsesvd
(https://pypi.python.org/pypi/sparsesvd/).
For NMF, we adapted http://www.csie.ntu.edu.
tw/?cjlin/nmf/ (Lin, 2007).
34
Target Method Neighbours
belief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmatic
false belief
Fulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-set
Lexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehood
Additive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denial
Table 2: Top nearest neighbours of belief and of false belief obtained through composition with the
Fulladd, Lexfunc and Additive models.
Method Fulladd Lexfunc Add. Dilation
Time (s.) 2864 787 46 68
Table 4: Composition model parameter estimation
times (in seconds) for 1 million training points in
300-dimensional space.
Matrix size (nnz) Ppmi Query
100Kx300 (30M) 5.8 0.5
100Kx100K (250M) 52.6 9.5
Table 5: Running times (in seconds) for 1) appli-
cation of ppmi weighting and 2) querying for the
top neighbours of a word (cosine similarity) for
different matrix sizes (nnz: number of non-zero
entries, in millions).
GHz machine). The price to pay for fast computa-
tions is that data must be stored in main memory.
We do not think that this is a major inconvenience.
For example, a typical symmetric co-occurrence
matrix extracted from a corpus of several billion
words, defining context in terms of 5-word win-
dows and considering the top 100K?100K most
frequent words, contains? 250 million entries and
requires only 2GB of memory for (double preci-
sion) storage.
Simple design We have opted for a very simple
and intuitive design as the classes interact in
very natural ways: A semantic space stores
the actual data matrix and structures to index
its rows and columns, and supports similarity
queries and transformations. Transformations
take one semantic space as input to return
another, transformed, space. Composition func-
tions take one or more input spaces and yield
a composed-elements space, which can further
undergo transformations and be used for similarity
queries. In fact, DISSECT semantic spaces also
support higher-order tensor representations, not
just vectors. Higher-order representations are
used, for example, to represent transitive verbs
and other multi-argument functors by Coecke
et al (2010) and Grefenstette et al (2013).
See http://clic.cimec.unitn.it/
composes/toolkit/composing.html for
an example of using DISSECT for estimating
such tensors.
Extensive documentation The DISSECT
documentation can be found at http://clic.
cimec.unitn.it/composes/toolkit.
We provide a tutorial which guides the user
through the creation of some toy semantic spaces,
estimation of the parameters of composition
models and similarity computations in semantic
spaces. We also provide a full-scale example
of intransitive verb-subject composition. We
show how to go from co-occurrence counts to
composed representations and make the data used
in the examples available for download.
Comparison to existing software In terms of
design choices, DISSECT most resembles the
Gensim toolkit (R?ehu?r?ek and Sojka, 2010). How-
ever Gensim is intended for topic modeling, and
therefore diverges considerably from DISSECT in
its functionality. The SSpace package of Jurgens
and Stevens (2010) also overlaps to some degree
with DISSECT in terms of its intended use, how-
ever, like Gensim, it does not support composi-
tional operations that, as far as we know, are an
unique feature of DISSECT.
5 Future extensions
We implemented and are currently testing DIS-
SECT functions supporting other composition
methods, including the one proposed by Socher
et al (2012). Adding further methods is our top-
priority goal. In particular, several distributional
models of word meaning in context share impor-
tant similarities with composition models, and we
plan to add them to DISSECT. Dinu et al (2012)
show, for example, that well-performing, simpli-
fied variants of the method in Thater et al (2010),
Thater et al (2011) and Erk and Pado? (2008) can
be reduced to relatively simple matrix operations,
making them particularly suitable for a DISSECT
implementation.
35
DISSECT is currently optimized for the compo-
sition of many phrases of the same type. This is in
line with most of the current evaluations of com-
positional models, which focus on specific phe-
nomena, such as adjectival modification, noun-
noun compounds or intransitive verbs, to name a
few. In the future we plan to provide a module for
composing entire sentences, taking syntactic trees
as input and returning composed representations
for each node in the input trees.
Finally, we intend to make use of the exist-
ing Python plotting libraries to add a visualization
module to DISSECT.
6 Acknowledgments
We thank Angeliki Lazaridou for helpful dis-
cussions. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Stefan Thater, and So?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL HLT, pages 611?
615, Montreal, Canada.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. A general framework for the estimation of
distributional composition functions. In Proceed-
ings of ACL Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
In press.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Proceedings of
ACL, Sofia, Bulgaria. In press.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP,
pages 1134?1143, Chiang Mai, Thailand.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
36
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 238?247,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Don?t count, predict! A systematic comparison of
context-counting vs. context-predicting semantic vectors
Marco Baroni and Georgiana Dinu and Germ
?
an Kruszewski
Center for Mind/Brain Sciences (University of Trento, Italy)
(marco.baroni|georgiana.dinu|german.kruszewski)@unitn.it
Abstract
Context-predicting models (more com-
monly known as embeddings or neural
language models) are the new kids on the
distributional semantics block. Despite the
buzz surrounding these models, the litera-
ture is still lacking a systematic compari-
son of the predictive models with classic,
count-vector-based distributional semantic
approaches. In this paper, we perform
such an extensive evaluation, on a wide
range of lexical semantics tasks and across
many parameter settings. The results, to
our own surprise, show that the buzz is
fully justified, as the context-predicting
models obtain a thorough and resounding
victory against their count-based counter-
parts.
1 Introduction
A long tradition in computational linguistics has
shown that contextual information provides a good
approximation to word meaning, since semanti-
cally similar words tend to have similar contex-
tual distributions (Miller and Charles, 1991). In
concrete, distributional semantic models (DSMs)
use vectors that keep track of the contexts (e.g.,
co-occurring words) in which target terms appear
in a large corpus as proxies for meaning represen-
tations, and apply geometric techniques to these
vectors to measure the similarity in meaning of
the corresponding words (Clark, 2013; Erk, 2012;
Turney and Pantel, 2010).
It has been clear for decades now that raw co-
occurrence counts don?t work that well, and DSMs
achieve much higher performance when various
transformations are applied to the raw vectors,
for example by reweighting the counts for con-
text informativeness and smoothing them with di-
mensionality reduction techniques. This vector
optimization process is generally unsupervised,
and based on independent considerations (for ex-
ample, context reweighting is often justified by
information-theoretic considerations, dimension-
ality reduction optimizes the amount of preserved
variance, etc.). Occasionally, some kind of indi-
rect supervision is used: Several parameter set-
tings are tried, and the best setting is chosen based
on performance on a semantic task that has been
selected for tuning.
The last few years have seen the development
of a new generation of DSMs that frame the vec-
tor estimation problem directly as a supervised
task, where the weights in a word vector are set to
maximize the probability of the contexts in which
the word is observed in the corpus (Bengio et al,
2003; Collobert and Weston, 2008; Collobert et
al., 2011; Huang et al, 2012; Mikolov et al,
2013a; Turian et al, 2010). The traditional con-
struction of context vectors is turned on its head:
Instead of first collecting context vectors and then
reweighting these vectors based on various crite-
ria, the vector weights are directly set to optimally
predict the contexts in which the corresponding
words tend to appear. Since similar words occur
in similar contexts, the system naturally learns to
assign similar vectors to similar words.
This new way to train DSMs is attractive be-
cause it replaces the essentially heuristic stacking
of vector transforms in earlier models with a sin-
gle, well-defined supervised learning step. At the
same time, supervision comes at no manual anno-
tation cost, given that the context windows used
for training can be automatically extracted from
an unannotated corpus (indeed, they are the very
same data used to build traditional DSMs). More-
over, at least some of the relevant methods can ef-
ficiently scale up to process very large amounts of
input data.
1
1
The idea to directly learn a parameter vector based on
an objective optimum function is shared by Latent Dirichlet
238
We will refer to DSMs built in the traditional
way as count models (since they initialize vectors
with co-occurrence counts), and to their training-
based alternative as predict(ive) models.
2
Now,
the most natural question to ask, of course, is
which of the two approaches is best in empirical
terms. Surprisingly, despite the long tradition of
extensive evaluations of alternative count DSMs
on standard benchmarks (Agirre et al, 2009; Ba-
roni and Lenci, 2010; Bullinaria and Levy, 2007;
Bullinaria and Levy, 2012; Sahlgren, 2006; Pad?o
and Lapata, 2007), the existing literature contains
very little in terms of direct comparison of count
vs. predictive DSMs. This is in part due to the fact
that context-predicting vectors were first devel-
oped as an approach to language modeling and/or
as a way to initialize feature vectors in neural-
network-based ?deep learning? NLP architectures,
so their effectiveness as semantic representations
was initially seen as little more than an interest-
ing side effect. Sociological reasons might also be
partly responsible for the lack of systematic com-
parisons: Context-predictive models were devel-
oped within the neural-network community, with
little or no awareness of recent DSM work in com-
putational linguistics.
Whatever the reasons, we know of just three
works reporting direct comparisons, all limited in
their scope. Huang et al (2012) compare, in pass-
ing, one count model and several predict DSMs
on the standard WordSim353 benchmark (Table
3 of their paper). In this experiment, the count
model actually outperforms the best predictive ap-
proach. Instead, in a word-similarity-in-context
task (Table 5), the best predict model outperforms
the count model, albeit not by a large margin.
Blacoe and Lapata (2012) compare count and
predict representations as input to composition
functions. Count vectors make for better inputs
in a phrase similarity task, whereas the two repre-
sentations are comparable in a paraphrase classifi-
cation experiment.
3
Allocation (LDA) models (Blei et al, 2003; Griffiths et al,
2007), where parameters are set to optimize the joint prob-
ability distribution of words and documents. However, the
fully probabilistic LDA models have problems scaling up to
large data sets.
2
We owe the first term to Hinrich Sch?utze (p.c.). Predic-
tive DSMs are also called neural language models, because
their supervised context prediction training is performed with
neural networks, or, more cryptically, ?embeddings?.
3
We refer here to the updated results reported in
the erratum at http://homepages.inf.ed.ac.uk/
s1066731/pdf/emnlp2012erratum.pdf
Finally, Mikolov et al (2013d) compare their
predict models to ?Latent Semantic Analysis?
(LSA) count vectors on syntactic and semantic
analogy tasks, finding that the predict models are
highly superior. However, they provide very little
details about the LSA count vectors they use.
4
In this paper, we overcome the comparison
scarcity problem by providing a direct evaluation
of count and predict DSMs across many parameter
settings and on a large variety of mostly standard
lexical semantics benchmarks. Our title already
gave away what we discovered.
2 Distributional semantic models
Both count and predict models are extracted from
a corpus of about 2.8 billion tokens constructed
by concatenating ukWaC,
5
the English Wikipedia
6
and the British National Corpus.
7
For both model
types, we consider the top 300K most frequent
words in the corpus both as target and context ele-
ments.
2.1 Count models
We prepared the count models using the DISSECT
toolkit.
8
We extracted count vectors from sym-
metric context windows of two and five words to
either side of target. We considered two weight-
ing schemes: positive Pointwise Mutual Informa-
tion and Local Mutual Information (akin to the
widely used Log-Likelihood Ratio scheme) (Ev-
ert, 2005). We used both full and compressed vec-
tors. The latter were obtained by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000), Lin (2007) algorithm, with re-
duced sizes ranging from 200 to 500 in steps of
100. In total, 36 count models were evaluated.
Count models have such a long and rich his-
tory that we can only explore a small subset of
the counting, weighting and compressing meth-
ods proposed in the literature. However, it is
worth pointing out that the evaluated parameter
subset encompasses settings (narrow context win-
dow, positive PMI, SVD reduction) that have been
4
Chen et al (2013) present an extended empirical evalua-
tion, that is however limited to alternative context-predictive
models, and does not include the word2vec variant we use
here.
5
http://wacky.sslmit.unibo.it
6
http://en.wikipedia.org
7
http://www.natcorp.ox.ac.uk
8
http://clic.cimec.unitn.it/composes/
toolkit/
239
found to be most effective in the systematic explo-
rations of the parameter space conducted by Bul-
linaria and Levy (2007; 2012).
2.2 Predict models
We trained our predict models with the word2vec
toolkit.
9
The toolkit implements both the skip-
gram and CBOW approaches of Mikolov et
al. (2013a; 2013c). We experimented only with
the latter, which is also the more computationally-
efficient model of the two, following Mikolov et
al. (2013b) which recommends CBOW as more
suitable for larger datasets.
The CBOW model learns to predict the word in
the middle of a symmetric window based on the
sum of the vector representations of the words in
the window. We considered context windows of
2 and 5 words to either side of the central ele-
ment. We vary vector dimensionality within the
200 to 500 range in steps of 100. The word2vec
toolkit implements two efficient alternatives to the
standard computation of the output word proba-
bility distributions by a softmax classifier. Hi-
erarchical softmax is a computationally efficient
way to estimate the overall probability distribu-
tion using an output layer that is proportional to
log(unigram.perplexity(W )) instead of W (for
W the vocabulary size). As an alternative, nega-
tive sampling estimates the probability of an out-
put word by learning to distinguish it from draws
from a noise distribution. The number of these
draws (number of negative samples) is given by
a parameter k. We test both hierarchical softmax
and negative sampling with k values of 5 and 10.
Very frequent words such as the or a are not very
informative as context features. The word2vec
toolkit implements a method to downsize their ef-
fect (and simultaneously improve speed perfor-
mance). More precisely, words in the training
data are discarded with a probability that is pro-
portional to their frequency (capturing the same
intuition that motivates traditional count vector
weighting measures such as PMI). This is con-
trolled by a parameter t and words that occur with
higher frequency than t are aggressively subsam-
pled. We train models without subsampling and
with subsampling at t = 1e
?5
(the toolkit page
suggests 1e
?3
? 1e
?5
as a useful range based on
empirical observations).
In total, we evaluate 48 predict models, a num-
9
https://code.google.com/p/word2vec/
ber comparable to that of the count models we
consider.
2.3 Out-of-the-box models
Baroni and Lenci (2010) make the vectors of
their best-performing Distributional Memory (dm)
model available.
10
This model, based on the same
input corpus we use, exemplifies a ?linguistically
rich? count-based DSM, that relies on lemmas
instead or raw word forms, and has dimensions
that encode the syntactic relations and/or lexico-
syntactic patterns linking targets and contexts. Ba-
roni and Lenci showed, in a large scale evaluation,
that dm reaches near-state-of-the-art performance
in a variety of semantic tasks.
We also experiment with the popular predict
vectors made available by Ronan Collobert.
11
Fol-
lowing the earlier literature, with refer to them
as Collobert and Weston (cw) vectors. These are
100-dimensional vectors trained for two months
(!) on the Wikipedia. In particular, the vectors
were trained to optimize the task of choosing the
right word over a random alternative in the middle
of an 11-word context window (Collobert et al,
2011).
3 Evaluation materials
We test our models on a variety of benchmarks,
most of them already widely used to test and com-
pare DSMs. The following benchmark descrip-
tions also explain the figures of merit and state-
of-the-art results reported in Table 2.
Semantic relatedness A first set of semantic
benchmarks was constructed by asking human
subjects to rate the degree of semantic similarity
or relatedness between two words on a numeri-
cal scale. The performance of a computational
model is assessed in terms of correlation between
the average scores that subjects assigned to the
pairs and the cosines between the corresponding
vectors in the model space (following the previ-
ous art, we use Pearson correlation for rg, Spear-
man in all other cases). The classic data set of
Rubenstein and Goodenough (1965) (rg) consists
of 65 noun pairs. State of the art performance
on this set has been reported by Hassan and Mi-
halcea (2011) using a technique that exploits the
Wikipedia linking structure and word sense dis-
ambiguation techniques. Finkelstein et al (2002)
10
http://clic.cimec.unitn.it/dm/
11
http://ronan.collobert.com/senna/
240
introduced the widely used WordSim353 set (ws)
that, as the name suggests, consists of 353 pairs.
The current state of the art is reached by Halawi
et al (2012) with a method that is in the spirit
of the predict models, but lets synonymy infor-
mation from WordNet constrain the learning pro-
cess (by favoring solutions in which WordNet syn-
onyms are near in semantic space). Agirre et al
(2009) split the ws set into similarity (wss) and re-
latedness (wsr) subsets. The first contains tighter
taxonomic relations, such as synonymy and co-
hyponymy (king/queen) whereas the second en-
compasses broader, possibly topical or syntag-
matic relations (family/planning). We report state-
of-the-art performance on the two subsets from the
work of Agirre and colleagues, who used different
kinds of count vectors extracted from a very large
corpus (orders of magnitude larger than ours). Fi-
nally, we use (the test section of) MEN (men), that
comprises 1,000 word pairs. Bruni et al (2013),
the developers of this benchmark, achieve state-of-
the-art performance by extensive tuning on ad-hoc
training data, and by using both textual and image-
extracted features to represent word meaning.
Synonym detection The classic TOEFL (toefl)
set was introduced by Landauer and Dumais
(1997). It contains 80 multiple-choice questions
that pair a target term with 4 synonym candidates.
For example, for the target levied one must choose
between imposed (correct), believed, requested
and correlated. The DSMs compute cosines of
each candidate vector with the target, and pick the
candidate with largest cosine as their answer. Per-
formance is evaluated in terms of correct-answer
accuracy. Bullinaria and Levy (2012) achieved
100% accuracy by a very thorough exploration of
the count model parameter space.
Concept categorization Given a set of nominal
concepts, the task is to group them into natural cat-
egories (e.g., helicopters and motorcycles should
go to the vehicle class, dogs and elephants into the
mammal class). Following previous art, we tackle
categorization as an unsupervised clustering task.
The vectors produced by a model are clustered
into n groups (with n determined by the gold stan-
dard partition) using the CLUTO toolkit (Karypis,
2003), with the repeated bisections with global op-
timization method and CLUTO?s default settings
otherwise (these are standard choices in the liter-
ature). Performance is evaluated in terms of pu-
rity, a measure of the extent to which each cluster
contains concepts from a single gold category. If
the gold partition is reproduced perfectly, purity
reaches 100%; it approaches 0 as cluster quality
deteriorates. The Almuhareb-Poesio (ap) bench-
mark contains 402 concepts organized into 21 cat-
egories (Almuhareb, 2006). State-of-the-art purity
was reached by Rothenh?ausler and Sch?utze (2009)
with a count model based on carefully crafted syn-
tactic links. The ESSLLI 2008 Distributional Se-
mantic Workshop shared-task set (esslli) contains
44 concepts to be clustered into 6 categories (Ba-
roni et al, 2008) (we ignore here the 3- and 2-
way higher-level partitions coming with this set).
Katrenko and Adriaans (2008) reached top per-
formance on this set using the full Web as a cor-
pus and manually crafted, linguistically motivated
patterns. Finally, the Battig (battig) test set intro-
duced by Baroni et al (2010) includes 83 concepts
from 10 categories. Current state of the art was
reached by the window-based count model of Ba-
roni and Lenci (2010).
Selectional preferences We experiment with
two data sets that contain verb-noun pairs that
were rated by subjects for the typicality of the
noun as a subject or object of the verb (e.g., peo-
ple received a high average score as subject of
to eat, and a low score as object of the same
verb). We follow the procedure proposed by Ba-
roni and Lenci (2010) to tackle this challenge: For
each verb, we use the corpus-based tuples they
make available to select the 20 nouns that are most
strongly associated to the verb as subjects or ob-
jects, and we average the vectors of these nouns
to obtain a ?prototype? vector for the relevant ar-
gument slot. We then measure the cosine of the
vector for a target noun with the relevant proto-
type vector (e.g., the cosine of people with the eat-
ing subject prototype vector). Systems are eval-
uated by Spearman correlation of these cosines
with the averaged human typicality ratings. Our
first data set was introduced by Ulrike Pad?o (2007)
and includes 211 pairs (up). Top-performance was
reached by the supervised count vector system of
Herda?gdelen and Baroni (2009) (supervised in the
sense that they directly trained a classifier on gold
data, as opposed to the 0-cost supervision of the
context-learning methods). The mcrae set (McRae
et al, 1998) consists of 100 noun?verb pairs, with
top performance reached by the DepDM system of
Baroni and Lenci (2010), a count DSM relying on
241
syntactic information.
Analogy While all the previous data sets are rel-
atively standard in the DSM field to test traditional
count models, our last benchmark was introduced
in Mikolov et al (2013a) specifically to test pre-
dict models. The data-set contains about 9K se-
mantic and 10.5K syntactic analogy questions. A
semantic question gives an example pair (brother-
sister), a test word (grandson) and asks to find
another word that instantiates the relation illus-
trated by the example with respect to the test word
(granddaughter). A syntactic question is similar,
but in this case the relationship is of a grammatical
nature (work?works, speak. . . speaks). Mikolov
and colleagues tackle the challenge by subtract-
ing the second example term vector from the first,
adding the test term, and looking for the nearest
neighbour of the resulting vector (what is the near-
est neighbour of
~
brother?
~
sister+
~
grandson?).
Systems are evaluated in terms of proportion of
questions where the nearest neighbour from the
whole semantic space is the correct answer (the
given example and test vector triples are excluded
from the nearest neighbour search). Mikolov et al
(2013a) reach top accuracy on the syntactic subset
(ansyn) with a CBOW predict model akin to ours
(but trained on a corpus twice as large). Top ac-
curacy on the entire data set (an) and on the se-
mantic subset (ansem) was reached by Mikolov
et al (2013c) using a skip-gram predict model.
Note however that, because of the way the task
is framed, performance also depends on the size
of the vocabulary to be searched: Mikolov et al
(2013a) pick the nearest neighbour among vectors
for 1M words, Mikolov et al (2013c) among 700K
words, and we among 300K words.
Some characteristics of the benchmarks we use
are summarized in Table 1.
4 Results
Table 2 summarizes the evaluation results. The
first block of the table reports the maximum per-
task performance (across all considered parameter
settings) for count and predict vectors. The latter
emerge as clear winners, with a large margin over
count vectors in most tasks. Indeed, the predic-
tive models achieve an impressive overall perfor-
mance, beating the current state of the art in sev-
eral cases, and approaching it in many more. It is
worth stressing that, as reviewed in Section 3, the
state-of-the-art results were obtained in almost all
cases using specialized approaches that rely on ex-
ternal knowledge, manually-crafted rules, parsing,
larger corpora and/or task-specific tuning. Our
predict results were instead achieved by simply
downloading the word2vec toolkit and running it
with a range of parameter choices recommended
by the toolkit developers.
The success of the predict models cannot be
blamed on poor performance of the count mod-
els. Besides the fact that this would not explain
the near-state-of-the-art performance of the pre-
dict vectors, the count model results are actually
quite good in absolute terms. Indeed, in several
cases they are close, or even better than those at-
tained by dm, a linguistically-sophisticated count-
based approach that was shown to reach top per-
formance across a variety of tasks by Baroni and
Lenci (2010).
Interestingly, count vectors achieve perfor-
mance comparable to that of predict vectors only
on the selectional preference tasks. The up task
in particular is also the only benchmark on which
predict models are seriously lagging behind state-
of-the-art and dm performance. Recall from Sec-
tion 3 that we tackle selectional preference by cre-
ating average vectors representing typical verb ar-
guments. We conjecture that this averaging ap-
proach, that worked well for dm vectors, might
be problematic for prediction-trained vectors, and
we plan to explore alternative methods to build the
prototypes in future research.
Are our results robust to parameter choices, or
are they due to very specific and brittle settings?
The next few blocks of Table 2 address this ques-
tion. The second block reports results obtained
with single count and predict models that are best
in terms of average performance rank across tasks
(these are the models on the top rows of tables
3 and 4, respectively). We see that, for both ap-
proaches, performance is not seriously affected by
using the single best setup rather than task-specific
settings, except for a considerable drop in perfor-
mance for the best predict model on esslli (due to
the small size of this data set?), and an even more
dramatic drop of the count model on ansem. A
more cogent and interesting evaluation is reported
in the third block of Table 2, where we see what
happens if we use the single models with worst
performance across tasks (recall from Section 2
above that, in any case, we are exploring a space
of reasonable parameter settings, of the sort that an
242
name task measure source soa
rg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011)
(1965)
ws relatedness Spearman Finkelstein et al (2002) Halawi et al (2012)
wss relatedness Spearman Agirre et al (2009) Agirre et al (2009)
wsr relatedness Spearman Agirre et al (2009) Agirre et al (2009)
men relatedness Spearman Bruni et al (2013) Bruni et al (2013)
toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012)
(1997)
ap categorization purity Almuhareb (2006) Rothenh?ausler and Sch?utze
(2009)
esslli categorization purity Baroni et al (2008) Katrenko and Adriaans
(2008)
battig categorization purity Baroni et al (2010) Baroni and Lenci (2010)
up sel pref Spearman Pad?o (2007) Herda?gdelen and Baroni
(2009)
mcrae sel pref Spearman McRae et al (1998) Baroni and Lenci (2010)
an analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
ansyn analogy accuracy Mikolov et al (2013a) Mikolov et al (2013a)
ansem analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference
(source) and reference to current state-of-the-art system (soa).
rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansem
best setup on each task
cnt 74 62 70 59 72 76 66 84 98 41 27 49 43 60
pre 84 75 80 70 80 91 75 86 99 41 28 68 71 66
best setup across tasks
cnt 70 62 70 57 72 76 64 84 98 37 27 43 41 44
pre 83 73 78 68 80 86 71 77 98 41 26 67 69 64
worst setup across tasks
cnt 11 16 23 4 21 49 24 43 38 -6 -10 1 0 1
pre 74 60 73 48 68 71 65 82 88 33 20 27 40 10
best setup on rg
cnt (74) 59 66 52 71 64 64 84 98 37 20 35 42 26
pre (84) 71 76 64 79 85 72 84 98 39 25 66 70 61
other models
soa 86 81 77 62 76 100 79 91 96 60 32 61 64 61
dm 82 35 60 13 42 77 76 84 94 51 29 NA NA NA
cw 48 48 61 38 57 56 58 61 70 28 15 11 12 9
Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks. See Section 3 and
Table 1 for figures of merit and state-of-the-art results (soa). Since dm has very low coverage of the an*
data sets, we do not report its performance there.
243
experimenter might be tempted to choose without
tuning). The count model performance is severely
affected by this unlucky choice (2-word window,
Local Mutual Information, NMF, 400 dimensions,
mean performance rank: 83), whereas the predict
approach is much more robust: To put its worst in-
stantiation (2-word window, hierarchical softmax,
no subsampling, 200 dimensions, mean rank: 51)
into perspective, its performance is more than 10%
below the best count model only for the an and
ansem tasks, and actually higher than it in 3 cases
(note how on esslli the worst predict models per-
forms much better than the best one, confirming
our suspicion about the brittleness of this small
data set). The fourth block reports performance in
what might be the most realistic scenario, namely
by tuning the parameters on a development task.
Specifically, we pick the models that work best
on the small rg set, and report their performance
on all tasks (we obtained similar results by pick-
ing other tuning sets). The selected count model
is the third best overall model of its class as re-
ported in Table 3. The selected predict model is
the fourth best model in Table 4. The overall count
performance is not greatly affected by this choice.
Again, predict models confirm their robustness,
in that their rg-tuned performance is always close
(and in 3 cases better) than the one achieved by the
best overall setup.
Tables 3 and 4 let us take a closer look at
the most important count and predict parame-
ters, by reporting the characteristics of the best
models (in terms of average performance-based
ranking across tasks) from both classes. For the
count models, PMI is clearly the better weight-
ing scheme, and SVD outperforms NMF as a di-
mensionality reduction technique. However, no
compression at all (using all 300K original dimen-
sions) works best. Compare this to the best over-
all predict vectors, that have 400 dimensions only,
making them much more practical to use. For the
predict models, we observe in Table 4 that nega-
tive sampling, where the task is to distinguish the
target output word from samples drawn from the
noise distribution, outperforms the more costly hi-
erarchical softmax method. Subsampling frequent
words, which downsizes the importance of these
words similarly to PMI weighting in count mod-
els, is also bringing significant improvements.
Finally, we go back to Table 2 to point out the
poor performance of the out-of-the-box cw model.
window weight compress dim. mean
rank
2 PMI no 300K 35
5 PMI no 300K 38
2 PMI SVD 500 42
2 PMI SVD 400 46
5 PMI SVD 500 47
2 PMI SVD 300 50
5 PMI SVD 400 51
2 PMI NMF 300 52
2 PMI NMF 400 53
5 PMI SVD 300 53
Table 3: Top count models in terms of mean
performance-based model ranking across all tasks.
The first row states that the window-2, PMI, 300K
count model was the best count model, and, across
all tasks, its average rank, when ALL models are
decreasingly ordered by performance, was 35. See
Section 2.1 for explanation of the parameters.
We must leave the investigation of the parameters
that make our predict vectors so much better than
cw (more varied training corpus? window size?
objective function being used? subsampling? . . . )
to further work. Still, our results show that it?s
not just training by context prediction that ensures
good performance. The cw approach is very popu-
lar (for example both Huang et al (2012) and Bla-
coe and Lapata (2012) used it in the studies we dis-
cussed in Section 1). Had we also based our sys-
tematic comparison of count and predict vectors
on the cw model, we would have reached opposite
conclusions from the ones we can draw from our
word2vec-trained vectors!
5 Conclusion
This paper has presented the first systematic com-
parative evaluation of count and predict vectors.
As seasoned distributional semanticists with thor-
ough experience in developing and using count
vectors, we set out to conduct this study because
we were annoyed by the triumphalist overtones of-
ten surrounding predict models, despite the almost
complete lack of a proper comparison to count
vectors.
12
Our secret wish was to discover that it is
all hype, and count vectors are far superior to their
predictive counterparts. A more realistic expec-
12
Here is an example, where word2vec is called the crown
jewel of natural language processing: http://bit.ly/
1ipv72M
244
win. hier. neg. subsamp. dim mean
softm. samp. rank
5 no 10 yes 400 10
2 no 10 yes 300 13
5 no 5 yes 400 13
5 no 5 yes 300 13
5 no 10 yes 300 13
2 no 10 yes 400 13
2 no 5 yes 400 15
5 no 10 yes 200 15
2 no 10 yes 500 15
2 no 5 yes 300 16
Table 4: Top predict models in terms of mean
performance-based model ranking across all tasks.
See Section 2.2 for explanation of the parameters.
tation was that a complex picture would emerge,
with predict and count vectors beating each other
on different tasks. Instead, we found that the pre-
dict models are so good that, while the triumphal-
ist overtones still sound excessive, there are very
good reasons to switch to the new architecture.
However, due to space limitations we have only
focused here on quantitative measures: It remains
to be seen whether the two types of models are
complementary in the errors they make, in which
case combined models could be an interesting av-
enue for further work.
The space of possible parameters of count
DSMs is very large, and it?s entirely possible that
some options we did not consider would have im-
proved count vector performance somewhat. Still,
given that the predict vectors also outperformed
the syntax-based dm model, and often approxi-
mated state-of-the-art performance, a more profic-
uous way forward might be to focus on parameters
and extensions of the predict models instead: Af-
ter all, we obtained our already excellent results
by just trying a few variations of the word2vec de-
faults. Add to this that, beyond the standard lex-
ical semantics challenges we tested here, predict
models are currently been successfully applied in
cutting-edge domains such as representing phrases
(Mikolov et al, 2013c; Socher et al, 2012) or fus-
ing language and vision in a common semantic
space (Frome et al, 2013; Socher et al, 2013).
Based on the results reported here and the con-
siderations we just made, we would certainly rec-
ommend anybody interested in using DSMs for
theoretical or practical applications to go for the
predict models, with the important caveat that they
are not all created equal (cf. the big difference be-
tween word2vec and cw models). At the same
time, given the large amount of work that has been
carried out on count DSMs, we would like to ex-
plore, in the near future, how certain questions
and methods that have been considered with re-
spect to traditional DSMs will transfer to predict
models. For example, the developers of Latent
Semantic Analysis (Landauer and Dumais, 1997),
Topic Models (Griffiths et al, 2007) and related
DSMs have shown that the dimensions of these
models can be interpreted as general ?latent? se-
mantic domains, which gives the corresponding
models some a priori cognitive plausibility while
paving the way for interesting applications. An-
other important line of DSM research concerns
?context engineering?: There has been for exam-
ple much work on how to encode syntactic in-
formation into context features (Pad?o and Lapata,
2007), and more recent studies construct and com-
bine feature spaces expressing topical vs. func-
tional information (Turney, 2012). To give just
one last example, distributional semanticists have
looked at whether certain properties of vectors re-
flect semantic relations in the expected way: e.g.,
whether the vectors of hypernyms ?distribution-
ally include? the vectors of hyponyms in some
mathematical precise sense.
Do the dimensions of predict models also en-
code latent semantic domains? Do these models
afford the same flexibility of count vectors in cap-
turing linguistically rich contexts? Does the struc-
ture of predict vectors mimic meaningful seman-
tic relations? Does all of this even matter, or are
we on the cusp of discovering radically new ways
to tackle the same problems that have been ap-
proached as we just sketched in traditional distri-
butional semantics?
Either way, the results of the present investiga-
tion indicate that these are important directions for
future research in computational semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009.
245
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19?27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. Bridging the Gap between Semantic
Theory and Computational Simulations: Proceed-
ings of the ESSLLI Workshop on Distributional Lex-
ical Semantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elia Bruni, Nam Khanh Tran, and Marco Ba-
roni. 2013. Multimodal distributional seman-
tics. Journal of Artificial Intelligence Research.
In press; http://clic.cimec.unitn.it/
marco/publications/mmds-jair.pdf.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
John Bullinaria and Joseph Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: Stop-lists, stemming and SVD. Behavior
Research Methods, 44:890?907.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, and
Steven Skiena. 2013. The expressive power of
word embeddings. In Proceedings of the ICML
Workshop on Deep Learning for Audio, Speech and
Language Processing, Atlanta, GA. Published on-
line: https://sites.google.com/site/
deeplearningicml2013/accepted_
papers.
Stephen Clark. 2013. Vector space mod-
els of lexical meaning. In Shalom Lappin
and Chris Fox, editors, Handbook of Contem-
porary Semantics, 2nd ed. Blackwell, Malden,
MA. In press; http://www.cl.cam.ac.uk/
?
sc609/pubs/sem_handbook.pdf.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of
word relatedness with constraints. In Proceedings
of KDD, pages 1406?1414.
Samer Hassan and Rada Mihalcea. 2011. Semantic
relatedness using salient semantic analysis. In Pro-
ceedings of AAAI, pages 884?889, San Francisco,
CA.
Amac? Herda?gdelen and Marco Baroni. 2009. Bag-
Pack: A general framework to represent semantic
relations. In Proceedings of GEMS, pages 33?40,
Athens, Greece.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of ACL, pages 873?882, Jeju
Island, Korea.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota
Department of Computer Science.
246
Sophia Katrenko and Pieter Adriaans. 2008. Qualia
structures and their impact on the concrete noun
categorization task. In Proceedings of the ESS-
LLI Workshop on Distributional Lexical Semantics,
pages 17?24, Hamburg, Germany.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of the-
matic fit (and other constraints) in on-line sentence
comprehension. Journal of Memory and Language,
38:283?312.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013d. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
George Miller and Walter Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ulrike Pad?o. 2007. The Integration of Syntax and
Semantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Dissertation, Saarland Univer-
sity, Saarbr?ucken.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24, Athens, Greece.
Herbert Rubenstein and John Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL, pages 384?394, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
247
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 624?633,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How to make words with vectors:
Phrase generation in distributional semantics
Georgiana Dinu and Marco Baroni
Center for Mind/Brain Sciences
University of Trento, Italy
(georgiana.dinu|marco.baroni)@unitn.it
Abstract
We introduce the problem of generation
in distributional semantics: Given a distri-
butional vector representing some mean-
ing, how can we generate the phrase that
best expresses that meaning? We mo-
tivate this novel challenge on theoretical
and practical grounds and propose a sim-
ple data-driven approach to the estimation
of generation functions. We test this in
a monolingual scenario (paraphrase gen-
eration) as well as in a cross-lingual set-
ting (translation by synthesizing adjective-
noun phrase vectors in English and gener-
ating the equivalent expressions in Italian).
1 Introduction
Distributional methods for semantics approximate
the meaning of linguistic expressions with vectors
that summarize the contexts in which they occur
in large samples of text. This has been a very suc-
cessful approach to lexical semantics (Erk, 2012),
where semantic relatedness is assessed by compar-
ing vectors. Recently these methods have been
extended to phrases and sentences by means of
composition operations (see Baroni (2013) for an
overview). For example, given the vectors repre-
senting red and car, composition derives a vector
that approximates the meaning of red car.
However, the link between language and mean-
ing is, obviously, bidirectional: As message recip-
ients we are exposed to a linguistic expression and
we must compute its meaning (the synthesis prob-
lem). As message producers we start from the
meaning we want to communicate (a ?thought?)
and we must encode it into a word sequence (the
generation problem). If distributional semantics
is to be considered a proper semantic theory, then
it must deal not only with synthesis (going from
words to vectors), but also with generation (from
vectors to words).
Besides these theoretical considerations, phrase
generation from vectors has many useful applica-
tions. We can, for example, synthesize the vector
representing the meaning of a phrase or sentence,
and then generate alternative phrases or sentences
from this vector to accomplish true paraphrase
generation (as opposed to paraphrase detection or
ranking of candidate paraphrases).
Generation can be even more useful when the
source vector comes from another modality or lan-
guage. Recent work on grounding language in vi-
sion shows that it is possible to represent images
and linguistic expressions in a common vector-
based semantic space (Frome et al, 2013; Socher
et al, 2013). Given a vector representing an im-
age, generation can be used to productively con-
struct phrases or sentences that describe the im-
age (as opposed to simply retrieving an existing
description from a set of candidates). Translation
is another potential application of the generation
framework: Given a semantic space shared be-
tween two or more languages, one can compose a
word sequence in one language and generate trans-
lations in another, with the shared semantic vector
space functioning as interlingua.
Distributional semantics assumes a lexicon of
atomic expressions (that, for simplicity, we take
to be words), each associated to a vector. Thus,
at the single-word level, the problem of genera-
tion is solved by a trivial generation-by-synthesis
approach: Given an arbitrary target vector, ?gener-
ate? the corresponding word by searching through
the lexicon for the word with the closest vector to
the target. This is however unfeasible for larger
expressions: Given n vocabulary elements, this
approach requires checking n
k
phrases of length
k. This becomes prohibitive already for relatively
short phrases, as reasonably-sized vocabularies do
not go below tens of thousands of words. The
search space for 3-word phrases in a 10K-word
vocabulary is already in the order of trillions. In
624
this paper, we introduce a more direct approach to
phrase generation, inspired by the work in com-
positional distributional semantics. In short, we
revert the composition process and we propose
a framework of data-induced, syntax-dependent
functions that decompose a single vector into a
vector sequence. The generated vectors can then
be efficiently matched against those in the lexicon
or fed to the decomposition system again to pro-
duce longer phrases recursively.
2 Related work
To the best of our knowledge, we are the first to
explicitly and systematically pursue the generation
problem in distributional semantics. Kalchbrenner
and Blunsom (2013) use top-level, composed dis-
tributed representations of sentences to guide gen-
eration in a machine translation setting. More pre-
cisely, they condition the target language model
on the composed representation (addition of word
vectors) of the source language sentence.
Andreas and Ghahramani (2013) discuss the
the issue of generating language from vectors and
present a probabilistic generative model for distri-
butional vectors. However, their emphasis is on
reversing the generative story in order to derive
composed meaning representations from word se-
quences. The theoretical generating capabilities of
the methods they propose are briefly exemplified,
but not fully explored or tested.
Socher et al (2011) come closest to our target
problem. They introduce a bidirectional language-
to-meaning model for compositional distributional
semantics that is similar in spirit to ours. How-
ever, we present a clearer decoupling of synthesis
and generation and we use different (and simpler)
training methods and objective functions. More-
over, Socher and colleagues do not train separate
decomposition rules for different syntactic config-
urations, so it is not clear how they would be able
to control the generation of different output struc-
tures. Finally, the potential for generation is only
addressed in passing, by presenting a few cases
where the generated sequence has the same syn-
tactic structure of the input sequence.
3 General framework
We start by presenting the familiar synthesis set-
ting, focusing on two-word phrases. We then in-
troduce generation for the same structures. Fi-
nally, we show how synthesis and generation of
longer phrases is handled by recursive extension
of the two-word case. We assume a lexicon L,
that is, a bi-directional look-up table containing a
list of words L
w
linked to a matrix L
v
of vectors.
Both synthesis and generation involve a trivial lex-
icon look-up step to retrieve vectors associated to
words and vice versa: We ignore it in the exposi-
tion below.
3.1 Synthesis
To construct the vector representing a two-word
phrase, we must compose the vectors associated
to the input words. More formally, similarly to
Mitchell and Lapata (2008), we define a syntax-
dependent composition function yielding a phrase
vector ~p:
~p = f
comp
R
(~u,~v)
where ~u and ~v are the vector representations asso-
ciated to words u and v. f
comp
R
: R
d
? R
d
? R
d
(for d the dimensionality of vectors) is a compo-
sition function specific to the syntactic relation R
holding between the two words.
1
Although we are not bound to a specific com-
position model, throughout this paper we use the
method proposed by Guevara (2010) and Zanzotto
et al (2010) which defines composition as appli-
cation of linear transformations to the two con-
stituents followed by summing the resulting vec-
tors: f
comp
R
(~u,~v) = W
1
~u+W
2
~v. We will further
use the following equivalent formulation:
f
comp
R
(~u,~v) = W
R
[~u;~v]
where W
R
? R
d?2d
and [~u;~v] is the vertical con-
catenation of the two vectors (using Matlab no-
tation). Following Guevara, we learn W
R
using
examples of word and phrase vectors directly ex-
tracted from the corpus (for the rest of the pa-
per, we refer to these phrase vectors extracted
non-compositionally from the corpus as observed
vectors). To estimate, for example, the weights
in the W
AN
(adjective-noun) matrix, we use the
corpus-extracted vectors of the words in tuples
such as ?red, car, red.car?, ?evil, cat, evil.cat?,
etc. Given a set of training examples stacked into
matrices U , V (the constituent vectors) and P (the
corresponding observed vectors), we estimate W
R
by solving the least-squares regression problem:
1
Here we make the simplifying assumption that all vec-
tors have the same dimensionality, however this need not nec-
essarily be the case.
625
min
W
R
?R
d?2d
?P ?W
R
[U ;V ]? (1)
We use the approximation of observed phrase
vectors as objective because these vectors can pro-
vide direct evidence of the polysemous behaviour
of words: For example, the corpus-observed vec-
tors of green jacket and green politician reflect
how the meaning of green is affected by its occur-
rence with different nouns. Moreover, it has been
shown that for two-word phrases, despite their
relatively low frequency, such corpus-observed
representations are still difficult to outperform in
phrase similarity tasks (Dinu et al, 2013; Turney,
2012).
3.2 Generation
Generation of a two-word sequence from a vec-
tor proceeds in two steps: decomposition of the
phrase vectors into two constituent vectors, and
search for the nearest neighbours of each con-
stituent vector in L
v
(the lexical matrix) in order
to retrieve the corresponding words from L
w
.
Decomposition We define a syntax-dependent
decomposition function:
[~u;~v] = f
decomp
R
(~p)
where ~p is a phrase vector, ~u and ~v are vectors as-
sociated to words standing in the syntactic relation
R and f
decomp
R
: R
d
? R
d
? R
d
.
We assume that decomposition is also a linear
transformation, W
?
R
? R
2d?d
, which, given an in-
put phrase vector, returns two constituent vectors:
f
decomp
R
(~p) = W
?
R
~p
Again, we can learn from corpus-observed vectors
associated to tuples of word pairs and the corre-
sponding phrases by solving:
min
W
?
R
?R
2d?d
?[U ;V ]?W
?
R
P? (2)
If a composition function f
comp
R
is available, an
alternative is to learn a function that can best revert
this composition. The decomposition function is
then trained as follows:
min
W
?
R
?R
2d?d
?[U ;V ]?W
?
R
W
R
[U ;V ]? (3)
where the matrix W
R
is a given composition
function for the same relation R. Training with
observed phrases, as in eq. (2), should be better
at capturing the idiosyncrasies of the actual dis-
tribution of phrases in the corpus and it is more
robust by being independent from the availability
and quality of composition functions. On the other
hand, if the goal is to revert as faithfully as possi-
ble the composition process and retrieve the orig-
inal constituents (e.g., in a different modality or a
different language), then the objective in eq. (3) is
more motivated.
Nearest neighbour search We retrieve the near-
est neighbours of each constituent vector ~u ob-
tained by decomposition by applying a search
function s:
NN
~u
= s(~u, L
v
, t)
where NN
~u
is a list containing the t nearest
neighours of ~u from L
v
, the lexical vectors. De-
pending on the task, t might be set to 1 to retrieve
just one word sequence, or to larger values to re-
trieve t alternatives. The similarity measure used
to determine the nearest neighbours is another pa-
rameter of the search function; we omit it here as
we only experiment with the standard cosine mea-
sure (Turney and Pantel, 2010).
2
3.3 Recursive (de)composition
Extension to longer sequences is straightforward
if we assume binary tree representations as syn-
tactic structures. In synthesis, the top-level
vector can be obtained by applying composi-
tion functions recursively. For example, the
vector of big red car would be obtained as:
f
comp
AN
(
~
big, f
comp
AN
(
~
red, ~car)), where f
comp
AN
is the composition function for adjective-noun
phrase combinations. Conversely, for generation,
we decompose the phrase vector with f
decomp
AN
.
The first vector is used for retrieving the nearest
adjective from the lexicon, while the second vec-
tor is further decomposed.
In the experiments in this paper we assume that
the syntactic structure is given. In Section 7, we
discuss ways to eliminate this assumption.
2
Note that in terms of computational efficiency, cosine-
based nearest neighbour searches reduce to vector-matrix
multiplications, for which many efficient implementations
exist. Methods such as locality sensitive hashing can be used
for further speedups when working with particularly large vo-
cabularies (Andoni and Indyk, 2008).
626
4 Evaluation setting
In our empirical part, we focus on noun phrase
generation. A noun phrase can be a single noun or
a noun with one or more modifiers, where a mod-
ifier can be an adjective or a prepositional phrase.
A prepositional phrase is in turn composed of a
preposition and a noun phrase. We learn two com-
position (and corresponding decomposition) func-
tions: one for modifier-noun phrases, trained on
adjective-noun (AN) pairs, and a second one for
prepositional phrases, trained on preposition-noun
(PN) combinations. For the rest of this section we
describe the construction of the vector spaces and
the (de)composition function learning procedure.
Construction of vector spaces We test two
types of vector representations. The cbow model
introduced in Mikolov et al (2013a) learns vec-
tor representations using a neural network archi-
tecture by trying to predict a target word given the
words surrounding it. We use the word2vec soft-
ware
3
to build vectors of size 300 and using a con-
text window of 5 words to either side of the target.
We set the sub-sampling option to 1e-05 and esti-
mate the probability of a target word with the neg-
ative sampling method, drawing 10 samples from
the noise distribution (see Mikolov et al (2013a)
for details). We also implement a standard count-
based bag-of-words distributional space (Turney
and Pantel, 2010) which counts occurrences of a
target word with other words within a symmetric
window of size 5. We build a 300Kx300K sym-
metric co-occurrence matrix using the top most
frequent words in our source corpus, apply posi-
tive PMI weighting and Singular Value Decompo-
sition to reduce the space to 300 dimensions. For
both spaces, the vectors are finally normalized to
unit length.
4
For both types of vectors we use 2.8 billion to-
kens as input (ukWaC + Wikipedia + BNC). The
Italian language vectors for the cross-lingual ex-
periments of Section 6 were trained on 1.6 bil-
lion tokens from itWaC.
5
A word token is a word-
form + POS-tag string. We extract both word vec-
tors and the observed phrase vectors which are
3
Available at https://code.google.com/p/
word2vec/
4
The parameters of both models have been chosen without
specific tuning, based on their observed stable performance in
previous independent experiments.
5
Corpus sources: http://wacky.sslmit.unibo.
it, http://www.natcorp.ox.ac.uk
required for the training procedures. We sanity-
check the two spaces on MEN (Bruni et al, 2012),
a 3,000 items word similarity data set. cbow sig-
nificantly outperforms count (0.80 vs. 0.72 Spear-
man correlations with human judgments). count
performance is consistent with previously reported
results.
6
(De)composition function training The train-
ing data sets consist of the 50K most frequent
?u, v, p? tuples for each phrase type, for example,
?red, car, red.car? or ?in, car, in.car?.
7
We con-
catenate ~u and ~v vectors to obtain the [U ;V ] ma-
trix and we use the observed ~p vectors (e.g., the
corpus vector of the red.car bigram) to obtain the
phrase matrix P . We use these data sets to solve
the least squares regression problems in eqs. (1)
and (2), obtaining estimates of the composition
and decomposition matrices, respectively. For the
decomposition function in eq. (3), we replace the
observed phrase vectors with those composed with
f
comp
R
(~u,~v), where f
comp
R
is the previously esti-
mated composition function for relation R.
Composition function performance Since the
experiments below also use composed vectors as
input to the generation process, it is important to
provide independent evidence that the composi-
tion model is of high quality. This is indeed the
case: We tested our composition approach on the
task of retrieving observed AN and PN vectors,
based on their composed vectors (similarly to Ba-
roni and Zamparelli (2010), we want to retrieve the
observed red.car vector using f
comp
AN
(red, car)).
We obtain excellent results, with minimum accu-
racy of 0.23 (chance level <0.0001). We also test
on the AN-N paraphrasing test set used in Dinu
et al (2013) (in turn adapting Turney (2012)).
The dataset contains 620 ANs, each paired with
a single-noun paraphrase (e.g., false belief/fallacy,
personal appeal/charisma). The task is to rank
all nouns in the lexicon by their similarity to the
phrase, and return the rank of the correct para-
phrase. Results are reported in the first row of Ta-
ble 1. To facilitate comparison, we search, like
Dinu et al, through a vocabulary containing the
20K most frequent nouns. The count vectors re-
sults are similar to those reported by Dinu and col-
leagues for the same model, and with cbow vec-
6
See Baroni et al (2014) for an extensive comparison of
the two types of vector representations.
7
For PNs, we ignore determiners and we collapse, for ex-
ample, in.the.car and in.car occurrences.
627
Input Output cbow count
A?N N 11 171
N A, N 67,29 204,168
Table 1: Median rank on the AN-N set of Dinu et
al. (2013) (e.g., personal appeal/charisma). First
row: the A and N are composed and the closest
N is returned as a paraphrase. Second row: the
N vector is decomposed into A and N vectors and
their nearest (POS-tag consistent) neighbours are
returned.
tors we obtain a median rank that is considerably
higher than that of the methods they test.
5 Noun phrase generation
5.1 One-step decomposition
We start with testing one-step decomposition by
generating two-word phrases. A first straightfor-
ward evaluation consists in decomposing a phrase
vector into the correct constituent words. For this
purpose, we randomly select (and consequently re-
move) from the training sets 200 phrases of each
type (AN and PN) and apply decomposition op-
erations to 1) their corpus-observed vectors and
2) their composed representations. We generate
two words by returning the nearest neighbours
(with appropriate POS tags) of the two vectors
produced by the decomposition functions. Ta-
ble 2 reports generation accuracy, i.e., the pro-
portion of times in which we retrieved the cor-
rect constituents. The search space consists of
the top most frequent 20K nouns, 20K adjec-
tives and 25 prepositions respectively, leading to
chance accuracy <0.0001 for nouns and adjectives
and <0.05 for prepositions. We obtain relatively
high accuracy, with cbow vectors consistently out-
performing count ones. Decomposing composed
rather than observed phrase representations is eas-
ier, which is to be expected given that composed
representations are obtained with a simpler, lin-
ear model. Most of the errors consist in generat-
ing synonyms (hard case?difficult case, true cost
? actual cost) or related phrases (stereo speak-
ers?omni-directional sound).
Next, we use the AN-N dataset of Dinu and
colleagues for a more interesting evaluation of
one-step decomposition. In particular, we reverse
the original paraphrasing direction by attempting
to generate, for example, personal charm from
charisma. It is worth stressing the nature of the
Input Output cbow count
A.N A, N 0.36,0.61 0.20,0.41
P.N P, N 0.93,0.79 0.60,0.57
A?N A, N 1.00,1.00 0.86,0.99
P?N P, N 1.00,1.00 1.00,1.00
Table 2: Accuracy of generation models at re-
trieving (at rank 1) the constituent words of
adjective-noun (AN) and preposition-noun (PN)
phrases. Observed (A.N) and composed repre-
sentations (A?N) are decomposed with observed-
(eq. 2) and composed-trained (eq. 3) functions re-
spectively.
paraphrase-by-generation task we tackle here and
in the next experiments. Compositional distri-
butional semantic systems are often evaluated on
phrase and sentence paraphrasing data sets (Bla-
coe and Lapata, 2012; Mitchell and Lapata, 2010;
Socher et al, 2011; Turney, 2012). However,
these experiments assume a pre-compiled list of
candidate paraphrases, and the task is to rank
correct paraphrases above foils (paraphrase rank-
ing) or to decide, for a given pair, if the two
phrases/sentences are mutual paraphrases (para-
phrase detection). Here, instead, we do not as-
sume a given set of candidates: For example, in
N?AN paraphrasing, any of 20K
2
possible com-
binations of adjectives and nouns from the lexicon
could be generated. This is a much more challeng-
ing task and it paves the way to more realistic ap-
plications of distributional semantics in generation
scenarios.
The median ranks of the gold A and N of the
Dinu set are shown in the second row of Table
1. As the top-generated noun is almost always,
uninterestingly, the input one, we return the next
noun. Here we report results for the more moti-
vated corpus-observed training of eq. (2) (unsur-
prisingly, using composed-phrase training for the
task of decomposing single nouns leads to lower
performance).
Although considerably more difficult than the
previous task, the results are still very good, with
median ranks under 100 for the cbow vectors (ran-
dom median rank at 10K). Also, the dataset pro-
vides only one AN paraphrase for each noun, out
of many acceptable ones. Examples of generated
phrases are given in Table 3. In addition to gen-
erating topically related ANs, we also see nouns
disambiguated in different ways than intended in
628
Input Output Gold
reasoning deductive thinking abstract thought
jurisdiction legal authority legal power
thunderstorm thundery storm electrical storm
folk local music common people
superstition old-fashioned religion superstitious notion
vitriol political bitterness sulfuric acid
zoom fantastic camera rapid growth
religion religious religion religious belief
Table 3: Examples of generating ANs from Ns us-
ing the data set of Dinu et al (2013).
the gold standard (for example vitriol and folk in
Table 3). Other interesting errors consist of de-
composing a noun into two words which both have
the same meaning as the noun, generating for ex-
ample religion? religious religions. We observe
moreover that sometimes the decomposition re-
flects selectional preference effects, by generat-
ing adjectives that denote typical properties of the
noun to be paraphrased (e.g., animosity is a (po-
litical, personal,...) hostility or a fridge is a (big,
large, small,...) refrigerator). This effect could be
exploited for tasks such as property-based concept
description (Kelly et al, 2012).
5.2 Recursive decomposition
We continue by testing generation through recur-
sive decomposition on the task of generating noun-
preposition-noun (NPN) paraphrases of adjective-
nouns (AN) phrases. We introduce a dataset con-
taining 192 AN-NPN pairs (such as pre-election
promises? promises before election), which was
created by the second author and additionally cor-
rected by an English native speaker. The data set
was created by analyzing a list of randomly se-
lected frequent ANs. 49 further ANs (with adjec-
tives such as amazing and great) were judged not
NPN-paraphrasable and were used for the experi-
ment reported in Section 7. The paraphrased sub-
set focuses on preposition diversity and on includ-
ing prepositions which are rich in semantic content
and relevant to paraphrasing the AN. This has led
to excluding of, which in most cases has the purely
syntactic function of connecting the two nouns.
The data set contains the following 14 preposi-
tions: after, against, at, before, between, by, for,
from, in, on, per, under, with, without.
8
NPN phrase generation involves the applica-
tion of two decomposition functions. In the first
8
This dataset is available at http://clic.cimec.
unitn.it/composes
step we decompose using the modifier-noun rule
(f
decomp
AN
). We generate a noun from the head
slot vector and the ?adjective? vector is further de-
composed using f
decomp
PN
(returning the top noun
which is not identical to the previously generated
one). The results, in terms of top 1 accuracy and
median rank, are shown in Table 4. Examples are
given in Table 5.
For observed phrase vector training, accuracy
and rank are well above chance for all constituents
(random accuracy 0.00005 for nouns and 0.04 for
prepositions, corresponding median ranks: 10K,
12). Preposition generation is clearly a more diffi-
cult task. This is due at least in part to their highly
ambiguous and broad semantics, and the way in
which they interact with the nouns. For exam-
ple, cable through ocean in Table 5 is a reason-
able paraphrase of undersea cable despite the gold
preposition being under. Other than several cases
which are acceptable paraphrases but not in the
gold standard, phrases related in meaning but not
synonymous are the most common error (overcast
skies ? skies in sunshine). We also observe that
often the A and N meanings are not fully separated
when decomposing and ?traces? of the adjective
or of the original noun meaning can be found in
both generated nouns (for example nearby school
? schools after school). To a lesser degree, this
might be desirable as a disambiguation-in-context
effect as, for example, in underground cavern, in
secret would not be a context-appropriate para-
phrase of underground.
6 Noun phrase translation
This section describes preliminary experiments
performed in a cross-lingual setting on the task
of composing English AN phrases and generating
Italian translations.
Creation of cross-lingual vector spaces A
common semantic space is required in order to
map words and phrases across languages. This
problem has been extensively addressed in the
bilingual lexicon acquisition literature (Haghighi
et al, 2008; Koehn and Knight, 2002). We opt for
a very simple yet accurate method (Klementiev et
al., 2012; Rapp, 1999) in which a bilingual dictio-
nary is used to identify a set of shared dimensions
across spaces and the vectors of both languages are
projected into the subspace defined by these (Sub-
space Projection - SP). This method is applicable
to count-type vector spaces, for which the dimen-
629
Input Output Training cbow count
A?N N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5)
A?N N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5)
Table 4: Top 1 accuracy (median rank) on the AN?NPN paraphrasing data set. AN phrases are com-
posed and then recursively decomposed into N, (P, N). Comma-delimited scores reported for first noun,
preposition, second noun in this order. Training is performed on observed (eq. 2) and composed (eq. 3)
phrase representations.
Input Output Gold
mountainous region region in highlands region with mountains
undersea cable cable through ocean cable under sea
underground cavern cavern through rock cavern under ground
interdisciplinary field field into research field between disciplines
inter-war years years during 1930s years between wars
post-operative pain pain through patient pain after operation
pre-war days days after wartime days before war
intergroup differences differences between intergroup differences between minorities
superficial level level between levels level on surface
Table 5: Examples of generating NPN phrases from composed ANs.
sions correspond to actual words. As the cbow di-
mensions do not correspond to words, we align the
cbow spaces by using a small dictionary to learn
a linear map which transforms the English vectors
into Italian ones as done in Mikolov et al (2013b).
This method (Translation Matrix - TM) is applica-
ble to both cbow and count spaces. We tune the pa-
rameters (TM or SP for count and dictionary size
5K or 25K for both spaces) on a standard task of
translating English words into Italian. We obtain
TM-5K for cbow and SP-25K for count as opti-
mal settings. The two methods perform similarly
for low frequency words while cbow-TM-5K sig-
nificantly outperforms count-SP-25K for high fre-
quency words. Our results for the cbow-TM-5K
setting are similar to those reported by Mikolov et
al. (2013b).
Cross-lingual decomposition training Train-
ing proceeds as in the monolingual case, this time
concatenating the training data sets and estimating
a single (de)-composition function for the two lan-
guages in the shared semantic space. We train both
on observed phrase representations (eq. 2) and on
composed phrase representations (eq. 3).
Adjective-noun translation dataset We ran-
domly extract 1,000 AN-AN En-It phrase pairs
from a phrase table built from parallel movie sub-
titles, available at http://opus.lingfil.
uu.se/ (OpenSubtitles2012, en-it) (Tiedemann,
2012).
Input Output cbow count
A?N(En) A,N (It) 0.31,0.59 0.24,0.54
A?N (It) A,N(En) 0.50,0.62 0.28,0.48
Table 6: Accuracy of En?It and It?En phrase
translation: phrases are composed in source lan-
guage and decomposed in target language. Train-
ing on composed phrase representations (eq. (3))
(with observed phrase training (eq. 2) results are
?50% lower).
Results are presented in Table 6. While in
these preliminary experiments we lack a proper
term of comparison, the performance is very good
both quantitatively (random < 0.0001) and quali-
tatively. The En?It examples in Table 7 are repre-
sentative. In many cases (e.g., vicious killer, rough
neighborhood) we generate translations that are
arguably more natural than those in the gold stan-
dard. Again, some differences can be explained
by different disambiguations (chest as breast, as
in the generated translation, or box, as in the gold).
Translation into related but not equivalent phrases
and generating the same meaning in both con-
stituents (stellar star) are again the most signifi-
cant errors. We also see cases in which this has the
desired effect of disambiguating the constituents,
such as in the examples in Table 8, showing the
nearest neighbours when translating black tie and
indissoluble tie.
630
Input Output Gold
vicious killer assassino feroce (ferocious killer) killer pericoloso
spectacular woman donna affascinante (fascinating woman) donna eccezionale
huge chest petto grande (big chest) scrigno immenso
rough neighborhood zona malfamata (ill-repute zone) quartiere difficile
mortal sin peccato eterno (eternal sin) pecato mortale
canine star stella stellare (stellar star) star canina
Table 7: En?It translation examples (back-translations of generated phrases in parenthesis).
black tie
cravatta (tie) nero (black)
velluto (velvet) bianco (white)
giacca (jacket) giallo (yellow)
indissoluble tie
alleanza (alliance) indissolubile (indissoluble)
legame (bond) sacramentale (sacramental)
amicizia (friendship) inscindibile (inseparable)
Table 8: Top 3 translations of black tie and indis-
soluble tie, showing correct disambiguation of tie.
7 Generation confidence and generation
quality
In Section 3.2 we have defined a search function
s returning a list of lexical nearest neighbours for
a constituent vector produced by decomposition.
Together with the neighbours, this function can
naturally return their similarity score (in our case,
the cosine). We call the score associated to the
top neighbour the generation confidence: if this
score is low, the vector has no good match in the
lexicon. We observe significant Spearman cor-
relations between the generation confidence of a
constituent and its quality (e.g., accuracy, inverse
rank) in all the experiments. For example, for the
AN(En)?AN(It) experiment, the correlations be-
tween the confidence scores and the inverse ranks
for As and Ns, for both cbow and count vectors,
range between 0.34 (p < 1e
?28
) and 0.42. In
the translation experiments, we can use this to au-
tomatically determine a subset on which we can
translate with very high accuracy. Table 9 shows
AN-AN accuracies and coverage when translating
only if confidence is above a certain threshold.
Throughout this paper we have assumed that the
syntactic structure of the phrase to be generated is
given. In future work we will exploit the corre-
lation between confidence and quality for the pur-
pose of eliminating this assumption. As a concrete
example, we can use confidence scores to distin-
guish the two subsets of the AN-NPN dataset in-
troduced in Section 5: the ANs which are para-
phrasable with an NPN from those that do not
En?It It?En
Thr. Accuracy Cov. Accuracy Cov.
0.00 0.21 100% 0.32 100%
0.55 0.25 70% 0.40 63%
0.60 0.31 32% 0.45 37%
0.65 0.45 9% 0.52 16%
Table 9: AN-AN translation accuracy (both A and
N correct) when imposing a confidence threshold
(random: 1/20K
2
).
Figure 1: ROC of distinguishing ANs para-
phrasable as NPNs from non-paraphrasable ones.
have this property. We assign an AN to the NPN-
paraphrasable class if the mean confidence of the
PN expansion in its attempted N(PN) decomposi-
tion is above a certain threshold. We plot the ROC
curve in Figure 1. We obtain a significant AUC of
0.71.
8 Conclusion
In this paper we have outlined a framework for
the task of generation with distributional semantic
models. We proposed a simple but effective ap-
proach to reverting the composition process to ob-
tain meaningful reformulations of phrases through
a synthesis-generation process.
For future work we would like to experiment
with more complex models for (de-)composition
in order to improve the performance on the tasks
we used in this paper. Following this, we
631
would like to extend the framework to handle
arbitrary phrases, including making (confidence-
based) choices on the syntactic structure of the
phrase to be generated, which we have assumed
to be given throughout this paper.
In terms of applications, we believe that the line
of research in machine translation that is currently
focusing on replacing parallel resources with large
amounts of monolingual text provides an inter-
esting setup to test our methods. For example,
Klementiev et al (2012) reconstruct phrase ta-
bles based on phrase similarity scores in seman-
tic space. However, they resort to scoring phrase
pairs extracted from an aligned parallel corpus, as
they do not have a method to freely generate these.
Similarly, in the recent work on common vector
spaces for the representation of images and text,
the current emphasis is on retrieving existing cap-
tions (Socher et al, 2014) and not actual genera-
tion of image descriptions.
From a more theoretical point of view, our work
fills an important gap in distributional semantics,
making it a bidirectional theory of the connec-
tion between language and meaning. We can now
translate linguistic strings into vector ?thoughts?,
and the latter into their most appropriate linguis-
tic expression. Several neuroscientific studies sug-
gest that thoughts are represented in the brain by
patterns of activation over broad neural areas, and
vectors are a natural way to encode such patterns
(Haxby et al, 2001; Huth et al, 2012). Some
research has already established a connection be-
tween neural and distributional semantic vector
spaces (Mitchell et al, 2008; Murphy et al, 2012).
Generation might be the missing link to power-
ful computational models that take the neural foot-
print of a thought as input and produce its linguis-
tic expression.
Acknowledgments
We thank Kevin Knight, Andrew Anderson,
Roberto Zamparelli, Angeliki Lazaridou, Nghia
The Pham, Germ?an Kruszewski and Peter Tur-
ney for helpful discussions and the anonymous re-
viewers for their useful comments. We acknowl-
edge the ERC 2011 Starting Independent Research
Grant n. 283554 (COMPOSES).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neigh-
bor in high dimensions. Commun. ACM, 51(1):117?
122, January.
Jacob Andreas and Zoubin Ghahramani. 2013. A
generative model of vector space semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 91?
99, Sofia, Bulgaria.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of ACL, To appear, Baltimore, MD.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511?522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779, Columbus, OH, USA, June.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces
and objects in ventral temporal cortex. Science,
293:2425?2430.
632
Alexander Huth, Shinji Nishimoto, An Vu, and Jack
Gallant. 2012. A continuous semantic space de-
scribes the representation of thousands of object and
action categories across the human brain. Neuron,
76(6):1210?1224.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, Seattle, October. Associa-
tion for Computational Linguistics.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11?20, Montreal, Canada.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of EACL, pages 130?140, Avignon,
France.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16, Philadelphia, PA,
USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason,
and Marcel Just. 2008. Predicting human brain ac-
tivity associated with the meanings of nouns. Sci-
ence, 320:1191?1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Selecting corpus-semantic models for neu-
rolinguistic decoding. In Proceedings of *SEM,
pages 114?123, Montreal, Canada.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Richard Socher, Quoc Le, Christopher Manning, and
Andrew Ng. 2014. Grounded compositional se-
mantics for finding and describing images with sen-
tences. Transactions of the Association for Compu-
tational Linguistics. In press.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
633
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
New Directions in Vector Space Models of Meaning
Phil Blunsom, Edward Grefenstette
and Karl Moritz Hermann
?
University of Oxford
first.last@cs.ox.ac.uk
Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
1 Abstract
Symbolic approaches have dominated NLP as a
means to model syntactic and semantic aspects of
natural language. While powerful inferential tools
exist for such models, they suffer from an inabil-
ity to capture correlation between words and to
provide a continuous model for word, phrase, and
document similarity. Distributed representations
are one mechanism to overcome these constraints.
This tutorial will supply NLP researchers with
the mathematical and conceptual background to
make use of vector-based models of meaning in
their own research. We will begin by motivating
the need for a transition from symbolic represen-
tations to distributed ones. We will briefly cover
how collocational (distributional) vectors can be
used and manipulated to model word meaning. We
will discuss the progress from distributional to dis-
tributed representations, and how neural networks
allow us to learn word vectors and condition them
on metadata such as parallel texts, topic labels, or
sentiment labels. Finally, we will present various
forms of semantic vector composition, and discuss
their relative strengths and weaknesses, and their
application to problems such as language mod-
elling, paraphrasing, machine translation and doc-
ument classification.
This tutorial aims to bring researchers up to
speed with recent developments in this fast-
moving field. It aims to strike a balance be-
tween providing a general introduction to vector-
based models of meaning, an analysis of diverg-
ing strands of research in the field, and also being
a hands-on tutorial to equip NLP researchers with
the necessary tools and background knowledge to
start working on such models. Attendees should
be comfortable with basic probability, linear alge-
bra, and continuous mathematics. No substantial
knowledge of machine learning is required.
?
Instructors listed in alphabetical order.
2 Outline
1. Motivation: Meaning in space
2. Learning distributional models for words
3. Neural language modelling and distributed
representations
(a) Neural language model fundamentals
(b) Recurrent neural language models
(c) Conditional neural language models
4. Semantic composition in vector spaces
(a) Algebraic and tensor-based composition
(b) The role of non-linearities
(c) Learning recursive neural models
(d) Convolutional maps and composition
3 Instructors
Phil Blunsom is an Associate Professor at the
University of Oxford?s Department of Computer
Science. His research centres on the probabilistic
modelling of natural languages, with a particular
interest in automating the discovery of structure
and meaning in text.
Georgiana Dinu is a postdoctoral researcher
at the University of Trento. Her research re-
volves around distributional semantics with a fo-
cus on compositionality within the distributional
paradigm.
Edward Grefenstette is a postdoctoral researcher
at Oxford?s Department of Computer Science. He
works on the relation between vector represen-
tations of language meaning and structured logi-
cal reasoning. His work in this area was recently
recognised by a best paper award at *SEM 2013.
Karl Moritz Hermann is a final-year DPhil stu-
dent at the Department of Computer Science in
Oxford. His research studies distributed and com-
positional semantics, with a particular emphasis
on mechanisms to reduce task-specific and mono-
lingual syntactic bias in such representations.
8
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603?607,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Saarland: Vector-based models of semantic textual similarity
Georgiana Dinu
Center of Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
Stefan Thater
Dept. of Computational Linguistics
Universita?t des Saarlandes
stth@coli.uni-saarland.de
Abstract
This paper describes our system for the Se-
meval 2012 Sentence Textual Similarity task.
The system is based on a combination of few
simple vector space-based methods for word
meaning similarity. Evaluation results show
that a simple combination of these unsuper-
vised data-driven methods can be quite suc-
cessful. The simple vector space components
achieve high performance on short sentences;
on longer, more complex sentences, they are
outperformed by a surprisingly competitive
word overlap baseline, but they still bring im-
provements over this baseline when incorpo-
rated into a mixture model.
1 Introduction
Vector space models are widely-used methods for
word meaning similarity which exploit the so-called
distributional hypothesis, stating that semantically
similar words tend to occur in similar contexts. Word
meaning is represented by the contexts in which a
word occurs, and similarity is computed by compar-
ing these contexts in a high-dimensional vector space
(Turney and Pantel, 2010). Distributional models of
word meaning are attractive because they are sim-
ple, have wide coverage, and can be easily acquired
at virtually no cost in an unsupervised way. Fur-
thermore, recent research has shown that, at least
to some extent, these models can be generalized to
capture similarity beyond the (isolated) word level,
either as lexical meaning modulated by context, or
as vectorial meaning representations for phrases and
sentences. In this paper we evaluate the use of some
of these models for the Semantic Textual Similarity
(STS) task, which measures the degree of semantic
equivalence between two sentences.
In recent work Mitchell and Lapata (2008) has
drawn the attention to the question of building vecto-
rial meaning representations for sentences by combin-
ing individual word vectors. They propose a family of
simple ?compositional? models that compute a vector
for a phrase or a sentence by combining vectors of
the constituent words, using different operations such
as vector addition or component-wise multiplication.
More refined models have been proposed recently by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011).
Thater et al (2011) and others take a slightly dif-
ferent perspective on the problem: Instead of com-
puting a vector representation for a complete phrase
or sentence, they focus on the problem of ?disam-
biguating? the vector representation of a target word
based on distributional information about the words
in the target?s context. While this approach is not
?compositional? in the sense described above, it still
captures some meaning of the complete phrase in
which a target word occurs.
In this paper, we report on the system we used in
the Semeval 2012 Sentence Textual Similarity shared
task and describe an approach that uses a combina-
tion of few simple vector-based components. We
extend the model of Thater et al (2011), which has
been shown to perform well on a closely related para-
phrase ranking task, with an additive composition op-
eration along the lines of Mitchell and Lapata (2008),
and compare it with a simple alignment-based ap-
proach which in turn uses vector-based similarity
scores. Results show that in particular the alignment-
based approach can achieve good performance on
the Microsoft Research Video Description dataset.
On the other datasets, all vector-based components
are outperformed by a surprisingly competitive word
603
overlap baseline, but they still bring improvements
over this baseline when incorporated into a mixture
model. On the test dataset, the mixture model ranks
10th and 13th on the Microsoft Research Paraphrase
and Video Description datasets, respectively, which
we take this to be a quite promising result given that
we use only few relatively simple vector based com-
ponents to compute similarity scores for sentences.
The rest of the paper is structured as follows: Sec-
tion 2 presents the individual vector-based compo-
nents used by our system. In Section 3 we present
detailed evaluation results on the training set, as well
as results for our system on the test set, while Sec-
tion 4 concludes the paper.
2 Systems for Sentence Similarity
Our system is based on four different components:
We use two different vector space models to repre-
sent word meaning?a basic bag-of-words model
and a slightly simplified variant of the contextual-
ization model of Thater et al (2011)?and two dif-
ferent methods to compute similarity scores for sen-
tences based on these two vector space models?one
?compositional? method that computes vectors for
sentences by summing over the vectors of the con-
stituent words, and one alignment-based method that
uses vector-based similarity scores for word pairs to
compute an alignment between the words in the two
sentences.
2.1 Vector Space Models
For the basic vector-space model, we assume a set
W of words, and represent the meaning of a word
w ?W by a vector in the vector space V spanned by
the set of basis vectors {~ew? | w? ?W} as follows:
vbasic(w) = ?
w??W
f (w,w?)~ew?
where f is a function that assigns a co-occurrence
value to the word pair (w,w?). In the experiments
reported below, we use pointwise mutual information
estimated on co-occurrence frequencies for words
within a 5-word window around the target word on
either side.1
1We use a 5-word window here as this setting has been shown
to give best results on a closely related task in the literature
(Mitchell and Lapata, 2008)
This basic ?bag of words? vector space model rep-
resents word meaning by summing over all contexts
in which the target word occurs. Since words are of-
ten ambiguous, this means that context words pertain-
ing to different senses of the target word are mixed
within a single vector representation, which can lead
to ?noisy? similarity scores. The vector for the noun
coach, for instance, contains context words like teach
and tell (person sense) as well as derail and crash
(vehicle sense).
To address this problem, Thater et al (2011) pro-
pose a ?contextualization? model in which the indi-
vidual components of the target word?s vector are re-
weighted, based on distributional information about
the words in the target?s context. Let us assume that
the context consist of a single word c. The vector for
a target w in context c is then defined as:
v(w,c) = ?
w??W
?(c,w?) f (w,w?)~ew?
where ? is some similarity score that quantifies to
what extent the vector dimension that corresponds
to w? is compatible with the observed context c. In
the experiments reported below, we take ? to be the
cosine similarity of c and w?; see Section 3 for details.
In the experiments reported below, we use all
words in the syntactic context of the target word to
contextualize the target:
vctx(w) = ?
c?C(w)
v(w,c)
where C(w) is the context in which w occurs, i.e. all
words related to w by a dependency relation such as
subject or object, including inverse relations.
Remark. The contextualization model presented
above is a slightly simplified version of the original
model of Thater et al (2011): it uses standard bag-of-
words vectors instead of syntax-based vectors. This
simplified version performs better on the training
dataset. Furthermore, the simplified model has been
shown to be equivalent to the models of Erk and
Pado? (2008) and Thater et al (2010) by Dinu and
Thater (2012), so the results reported below carry
over directly to these other models as well.
2.2 Vector Composition and Alignment
The two vector space models sketched above repre-
sent the meaning of words, and thus cannot be applied
604
directly to model similarity of phrases or sentences.
One obvious and straightforward way to extend these
models to the sentence level is to follow Mitchell and
Lapata (2008) and represent sentences by vectors
obtained by summing over the individual vectors of
the constituent words. These ?compositional? mod-
els can then be used to compute similarity scores
between sentence pairs in a straightforward way, sim-
ply by computing the cosine of the angle between
vectors (or some other similarity score) for the two
sentences:
simadd(S,S
?) = cos
(
?
w?S
v(w), ?
w??S?
v(w?)
)
(1)
where v(w) can be instantiated either with basic or
with ctx vectors.
In addition to the compositional models, we also
experimented with an alignment-based approach: In-
stead of computing vectors for complete sentences,
we compute an alignment between the words in the
two sentences. To be more precise, we compute
cosine similarity scores between all possible pairs
of words (tokens) of the two sentences; based on
these similarity scores, we then compute a one-to-one
alignment between the words in the two sentences2,
using a greedy search strategy (see Fig. 1). We assign
a weight to each link in the alignment which is simply
the cosine similarity score of the corresponding word
pair and take the sum of the link weights, normalized
by the maximal length of the two sentences to be the
corresponding similarity score for the two sentences.
The final score is then:
simalign(S,S
?) =
?(w,w?)?ALIGN(S,S?) cos(v(w),v(w
?))
max(|S|, |S?|)
where v(w) is the vector for w, which again can be
either the basic or the contextualized vector.
3 Evaluation
In this section we present our experimental results.
In addition to the models described in Section 2, we
define a baseline model which simply computes the
word overlap between two sentences as:
simoverlap(S,S
?) =
|S?S?|
|S?S?|
(2)
2Note that this can result in some words not being aligned
function ALIGN(S1,S2)
alignment? /0
marked? /0
pairs?{?w,w?? | w ? S1,w? ? S2}
while pairs not empty do
?w,w?? ? highest cosine pair in pairs
if w /? marked and w? /? marked then
alignment? ?w,w?? ? alignment
marked?{w,w?} ?marked
end if
pairs? pairs \ {?w,w??}
end while
return alignment
end function
Figure 1: The alignment algorithm
The score assigned by this method is simply the num-
ber of words that the two sentences have in common
divided by their total number of words. Finally, we
also propose a straightforward mixture model which
combines all of the above methods. We use the train-
ing data to fit a degree two polynomial over these
individual predictors using least squares regression.
We report cross-validation scores.
3.1 Evaluation setup
The vector space used in all experiments is a bag-of-
words space containing word co-occurrence counts.
We use the GigaWord (1.7 billion tokens) as input
corpus and extract word co-occurrences within a
symmetric 5-word context window. Co-occurrence
counts smaller than three are set to 0 and we further
apply (positive) pmi weighting.
3.2 Training results
The training data results are shown in Figure 2. The
best performance on the video dataset is achieved
by the alignment method using a basic vector rep-
resentation to compute word-level similarity. All
vector-space methods perform considerably better
than the simple word overlap baseline on this dataset,
the alignment method achieving almost 20% gain
over this baseline. This indicates that information
about the meaning of the words is very beneficial for
this type of data, consisting of small, well-structured
sentences.
Using the alignment method with contextualized
605
Component MSRvid MSRpar SMTeur
basic/add 70.9 33.3 31.8
ctx/add 65.7 23.0 30.4
basic/align 74.6 40.5 32.1
overlap 56.8 59.5 50.0
mixture 78.1 61.8 54.1
Figure 2: Results on the training set.
vector representations (omitted in the table) does not
bring any improvement and it performs similarly to
the ctx/add method. This suggests that aligning sim-
ilar words in the two sentences does not benefit from
further meaning disambiguation through contextual-
ized vectors and that some level of disambiguation
may be implicitly performed.
On the paraphrase and europarl datasets, the over-
lap baseline outperforms, by a large margin, the vec-
tor space models. This is not surprising, as it is
known that word overlap baselines can be very com-
petitive on Recognizing Textual Entailment datasets,
to which these two datasets bare a large resemblance.
In particular this indicates that the methods proposed
for combining vector representations of words do
not provide, in the current state, accurate models for
modeling the meaning of larger sentences.
We also report 10-fold cross-validation scores ob-
tained with the mixture model. On all datasets, this
outperforms the individual methods, improving by
a margin of 2%-4% the best single methods. In par-
ticular, on the paraphrase and europarl datasets, this
shows that despite the considerably inferior perfor-
mance of the vector-based methods, these can still
help improve the overall performance.
This is also reflected in Table 3, where we evaluate
the performance of the mixture method when, in
turn, one of the individual components is excluded:
with few exceptions, all components contribute to the
performance of the mixtures.
3.3 Test results
We have submitted as our official runs the best sin-
gle vector space model, performing alignment with
basic vector similarity, as well as the mixture meth-
ods. The mixture method uses weights individually
learned for each of the datasets made available during
Component MSRvid MSRpar SMTeur
basic/add ?2.1 ?0.1 ?1.5
ctx/add ?0.6 +1.3 +0.4
basic/align ?4.1 ?1.9 ?2.6
overlap ?0.1 ?17.0 ?23.0
Figure 3: Results on the training set when removing indi-
vidual components from the mixture model.
training. For the two surprise datasets we carry over
the weights of what we have considered to be the
most similar training-available sets: video weights of
ontonotes and paraphrase weights for news.
The test data results are given in 4. We report
the results for the individual datasets as well as the
mean Pearson correlation, weighted by the sizes of
the datasets. The table also shows the performance
of the official task baseline as well as the top three
runs accoring to the overall weighted mean score.
As expected, the mixture method outperforms by
a large margin the alignment model, achieving rank
10 and rank 13 on the video and paraphrase datasets.
Overall the mixture method ranks 43 according to the
weighted mean measure (rank 22 if correcting our of-
ficial submission which contained the wrong output
file for the europarl dataset). The other more con-
troversial measures rank our official, not corrected,
submission at position 13 (RankNrm) and 71 (Rank),
overall. This is an encouraging result, as the individ-
ual components we have used are all unsupervised,
obtained solely from large amounts of unlabeled data,
and with no other additional resources. The training
data made available has only been used to learn a
set of weights for combining these individual compo-
nents.
4 Conclusions
This paper describes an approach that combines few
simple vector space-based components to model sen-
tence similarity. We have extended the state-of-the-
art model for contextualized meaning representations
of Thater et al (2011) with an additive composi-
tion operation along the lines of Mitchell and Lap-
ata (2008). We have combined this with a simple
alignment-based method and a word overlap baseline
into a mixture model.
Our system achieves promising results in particular
606
Dataset basic/align mixture baseline Run1 Run2 Run3
MSRvid 77.1 83.1 30.0 87.3 88.0 85.6
MSRpar 40.4 63.1 43.3 68.3 73.4 64.0
SMTeur 26.8 13.9 (37.1?) 45.4 52.8 47.7 51.5
OnWN 57.2 59.6 58.6 66.4 67.9 71.0
SMTnews 35.0 38.0 39.1 49.3 39.8 48.3
ALL 49.5 45.4 31.1 82.3 81.3 73.3
Rank 65 71 87 1 3 15
ALLNrm 78.7 82.5 67.3 85.7 86.3 85.2
RankNrm 50 13 85 2 1 5
Mean 50.6 56.6 (60.0?) 43.5 67.7 67.5 67.0
RankMean 60 43 (22?) 70 1 2 3
Figure 4: Results on the test set. ? ? corrected score (official results score wrong prediction file we have submitted for
the europarl dataset). Official baseline and top three runs according to the weighted mean measure.
on the Microsoft Research Paraphrase and Video
Description datasets, on which it ranks 13th and 10th,
respectively. We take this to be a promising result,
given that our focus has not been the development
of a highly-competitive complex system, but rather
on investigating what performance can be achieved
when using only vector space methods.
An interesting observation is that the methods for
combining word vector representations (the vector
addition, or the meaning contextualization) can be
beneficial for modeling the similarity of the small,
well-structured sentences of the video dataset, how-
ever they do not perform well on comparing longer,
more complex sentences. In future work we plan to
further investigate methods for composition in vector
space models using the STS datasets, in addition to
the small, controlled datasets that have been typically
used in this line of research.
Acknowledgments. This work was supported by
the Cluster of Excellence ?Multimodal Computing
and Interaction,? funded by the German Excellence
Initiative.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.
Georgiana Dinu and Stefan Thater. 2012. A comparison
of models of word meaning in context. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. Short paper, to appear.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, HI, USA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of the 48th Annual Meeting of the Association for Com-
putational Linguistics, Uppsala, Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
1134?1143, Chiang Mai, Thailand, November. Asian
Federation of Natural Language Processing.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
607
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 27?32,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Relatedness Curves for Acquiring Paraphrases
Georgiana Dinu
Saarland University
Saarbruecken, Germany
dinu@coli.uni-sb.de
Grzegorz Chrupa?a
Saarland University
Saarbruecken, Germany
gchrupala@lsv.uni-saarland.de
Abstract
In this paper we investigate methods
for computing similarity of two phrases
based on their relatedness scores across
all ranks k in a SVD approximation of
a phrase/term co-occurrence matrix. We
confirm the major observations made in
previous work and our preliminary experi-
ments indicate that these methods can lead
to reliable similarity scores which in turn
can be used for the task of paraphrasing.
1 Introduction
Distributional methods for word similarity use
large amounts of text to acquire similarity judg-
ments based solely on co-occurrence statistics.
Typically each word is assigned a representation
as a point in a high dimensional space, where the
dimensions represent contextual features; follow-
ing this, vector similarity measures are used to
judge the meaning relatedness of words. One way
to make these computations more reliable is to use
Singular Value Decomposition (SVD) in order to
obtain a lower rank approximation of an original
co-occurrence matrix.
SVD is a matrix factorization method which
has applications in a large number of fields such
as signal processing or statistics. In natural lan-
guage processing methods such as Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990)
use SVD to obtain a factorization of a (typically)
word/document co-occurrence matrix. The under-
lying idea in these models is that the dimension-
ality reduction will produce meaningful dimen-
sions which represent concepts rather than just
terms, rendering similarity measures on these vec-
tors more accurate. Over the years, it has been
shown that these methods can closely match hu-
man similarity judgments and that they can be
used in various applications such as information
retrieval, document classification, essay grading
etc. However it has been noted that the success
of these methods is drastically determined by the
choice of dimension k to which the original space
is reduced.
(Bast and Majumdar, 2005) investigates exactly
this aspect and proves that no fixed choice of di-
mension is appropriate. The authors show that two
terms can be reliably compared only by investigat-
ing the curve of their relatedness scores over all
dimensions k. The authors use a term/document
matrix and analyze relatedness curves for inducing
a hard related/not-related decision and show that
their algorithms significantly improve over previ-
ous methods for information retrieval.
In this paper we investigate: 1) how the findings
of (Bast and Majumdar, 2005) carry over to ac-
quiring paraphrases using SVD on a phrase/term
co-occurrence matrix and 2) if reliable similarity
scores can be obtained from the analysis of relat-
edness curves.
2 Background
2.1 Singular Value Decomposition
Models such as LSA use Singular Value Decom-
position, in order to obtain term representations
over a space of concepts.
Given a co-occurrence matrix X of size (t, d),
we can compute the singular value decomposition:
U?V T of rank r. Matrices U and V T of sizes
(t, r) and (r, d) are the left and right singular vec-
tors; ? is the (r, r) diagonal matrix of singular
values (ordered in descending order)1. Similarity
between terms i and j is computed as the scalar
product between the two vectors associated to the
words in the U matrix:
sim(ui, uj) = ?
k
l=1uilujl
1Any approximation of rank k < r can simply be ob-
tained from an approximation or rank r by deleting rows and
columns.
27
2.2 Relatedness curves
Finding the optimal dimensionality k has proven
to be an extremely important and not trivial step.
(Bast and Majumdar, 2005) show that no single cut
dimension is appropriate to compute the similarity
of two terms but this should be deduced from the
curve of similarity scores over all dimensions k.
The curve of relatedness for two terms ui and uj is
given by their scalar product across all dimensions
k, k smaller than a rank r:
k ? ?kl=1uilujl, for k = 1, ..., r
They show that a smooth curve indicates closely
related terms, while a curve exhibiting many direc-
tion changes indicates unrelated terms; the actual
values of the similarity scores are often mislead-
ing, which explains why a good cut dimension k
is so difficult to find.
2.3 Vector space representation of phrases
We choose to apply this to acquiring paraphrases
(or inference rules, i.e. entailments which hold in
just one direction) in the sense of DIRT (Lin and
Pantel, 2001).
In the DIRT algorithm a phrase is a noun-
ending path in a dependency graph and the goal
is to acquire inference rules such as (X solve Y,
X find solution to Y). We will call dependency
paths patterns. The input data consists of large
amounts of parsed text, from which patterns to-
gether with X-filler and Y-filler frequency counts
are extracted.
In this setting, a pattern receives two vector rep-
resentation, one in a X-filler space and one in the
Y-filler space. In order to compute the similarity
between two patterns, these are compared in the
X space and in the Y space, and the two result-
ing scores are multiplied. (The DIRT algorithm
uses Lin measure for computing similarity, which
is given in Section 4). Obtaining these vectors
from the frequency counts is straightforward and
it is exemplified in Table 1 which shows a frag-
ment of a Y-filler DIRT-like vector space.
.. case problem ..
(X solve Y, Y) .. 6.1 4.4 ..
(X settle Y, Y) .. 5.2 5.9 ..
Table 1: DIRT-like vector representation in the Y-filler
space. The values represent mutual information.
3 Relatedness curves for acquiring
paraphrases
3.1 Setup
We parsed the XIE fragment of GigaWord (ap-
prox. 100 mil. tokens) with Stanford dependency
parser. From this we built a pattern/word matrix of
size (85000, 3000) containing co-occurrence data
of the most frequent patterns with the most fre-
quent words2. We perform SVD factorization on
this matrix of rank k = 800. For each pair of pat-
terns, we can associate two relatedness curves: a
X curve and Y curve given by the scalar products
of their vectors in the U matrix, across dimensions
k : 1, ..., 800.
3.2 Evaluating smoothness of the relatedness
curves
In Figure 1 we plotted the X and Y curves of com-
paring the pattern X subj???? win dobj???? Y with itself.
Figure 1: X-filler and Y-filler relatedness curves
for the identity pair (X subj???? win dobj???? Y,X subj????
win
dobj
???? Y )
Figure 2: X-filler and Y-filler relatedness curves
for (X subj???? leader prp??? of pobj???? Y,X pobj???? by prp???
lead
subj
???? Y )
Normally, the X and Y curves for the identical
pair are monotonically increasing. However what
can be noticed is that the actual values of these
functions differ by one order of magnitude in the
X and in the Y curves of identical patterns, show-
ing that in themselves they are not a good indica-
2Even if conceptually we have two semantic spaces (given
by X-fillers and Y-fillers), in reality we can work with a sin-
gle matrix, containing for each pattern also its reverse, both
represented solely in a X-filler space
28
Figure 3: X-filler and Y-filler relatedness curves
for (X subj???? win dobj???? Y,X subj???? murder dobj???? Y )
tor of similarity. In Figure 2 we investigate a pair
of closely related patterns: (X subj???? leader prp???
of
pobj
???? Y,X
pobj
???? by
prp
??? lead
subj
???? Y ). It can be
noticed that while still not comparable to those of
the identical pair, these curves are much smoother
than the ones associated to the pair of unrelated
patterns in Figure 33.
However, unlike in the information retrieval
scenario in (Bast and Majumdar, 2005), for which
a hard related/not-related assignment works best,
for acquiring paraphrases we need to quantify the
smoothness of the curves. We describe two func-
tions for evaluating curve smoothness which we
will use to compute scores in X-filler and Y-filler
semantic spaces.
Smooth function 1 This function simply com-
putes the number of changes in the direction of the
curve, as the percentage of times the scalar prod-
uct increases or remains equal from step l to step
l + 1:
CurveS1(ui, uj) =
?uilujl?01
k
, l = 1, ..., k
An increasing curve will be assigned the maximal
value 1, while for a curve that is monotonically
decreasing the score will be 0.
Smooth function 2 (Bast and Majumdar, 2005)
The second smooth function is given by:
CurveS2(ui, uj) =
max?min
?kl=1abs(uilujl)
where max and min are the largest and smallest
values in the curves. A curve which is always in-
creasing or always decreasing will get a score of 1.
Unlike the previous method this function is sensi-
tive to the absolute values in the drops of a curve.
3The drop out dimension discussed in (Bast and Majum-
dar, 2005) Section 3, does not seem to exist for our data. This
is to be expected since this result stems from a definition of
perfectly related terms which is adapted to the particularities
of term/document matrices, and not of term/term matrices.
A curve with large drops, irrelevant of their cardi-
nality, will be penalized by being assigned a low
score.
4 Experimental results
In order to compute the similarity score between
two phrases, we follow (Lin and Pantel, 2001)
and compute two similarity scores, corresponding
to the X-fillers and Y-fillers, and multiply them.
Given a similarity function, any pattern encoun-
tered in the corpus can be paraphrased by return-
ing its most similar patterns.
We implement five similarity functions on the
data we have described in the previous section.
The first one is the DIRT algorithm and it is the
only method using the original co-occurrence ma-
trix in which raw counts are replaced by point-
wise mutual information scores.
DIRT method The similarity function for two
vectors pi and pj is:
simLin(pi, pj) =
?
l?I(pi)?I(pj)
(pil + pjl)
?
l?I(pi)
pil +
?
l?I(pj)
pjl
where values in pi and pj are point-wise mu-
tual information, and I(?) gives the indices of non-
negative values in a vector.
Methods on SVD factorization All these meth-
ods perform computations the (85000, 800) U ma-
trix in the SVD factorization. On this we imple-
ment two methods which do an arbitrary dimen-
sion cut of k = 600: 1) SP-600 (scalar product)
and 2) COS-600 (cosine similarity). The other
two algorithms: CurveS1 and CurveS2 use the
two curve smoothness functions in Section 3.2; the
curves plot the scalar product corresponding to the
two patterns, from dimension 1 to 800.
Data In these preliminary experiments we limit
ourselves to paraphrasing a set of patterns ex-
tracted from a subset of the TREC02-TREC06
question answering tracks. From these questions
we extracted and paraphrased the most frequently
occurring 20 patterns. Since judging the cor-
rectness of these paraphrases ?out-of-context? is
rather difficult we limit ourselves to giving exam-
ples and analyzing errors made on this data; im-
portant observations can be clearly made this way,
however in future work we plan to build a proper
evaluation setting (e.g. task-based or instance-
based in the sense of (Szpektor et al, 2007)) for
29
a more detailed analysis of the performance on the
methods discussed.
4.1 Results
We list the paraphrases obtained with the different
methods for the pattern X subj???? show dobj???? Y . This
pattern has been chosen out of the total set due
to its medium difficulty in terms of paraphrasing;
some of the patterns in our list are relatively ac-
curately paraphrased by all methods, such as win,
while others such as marry are almost impossible
to paraphrase, for all methods. In Table 2 we list
the top 10 expansions returned by the four meth-
ods using the SVD factorization. In bold we mark
correct patterns, which we consider to be patterns
for which there is a context in which the entail-
ment holds in at least one direction.
As it is clearly reflected in this example the SP-
600 is much worse than any of the curve analy-
sis methods; however using cosine as similarity
measure at the same arbitrarily chosen dimension
(COS-600) brings major improvements.
The two curve smoothness methods exhibit a
systematic difference between them. In this ex-
ample, and also across all 20 instances we have
considered, CurveS1 ranks as most similar, a large
variety of patterns with the same lexical root (in
which, of course, syntax is often incorrect). Only
following this we can find patterns expressing lex-
ical variations; these again will be present in many
syntactic variations. This sets CurveS1 apart from
both CurveS2 and from COS-600 methods. These
latter two methods, although conceptually differ-
ent seem to exhibit surprisingly similar behavior.
The behavior of CurveS1 smoothing method is
difficult to judge without a proper evaluation; it
can be the case that the errors (mostly in syntac-
tic relations) are indeed errors of the algorithm or
that the parser introduces them already in our input
data.
Table 3 shows the top 10 paraphrases returned
by the DIRT algorithm. The DIRT paraphrases are
rather accurate, however it is interesting to observe
that DIRT and SVD methods can extract differ-
ent paraphrases. Table 4 gives examples of correct
paraphrases which are identified by DIRT but not
CurveS2 and the other way around. This seems to
indicate that these algorithms do capture different
aspects of the data and can be combined for bet-
ter results. An important aspect here is the fact
that obtaining highly accurate paraphrases at the
DIRT
subj
???? reflect
dobj
????
subj
???? indicate
dobj
????
subj
???? demonstrate
dobj
????
pobj
???? in
prp
??? show
dobj
????
pobj
???? to
prp
??? show
dobj
????
subj
???? represent
dobj
????
subj
???? show
prp
??? in
pobj
????
subj
???? display
dobj
????
subj
???? bring
dobj
????
pobj
???? with
prp
??? show
dobj
????
Table 3: Top 10 paraphrases for X subj???? show dobj????
Y
cost of losing coverage is not particularly difficult4
however not very useful. Previous work such as
(Dinu and Wang, 2009) has shown that for these
resources, the coverage is a rather important as-
pect, since they have to capture the great variety
of ways in which a meaning can be expressed in
different contexts.
CurveS2 DIRT
subj
???? show
dobj
????
pobj
???? in
prp
??? indicate
dobj
????
subj
???? display
dobj
????
pobj
???? in
prp
??? reflect
dobj
????
subj
???? confirm
dobj
????
dobj
???? interpret
prp
??? as
pobj
????
subj
???? point
prp
??? to
pobj
????
subj
???? win
dobj
????
subj
???? vie
prp
??? for
pobj
????
pos
??? victory
prp
??? in
pobj
????
subj
???? compete
prp
??? for
pobj
????
subj
???? win
dobj
???? title
nn
???
subj
???? secure
dobj
????
appos
????? winner
nn
???
subj
???? enter
dobj
????
subj
???? march
prp
??? into
pobj
????
subj
???? start
prp
??? in
pobj
????
subj
???? advance
prp
??? into
pobj
????
subj
???? play
prp
??? in
pobj
????
pos
??? entry
prp
??? to
pobj
????
subj
???? join
prp
??? in
pobj
????
Table 4: Example of paraphrases (i.e. ranked in
the top 30) identified by one method and not the
other
4.2 Discussion
In this section we attempt to get more insight into
the way the relatedness curves relate to the intu-
itive notion of similarity, by examining curves of
incorrect paraphrases extracted by our methods.
The first error we consider, is the pattern X pos???
confidence
pobj
???? of
prp
??? Y which is judged as be-
ing very similar to show by SP-600, COS-600 as
well as CurveS2. Figure 4 shows the relatedness
curves. As it can be noticed, both the X and Y
similarities grow dramatically around dimension
4High precision can be very easily achieved simply by in-
tersecting the sets of paraphrases returned by two or more of
the methods implemented
30
SP-600 COS-600 CurveS1 CurveS2
pos
??? confidence
pobj
???? of
prp
???
subj
???? indicate
dobj
????
subj
???? show
prp
??? in
pobj
????
subj
???? indicate
dobj
????
subj
???? boost
dobj
???? rate
nn
???
subj
???? show
prp
??? of
pobj
????
subj
???? indicate
dobj
????
subj
???? reflect
dobj
????
subj
???? show
prp
??? of
pobj
????
subj
???? represent
dobj
????
subj
???? show
prp
??? with
pobj
????
subj
???? represent
dobj
????
prp
??? to
pobj
???? percent
nn
???
pobj
???? by
prp
??? show
partmod
???????
pobj
???? with
prp
??? show
dobj
????
subj
???? bring
dobj
???? rate
nn
???
subj
???? total
dobj
???? yuan
appos
?????
pobj
???? in
prp
??? reflect
dobj
????
subj
???? show
tmod
?????
subj
???? show
prp
??? of
pobj
????
subj
???? hit
dobj
???? dollar
appos
?????
pos
??? confidence
pobj
???? of
prp
???
subj
???? show
prp
??? despite
pobj
????
dobj
???? interpret
prp
??? as
pobj
????
subj
???? reach
dobj
???? dollar
appos
?????
pobj
???? by
prp
??? reflect
dobj
????
pobj
???? during
prp
??? show
dobj
????
pos
??? confidence
pobj
???? of
prp
???
subj
???? slash
dobj
???? rate
nn
???
pobj
???? in
prp
??? indicate
dobj
????
pobj
???? in
prp
??? show
dobj
????
subj
???? show
dobj
???? rate
nn
???
nn
??? confidence
pobj
???? of
prp
???
subj
???? reflect
dobj
????
pobj
???? by
prp
??? show
partmod
???????
subj
???? put
dobj
???? rate
nn
???
subj
???? raise
dobj
???? rate
nn
???
subj
???? interpret
prp
??? as
pobj
????
pobj
???? on
prp
??? show
dobj
????
pobj
???? by
prp
??? show
partmod
???????
Table 2: Top 10 paraphrases for X subj???? show dobj???? Y
500. Therefore the scalar product will be very high
at cut point 600, leading to methods? SP-600 and
COS-600 error. However the two curve methods
are sensitive to the shape of the relatedness curves.
Since CurveS2 is sensitive to actual drop values in
these curves, this pair will still be ranked very sim-
ilar. The curves do decrease by small amounts in
many points which is why method CurveS1 does
score these two patterns as very similar.
An interesting point to be made here is that, this
pair is ranked similar by three methods out of four
because of the dramatic increase in relatedness at
around dimension 500. However, intuitively, such
an increase should be more relevant at earlier di-
mensions, which correspond to the larger eigen-
values, and therefore to the most relevant con-
cepts. Indeed, in the data we have analyzed, highly
similar patterns exhibit large increases at earlier
(first 100-200) dimensions, similarly to the exam-
ples given in Figure 1 and Figure 2. This leads
us to a particular aspect that we would like to in-
vestigate in future work, which is to analyze the
behavior of a relatedness curve in relation to rel-
evance weights obtained from the eigenvalues of
the matrix factorization.
In Figure 5 we plot a second error, the relat-
edness curves of show with X subj???? boost dobj????
rate
nn
??? Y which is as error made only by the SP-
600 method. The similarity reflected in curve Y
is relatively high (given by the large overlap of Y-
filler interest), however we obtain a very high X
similarity only due to the peak of the scalar prod-
uct exactly around the cut dimension 600.
5 Conclusion
In this paper we have investigated the relevance of
judging similarity of two phrases across all ranks
k in a SVD approximation of a phrase/term co-
Figure 4: X-filler and Y-filler relatedness curves
for (X subj???? show dobj???? Y,X pos??? confidence pobj????
of
prp
??? Y )
Figure 5: X-filler and Y-filler relatedness curves
for (X subj???? show dobj???? Y,X subj???? boost dobj????
rate
nn
??? Y )
occurrence matrix. We confirm the major observa-
tions made in previous work and our preliminary
experiments indicate that reliable similarity scores
for paraphrasing can be obtained from the analysis
of relatedness scores across all dimensions.
In the future we plan to 1) use the observations
we have made in Section 4.2 to focus on iden-
tifying good curve-smoothness functions and 2)
build an appropriate evaluation setting in order to
be able to accurately judge the performance of the
methods we propose.
Finally, in this paper we have investigated these
aspects for the task of paraphrasing in a particular
setting, however our findings can be applied to any
vector space method for semantic similarity.
31
References
Scott C. Deerwester and Susan T. Dumais and Thomas
K. Landauer and George W. Furnas and Richard A.
Harshman 1990. Indexing by Latent Semantic Anal-
ysis In JASIS.
Bast, Holger and Majumdar, Debapriyo. 2005. Why
spectral retrieval works. SIGIR ?05: Proceedings of
the 28th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of Inference Rules from Text. In Proceedings of
the ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009).
Idan Szpektor and Eyal Shnarch and Ido Dagan 2007.
Instance-based Evaluation of Entailment Rule Ac-
quisition. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics.
32
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50?58,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
General estimation and evaluation
of compositional distributional semantic models
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
In recent years, there has been widespread
interest in compositional distributional
semantic models (cDSMs), that derive
meaning representations for phrases from
their parts. We present an evaluation of al-
ternative cDSMs under truly comparable
conditions. In particular, we extend the
idea of Baroni and Zamparelli (2010) and
Guevara (2010) to use corpus-extracted
examples of the target phrases for param-
eter estimation to the other models pro-
posed in the literature, so that all models
can be tested under the same training con-
ditions. The linguistically motivated func-
tional model of Baroni and Zamparelli
(2010) and Coecke et al (2010) emerges
as the winner in all our tests.
1 Introduction
The need to assess similarity in meaning is cen-
tral to many language technology applications,
and distributional methods are the most robust ap-
proach to the task. These methods measure word
similarity based on patterns of occurrence in large
corpora, following the intuition that similar words
occur in similar contexts. More precisely, vector
space models, the most widely used distributional
models, represent words as high-dimensional vec-
tors, where the dimensions represent (functions
of) context features, such as co-occurring context
words. The relatedness of two words is assessed
by comparing their vector representations.
The question of assessing meaning similarity
above the word level within the distributional
paradigm has received a lot of attention in re-
cent years. A number of compositional frame-
works have been proposed in the literature, each
of these defining operations to combine word vec-
tors into representations for phrases or even en-
tire sentences. These range from simple but ro-
bust methods such as vector addition to more ad-
vanced methods, such as learning function words
as tensors and composing constituents through in-
ner product operations. Empirical evaluations in
which alternative methods are tested in compara-
ble settings are thus called for. This is compli-
cated by the fact that the proposed compositional
frameworks package together a number of choices
that are conceptually distinct, but difficult to disen-
tangle. Broadly, these concern (i) the input repre-
sentations fed to composition; (ii) the composition
operation proper; (iii) the method to estimate the
parameters of the composition operation.
For example, Mitchell and Lapata in their clas-
sic 2010 study propose a set of composition op-
erations (multiplicative, additive, etc.), but they
also experiment with two different kinds of input
representations (vectors recording co-occurrence
with words vs. distributions over latent topics) and
use supervised training via a grid search over pa-
rameter settings to estimate their models. Gue-
vara (2010), to give just one further example, is
not only proposing a different composition method
with respect to Mitchell and Lapata, but he is
also adopting different input vectors (word co-
occurrences compressed via SVD) and an unsu-
pervised estimation method based on minimizing
the distance of composed vectors to their equiva-
lents directly extracted from the source corpus.
Blacoe and Lapata (2012) have recently high-
lighted the importance of teasing apart the differ-
ent aspects of a composition framework, present-
ing an evaluation in which different input vector
representations are crossed with different compo-
sition methods. However, two out of three com-
position methods they evaluate are parameter-free,
so that they can side-step the issue of fixing the pa-
rameter estimation method.
In this work, we evaluate all composition meth-
ods we know of, excluding a few that lag be-
50
hind the state of the art or are special cases of
those we consider, while keeping the estimation
method constant. This evaluation is made pos-
sible by our extension to all target composition
models of the corpus-extracted phrase approxima-
tion method originally proposed in ad-hoc settings
by Baroni and Zamparelli (2010) and Guevara
(2010). For the models for which it is feasible,
we compare the phrase approximation approach
to supervised estimation with crossvalidation, and
show that phrase approximation is competitive,
thus confirming that we are not comparing mod-
els under poor training conditions. Our tests are
conducted over three tasks that involve different
syntactic constructions and evaluation setups. Fi-
nally, we consider a range of parameter settings for
the input vector representations, to insure that our
results are not too brittle or parameter-dependent.1
2 Composition frameworks
Distributional semantic models (DSMs) approxi-
mate word meanings with vectors recording their
patterns of co-occurrence with corpus contexts
(e.g., other words). There is an extensive literature
on how to develop such models and on their eval-
uation (see, e.g., Clark (2012), Erk (2012), Tur-
ney and Pantel (2010)). We focus here on compo-
sitional DSMs (cDSMs). After discussing some
options pertaining to the input vectors, we review
all the composition operations we are aware of
(excluding only the tensor-product-based models
shown by Mitchell and Lapata (2010) to be much
worse than simpler models),2 and then methods to
estimate their parameters.
Input vectors Different studies have assumed
different distributional inputs to composition.
These include bag-of-words co-occurrence vec-
tors, possibly mapped to lower dimensionality
with SVD or other techniques (Mitchell and La-
pata (2010) and many others), vectors whose di-
1We made the software we used to construct seman-
tic models and estimate and test composition methods
available online at http://clic.cimec.unitn.it/
composes/toolkit/
2Erk and Pado? (2008) and Thater et al (2010) use in-
put vectors that have been adapted to their phrasal contexts,
but then apply straightforward composition operations such
as addition and multiplication to these contextualized vec-
tors. Their approaches are thus not alternative cDSMs, but
special ways to construct the input vectors. Grefenstette and
Sadrzadeh (2011a; 2011b) and Kartsaklis et al (2012) pro-
pose estimation techniques for the tensors in the functional
model of Coecke et al (2010). Turney (2012) does not com-
pose representations but similarity scores.
Model Composition function Parameters
Add w1~u + w2~v w1, w2
Mult ~uw1  ~vw2 w1, w2
Dil ||~u||22~v + (? ? 1)?~u,~v?~u ?
Fulladd W1~u + W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Fulllex tanh([W1,W2]
h
Au~v
Av~u
i
) W1,W2,
Au, Av ? Rm?m
Table 1: Composition functions of inputs (u, v).
mensions record the syntactic link between targets
and collocates (Erk and Pado?, 2008; Thater et al,
2010), and most recently vectors based on neural
language models (Socher et al, 2011; Socher et
al., 2012). Blacoe and Lapata (2012) compared
the three representations on phrase similarity and
paraphrase detection, concluding that ?simple is
best?, that is, the bag-of-words approach performs
at least as good or better than either syntax-based
or neural representations across the board. Here,
we take their message home and we focus on bag-
of-words representations, exploring the impact of
various parameters within this approach.
Most frameworks assume that word vectors
constitute rigid inputs fixed before composition,
often using a separate word-similarity task inde-
pendent of composition. The only exception is
Socher et al (2012), where the values in the in-
put vectors are re-estimated during composition
parameter optimization. Our re-implementation of
their method assumes rigid input vectors instead.
Composition operations Mitchell and Lapata
(2008; 2010) present a set of simple but effec-
tive models in which each component of the output
vector is a function of the corresponding compo-
nents of the inputs. Given input vectors ~u and ~v,
the weighted additive model (Add) returns their
weighted sum: ~p = w1~u + w2~v. In the dilation
model (Dil), the output vector is obtained by de-
composing one of the input vectors, say ~v, into
a vector parallel to ~u and an orthogonal vector,
and then dilating only the parallel vector by a fac-
tor ? before re-combining (formula in Table 1).
Mitchell and Lapata also propose a simple mul-
tiplicative model in which the output components
are obtained by component-wise multiplication of
the corresponding input components. We intro-
duce here its natural weighted extension (Mult),
that takes w1 and w2 powers of the components
before multiplying, such that each phrase compo-
nent pi is given by: pi = u
w1
i v
w2
i .
51
Guevara (2010) and Zanzotto et al (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: ~p =
W1~u + W2~v.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a function from nouns onto (modified) nouns.
Given that linear functions can be expressed by
matrices and their application by matrix-by-vector
multiplication, a functor (such as the adjective) is
represented by a matrix Au to be composed with
the argument vector ~v (e.g., the noun) by multi-
plication, returning the lexical function (Lexfunc)
representation of the phrase: ~p = Au~v.
The method proposed by Socher et al (2012)
(see Socher et al (2011) for an earlier proposal
from the same team) can be seen as a combination
and non-linear extension of Fulladd and Lexfunc
(that we thus call Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (~u,Au) and (~v,Av), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products Au~v
and Av~u (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Estimating composition parameters If we
have manually labeled example data for a target
task, we can use supervised machine learning to
optimize parameters. Mitchell and Lapata (2008;
2010), since their models have just a few param-
eters to optimize, use a direct grid search for the
parameter setting that performs best on the train-
ing data. Socher et al (2012) train their models
using multinomial softmax classifiers.
If our goal is to develop a cDSM optimized for
a specific task, supervised methods are undoubt-
edly the most promising approach. However, ev-
ery time we face a new task, parameters must be
re-estimated from scratch, which goes against the
idea of distributional semantics as a general sim-
ilarity resource (Baroni and Lenci, 2010). More-
over, supervised methods are highly composition-
model-dependent, and for models such as Fulladd
and Lexfunc we are not aware of proposals about
how to estimate them in a supervised manner.
Socher et al (2011) propose an autoencoding
strategy. Given a decomposition function that re-
constructs the constituent vectors from a phrase
vector (e.g., it re-generates green and jacket vec-
tors from the composed green jacket vector), the
composition parameters minimize the distance be-
tween the original and reconstructed input vectors.
This method does not require hand-labeled train-
ing data, but it is restricted to cDSMs for which
an appropriate decomposition function can be de-
fined, and even in this case the learning problem
might lack a closed-form solution.
Guevara (2010) and Baroni and Zamparelli
(2010) optimize parameters using examples of
how the output vectors should look like that are
directly extracted from the corpus. To learn, say, a
Lexfunc matrix representing the adjective green,
we extract from the corpus example vectors of
?N, green N? pairs that occur with sufficient fre-
quency (?car, green car?, ?jacket, green jacket?,
?politician, green politician?, . . . ). We then use
least-squares methods to find weights for the green
matrix that minimize the distance between the
green N vectors generated by the model given the
input N and the corresponding corpus-observed
phrase vectors. This is a very general approach, it
does not require hand-labeled data, and it has the
nice property that corpus-harvested phrase vec-
tors provide direct evidence of the polysemous be-
haviour of functors (the green jacket vs. politician
contexts, for example, will be very different). In
the next section, we extend the corpus-extracted
phrase approximation method to all cDSMs de-
scribed above, with closed-form solutions for all
but the Fulllex model, for which we propose a
rapidly converging iterative estimation method.
3 Least-squares model estimation using
corpus-extracted phrase vectors3
Notation Given two matricesX,Y ? Rm?n we
denote their inner product by ?X,Y ?, (?X,Y ? =
?m
i=1
?n
j=1 xijyij). Similarly we denote by
?u, v? the dot product of two vectors u, v ? Rm?1
and by ||u|| the Euclidean norm of a vector:
3Proofs omitted due to space constraints.
52
||u|| = ?u, u?1/2. We use the following Frobe-
nius norm notation: ||X||F = ?X,X?
1/2. Vectors
are assumed to be column vectors and we use xi
to stand for the i-th (m ? 1)-dimensional column
of matrix X . We use [X,Y ] ? Rm?2n to denote
the horizontal concatenation of two matrices while[
X
Y
]
? R2m?n is their vertical concatenation.
General problem statement We assume vocab-
ularies of constituents U , V and that of resulting
phrases P . The training data consist of a set of
tuples (u, v, p) where p stands for the phrase asso-
ciated to the constituents u and v:
T = {(ui, vi, pi)|(ui, vi, pi) ? U?V?P, 1 ? i ? k}
We build the matrices U, V, P ? Rm?k by con-
catenating the vectors associated to the training
data elements as columns.4
Given the training data matrices, the general
problem can be stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where fcomp? is a composition function and ?
stands for a list of parameters that this composition
function is associated to. The composition func-
tions are defined: fcomp? : Rm?1 ? Rm?1 ?
Rm?1 and fcomp?(U, V ) stands for their natural
extension when applied on the individual columns
of the U and V matrices.
Add The weighted additive model returns the
sum of the composing vectors which have been
re-weighted by some scalars w1 and w2: ~p =
w1~u + w2~v. The problem becomes:
w?1, w
?
2 = arg min
w1,w2?R
||P ? w1U ? w2V ||F
The optimal w1 and w2 are given by:
w?1 =
||V ||2F ?U,P ? ? ?U, V ??V, P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(1)
w?2 =
||U ||2F ?V, P ? ? ?U, V ??U,P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(2)
4In reality, not all composition models require u, v and p
to have the same dimensionality.
Dil Given two vectors ~u and ~v, the dilation
model computes the phrase vector ~p = ||~u||2~v +
(? ? 1)?~u,~v?~u where the parameter ? is a scalar.
The problem becomes:
?? = arg min
??R
||P ?V D||ui||2 ?UD(??1)?ui,vi?||F
where by D||ui||2 and D(??1)?ui,vi? we denote
diagonal matrices with diagonal elements (i, i)
given by ||ui||2 and (? ? 1)?ui, vi? respectively.
The solution is:
?? = 1?
?k
i=1?ui, (||ui||
2vi ? pi)??ui, vi?
?k
i=1?ui, vi?
2||ui||2
Mult Given two vectors ~u and ~v, the weighted
multiplicative model computes the phrase vector
~p = ~uw1  ~vw2 where  stands for component-
wise multiplication. We assume for this model that
U, V, P ? Rm?n++ , i.e. that the entries are strictly
larger than 0: in practice we add a small smooth-
ing constant to all elements to achieve this (Mult
performs badly on negative entries, such as those
produced by SVD). We use the w1 and w2 weights
obtained when solving the much simpler related
problem:5
w?1, w
?
2 = arg min
w1,w2?R
||log(P )?log(U.?w1V.
?w2)||F
where .? stands for the component-wise power op-
eration. The solution is the same as that for Add,
given in equations (1) and (2), with U ? log(U),
V ? log(V ) and P ? log(P ).
Fulladd The full additive model assumes the
composition of two vectors to be ~p = W1~u+W2~v
where W1,W2 ? Rm?m. The problem is:
[W1,W2]
? = arg min
[W1,W2]?Rm?2m
||P?[W1W2]
[
U
V
]
||
This is a multivariate linear regression prob-
lem (Hastie et al, 2009) for which the least
squares estimate is given by: [W1,W2] =
((XTX)?1XTY )T where we use X = [UT , V T ]
and Y = P T .
Lexfunc The lexical function composition
method learns a matrix representation for each
functor (given by U here) and defines composition
as matrix-vector multiplication. More precisely:
5In practice training Mult this way achieves similar or
lower errors in comparison to Add.
53
~p = Au~v where Au is a matrix associated to each
functor u ? U . We denote by Tu the training
data subset associated to an element u, which
contains only tuples which have u as first element.
Learning the matrix representations amounts to
solving the set of problems:
Au = arg min
Au?Rm?m
||Pu ?AuVu||
for each u ? U where Pu, Vu ? Rm?|Tu|
are the matrices corresponding to the Tu train-
ing subset. The solutions are given by: Au =
((VuV Tu )
?1VuP Tu )
T . This composition function
does not use the functor vectors.
Fulllex This model can be seen as a generaliza-
tion of Lexfunc which makes no assumption on
which of the constituents is a functor, so that both
words get a matrix and a vector representation.
The composition function is:
~p = tanh([W1,W2]
[
Au~v
Av~u
]
)
where Au and Av are the matrices associated to
constituents u and v and [W1,W2] ? Rm?2m.
The estimation problem is given in Figure 1.
This is the only composition model which does
not have a closed-form solution. We use a block
coordinate descent method, in which we fix each
of the matrix variables but one and solve the corre-
sponding least-squares linear regression problem,
for which we can use the closed-form solution.
Fixing everything but [W1,W2]:
[W ?1 ,W
?
2 ] = ((X
TX)?1XTY )T
X =
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]T
Y = atanh(P T )
Fixing everything but Au for some element u,
the objective function becomes:
||atanh(Pu)?W1AuVu?W2[Av1~u, ..., Avk?~u]||F
where v1...vk? ? V are the elements occurring
with u in the training data and Vu the matrix result-
ing from their concatenation. The update formula
for the Au matrices becomes:
A?u = W
?1
1 ((X
TX)?1XTY )T
X = V Tu
Y = (atanh(Pu)?W2[Av1~u, ..., Avk?~u])
T
In all our experiments, Fulllex estimation con-
verges after very few passes though the matrices.
Despite the very large number of parameters of
this model, when evaluating on the test data we ob-
serve that using a higher dimensional space (such
as 200 dimensions) still performs better than a
lower dimensional one (e.g., 50 dimensions).
4 Evaluation setup and implementation
4.1 Datasets
We evaluate the composition methods on three
phrase-based benchmarks that test the models on
a variety of composition processes and similarity-
based tasks.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2008), focuses on
simple sentences consisting of intransitive verbs
and their noun subjects. It contains a total of
120 sentence pairs together with human similar-
ity judgments on a 7-point scale. For exam-
ple, conflict erupts/conflict bursts is scored 7, skin
glows/skin burns is scored 1. On average, each
pair is rated by 30 participants. Rather than eval-
uating against mean scores, we use each rating as
a separate data point, as done by Mitchell and La-
pata. We report Spearman correlations between
human-assigned scores and model cosine scores.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We
focus on the latter, and we frame the task dif-
ferently from Turney?s original definition due to
data sparsity issues.6 In our version, the dataset
contains 620 ANs, each paired with a single-
noun paraphrase. Examples include: archaeolog-
ical site/dig, spousal relationship/marriage and
dangerous undertaking/adventure. We evaluate a
model by computing the cosine of all 20K nouns in
our semantic space with the target AN, and look-
ing at the rank of the correct paraphrase in this list.
The lower the rank, the better the model. We re-
port median rank across the test items.
Determiner phrases The last dataset, intro-
duced in Bernardi et al (2013), focuses on a
class of grammatical terms (rather than content
6Turney used a corpus of about 50 billion words, almost
20 times larger than ours, and we have very poor or no cov-
erage of many original items, making the ?multiple-choice?
evaluation proposed by Turney meaningless in our case.
54
W ?1 ,W
?
2 , A
?
u1 , ..., A
?
v1 , ... =arg min
Rm?m
||atanh(P T )? [W1,W2]
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]
||F
=arg min
Rm?m
||atanh(P T )?W1[Au1 ~v1, ..., Auk ~vk]?W2[Av1 ~u1, ..., Avk ~uk]||F
Figure 1: Fulllex estimation problem.
words), namely determiners. It is a multiple-
choice test where target nouns (e.g., amnesia)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., no mem-
ory). The task differs from the previous one also
because here the targets are single words, and the
related items are composite. There are 173 tar-
get nouns in total, each paired with one correct
DP response, as well as 5 foils, namely the de-
terminer (no) and noun (memory) from the correct
response and three more DPs, two of which con-
tain the same noun as the correct phrase (less mem-
ory, all memory), the third the same determiner
(no repertoire). Other examples of targets/related-
phrases are polysemy/several senses and tril-
ogy/three books. The models compute cosines be-
tween target noun and responses and are scored
based on their accuracy at ranking the correct
phrase first.
4.2 Input vectors
We extracted distributional semantic vectors us-
ing as source corpus the concatenation of ukWaC,
Wikipedia (2009 dump) and BNC, 2.8 billion to-
kens in total.7 We use a bag-of-words approach
and we count co-occurrences within sentences and
with a limit of maximally 50 words surrounding
the target word. By tuning on the MEN lexical
relatedness dataset,8 we decided to use the top
10K most frequent content lemmas as context fea-
tures (vs. top 10K inflected forms), and we experi-
mented with positive Pointwise and Local Mutual
Information (Evert, 2005) as association measures
(vs. raw counts, log transform and a probability
ratio measure) and dimensionality reduction by
Non-negative Matrix Factorization (NMF, Lee and
Seung (2000)) and Singular Value Decomposition
(SVD, Golub and Van Loan (1996)) (both outper-
forming full dimensionality vectors on MEN). For
7http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
8http://clic.cimec.unitn.it/?elia.
bruni/MEN
both reduction techniques, we varied the number
of dimensions to be preserved from 50 to 300 in
50-unit intervals. As Local Mutual Information
performed very poorly across composition exper-
iments and other parameter choices, we dropped
it. We will thus report, for each experiment and
composition method, the distribution of the rele-
vant performance measure across 12 input settings
(NMF vs. SVD times 6 dimensionalities). How-
ever, since the Mult model, as expected, worked
very poorly when the input vectors contained neg-
ative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
4.3 Composition model estimation
Training by approximating the corpus-extracted
phrase vectors requires corpus-based examples of
input (constituent word) and output (phrase) vec-
tors for the composition processes to be learned.
In all cases, training examples are simply selected
based on corpus frequency. For the first experi-
ment, we have 42 distinct target verbs and a total
of ?20K training instances, that is, ??noun, verb?,
noun-verb? tuples (505 per verb on average). For
the second experiment, we have 479 adjectives and
?1 million ??adjective, noun?, adjective-noun?
training tuples (2K per adjective on average). In
the third, 50 determiners and 50K ??determiner,
noun?, determiner-noun? tuples (1K per deter-
miner). For all models except Lexfunc and Ful-
llex, training examples are pooled across target el-
ements to learn a single set of parameters. The
Lexfunc model takes only argument word vectors
as inputs (the functors in the three datasets are
verbs, adjectives and determiners, respectively). A
separate weight matrix is learned for each func-
tor, using the corresponding training data.9 The
Fulllex method jointly learns distinct matrix rep-
resentations for both left- and right-hand side con-
9For the Lexfunc model we have experimented with least
squeares regression with and without regularization, obtain-
ing similar results.
55
stituents. For this reason, we must train this model
on balanced datasets. More precisely, for the in-
transitive verb experiments, we use training data
containing noun-verb phrases in which the verbs
and the nouns are present in the lists of 1,500
most frequent verbs/nouns respectively, adding to
these the verbs and nouns present in our dataset.
We obtain 400K training tuples. We create the
training data similarity for the other datasets ob-
taining 440K adjective-noun and 50K determiner
phrase training tuples, respectively (we also exper-
imented with Fulllex trained on the same tuples
used for the other models, obtaining considerably
worse results than those reported). Finally, for Dil
we treat direction of stretching as a further param-
eter to be optimized, and find that for intransitives
it is better to stretch verbs, in the other datasets
nouns.
For the simple composition models for which
parameters consist of one or two scalars, namely
Add, Mult and Dil, we also tune the parame-
ters through 5-fold crossvalidation on the datasets,
directly optimizing the parameters on the target
tasks. For Add and Mult, we search w1, w2
through the crossproduct of the interval [0 : 5] in
0.2-sized steps. For Dil we use ? ? [0 : 20], again
in 0.2-sized steps.
5 Evaluation results
We begin with some remarks pertaining to the
overall quality of and motivation for corpus-
phrase-based estimation. In seven out of nine
comparisons of this unsupervised technique with
fully supervised crossvalidation (3 ?simple? mod-
els ?Add, Dil and Mult? times 3 test sets), there
was no significant difference between the two esti-
mation methods.10 Supervised estimation outper-
formed the corpus-phrase-based method only for
Dil on the intransitive sentence and AN bench-
marks, but crossvalidated Dil was outperformed
by at least one phrase-estimated simple model on
both benchmarks.
The rightmost boxes in the panels of Fig-
ure 2 depict the performance distribution for us-
ing phrase vectors directly extracted from the
corpus to tackle the various tasks. This non-
compositional approach outperforms all composi-
tional methods in two tasks over three, and it is
one of the best approaches in the third, although
10Significance assessed through Tukey Honestly Signifi-
cant Difference tests (Abdi and Williams, 2010), ? = 0.05.
in all cases even its top scores are far from the
theoretical ceiling. Still, performance is impres-
sive, especially in light of the fact that the non-
compositional approach suffers of serious data-
sparseness problems. Performance on the intran-
sitive task is above state-of-the-art despite the fact
that for almost half of the cases one test phrase
is not in the corpus, resulting in 0 vectors and
consequently 0 similarity pairs. The other bench-
marks have better corpus-phrase coverage (nearly
perfect AN coverage; for DPs, about 90% correct
phrase responses are in the corpus), but many tar-
get phrases occur only rarely, leading to unreliable
distributional vectors. We interpret these results as
a goodmotivation for corpus-phrase-based estima-
tion. On the one hand they show how good these
vectors are, and thus that they are sensible targets
of learning. On the other hand, they do not suffice,
since natural language is infinitely productive and
thus no corpus can provide full phrase coverage,
justifying the whole compositional enterprise.
The other boxes in Figure 2 report the perfor-
mance of the composition methods trained by cor-
pus phrase approximation. Nearly all models are
significantly above chance in all tasks, except for
Fulladd on intransitive sentences. To put AN me-
dian ranks into perspective, consider that a median
rank as high as 8,300 has near-0 probability to oc-
cur by chance. For DP accuracy, random guessing
gets 0.17% accuracy.
Lexfunc emerges consistently as the best model.
On intransitive constructions, it significantly out-
performs all other models except Mult, but the dif-
ference approaches significance even with respect
to the latter (p = 0.071). On this task, Lexfunc?s
median correlation (0.26) is nearly equivalent to
the best correlation across a wide range of parame-
ters reported by Erk and Pado? (2008) (0.27). In the
AN task, Lexfunc significantly outperforms Ful-
llex and Dil and, visually, its distribution is slightly
more skewed towards lower (better) ranks than any
other model. In the DP task, Lexfunc significantly
outperforms Add and Mult and, visually, most of
its distribution lies above that of the other mod-
els. Most importantly, Lexfunc is the only model
that is consistent across the three tasks, with all
other models displaying instead a brittle perfor-
mance pattern.11
Still, the top-performance range of all models
11No systematic trend emerged pertaining to the input vec-
tor parameters (SVD vs. NMF and retained dimension num-
ber).
56
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.00
0.05
0.10
0.15
0.20
0.25
0.30 Intransitive sentences
l
l
l
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us1
000
800
600
400
200
ANs
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.15
0.20
0.25
0.30
0.35
DPs
Figure 2: Boxplots displaying composition model performance distribution on three benchmarks, across
input vector settings (6 datapoints for Mult, 12 for all other models). For intransitive sentences, figure of
merit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct response
accuracy. The boxplots display the distribution median as a thick horizontal line within a box extending
from first to third quartile. Whiskers cover 1.5 of interquartile range in each direction from the box, and
extreme outliers outside this extended range are plotted as circles.
on the three tasks is underwhelming, and none of
them succeeds in exploiting compositionality to
do significantly better than using whatever phrase
vectors can be extracted from the corpus directly.
Clearly, much work is still needed to develop truly
successful cDSMs.
The AN results might look particularly worry-
ing, considering that even the top (lowest) median
ranks are above 100. A qualitative analysis, how-
ever, suggests that the actual performance is not
as bad as the numerical scores suggest, since of-
ten the nearest neighbours of the ANs to be para-
phrased are nouns that are as strongly related to
the ANs as the gold standard response (although
not necessarily proper paraphrases). For example,
the gold response to colorimetric analysis is col-
orimetry, whereas the Lexfunc (NMF, 300 dimen-
sions) nearest neighbour is chromatography; the
gold response to heavy particle is baryon, whereas
Lexfunc proposes muon; for melodic phrase the
gold is tune and Lexfunc has appoggiatura; for in-
door garden, the gold is hothouse but Lexfunc pro-
poses glasshouse (followed by the more sophisti-
cated orangery!), and so on and so forth.
6 Conclusion
We extended the unsupervised corpus-extracted
phrase approximation method of Guevara (2010)
and Baroni and Zamparelli (2010) to estimate
all known state-of-the-art cDSMs, using closed-
form solutions or simple iterative procedures in
all cases. Equipped with a general estimation ap-
proach, we thoroughly evaluated the cDSMs in
a comparable setting. The linguistically moti-
vated Lexfunc model of Baroni and Zamparelli
(2010) and Coecke et al (2010) was the win-
ner across three composition tasks, also outper-
forming the more complex Fulllex model, our re-
implementation of Socher et al?s (2012) compo-
sition method (of course, the composition method
is only one aspect of Socher et al?s architecture).
All other composition methods behaved inconsis-
tently.
In the near future, we want to focus on improv-
ing estimation itself. In particular, we want to
explore ways to automatically select good phrase
examples for training, beyond simple frequency
thresholds. We tested composition methods on
two-word phrase benchmarks. Another natural
next step is to apply the composition rules recur-
sively, to obtain representations of larger chunks,
up to full sentences, coming, in this way, nearer to
the ultimate goal of compositional distributional
semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
57
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), Sofia, Bulgaria. In press.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394?1404, Edinburgh,
UK.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62?66, Ed-
inburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING: Posters, pages 549?558, Mumbai, India.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
58
