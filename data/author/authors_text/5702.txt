Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 601?608
Manchester, August 2008
Modeling the Structure and Dynamics of the
Consonant Inventories: A Complex Network Approach
Animesh Mukherjee1, Monojit Choudhury2, Anupam Basu1, Niloy Ganguly1
1Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur, India ? 721302
2Microsoft Research India, Bangalore, India ? 560080
{animeshm,anupam,niloy}@cse.iitkgp.ernet.in,
monojitc@microsoft.com
Abstract
We study the self-organization of the con-
sonant inventories through a complex net-
work approach. We observe that the dis-
tribution of occurrence as well as co-
occurrence of the consonants across lan-
guages follow a power-law behavior. The
co-occurrence network of consonants ex-
hibits a high clustering coefficient. We
propose four novel synthesis models for
these networks (each of which is a refine-
ment of the earlier) so as to successively
match with higher accuracy (a) the above
mentioned topological properties as well
as (b) the linguistic property of feature
economy exhibited by the consonant inven-
tories. We conclude by arguing that a pos-
sible interpretation of this mechanism of
network growth is the process of child lan-
guage acquisition. Such models essentially
increase our understanding of the struc-
ture of languages that is influenced by their
evolutionary dynamics and this, in turn,
can be extremely useful for building future
NLP applications.
1 Introduction
A large number of regular patterns are observed
across the sound inventories of human languages.
These regularities are arguably a consequence of
the self-organization that is instrumental in the
emergence of these inventories (de Boer, 2000).
Many attempts have been made by functional pho-
nologists for explaining this self-organizing behav-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
ior through certain general principles such as max-
imal perceptual contrast (Liljencrants and Lind-
blom, 1972), ease of articulation (Lindblom and
Maddieson, 1988; de Boer, 2000), and ease of
learnability (de Boer, 2000). In fact, there are a
lot of studies that attempt to explain the emergence
of the vowel inventories through the application of
one or more of the above principles (Liljencrants
and Lindblom, 1972; de Boer, 2000). Some studies
have also been carried out in the area of linguistics
that seek to reason the observed patterns in the con-
sonant inventories (Trubetzkoy, 1939; Lindblom
and Maddieson, 1988; Boersma, 1998; Clements,
2008). Nevertheless, most of these works are con-
fined to certain individual principles rather than
formulating a general theory describing the emer-
gence of these regular patterns across the conso-
nant inventories.
The self-organization of the consonant inven-
tories emerges due to an interaction of different
forces acting upon them. In order to identify the
nature of these interactions one has to understand
the growth dynamics of these inventories. The the-
ories of complex networks provide a number of
growth models that have proved to be extremely
successful in explaining the evolutionary dynam-
ics of various social (Newman, 2001; Ramasco et
al., 2004), biological (Jeong et al, 2000) and other
natural systems. The basic framework for the cur-
rent study develops around two such complex net-
works namely, the Phoneme-Language Network
or PlaNet (Choudhury et al, 2006) and its one-
mode projection, the Phoneme-Phoneme Network
or PhoNet (Mukherjee et al2007a). We begin by
analyzing some of the structural properties (Sec. 2)
of the networks and observe that the consonant
nodes in both PlaNet and PhoNet follow a power-
law-like degree distribution. Moreover, PhoNet
601
is characterized by a high clustering coefficient,
a property that has been found to be prevalent in
many other social networks (Newman, 2001; Ra-
masco et al, 2004).
We propose four synthesis models for PlaNet
(Sec. 3), each of which employ a variant of a pref-
erential attachment (Baraba?si and Albert, 1999)
based growth kernel1. While the first two mod-
els are independent of the characteristic proper-
ties of the (consonant) nodes, the following two
use them. These models are successively refined
not only to reproduce the topological properties of
PlaNet and PhoNet, but also to match the linguis-
tic property of feature economy (Boersma, 1998;
Clements, 2008) that is observed across the conso-
nant inventories. The underlying growth rules for
each of these individual models helps us to inter-
pret the cause of the emergence of at least one (or
more) of the aforementioned properties. We con-
clude (Sec. 4) by providing a possible interpreta-
tion of the proposed mathematical model that we
finally develop in terms of child language acquisi-
tion.
There are three major contributions of this work.
Firstly, it provides a fascinating account of the
structure and the evolution of the human speech
sound systems. Furthermore, the introduction of
the node property based synthesis model is a sig-
nificant contribution to the field of complex net-
works. On a broader perspective, this work shows
how statistical mechanics can be applied in under-
standing the structure of a linguistic system, which
in turn can be extremely useful in developing fu-
ture NLP applications.
2 Properties of the Consonant
Inventories
In this section, we briefly recapitulate the defi-
nitions of PlaNet and PhoNet, the data source,
construction procedure for the networks and some
of their important structural properties. We also
revisit the concept of feature economy and the
method used for its quantification.
2.1 Structural Properties of the Consonant
Networks
PlaNet is a bipartite graph G = ? V
L
, V
C
, E
pl
? con-
sisting of two sets of nodes namely, V
L
(labeled by
the languages) and V
C
(labeled by the consonants);
1The word kernel here refers to the function or mathemat-
ical formula that drives the growth of the network.
Figure 1: Illustration of the nodes and edges of
PlaNet and PhoNet.
E
pl
is the set of edges running between V
L
and V
C
.
There is an edge e ? E
pl
from a node v
l
? V
L
to a
node v
c
? V
C
iff the consonant c is present in the
inventory of language l.
PhoNet is the one-mode projection of PlaNet
onto the consonant nodes i.e., a network of con-
sonants in which two nodes are linked by an edge
with weight as many times as they co-occur across
languages. Hence, it can be represented by a graph
G = ? V
C
, E
ph
?, where V
C
is the set of conso-
nant nodes and E
ph
is the set of edges connecting
these nodes in G. There is an edge e ? E
ph
if the
two nodes (read consonants) that are connected by
e co-occur in at least one language and the number
of languages they co-occur in defines the weight of
the edge e. Figure 1 shows the nodes and the edges
of PlaNet and PhoNet.
Data Source and Network Construction: Like
many other earlier studies (Liljencrants and Lind-
blom, 1972; Lindblom and Maddieson, 1988; de
Boer, 2000; Hinskens and Weijer, 2003), we use
the UCLA Phonological Segment Inventory Data-
base (UPSID) (Maddieson, 1984) as the source of
our data. There are 317 languages in the data-
base with a total of 541 consonants found across
them. Each consonant is characterized by a set of
phonological features (Trubetzkoy, 1931), which
distinguishes it from others. UPSID uses articula-
tory features to describe the consonants, which can
be broadly categorized into three different types
namely the manner of articulation, the place of
articulation and phonation. Manner of articu-
lation specifies how the flow of air takes place
in the vocal tract during articulation of a conso-
nant, whereas place of articulation specifies the
active speech organ and also the place where it
acts. Phonation describes the vibration of the vo-
602
Manner of Articulation Place of Articulation Phonation
tap velar voiced
flap uvular voiceless
trill dental
click palatal
nasal glottal
plosive bilabial
r-sound alveolar
fricative retroflex
affricate pharyngeal
implosive labial-velar
approximant labio-dental
ejective stop labial-palatal
affricated click dental-palatal
ejective affricate dental-alveolar
ejective fricative palato-alveolar
lateral approximant
Table 1: The table shows some of the important
features listed in UPSID. Over 99% of the UPSID
languages have bilabial, dental-alveolar and velar
plosives. Furthermore, voiceless plosives outnum-
ber the voiced ones (92% vs. 67%). 93% of the
languages have at least one fricative, 97% have at
least one nasal and 96% have at least one liquid.
Approximants occur in fewer than 95% of the lan-
guages.
cal cords during the articulation of a consonant.
Apart from these three major classes there are also
some secondary articulatory features found in cer-
tain languages. There are around 52 features listed
in UPSID; the important ones are noted in Table 1.
Note that in UPSID the features are assumed to be
binary-valued and therefore, each consonant can
be represented by a binary vector.
We have used UPSID in order to construct
PlaNet and PhoNet. Consequently, |V
L
| = 317 (in
PlaNet) and |V
C
| = 541. The number of edges in
PlaNet and PhoNet are 7022 and 30412 respec-
tively.
Degree Distributions of PlaNet and PhoNet:
The degree distribution is the fraction of nodes, de-
noted by P
k
, which have a degree2 greater than or
equal to k (Newman, 2003). The degree distribu-
tion of the consonant nodes in PlaNet and PhoNet
are shown in Figure 2 in the log-log scale. Both the
plots show a power-law behavior (P
k
? k
??) with
exponential cut-offs towards the ends. The value
of ? is 0.71 for PlaNet and 0.89 for PhoNet.
Clustering Coefficient of PhoNet: The clus-
tering coefficient for a node i is the proportion of
links between the nodes that are the neighbors of
i divided by the number of links that could pos-
sibly exist between them (Newman, 2003). Since
PhoNet is a weighted graph the above definition is
2For a weighted graph like PhoNet, the degree of a node i
is the sum of weights on the edges that are incident on i.
suitably modified by the one presented in (Barrat
et al, 2004). According to this definition, the clus-
tering coefficient for a node i is,
c
i
=
1
(
?
?j
w
ij
)
(k
i
? 1)
?
?j,l
(w
ij
+ w
il
)
2
a
ij
a
il
a
jl
(1)
where j and l are neighbors of i; k
i
represents the
plain degree of the node i; w
ij
, w
jl
and w
il
de-
note the weights of the edges connecting nodes i
and j, j and l, and i and l respectively; a
ij
, a
il
,
a
jl
are boolean variables, which are true iff there
is an edge between the nodes i and j, i and l, and j
and l respectively. The clustering coefficient of the
network (c
av
) is equal to the average clustering co-
efficient of the nodes. The value of c
av
for PhoNet
is 0.89, which is significantly higher than that of a
random graph with the same number of nodes and
edges (0.08).
2.2 Linguistic Properties: Feature Economy
and its Quantification
The principle of feature economy states that lan-
guages tend to use a small number of distinctive
features and maximize their combinatorial pos-
sibilities to generate a large number of conso-
nants (Boersma, 1998; Clements, 2008). Stated
differently, a given consonant will have a higher
than expected chance of occurrence in invento-
ries in which all of its features have already dis-
tinctively occurred in the other consonants. This
principle immediately implies that the consonants
chosen by a language should share a considerable
number of features among them. The quantifica-
tion process, which is a refinement of the idea pre-
sented in (Mukherjee et al2007b), is as follows.
Feature Entropy: For an inventory of size N ,
let there be p
f
consonants for which a particular
feature f (recall that we assume f to be binary-
valued) is present and q
f
other consonants for
which the same is absent. Therefore, the proba-
bility that a consonant (chosen uniformly at ran-
dom from this inventory) contains the feature f is
p
f
N
and the probability that it does not contain the
feature is qf
N
(=1?pf
N
). One can think of f as an in-
dependent random variable, which can take values
1 and 0, and pf
N
and qf
N
define the probability dis-
tribution of f . Therefore, for any given inventory,
we can define the binary entropy H
f
(Shannon and
Weaver, 1949) for the feature f as
H
f
= ?
p
f
N
log
2
p
f
N
?
q
f
N
log
2
q
f
N
(2)
603
Figure 2: Degree distribution (DD) of PlaNet alng with that of PlaNet
syn
obtained from Model I and II
respectively; (b) DD of PhoNet alng with that of PhoNet
syn
obtained from Model I and II respectively.
Both the plots are in log-log scale.
If F is the set of all features present in the conso-
nants forming the inventory, then feature entropy
F
E
is the sum of the binary entropies with respect
to all the features, that is
F
E
=
?
f?F
H
f
=
?
f?F
(?
p
f
N
log
2
p
f
N
?
q
f
N
log
2
q
f
N
)
(3)
Since we have assumed that f is an independent
random variable, F
E
is the joint entropy of the
system. In other words, F
E
provides an estimate
of the number of discriminative features present
in the consonants of an inventory that a speaker
(e.g., parent) has to communicate to a learner (e.g.,
child) during language transmission. The lower the
value of F
E
the higher is the feature economy. The
curve marked as (R) in Figure 3 shows the average
feature entropy of the consonant inventories of a
particular size3 (y-axis) versus the inventory size
(x-axis).
3 Synthesis Models
In this section, we describe four synthesis mod-
els that incrementally attempt to explain the emer-
gence of the structural properties of PlaNet and
PhoNet as well as the feature entropy exhibited by
the consonant inventories. In all these models, we
assume that the distribution of the consonant in-
ventory size, i.e., the degrees of the language nodes
in PlaNet, are known a priori.
3Let there be n inventories of a particular size k. The
average feature entropy of the inventories of size k is
1
n
?
n
i=1
F
E
i
, where F
E
i
signifies the feature entropy of the
i
th inventory of size k.
3.1 Model I: Preferential Attachment Kernel
This model employs a modified version of the ker-
nel described in (Choudhury et al, 2006), which is
the only work in literature that attempts to explain
the emergence of the consonant inventories in the
framework of complex networks.
Let us assume that a language node L
i
? V
L
has a degree k
i
. The consonant nodes in V
C
are
assumed to be unlabeled, i.e, they are not marked
by the distinctive features that characterize them.
We first sort the nodes L
1
through L
317
in the as-
cending order of their degrees. At each time step a
node L
j
, chosen in order, preferentially attaches it-
self with k
j
distinct nodes (call each such node C
i
)
of the set V
C
. The probability Pr(C
i
) with which
the node L
j
attaches itself to the node C
i
is given
by,
Pr(C
i
) =
d
i
?
+ ?
?
i
?
?V
?
C
(d
i
?
?
+ ?)
(4)
where, d
i
is the current degree of the node C
i
,
V
?
C
is the set of nodes in V
C
that are not already
connected to L
j
, ? is the smoothing parameter
that facilitates random attachments and ? indi-
cates whether the attachment kernel is sub-linear
(? < 1), linear (? = 1) or super-linear (? > 1).
Note that the modification from the earlier ker-
nel (Choudhury et al, 2006) is brought about by
the introduction of ?. The above process is re-
peated until all the language nodes L
j
? V
L
get
connected to k
j
consonant nodes (refer to Figure.
6 of (Choudhury et al, 2006) for an illustration of
the steps of the synthesis process). Thus, we have
604
the synthesized version of PlaNet, which we shall
call PlaNet
syn
henceforth.
The Simulation Results: We simulate the
above model to obtain PlaNet
syn
for 100 differ-
ent runs and average the results over all of them.
We find that the degree distributions that emerge
fit the empirical data well for ? ? [1.4,1.5] and
? ? [0.4,0.6], the best being at ? = 1.44 and ? = 0.5
(shown in Figure 2). In fact, the mean error4 be-
tween the real and the synthesized distributions for
the best choice of parameters is as small as 0.01.
Note that this error in case of the model presented
in (Choudhury et al, 2006) was 0.03. Furthermore,
as we shall see shortly, a super-linear kernel can
explain various other topological properties more
accurately than a linear kernel.
In absence of preferential attachment i.e., when
all the connections to the consonant nodes are
equiprobable, the mean error rises to 0.35.
A possible reason behind the success of this
model is the fact that language is a constantly
changing system and preferential attachment plays
a significant role in this change. For instance, dur-
ing the change those consonants that belong to lan-
guages that are more prevalent among the speak-
ers of a generation have higher chances of being
transmitted to the speakers of the subsequent gen-
erations (Blevins, 2004). This heterogeneity in the
choice of the consonants manifests itself as pref-
erential attachment. We conjecture that the value
of ? is a function of the societal structure and the
cognitive capabilities of human beings. The exact
nature of this function is currently not known and
a topic for future research. The parameter ? in this
case may be thought of as modeling the random-
ness of the system.
Nevertheless, the degree distribution of
PhoNet
syn
, which is the one-mode projection
of PlaNet
syn
, does not match the real data well
(see Figure 2). The mean error between the two
distributions is 0.45. Furthermore, the clustering
coefficient of PhoNet
syn
is 0.55 and differs largely
from that of PhoNet. The primary reason for this
deviation in the results is that PhoNet exhibits
strong patterns of co-occurrences (Mukherjee et
al.2007a) and this fact is not taken into account
by Model I. In order to circumvent the above
4Mean error is defined as the average difference between
the ordinate pairs (say y and y? ) where the abscissas are equal.
In other words, if there are N such ordinate pairs then the
mean error can be expressed as
?
|y?y
?
|
N
.
problem, we introduce the concept of triad (i.e.,
fully connected triplet) formation and thereby
refine the model in the following section.
3.2 Model II: Kernel based on Triad
Formation
The triad model (Peltoma?ki and Alava, 2006)
builds up on the concept of neighborhood forma-
tion. Two consonant nodes C
1
and C
2
become
neighbors if a language node at any step of the
synthesis process attaches itself to both C
1
and
C
2
. Let the probability of triad formation be de-
noted by p
t
. At each time step a language node
L
j
(chosen from the set of language nodes sorted
in ascending order of their degrees) makes the first
connection preferentially to a consonant node C
i
? V
C
to which L
j
is not already connected fol-
lowing the distribution Pr(C
i
). For the rest of the
(k
j
-1) connections L
j
attaches itself preferentially
to only the neighbors of C
i
to which L
j
is not yet
connected with a probability p
t
. Consequently, L
j
connects itself preferentially to the non-neighbors
of C
i
to which L
j
is not yet connected with a prob-
ability (1 ? p
t
). The neighbor set of C
i
gets up-
dated accordingly. Note that every time the node
C
i
and its neighbors are chosen they together im-
pose a clique on the one-mode projection. This
phenomenon leads to the formation of a large num-
ber of triangles in the one-mode projection thereby
increasing the clustering coefficient of the resultant
network.
The Simulation Results: We carry out 100 dif-
ferent simulation runs of the above model for a par-
ticular set of parameter values to obtain PlaNet
syn
and average the results over all of them. We ex-
plore several parameter settings in the range as fol-
lows: ? ? [1,1.5] (in steps of 0.1), ? ? [0.2,0.4]
(in steps of 0.1) and p
t
? [0.70,0.95] (in steps of
0.05). We also observe that if we traverse any fur-
ther along one or more of the dimensions of the pa-
rameter space then the results get worse. The best
result emerges for ? = 1.3, ? = 0.3 and p
t
= 0.8.
Figure 2 shows the degree distribution of the
consonant nodes of PlaNet
syn
and PlaNet. The
mean error between the two distributions is 0.04
approximately and is therefore worse than the re-
sult obtained from Model I. Nevertheless, the aver-
age clustering coefficient of PhoNet
syn
in this case
is 0.85, which is within 4.5% of that of PhoNet.
Moreover, in this process the mean error between
the degree distribution of PhoNet
syn
and PhoNet
605
(as illustrated in Figure 2) has got reduced drasti-
cally from 0.45 to 0.03.
One can again find a possible association of this
model with the phenomena of language change. If
a group of consonants largely co-occur in the lan-
guages of a generation of speakers then it is very
likely that all of them get transmitted together in
the subsequent generations (Blevins, 2004). The
triad formation probability ensures that if a pair of
consonant nodes become neighbors of each other
in a particular step of the synthesis process then
the choice of such a pair should be highly pre-
ferred in the subsequent steps of the process. This
is coherent with the aforementioned phenomenon
of transmission of consonants in groups over lin-
guistic generations. Since the value of p
t
that we
obtain is quite high, it may be argued that such
transmissions are largely prevalent in nature.
Although Model II reproduces the structural
properties of PlaNet and PhoNet quite accurately,
as we shall see shortly, it fails to generate inven-
tories that closely match the real ones in terms
of feature entropy. However, at this point, recall
that Model II assumes that the consonant nodes are
unlabeled; therefore, the inventories that are pro-
duced as a result of the synthesis are composed of
consonants, which unlike the real inventories, are
not marked by their distinctive features. In order
to label them we perform the following,
The Labeling Scheme:
1. Sort the consonants of UPSID in the decreasing
order of their frequency of occurrence and call this
list of consonants ListC[1 ? ? ? 541],
2. Sort the V
C
nodes of PlaNet
syn
in decreasing
order of their degree and call this list of nodes
ListN [1 ? ? ? 541],
3. ?
1?i?541
ListN [i] ?? ListC[i]
The Figure 3 indicates that the curve for the real
inventories (R) and those obtained from Model II
(M2) are significantly different from each other.
This difference arises due to the fact that in Model
II, the choice of a consonant from the set of neigh-
bors is solely degree-dependent, where the rela-
tionships between the features are not taken into
consideration. Therefore, in order to eliminate this
problem, we introduce the model using the feature-
based kernel in the next section.
3.3 Model III: Feature-based Kernel
In this model, we assume that each of the conso-
nant nodes are labeled, that is each of them are
Figure 3: Average feature entropy of the invento-
ries of a particular size (y-axis) versus the inven-
tory size (x-axis).
marked by a set of distinctive features. The attach-
ment kernel in this case has two components one
of which is preferential while the other favors the
choice of those consonants that are at a low fea-
ture distance (the number of feature positions they
differ at) from the already chosen ones. Let us de-
note the feature distance between two consonants
C
i
and C ?
i
by D(C
i
, C
?
i
). We define the affinity,
A(C
i
, C
?
i
), between C
i
and C ?
i
as
A(C
i
, C
?
i
) =
1
D(C
i
, C
?
i
)
(5)
Therefore, the lower the feature distance between
C
i
and C ?
i
the higher is the affinity between them.
At each time step a language node estab-
lishes the first connection with a consonant node
(say C
i
) preferentially following the distribution
Pr(C
i
) like the previous models. The rest of
the connections to any arbitrary consonant node
C
?
i
(not yet connected to the language node) are
made following the distribution (1?w)Pr(C ?
i
) +
wPr
aff
(C
i
, C
?
i
), where
Pr
aff
(C
i
, C
?
i
) =
A(C
i
, C
?
i
)
?
?C
?
i
A(C
i
, C
?
i
)
(6)
and 0 < w < 1.
Simulation Results: We perform 100 different
simulation runs of the above model for a particular
set of parameter values to obtain PlaNet
syn
and av-
erage the results over all of them. We explore dif-
ferent parameter settings in the range as follows:
? ? [1,2] (in steps of 0.1), ? ? [0.1,1] (in steps
of 0.1) and w ? [0.1,0.5] (in steps of 0.05). The
606
best result in terms of the structural properties of
PlaNet and PhoNet emerges for ? = 1.6, ? = 0.3
and w = 0.2.
In this case, the mean error between the de-
gree distribution curves for PlaNet
syn
and PlaNet
is 0.05 and that between of PhoNet
syn
and PhoNet
is 0.02. Furthermore, the clustering coefficient of
PhoNet
syn
in this case is 0.84, which is within
5.6% of that of PhoNet. The above results show
that the structural properties of the synthesized
networks in this case are quite similar to those
obtained through the triad model. Nevertheless,
the average feature entropy of the inventories pro-
duced (see curve M3 in Figure 3) are more close to
that of the real ones now (for quantitative compar-
ison see Table 2).
Therefore, it turns out that the groups of con-
sonants that largely co-occur in the languages
of a linguistic generation are actually driven by
the principle of feature economy (see (Clements,
2008; Mukherjee et al2007a) for details).
However, note that even for Model III the nodes
that are chosen for attachment in the initial stages
of the synthesis process are arbitrary and conse-
quently, the labels of the nodes of PlaNet
syn
do
not have a one-to-one correspondence with that of
PlaNet, which is the main reason behind the differ-
ence in the result between them. In order to over-
come this problem we can make use of a small set
of real inventories to bootstrap the model.
3.4 Model IV: Feature-based Kernel and
Bootstrapping
In order to create a bias towards the labeling
scheme prevalent in PlaNet, we use 30 (around
10% of the) real languages as a seed (chosen ran-
domly) for Model III; i.e., they are used by the
model for bootstrapping. The idea is summarized
below.
1. Select 30 real inventories at random and con-
struct a PlaNet from them. Call this network the
initial PlaNet
syn
.
2. The rest of the language nodes are incrementally
added to this initial PlaNet
syn
using Model III.
Simulation Results: The best fit now emerges
at ? = 1.35, ? = 0.3 and w = 0.15. The mean er-
ror between the degree distribution of PlaNet and
PlaNet
syn
is 0.05 and that between PhoNet and
PhoNet
syn
is 0.02. The clustering coefficient of
PhoNet
syn
is 0.83 in this case (within 6.7% of that
of PhoNet).
Results Model I Model II Model III Model IV
ME: DD of PlaNet & PlaNet
syn
0.01 0.04 0.05 0.05
ME: DD of PhoNet & PhoNet
syn
0.45 0.03 0.02 0.02
% Err: Clustering Coefficient 38.2 04.5 05.6 06.7
ME: Avg. F
E
of Real & Synth. Inv. 3.40 3.00 2.10 0.93
? 1.44 1.30 1.60 1.35
? 0.5 0.3 0.3 0.3
p
t
? 0.8 ? ?
w ? ? 0.20 0.15
Table 2: Important results obtained from each of
the models. ME: Mean Error, DD: Degree Distri-
bution.
The inventories that are produced as a result of
the bootstrapping have an average feature entropy
closer to the real inventories (see curve M4 in Fig-
ure 3) than the earlier models. Hence, we find
that this improved labeling strategy brings about
a global betterment in our results unlike in the pre-
vious cases. The larger the number of languages
used for the purpose of bootstrapping the better are
the results mainly in terms of the match in the fea-
ture entropy curves.
4 Conclusion
We dedicated the preceding sections of this article
to analyze and synthesize the consonant invento-
ries of the world?s languages in the framework of a
complex network. Table 2 summarizes the results
obtained from the four models so that the reader
can easily compare them. Some of our important
observations are
? The distribution of occurrence and co-occurrence
of consonants across languages roughly follow a
power law,
? The co-occurrence network of consonants has a
large clustering coefficient,
? Groups of consonants that largely co-occur
across languages are driven by feature economy
(which can be expressed through feature entropy),
? Each of the above properties emerges due to dif-
ferent reasons, which are successively unfurled by
our models.
So far, we have tried to explain the physical sig-
nificance of our models in terms of the process
of language change. Language change is a col-
lective phenomenon that functions at the level of
a population of speakers (Steels, 2000). Never-
theless, it is also possible to explain the signif-
icance of the models at the level of an individ-
ual, primarily in terms of the process of language
acquisition, which largely governs the course of
language change. In the initial years of language
development every child passes through a stage
607
called babbling during which he/she learns to pro-
duce non-meaningful sequences of consonants and
vowels, some of which are not even used in the
language to which they are exposed (Jakobson,
1968; Locke, 1983). Clear preferences can be
observed for learning certain sounds such as plo-
sives and nasals, whereas fricatives and liquids are
avoided. In fact, this hierarchy of preference dur-
ing the babbling stage follows the cross-linguistic
frequency distribution of the consonants. This in-
nate frequency dependent preference towards cer-
tain phonemes might be because of phonetic rea-
sons (i.e., for articulatory/perceptual benefits). In
all our models, this innate preference gets cap-
tured through the process of preferential attach-
ment. However, at the same time, in the context of
learning a particular inventory the ease of learning
the individual consonants also plays an important
role. The lower the number of new feature distinc-
tions to be learnt, the higher the ease of learning
the consonant. Therefore, there are two orthogonal
preferences: (a) the occurrence frequency depen-
dent preference (that is innate), and (b) the feature-
dependent preference (that increases the ease of
learning), which are instrumental in the acquisi-
tion of the inventories. The feature-based kernel is
essentially a linear combination of these two mu-
tually orthogonal factors.
References
A.-L. Baraba?si and R. Albert. 1999. Emergence of
scaling in random networks. Science 286, 509-512.
A. Barrat, M. Barthe?lemy, R. Pastor-Satorras and A.
Vespignani. 2004. The architecture of complex
weighted networks. PNAS 101, 3747?3752.
J. Blevins. 2004. Evolutionary Phonology: The
Emergence of Sound Patterns, Cambridge University
Press, Cambridge.
B. de Boer. 2000. Self-organisation in vowel systems.
Journal of Phonetics 28(4), 441?465.
P. Boersma. 1998. Functional Phonology, The Hague:
Holland Academic Graphics.
M. Choudhury, A. Mukherjee, A. Basu and N. Ganguly.
2006. Analysis and synthesis of the distribution of
consonants over languages: A complex network ap-
proach. Proceedings of COLING-ACL06, 128?135.
G. N. Clements. 2008. The role of features in speech
sound inventories. In Eric Raimy & Charles Cairns,
eds.,Contemporary Views on Architecture and Rep-
resentations in Phonological Theory, Cambridge,
MA: MIT Press.
F. Hinskens and J. Weijer. 2003. Patterns of segmen-
tal modification in consonant inventories: A cross-
linguistic study. Linguistics 41(6), 1041?1084.
R. Jakobson. 1968. Child Language, Aphasia and
Phonological Universals. The Hague: Mouton.
H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai and A.
L. Baraba?si. 2000. The large-scale organization of
metabolic networks. Nature 406 651-654.
J. Liljencrants and B. Lindblom. 1972. Numerical sim-
ulation of vowel quality systems: the role of percep-
tual contrast. Language 48, 839?862.
B. Lindblom and I. Maddieson. 1988. Phonetic univer-
sals in consonant systems. Language, Speech, and
Mind, 62?78, Routledge, London.
J. L. Locke. 1983. Phonological Acquisition and
Change. Academic Press New York.
I. Maddieson. 1984. Patterns of Sounds, Cambridge
University Press, Cambridge.
A. Mukherjee, M. Choudhury, A. Basu and N. Gan-
guly. 2007a. Modeling the co-occurrence principles
of the consonant inventories: A complex network ap-
proach. Int. Jour. of Mod. Phys. C 18(2), 281?295.
A. Mukherjee, M. Choudhury, A. Basu and N. Ganguly.
2007b. Redundancy ratio: An invariant property of
the consonant inventories of the world?s languages
Proceedings of ACL07, 104?111.
M. E. J. Newman. 2001. Scientific collaboration net-
works. Physical Review E 64, 016131.
M. E. J. Newman. 2003. The structure and function of
complex networks. SIAM Review 45, 167?256.
M. Peltoma?ki and M. Alava. 2006. Correlations in bi-
partite collaboration networks. Journal of Statistical
Mechanics: Theory and Experiment, P01010.
J. J. Ramasco, S. N. Dorogovtsev and R. Pastor-
Satorras. 2004. Self-organization of collaboration
networks. Physical Review E 70, 036106.
C. E. Shannon and W. Weaver. 1949. The Mathe-
matical Theory of Information. University of Illinois
Press, Urbana.
L. Steels. 2000. Language as a complex adaptive
system. In: Schoenauer, M., editor, Proceedings of
PPSN VI, LNCS, 17?26.
N. Trubetzkoy. 1931. Die phonologischen systeme.
TCLP 4, 96?116.
N. Trubetzkoy. 1969. Principles of Phonology. Eng-
lish translation of Grundzu?ge der Phonologie, 1939,
University of California Press, Berkeley.
608
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 585?593,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Discovering Global Patterns in Linguistic Networks through
Spectral Analysis: A Case Study of the Consonant Inventories
Animesh Mukherjee?
Indian Institute of Technology, Kharagpur
animeshm@cse.iitkgp.ernet.in
Monojit Choudhury and Ravi Kannan
Microsoft Research India
{monojitc,kannan}@microsoft.com
Abstract
Recent research has shown that language
and the socio-cognitive phenomena asso-
ciated with it can be aptly modeled and
visualized through networks of linguistic
entities. However, most of the existing
works on linguistic networks focus only
on the local properties of the networks.
This study is an attempt to analyze the
structure of languages via a purely struc-
tural technique, namely spectral analysis,
which is ideally suited for discovering the
global correlations in a network. Appli-
cation of this technique to PhoNet, the
co-occurrence network of consonants, not
only reveals several natural linguistic prin-
ciples governing the structure of the con-
sonant inventories, but is also able to quan-
tify their relative importance. We believe
that this powerful technique can be suc-
cessfully applied, in general, to study the
structure of natural languages.
1 Introduction
Language and the associated socio-cognitive phe-
nomena can be modeled as networks, where the
nodes correspond to linguistic entities and the
edges denote the pairwise interaction or relation-
ship between these entities. The study of lin-
guistic networks has been quite popular in the re-
cent times and has provided us with several in-
teresting insights into the nature of language (see
Choudhury and Mukherjee (to appear) for an ex-
tensive survey). Examples include study of the
WordNet (Sigman and Cecchi, 2002), syntactic
dependency network of words (Ferrer-i-Cancho,
2005) and network of co-occurrence of conso-
nants in sound inventories (Mukherjee et al, 2008;
Mukherjee et al, 2007).
?This research has been conducted during the author?s in-
ternship at Microsoft Research India.
Most of the existing studies on linguistic net-
works, however, focus only on the local structural
properties such as the degree and clustering coef-
ficient of the nodes, and shortest paths between
pairs of nodes. On the other hand, although it is
a well known fact that the spectrum of a network
can provide important information about its global
structure, the use of this powerful mathematical
machinery to infer global patterns in linguistic net-
works is rarely found in the literature. Note that
spectral analysis, however, has been successfully
employed in the domains of biological and social
networks (Farkas et al, 2001; Gkantsidis et al,
2003; Banerjee and Jost, 2007). In the context of
linguistic networks, (Belkin and Goldsmith, 2002)
is the only work we are aware of that analyzes the
eigenvectors to obtain a two dimensional visualize
of the network. Nevertheless, the work does not
study the spectrum of the graph.
The aim of the present work is to demonstrate
the use of spectral analysis for discovering the
global patterns in linguistic networks. These pat-
terns, in turn, are then interpreted in the light of ex-
isting linguistic theories to gather deeper insights
into the nature of the underlying linguistic phe-
nomena. We apply this rather generic technique
to find the principles that are responsible for shap-
ing the consonant inventories, which is a well re-
searched problem in phonology since 1931 (Tru-
betzkoy, 1931; Lindblom and Maddieson, 1988;
Boersma, 1998; Clements, 2008). The analysis
is carried out on a network defined in (Mukherjee
et al, 2007), where the consonants are the nodes
and there is an edge between two nodes u and v
if the consonants corresponding to them co-occur
in a language. The number of times they co-occur
across languages define the weight of the edge. We
explain the results obtained from the spectral anal-
ysis of the network post-facto using three linguis-
tic principles. The method also automatically re-
veals the quantitative importance of each of these
585
principles.
It is worth mentioning here that earlier re-
searchers have also noted the importance of the
aforementioned principles. However, what was
not known was how much importance one should
associate with each of these principles. We also
note that the technique of spectral analysis neither
explicitly nor implicitly assumes that these princi-
ples exist or are important, but deduces them auto-
matically. Thus, we believe that spectral analysis
is a promising approach that is well suited to the
discovery of linguistic principles underlying a set
of observations represented as a network of enti-
ties. The fact that the principles ?discovered? in
this study are already well established results adds
to the credibility of the method. Spectral analysis
of large linguistic networks in the future can possi-
bly reveal hitherto unknown universal principles.
The rest of the paper is organized as follows.
Sec. 2 introduces the technique of spectral anal-
ysis of networks and illustrates some of its ap-
plications. The problem of consonant inventories
and how it can be modeled and studied within the
framework of linguistic networks are described in
Sec. 3. Sec. 4 presents the spectral analysis of
the consonant co-occurrence network, the obser-
vations and interpretations. Sec. 5 concludes by
summarizing the work and the contributions and
listing out future research directions.
2 A Primer to Spectral Analysis
Spectral analysis1 is a powerful tool capable of
revealing the global structural patterns underly-
ing an enormous and complicated environment
of interacting entities. Essentially, it refers to
the systematic study of the eigenvalues and the
eigenvectors of the adjacency matrix of the net-
work of these interacting entities. Here we shall
briefly review the basic concepts involved in spec-
tral analysis and describe some of its applications
(see (Chung, 1994; Kannan and Vempala, 2008)
for details).
A network or a graph consisting of n nodes (la-
beled as 1 through n) can be represented by a n?n
square matrix A, where the entry aij represents the
weight of the edge from node i to node j. A, which
is known as the adjacency matrix, is symmetric for
an undirected graph and have binary entries for an
1The term spectral analysis is also used in the context of
signal processing, where it refers to the study of the frequency
spectrum of a signal.
unweighted graph. ? is an eigenvalue of A if there
is an n-dimensional vector x such that
Ax = ?x
Any real symmetric matrix A has n (possibly non-
distinct) eigenvalues ?0 ? ?1 ? . . . ? ?n?1, and
corresponding n eigenvectors that are mutually or-
thogonal. The spectrum of a graph is the set of the
distinct eigenvalues of the graph and their corre-
sponding multiplicities. It is usually represented
as a plot with the eigenvalues in x-axis and their
multiplicities plotted in the y-axis.
The spectrum of real and random graphs dis-
play several interesting properties. Banerjee and
Jost (2007) report the spectrum of several biologi-
cal networks that are significantly different from
the spectrum of artificially generated graphs2.
Spectral analysis is also closely related to Prin-
cipal Component Analysis and Multidimensional
Scaling. If the first few (say d) eigenvalues of a
matrix are much higher than the rest of the eigen-
values, then it can be concluded that the rows of
the matrix can be approximately represented as
linear combinations of d orthogonal vectors. This
further implies that the corresponding graph has
a few motifs (subgraphs) that are repeated a large
number of time to obtain the global structure of
the graph (Banerjee and Jost, to appear).
Spectral properties are representative of an n-
dimensional average behavior of the underlying
system, thereby providing considerable insight
into its global organization. For example, the prin-
cipal eigenvector (i.e., the eigenvector correspond-
ing to the largest eigenvalue) is the direction in
which the sum of the square of the projections
of the row vectors of the matrix is maximum. In
fact, the principal eigenvector of a graph is used to
compute the centrality of the nodes, which is also
known as PageRank in the context of WWW. Sim-
ilarly, the second eigen vector component is used
for graph clustering.
In the next two sections we describe how spec-
tral analysis can be applied to discover the orga-
nizing principles underneath the structure of con-
sonant inventories.
2Banerjee and Jost (2007) report the spectrum of the
graph?s Laplacian matrix rather than the adjacency matrix.
It is increasingly popular these days to analyze the spectral
properties of the graph?s Laplacian matrix. However, for rea-
sons explained later, here we will be conduct spectral analysis
of the adjacency matrix rather than its Laplacian.
586
Figure 1: Illustration of the nodes and edges of PlaNet and PhoNet alng with their respective adjacency
matrix representations.
3 Consonant Co-occurrence Network
The most basic unit of human languages are the
speech sounds. The repertoire of sounds that make
up the sound inventory of a language are not cho-
sen arbitrarily even though the speakers are ca-
pable of producing and perceiving a plethora of
them. In contrast, these inventories show excep-
tionally regular patterns across the languages of
the world, which is in fact, a common point of
consensus in phonology. Right from the begin-
ning of the 20th century, there have been a large
number of linguistically motivated attempts (Tru-
betzkoy, 1969; Lindblom and Maddieson, 1988;
Boersma, 1998; Clements, 2008) to explain the
formation of these patterns across the consonant
inventories. More recently, Mukherjee and his col-
leagues (Choudhury et al, 2006; Mukherjee et al,
2007; Mukherjee et al, 2008) studied this problem
in the framework of complex networks. Since here
we shall conduct a spectral analysis of the network
defined in Mukherjee et al (2007), we briefly sur-
vey the models and the important results of their
work.
Choudhury et al (2006) introduced a bipartite
network model for the consonant inventories. For-
mally, a set of consonant inventories is represented
as a graph G = ?VL, VC , Elc?, where the nodes in
one partition correspond to the languages (VL) and
that in the other partition correspond to the conso-
nants (VC). There is an edge (vl, vc) between a
language node vl ? VL (representing the language
l) and a consonant node vc ? VC (representing the
consonant c) iff the consonant c is present in the
inventory of the language l. This network is called
the Phoneme-Language Network or PlaNet and
represent the connections between the language
and the consonant nodes through a 0-1 matrix A
as shown by a hypothetical example in Fig. 1. Fur-
ther, in (Mukherjee et al, 2007), the authors define
the Phoneme-Phoneme Network or PhoNet as the
one-mode projection of PlaNet onto the consonant
nodes, i.e., a network G = ?VC , Ecc? ?, where the
nodes are the consonants and two nodes vc and
vc? are linked by an edge with weight equal to the
number of languages in which both c and c? occur
together. In other words, PhoNet can be expressed
as a matrixB (see Fig. 1) such thatB = AAT?D
where D is a diagonal matrix with its entries cor-
responding to the frequency of occurrence of the
consonants. Similarly, we can also construct the
one-mode projection of PlaNet onto the language
nodes (which we shall refer to as the Language-
Language Graph or LangGraph) can be expressed
as B? = ATA ?D?, where D? is a diagonal ma-
trix with its entries corresponding to the size of the
consonant inventories for each language.
The matrix A and hence, B and B? have been
constructed from the UCLA Phonological Seg-
ment Inventory Database (UPSID) (Maddieson,
1984) that hosts the consonant inventories of 317
languages with a total of 541 consonants found
across them. Note that, UPSID uses articulatory
587
features to describe the consonants and assumes
these features to be binary-valued, which in turn
implies that every consonant can be represented
by a binary vector. Later on, we shall use this rep-
resentation for our experiments.
By construction, we have |VL| = 317, |VC | =
541, |Elc| = 7022, and |Ecc? | = 30412. Conse-
quently, the order of the matrix A is 541 ? 317
and that of the matrix B? is 541 ? 541. It has been
found that the degree distribution of both PlaNet
and PhoNet roughly indicate a power-law behavior
with exponential cut-offs towards the tail (Choud-
hury et al, 2006; Mukherjee et al, 2007). Further-
more, PhoNet is also characterized by a very high
clustering coefficient. The topological properties
of the two networks and the generative model
explaining the emergence of these properties are
summarized in (Mukherjee et al, 2008). However,
all the above properties are useful in characteriz-
ing the local patterns of the network and provide
very little insight about its global structure.
4 Spectral Analysis of PhoNet
In this section we describe the procedure and re-
sults of the spectral analysis of PhoNet. We begin
with computation of the spectrum of PhoNet. Af-
ter the analysis of the spectrum, we systematically
investigate the top few eigenvectors of PhoNet
and attempt to characterize their linguistic signif-
icance. In the process, we also analyze the corre-
sponding eigenvectors of LanGraph that helps us
in characterizing the properties of languages.
4.1 Spectrum of PhoNet
Using a simple Matlab script we compute the
spectrum (i.e., the list of eignevalues along with
their multiplicities) of the matrix B correspond-
ing to PhoNet. Fig. 2(a) shows the spectral plot,
which has been obtained through binning3 with a
fixed bin size of 20. In order to have a better visu-
alization of the spectrum, in Figs. 2(b) and (c) we
further plot the top 50 (absolute) eigenvalues from
the two ends of the spectrum versus the index rep-
resenting their sorted order in doubly-logarithmic
scale. Some of the important observations that one
can make from these results are as follows.
First, the major bulk of the eigenvalues are con-
centrated at around 0. This indicates that though
3Binning is the process of dividing the entire range of a
variable into smaller intervals and counting the number of
observations within each bin or interval. In fixed binning, all
the intervals are of the same size.
the order of B is 541 ? 541, its numerical rank is
quite low. Second, there are at least a few very
large eigenvalues that dominate the entire spec-
trum. In fact, 89% of the spectrum, or the square
of the Frobenius norm, is occupied by the princi-
pal (i.e., the topmost) eigenvalue, 92% is occupied
by the first and the second eigenvalues taken to-
gether, while 93% is occupied by the first three
taken together. The individual contribution of the
other eigenvalues to the spectrum is significantly
lower than that of the top three. Third, the eigen-
values on either ends of the spectrum tend to decay
gradually, mostly indicating a power-law behavior.
The power-law exponents at the positive and the
negative ends are -1.33 (the R2 value of the fit is
0.98) and -0.88 (R2 ? 0.92) respectively.
The numerically low rank of PhoNet suggests
that there are certain prototypical structures that
frequently repeat themselves across the consonant
inventories, thereby, increasing the number of 0
eigenvalues to a large extent. In other words, all
the rows of the matrix B (i.e., the inventories) can
be expressed as the linear combination of a few
independent row vectors, also known as factors.
Furthermore, the fact that the principal eigen-
value constitutes 89% of the Frobenius norm of the
spectrum implies that there exist one very strong
organizing principle which should be able to ex-
plain the basic structure of the inventories to a very
good extent. Since the second and third eigen-
values are also significantly larger than the rest
of the eigenvalues, one should expect two other
organizing principles, which along with the basic
principle, should be able to explain, (almost) com-
pletely, the structure of the inventories. In order
to ?discover? these principles, we now focus our
attention to the first three eigenvectors of PhoNet.
4.2 The First Eigenvector of PhoNet
Fig. 2(d) shows the first eigenvector component
for each consonant node versus its frequency of
occurrence across the language inventories (i.e., its
degree in PlaNet). The figure clearly indicates that
the two are highly correlated (r = 0.99), which in
turn means that 89% of the spectrum and hence,
the organization of the consonant inventories, can
be explained to a large extent by the occurrence
frequency of the consonants. The question arises:
Does this tell us something special about the struc-
ture of PhoNet or is it always the case for any sym-
metric matrix that the principal eigenvector will
588
Figure 2: Eigenvalues and eigenvectors of B. (a) Binned distribution of the eigenvalues (bin size = 20)
versus their multiplicities. (b) the top 50 (absolute) eigenvalues from the positive end of the spectrum and
their ranks. (c) Same as (b) for the negative end of the spectrum. (d), (e) and (f) respectively represents
the first, second and the third eigenvector components versus the occurrence frequency of the consonants.
be highly correlated with the frequency? We as-
sert that the former is true, and indeed, the high
correlation between the principal eigenvector and
the frequency indicates high ?proportionate co-
occurrence? - a term which we will explain.
To see this, consider the following 2n? 2n ma-
trix X
X =
?
???????
0 M1 0 0 0 . . .
M1 0 0 0 0 . . .
0 0 0 M2 0 . . .
0 0 M2 0 0 . . .
... ... ... ... ... . . .
?
???????
where Xi,i+1 = Xi+1,i = M(i+1)/2 for all odd
i and 0 elsewhere. Also, M1 > M2 > . . . >
Mn ? 1. Essentially, this matrix represents a
graph which is a collection of n disconnected
edges, each having weights M1, M2, and so on.
It is easy to see that the principal eigenvector of
this matrix is (1/?2, 1/?2, 0, 0, . . . , 0)>, which
of course is very different from the frequency vec-
tor: (M1,M1,M2,M2, . . . ,Mn,Mn)>.
At the other extreme, consider an n ? n ma-
trix X with Xi,j = Cfifj for some vector f =
(f1, f2, . . . fn)> that represents the frequency of
the nodes and a normalization constant C. This is
what we refer to as ?proportionate co-occurrence?
because the extent of co-occurrence between the
nodes i and j (which is Xi,j or the weight of the
edge between i and j) is exactly proportionate to
the frequencies of the two nodes. The principal
eigenvector in this case is f itself, and thus, corre-
lates perfectly with the frequencies. Unlike this
hypothetical matrix X, PhoNet has all 0 entries
in the diagonal. Nevertheless, this perturbation,
which is equivalent to subtracting f2i from the ith
diagonal, seems to be sufficiently small to preserve
the ?proportionate co-occurrence? behavior of the
adjacency matrix thereby resulting into a high cor-
relation between the principal eigenvector compo-
nent and the frequencies.
On the other hand, to construct the Lapla-
cian matrix, we would have subtracted fi
?n
j=1 fj
from the ith diagonal entry, which is a much
larger quantity than f2i . In fact, this operation
would have completely destroyed the correlation
between the frequency and the principal eigen-
vector component because the eigenvector corre-
sponding to the smallest4 eigenvalue of the Lapla-
cian matrix is [1, 1, . . . , 1]>.
Since the first eigenvector of B is perfectly cor-
4The role played by the top eigenvalues and eigenvectors
in the spectral analysis of the adjacency matrix is compara-
ble to that of the smallest eigenvalues and the corresponding
eigenvectors of the Laplacian matrix (Chung, 1994)
589
related with the frequency of occurrence of the
consonants across languages it is reasonable to
argue that there is a universally observed innate
preference towards certain consonants. This pref-
erence is often described through the linguistic
concept of markedness, which in the context of
phonology tells us that the substantive conditions
that underlie the human capacity of speech pro-
duction and perception renders certain consonants
more favorable to be included in the inventory than
some other consonants (Clements, 2008). We ob-
serve that markedness plays a very important role
in shaping the global structure of the consonant in-
ventories. In fact, if we arrange the consonants in a
non-increasing order of the first eigenvector com-
ponents (which is equivalent to increasing order
of statistical markedness), and compare the set of
consonants present in an inventory of size s with
that of the first s entries from this hierarchy, we
find that the two are, on an average, more than
50% similar. This figure is surprisingly high be-
cause, in spite of the fact that ?s s ? 5412 , on an
average s2 consonants in an inventory are drawn
from the first s entries of the markedness hierarchy
(a small set), whereas the rest s2 are drawn from the
remaining (541? s) entries (a much larger set).
The high degree of proportionate co-occurrence
in PhoNet implied by this high correlation be-
tween the principal eigenvector and frequency fur-
ther indicates that the innate preference towards
certain phonemes is independent of the presence
of other phonemes in the inventory of a language.
4.3 The Second Eigenvector of PhoNet
Fig. 2(e) shows the second eigenvector component
for each node versus their occurrence frequency. It
is evident from the figure that the consonants have
been clustered into three groups. Those that have
a very low or a very high frequency club around 0
whereas, the medium frequency zone has clearly
split into two parts. In order to investigate the ba-
sis for this split we carry out the following experi-
ment.
Experiment I
(i) Remove all consonants whose frequency of oc-
currence across the inventories is very low (< 5).
(ii) Denote the absolute maximum value of the
positive component of the second eigenvector as
MAX+ and the absolute maximum value of the
negative component as MAX?. If the absolute
value of a positive component is less than 15% of
MAX+ then assign a neutral class to the corre-
sponding consonant; else assign it a positive class.
Denote the set of consonants in the positive class
by C+. Similarly, if the absolute value of a nega-
tive component is less than 15% of MAX? then
assign a neutral class to the corresponding conso-
nant; else assign it a negative class. Denote the set
of consonants in the negative class by C?.
(iii) Using the above training set of the classified
consonants (represented as boolean feature vec-
tors) learn a decision tree (C4.5 algorithm (Quin-
lan, 1993)) to determine the features that are re-
sponsible for the split of the medium frequency
zone into the negative and the positive classes.
Fig. 3(a) shows the decision rules learnt from
the above training set. It is clear from these rules
that the split into C? and C+ has taken place
mainly based on whether the consonants have
the combined ?dental alveolar? feature (negative
class) or the ?dental? and the ?alveolar? features
separately (positive class). Such a combined fea-
ture is often termed ambiguous and its presence in
a particular consonant c of a language l indicates
that the speakers of l are unable to make a distinc-
tion as to whether c is articulated with the tongue
against the upper teeth or the alveolar ridge. In
contrast, if the features are present separately then
the speakers are capable of making this distinc-
tion. In fact, through the following experiment,
we find that the consonant inventories of almost
all the languages in UPSID get classified based on
whether they preserve this distinction or not.
Experiment II
(i) Construct B? = ATA ? D? (i.e., the adjacency
matrix of LangGraph).
(ii) Compute the second eigenvector of B?. Once
again, the positive and the negative components
split the languages into two distinct groups L+ and
L? respectively.
(iii) For each language l ? L+ count the num-
ber of consonants in C+ that occur in l. Sum up
the counts for all the languages in L+ and nor-
malize this sum by |L+||C+|. Similarly, perform
the same step for the pairs (L+,C?), (L?,C+) and
(L?,C?).
From the above experiment, the values obtained
for the pairs (i) (L+,C+), (L+,C?) are 0.35, 0.08
respectively, and (ii) (L?,C+), (L?,C?) are 0.07,
0.32 respectively. This immediately implies that
almost all the languages in L+ preserve the den-
tal/alveolar distinction while those in L? do not.
590
Figure 3: Decision rules obtained from the study of (a) the second, and (b) the third eigenvectors. The
classification errors for both (a) and (b) are less than 15%.
4.4 The Third Eigenvector of PhoNet
We next investigate the relationship between the
third eigenvector components of B and the occur-
rence frequency of the consonants (Fig. 2(f)). The
consonants are once again found to get clustered
into three groups, though not as clearly as in the
previous case. Therefore, in order to determine the
basis of the split, we repeat experiments I and II.
Fig. 3(b) clearly indicates that in this case the con-
sonants in C+ lack the complex features that are
considered difficult for articulation. On the other
hand, the consonants in C? are mostly composed
of such complex features. The values obtained for
the pairs (i) (L+,C+), (L+,C?) are 0.34, 0.06 re-
spectively, and (ii) (L?,C+), (L?,C?) are 0.19,
0.18 respectively. This implies that while there is
a prevalence of the consonants from C+ in the lan-
guages of L+, the consonants from C? are almost
absent. However, there is an equal prevalence of
the consonants from C+ and C? in the languages
of L?. Therefore, it can be argued that the pres-
ence of the consonants from C? in a language can
(phonologically) imply the presence of the conso-
nants from C+, but not vice versa. We do not find
any such aforementioned pattern for the fourth and
the higher eigenvector components.
4.5 Control Experiment
As a control experiment we generated a set of ran-
dom inventories and carried out the experiments
I and II on the adjacency matrix, BR, of the ran-
dom version of PhoNet. We construct these in-
ventories as follows. Let the frequency of occur-
rence for each consonant c in UPSID be denoted
by fc. Let there be 317 bins each corresponding to
a language in UPSID. fc bins are then chosen uni-
formly at random and the consonant c is packed
into these bins. Thus the consonant inventories
of the 317 languages corresponding to the bins
are generated. Note that this method of inventory
construction leads to proportionate co-occurrence.
Consequently, the first eigenvector components of
BR are highly correlated to the occurrence fre-
quency of the consonants. However, the plots of
the second and the third eigenvector components
versus the occurrence frequency of the consonants
indicate absolutely no pattern thereby, resulting in
a large number of decision rules and very high
classification errors (upto 50%).
591
5 Discussion and Conclusion
Are there any linguistic inferences that can be
drawn from the results obtained through the
study of the spectral plot and the eigenvectors of
PhoNet? In fact, one can correlate several phono-
logical theories to the aforementioned observa-
tions, which have been construed by the past re-
searchers through very specific studies.
One of the most important problems in defin-
ing a feature-based classificatory system is to de-
cide when a sound in one language is different
from a similar sound in another language. Ac-
cording to Ladefoged (2005) ?two sounds in dif-
ferent languages should be considered as distinct
if we can point to a third language in which the
same two sounds distinguish words?. The den-
tal versus alveolar distinction that we find to be
highly instrumental in splitting the world?s lan-
guages into two different groups (i.e., L+ and L?
obtained from the analysis of the second eigen-
vectors of B and B?) also has a strong classifi-
catory basis. It may well be the case that cer-
tain categories of sounds like the dental and the
alveolar sibilants are not sufficiently distinct to
constitute a reliable linguistic contrast (see (Lade-
foged, 2005) for reference). Nevertheless, by al-
lowing the possibility for the dental versus alveo-
lar distinction, one does not increase the complex-
ity or introduce any redundancy in the classifica-
tory system. This is because, such a distinction
is prevalent in many other sounds, some of which
are (a) nasals in Tamil (Shanmugam, 1972) and
Malayalam (Shanmugam, 1972; Ladefoged and
Maddieson, 1996), (b) laterals in Albanian (Lade-
foged and Maddieson, 1996), and (c) stops in cer-
tain dialectal variations of Swahili (Hayward et al,
1989). Therefore, it is sensible to conclude that the
two distinct groups L+ and L? induced by our al-
gorithm are true representatives of two important
linguistic typologies.
The results obtained from the analysis of the
third eigenvectors of B and B? indicate that im-
plicational universals also play a crucial role in
determining linguistic typologies. The two ty-
pologies that are predominant in this case con-
sist of (a) languages using only those sounds that
have simple features (e.g., plosives), and (b) lan-
guages using sounds with complex features (e.g.,
lateral, ejectives, and fricatives) that automatically
imply the presence of the sounds having sim-
ple features. The distinction between the simple
and complex phonological features is a very com-
mon hypothesis underlying the implicational hier-
archy and the corresponding typological classifi-
cation (Clements, 2008). In this context, Locke
and Pearson (1992) remark that ?Infants heavily
favor stop consonants over fricatives, and there
are languages that have stops and no fricatives but
no languages that exemplify the reverse pattern.
[Such] ?phonologically universal? patterns, which
cut across languages and speakers are, in fact, the
phonetic properties of Homo sapiens.? (as quoted
in (Vallee et al, 2002)).
Therefore, it turns out that the methodology pre-
sented here essentially facilitates the induction of
linguistic typologies. Indeed, spectral analysis de-
rives, in a unified way, the importance of these
principles and at the same time quantifies their ap-
plicability in explaining the structural patterns ob-
served across the inventories. In this context, there
are at least two other novelties of this work. The
first novelty is in the systematic study of the spec-
tral plots (i.e., the distribution of the eigenvalues),
which is in general rare for linguistic networks,
although there have been quite a number of such
studies in the domain of biological and social net-
works (Farkas et al, 2001; Gkantsidis et al, 2003;
Banerjee and Jost, 2007). The second novelty is
in the fact that there is not much work in the com-
plex network literature that investigates the nature
of the eigenvectors and their interactions to infer
the organizing principles of the system represented
through the network.
To summarize, spectral analysis of the com-
plex network of speech sounds is able to provide
a holistic as well as quantitative explanation of
the organizing principles of the sound inventories.
This scheme for typology induction is not depen-
dent on the specific data set used as long as it is
representative of the real world. Thus, we believe
that the scheme introduced here can be applied as
a generic technique for typological classifications
of phonological, syntactic and semantic networks;
each of these are equally interesting from the per-
spective of understanding the structure and evolu-
tion of human language, and are topics of future
research.
Acknowledgement
We would like to thank Kalika Bali for her valu-
able inputs towards the linguistic analysis.
592
References
A. Banerjee and J. Jost. 2007. Spectral plots and the
representation and interpretation of biological data.
Theory in Biosciences, 126(1):15?21.
A. Banerjee and J. Jost. to appear. Graph spectra as a
systematic tool in computational biology. Discrete
Applied Mathematics.
M. Belkin and J. Goldsmith. 2002. Using eigenvectors
of the bigram graph to infer morpheme identity. In
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 41?47.
Association for Computational Linguistics.
P. Boersma. 1998. Functional Phonology. The Hague:
Holland Academic Graphics.
M. Choudhury and A. Mukherjee. to appear. The
structure and dynamics of linguistic networks. In
N. Ganguly, A. Deutsch, and A. Mukherjee, editors,
Dynamics on and of Complex Networks: Applica-
tions to Biology, Computer Science, Economics, and
the Social Sciences. Birkhauser.
M. Choudhury, A. Mukherjee, A. Basu, and N. Gan-
guly. 2006. Analysis and synthesis of the distribu-
tion of consonants over languages: A complex net-
work approach. In COLING-ACL?06, pages 128?
135.
F. R. K. Chung. 1994. Spectral Graph Theory. Num-
ber 2 in CBMS Regional Conference Series in Math-
ematics. American Mathematical Society.
G. N. Clements. 2008. The role of features in speech
sound inventories. In E. Raimy and C. Cairns, edi-
tors, Contemporary Views on Architecture and Rep-
resentations in Phonological Theory. Cambridge,
MA: MIT Press.
E. J. Farkas, I. Derenyi, A. -L. Baraba?si, and T. Vic-
seck. 2001. Real-world graphs: Beyond the semi-
circle law. Phy. Rev. E, 64:026704.
R. Ferrer-i-Cancho. 2005. The structure of syntac-
tic dependency networks: Insights from recent ad-
vances in network theory. In Levickij V. and Altm-
man G., editors, Problems of quantitative linguistics,
pages 60?75.
C. Gkantsidis, M. Mihail, and E. Zegura. 2003.
Spectral analysis of internet topologies. In INFO-
COM?03, pages 364?374.
K. M. Hayward, Y. A. Omar, and M. Goesche. 1989.
Dental and alveolar stops in Kimvita Swahili: An
electropalatographic study. African Languages and
Cultures, 2(1):51?72.
R. Kannan and S. Vempala. 2008. Spec-
tral Algorithms. Course Lecture Notes:
http://www.cc.gatech.edu/?vempala/spectral/spectral.pdf.
P. Ladefoged and I. Maddieson. 1996. Sounds of the
Worlds Languages. Oxford: Blackwell.
P. Ladefoged. 2005. Features and parameters for
different purposes. In Working Papers in Phonet-
ics, volume 104, pages 1?13. Dept. of Linguistics,
UCLA.
B. Lindblom and I. Maddieson. 1988. Phonetic univer-
sals in consonant systems. In M. Hyman and C. N.
Li, editors, Language, Speech, and Mind, pages 62?
78.
J. L. Locke and D. M. Pearson. 1992. Vocal learn-
ing and the emergence of phonological capacity. A
neurobiological approach. In Phonological devel-
opment. Models, Research, Implications, pages 91?
129. York Press.
I. Maddieson. 1984. Patterns of Sounds. Cambridge
University Press.
A. Mukherjee, M. Choudhury, A. Basu, and N. Gan-
guly. 2007. Modeling the co-occurrence principles
of the consonant inventories: A complex network
approach. Int. Jour. of Mod. Phys. C, 18(2):281?
295.
A. Mukherjee, M. Choudhury, A. Basu, and N. Gan-
guly. 2008. Modeling the structure and dynamics of
the consonant inventories: A complex network ap-
proach. In COLING-08, pages 601?608.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
S. V. Shanmugam. 1972. Dental and alveolar nasals in
Dravidian. In Bulletin of the School of Oriental and
African Studies, volume 35, pages 74?84. University
of London.
M. Sigman and G. A. Cecchi. 2002. Global organi-
zation of the wordnet lexicon. Proceedings of the
National Academy of Science, 99(3):1742?1747.
N. Trubetzkoy. 1931. Die phonologischen systeme.
TCLP, 4:96?116.
N. Trubetzkoy. 1969. Principles of Phonology. Uni-
versity of California Press, Berkeley.
N. Vallee, L J Boe, J. L. Schwartz, P. Badin, and
C. Abry. 2002. The weight of phonetic substance in
the structure of sound inventories. ZASPiL, 28:145?
168.
593
Social Network Inspired Models of NLP and Language Evolution
Monojit Choudhury
Microsoft Research Lab India
196/36 2nd Main, Sadashivnagar, Bangalore, India 560080
monojitc@microsoft.com
Animesh Mukherjee and Niloy Ganguly
Department of Computer Science and Engineering
Indian Institute of Technology Kharagpur, India 721302
{animesh,niloy}@cse.iitkgp.ernet.in
Abstract
Human language with all its intricacies is per-
haps one of the finest examples of a complex sys-
tem. Therefore, it becomes absolutely necessary
to study the faculty of language from the perspec-
tive of a complex system. Of late, there has been
an upsurge in the use of networks in modeling the
complex dynamics of various natural and artificial
systems. While some of these works aim at us-
ing social network techniques to build certain end-
user applications, others are more fundamental in
the sense that they employ these techniques to ex-
plain the emergent properties of a complex system
as a whole. A substantial amount of research have
also been done in the field of linguistics to employ
social networks in the design of efficient solutions
for numerous problems in NLP and language evolu-
tion. The objective of this tutorial is to show how
language and its dynamics can be successfully stud-
ied in the framework of social networks. The tuto-
rial will particularly demonstrate the relevance of so-
cial network-based methods in the development of a
large variety of NLP applications and in understand-
ing the dynamics of language evolution and change.
The tutorial is divided into two parts. Part I begins
with a brief introduction to this field showing how
linguistic entities and the interactions between them
can be respectively represented through the nodes
and edges of a network. This will be followed by a
comprehensive survey of the general theory of social
networks with a special emphasis on the methods of
analysis and models of synthesis for such networks.
Part II presents three case studies. The first case
study is on unsupervised POS tagging, the second
one involves modeling of the mental lexicon and ap-
plications of such models in spell checking and word
sense disambiguation. The third case study demon-
strates the usefulness of social networks in explain-
ing some of the evolutionary dynamics of language
pertaining to the sound inventories.
The tutorial is concluded by (a) comparing the
above methods with more traditional methods of do-
ing NLP, (b) providing pointers as to where to look
for/publish materials in this area, and, (c) indicating
some of the future research directions.
Biography
Monojit Choudhury is a post doctoral researcher
at Microsoft Research, India. He has submitted his
PhD from the Department of Computer Science and
Engineering, IIT Kharagpur and earlier, received his
B.Tech from the same department. Mr. Choudhury
received the Young Scientist Award from the Indian
Science Congress Association in 2003.
Animesh Mukherjee is a PhD student in the De-
partment of Computer Science and Engineering, IIT
Kharagpur and also a Microsoft Research fellow.
He received his MTech from the same department,
and BTech from Haldia Institute of Technology. Mr.
Mukherjee received the Young Scientist Award from
the Indian Science Congress Association in 2006.
Niloy Ganguly is an assistant professor in the De-
partment of Computer Science and Engineering, IIT
Kharagpur. He has received his PhD in Computer
Science from Bengal Engineering and Science Uni-
versity, Calcutta and his Bachelors from IIT Kharag-
pur. He was a post doctoral fellow in Technical
University of Dresden, Germany. He has numer-
ous publications in international journals and con-
ferences including ACL, PODC, SIGCOMM, ACM
and IEEE Trans.
937
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 128?135,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Analysis and Synthesis of the Distribution of Consonants over Languages:
A Complex Network Approach
Monojit Choudhury and Animesh Mukherjee and Anupam Basu and Niloy Ganguly
Department of Computer Science and Engineering,
Indian Institute of Technology Kharagpur
{monojit,animeshm,anupam,niloy}@cse.iitkgp.ernet.in
Abstract
Cross-linguistic similarities are reflected
by the speech sound systems of languages
all over the world. In this work we try
to model such similarities observed in the
consonant inventories, through a complex
bipartite network. We present a systematic
study of some of the appealing features of
these inventories with the help of the bi-
partite network. An important observation
is that the occurrence of consonants fol-
lows a two regime power law distribution.
We find that the consonant inventory size
distribution together with the principle of
preferential attachment are the main rea-
sons behind the emergence of such a two
regime behavior. In order to further sup-
port our explanation we present a synthe-
sis model for this network based on the
general theory of preferential attachment.
1 Introduction
Sound systems of the world?s languages show re-
markable regularities. Any arbitrary set of conso-
nants and vowels does not make up the sound sys-
tem of a particular language. Several lines of re-
search suggest that cross-linguistic similarities get
reflected in the consonant and vowel inventories
of the languages all over the world (Greenberg,
1966; Pinker, 1994; Ladefoged and Maddieson,
1996). Previously it has been argued that these
similarities are the results of certain general prin-
ciples like maximal perceptual contrast (Lindblom
and Maddieson, 1988), feature economy (Mar-
tinet, 1968; Boersma, 1998; Clements, 2004) and
robustness (Jakobson and Halle, 1956; Chomsky
and Halle, 1968). Maximal perceptual contrast
between the phonemes of a language is desir-
able for proper perception in a noisy environment.
In fact the organization of the vowel inventories
across languages has been satisfactorily explained
in terms of the single principle of maximal percep-
tual contrast (Jakobson, 1941; Wang, 1968).
There have been several attempts to reason
the observed patterns in consonant inventories
since 1930s (Trubetzkoy, 1969/1939; Lindblom
and Maddieson, 1988; Boersma, 1998; Flemming,
2002; Clements, 2004), but unlike the case of vow-
els, the structure of consonant inventories lacks a
complete and holistic explanation (de Boer, 2000).
Most of the works are confined to certain indi-
vidual principles (Abry, 2003; Hinskens and Wei-
jer, 2003) rather than formulating a general the-
ory describing the structural patterns and/or their
stability. Thus, the structure of the consonant in-
ventories continues to be a complex jigsaw puzzle,
though the parts and pieces are known.
In this work we attempt to represent the cross-
linguistic similarities that exist in the consonant
inventories of the world?s languages through a
bipartite network named PlaNet (the Phoneme
Language Network). PlaNet has two different sets
of nodes, one labeled by the languages while the
other labeled by the consonants. Edges run be-
tween these two sets depending on whether or not
a particular consonant occurs in a particular lan-
guage. This representation is motivated by similar
modeling of certain complex phenomena observed
in nature and society, such as,
? Movie-actor network, where movies and ac-
tors constitute the two partitions and an edge
between them signifies that a particular actor
acted in a particular movie (Ramasco et al,
2004).
128
? Article-author network, where the edges de-
note which person has authored which arti-
cles (Newman, 2001b).
? Metabolic network of organisms, where the
corresponding partitions are chemical com-
pounds and metabolic reactions. Edges run
between partitions depending on whether a
particular compound is a substrate or result
of a reaction (Jeong et al, 2000).
Modeling of complex systems as networks has
proved to be a comprehensive and emerging way
of capturing the underlying generating mecha-
nism of such systems (for a review on complex
networks and their generation see (Albert and
Baraba?si, 2002; Newman, 2003)). There have
been some attempts as well to model the intri-
cacies of human languages through complex net-
works. Word networks based on synonymy (Yook
et al, 2001b), co-occurrence (Cancho et al, 2001),
and phonemic edit-distance (Vitevitch, 2005) are
examples of such attempts. The present work also
uses the concept of complex networks to develop
a platform for a holistic analysis as well as synthe-
sis of the distribution of the consonants across the
languages.
In the current work, with the help of PlaNet we
provide a systematic study of certain interesting
features of the consonant inventories. An impor-
tant property that we observe is the two regime
power law degree distribution1 of the nodes la-
beled by the consonants. We try to explain this
property in the light of the size of the consonant
inventories coupled with the principle of preferen-
tial attachment (Baraba?si and Albert, 1999). Next
we present a simplified mathematical model ex-
plaining the emergence of the two regimes. In or-
der to support our analytical explanations, we also
provide a synthesis model for PlaNet.
The rest of the paper is organized into five sec-
tions. In section 2 we formally define PlaNet, out-
line its construction procedure and present some
studies on its degree distribution. We dedicate sec-
tion 3 to state and explain the inferences that can
be drawn from the degree distribution studies of
PlaNet. In section 4 we provide a simplified the-
oretical explanation of the analytical results ob-
1Two regime power law distributions have also been
observed in syntactic networks of words (Cancho et al,
2001), network of mathematics collaborators (Grossman et
al., 1995), and language diversity over countries (Gomes et
al., 1999).
Figure 1: Illustration of the nodes and edges of
PlaNet
tained. In section 5 we present a synthesis model
for PlaNet to hold up the inferences that we draw
in section 3. Finally we conclude in section 6 by
summarizing our contributions, pointing out some
of the implications of the current work and indi-
cating the possible future directions.
2 PlaNet: The Phoneme-Language
Network
We define the network of consonants and lan-
guages, PlaNet, as a bipartite graph represented as
G = ?VL, VC , E? where VL is the set of nodes la-
beled by the languages and VC is the set of nodes
labeled by the consonants. E is the set of edges
that run between VL and VC . There is an edge e ?
E between two nodes vl ? VL and vc ? VC if and
only if the consonant c occurs in the language l.
Figure 1 illustrates the nodes and edges of PlaNet.
2.1 Construction of PlaNet
Many typological studies (Lindblom and Mad-
dieson, 1988; Ladefoged and Maddieson, 1996;
Hinskens and Weijer, 2003) of segmental inven-
tories have been carried out in past on the UCLA
Phonological Segment Inventory Database (UP-
SID) (Maddieson, 1984). UPSID initially had 317
languages and was later extended to include 451
languages covering all the major language families
of the world. In this work we have used the older
version of UPSID comprising of 317 languages
and 541 consonants (henceforth UPSID317), for
constructing PlaNet. Consequently, there are 317
elements (nodes) in the set VL and 541 elements
129
(nodes) in the set VC . The number of elements
(edges) in the set E as computed from PlaNet is
7022. At this point it is important to mention that
in order to avoid any confusion in the construc-
tion of PlaNet we have appropriately filtered out
the anomalous and the ambiguous segments (Mad-
dieson, 1984) from it. We have completely ig-
nored the anomalous segments from the data set
(since the existence of such segments is doubtful),
and included the ambiguous ones as separate seg-
ments because there are no descriptive sources ex-
plaining how such ambiguities might be resolved.
A similar approach has also been described in Per-
icliev and Valde?s-Pe?rez (2002).
2.2 Degree Distribution of PlaNet
The degree of a node u, denoted by ku is defined as
the number of edges connected to u. The term de-
gree distribution is used to denote the way degrees
(ku) are distributed over the nodes (u). The de-
gree distribution studies find a lot of importance in
understanding the complex topology of any large
network, which is very difficult to visualize oth-
erwise. Since PlaNet is bipartite in nature it has
two degree distribution curves one corresponding
to the nodes in the set VL and the other corre-
sponding to the nodes in the set VC .
Degree distribution of the nodes in VL: Fig-
ure 2 shows the degree distribution of the nodes
in VL where the x-axis denotes the degree of each
node expressed as a fraction of the maximum de-
gree and the y-axis denotes the number of nodes
having a given degree expressed as a fraction of
the total number of nodes in VL .
It is evident from Figure 2 that the number of
consonants appearing in different languages fol-
low a ?-distribution 2 (see (Bulmer, 1979) for ref-
erence). The figure shows an asymmetric right
skewed distribution with the values of ? and ?
equal to 7.06 and 47.64 (obtained using maximum
likelihood estimation method) respectively. The
asymmetry points to the fact that languages usu-
ally tend to have smaller consonant inventory size,
2A random variable is said to have a ?-distribution with
parameters ?> 0 and ? > 0 if and only if its probability mass
function is given by
f(x) =
?(?+ ?)
?(?)?(?)
x??1(1 ? x)??1
for 0 < x < 1 and f(x) = 0 otherwise. ?(?) is the Euler?s
gamma function.
Figure 2: Degree distribution of PlaNet for the set
VL. The figure in the inner box is a magnified
version of a portion of the original figure.
the best value being somewhere between 10 and
30. The distribution peaks roughly at 21 indicating
that majority of the languages in UPSID317 have a
consonant inventory size of around 21 consonants.
Degree distribution of the nodes in VC: Fig-
ure 3 illustrates two different types of degree dis-
tribution plots for the nodes in VC ; Figure 3(a)
corresponding to the rank, i.e., the sorted order of
degrees, (x-axis) versus degree (y-axis) and Fig-
ure 3(b) corresponding to the degree (k) (x-axis)
versus Pk (y-axis) where Pk is the fraction of
nodes having degree greater than or equal to k.
Figure 3 clearly shows that both the curves have
two distinct regimes and the distribution is scale-
free. Regime 1 in Figure 3(a) consists of 21 con-
sonants which have a very high frequency (i.e.,
the degree k) of occurrence. Regime 2 of Fig-
ure 3(b) also correspond to these 21 consonants.
On the other hand Regime 2 of Figure 3(a) as well
as Regime 1 of Figure 3(b) comprises of the rest
of the consonants. The point marked as x in both
the figures indicates the breakpoint. Each of the
regime in both Figure 3(a) and (b) exhibit a power
law of the form
y = Ax??
In Figure 3(a) y represents the degree k of a node
corresponding to its rank x whereas in Figure 3(b)
y corresponds to Pk and x, the degree k. The val-
ues of the parameters A and ?, for Regime 1 and
Regime 2 in both the figures, as computed by the
least square error method, are shown in Table 1.
130
Regime Figure 3(a) Figure 3(b)
Regime 1 A = 368.70 ? = 0.4 A = 1.040 ? = 0.71
Regime 2 A = 12456.5 ? = 1.54 A = 2326.2 ? = 2.36
Table 1: The values of the parameters A and ?
Figure 3: Degree distribution of PlaNet for the set
VC in a log-log scale
It becomes necessary to mention here that such
power law distributions, known variously as Zipf?s
law (Zipf, 1949), are also observed in an extra-
ordinarily diverse range of phenomena including
the frequency of the use of words in human lan-
guage (Zipf, 1949), the number of papers scien-
tists write (Lotka, 1926), the number of hits on
web pages (Adamic and Huberman, 2000) and so
on. Thus our inferences, detailed out in the next
section, mainly centers around this power law be-
havior.
3 Inferences Drawn from the Analysis of
PlaNet
In most of the networked systems like the society,
the Internet, the World Wide Web, and many oth-
ers, power law degree distribution emerges for the
phenomenon of preferential attachment, i.e., when
?the rich get richer? (Simon, 1955). With refer-
ence to PlaNet this preferential attachment can be
interpreted as the tendency of a language to choose
a consonant that has been already chosen by a
large number of other languages. We posit that it is
this preferential property of languages that results
in the power law degree distributions observed in
Figure 3(a) and (b).
Nevertheless there is one question that still re-
mains unanswered. Whereas the power law distri-
bution is well understood, the reason for the two
distinct regimes (with a sharp break) still remains
unexplored. We hypothesize that,
Hypothesis The typical distribution of the conso-
nant inventory size over languages coupled with
the principle of preferential attachment enforces
the two distinct regimes to appear in the power
law curves.
As the average consonant inventory size in
UPSID317 is 21, so following the principle of
preferential attachment, on an average, the first
21 most frequent consonants are much more pre-
ferred than the rest. Consequently, the nature of
the frequency distribution for the highly frequent
consonants is different from the less frequent ones,
and hence there is a transition from Regime 1 to
Regime 2 in the Figure 3(a) and (b).
Support Experiment: In order to establish that
the consonant inventory size plays an important
role in giving rise to the two regimes discussed
above we present a support experiment in which
we try to observe whether the breakpoint x shifts
as we shift the average consonant inventory size.
Experiment: In order to shift the average con-
sonant inventory size from 21 to 25, 30 and 38
we neglected the contribution of the languages
with consonant inventory size less than n where
n is 15, 20 and 25 respectively and subsequently
recorded the degree distributions obtained each
time. We did not carry out our experiments for
average consonant inventory size more than 38 be-
cause the number of such languages are very rare
in UPSID317.
Observations: Figure 4 shows the effect of this
shifting of the average consonant inventory size on
the rank versus degree distribution curves. Table 2
presents the results observed from these curves
with the left column indicating the average inven-
tory size and the right column the breakpoint x.
131
Figure 4: Degree distributions at different average
consonant inventory sizes
Avg. consonant inv. size Transition
25 25
30 30
38 37
Table 2: The transition points for different average
consonant inventory size
The table clearly indicates that the transition oc-
curs at values corresponding to the average conso-
nant inventory size in each of the three cases.
Inferences: It is quite evident from our observa-
tions that the breakpoint x has a strong correlation
with the average consonant inventory size, which
therefore plays a key role in the emergence of the
two regime degree distribution curves.
In the next section we provide a simplistic math-
ematical model for explaining the two regime
power law with a breakpoint corresponding to the
average consonant inventory size.
4 Theoretical Explanation for the Two
Regimes
Let us assume that the inventory of all the lan-
guages comprises of 21 consonants. We further as-
sume that the consonants are arranged in their hier-
archy of preference. A language traverses the hier-
archy of consonants and at every step decides with
a probability p to choose the current consonant. It
stops as soon as it has chosen all the 21 conso-
nants. Since languages must traverse through the
first 21 consonants regardless of whether the pre-
vious consonants are chosen or not, the probability
of choosing any one of these 21 consonants must
be p. But the case is different for the 22nd conso-
nant, which is chosen by a language if it has pre-
viously chosen zero, one, two, or at most 20, but
not all of the first 21 consonants. Therefore, the
probability of the 22nd consonant being chosen is,
P (22) = p
20?
i=0
(
21
i
)
pi(1? p)21?i
where (
21
i
)
pi(1? p)21?i
denotes the probability of choosing i consonants
from the first 21. In general the probability of
choosing the n+1th consonant from the hierarchy
is given by,
P (n+ 1) = p
20?
i=0
(
n
i
)
pi(1? p)n?i
Figure 5 shows the plot of the function P (n) for
various values of p which are 0.99, 0.95, 0.9, 0.85,
0.75 and 0.7 respectively in log-log scale. All the
curves, for different values of p, have a nature sim-
ilar to that of the degree distribution plot we ob-
tained for PlaNet. This is indicative of the fact that
languages choose consonants from the hierarchy
with a probability function comparable to P (n).
Owing to the simplified assumption that all
the languages have only 21 consonants, the first
regime is a straight line; however we believe a
more rigorous mathematical model can be built
taking into consideration the ?-distribution rather
than just the mean value of the inventory size that
can explain the negative slope of the first regime.
We look forward to do the same as a part of our fu-
ture work. Rather, here we try to investigate the ef-
fect of the exact distribution of the language inven-
tory size on the nature of the degree distribution of
the consonants through a synthetic approach based
on the principle of preferential attachment, which
is described in the subsequent section.
5 The Synthesis Model based on
Preferential Attachment
Albert and Baraba?si (1999) observed that a com-
mon property of many large networks is that the
vertex connectivities follow a scale-free power
law distribution. They remarked that two generic
mechanisms can be considered to be the cause
of this observation: (i) networks expand contin-
uously by the addition of new vertices, and (ii)
new vertices attach preferentially to sites (vertices)
that are already well connected. They found that
132
Figure 5: Plot of the function P (n) in log-log
scale
a model based on these two ingredients repro-
duces the observed stationary scale-free distrib-
utions, which in turn indicates that the develop-
ment of large networks is governed by robust self-
organizing phenomena that go beyond the particu-
lars of the individual systems.
Inspired by their work and the empirical as well
as the mathematical analysis presented above, we
propose a preferential attachment model for syn-
thesizing PlaNet (PlaNetsyn henceforth) in which
the degree distribution of the nodes in VL is
known. Hence VL={L1, L2, . . ., L317} have
degrees (consonant inventory size) {k1, k2, . . .,
k317} respectively. We assume that the nodes in
the set VC are unlabeled. At each time step, a
node Lj (j = 1 to 317) from VL tries to attach itself
with a new node i ? VC to which it is not already
connected. The probability Pr(i) with which the
node Lj gets attached to i depends on the current
degree of i and is given by
Pr(i) =
ki + 
?
i??Vj
(ki? + )
where ki is the current degree of the node i, Vj
is the set of nodes in VC to which Lj is not al-
ready connected and  is the smoothing parameter
which is used to reduce bias and favor at least a
few attachments with nodes in Vj that do not have
a high Pr(i). The above process is repeated un-
til all Lj ? VL get connected to exactly kj nodes
in VC . The entire idea is summarized in Algo-
rithm 1. Figure 6 shows a partial step of the syn-
thesis process illustrated in Algorithm 1.
Simulation Results: Simulations reveal that for
PlaNetsyn the degree distribution of the nodes be-
longing to VC fit well with the analytical results
we obtained earlier in section 2. Good fits emerge
repeat
for j = 1 to 317 do
if there is a node Lj ? VL with at least
one or more consonants to be chosen
from VC then
Compute Vj = VC-V (Lj), where
V (Lj) is the set of nodes in VC to
which Lj is already connected;
end
for each node i ? Vj do
Pr(i) =
ki + 
?
i??Vj
(ki? + )
where ki is the current degree of
the node i and  is the model
parameter. Pr(i) is the
probability of connecting Lj to i.
end
Connect Lj to a node i ? Vj
following the distribution Pr(i);
end
until all languages complete their inventory
quota ;
Algorithm 1: Algorithm for synthesis of
PlaNet based on preferential attachment
Figure 6: A partial step of the synthesis process.
When the language L4 has to connect itself with
one of the nodes in the set VC it does so with the
one having the highest degree (=3) rather than with
others in order to achieve preferential attachment
which is the working principle of our algorithm
for the range 0.06 ?  ? 0.08 with the best being
at  = 0.0701. Figure 7 shows the degree k versus
133
Figure 7: Degree distribution of the nodes in
VC for both PlaNetsyn, PlaNet, and when the
model incorporates no preferential attachment; for
PlaNetsyn,  = 0.0701 and the results are averaged
over 100 simulation runs
Pk plots for  = 0.0701 averaged over 100 simula-
tion runs.
The mean error3 between the degree distribu-
tion plots of PlaNet and PlaNetsyn is 0.03 which
intuitively signifies that on an average the varia-
tion in the two curves is 3%. On the contrary, if
there were no preferential attachment incorporated
in the model (i.e., all connections were equiprob-
able) then the mean error would have been 0.35
(35% variation on an average).
6 Conclusions, Discussion and Future
Work
In this paper, we have analyzed and synthesized
the consonant inventories of the world?s languages
in terms of a complex network. We dedicated the
preceding sections essentially to,
? Represent the consonant inventories through
a bipartite network called PlaNet,
? Provide a systematic study of certain impor-
tant properties of the consonant inventories
with the help of PlaNet,
? Propose analytical explanations for the two
regime power law curves (obtained from
PlaNet) on the basis of the distribution of the
consonant inventory size over languages to-
gether with the principle of preferential at-
tachment,
3Mean error is defined as the average difference between
the ordinate pairs where the abscissas are equal.
? Provide a simplified mathematical model to
support our analytical explanations, and
? Develop a synthesis model for PlaNet based
on preferential attachment where the conso-
nant inventory size distribution is known a
priori.
We believe that the general explanation pro-
vided here for the two regime power law is a fun-
damental result, and can have a far reaching im-
pact, because two regime behavior is observed in
many other networked systems.
Until now we have been mainly dealing with the
computational aspects of the distribution of conso-
nants over the languages rather than exploring the
real world dynamics that gives rise to such a distri-
bution. An issue that draws immediate attention is
that how preferential attachment, which is a gen-
eral phenomenon associated with network evolu-
tion, can play a prime role in shaping the conso-
nant inventories of the world?s languages. The an-
swer perhaps is hidden in the fact that language is
an evolving system and its present structure is de-
termined by its past evolutionary history. Indeed
an explanation based on this evolutionary model,
with an initial disparity in the distribution of con-
sonants over languages, can be intuitively verified
as follows ? let there be a language community
of N speakers communicating among themselves
by means of only two consonants say /k/ and /g/.
If we assume that every speaker has l descendants
and language inventories are transmitted with high
fidelity, then after i generations it is expected that
the community will consist of mli /k/ speakers and
nli /g/ speakers. Now if m > n and l > 1, then for
sufficiently large i, mli  nli. Stated differently,
the /k/ speakers by far outnumbers the /g/ speak-
ers even if initially the number of /k/ speakers is
only slightly higher than that of the /g/ speakers.
This phenomenon is similar to that of preferen-
tial attachment where language communities get
attached to, i.e., select, consonants that are already
highly preferred. Nevertheless, it remains to be
seen where from such an initial disparity in the dis-
tribution of the consonants over languages might
have originated.
In this paper, we mainly dealt with the occur-
rence principles of the consonants in the invento-
ries of the world?s languages. The work can be fur-
ther extended to identify the co-occurrence likeli-
hood of the consonants in the language inventories
134
and subsequently identify the groups or commu-
nities within them. Information about such com-
munities can then help in providing an improved
insight about the organizing principles of the con-
sonant inventories.
References
C. Abry. 2003. [b]-[d]-[g] as a universal triangle as
acoustically optimal as [i]-[a]-[u]. 15th Int. Congr.
Phonetics Sciences ICPhS, 727?730.
L. A. Adamic and B. A. Huberman. 2000. The na-
ture of markets in the World Wide Web. Quarterly
Journal of Electronic Commerce 1, 512.
R. Albert and A.-L. Baraba?si. 2002. Statistical me-
chanics of complex networks. Reviews of Modern
Physics 74, 47?97.
A.-L. Baraba?si and R. Albert. 1999. Emergence of
scaling in random networks. Science 286, 509-512.
Bart de Boer. 2000. Self-Organisation in Vowel Sys-
tems. Journal of Phonetics, Elsevier.
P. Boersma. 1998. Functional Phonology. (Doctoral
thesis, University of Amsterdam), The Hague: Hol-
land Academic Graphics.
M. G. Bulmer. 1979. Principles of Statistics, Mathe-
matics.
Ferrer i Cancho and R. V. Sole?. 2001. Santa Fe work-
ing paper 01-03-016.
N. Chomsky and M. Halle. 1968. The Sound Pattern
of English, New York: Harper and Row.
N. Clements. 2004. Features and Sound Inventories.
Symposium on Phonological Theory: Representa-
tions and Architecture, CUNY.
E. Flemming. 2002. Auditory Representations in
Phonology, New York and London: Routledge.
M. A. F. Gomes, G. L. Vasconcelos, I. J. Tsang, and I.
R. Tsang. 1999. Scaling relations for diversity of
languages. Physica A, 271, 489.
J. H. Greenberg. 1966. Language Universals with Spe-
cial Reference to Feature Hierarchies, The Hague
Mouton.
J. W. Grossman and P. D. F. Ion. 1995. On a portion
of the well-known collaboration graph. Congressus
Numerantium, 108, 129-131.
F. Hinskens and J. Weijer. 2003. Patterns of segmen-
tal modification in consonant inventories: a cross-
linguistic study. Linguistics.
R. Jakobson. 1941. Kindersprache, Aphasie und allge-
meine Lautgesetze, Uppsala, Reprinted in Selected
Writings I. Mouton, The Hague, 1962, pages 328-
401.
H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai, and A.
L. Baraba?si. 2000. The large-scale organization of
metabolic networks. Nature, 406:651-654.
R. Jakobson and M. Halle. 1956. Fundamentals of
Language, The Hague: Mouton and Co.
P. Ladefoged and I. Maddieson. 1996. Sounds of the
Worlds Languages, Oxford: Blackwell.
B. Lindblom and I. Maddieson. 1988. Phonetic Uni-
versals in Consonant Systems. In L.M. Hyman and
C.N. Li, eds., Language, Speech, and Mind, Rout-
ledge, London, 62?78.
A. J. Lotka. 1926. The frequency distribution of scien-
tific production. J. Wash. Acad. Sci. 16, 317-323.
I. Maddieson. 1984. Patterns of Sounds, Cambridge
University Press, Cambridge.
A. Martinet. 1968. Phonetics and linguistic evolu-
tion. In Bertil Malmberg (ed.), Manual of phonetics,
revised and extended edition, Amsterdam: North-
Holland Publishing Co. 464?487.
M. E. J. Newman. 2001b. Scientific collaboration net-
works. I and II. Phys. Rev., E 64.
M. E. J. Newman. 2003. The structure and function of
complex networks. SIAM Review 45, 167?256.
V. Pericliev, R. E. Valde?s-Pe?rez. 2002. Differentiating
451 languages in terms of their segment inventories.
Studia Linguistica, Blackwell Publishing.
S. Pinker. 1994. The Language Instinct, New York:
Morrowo.
Jose? J. Ramasco, S. N. Dorogovtsev, and Romualdo
Pastor-Satorras. 2004. Self-organization of collabo-
ration networks. Physical Review E, 70, 036106.
H. A. Simon. 1955. On a class of skew distribution
functions. Biometrika 42, 425-440.
N. Trubetzkoy. 1969. Principles of phonology.
(English translation of Grundzu?ge der Phonologie,
1939), Berkeley: University of California Press.
M. S. Vitevitch. 2005. Phonological neighbors in a
small world: What can graph theory tell us about
word learning? Spring 2005 Talk Series on Networks
and Complex Systems, Indiana University, Bloom-
ington.
William S.-Y. Wang. 1968. The basis of speech,
Project on Linguistic Analysis Reports, University
of California at Berkeley. Reprinted in The Learning
of Language, ed. by C. E. Reed, 1971.
S. Yook, H. Jeong and A.-L. Baraba?si. 2001b. preprint.
G. K. Zipf. 1949. Human Behaviour and the Principle
of Least Effort, Addison-Wesley, Reading, MA.
135
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 104?111,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Redundancy Ratio: An Invariant Property of the
Consonant Inventories of the World?s Languages
Animesh Mukherjee, Monojit Choudhury, Anupam Basu, Niloy Ganguly
Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur
{animeshm,monojit,anupam,niloy}@cse.iitkgp.ernet.in
Abstract
In this paper, we put forward an information
theoretic definition of the redundancy that is
observed across the sound inventories of the
world?s languages. Through rigorous statis-
tical analysis, we find that this redundancy
is an invariant property of the consonant in-
ventories. The statistical analysis further un-
folds that the vowel inventories do not ex-
hibit any such property, which in turn points
to the fact that the organizing principles of
the vowel and the consonant inventories are
quite different in nature.
1 Introduction
Redundancy is a strikingly common phenomenon
that is observed across many natural systems. This
redundancy is present mainly to reduce the risk
of the complete loss of information that might oc-
cur due to accidental errors (Krakauer and Plotkin,
2002). Moreover, redundancy is found in every level
of granularity of a system. For instance, in biologi-
cal systems we find redundancy in the codons (Lesk,
2002), in the genes (Woollard, 2005) and as well in
the proteins (Gatlin, 1974). A linguistic system is
also not an exception. There is for example, a num-
ber of words with the same meaning (synonyms) in
almost every language of the world. Similarly, the
basic unit of language, the human speech sounds or
the phonemes, is also expected to exhibit some sort
of a redundancy in the information that it encodes.
In this work, we attempt to mathematically cap-
ture the redundancy observed across the sound
(more specifically the consonant) inventories of
the world?s languages. For this purpose, we
present an information theoretic definition of redun-
dancy, which is calculated based on the set of fea-
tures1 (Trubetzkoy, 1931) that are used to express
the consonants. An interesting observation is that
this quantitative feature-based measure of redun-
dancy is almost an invariance over the consonant
inventories of the world?s languages. The observa-
tion is important since it can shed enough light on
the organization of the consonant inventories, which
unlike the vowel inventories, lack a complete and
holistic explanation. The invariance of our measure
implies that every inventory tries to be similar in
terms of the measure, which leads us to argue that
redundancy plays a very important role in shaping
the structure of the consonant inventories. In order
to validate this argument we determine the possibil-
ity of observing such an invariance if the consonant
inventories had evolved by random chance. We find
that the redundancy observed across the randomly
generated inventories is substantially different from
their real counterparts, which leads us to conclude
that the invariance is not just ?by-chance? and the
measure that we define, indeed, largely governs the
organizing principles of the consonant inventories.
1In phonology, features are the elements, which distin-
guish one phoneme from another. The features that distinguish
the consonants can be broadly categorized into three different
classes namely the manner of articulation, the place of articu-
lation and phonation. Manner of articulation specifies how the
flow of air takes place in the vocal tract during articulation of
a consonant, whereas place of articulation specifies the active
speech organ and also the place where it acts. Phonation de-
scribes the activity regarding the vibration of the vocal cords
during the articulation of a consonant.
104
Interestingly, this redundancy, when measured for
the vowel inventories, does not exhibit any similar
invariance. This immediately reveals that the prin-
ciples that govern the formation of these two types
of inventories are quite different in nature. Such
an observation is significant since whether or not
these principles are similar/different for the two in-
ventories had been a question giving rise to peren-
nial debate among the past researchers (Trubet-
zkoy, 1969/1939; Lindblom and Maddieson, 1988;
Boersma, 1998; Clements, 2004). A possible rea-
son for the observed dichotomy in the behavior of
the vowel and consonant inventories with respect to
redundancy can be as follows: while the organiza-
tion of the vowel inventories is known to be gov-
erned by a single force - the maximal perceptual
contrast (Jakobson, 1941; Liljencrants and Lind-
blom, 1972; de Boer, 2000)), consonant invento-
ries are shaped by a complex interplay of several
forces (Mukherjee et al, 2006). The invariance of
redundancy, perhaps, reflects some sort of an equi-
librium that arises from the interaction of these di-
vergent forces.
The rest of the paper is structured as follows. In
section 2 we briefly discuss the earlier works in con-
nection to the sound inventories and then systemat-
ically build up the quantitative definition of redun-
dancy from the linguistic theories that are already
available in the literature. Section 3 details out the
data source necessary for the experiments, describes
the baseline for the experiments, reports the exper-
iments performed, and presents the results obtained
each time comparing the same with the baseline re-
sults. Finally we conclude in section 4 by summa-
rizing our contributions, pointing out some of the
implications of the current work and indicating the
possible future directions.
2 Formulation of Redundancy
Linguistic research has documented a wide range of
regularities across the sound systems of the world?s
languages. It has been postulated earlier by func-
tional phonologists that such regularities are the con-
sequences of certain general principles like maxi-
mal perceptual contrast (Liljencrants and Lindblom,
1972), which is desirable between the phonemes of
a language for proper perception of each individ-
ual phoneme in a noisy environment, ease of artic-
ulation (Lindblom and Maddieson, 1988; de Boer,
2000), which requires that the sound systems of
all languages are formed of certain universal (and
highly frequent) sounds, and ease of learnability (de
Boer, 2000), which is necessary for a speaker to
learn the sounds of a language with minimum ef-
fort. In fact, the organization of the vowel inven-
tories (especially those with a smaller size) across
languages has been satisfactorily explained in terms
of the single principle of maximal perceptual con-
trast (Jakobson, 1941; Liljencrants and Lindblom,
1972; de Boer, 2000).
On the other hand, in spite of several at-
tempts (Lindblom and Maddieson, 1988; Boersma,
1998; Clements, 2004) the organization of the con-
sonant inventories lacks a satisfactory explanation.
However, one of the earliest observations about the
consonant inventories has been that consonants tend
to occur in pairs that exhibit strong correlation in
terms of their features (Trubetzkoy, 1931). In or-
der to explain these trends, feature economy was
proposed as the organizing principle of the con-
sonant inventories (Martinet, 1955). According to
this principle, languages tend to maximize the com-
binatorial possibilities of a few distinctive features
to generate a large number of consonants. Stated
differently, a given consonant will have a higher
than expected chance of occurrence in inventories in
which all of its features have distinctively occurred
in other consonants. The idea is illustrated, with an
example, through Table 1. Various attempts have
been made in the past to explain the aforementioned
trends through linguistic insights (Boersma, 1998;
Clements, 2004) mainly establishing their statistical
significance. On the contrary, there has been very
little work pertaining to the quantification of feature
economy except in (Clements, 2004), where the au-
thor defines economy index, which is the ratio of the
size of an inventory to the number of features that
characterizes the inventory. However, this definition
does not take into account the complexity that is in-
volved in communicating the information about the
inventory in terms of its constituent features.
Inspired by the aforementioned studies and
the concepts of information theory (Shannon and
Weaver, 1949) we try to quantitatively capture the
amount of redundancy found across the consonant
105
plosive voiced voiceless
dental /d/ /t/
bilabial /b/ /p/
Table 1: The table shows four plosives. If a language
has in its consonant inventory any three of the four
phonemes listed in this table, then there is a higher
than average chance that it will also have the fourth
phoneme of the table in its inventory.
inventories in terms of their constituent features. Let
us assume that we want to communicate the infor-
mation about an inventory of size N over a transmis-
sion channel. Ideally, one should require logN bits
to do the same (where the logarithm is with respect
to base 2). However, since every natural system is
to some extent redundant and languages are no ex-
ceptions, the number of bits actually used to encode
the information is more than logN . If we assume
that the features are boolean in nature, then we can
compute the number of bits used by a language to
encode the information about its inventory by mea-
suring the entropy as follows. For an inventory of
size N let there be pf consonants for which a partic-
ular feature f (where f is assumed to be boolean in
nature) is present and qf other consonants for which
the same is absent. Thus the probability that a par-
ticular consonant chosen uniformly at random from
this inventory has the feature f is pfN and the prob-
ability that the consonant lacks the feature f is qfN
(=1?pfN ). If F is the set of all features present in
the consonants forming the inventory, then feature
entropy FE can be expressed as
FE =
?
f?F
(?
pf
N
log
pf
N
?
qf
N
log
qf
N
) (1)
FE is therefore the measure of the minimum number
of bits that is required to communicate the informa-
tion about the entire inventory through the transmis-
sion channel. The lower the value of FE the better
it is in terms of the information transmission over-
head. In order to capture the redundancy involved in
the encoding we define the term redundancy ratio as
follows,
RR =
FE
logN
(2)
which expresses the excess number of bits that is
used by the constituent consonants of the inventory
Figure 1: The process of computing RR for a hypo-
thetical inventory.
in terms of a ratio. The process of computing the
value of RR for a hypothetical consonant inventory
is illustrated in Figure 1.
In the following section, we present the experi-
mental setup and also report the experiments which
we perform based on the above definition of redun-
dancy. We subsequently show that redundancy ratio
is invariant across the consonant inventories whereas
the same is not true in the case of the vowel invento-
ries.
3 Experiments and Results
In this section we discuss the data source necessary
for the experiments, describe the baseline for the
experiments, report the experiments performed, and
present the results obtained each time comparing the
same with the baseline results.
3.1 Data Source
Many typological studies (Ladefoged and Mad-
dieson, 1996; Lindblom and Maddieson, 1988)
of segmental inventories have been carried out in
past on the UCLA Phonological Segment Inven-
tory Database (UPSID) (Maddieson, 1984). UPSID
gathers phonological systems of languages from all
over the world, sampling more or less uniformly all
the linguistic families. In this work we have used
UPSID comprising of 317 languages and 541 con-
sonants found across them, for our experiments.
106
3.2 Redundancy Ratio across the Consonant
Inventories
In this section we measure the redundancy ratio (de-
scribed earlier) of the consonant inventories of the
languages recorded in UPSID. Figure 2 shows the
scatter-plot of the redundancy ratio RR of each of
the consonant inventories (y-axis) versus the inven-
tory size (x-axis). The plot immediately reveals that
the measure (i.e., RR) is almost invariant across the
consonant inventories with respect to the inventory
size. In fact, we can fit the scatter-plot with a straight
line (by means of least square regression), which as
depicted in Figure 2, has a negligible slope (m = ?
0.018) and this in turn further confirms the above
fact that RR is an invariant property of the conso-
nant inventories with regard to their size. It is im-
portant to mention here that in this experiment we
report the redundancy ratio of all the inventories of
size less than or equal to 40. We neglect the inven-
tories of the size greater than 40 since they are ex-
tremely rare (less than 0.5% of the languages of UP-
SID), and therefore, cannot provide us with statis-
tically meaningful estimates. The same convention
has been followed in all the subsequent experiments.
Nevertheless, we have also computed the values of
RR for larger inventories, whereby we have found
that for an inventory size ? 60 the results are sim-
ilar to those reported here. It is interesting to note
that the largest of the consonant inventories Ga (size
= 173) has an RR = 1.9, which is lower than all the
other inventories.
The aforementioned claim that RR is an invari-
ant across consonant inventories can be validated by
performing a standard test of hypothesis. For this
purpose, we randomly construct language invento-
ries, as discussed later, and formulate a null hypoth-
esis based on them.
Null Hypothesis: The invariance in the distribution
of RRs observed across the real consonant invento-
ries is also prevalent across the randomly generated
inventories.
Having formulated the null hypothesis we now
systematically attempt to reject the same with a very
high probability. For this purpose we first construct
random inventories and then perform a two sample
t-test (Cohen, 1995) comparing the RRs of the real
and the random inventories. The results show that
Figure 2: The scatter-plot of the redundancy ratio
RR of each of the consonant inventories (y-axis)
versus the inventory size (x-axis). The straight line-
fit is also depicted by the bold line in the figure.
indeed the null hypothesis can be rejected with a
very high probability. We proceed as follows.
3.2.1 Construction of Random Inventories
We employ two different models to generate the
random inventories. In the first model the invento-
ries are filled uniformly at random from the pool of
541 consonants. In the second model we assume
that the distribution of the occurrence of the conso-
nants over languages is known a priori. Note that
in both of these cases, the size of the random in-
ventories is same as its real counterpart. The results
show that the distribution of RRs obtained from the
second model has a closer match with the real in-
ventories than that of the first model. This indicates
that the occurrence frequency to some extent gov-
erns the law of organization of the consonant inven-
tories. The detail of each of the models follow.
Model I ? Purely Random Model: In this model
we assume that the distribution of the consonant in-
ventory size is known a priori. For each language
inventory L let the size recorded in UPSID be de-
noted by sL. Let there be 317 bins corresponding to
each consonant inventory L. A bin corresponding to
an inventory L is packed with sL consonants chosen
uniformly at random (without repetition) from the
pool of 541 available consonants. Thus the conso-
nant inventories of the 317 languages corresponding
to the bins are generated. The method is summarized
107
in Algorithm 1.
for I = 1 to 317 do
for size = 1 to sL do
Choose a consonant c uniformly at
random (without repetition) from the
pool of 541 available consonants;
Pack the consonant c in the bin
corresponding to the inventory L;
end
end
Algorithm 1: Algorithm to construct random in-
ventories using Model I
Model II ? Occurrence Frequency based Random
Model: For each consonant c let the frequency of
occurrence in UPSID be denoted by fc. Let there be
317 bins each corresponding to a language in UP-
SID. fc bins are then chosen uniformly at random
and the consonant c is packed into these bins. Thus
the consonant inventories of the 317 languages cor-
responding to the bins are generated. The entire idea
is summarized in Algorithm 2.
for each consonant c do
for i = 1 to fc do
Choose one of the 317 bins,
corresponding to the languages in
UPSID, uniformly at random;
Pack the consonant c into the bin so
chosen if it has not been already packed
into this bin earlier;
end
end
Algorithm 2: Algorithm to construct random in-
ventories using Model II
3.2.2 Results Obtained from the Random
Models
In this section we enumerate the results obtained
by computing the RRs of the randomly generated
inventories using Model I and Model II respectively.
We compare the results with those of the real inven-
Parameters Real Inv. Random Inv.
Mean 2.51177 3.59331
SDV 0.209531 0.475072
Parameters Values
t 12.15
DF 66
p ? 9.289e-17
Table 2: The results of the t-test comparing the dis-
tribution of RRs for the real and the random invento-
ries (obtained through Model I). SDV: standard devi-
ation, t: t-value of the test, DF: degrees of freedom,
p: residual uncertainty.
tories and in each case show that the null hypothesis
can be rejected with a significantly high probability.
Results from Model I: Figure 3 illustrates, for all
the inventories obtained from 100 different simula-
tion runs of Algorithm 1, the average redundancy
ratio exhibited by the inventories of a particular size
(y-axis), versus the inventory size (x-axis). The
term ?redundancy ratio exhibited by the inventories
of a particular size? actually means the following.
Let there be n consonant inventories of a particu-
lar inventory-size k. The average redundancy ra-
tio of the inventories of size k is therefore given by
1
n
?n
i=1 RRi where RRi signifies the redundancy ra-
tio of the ith inventory of size k. In Figure 3 we also
present the same curve for the real consonant inven-
tories appearing in UPSID. In these curves we fur-
ther depict the error bars spanning the entire range of
values starting from the minimum RR to the max-
imum RR for a given inventory size. The curves
show that in case of real inventories the error bars
span a very small range as compared to that of the
randomly constructed ones. Moreover, the slopes of
the curves are also significantly different. In order
to test whether this difference is significant, we per-
form a t-test comparing the distribution of the val-
ues of RR that gives rise to such curves for the real
and the random inventories. The results of the test
are noted in Table 2. These statistics clearly shows
that the distribution of RRs for the real and the ran-
dom inventories are significantly different in nature.
Stated differently, we can reject the null hypothesis
with (100 - 9.29e-15)% confidence.
Results from Model II: Figure 4 illustrates, for
all the inventories obtained from 100 different simu-
108
Figure 3: Curves showing the average redundancy
ratio exhibited by the real as well as the random in-
ventories (obtained through Model I) of a particular
size (y-axis), versus the inventory size (x-axis).
lation runs of Algorithm 2, the average redundancy
ratio exhibited by the inventories of a particular size
(y-axis), versus the inventory size (x-axis). The fig-
ure shows the same curve for the real consonant in-
ventories also. For each of the curve, the error bars
span the entire range of values starting from the min-
imum RR to the maximum RR for a given inventory
size. It is quite evident from the figure that the error
bars for the curve representing the real inventories
are smaller than those of the random ones. The na-
ture of the two curves are also different though the
difference is not as pronounced as in case of Model I.
This is indicative of the fact that it is not only the oc-
currence frequency that governs the organization of
the consonant inventories and there is a more com-
plex phenomenon that results in such an invariant
property. In fact, in this case also, the t-test statistics
comparing the distribution of RRs for the real and
the random inventories, reported in Table 3, allows
us to reject the null hypothesis with (100?2.55e?3)%
confidence.
3.3 Comparison with Vowel Inventories
Until now we have been looking into the organiza-
tional aspects of the consonant inventories. In this
section we show that this organization is largely dif-
ferent from that of the vowel inventories in the sense
that there is no such invariance observed across the
vowel inventories unlike that of consonants. For
this reason we start by computing the RRs of all
Figure 4: Curves showing the average redundancy
ratio exhibited by the real as well as the random in-
ventories (obtained through Model II) of a particular
size (y-axis), versus the inventory size (x-axis).
Parameters Real Inv. Random Inv.
Mean 2.51177 2.76679
SDV 0.209531 0.228017
Parameters Values
t 4.583
DF 60
p ? 2.552e-05
Table 3: The results of the t-test comparing the dis-
tribution of RRs for the real and the random inven-
tories (obtained through Model II).
the vowel inventories appearing in UPSID. Figure 5
shows the scatter plot of the redundancy ratio of each
of the vowel inventories (y-axis) versus the inven-
tory size (x-axis). The plot clearly indicates that the
measure (i.e., RR) is not invariant across the vowel
inventories and in fact, the straight line that fits the
distribution has a slope of ?0.14, which is around 10
times higher than that of the consonant inventories.
Figure 6 illustrates the average redundancy ratio
exhibited by the vowel and the consonant inventories
of a particular size (y-axis), versus the inventory size
(x-axis). The error bars indicating the variability of
RR among the inventories of a fixed size also span a
much larger range for the vowel inventories than for
the consonant inventories.
The significance of the difference in the nature of
the distribution of RRs for the vowel and the conso-
nant inventories can be again estimated by perform-
ing a t-test. The null hypothesis in this case is as
follows.
109
Figure 5: The scatter-plot of the redundancy ratio
RR of each of the vowel inventories (y-axis) versus
the inventory size (x-axis). The straight line-fit is
depicted by the bold line in the figure.
Figure 6: Curves showing the average redundancy
ratio exhibited by the vowel as well as the consonant
inventories of a particular size (y-axis), versus the
inventory size (x-axis).
Null Hypothesis: The nature of the distribution of
RRs for the vowel and the consonant inventories is
same.
We can now perform the t-test to verify whether
we can reject the above hypothesis. Table 4 presents
the results of the test. The statistics immediately
confirms that the null hypothesis can be rejected
with 99.932% confidence.
Parameters Consonant Inv. Vowel Inv.
Mean 2.51177 2.98797
SDV 0.209531 0.726547
Parameters Values
t 3.612
DF 54
p ? 0.000683
Table 4: The results of the t-test comparing the dis-
tribution of RRs for the consonant and the vowel
inventories.
4 Conclusions, Discussion and Future
Work
In this paper we have mathematically captured the
redundancy observed across the sound inventories of
the world?s languages. We started by systematically
defining the term redundancy ratio and measuring
the value of the same for the inventories. Some of
our important findings are,
1. Redundancy ratio is an invariant property of the
consonant inventories with respect to the inventory
size.
2. A more complex phenomenon than merely the
occurrence frequency results in such an invariance.
3. Unlike the consonant inventories, the vowel in-
ventories are not indicative of such an invariance.
Until now we have concentrated on establishing
the invariance of the redundancy ratio across the
consonant inventories rather than reasoning why it
could have emerged. One possible way to answer
this question is to look for the error correcting ca-
pability of the encoding scheme that nature had em-
ployed for characterization of the consonants. Ide-
ally, if redundancy has to be invariant, then this ca-
pability should be almost constant. As a proof of
concept we randomly select a consonant from in-
ventories of different size and compute its hamming
distance from the rest of the consonants in the inven-
tory. Figure 7 shows for a randomly chosen conso-
nant c from an inventory of size 10, 15, 20 and 30
respectively, the number of the consonants at a par-
ticular hamming distance from c (y-axis) versus the
hamming distance (x-axis). The curve clearly indi-
cates that majority of the consonants are at a ham-
ming distance of 4 from c, which in turn implies that
the encoding scheme has almost a fixed error cor-
recting capability of 1 bit. This can be the precise
reason behind the invariance of the redundancy ra-
110
Figure 7: Histograms showing the the number of consonants at a particular hamming distance (y-axis), from
a randomly chosen consonant c, versus the hamming distance (x-axis).
tio. Initial studies into the vowel inventories show
that for a randomly chosen vowel, its hamming dis-
tance from the other vowels in the same inventory
varies with the inventory size. In other words, the er-
ror correcting capability of a vowel inventory seems
to be dependent on the size of the inventory.
We believe that these results are significant as well
as insightful. Nevertheless, one should be aware of
the fact that the formulation of RR heavily banks
on the set of features that are used to represent the
phonemes. Unfortunately, there is no consensus on
the set of representative features, even though there
are numerous suggestions available in the literature.
However, the basic concept of RR and the process of
analysis presented here is independent of the choice
of the feature set. In the current study we have used
the binary features provided in UPSID, which could
be very well replaced by other representations, in-
cluding multi-valued feature systems; we look for-
ward to do the same as a part of our future work.
References
B. de Boer. 2000. Self-organisation in vowel systems.
Journal of Phonetics, 28(4), 441?465.
P. Boersma. 1998. Functional phonology, Doctoral the-
sis, University of Amsterdam, The Hague: Holland
Academic Graphics.
N. Clements. 2004. Features and sound inventories.
Symposium on Phonological Theory: Representations
and Architecture, CUNY.
P. R. Cohen. 1995. Empirical methods for artificial in-
telligence, MIT Press, Cambridge.
L. L. Gatlin. 1974. Conservation of Shannon?s redun-
dancy for proteins Jour. Mol. Evol., 3, 189?208.
R. Jakobson. 1941. Kindersprache, aphasie und all-
gemeine lautgesetze, Uppsala, Reprinted in Selected
Writings I. Mouton, The Hague, 1962, 328-401.
D. C. Krakauer and J. B. Plotkin. 2002. Redundancy,
antiredundancy, and the robustness of genomes. PNAS,
99(3), 1405-1409.
A. M. Lesk. 2002. Introduction to bioinformatics, Ox-
ford University Press, New York.
P. Ladefoged and I. Maddieson. 1996. Sounds of the
world?s languages, Oxford: Blackwell.
J. Liljencrants and B. Lindblom. 1972. Numerical simu-
lation of vowel quality systems: the role of perceptual
contrast. Language, 48, 839?862.
B. Lindblom and I. Maddieson. 1988. Phonetic uni-
versals in consonant systems. Language, Speech, and
Mind, 62?78.
I. Maddieson. 1984. Patterns of sounds, Cambridge Uni-
versity Press, Cambridge.
A. Martinet 1955. `Economie des changements
phone?tiques, Berne: A. Francke.
A. Mukherjee, M. Choudhury, A. Basu and N. Ganguly.
2006. Modeling the co-occurrence principles of the
consonant inventories: A complex network approach.
arXiv:physics/0606132 (preprint).
C. E. Shannon and W. Weaver. 1949. The mathematical
theory of information, Urbana: University of Illinois
Press.
N. Trubetzkoy. 1931. Die phonologischen systeme.
TCLP, 4, 96?116.
N. Trubetzkoy. 1969. Principles of phonology, Berkeley:
University of California Press.
A. Woollard. 2005. Gene duplications and genetic re-
dundancy in C. elegans, WormBook.
111
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245?248,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Syntax is from Mars while Semantics from Venus!
Insights from Spectral Analysis of Distributional Similarity Networks
Chris Biemann
Microsoft/Powerset, San Francisco
Chris.Biemann@microsoft.com
Monojit Choudhury
Microsoft Research Lab India
monojitc@microsoft.com
Animesh Mukherjee
Indian Institute of Technology Kharagpur, India
animeshm@cse.iitkgp.ac.in
Abstract
We study the global topology of the syn-
tactic and semantic distributional similar-
ity networks for English through the tech-
nique of spectral analysis. We observe that
while the syntactic network has a hierar-
chical structure with strong communities
and their mixtures, the semantic network
has several tightly knit communities along
with a large core without any such well-
defined community structure.
1 Introduction
Syntax and semantics are two tightly coupled, yet
very different properties of any natural language
? as if one is from ?Mars? and the other from
?Venus?. Indeed, this exploratory work shows that
the distributional properties of syntax are quite dif-
ferent from those of semantics. Distributional hy-
pothesis states that the words that occur in the
same contexts tend to have similar meanings (Har-
ris, 1968). Using this hypothesis, one can define a
vector space model for words where every word
is a point in some n-dimensional space and the
distance between them can be interpreted as the
inverse of the semantic or syntactic similarity be-
tween their corresponding distributional patterns.
Usually, the co-occurrence patterns with respect to
the function words are used to define the syntactic
context, whereas that with respect to the content
words define the semantic context. An alternative,
but equally popular, visualization of distributional
similarity is through graphs or networks, where
each word is represented as nodes and weighted
edges indicate the extent of distributional similar-
ity between them.
What are the commonalities and differences be-
tween the syntactic and semantic distributional
patterns of the words of a language? This study is
an initial attempt to answer this fundamental and
intriguing question, whereby we construct the syn-
tactic and semantic distributional similarity net-
work (DSN) and analyze their spectrum to un-
derstand their global topology. We observe that
there are significant differences between the two
networks: the syntactic network has well-defined
hierarchical community structure implying a sys-
tematic organization of natural classes and their
mixtures (e.g., words which are both nouns and
verbs); on the other hand, the semantic network
has several isolated clusters or the so called tightly
knit communities and a core component that lacks
a clear community structure. Spectral analysis
also reveals the basis of formation of the natu-
ral classes or communities within these networks.
These observations collectively point towards a
well accepted fact that the semantic space of nat-
ural languages has extremely high dimension with
no clearly observable subspaces, which makes the-
orizing and engineering harder compared to its
syntactic counterpart.
Spectral analysis is the backbone of several
techniques, such as multi-dimensional scaling,
principle component analysis and latent semantic
analysis, that are commonly used in NLP. In re-
cent times, there have been some work on spec-
tral analysis of linguistic networks as well. Belkin
and Goldsmith (2002) applied spectral analysis to
understand the struture of morpho-syntactic net-
works of English words. The current work, on
the other hand, is along the lines of Mukherjee et
al. (2009), where the aim is to understand not only
the principles of organization, but also the global
topology of the network through the study of the
spectrum. The most important contribution here,
however, lies in the comparison of the topology
of the syntactic and semantic DSNs, which, to the
best of our knowledge, has not been explored pre-
viously.
245
2 Network Construction
The syntactic and semantic DSNs are constructed
from a raw text corpus. This work is restricted to
the study of English DSNs only
1
.
Syntactic DSN: We define our syntactic net-
work in a similar way as previous works in unsu-
pervised parts-of-speech induction (cf. (Sch?utze,
1995; Biemann, 2006)): The most frequent 200
words in the corpus (July 2008 dump of English
Wikipedia) are used as features in a word window
of ?2 around the target words. Thus, each target
word is described by an 800-dimensional feature
vector, containing the number of times we observe
one of the most frequent 200 words in the respec-
tive positions relative to the target word. In our
experiments, we collect data for the most frequent
1000 and 5000 target words, arguing that all syn-
tactic classes should be represented in those. A
similarity measure between target words is defined
by the cosine between the feature vectors. The
syntactic graph is formed by inserting the target
words as nodes and connecting nodes with edge
weights equal to their cosine similarity if this sim-
ilarity exceeds a threshold t = 0.66.
Semantic DSN: The construction of this net-
work is inspired by (Lin, 1998). Specifically,
we parsed a dump of English Wikipedia (July
2008) with the XLE parser (Riezler et al, 2002)
and extracted the following dependency relations
for nouns: Verb-Subject, Verb-Object, Noun-
coordination, NN-compound, Adj-Mod. These
lexicalized relations act as features for the nouns.
Verbs are recorded together with their subcatego-
rization frame, i.e. the same verb lemmas in dif-
ferent subcat frames would be treated as if they
were different verbs. We compute log-likelihood
significance between features and target nouns (as
in (Dunning, 1993)) and keep only the most signif-
icant 200 features per target word. Each feature f
gets a feature weight that is inversely proportional
to the logarithm of the number of target words it
applies on. The similarity of two target nouns is
then computed as the sum of the feature weights
they share. For our analysis, we restrict the graph
to the most frequent 5000 target common nouns
and keep only the 200 highest weighted edges per
target noun. Note that the degree of a node can
1
As shown in (Nath et al, 2008), the basic structure
of these networks are insensitive to minor variations in the
parameters (e.g., thresholds and number of words) and the
choice of distance metric.
Figure 1: The spectrum of the syntactic and se-
mantic DSNs of 1000 nodes.
still be larger than 200 if this node is contained in
many 200 highest weighted edges of other target
nouns.
3 Spectrum of DSNs
Spectral analysis refers to the systematic study of
the eigenvalues and eigenvectors of a network. Al-
though here we study the spectrum of the adja-
cency matrix of the weighted networks, it is also
quite common to study the spectrum of the Lapla-
cian of the adjacency matrix (see for example,
Belkin and Goldsmith (2002)). Fig. 1 compares
the spectrum of the syntactic and semantic DSNs
with 1000 nodes, which has been computed as fol-
lows. First, the 1000 eigenvalues of the adjacency
matrix are sorted in descending order. Then we
compute the spectral coverage till the ith eigen-
value by adding the squares of the first i eigenval-
ues and normalizing it by the sum of the squares
of all the eigenvalues - a quantity also known as
the Frobenius norm of the matrix.
We observe that for the semantic DSN the first
10 eigenvalues cover only 40% of the spectrum
and the first 500 together make up 75% of the
spectrum. On the other hand, for the syntactic
DSN, the first 10 eigenvalues cover 75% of the
spectrum while the first 20 covers 80%. In other
words, the structure of the syntactic DSN is gov-
erned by a few (order of 10) significant principles,
whereas that of the semantic DSN is controlled by
a large number of equally insignificant factors.
The aforementioned observation has the fol-
lowing alternative, but equivalent interpretations:
(a) the syntactic DSN can be clustered in lower
dimensions (e.g., 10 or 20) because, most of
the rows in the matrix can be approximately ex-
pressed as a linear combination of the top 10 to 20
246
Figure 2: Plot of corpus frequency based rank vs.
eigenvector centrality of the words in the DSNs of
5000 nodes.
eigenvectors. Furthermore, the graceful decay of
the eigenvalues of the syntactic DSN implies the
existence of a hierarchical community structure,
which has been independently verified by Nath et
al. (2008) through analysis of the degree distribu-
tion of such networks; and (b) a random walk con-
ducted on the semantic DSN will have a high ten-
dency to drift away very soon from the semantic
class of the starting node, whereas in the syntactic
DSN, the random walk is expected to stay within
the same syntactic class for a long time. There-
fore, it is reasonable to advocate that characteriza-
tion and processing of syntatic classes is far less
confusing than that of the semantic classes ? a fact
that requires no emphasis.
4 Eigenvector Analysis
The first eigenvalue tells us to what extent the
rows of the adjacency matrix are correlated and
therefore, the corresponding eigenvector is not a
dimension pointing to any classificatory basis of
the words. However, as we shall see shortly, the
other eigenvectors corresponding to the signifi-
cantly high eigenvalues are important classifica-
tory dimensions.
Fig 2 shows the plot of the first eigenvector
component (aka eigenvector centrality) of a word
versus its rank based on the corpus frequency. We
observe that the very high frequency (i.e., low
rank) nodes in both the networks have low eigen-
vector centrality, whereas the medium frequency
nodes display a wide range of centrality values.
However, the most striking difference between the
networks is that while in the syntactic DSN the
centrality values are approximately normally dis-
tributed for the medium frequency words, the least
frequent words enjoy the highest centrality for the
semantic DSN. Furthermore, we observe that the
most central nodes in the semantic DSN corre-
spond to semantically unambiguous words of sim-
ilar nature (e.g., deterioration, abandonment, frag-
mentation, turmoil). This indicates the existence
of several ?tightly knit communities consisting of
not so high frequency words? which pull in a sig-
nificant fraction of the overall centrality. Since
the high frequency words are usually polysemous,
they on the other hand form a large, but non-
cliqueish structure at the core of the network with
a few connections to the tightly knit communities.
This is known as the tightly knit community ef-
fect (TKC effect) that renders very low central-
ity values to the ?truly? central nodes of the net-
work (Lempel and Moran, 2000). The structure
of the syntactic DSN, however, is not governed by
the TKC effect to such an extreme extent. Hence,
one can expect to easily identify the natural classes
of the syntactic DSN, but not its semantic counter-
part.
In fact, this observation is further corroborated
by the higher eigenvectors. Fig. 3 shows the plot
of the second eigenvector component versus the
fourth one for the two DSNs consisting of 5000
words. It is observed that for the syntactic net-
work, the words get neatly clustered into two sets
comprised of words with the positive and negative
second eigenvector components. The same plot
for the semantic DSN shows that a large number of
words have both the components close to zero and
only a few words stand out on one side of the axes
? those with positive second eigenvector compo-
nent and those with negative fourth eigenvector
component. In essence, none of these eigenvec-
tors can neatly classify the words into two sets ?
a trend which is observed for all the higher eigen-
vectors (we conducted experiments for up to the
twentieth eigenvector).
Study of the individual eignevectors further re-
veals that the nodes with either the extreme pos-
itive or the extreme negative components have
strong linguistic correlates. For instance, in the
syntactic DSN, the two ends of the second eigen-
247
Figure 3: Plot of the second vs. fourth eigenvector
components of the words in the DSNs.
vector correspond to nouns and adjectives; one of
the ends of the fourth, fifth, sixth and the twelfth
eigenvectors respectively correspond to location
nouns, prepositions, first names and initials, and
verbs. In the semantic DSN, one of the ends of
the second, third, fourth and tenth eigenvectors
respectively correspond to professions, abstract
terms, food items and body parts. One would ex-
pect that the higher eigenvectors (say the 50
th
one)
would show no clear classificatory basis for the
syntactic DSN, while for the semantic DSN those
could be still associated with prominent linguistic
correlates.
5 Conclusion and Future Work
Here, we presented some initial investigations into
the nature of the syntactic and semantic DSNs
through the method of spectral analysis, whereby
we could observe that the global topology of the
two networks are significantly different in terms
of the organization of their natural classes. While
the syntactic DSN seems to exhibit a hierarchi-
cal structure with a few strong natural classes and
their mixtures, the semantic DSN is composed of
several tightly knit small communities along with
a large core consisting of very many smaller ill-
defined and ambiguous sets of words. To visual-
ize, one could draw an analogy of the syntactic
and semantic DSNs respectively to ?crystalline?
and ?amorphous? solids.
This work can be furthered in several directions,
such as, (a) testing the robustness of the findings
across languages, different network construction
policies, and corpora of different sizes and from
various domains; (b) clustering of the words on the
basis of eigenvector components and using them in
NLP applications such as unsupervised POS tag-
ging and WSD; and (c) spectral analysis of Word-
Net and other manually constructed ontologies.
Acknowledgement
CB and AM are grateful to Microsoft Research
India, respectively for hosting him while this re-
search was conducted, and financial support.
References
M. Belkin and J. Goldsmith 2002. Using eigenvec-
tors of the bigram graph to infer morpheme identity.
In Proceedings of the ACL-02Workshop onMorpho-
logical and Phonological Learning, pages 4147, As-
sociation for Computational Linguistics.
Chris Biemann 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL-06 Student Re-
search Workshop.
Ted Dunning 1993. Accurate methods for the statis-
tics of surprise and coincidence. In Computational
Linguistics 19, 1, pages 61?74
Z.S. Harris 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
R. Lempel and S. Moran 2000. The stochastic ap-
proach for link-structure analysis (SALSA) and the
TKC effect. In Computer Networks, 33, pages 387-
401
Dekang Lin 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING?98.
Animesh Mukherjee, Monojit Choudhury and Ravi
Kannan 2009. Discovering Global Patterns in Lin-
guistic Networks through Spectral Analysis: A Case
Study of the Consonant Inventories. In The Pro-
ceedings of EACL 2009, pages 585-593.
Joydeep Nath, Monojit Choudhury, Animesh Mukher-
jee, Christian Biemann and Niloy Ganguly 2008.
Unsupervised parts-of-speech induction for Bengali.
In The Proceedings of LREC?08, ELRA.
S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell, M. Johnson 2002. Parsing the Wall Street
Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In Proceedings
of the 40th Annual Meeting of the ACL, pages 271-
278.
Hinrich Sch?utze 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141-148.
248
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 81?88,
Rochester, April 2007 c?2007 Association for Computational Linguistics
How Difficult is it to Develop a Perfect Spell-checker?
A Cross-linguistic Analysis through Complex Network Approach
Monojit Choudhury1, Markose Thomas2, Animesh Mukherjee1,
Anupam Basu1, and Niloy Ganguly1
1Department of Computer Science and Engineering, IIT Kharagpur, India
{monojit,animeshm,anupam,niloy}@cse.iitkgp.ernet.in
2Google Inc. Bangalore, India
markysays@gmail.com
Abstract
The difficulties involved in spelling er-
ror detection and correction in a lan-
guage have been investigated in this work
through the conceptualization of SpellNet
? the weighted network of words, where
edges indicate orthographic proximity be-
tween two words. We construct SpellNets
for three languages - Bengali, English and
Hindi. Through appropriate mathemati-
cal analysis and/or intuitive justification,
we interpret the different topological met-
rics of SpellNet from the perspective of
the issues related to spell-checking. We
make many interesting observations, the
most significant among them being that
the probability of making a real word error
in a language is propotionate to the aver-
age weighted degree of SpellNet, which is
found to be highest for Hindi, followed by
Bengali and English.
1 Introduction
Spell-checking is a well researched area in NLP,
which deals with detection and automatic correc-
tion of spelling errors in an electronic text docu-
ment. Several approaches to spell-checking have
been described in the literature that use statistical,
rule-based, dictionary-based or hybrid techniques
(see (Kukich, 1992) for a dated but substantial sur-
vey). Spelling errors are broadly classified as non-
word errors (NWE) and real word errors (RWE). If
the misspelt string is a valid word in the language,
then it is called an RWE, else it is an NWE. For ex-
ample, in English, the word ?fun? might be misspelt
as ?gun? or ?vun?; while the former is an RWE, the
latter is a case of NWE. It is easy to detect an NWE,
but correction process is non-trivial. RWE, on the
other hand are extremely difficult to detect as it re-
quires syntactic and semantic analysis of the text,
though the difficulty of correction is comparable to
that of NWE (see (Hirst and Budanitsky, 2005) and
references therein).
Given a lexicon of a particular language, how
hard is it to develop a perfect spell-checker for that
language? Since context-insensitive spell-checkers
cannot detect RWE and neither they can effectively
correct NWE, the difficulty in building a perfect
spell-checker, therefore, is reflected by quantities
such as the probability of a misspelling being RWE,
probability of more than one word being orthograph-
ically closer to an NWE, and so on. In this work,
we make an attempt to understand and formalize
some of these issues related to the challenges of
spell-checking through a complex network approach
(see (Albert and Baraba?si, 2002; Newman, 2003)
for a review of the field). This in turn allows us to
provide language-specific quantitative bounds on the
performance level of spell-checkers.
In order to formally represent the orthographic
structure (spelling conventions) of a language, we
conceptualize the lexicon as a weighted network,
where the nodes represent the words and the weights
of the edges indicate the orthoraphic similarity be-
tween the pair of nodes (read words) they connect.
We shall call this network the Spelling Network or
SpellNet for short. We build the SpellNets for three
languages ? Bengali, English and Hindi, and carry
out standard topological analysis of the networks
following complex network theory. Through appro-
priate mathematical analysis and/or intuitive justi-
81
fication, we interpret the different topological met-
rics of SpellNet from the perspective of difficulties
related to spell-checking. Finally, we make sev-
eral cross-linguistic observations, both invariances
and variances, revealing quite a few interesting facts.
For example, we see that among the three languages
studied, the probability of RWE is highest in Hindi
followed by Bengali and English. A similar obser-
vation has been previously reported in (Bhatt et al,
2005) for RWEs in Bengali and English.
Apart from providing insight into spell-checking,
the complex structure of SpellNet alo reveals the
self-organization and evolutionary dynamics under-
lying the orthographic properties of natural lan-
guages. In recent times, complex networks have
been successfully employed to model and explain
the structure and organization of several natural
and social phenomena, such as the foodweb, pro-
tien interaction, formation of language invento-
ries (Choudhury et al, 2006), syntactic structure of
languages (i Cancho and Sole?, 2004), WWW, social
collaboration, scientific citations and many more
(see (Albert and Baraba?si, 2002; Newman, 2003)
and references therein). This work is inspired by
the aforementioned models, and more specifically
a couple of similar works on phonological neigh-
bors? network of words (Kapatsinski, 2006; Vite-
vitch, 2005), which try to explain the human per-
ceptual and cognitive processes in terms of the orga-
nization of the mental lexicon.
The rest of the paper is organized as follows. Sec-
tion 2 defines the structure and construction pro-
cedure of SpellNet. Section 3 and 4 describes the
degree and clustering related properties of Spell-
Net and their significance in the context of spell-
checking, respectively. Section 5 summarizes the
findings and discusses possible directions for future
work. The derivation of the probability of RWE in a
language is presented in Appendix A.
2 SpellNet: Definition and Construction
In order to study and formalize the orthographic
characteristics of a language, we model the lexicon
? of the language as an undirected and fully con-
nected weighted graph G(V,E). Each word w ? ?
is represented by a vertex vw ? V , and for every
pair of vertices vw and vw? in V , there is an edge
Figure 1: The structure of SpellNet: (a) the weighted
SpellNet for 6 English words, (b) Thresholded coun-
terpart of (a), for ? = 1
(vw, vw?) ? E. The weight of the edge (vw, vw?), is
equal to ed(w,w?) ? the orthographic edit distance
between w and w? (considering substitution, dele-
tion and insertion to have a cost of 1). Each node
vw ? V is also assigned a node weight WV (vw)
equal to the unigram occurrence frequency of the
word w. We shall refer to the graph G(V,E) as the
SpellNet. Figure 1(a) shows a hypothetical SpellNet
for 6 common English words.
We define unweighted versions of the graph
G(V,E) through the concept of thresholding as
described below. For a threshold ?, the graph
G?(V,E?) is an unweighted sub-graph of G(V,E),
where an edge (vw, vw?) ? E is assigned a weight 1
in E? if and only if the weight of the edge is less than
or equal to ?, else it is assigned a weight 0. In other
words, E? consists of only those edges in E whose
edge weight is less than or equal to ?. Note that all
the edges in E? are unweighted. Figure 1(b) shows
the thresholded SpellNet shown in 1(a) for ? = 1.
2.1 Construction of SpellNets
We construct the SpellNets for three languages ?
Bengali, English and Hindi. While the two Indian
languages ? Bengali and Hindi ? use Brahmi derived
scripts ? Bengali and Devanagari respectively, En-
glish uses the Roman script. Moreover, the orthog-
raphy of the two Indian languages are highly phone-
mic in nature, in contrast to the morpheme-based or-
thography of English. Another point of disparity lies
in the fact that while the English alphabet consists
of 26 characters, the alphabet size of both Hindi and
Bengali is around 50.
82
The lexica for the three languages have
been taken from public sources. For En-
glish it has been obtained from the website
www.audiencedialogue.org/susteng.html; for Hindi
and Bengali, the word lists as well as the unigram
frequencies have been estimated from the mono-
lingual corpora published by Central Institute of
Indian Languages. We chose to work with the most
frequent 10000 words, as the medium size of the
two Indian language corpora (around 3M words
each) does not provide sufficient data for estimation
of the unigram frequencies of a large number
of words (say 50000). Therefore, all the results
described in this work pertain to the SpellNets
corresponding to the most frequent 10000 words.
However, we believe that the trends observed do not
reverse as we increase the size of the networks.
In this paper, we focus on the networks at three
different thresholds, that is for ? = 1, 3, 5, and study
the properties of G? for the three languages. We
do not go for higher thresholds as the networks be-
come completely connected at ? = 5. Table 1 re-
ports the values of different topological metrics of
the SpellNets for the three languages at three thresh-
olds. In the following two sections, we describe in
detail some of the topological properties of Spell-
Net, their implications to spell-checking, and obser-
vations in the three languages.
3 Degree Distribution
The degree of a vertex in a network is the number of
edges incident on that vertex. Let Pk be the prob-
ability that a randomly chosen vertex has degree k
or more than k. A plot of Pk for any given network
can be formed by making a histogram of the degrees
of the vertices, and this plot is known as the cumu-
lative degree distribution of the network (Newman,
2003). The (cumulative) degree distribution of a net-
work provides important insights into the topologi-
cal properties of the network.
Figure 2 shows the plots for the cumulative de-
gree distribution for ? = 1, 3, 5, plotted on a log-
linear scale. The linear nature of the curves in the
semi-logarithmic scale indicates that the distribution
is exponential in nature. The exponential behaviour
is clearly visible for ? = 1, however at higher thresh-
olds, there are very few nodes in the network with
low degrees, and therefore only the tail of the curve
shows a pure exponential behavior. We also observe
that the steepness (i.e. slope) of the log(Pk) with re-
spect to k increases with ?. It is interesting to note
that although most of the naturally and socially oc-
curring networks exhibit a power-law degree distri-
bution (see (Albert and Baraba?si, 2002; Newman,
2003; i Cancho and Sole?, 2004; Choudhury et al,
2006) and references therein), SpellNets feature ex-
ponential degree distribution. Nevertheless, similar
results have also been reported for the phonological
neighbors? network (Kapatsinski, 2006).
3.1 Average Degree
Let the degree of the node v be denoted by k(v). We
define the quantities ? the average degree ?k? and the
weighted average degree ?kwt? for a given network
as follows (we drop the subscript w for clarity of
notation).
?k? = 1N
?
v?V
k(v) (1)
?kwt? =
?
v?V k(v)WV (v)?
v?V WV (v)
(2)
where N is the number of nodes in the network.
Implication: The average weighted degree of
SpellNet can be interpreted as the probability of
RWE in a language. This correlation can be derived
as follows. Given a lexicon ? of a language, it can
be shown that the probability of RWE in a language,
denoted by prwe(?) is given by the following equa-
tion (see Appendix A for the derivation)
prwe(?) =
?
w??
?
w???
w 6=w?
?ed(w,w?)p(w) (3)
Let neighbor(w, d) be the number of words in ?
whose edit distance from w is d. Eqn 3 can be rewrit-
ten in terms of neighbor(w, d) as follows.
prwe(?) =
?
w??
??
d=1
?d neighbor(w, d)p(w) (4)
Practically, we can always assume that d is bounded
by a small positive integer. In other words, the
number of errors simultaneously made on a word
is always small (usually assumed to be 1 or a
83
English Hindi Bengali
? = 1 ? = 3 ? = 5 ? = 1 ? = 3 ? = 5 ? = 1 ? = 3 ? = 5
M 8.97k 0.70M 8.46M 17.6k 1.73M 17.1M 11.9k 1.11M 13.2M
?k? 2.79 140.25 1692.65 4.52 347.93 3440.06 3.38 223.72 2640.11
?kwt? 6.81 408.03 1812.56 13.45 751.24 4629.36 7.73 447.16 3645.37
rdd 0.696 0.480 0.289 0.696 0.364 0.129 0.702 0.389 0.155?CC? 0.101 0.340 0.563 0.172 0.400 0.697 0.131 0.381 0.645
?CCwt? 0.221 0.412 0.680 0.341 0.436 0.760 0.229 0.418 0.681
?l? 7.07 3.50 N.E 7.47 2.74 N.E 8.19 2.95 N.E
D 24 14 N.E 26 12 N.E 29 12 N.E
Table 1: Various topological metrics and their associated values for the SpellNets of the three languages
at thresholds 1, 3 and 5. Metrics: M ? number of edges; ?k? ? average degree; ?kwt? ? average weighted
degree; ?CC? ? average clustering coefficient; ?CCwt? - average weighted clustering coefficient; rdd ?
Pearson correlation coefficient between degrees of neighbors; ?l? ? average shortest path; D ? diameter.
N.E ? Not Estimated. See the text for further details on definition, computation and significance of the
metrics.
 1e-04
 0.001
 0.01
 0.1
 1
 0  10  20  30  40  50  60
P k
Degree
Threshold 1
EnglishHindiBengali
 1e-04
 0.001
 0.01
 0.1
 1
 0  500  1000  1500  2000  2500
P k
Degree
Threshold 3
EnglishHindiBengali
 1e-04
 0.001
 0.01
 0.1
 1
 0  1000 2000 3000 4000 5000 6000 7000 8000
P k
Degree
Threshold 5
EnglishHindiBengali
Figure 2: Cumulative degree distribution of SpellNets at different thresholds presented in semi-logarithmic
scale.
slowly growing function of the word length (Kukich,
1992)). Let us denote this bound by ?. Therefore,
prwe(?) ?
?
w??
??
d=1
?d neighbor(w, d)p(w) (5)
Since ? < 1, we can substitute ?d by ? to get an
upper bound on prwe(?), which gives
prwe(?) < ?
?
w??
??
d=1
neighbor(w, d)p(w) (6)
The term ??d=1 neighbor(w, d) computes the
number of words in the lexicon, whose edit distance
from w is atmost ?. This is nothing but k(vw), i.e.
the degree of the node vw, in G?. Moreover, the term
p(w) is proportionate to the node weight WV (vw).
Thus, rewriting Eqn 6 in terms of the network pa-
rameters for G?, we get (subscript w is dropped for
clarity)
prwe(?) < ?
?
v?V k(v)WV (v)?
v?V WV (v)
(7)
Comparing Eqn 2 with the above equation, we can
directly obtain the relation
prwe(?) < C1?kwt? (8)
where C1 is some constant of proportionality. Note
that for ? = 1, prwe(?) ? ?kwt?. If we ignore
the distribution of the words, that is if we assume
p(w) = 1/N , then prwe(?) ? ?k?.
Thus, the quantity ?kwt? provides a good estimate
of the probability of RWE in a language.
Observations and Inference: At ? = 1, the av-
erage weighted degrees for Hindi, Bengali and En-
glish are 13.81, 7.73 and 6.61 respectively. Thus, the
probability of RWE in Hindi is significantly higher
84
 1
 10
 100
 10  100  1000  10000 100000 1e+06
Deg
ree
Frequency
Threshold 1
 1
 10
 100
 1000
 10000
 10  100  1000 10000 100000 1e+06
Deg
ree
Frequency
Threshold 3
 1
 10
 100
 1000
 10000
 10  100  1000 10000 100000 1e+06
Deg
ree
Frequency
Threshold 5
Figure 3: Scatter-plots for degree versus unigram
frequency at different ? for Hindi
than that of Bengali, which in turn is higher than
that of English (Bhatt et al, 2005). Similar trends
are observed at all the thresholds for both ?kwt? and
?k?. This is also evident from Figures 2, which show
the distribution of Hindi to lie above that of Bengali,
which lies above English (for all thresholds).
The average degree ?k? is substantially smaller
(0.5 to 0.33 times) than the average weighted de-
gree ?kwt? for all the 9 SpellNets. This suggests
that the higher degree nodes in SpellNet have higher
node weight (i.e. occurrence frequency). Indeed, as
shown in Figure 3 for Hindi, the high unigram fre-
quency of a node implies higher degree, though the
reverse is not true. The scatter-plots for the other
languages are similar in nature.
3.2 Correlation between Degrees of Neighbors
The relation between the degrees of adjacent words
is described by the degree assortativity coefficient.
One way to define the assortativity of a network is
through the Pearson correlation coefficient between
the degrees of the two vertices connected by an edge.
Each edge (u, v) in the network adds a data item
corresponding to the degrees of u and v to two data
sets x and y respectively. The Pearson correlation
coefficient for the data sets x and y of n items each
is then defined as
r = n
?xy ??x? y?[n?x2 ? (?x)2][n? y2 ? (? y)2]
Observation: r is positive for the networks in
which words tend to associate with other words of
similar degree (i.e. high degree with high degree
and vice versa), and it is negative for networks in
which words associate with words having degrees
in the opposite spectrum. Refering to table 1, we
see that the correlation coefficient rdd is roughly the
same and equal to around 0.7 for all languages at
? = 1. As ? increases, the correlation decreases as
expected, due to the addition of edges between dis-
similar words.
Implication: The high positive correlation coeffi-
cients suggest that SpellNets feature assortative mix-
ing of nodes in terms of degrees. If there is an RWE
corresponding to a high degree node vw, then due
to the assortative mixing of nodes, the misspelling
w? obtained from w, is also expected to have a high
degree. Since w? has a high degree, even after detec-
tion of the fact that w? is a misspelling, choosing the
right suggestion (i.e. w) is extremely difficult un-
less the linguistic context of the word is taken into
account. Thus, more often than not it is difficult to
correct an RWE, even after successful detection.
4 Clustering and Small World Properties
In the previous section, we looked at some of the de-
gree based features of SpellNets. These features pro-
vide us insights regarding the probability of RWE in
a language and the level of difficulty in correcting
the same. In this section, we discuss some of the
other characteristics of SpellNets that are useful in
predicting the difficulty of non-word error correc-
tion.
4.1 Clustering Coefficient
Recall that in the presence of a complete list of valid
words in a language, detection of NWE is a trivial
task. However, correction of NWE is far from triv-
ial. Spell-checkers usually generate a suggestion list
of possible candidate words that are within a small
edit distance of the misspelling. Thus, correction be-
comes hard as the number of words within a given
edit distance from the misspelling increases. Sup-
pose that a word w ? ? is transformed into w? due
to some typing error, such that w? /? ?. Also assume
that ed(w,w?) ? ?. We want to estimate the number
of words in ? that are within an edit distance ? of
w?. In other words we are interested in finding out
the degree of the node vw? in G?, but since there is
no such node in SpellNet, we cannot compute this
quantity directly. Nevertheless, we can provide an
85
approximate estimate of the same as follows.
Let us conceive of a hypothetical node vw? . By
definition of SpellNet, there should be an edge con-
necting vw? and vw in G?. A crude estimate of
k(vw?) can be ?kwt? of G?. Due to the assortative
nature of the network, we expect to see a high corre-
lation between the values of k(vw) and k(vw?), and
therefore, a slightly better estimate of k(vw?) could
be k(vw). However, as vw? is not a part of the net-
work, it?s behavior in SpellNet may not resemble
that of a real node, and such estimates can be grossly
erroneous.
One way to circumvent this problem is to look
at the local neighborhood of the node vw. Let us
ask the question ? what is the probability that two
randomly chosen neighbors of vw in G? are con-
nected to each other? If this probability is high, then
we can expect the local neighborhood of vw to be
dense in the sense that almost all the neighbors of
vw are connected to each other forming a clique-like
local structure. Since vw? is a neighbor of vw, it is
a part of this dense cluster, and therefore, its degree
k(vw?) is of the order of k(vw). On the other hand,
if this probability is low, then even if k(vw) is high,
the space around vw is sparse, and the local neigh-
borhood is star-like. In such a situation, we expect
k(vw?) to be low.
The topological property that measures the prob-
ability of the neighbors of a node being connected
is called the clustering coefficient (CC). One of the
ways to define the clustering coefficient C(v) for a
vertex v in a network is
C(v) = number of triangles connected to vertex vnumber of triplets centered on v
For vertices with degree 0 or 1, we put C(v) = 0.
Then the clustering coefficient for the whole net-
work ?CC? is the mean CC of the nodes in the net-
work. A corresponding weighted version of the CC
?CCwt? can be defined by taking the node weights
into account.
Implication: The higher the value of
k(vw)C(vw) for a node, the higher is the probability
that an NWE made while typing w is hard to correct
due to the presence of a large number of ortho-
graphic neighbors of the misspelling. Therefore,
in a way ?CCwt? reflects the level of difficulty in
correcting NWE for the language in general.
Observation and Inference: At threshold 1,
the values of ?CC? as well as ?CCwt? is higher
for Hindi (0.172 and 0.341 respectively) and Ben-
gali (0.131 and 0.229 respectively) than that of En-
glish (0.101 and 0.221 respectively), though for
higher thresholds, the difference between the CC
for the languages reduces. This observation further
strengthens our claim that the level of difficulty in
spelling error detection and correction are language
dependent, and for the three languages studied, it is
hardest for Hindi, followed by Bengali and English.
4.2 Small World Property
As an aside, it is interesting to see whether the Spell-
Nets exhibit the so called small world effect that is
prevalent in many social and natural systems (see
(Albert and Baraba?si, 2002; Newman, 2003) for def-
inition and examles). A network is said to be a small
world if it has a high clustering coefficient and if the
average shortest path between any two nodes of the
network is small.
Observation: We observe that SpellNets indeed
feature a high CC that grows with the threshold. The
average shortest path, denoted by ?l? in Table 1, for
? = 1 is around 7 for all the languages, and reduces
to around 3 for ? = 3; at ? = 5 the networks are
near-cliques. Thus, SpellNet is a small world net-
work.
Implication: By the application of triangle in-
equality of edit distance, it can be easily shown that
?l? ? ? provides an upper bound on the average edit
distance between all pairs of the words in the lexi-
con. Thus, a small world network, which implies a
small ?l?, in turn implies that as we increase the error
bound (i.e. ?), the number of edges increases sharply
in the network and soon the network becomes fully
connected. Therefore, it becomes increasingly more
difficult to correct or detect the errors, as any word
can be a possible suggestion for any misspelling. In
fact this is independently observed through the ex-
ponential rise in M ? the number of edges, and fall
in ?l? as we increase ?.
Inference: It is impossible to correct very noisy
texts, where the nature of the noise is random and
words are distorted by a large edit distance (say 3 or
more).
86
5 Conclusion
In this work, we have proposed the network of ortho-
graphic neighbors of words or the SpellNet and stud-
ied the structure of the same across three languages.
We have also made an attempt to relate some of the
topological properties of SpellNet to spelling error
distribution and hardness of spell-checking in a lan-
guage. The important observations of this study are
summarized below.
? The probability of RWE in a language can
be equated to the average weighted degree of
SpellNet. This probablity is highest in Hindi
followed by Bengali and English.
? In all the languages, the words that are more
prone to undergo an RWE are more likely to be
misspelt. Effectively, this makes RWE correc-
tion very hard.
? The hardness of NWE correction correlates
with the weighted clustering coefficient of the
network. This is highest for Hindi, followed by
Bengali and English.
? The basic topology of SpellNet seems to be an
invariant across languages. For example, all
the networks feature exponential degree distri-
bution, high clustering, assortative mixing with
respect to degree and node weight, small world
effect and positive correlation between degree
and node weight, and CC and degree. However,
the networks vary to a large extent in terms of
the actual values of some of these metrics.
Arguably, the language-invariant properties of
SpellNet can be attributed to the organization of
the human mental lexicon (see (Kapatsinski, 2006)
and references therein), self-organization of ortho-
graphic systems and certain properties of edit dis-
tance measure. The differences across the lan-
guages, perhaps, are an outcome of the specific or-
thographic features, such as the size of the alphabet.
Another interesting observation is that the phonemic
nature of the orthography strongly correlates with
the difficulty of spell-checking. Among the three
languages, Hindi has the most phonemic and En-
glish the least phonemic orthography. This corre-
lation calls for further investigation.
Throughout the present discussion, we have fo-
cussed on spell-checkers that ignore the context;
consequently, many of the aforementioned results,
especially those involving spelling correction, are
valid only for context-insensitive spell-checkers.
Nevertheless, many of the practically useful spell-
checkers incorporate context information and the
current analysis on SpellNet can be extended for
such spell-checkers by conceptualizing a network
of words that capture the word co-occurrence pat-
terns (Biemann, 2006). The word co-occurrence
network can be superimposed on SpellNet and the
properties of the resulting structure can be appro-
priately analyzed to obtain similar bounds on hard-
ness of context-sensitive spell-checkers. We deem
this to be a part of our future work. Another way
to improve the study could be to incorporate a more
realistic measure for the orthographic similarity be-
tween the words. Nevertheless, such a modification
will have no effect on the analysis technique, though
the results of the analysis may be different from the
ones reported here.
Appendix A: Derivation of the Probability
of RWE
We take a noisy channel approach, which is a com-
mon technique in NLP (for example (Brown et al,
1993)), including spellchecking (Kernighan et al,
1990). Depending on the situation. the channel may
model typing or OCR errors. Suppose that a word w,
while passing through the channel, gets transformed
to a word w?. Therefore, the aim of spelling cor-
rection is to find the w? ? ? (the lexicon), which
maximizes p(w?|w?), that is
argmax
w??
p(w|w?) = argmax
w??
p(w?|w)p(w)
(9)
The likelihood p(w?|w) models the noisy channel,
whereas the term p(w) is traditionally referred to
as the language model (see (Jurafsky and Martin,
2000) for an introduction). In this equation, as well
as throughout this discussion, we shall assume a uni-
gram language model, where p(w) is the normalized
frequency of occurrence of w in a standard corpus.
We define the probability of RWE for a word w,
87
prwe(w), as follows
prwe(w) =
?
w???
w 6=w?
p(w?|w) (10)
Stated differently, prwe(w) is a measure of the prob-
ability that while passing through the channel, w
gets transformed into a form w?, such that w? ? ?
and w? 6= w. The probability of RWE in the lan-
guage, denoted by prwe(?), can then be defined in
terms of the probability prwe(w) as follows.
prwe(?) =
?
w??
prwe(w)p(w) (11)
=
?
w??
?
w???
w 6=w?
p(w?|w)p(w)
In order to obtain an estimate of the likelihood
p(w?|w), we use the concept of edit distance (also
known as Levenstein distance (Levenstein, 1965)).
We shall denote the edit distance between two words
w and w? by ed(w,w?). If we assume that the proba-
bility of a single error (i.e. a character deletion, sub-
stitution or insertion) is ? and errors are independent
of each other, then we can approximate the likeli-
hood estimate as follows.
p(w?|w) = ?ed(w,w?) (12)
Exponentiation of edit distance is a common mea-
sure of word similarity or likelihood (see for exam-
ple (Bailey and Hahn, 2001)).
Substituting for p(w?|w) in Eqn 11, we get
prwe(?) =
?
w??
?
w???
w 6=w?
?ed(w,w?)p(w) (13)
References
R. Albert and A. L. Baraba?si. 2002. Statistical mechan-
ics of complex networks. Reviews of Modern Physics,
74:47?97.
Todd M. Bailey and Ulrike Hahn. 2001. Determinants of
wordlikeness: Phonotactics or lexical neighborhoods?
Journal of Memory and Language, 44:568 ? 591.
A. Bhatt, M. Choudhury, S. Sarkar, and A. Basu. 2005.
Exploring the limits of spellcheckers: A compara-
tive study in bengali and english. In Proceedings of
the Symposium on Indian Morphology, Phonology and
Language Engineering (SIMPLE?05), pages 60?65.
C. Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Pro-
ceedings of the COLING/ACL 2006 Student Research
Workshop, pages 7?12.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?312.
M. Choudhury, A. Mukherjee, A. Basu, and N. Ganguly.
2006. Analysis and synthesis of the distribution of
consonants over languages: A complex network ap-
proach. In Proceedings of the COLING/ACL Main
Conference Poster Sessions, pages 128?135.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11:87 ? 111.
R. Ferrer i Cancho and R. V. Sole?. 2004. Patterns in
syntactic dependency networks. Physical Review E,
69:051915.
D. Jurafsky and J. H. Martin. 2000. An Introduction
to Natural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice Hall.
V. Kapatsinski. 2006. Sound similarity relations in
the mental lexicon: Modeling the lexicon as a com-
plex network. Speech research Lab Progress Report,
27:133 ? 152.
M. D. Kernighan, K. W. Church, and W. A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In Proceedings of COLING, pages 205?
210, NJ, USA. ACL.
K. Kukich. 1992. Technique for automatically correcting
words in text. ACM Computing Surveys, 24:377 ? 439.
V. I. Levenstein. 1965. Binary codes capable of cor-
recting deletions, insertions and reversals. Doklady
Akademii Nauk SSSR, 19:1 ? 36.
M. E. J. Newman. 2003. The structure and function of
complex networks. SIAM Review, 45:167?256.
M. S. Vitevitch. 2005. Phonological neighbors in a small
world: What can graph theory tell us about word learn-
ing? Spring 2005 Talk Series on Networks and Com-
plex Systems, Indiana University.
88
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 101?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Emergence of Community Structures in Vowel Inventories:
An Analysis based on Complex Networks
Animesh Mukherjee, Monojit Choudhury, Anupam Basu, Niloy Ganguly
Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur
{animeshm,monojit,anupam,niloy}@cse.iitkgp.ernet.in
Abstract
In this work, we attempt to capture patterns
of co-occurrence across vowel systems and
at the same time figure out the nature of the
force leading to the emergence of such pat-
terns. For this purpose we define a weighted
network where the vowels are the nodes
and an edge between two nodes (read vow-
els) signify their co-occurrence likelihood
over the vowel inventories. Through this
network we identify communities of vow-
els, which essentially reflect their patterns
of co-occurrence across languages. We ob-
serve that in the assortative vowel communi-
ties the constituent nodes (read vowels) are
largely uncorrelated in terms of their fea-
tures indicating that they are formed based
on the principle of maximal perceptual con-
trast. However, in the rest of the communi-
ties, strong correlations are reflected among
the constituent vowels with respect to their
features indicating that it is the principle of
feature economy that binds them together.
1 Introduction
Linguistic research has documented a wide range of
regularities across the sound systems of the world?s
languages (Liljencrants and Lindblom, 1972; Lind-
blom, 1986; de Boer, 2000; Choudhury et al, 2006;
Mukherjee et al, 2006a; Mukherjee et al, 2006b).
Functional phonologists argue that such regulari-
ties are the consequences of certain general princi-
ples like maximal perceptual contrast (Liljencrants
and Lindblom, 1972), which is desirable between
the phonemes of a language for proper percep-
tion of each individual phoneme in a noisy envi-
ronment, ease of articulation (Lindblom and Mad-
dieson, 1988; de Boer, 2000), which requires that
the sound systems of all languages are formed of
certain universal (and highly frequent) sounds, and
ease of learnability (de Boer, 2000), which is re-
quired so that a speaker can learn the sounds of
a language with minimum effort. In the study of
vowel systems the optimizing principle, which has
a long tradition (Jakobson, 1941; Wang, 1968) in
linguistics, is maximal perceptual contrast. A num-
ber of numerical studies based on this principle have
been reported in literature (Liljencrants and Lind-
blom, 1972; Lindblom, 1986; Schwartz et al, 1997).
Of late, there have been some attempts to explain the
vowel systems through multi agent simulations (de
Boer, 2000) and genetic algorithms (Ke et al, 2003);
all of these experiments also use the principle of per-
ceptual contrast for optimization purposes.
An exception to the above trend is a school of
linguists (Boersma, 1998; Clements, 2004) who ar-
gue that perceptual contrast-based theories fail to ac-
count for certain fundamental aspects such as the
patterns of co-occurrence of vowels based on sim-
ilar acoustic/articulatory features1 observed across
1In linguistics, features are the elements, which distinguish
one phoneme from another. The features that describe the
vowles can be broadly categorized into three different classes
namely the height, the backness and the roundedness. Height
refers to the vertical position of the tongue relative to either the
roof of the mouth or the aperture of the jaw. Backness refers
to the horizontal tongue position during the articulation of a
vowel relative to the back of the mouth. Roundedness refers to
whether the lips are rounded or not during the articulation of a
101
the vowel inventories. Instead, they posit that the
observed patterns, especially found in larger size in-
ventories (Boersma, 1998), can be explained only
through the principle of feature economy (de Groot,
1931; Martinet, 1955). According to this principle,
languages tend to maximize the combinatorial pos-
sibilities of a few distinctive features to generate a
large number of sounds.
The aforementioned ideas can be possibly linked
together through the example illustrated by Figure 1.
As shown in the figure, the initial plane P constitutes
of a set of three very frequently occurring vowels /i/,
/a/ and /u/, which usually make up the smaller in-
ventories and do not have any single feature in com-
mon. Thus, smaller inventories are quite likely to
have vowels that exhibit a large extent of contrast
in their constituent features. However, in bigger in-
ventories, members from the higher planes (P? and
P
??) are also present and they in turn exhibit fea-
ture economy. For instance, in the plane P? com-
prising of the set of vowels /?i/, /a?/, /u?/, we find a
nasal modification applied equally on all the three
members of the set. This is actually indicative of an
economic behavior that the larger inventories show
while choosing a new feature in order to reduce the
learnability effort of the speakers. The third plane
P
?? reinforces this idea by showing that the larger
the size of the inventories the greater is the urge for
this economy in the choice of new features. An-
other interesting facet of the figure are the relations
that exist across the planes (indicated by the bro-
ken lines). All these relations are representative of a
common linguistic concept of robustness (Clements,
2004) in which one less frequently occurring vowel
(say /?i/) implies the presence of the other (and not
vice versa) frequently occurring vowel (say /i/) in a
language inventory. These cross-planar relations are
also indicative of feature economy since all the fea-
tures present in the frequent vowel (e.g., /i/) are also
shared by the less frequent one (e.g., /?i/). In sum-
mary, while the basis of organization of the vowel
inventories is perceptual contrast as indicated by
the plane P in Figure 1, economic modifications of
the perceptually distinct vowels takes place with the
vowel. There are however still more possible features of vowel
quality, such as the velum position (e.g., nasality), type of vocal
fold vibration (i.e., phonation), and tongue root position (i.e.,
secondary place of articulation).
increase in the inventory size (as indicated by the
planes P ? and P ?? in Figure 1).
In this work we attempt to corroborate the above
conjecture by automatically capturing the patterns of
co-occurrence that are prevalent in and across the
planes illustrated in Figure 1. In order to do so,
we define the ?Vowel-Vowel Network? or VoNet,
which is a weighted network where the vowels are
the nodes and an edge between two nodes (read vow-
els) signify their co-occurrence likelihood over the
vowel inventories. We conduct community struc-
ture analysis of different versions of VoNet in or-
der to capture the patterns of co-occurrence in and
across the planes P , P ? and P ?? shown in Figure 1.
The plane P consists of the communities, which
are formed of those vowels that have a very high
frequency of occurrence (usually assortative (New-
man, 2003) in nature). We observe that the con-
stituent nodes (read vowels) of these assortative
vowel communities are largely uncorrelated in terms
of their features. On the other hand, the commu-
nities obtained from VoNet, in which the links be-
tween the assortative nodes are absent, corresponds
to the co-occurrence patterns of the planes P? and
P
??
. In these communities, strong correlations are
reflected among the constituent vowels with respect
to their features. Moreover, the co-occurrences
across the planes can be captured by the community
analysis of VoNet where only the connections be-
tween the assortative and the non-assortative nodes,
with the non-assortative node co-occurring very fre-
quently with the assortative one, are retained while
the rest of the connections are filtered out. We find
that these communities again exhibit a high correla-
tion among the constituent vowels.
This article is organized as follows: Section 2 de-
scribes the experimental setup in order to explore
the co-occurrence principles of the vowel inven-
tories. In this section we formally define VoNet,
outline its construction procedure, and present a
community-finding algorithm in order to capture the
co-occurrence patterns across the vowel systems. In
section 3 we report the experiments performed to
obtain the community structures, which are repre-
sentative of the co-occurrence patterns in and across
the planes discussed above. Finally, we conclude in
section 4 by summarizing our contributions, point-
ing out some of the implications of the current work
102
Figure 1: The organizational principles of the vowels (in decreasing frequency of occurrence) indicated
through different hypothetical planes.
and indicating the possible future directions.
2 Experimental Setup
In this section we systematically develop the ex-
perimental setup in order to investigate the co-
occurrence principles of the vowel inventories. For
this purpose, we formally define VoNet, outline
its construction procedure, describe a community-
finding algorithm to decompose VoNet to obtain the
community structures that essentially reflects the co-
occurrence patterns of the vowel inventories.
2.1 Definition and Construction of VoNet
Definition of VoNet: We define VoNet as a network
of vowels, represented as G = ? V
V
, E ? where V
V
is the set of nodes labeled by the vowels and E is
the set of edges occurring in VoNet. There is an
edge e ? E between two nodes, if and only if there
exists one or more language(s) where the nodes
(read vowels) co-occur. The weight of the edge e
(also edge-weight) is the number of languages in
which the vowels connected by e co-occur. The
weight of a node u (also node-weight) is the number
of languages in which the vowel represented by u
occurs. In other words, if a vowel v
i
represented by
the node u occurs in the inventory of n languages
then the node-weight of u is assigned the value
n. Also if the vowel v
j
is represented by the node
v and there are w languages in which vowels v
i
and v
j
occur together then the weight of the edge
connecting u and v is assigned the value v. Figure 2
illustrates this structure by reproducing some of the
nodes and edges of VoNet.
Construction of VoNet: Many typological stud-
ies (Lindblom and Maddieson, 1988; Ladefoged
and Maddieson, 1996; Hinskens and Weijer, 2003;
Choudhury et al, 2006; Mukherjee et al, 2006a;
Mukherjee et al, 2006b) of segmental inventories
have been carried out in past on the UCLA Phono-
logical Segment Inventory Database (UPSID) (Mad-
dieson, 1984). Currently UPSID records the sound
inventories of 451 languages covering all the major
language families of the world. The selection of the
languages for the inclusion on UPSID is governed
by a quota principle seeking maximum genetic di-
versity among extant languages in order to reduce
bias towards any particular family. In this work we
have therefore used UPSID comprising of these 451
languages and 180 vowels found across them, for
103
Figure 3: A partial illustration of VoNet. All edges in this figure have an edge-weight greater than or equal to
15. The number on each node corresponds to a particular vowel. For instance, node number 72 corresponds
to /?i/.
constructing VoNet. Consequently, the set V
V
com-
prises 180 elements (nodes) and the set E comprises
3135 elements (edges). Figure 3 presents a partial
illustration of VoNet as constructed from UPSID.
2.2 Finding Community Structures
We attempt to identify the communities appearing
in VoNet by the extended Radicchi et al (Radic-
chi et al, 2003) algorithm for weighted networks
presented in (Mukherjee et al, 2006a). The ba-
sic idea is that if the weights on the edges form-
ing a triangle (loops of length three) are comparable
then the group of vowels represented by this trian-
gle highly occur together rendering a pattern of co-
occurrence while if these weights are not compara-
ble then there is no such pattern. In order to capture
this property we define a strength metric S (in the
lines of (Mukherjee et al, 2006a)) for each of the
edges of VoNet as follows. Let the weight of the
edge (u,v), where u, v ? V
V
, be denoted by w
uv
.
We define S as,
S =
w
uv
?
?
i?V
C
?{u,v}
(w
ui
? w
vi
)
2
(1)
if
?
?
i?V
C
?{u,v}
(w
ui
? w
vi
)
2
> 0 else S = ?.
The denominator in this expression essentially tries
to capture whether or not the weights on the edges
forming triangles are comparable (the higher the
value of S the more comparable the weights are).
The network can be then decomposed into clusters
104
Figure 2: A partial illustration of the nodes and
edges in VoNet. The labels of the nodes denote the
vowels represented in IPA (International Phonetic
Alphabet). The numerical values against the edges
and nodes represent their corresponding weights.
For example /i/ occurs in 393 languages; /e/ occurs
in 124 languages while they co-occur in 117 lan-
guages.
or communities by removing edges that have S less
than a specified threshold (say ?).
At this point it is worthwhile to clarify the sig-
nificance of a vowel community. A community of
vowels actually refers to a set of vowels which occur
together in the language inventories very frequently.
In other words, there is a higher than expected prob-
ability of finding a vowel v in an inventory which al-
ready hosts the other members of the community to
which v belongs. For instance, if /i/, /a/ and /u/ form
a vowel community and if /i/ and /a/ are present in
any inventory then there is a very high chance that
the third member /u/ is also present in the inventory.
3 Experiments and Results
In this section we describe the experiments per-
formed and the results obtained from the analysis of
VoNet. In order to find the co-occurrence patterns
in and across the planes of Figure 1 we define three
versions of VoNet namely VoNet
assort
, VoNet
rest
and VoNet
rest
? . The construction procedure for
each of these versions are presented below.
Construction of VoNet
assort
: VoNet
assort
com-
prises the assortative2 nodes having node-weights
2The term ?assortative node? here refers to the nodes having
a very high node-weight, i.e., consonants having a very high
above 120 (i.e, vowels occurring in more than 120
languages in UPSID), along with only the edges
inter-connecting these nodes. The rest of the nodes
(having node-weight less than 120) and edges are
removed from the network. We make a choice
of this node-weight for classifying the assortative
nodes from the non-assortative ones by observing
the distribution of the occurrence frequency of the
vowels illustrated in Figure 4. The curve shows
the frequency of a vowel (y-axis) versus the rank
of the vowel according to this frequency (x-axis)
in log-log scale. The high frequency zone (marked
by a circle in the figure) can be easily distinguished
from the low-frequency one since there is distinct
gap featuring between the two in the curve.
Figure 4: The frequency (y-axis) versus rank (x-
axis) curve in log-log scale illustrating the distrib-
ution of the occurrence of the vowels over the lan-
guage inventories of UPSID.
Figure 5 illustrates how VoNet
assort
is con-
structed from VoNet. Presently, the number of
nodes in VoNet
assort
is 9 and the number of edges
is 36.
Construction of VoNet
rest
: VoNet
rest
comprises
all the nodes as that of VoNet. It also has all
the edges of VoNet except for those edges that
inter-connect the assortative nodes. Figure 6 shows
how VoNet
rest
can be constructed from VoNet. The
number of nodes and edges in VoNet
rest
are 180
frequency of occurrence.
105
Figure 5: The construction procedure of VoNet
assort
from VoNet.
and 12933 respectively.
Construction of VoNet
rest
?: VoNet
rest
? again
comprises all the nodes as that of VoNet. It con-
sists of only the edges that connect an assorta-
tive node with a non-assortative one if the non-
assortative node co-occurs more than ninety five per-
cent of times with the assortative nodes. The basic
idea behind such a construction is to capture the co-
occurrence patterns based on robustness (Clements,
2004) (discussed earlier in the introductory section)
that actually defines the cross-planar relationships in
Figure 1. Figure 7 shows how VoNet
rest
? can be
constructed from VoNet. The number of nodes in
VoNet
rest
? is 180 while the number of edges is 1144.
We separately apply the community-finding al-
gorithm (discussed earlier) on each of VoNet
assort
,
VoNet
rest
and VoNet
rest
? in order to obtain the re-
spective vowel communities. We can obtain dif-
ferent sets of communities by varying the threshold
?. A few assortative vowel communities (obtained
from VoNet
assort
) are noted in Table 1. Some of the
3We have neglected nodes with node-weight less than 3
since these nodes correspond to vowels that occur in less than 3
languages in UPSID and the communities they form are there-
fore statistically insignificant.
4The network does not get disconnected due to this construc-
tion since, there is always a small fraction of edges that run be-
tween assortative and low node-weight non-assortative nodes of
otherwise disjoint groups.
communities obtained from VoNet
rest
are presented
in Table 2. We also note some of the communities
obtained from VoNet
rest
? in Table 3.
Tables 1 , 2 and 3 indicate that the communi-
ties in VoNet
assort
are formed based on the princi-
ple of perceptual contrast whereas the formation of
the communities in VoNet
rest
as well as VoNet
rest
?
is largely governed by feature economy. Hence,
the smaller vowel inventories which are composed
of mainly the members of VoNet
assort
are orga-
nized based on the principle of maximal percep-
tual contrast whereas the larger vowel inventories,
which also contain members from VoNet
rest
and
VoNet
rest
? apart from VoNet
assort
, show a consider-
able extent of feature economy. Note that the groups
presented in the tables are quite representative and
the technique described above indeed captures many
other such groups; however, due to paucity of space
we are unable to present all of them here.
4 Conclusion
In this paper we explored the co-occurrence prin-
ciples of the vowels, across the inventories of the
world?s languages. In order to do so we started with
a concise review of the available literature on vowel
inventories. We proposed an automatic procedure
to extract the co-occurrence patterns of the vowels
across languages.
Some of our important findings from this work
are,
? The smaller vowel inventories (corresponding
to the communities of
VoNet
assort
) tend to be organized based on the
principle of maximal perceptual contrast;
? On the other hand, the larger vowel invento-
ries (mainly comprising of the communities of
VoNet
rest
) reflect a considerable extent of fea-
ture economy;
? Co-occurrences based on robustness are preva-
lent across vowel inventories (captured through
the communities of VoNet
rest
?) and their emer-
gence is again a consequence of feature econ-
omy.
Until now, we have concentrated mainly on the
methodology that can be used to automatically cap-
106
Figure 6: The construction procedure of VoNet
rest
from VoNet.
Figure 7: The construction procedure of VoNet
rest
? from VoNet.
Community Features in Contrast
/i/, /a/, /u/ (low/high), (front/central/back), (unrounded/rounded)
/e/, /o/ (higher-mid/mid), (front/back), (unrounded/rounded)
Table 1: Assortative vowel communities. The contrastive features separated by slashes (/) are shown within
parentheses. Comma-separated entries represent the features that are in use from the three respective classes
namely the height, the backness, and the roundedness.
ture the co-occurrence patterns across the vowel sys-
tems. However, it would be also interesting to in-
vestigate the extent to which these patterns are gov-
erned by the forces of maximal perceptual contrast
and feature economy. Such an investigation calls
for quantitative definitions of the above forces and
107
Community Features in Common
/
?
i/, /a?/, /u?/ nasalized
/?i:/, /a?:/, /u?:/ long, nasalized
/i:/, /u:/, /a:/, /o:/, /e:/ long
Table 2: Some of the vowel communities obtained from VoNet
rest
.
Community Features in Common
/i/, /?i/ high, front, unrounded
/a/, /a?/ low, central, unrounded
/u/, /u?/ high, back, rounded
Table 3: Some of the vowel communities obtained from VoNet
rest
? . Comma-separated entries represent the
features that are in use from the three respective classes namely the height, the backness, and the rounded-
ness.
a thorough evaluation of the vowel communities in
terms of these definitions. We look forward to ac-
complish the same as a part of our future work.
References
B. de Boer. 2000. Self-organisation in vowel systems,
Journal of Phonetics, 28(4), 441?465.
P. Boersma. 1998. Functional phonology, Doctoral the-
sis, University of Amsterdam, The Hague: Holland
Academic Graphics.
M. Choudhury, A. Mukherjee, A. Basu and N. Ganguly.
2006. Analysis and synthesis of the distribution of
consonants over languages: A complex network ap-
proach, Proceedings of COLING?ACL, 128?135, Syd-
ney, Australia.
N. Clements. 2004. Features and sound inventories,
Symposium on Phonological Theory: Representations
and Architecture, CUNY.
A. W. de Groot. 1931. Phonologie und Phonetik als
funktionswissenschaften, Travaux du Cercle Linguis-
tique de, 4, 116?147.
F. Hinskens and J. Weijer. 2003. Patterns of segmen-
tal modification in consonant inventories: A cross-
linguistic study, Linguistics, 41, 6.
R. Jakobson. 2003. Kindersprache, aphasie und allge-
meine lautgesetze, Uppsala, reprinted in Selected Writ-
ings I. Mouton, (The Hague, 1962), 328-401.
J. Ke, M. Ogura and W.S.-Y. Wang. 2003. Optimization
models of sound systems using genetic algorithms,
Computational Linguistics, 29(1), 1?18.
P. Ladefoged and I. Maddieson. 1996. Sounds of the
worlds languages, Oxford: Blackwell.
J. Liljencrants and B. Lindblom. 1972. Numerical simu-
lation of vowel quality systems: the role of perceptual
contrast, Language, 48, 839?862.
B. Lindblom. 1986. Phonetic universals in vowel sys-
tems, Experimental Phonology, 13?44.
B. Lindblom and I. Maddieson. 1988. Phonetic uni-
versals in consonant systems, Language, Speech, and
Mind, Routledge, London, 62?78.
I. Maddieson. Patterns of sounds, 1984. Cambridge
University Press, Cambridge.
A. Martinet. 1955. `Economie des changements
phone?tiques, Berne: A. Francke.
A. Mukherjee, M. Choudhury, A. Basu and N. Ganguly.
2006. Modeling the co-occurrence principles of the
consonant inventories: A complex network approach,
arXiv:physics/0606132 (preprint).
A. Mukherjee, M. Choudhury, A. Basu and N. Gan-
guly. 2006. Self-organization of the Sound In-
ventories: Analysis and Synthesis of the Occur-
rence and Co-occurrence Networks of Consonants.
arXiv:physics/0610120 (preprint).
M. E. J. Newman. 2003. The structure and function of
complex networks, SIAM Review, 45, 167?256.
F. Radicchi, C. Castellano, F. Cecconi, V. Loreto and D.
Parisi. 2003. Defining and identifying communities in
networks, PNAS, 101(9), 2658?2663.
J-L. Schwartz, L-J. Bo?e, N. Valle?e and C. Abry. 1997.
The dispersion-focalization theory of vowel systems,
Journal of Phonetics, 25, 255?286.
W. S.-Y. Wang. 1968. The basis of speech. Project
on linguistic analysis reports, University of California,
Berkeley, (reprinted in The Learning of Language in
1971).
108
Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 51?58,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Language Diversity across the Consonant Inventories:
A Study in the Framework of Complex Networks
Monojit Choudhury
Microsoft Research India, Bangalore, India ? 560080
Email: monojitc@microsoft.com
Animesh Mukherjee, Anupam Basu and Niloy Ganguly
Indian Institute of Technology, Kharagpur, India ? 721302
Ashish Garg and Vaibhav Jalan
Malaviya National Institute of Technology, Jaipur, India ? 302017
Abstract
In this paper, we attempt to explain the
emergence of the linguistic diversity that
exists across the consonant inventories of
some of the major language families of the
world through a complex network based
growth model. There is only a single pa-
rameter for this model that is meant to
introduce a small amount of randomness
in the otherwise preferential attachment
based growth process. The experiments
with this model parameter indicates that
the choice of consonants among the lan-
guages within a family are far more pref-
erential than it is across the families. Fur-
thermore, our observations indicate that
this parameter might bear a correlation
with the period of existence of the lan-
guage families under investigation. These
findings lead us to argue that preferential
attachement seems to be an appropriate
high level abstraction for language acqui-
sition and change.
1 Introduction
In one of their seminal papers (Hauser et al,
2002), Noam Chomsky and his co-authors re-
marked that if a Martian ever graced our planet
then it would be awe-struck by the unique abil-
ity of the humans to communicate among them-
selves through the medium of language. How-
ever, if our Martian naturalist were meticulous
then it might also note the surprising co-existence
of 6700 such mutually unintelligible languages
across the world. Till date, the terrestrial scientists
have no definitive answer as to why this linguistic
diversity exists (Pinker, 1994). Previous work in
the area of language evolution has tried to explain
the emergence of this diversity through two differ-
ent background models. The first one assumes that
there is a set of predefined language configurations
and the movement of a particular language on this
landscape is no more than a random walk (Tom-
lin, 1986; Dryer, 1992). The second line of re-
search attempts to relate the ecological, cultural
and demographic parameters with the linguistic
parameters responsible for this diversity (Arita and
Taylor, 1996; Kirby, 1998; Livingstone and Fyfe,
1999; Nettle, 1999). From the above studies, it
turns out that linguistic diversity is an outcome of
the language dynamics in terms of its evolution,
acquisition and change.
In this work, we attempt to investigate the di-
versity that exists across the consonant inventories
of the world?s languages through an evolutionary
framework based on network growth. The use of
a network based model is motivated from the fact
that in the recent years, complex networks have
proved to be an extremely suitable framework for
modeling and studying the structure and dynam-
ics of linguistic systems (Cancho and Sole?, 2001;
Dorogovtsev and Mendes, 2001; Cancho and Sole?,
2004; Sole? et al, 2005).
Along the lines of the study presented
in (Choudhury et al, 2006), we model the struc-
ture of the inventories through a bipartite network,
which has two different sets of nodes, one la-
beled by the languages and the other by the con-
sonants. Edges run in between these two sets
depending on whether a particular consonant is
found in a particular language. This network
is termed the Phoneme?Language Network or
PlaNet in (Choudhury et al, 2006). We construct
five such networks that respectively represent the
consonant inventories belonging to the five ma-
51
jor language families namely, the Indo-European
(IE-PlaNet), the Afro-Asiatic (AA-PlaNet), the
Niger-Congo (NC-PlaNet), the Austronesian (AN-
PlaNet) and the Sino-Tibetan (ST-PlaNet).
The emergence of the distribution of occurrence
of the consonants across the languages of a fam-
ily can be explained through a growth model for
the PlaNet representing the family. We employ the
preferential attachment based growth model intro-
duced in (Choudhury et al, 2006) and later ana-
lytically solved in (Peruani et al, 2007) to explain
this emergence for each of the five families. The
model involves a single parameter that is essen-
tially meant to introduce randomness in the oth-
erwise predominantly preferential growth process.
We observe that if we combine the inventories for
all the families together and then attempt to fit this
new data with our model, the value of the param-
eter is significantly different from that of the in-
dividual families. This indicates that the dynam-
ics within the families is quite different from that
across them. There are possibly two factors that
regulate this dynamics: the innate preference of
the speakers towards acquiring certain linguistic
structures over others and shared ancestry of the
languages within a family.
The prime contribution of this paper lies in the
mathematical model that naturally captures and
quantifies the diversification process of the lan-
guage inventories. This diversification, which is
arguably an effect of language acquisition and
change, can be viewed as a manifestation of the
process of preferential attachment at a higher level
of abstraction.
The rest of the paper is laid out as follows. Sec-
tion 2 states the definition of PlaNet, briefly de-
scribes the data source and outlines the construc-
tion procedure for the five networks. In section 3
we review the growth model for the networks. The
experiments and the results are explained in the
next section. Section 5 concludes the paper by ex-
plaining how preferential attachment could possi-
bly model the phenomena of language acquisition,
change and evolution.
2 Definition and Construction of the
Networks
In this section, we revisit the definition of PlaNet,
discuss briefly about the data source, and explain
how we constructed the networks for each of the
families.
Figure 1: Illustration of the nodes and edges of
PlaNet.
2.1 Definition of PlaNet
PlaNet is a bipartite graph G = ? VL,VC ,Epl ? con-
sisting of two sets of nodes namely, VL (labeled
by the languages) and VC (labeled by the conso-
nants); Epl is the set of edges running between VL
and VC . There is an edge e ? Epl from a node
vl ? VL to a node vc ? VC iff the consonant c is
present in the inventory of the language l. Figure 1
illustrates the nodes and edges of PlaNet.
2.2 Data Source
We use the UCLA Phonological Segment Inven-
tory Database (UPSID) (Maddieson, 1984) as the
source of data for this work. The choice of this
database is motivated by a large number of typo-
logical studies (Lindblom and Maddieson, 1988;
Ladefoged and Maddieson, 1996; de Boer, 2000;
Hinskens and Weijer, 2003) that have been car-
ried out on it by earlier researchers. It is a well
known fact that UPSID suffers from several prob-
lems, especially those involving representational
issues (Vaux and Samuels, 2005). Therefore,
any analysis carried on UPSID and the inferences
drawn from them are subject to questions. How-
ever, the current analysis requires a large amount
of segment inventory data and to the best of our
knowledge UPSID is the biggest database of this
kind. Moreover, we would like to emphasize that
the prime contribution of this work lies in the
mathematical modeling of the data rather than the
results obtained, which, as we shall see shortly, are
not very surprising or novel. The current model
applied to a different database of segment inven-
52
tories may lead to different results, though we be-
lieve that the basic trends will remain similar. In
essence, the results described here should be taken
as indicative and not sacrosanct.
There are 317 languages in the database with
541 consonants found across them. From these
data we manually sort the languages into five
groups representing the five families. Note that we
included a language in any group if and only if we
could find a direct evidence of its presence in the
corresponding family. A brief description of each
of these groups and languages found within them
are listed below (Haspelmath et al, 2005; Gordon,
2005).
Indo-European: This family includes most of the
major languages of Europe and south, central and
south-west Asia. Currently, it has around 3 bil-
lion native speakers, which is largest among all
the recognized families of languages in the world.
The total number of languages appearing in this
family is 449. The earliest evidences of the Indo-
European languages have been found to date 4000
years back.
Languages ? Albanian, Lithuanian, Breton, Irish,
German, Norwegian, Greek, Bengali, Hindi-
Urdu, Kashmiri, Sinhalese, Farsi, Kurdish, Pashto,
French, Romanian, Spanish, Russian, Bulgarian.
Afro-Asiatic: Afro-Asiatic languages have about
200 million native speakers spread over north,
east, west, central and south-west Africa. This
family is divided into five subgroups with a total of
375 languages. The proto-language of this family
began to diverge into separate branches approxi-
mately 6000 years ago.
Languages ? Shilha, Margi, Angas, Dera, Hausa,
Kanakuru, Ngizim, Awiya, Somali, Iraqw, Dizi,
Kefa, Kullo, Hamer, Arabic, Amharic, Socotri.
Niger-Congo: The majority of the languages that
belong to this family are found in the sub-Saharan
parts of Africa. The number of native speakers
is around 300 million and the total number of
languages is 1514. This family descends from a
proto-language, which dates back 5000 years.
Languages ? Diola, Temne, Wolof, Akan, Amo,
Bariba, Beembe, Birom, Cham, Dagbani, Doayo,
Efik, Ga, Gbeya, Igbo, Ik, Koma, Lelemi, Senadi,
Tampulma, Tarok, Teke, Zande, Zulu, Kadugli,
Moro, Bisa, Dan, Bambara, Kpelle.
Austronesian: The languages of the Austronesian
family are widely dispersed throughout the islands
of south-east Asia and the Pacific. There are 1268
Networks |VL| |VC | |Epl|
IE-PlaNet 19 148 534
AA-PlaNet 17 123 453
NC-PlaNet 30 135 692
AN-PlaNet 12 82 221
ST-PlaNet 9 71 201
Table 1: Number of nodes and edges in the five
bipartite networks corresponding to the five fami-
lies.
languages in this family, which are spoken by a
population of 6 million native speakers. Around
4000 years back it separated out from its ancestral
branch.
Languages ? Rukai, Tsou, Hawaiian, Iai, Adz-
era, Kaliai, Roro, Malagasy, Chamorro, Tagalog,
Batak, Javanese.
Sino-Tibetan: Most of the languages in this fam-
ily are distributed over the entire east Asia. With
a population of around 2 billion native speakers it
ranks second after Indo-European. The total num-
ber of languages in this family is 403. Some of the
first evidences of this family can be traced 6000
years back.
Languages ? Hakka, Mandarin, Taishan, Jingpho,
Ao, Karen, Burmese, Lahu, Dafla.
2.3 Construction of the Networks
We use the consonant inventories of the languages
enlisted above to construct the five bipartite net-
works ? IE-PlaNet, AA-PlaNet, NC-PlaNet, AN-
PlaNet and ST-PlaNet. The number of nodes and
edges in each of these networks are noted in Ta-
ble 1.
3 The Growth Model for the Networks
As mentioned earlier, we employ the growth
model introduced in (Choudhury et al, 2006) and
later (approximately) solved in (Peruani et al,
2007) to explain the emergence of the degree dis-
tribution of the consonant nodes for the five bipar-
tite networks. For the purpose of readability, we
briefly summarize the idea below.
Degree Distribution: The degree of a node v, de-
noted by k, is the number of edges incident on
v. The degree distribution is the fraction of nodes
pk that have a degree equal to k (Newman, 2003).
The cumulative degree distribution Pk is the frac-
tion of nodes having degree greater than or equal
to k. Therefore, if there are N nodes in a network
53
then,
Pk =
N?
k=k?
pk? (1)
Model Description: The model assumes that the
size of the consonant inventories (i.e., the degree
of the language nodes in PlaNet) are known a pri-
ori.
Let the degree of a language node Li ? VL
be denoted by di (i.e., di refers to the inventory
size of the language Li in UPSID). The conso-
nant nodes in VC are assumed to be unlabeled, i.e,
they are not marked by the articulatory/acoustic
features (see (Trubetzkoy, 1931) for further refer-
ence) that characterize them. In other words, the
model does not take into account the phonetic sim-
ilarity among the segments. The nodes L1 through
L317 are sorted in the ascending order of their de-
grees. At each time step a node Lj , chosen in
order, preferentially gets connected to dj distinct
nodes (call each such node C) of the set VC . The
probability Pr(C) with which the node Lj gets
connected to the node C is given by,
Pr(C) = k + ??
?C? (k? + ?)
(2)
where k is the current degree of the node C, C ?
represents the nodes in VC that are not already
connected to Lj and ? is the model parameter that
is meant to introduce a small amount of random-
ness into the growth process. The above steps are
repeated until all the language nodes Lj ? VL get
connected to dj consonant nodes.
Intuitively, the model works as follows: If a
consonant is very frequently found in the invento-
ries of the languages, then there is a higher chance
of that consonant being included in the inventory
of a ?new language?. Here the term ?new lan-
guage? can be interpreted either as a new and hith-
erto unseen sample from the universal set of lan-
guages, or the formation of a new language due
to some form of language change. The param-
eter ? on the other hand ensures that the conso-
nants which are found in none of the languages
from the current sample also have a chance of be-
ing included in the new language. It is similar to
the add-? smoothing used to avoid zero probabil-
ities while estimating probability distributions. It
is easy to see that for very large values of ? the fre-
quency factor will play a very minor role and the
consonants will be chosen randomly by the new
language, irrespective of its present prevalence. It
is natural to ask why and how this particular pro-
cess would model the growth of the language in-
ventories. We defer this question until the last sec-
tion of the paper, and instead focus on some empir-
ical studies to see if the model can really explain
the observed data.
Peruani et al (2007) analytically derived an ap-
proximate expression for the degree distribution of
the consonant nodes for this model. Let the aver-
age consonant inventory size be denoted by ? and
the number of consonant nodes be N. The solu-
tion obtained in (Peruani et al, 2007) is based on
the assumption that at each time step t, a language
node gets attached to ? consonant nodes, follow-
ing the distribution Pr(C). Under the above as-
sumptions, the degree distribution pk,t for the con-
sonant nodes, obtained by solving the model, is a
?-distribution as follows
pk,t ' A
(k
t
)??1 (
1? kt
)N?
? ???1 (3)
where A is a constant term. Using equations 1
and 3 one can easily compute the value of Pk,t.
There is a subtle point that needs a mention
here. The concept of a time step is very crucial
for a growing network. It might refer to the addi-
tion of an edge or a node to the network. While
these two concepts coincide when every new node
has exactly one edge, there are obvious differences
when the new node has degree greater than one.
The analysis presented in Peruani et al (2007)
holds good for the case when only one edge is
added per time step. However, if the degree of the
new node being introduced to the system is much
less than N , then Eq. 3 is a good approximation of
the emergent degree distribution for the case when
a node with more than one edge is added per time
step. Therefore, the experiments presented in the
next section attempt to fit the degree distribution
of the real networks with Eq. 3 by tuning the pa-
rameter ?.
4 Experiments and Results
In this section, we attempt to fit the degree dis-
tribution of the five empirical networks with the
expression for Pk,t described in the previous sec-
tion. For all the experiments we set N = 541, t =
number of languages in the family under investi-
gation and ? = average degree of the language
nodes of the PlaNet representing the family under
investigation, that is, the average inventory size for
54
Network ? for least LSE Value of LSE
IE-PlaNet 0.055 0.16AA-PlaNet 0.040 0.24NC-PlaNet 0.035 0.19AN-PlaNet 0.030 0.17ST-PlaNet 0.035 0.03Combined-PlaNet 0.070 1.47
Table 2: The values of ? and the least LSE for the
different networks. Combined-PlaNet refers to the
network constructed after mixing all the languages
from all the families. For all the experiments
the family. Therefore, given the value of k we
can compute pk,t using Eq. 3 if ? is known, and
from pk,t we can further compute Pk,t. In order to
find the best fitting theoretical degree distribution,
we vary the value of ? in steps of 0.005 within the
range of 0 to 1 and choose that ? for which the log-
arithmic standard error1 (LSE) between the the-
oretical degree distribution and the epirically ob-
served degree distribution of the real network and
the equation is least. LSE is defined as the sum of
the square of the difference between the logarithm
of the ordinate pairs (say y and y?) for which the
abscissas are equal. The best fits obtained for each
of the five networks are shown in Figure 2. The
values of ? and the corresponding least LSE for
each of them are noted in Table 2. We make the
following significant and interesting observations.
Observation I: The very low value of the parame-
ter ? indicates that the choice of consonants within
the languages of a family is strongly preferential.
In this context, ? may be thought of as modeling
the (accidental) errors or drifts that can occur dur-
ing language transmission. The fact that the val-
ues of ? across the four major language families,
namely Afro-Asiatic,Niger-Congo, Sino-Tibetan
and Austronesian, are comparable indicates that
the rate of error propagation is a universal factor
that is largely constant across the families. The
value of ? for IE-PlaNet is slightly higher than
the other four families, which might be an effect
of higher diversification within the family due to
geographical or socio-political factors. Neverthe-
less, it is still smaller than the ? of the Combined-
1LSE = (log y ? log y?)2. We use LSE as the good-
ness of the fit because the degree distributions of PlaNets are
highly skewed. There are very few high degree nodes and a
large number of low degree nodes. The logarithmic error en-
sures that even very small errors made while fitting the high
degrees are penalized equally as compared to that of the low
degrees. Standard error would not capture this fact and de-
clare a fit as good if it is able to replicate the distribution for
low degrees, but fits the high degrees poorly .
PlaNet.
The optimal ? obtained for Combined-PlaNet is
higher than that of all the families (see Table 2),
though it is comparable to the Indo-European
PlaNet. This points to the fact that the choice
of consonants within the languages of a family is
far more preferential than it is across the families;
this fact is possibly an outcome of shared ances-
try. In other words, the inventories of genetically
related languages are similar (i.e., they share a lot
of consonants) because they have evolved from the
same parent language through a series of linguis-
tic changes, and the chances that they use a large
number of consonants used by the parent language
is naturally high.
Observation II: We observe a very interesting
relationship between the approximate age of the
language family and the values of ? obtained in
each case (see Table 3). The only anomaly is the
Indo-European branch, which possibly indicates
that this might be much older than it is believed
to be. In fact, a recent study (Balter, 2003) has
shown that the age of this family dates back to
8000 years. If this last argument is assumed to
be true then the values of ? have a one-to-one cor-
respondence with the approximate period of ex-
istence of the language families. As a matter of
fact, this correlation can be intuitively justified ?
the higher is the period of existence of a family, the
higher are the chances of transmission errors lead-
ing to its diversification into smaller subgroups,
and hence, the values of ? comes out to be more
for the older families. It should be noted that the
difference between the values of ? for the language
families are not significant2. Therefore, the afore-
mentioned observation should be interpreted only
as an interesting possibility; more experimentation
is required for making any stronger claim.
4.1 Control Experiment
How could one be sure that the aforementioned
observations are not an obvious outcome of the
construction of the PlaNet or some spurious cor-
relations? To this end, we conduct a control ex-
periment where a set of inventories is randomly
selected from UPSID to represent a family. The
2Note that in order to obtain the best fit for the cumulative
distribution, ? has been varied in steps of 0.005. Therefore,
the values of ? in Table 2 cannot be more accurate than ? ?
0.005. However, in many cases the difference between the
best-fit ? for two language families is exactly 0.005, which
indicates that the difference is not significant.
55
Figure 2: The degree distribution of the different real networks (black dots) along with the fits obtained
from the equation for the optimal values of ? (grey lines).
Families Age (in years) ?
Austronasean 4000 0.030
Niger-Congo 5000 0.035
Sino-Tibetan 6000 0.035
Afro-Asiatic 6000 0.040
Indo-European 4000 (or 8000) 0.055
Table 3: Table showing the relationship between
the age of a family and the value of ?.
number of languages chosen is the same as that of
the PlaNets of the various language families. We
observe that the average value of ? for these ran-
domly constructed PlaNets is 0.068, which, as one
would expect, is close to that of the Combined-
PlaNet. This reinforces the fact that the inherent
proximity among the languages of a real family is
not due to chance.
4.2 Correlation between Families
It can be shown theoretically that if we merge two
PlaNets (say PlaNet1 and PlaNet2) synthesized us-
ing the growth model described here using param-
eters ?1 and ?2, then the ? of the combined PlaNet
can be much greater than both ?1 and ?2 when
there is a low correlation between the degrees of
the consonant nodes between the two PlaNets.
This can be understood as follows. Suppose that
the consonant /k/ is very frequent (i.e., has a high
degree) in PlaNet1, but the consonant /m/ is not.
On the other hand suppose that /m/ is very fre-
quenct in PlaNeT2, but /k/ is not. In the combined
PlaNet the degrees of /m/ and /k/ will even out and
the degree distribution will therefore, be much less
skewed than the original degree distributions of
PlaNet1 and PlaNet2. This is equivalent to the fact
that while ?1 and ?2 were very small, the ? of the
combined PlaNet is quite high. By the same logic
it follows that if the degrees of the consonants are
highly correlated in PlaNet1 and PlaNet2, then the
combined PlaNet will have an ? that is compara-
ble in magnitude to ?1 and ?2. The fact that the
? for the Combined-PlaNet is higher than that of
family-specific PlaNets, therefore, implies that the
correlation between the frequencies of the conso-
nants across language families is not very high.
In order to verify the above observation we esti-
mate the correlation between the frequency of oc-
currence of the consonants for the different lan-
guage family pairs (i.e., how the frequencies of
the consonants /p/, /t/, /k/, /m/, /n/ . . . are corre-
lated across the different families). Table 4 notes
the value of this correlation among the five fami-
lies. The values in Table 4 indicate that, in general,
the families are somewhat weakly correlated with
each other, the average correlation being ? 0.47.
Note that, the correlation between the Afro-
Asiatic and the Niger-Congo families is high not
only because they share the same African origin,
but also due to higher chances of language con-
tacts among their groups of speakers. On the other
hand, the Indo-European and the Sino-Tibetan
families show least correlation because it is usu-
56
Families IE AA NC AN ST
IE ? 0.49 0.48 0.42 0.25
AA 0.49 ? 0.66 0.53 0.43
NC 0.48 0.66 ? 0.55 0.37
AN 0.42 0.53 0.55 ? 0.50
ST 0.25 0.43 0.37 0.50 ?
Table 4: The Pearson?s correlation between the
frequency distributions obtained for the family
pairs. IE: Indo-European, AA: Afro-Asiatic,
NC: Niger-Congo, AN: Austronesian, ST: Sino-
Tibetan.
ally believed that they share absolutely no genetic
connections. Interestingly, similar trends are ob-
served for the values of the parameter ?. If we
combine the languages of the Afro-Asiatic and the
Niger-Congo families and try to fit the new data
then ? turns out to be 0.035 while if we do the same
for the Indo-European and the Sino-Tibetan fam-
ilies then ? is 0.058. For many of the other com-
binations the value of ? and the correlation coeffi-
cient have a one-to-one correspondence. However,
there are clear exceptions also. For instance, if we
combine the Afro-Asiatic and the Indo-European
families then the value of ? is very low (close to
0.04) although the correlation between them is not
very high. The reasons for these exceptions should
be interesting and we plan to further explore this
issue in future.
5 Conclusion
In this paper, we presented a method of network
evolution to capture the emergence of linguistic
diversity that manifests in the five major language
families of the world. How does the growth model,
if at all, captures the process of language dynam-
ics? We argue that preferential attachment is a
high level abstraction of language acquisition as
well as language change. We sketch out two pos-
sible explanations for this fact, both of which are
merely speculations at this point and call for de-
tailed experimentation.
It is a well known fact that the process of lan-
guage acquisition by an individual largely gov-
erns the course of language change in a linguis-
tic community. In the initial years of language
development every child passes through a stage
called babbling during which he/she learns to pro-
duce non-meaningful sequences of consonants and
vowels, some of which are not even used in the
language to which they are exposed (Jakobson,
1968; Locke, 1983). Clear preferences can be
observed for learning certain sounds such as plo-
sives and nasals, whereas fricatives and liquids are
avoided. In fact, this hierarchy of preference dur-
ing the babbling stage follows the cross-linguistic
frequency distribution of the consonants. This in-
nate frequency dependent preference towards cer-
tain phonemes might be because of phonetic rea-
sons (i.e., for articulatory/perceptual benefits). It
can be argued that in the current model, this in-
nate preference gets captured through the process
of preferential attachment.
An alternative explanation could be conceived
of based on the phenomenon of language trans-
mission. Let there be a community of N speak-
ers communicating among themselves by means
of only two consonants say /k/ and /g/. Let the
number of /k/ speakers be m and that of /g/ speak-
ers be n. If we assume that each speaker has l de-
scendants and that language inventories are trans-
mitted with high fidelity then after i generations,
the number of /k/ speakers should be mli and that
of /g/ speakers should be nli. Now if m > n
and l > 1 then for sufficiently large values of i
we have mli ? nli. Stated differently, the /k/
speakers by far outnumbers the /g/ speakers after a
few generations even though the initial difference
between them is quite small. This phenomenon
is similar to that of preferential attachment where
language communities get attached to, i.e., select
consonants that are already highly preferred. In
this context ? can be thought to model the acciden-
tal errors during transmission. Since these errors
accumulate over time, this can intuitively explain
why older language families have a higher value
of ? than the younger ones.
In fact, preferential attachment (PA) is a uni-
versally observed evolutionary mechanism that
is known to shape several physical, biological
and socio-economic systems (Newman, 2003).
This phenomenon has also been called for to ex-
plain various linguistic phenomena (Choudhury
and Mukherjee, to appear). We believe that PA
also provides a suitable abstraction for the mech-
anism of language acquisition. Acquisition of vo-
cabulary and growth of the mental lexicon are few
examples of PA in language acquisition. This
work illustrates another variant of PA applied to
explain the structure of consonant inventories and
their diversification across the language families.
57
References
T. Arita and C. E. Taylor. 1996. A simple model
for the evolution of communication. In L. J. Fo-
gel, P. J. Angeline and T. Ba?ck, editors, The Fifth
Annual Conference On Evolutionary Programming,
405?410. MIT Press.
M. Balter. 2003. Early date for the birth of Indo-
European languages. Science 302(5650), 1490.
A.-L. Baraba?si and R. Albert. 1999. Emergence of
scaling in random networks. Science 286, 509-512.
D. Bickerton. 1990. Language and Species, The Uni-
versity of Chicago Press, Chicago.
B. de Boer. 2000. Self-organization in vowel systems.
Journal of Phonetics, 28(4), 441?465.
R. Ferrer i Cancho and R. V. Sole?. 2001. The small-
world of human language. Proceedings of the Royal
Society of London, Series B, Biological Sciences,
268(1482), 1228?1235.
R. Ferrer i Cancho and R. V. Sole?. 2004. Patterns
in syntactic dependency networks. Phys. Rev. E,
69(051915).
R. G. Gordon (ed.) 2005. Ethnologue: Languages of
the World, Fifteenth edition, SIL International.
M. Haspelmath, M. S. Dryer, D. Gil and B. Comrie
(ed.) 2005. World Atlas of Language Structures,
Oxford University Press.
M. Choudhury, A. Mukherjee, A. Basu and N. Gan-
guly. 2006. Analysis and synthesis of the distri-
bution of consonants over languages: A complex
network approach. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, Main Conference Poster Ses-
sions, 128?135.
M. Choudhury and A. Mukherjee. to appear. The
structure and dynamics of linguistic networks. In N.
Ganguly, A. Deutsch and A. Mukherjee, editors, Dy-
namics on and of Complex Networks: Applications
to Biology, Computer Science, Economics, and the
Social Sciences, Birkhauser, Springer, Boston.
S. N. Dorogovtsev and J. F. F. Mendes. 2001. Lan-
guage as an evolving word web. Proceedings of the
Royal Society of London, Series B, Biological Sci-
ences, 268(1485), 2603?2606.
M. S. Dryer. 1992. The Greenbergian word order cor-
relations. Language, 68, 81?138.
M. D. Hauser, N. Chomsky and W. T. Fitch. 2002. The
faculty of language: What is it, who has it, and how
did it evolve? Science, 298, 1569?1579.
F. Hinskens and J. Weijer. 2003. Patterns of segmen-
tal modification in consonant inventories: a cross-
linguistic study. Linguistics, 41(6), 1041?1084.
R. Jakobson. 1968. Child Language, Aphasia and
Phonological Universals. The Hague: Mouton.
H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai and A.
L. Baraba?si. 2000. The large-scale organization of
metabolic networks. Nature, 406, 651-654.
S. Kirby. 1998. Fitness and the selective adaptation
of language. In J. R. Hurford, M. Studdert-Kennedy
and C. Knight, editors, Approaches to the Evolution
of Language: Social and Cognitive Bases, 359?383.
Cambridge: Cambridge University Press.
P. Ladefoged and I. Maddieson. 1996. Sounds of the
Worlds Languages, Oxford: Blackwell.
B. Lindblom and I. Maddieson. 1988. Phonetic univer-
sals in consonant systems. In L.M. Hyman and C.N.
Li, eds., Language, Speech, and Mind, Routledge,
London, 62?78.
D. Livingstone and C. Fyfe. 1999. Modelling the
evolution of linguistic diversity. In D. Floreano, J.
Nicoud and F. Mondada, editors, ECAL 99, 704?
708, Berlin: Springer-Verlag.
J. L. Locke. 1983. Phonological Acquisition and
Change. Academic Press New York.
I. Maddieson. 1984. Patterns of Sounds, Cambridge
University Press, Cambridge.
D. Nettle. 1999. Is the rate of linguistic change con-
stant? Lingua, 108(2):119?136.
M. E. J. Newman. 2001. Scientific collaboration net-
works. Physical Review E 64, 016131.
M. E. J. Newman. 2003. The structure and function of
complex networks. SIAM Review 45, 167?256.
F. Peruani, M. Choudhury, A. Mukherjee and N. Gan-
guly. 2007. Emergence of a non-scaling degree dis-
tribution in bipartite networks: a numerical and ana-
lytical study. Euro. Phys. Letters 76, 28001 (p1?p6).
S. Pinker. 1994. The Language Instinct, New York:
William Morrow.
E. Pulleyblank. 1993. The typology of Indo-European.
Journal of Indo-European Studies, p. 109.
Jose? J. Ramasco, S. N. Dorogovtsev, and Romualdo
Pastor-Satorras. 2004. Self-organization of collabo-
ration networks. Physical Review E, 70, 036106.
R. V. Sole? , B. C. Murtra, S. Valverde and L. Steels.
2005. Language networks: Their structure, function
and evolution. Santa Fe working paper, 05-12-042.
R. Tomlin. 1986. Basic Word Order: Functional Prin-
ciples, Croom Helm, London.
N. Trubetzkoy. 1931. Die phonologischen systeme.
TCLP 4, 96?116.
B. Vaux and B. Samuel. 2005. Laryngeal markedness
and aspiration Phonology 22(3), 96?116.
58
Coling 2010: Poster Volume, pages 162?170,
Beijing, August 2010
Global topology of word co-occurrence networks:
Beyond the two-regime power-law
Monojit Choudhury
Microsoft Research Lab India
monojitc@microsoft.com
Diptesh Chatterjee
Indian Institute of Technology Kharagpur
diptesh.chh.1987@gmail.com
Animesh Mukherjee
Complex Systems Lagrange Lab, ISI Foundation
animesh.mukherjee@isi.it
Abstract
Word co-occurrence networks are one
of the most common linguistic networks
studied in the past and they are known
to exhibit several interesting topological
characteristics. In this article, we inves-
tigate the global topological properties of
word co-occurrence networks and, in par-
ticular, present a detailed study of their
spectrum. Our experiments reveal cer-
tain universal trends found across the net-
works for seven different languages from
three different language families, which
are neither reported nor explained by any
of the previous studies and models of
word-cooccurrence networks. We hy-
pothesize that since word co-occurrences
are governed by syntactic properties of
a language, the network has much con-
strained topology than that predicted by
the previously proposed growth model. A
deeper empirical and theoretical investiga-
tion into the evolution of these networks
further suggests that they have a core-
periphery structure, where the core hardly
evolves with time and new words are only
attached to the periphery of the network.
These properties are fundamental to the
nature of word co-occurrence across lan-
guages.
1 Introduction
In a natural language, words interact among them-
selves in different ways ? some words co-occur
with certain words at a very high probability
than other words. These co-occurrences are non-
trivial, as in their patterns cannot be inferred from
the frequency distribution of the individual words.
Understanding the structure and the emergence of
these patterns can present us with important clues
and insights about how we evolved this extremely
complex phenomenon, that is language.
In this paper, we present an in-depth study of
the word co-occurrence patterns of a language in
the framework of complex networks. The choice
of this framework is strongly motivated by its
success in explaining various properties of word
co-occurrences previously (Ferrer-i-Cancho and
Sole?, 2001; Ferrer-i-Cancho et al 2007; Kapustin
and Jamsen, 2007). Local properties, such as
the degree distribution and clustering coefficient
of the word co-occurrence networks, have been
thoroughly studied for a few languages (Ferrer-
i-Cancho and Sole?, 2001; Ferrer-i-Cancho et al
2007; Kapustin and Jamsen, 2007) and many in-
teresting conclusions have been drawn. For in-
stance, it has been found that these networks are
small-world in nature and are characterized by a
two regime power-law degree distribution. Efforts
have also been made to explain the emergence of
such a two regime degree distribution through net-
work growth models (Dorogovstev and Mendes,
2001). Although it is tempting to believe that a
lot is known about word co-occurrences, in or-
der to obtain a deeper insight into how these co-
occurrence patterns emerged there are many other
interesting properties that need to be investigated.
One such property is the spectrum of the word co-
162
occurrence network which can provide important
information about its global organization. In fact,
the application of this powerful mathematical ma-
chinery to infer global patterns in linguistic net-
works is rarely found in the literature (few excep-
tions are (Belkin and Goldsmith, 2002; Mukher-
jee et al 2009)). However, note that spectral anal-
ysis has been quite successfully applied in the
analysis of biological and social networks (Baner-
jee and Jost, 2007; Farkas et al 2001).
The aim of the present work is to investigate
the spectral properties of a word co-occurrence
network in order to understand its global struc-
ture. In particular, we study the properties of
seven different languages namely Bangla (Indo-
European family), English (Indo-European fam-
ily), Estonian (Finno-Ugric family), French (Indo-
European family), German (Indo-European fam-
ily), Hindi (Indo-European family) and Tamil
(Dravidian family). Quite importantly, as we shall
see, the most popular growth model proposed by
Dorogovtsev and Mendes (DM) (Dorogovstev and
Mendes, 2001) for explaining the degree distribu-
tion of such a network is not adequate to repro-
duce the spectrum of the network. This observa-
tion holds for all the seven different languages un-
der investigation. We shall further attempt to iden-
tify the precise (linguistic) reasons behind this dif-
ference in the spectrum of the empirical network
and the one reproduced by the model. Finally, as
an additional objective, we shall present a hitherto
unreported deeper analysis of this popular model
and show how its most important parameter is cor-
related to the size of the corpus from which the
empirical network is constructed.
The rest of the paper is laid out as follows.
In section 2, we shall present a brief review of
the previous works on word co-occurrence net-
works. This is followed by a short primer to spec-
tral analysis. In section 4, we outline the construc-
tion methodology of the word co-occurrence net-
works and present the experiments comparing the
spectrum of these real networks with those gen-
erated by the DM model. Section 5 shows how
the most important parameter of the DM model
varies with the size of the corpus from which the
co-occurrence networks are constructed. Finally,
we conclude in section 6 by summarizing our con-
tributions and pointing out some of the implica-
tions of the current work.
2 Word Co-occurrence Networks
In this section, we present a short review of the
earlier works on word co-occurrence networks,
where the nodes are the words and an edge be-
tween two words indicate that the words have co-
occurred in a language in certain context(s). The
most basic and well studied form of word co-
occurrence networks are the word collocation net-
works, where two words are linked by an edge if
they are neighbors (i.e., they collocate) in a sen-
tence (Ferrer-i-Cancho and Sole?, 2001).
In (Ferrer-i-Cancho and Sole?, 2001), the au-
thors study the properties of two types of col-
location networks for English, namely the unre-
stricted and the restricted ones. While in the unre-
stricted network, all the collocation edges are pre-
served, in the restricted one only those edges are
preserved for which the probability of occurrence
of the edge is higher than the case when the two
words collocate independently. They found that
both the networks exhibit small-world properties;
while the average path length between any two
nodes in these networks is small (between 2 and
3), the clustering coefficients are high (0.69 for the
unrestricted and 0.44 for the restricted networks).
Nevertheless, the most striking observation about
these networks is that the degree distributions fol-
low a two regime power-law. The degree distribu-
tion of the 5000 most connected words (i.e., the
kernel lexicon) follow a power-law with an expo-
nent ?3.07, which is very close to that predicted
by the Baraba?si-Albert growth model (Baraba?si
and Albert, 1999). These findings led the au-
thors to argue that the word usage of the human
languages is preferential in nature, where the fre-
quency of a word defines the comprehensibility
and production capability. Thus, higher the us-
age frequency of a word, higher is the probability
that the speakers will be able to produce it eas-
ily and the listeners will comprehend it fast. This
idea is closely related to the recency effect in lin-
guistics (Akmajian, 1995).
Properties of word collocation networks have
also been studied for languages other than En-
glish (Ferrer-i-Cancho et al 2007; Kapustin and
163
Jamsen, 2007). The basic topological characteris-
tics of all these networks (e.g., scale-free, small
world, assortative) are similar across languages
and thus, point to the fact that like Zipf?s law,
these are also linguistic universals whose emer-
gence and existence call for a non-trivial psycho-
linguistic account.
In order to explain the two regime power-
law in word collocation networks, Dorogovtsev
and Mendes (Dorogovstev and Mendes, 2001)
proposed a preferential attachment based growth
model (henceforth referred to as the DM model).
In this model, at every time step t, a new word
(i.e., a node) enters the language (i.e., the net-
work) and connects itself preferentially to one of
the pre-existing nodes. Simultaneously, ct (where
c is a positive constant and a parameter of the
model) new edges are grown between pairs of
old nodes that are chosen preferentially. Through
mathematical analysis and simulations, the au-
thors successfully establish that this model gives
rise to a two regime power-law with exponents
very close to those observed in (Ferrer-i-Cancho
and Sole?, 2001). In fact, for English, the val-
ues kcross (i.e., the point where the two power
law regimes intersect) and kcut (i.e., the point
where the degree distribution cuts the x-axis) ob-
tained from the model are in perfect agreement
with those observed for the empirical network.
Although the DM model is capable of explain-
ing the local topological properties of the word
collocation network, as we shall see in the forth-
coming sections, it is unable to reproduce the
global properties (e.g., the spectrum) of the net-
work.
3 A Primer to Spectral Analysis
Spectral analysis1 is a powerful mathematical
method capable of revealing the global structural
patterns underlying an enormous and complicated
environment of interacting entities. Essentially, it
refers to the systematic investigation of the eigen-
values and the eigenvectors of the adjacency ma-
trix of the network of these interacting entities.
In this section, we shall briefly outline the basic
1The term spectral analysis is also used in the context
of signal processing, where it refers to the study of the fre-
quency spectrum of a signal.
concepts involved in spectral analysis and discuss
some of its applications (see (Chung, 1994) for
details).
A network consisting of n nodes (labeled as
1 through n) can be represented by an n ? n
square matrix A, where the entry aij represents
the weight of the edge from node i to node j. Note
that A, which is known as the adjacency matrix,
is symmetric for an undirected graph and have
binary entries for an unweighted graph. ? is an
eigenvalue of A if there is an n-dimensional vec-
tor x such that
Ax = ?x
Any real symmetric matrix A has n (possibly non-
distinct) eigenvalues ?0 ? ?1 ? . . . ? ?n?1,
and corresponding n eigenvectors that are mutu-
ally orthogonal. The spectrum of a network is
the set of the distinct eigenvalues of the graph and
their corresponding multiplicities. It is a distribu-
tion usually represented in the form of a plot with
the eigenvalues in x-axis and their multiplicities in
the y-axis.
The spectrum of real and random networks dis-
play several interesting properties. Banerjee and
Jost (Banerjee and Jost, 2007) report the spectrum
of several biological networks and show that these
are significantly different from the spectrum of ar-
tificially generated networks. It is worthwhile to
mention here that spectral analysis is also closely
related to Principal Component Analysis and Mul-
tidimensional Scaling. If the first few (say d)
eigenvalues of a matrix are much higher than the
rest of the eigenvalues, then one can conclude that
the rows of the matrix can be approximately rep-
resented as linear combinations of d orthogonal
vectors. This further implies that the correspond-
ing graph has a few motifs (subgraphs) that are re-
peated a large number of time to obtain the global
structure of the graph (Banerjee and Jost, 2009).
In the next section, we shall present a thorough
study of the spectrum of the word co-occurrence
networks across various languages.
4 Experiments and Results
For the purpose of our experiments, we con-
struct word collocation networks for seven dif-
ferent languages namely, Bangla, English, Esto-
164
Figure 1: Cumulative degree distributions for Bangla, English, Estonian, French, German, Hindi and
Tamil respectively. Each red line signifies the degree distribution for the empirical network while each
blue line signifies the one obtained from the DM model.
Lang. Tokens (Mill.) Words KLD c Max. Eig. (Real) Max. Eig. (DM)
English 32.5 97144 0.21 5.0e-4 849.1 756.8Hindi 20.2 99210 0.32 2.3e-4 472.5 329.5Bangla 12.7 100000 0.29 2.0e-3 326.2 245.0German 5.0 159842 0.19 6.3e-5 192.3 110.7Estonian 4.0 100000 0.25 1.1e-4 158.6 124.0Tamil 2.3 75929 0.24 9.9e-4 116.4 73.06French 1.8 100006 0.44 8.0e-5 236.1 170.1
Table 1: Summary of results comparing the structural properties of the empirical networks for the seven
languages and the corresponding best fits (in terms of KLD) obtained from the DM model.
nian, French, German, Hindi and Tamil. We used
the corpora available in the Lipezig Corpora Col-
lection (http://corpora.informatik.uni-leipzig.de/)
for English, Estonian, French and German. The
Hindi, Bangla and Tamil corpora were collected
by crawling some online newspapers. In these net-
works, each distinct word corresponds to a ver-
tex and two vertices are connected by an edge
if the corresponding two words are adjacent in
one or more sentences in the corpus. We assume
the network to be undirected and unweighted (as
in (Ferrer-i-Cancho and Sole?, 2001)).
As a following step, we simulate the DM model
and reproduce the degree distribution of the col-
location networks for the seven languages. We
vary the parameter c in order to minimize the KL
165
divergence (KLD) (Kullback and Leibler, 1951)
between the empirical and the synthesized dis-
tributions and, thereby, obtain the best match.
The results of these experiments are summarized
through Figure 1 and Table 1. The results clearly
show that the DM model is indeed capable of gen-
erating the degree distribution of the collocation
networks to a very close approximation for cer-
tain values of the parameter c (see Table 1 for the
values of c and the corresponding KLD).
Subsequently, for the purpose of spectral anal-
ysis, we construct subgraphs induced by the top
5000 nodes for each of the seven empirical net-
works as well as those generated by the DM model
(i.e., those for which the degree distribution fits
best in terms of KLD with the real data). We then
compute and compare the spectrum of the real
and the synthesized networks (see Figure 2 and
Table 1). It is quite apparent from these results
that the spectra of the empirical networks are sig-
nificantly different from those obtained using the
DM model. In general, the spectral plots indicate
that the adjacency matrices for networks obtained
from the DM model have a higher rank than those
for the empirical networks. Further, in case of the
synthesized networks, the first eigenvalue is sig-
nificantly larger than the second whereas for the
empirical networks the top 3 to 4 eigenvalues are
found to dominate. Interestingly, this property is
observed across all the languages under investiga-
tion.
We believe that the difference in the spectra is
due to the fact that the ordering of the words in
a sentence are strongly governed by the grammar
or the syntax of the language. Words belong to
a smaller set of lexico-syntactic categories, which
are more commonly known as the parts-of-speech
(POS). The co-occurrence patterns of the words
are influenced, primarily, by its POS category. For
instance, nouns are typically preceded by articles
or adjectives, whereas verbs might be preceded by
auxiliary verbs, adverbs or nouns, but never ar-
ticles or adjectives. Therefore, the words ?car?
and ?camera? are more likely to be structurally
similar in the word co-occurrence network, than
?car? and ?jumped?. In general, the local neigh-
borhoods of the words belonging to a particular
POS is expected to be very similar, which means
that several rows in the adjacency matrix will be
very similar to each other. Thus, the matrix is ex-
pected to have low rank.
In fact, this property is not only applicable to
syntax, but also semantics. For instance, even
though adjectives are typically followed by nouns,
semantic constraints make certain adjective-noun
co-occurrences (e.g., ?green leaves?) much more
likely than some others (e.g., ?green dreams? or
?happy leaves?). These notions are at the core of
latent semantics and vector space models of se-
mantics (see, for instance, Turney and Pantel (Tur-
ney and Pantel, 2010) for a recent study). The DM
model, on the other hand, is based on the recency
effect that says that the words which are produced
most recently are easier to remember and there-
fore, easier to produce in the future. Preferential
attachment models the recency effect in word pro-
duction, which perhaps is sufficient to replicate
the degree distribution of the networks. However,
the model fails to explain the global properties,
precisely because it does not take into account
the constraints that govern the distribution of the
words.
It is quite well known that the spectrum of a net-
work can be usually obtained by iteratively pow-
ering the adjacency matrix of the network (aka
power iteration method). Note that if the adja-
cency matrices of the empirical and the synthe-
sized networks are powered even once (i.e., they
are squared)2, their degree distributions match no
longer (see Figure 3). This result further cor-
roborates that although the degree distribution of
a word co-occurrence network is quite appropri-
ately reproduced by the DM model, more global
structural properties remain unexplained. We be-
lieve that word association in human languages
is not arbitrary and therefore, a model which ac-
counts for the clustering of words around their
POS categories might possibly turn out to present
a more accurate explanation of the spectral prop-
erties of the co-occurrence networks.
166
Figure 2: The spectrum for Bangla, English, Estonian, French, German, Hindi and Tamil respectively.
The last plot shows a portion of the spectrum for English magnified around 0 for better visualization.
All the curves are binned distributions with bin size = 100. The blue line in each case is the spectrum
for the network obtained from the DM model while each red line corresponds to the spectrum for the
empirical network.
5 Reinvestigating the DM Model
In this section, we shall delve deeper into explor-
ing the properties of the DM model since it is one
of the most popular and well accepted models for
explaining the emergence of word associations in
a language. In particular, we shall investigate the
influence of the model parameter c on the emer-
gent results.
If we plot the value of the parameter c (from
Table 1) versus the size of the corpora (from Ta-
ble 1) used to construct the empirical networks for
the different languages we find that the two are
highly correlated (see Figure 4).
2Note that this squared network is weighted in nature. We
threshold all edges below the weight 0.07 so that the resultant
network is neither too dense nor too sparse. The value of the
threshold is chosen based on the inspection of the data.
In order to further check the dependence of c
on the corpus size we perform the following ex-
periment. We draw samples of varying corpus
size and construct empirical networks from each
of them. We then simulate the DM model and at-
tempt to reproduce the degree distribution for each
of these empirical networks. In each case, we note
the value c for which the KLD between the empir-
ical and the corresponding synthesized network is
minimum. Figure 5 shows the result of the above
experiment for English. The figure clearly indi-
cates that as the corpus size increases the value of
the parameter c decreases. Similar trends are ob-
served for all the other languages.
In general, one can mathematically prove that
the parameter c is equal to the rate of change of
the average degree of the network with respect to
167
Figure 3: Cumulative degree distribution for the
squared version of the networks for English. The
red line is the degree distribution for the squared
version of the empirical network while the blue
line is degree distribution of the squared version
of the network obtained from the DM model. The
trends are similar for all the other languages.
the time t. The proof is as follows.
At every time step t, the number of new edges
formed is (1+ct). Since each edge contributes to
a total degree of 2 to the network, the sum of the
degrees of all the nodes in the network (ktot) is
ktot = 2
T?
t=1
(1 + ct) = 2T + cT (T + 1) (1)
At every time step, only one new node is added
to the network and therefore the total number of
nodes at the end of time T is exactly equal to T .
Thus the average degree of the network is
?k? = 2T + cT (T + 1)T = 2 + c(T + 1) (2)
The rate of change of average degree is
d?k?
dT = c (3)
and this completes the proof.
In fact, it is also possible to make a precise
empirical estimate of the value of the parameter
c. One can express the average degree of the co-
occurrence networks as the ratio of twice the bi-
gram frequency (i.e., twice the number of edges
in the network) to the unigram frequency (i.e., the
0 5 10 15 20 25 30 350.5
1
1.5
2
2.5
3
3.5
4
4.5
5 x 10?4
Corpus Size(Across Languages)
c
Figure 4: The parameter c versus the corpus size
for the seven languages.
Figure 5: The parameter c versus the corpus size
for English.
number of nodes or unique words in the network).
Therefore, if we can estimate this ratio we can eas-
ily estimate the value of c using equation 3. Let
us denote the total number of distinct bigrams and
unigrams after processing a corpus of size N by
B(N) and W (N) respectively. Hence we have
?k? = 2B(N)W (N) (4)
Further, the number of distinct new unigrams after
Language B(N) W (N) c
English 29.2N.67 59.3N.43 .009N?.20
Hindi 26.2N.66 49.7N.46 .009N?.26
Tamil 1.9N.91 6.4N.71 .207N?.50
Table 2: Summary of expressions for B(N),
W (N) and c for English, Hindi and Tamil.
168
Figure 6: Variation of B(N) and W (N) with N
for English (in doubly-logarithmic scale). The
blue dots correspond to variation of B(N) while
the red dots correspond to the variation of W (N).
processing a corpus of size N is equivalent to T
and therefore
T = W (N) (5)
Sampling experiments across different languages
demonstrate that W (N) and B(N) are of the form
?N? (? < 1) where ? and ? are constants. For
instance, Figure 6 shows in doubly-logarithmic
scale how B(N) and W (N) varies with N for
English. The R2 values obtained as a result of
fitting the B(N) versus N and the W (N) ver-
sus N plots using equations of the form ?N? for
English, Hindi and Tamil are greater than 0.99.
This reflects the high accuracy of the fits. Similar
trends are observed for all the other languages.
Finally, using equations 3, 4 and 5 we have
c = d?k?dT =
d?k?
dN
dN
dT (6)
and plugging the values of B(N) and W (N) in
equation 6 we find that c has the form ?N?? (? <
1) where ? and ? are language dependent positive
constants. The values of c obtained in this way
for three different languages English, Hindi and
Tamil are noted in Table 5.
Thus, we find that as N ? ?, c ? 0. In
other words, as the corpus size grows the number
of distinct new bigrams goes on decreasing and
ultimately reaches (almost) zero for a very large
sized corpus. Now, if one plugs in the values of c
and T obtained above in the expressions for kcross
and kcut in (Dorogovstev and Mendes, 2001), one
observes that limN?? kcrosskcut = 0. This impliesthat as the corpus size becomes very large, the
two-regime power law (almost) converges to a sin-
gle regime with an exponent equal to -3 as is ex-
hibited by the Baraba?si-Albert model (Baraba?si
and Albert, 1999). Therefore, it is reasonable to
conclude that although the DM model provides a
good explanation of the degree distribution of a
word co-occurrence network built from a medium
sized corpora, it does not perform well for very
small or very large sized corpora.
6 Conclusions
In this paper, we have tried to investigate in de-
tail the co-occurrence properties of words in a
language. Some of our important observations
are: (a) while the DM model is able to reproduce
the degree distributions of the word co-occurrence
networks, it is not quite appropriate for explaining
the spectrum of these networks; (b) the parameter
c in the DM model signifies the rate of change of
the average degree of the network with respect to
time; and (c) the DM model does not perform well
in explaining the degree distribution of a word co-
occurrence network when the corpus size is very
large.
It is worthwhile to mention here that our analy-
sis of the DM model leads us to a very important
observation. As N grows, the value of kcut grows
at a much faster rate than the value of kcross and
in the limit N ?? the value of kcut is so high as
compared to kcross that the ratio kcrosskcut becomes(almost) zero. In other words, the kernel lexicon,
formed of the words in the first regime of the two
regime power-law and required to ?say everything
or almost everything? (Ferrer-i-Cancho and Sole?,
2001) in a language, grows quite slowly as new
words creep into the language. In contrast, the pe-
ripheral lexicon making the other part of the two
regime grows very fast as new words enter the lan-
guage. Consequently, it may be argued that since
the kernel lexicon remains almost unaffected, the
effort to learn and retain a language by its speak-
ers increases only negligibly as new words creep
into the language.
169
References
A. Akmajian. Linguistics: An introduction to Lan-
guage and Communication. MIT Press, Cambridge,
MA, 1995.
A. Banerjee and J. Jost. Spectral plots and the repre-
sentation and interpretation of biological data. The-
ory in Biosciences, 126(1), 15-21, 2007.
A. Banerjee and J. Jost. Graph spectra as a system-
atic tool in computational biology. Discrete Applied
Mathematics, 157(10), 2425?2431, 2009.
A.-L. Baraba?si and R. Albert. Emergence of scaling in
random networks. Science, 286, 509-512, 1999.
M. Belkin and J. Goldsmith. Using eigenvectors of
the bigram graph to infer morpheme identity. In
Proceedings of Morphological and Phonological
Learning, Association for Computational Linguis-
tics, 41-47, 2002.
F. R. K. Chung. Spectral Graph Theory. Number 2 in
CBMS Regional Conference Series in Mathematics,
American Mathematical Society, 1994.
S. N. Dorogovstev and J. F .F. Mendes. Language as an
evolving word Web. Proceedings of the Royal Soci-
ety of London B, 268, 2603-2606, 2001.
I. J. Farkas, I. Dere?nyi, A. -L. Baraba?si and T. Vicsek.
Spectra of ?real-world? graphs: Beyond the semi-
circle law, Physical Review E, 64, 026704, 2001.
R. Ferrer-i-Cancho and R. V. Sole?. The small-world of
human language. Proceedings of the Royal Society
of London B, 268, 2261?2266, 2001.
R. Ferrer-i-Cancho, A. Mehler, O. Pustylnikov and
A. D??az-Guilera. Correlations in the organization
of large-scale syntactic dependency networks. In
Proceedings of TextGraphs-2: Graph-Based Algo-
rithms for Natural Language Processing, 65-72, As-
sociation for Computational Linguistics, 2007.
V. Kapustin and A. Jamsen. Vertex degree distribution
for the graph of word co-occurrences in Russian. In
Proceedings of TextGraphs-2: Graph-Based Algo-
rithms for Natural Language Processing, 89-92, As-
sociation for Computational Linguistics, 2007.
S. Kullback and R. A. Leibler. On information and
sufficiency. Annals of Mathematical Statistics 22(1),
79-86, 1951.
A. Mukerjee, M. Choudhury and R. Kannan. Discov-
ering global patterns in linguistic networks through
spectral analysis: A case study of the consonant in-
ventories. In Proceedings of EACL, 585?593, Asso-
ciation for Computational Linguistics, 2009.
P. D. Turney and P. Pantel. From frequency to meaning:
Vector space models of semantics. In JAIR, 37, 141-
188, 2010.
170
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1020?1029,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
That?s sick dude!:
Automatic identification of word sense change across different timescales
Sunny Mitra
1
, Ritwik Mitra
1
, Martin Riedl
2
,
Chris Biemann
2
, Animesh Mukherjee
1
, Pawan Goyal
1
1
Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India ? 721302
2
FG Language Technology, Computer Science Department, TU Darmstadt, Germany
1
{sunnym,ritwikm,animeshm,pawang}@cse.iitkgp.ernet.in
2
{riedl,biem}@cs.tu-darmstadt.de
Abstract
In this paper, we propose an unsupervised
method to identify noun sense changes
based on rigorous analysis of time-varying
text data available in the form of millions
of digitized books. We construct distribu-
tional thesauri based networks from data
at different time points and cluster each
of them separately to obtain word-centric
sense clusters corresponding to the differ-
ent time points. Subsequently, we com-
pare these sense clusters of two different
time points to find if (i) there is birth of
a new sense or (ii) if an older sense has
got split into more than one sense or (iii)
if a newer sense has been formed from the
joining of older senses or (iv) if a partic-
ular sense has died. We conduct a thor-
ough evaluation of the proposed method-
ology both manually as well as through
comparison with WordNet. Manual eval-
uation indicates that the algorithm could
correctly identify 60.4% birth cases from
a set of 48 randomly picked samples and
57% split/join cases from a set of 21 ran-
domly picked samples. Remarkably, in
44% cases the birth of a novel sense is
attested by WordNet, while in 46% cases
and 43% cases split and join are respec-
tively confirmed by WordNet. Our ap-
proach can be applied for lexicography, as
well as for applications like word sense
disambiguation or semantic search.
1 Introduction
Two of the fundamental components of a natu-
ral language communication are word sense dis-
covery (Jones, 1986) and word sense disambigua-
tion (Ide and Veronis, 1998). While discovery
corresponds to acquisition of vocabulary, disam-
biguation forms the basis of understanding. These
two aspects are not only important from the per-
spective of developing computer applications for
natural languages but also form the key compo-
nents of language evolution and change.
Words take different senses in different contexts
while appearing with other words. Context plays
a vital role in disambiguation of word senses as
well as in the interpretation of the actual mean-
ing of words. For instance, the word ?bank? has
several distinct interpretations, including that of a
?financial institution? and the ?shore of a river.?
Automatic discovery and disambiguation of word
senses from a given text is an important and chal-
lenging problem which has been extensively stud-
ied in the literature (Jones, 1986; Ide and Vero-
nis, 1998; Sch?utze, 1998; Navigli, 2009). How-
ever, another equally important aspect that has not
been so far well investigated corresponds to one
or more changes that a word might undergo in its
sense. This particular aspect is getting increas-
ingly attainable as more and more time-varying
text data become available in the form of millions
of digitized books (Goldberg and Orwant, 2013)
gathered over the last centuries. As a motivat-
ing example one could consider the word ?sick?
? while according to the standard English dictio-
naries the word is normally used to refer to some
sort of illness, a new meaning of ?sick? refer-
ring to something that is ?crazy? or ?cool? is cur-
rently getting popular in the English vernacular.
This change is further interesting because while
traditionally ?sick? has been associated to some-
thing negative in general, the current meaning as-
sociates positivity with it. In fact, a rock band
by the name of ?Sick Puppies? has been founded
which probably is inspired by the newer sense of
the word sick. The title of this paper has been
motivated by the above observation. Note that
this phenomena of change in word senses has ex-
isted ever since the beginning of human commu-
nication (Bamman and Crane, 2011; Michel et
1020
al., 2011; Wijaya and Yeniterzi, 2011; Mihalcea
and Nastase, 2012); however, with the advent of
modern technology and the availability of huge
volumes of time-varying data it now has become
possible to automatically track such changes and,
thereby, help the lexicographers in word sense dis-
covery, and design engineers in enhancing vari-
ous NLP/IR applications (e.g., disambiguation, se-
mantic search etc.) that are naturally sensitive to
change in word senses.
The above motivation forms the basis of the
central objective set in this paper, which is to de-
vise a completely unsupervised approach to track
noun sense changes in large texts available over
multiple timescales. Toward this objective we
make the following contributions: (a) devise a
time-varying graph clustering based sense induc-
tion algorithm, (b) use the time-varying sense
clusters to develop a split-join based approach for
identifying new senses of a word, and (c) evalu-
ate the performance of the algorithms on various
datasets using different suitable approaches along
with a detailed error analysis. Remarkably, com-
parison with the English WordNet indicates that
in 44% cases, as identified by our algorithm, there
has been a birth of a completely novel sense, in
46% cases a new sense has split off from an older
sense and in 43% cases two or more older senses
have merged in to form a new sense.
The remainder of the paper is organized as fol-
lows. In the next section we present a short re-
view of the literature. In Section 3 we briefly
describe the datasets and outline the process of
co-occurrence graph construction. In Section 4
we present an approach based on graph cluster-
ing to identify the time-varying sense clusters and
in Section 5 we present the split-merge based ap-
proach for tracking word sense changes. Evalu-
ation methods are summarized in Section 6. Fi-
nally, conclusions and further research directions
are outlined in Section 7.
2 Related work
Word sense disambiguation as well as word sense
discovery have both remained key areas of re-
search right from the very early initiatives in nat-
ural language processing research. Ide and Vero-
nis (1998) present a very concise survey of the his-
tory of ideas used in word sense disambiguation;
for a recent survey of the state-of-the-art one can
refer to (Navigli, 2009). Some of the first attempts
to automatic word sense discovery were made by
Karen Sp?arck Jones (1986); later in lexicography,
it has been extensively used as a pre-processing
step for preparing mono- and multi-lingual dictio-
naries (Kilgarriff and Tugwell, 2001; Kilgarriff,
2004). However, as we have already pointed out
that none of these works consider the temporal as-
pect of the problem.
In contrast, the current study, is inspired by
works on language dynamics and opinion spread-
ing (Mukherjee et al, 2011; Maity et al, 2012;
Loreto et al, 2012) and automatic topic detection
and tracking (Allan et al, 1998). However, our
work differs significantly from those proposed in
the above studies. Opinion formation deals with
the self-organisation and emergence of shared vo-
cabularies whereas our work focuses on how the
different senses of these vocabulary words change
over time and thus become ?out-of-vocabulary?.
Topic detection involves detecting the occurrence
of a new event such as a plane crash, a murder, a
jury trial result, or a political scandal in a stream
of news stories from multiple sources and track-
ing is the process of monitoring a stream of news
stories to find those that track (or discuss) the
same event. This is done on shorter timescales
(hours, days), whereas our study focuses on larger
timescales (decades, centuries) and we are inter-
ested in common nouns, verbs and adjectives as
opposed to events that are characterized mostly by
named entities. Other similar works on dynamic
topic modelling can be found in (Blei and Laf-
ferty, 2006; Wang and McCallum, 2006). Google
books n-gram viewer
1
is a phrase-usage graphing
tool which charts the yearly count of selected letter
combinations, words, or phrases as found in over
5.2 million digitized books. It only reports fre-
quency of word usage over the years, but does not
give any correlation among them as e.g., in (Heyer
et al, 2009), and does not analyze their senses.
A few approaches suggested by (Bond et al,
2009; P?a?akk?o and Lind?en, 2012) attempt to aug-
ment WordNet synsets primarily using methods
of annotation. Another recent work by Cook et
al. (2013) attempts to induce word senses and then
identify novel senses by comparing two different
corpora: the ?focus corpora? (i.e., a recent version
of the corpora) and the ?reference corpora? (older
version of the corpora). However, this method
is limited as it only considers two time points to
1
https://books.google.com/ngrams
1021
identify sense changes as opposed to our approach
which is over a much larger timescale, thereby, ef-
fectively allowing us to track the points of change
and the underlying causes. One of the closest
work to what we present here has been put forward
by (Tahmasebi et al, 2011), where the authors an-
alyze a newspaper corpus containing articles be-
tween 1785 and 1985. The authors mainly report
the frequency patterns of certain words that they
found to be candidates for change; however a de-
tailed cause analysis as to why and how a particu-
lar word underwent a sense change has not been
demonstrated. Further, systematic evaluation of
the results obtained by the authors has not been
provided.
All the above points together motivated us to
undertake the current work where we introduce,
for the first time, a completely unsupervised and
automatic method to identify the change of a word
sense and the cause for the same. Further, we also
present an extensive evaluation of the proposed al-
gorithm in order to test its overall accuracy and
performance.
3 Datasets and graph construction
In this section, we outline a brief description
of the dataset used for our experiments and
the graph construction procedure. The primary
source of data have been the millions of digitized
books made available through the Google Book
project (Goldberg and Orwant, 2013). The Google
Book syntactic n-grams dataset provides depen-
dency fragment counts by the years. However, in-
stead of using the plain syntactic n-grams, we use
a far richer representation of the data in the form of
a distributional thesaurus (Lin, 1997; Rychl?y and
Kilgarriff, 2007). In specific, we prepare a distri-
butional thesaurus (DT) for each of the time peri-
ods separately and subsequently construct the re-
quired networks. We briefly outline the procedure
of thesauri construction here referring the reader
to (Riedl and Biemann, 2013) for further details.
In this approach, we first extract each word and a
set of its context features, which are formed by la-
beled and directed dependency parse edges as pro-
vided in the dataset. Following this, we compute
the frequencies of the word, the context and the
words along with their context. Next we calculate
the lexicographer?s mutual information LMI (Kil-
garriff, 2004) between a word and its features and
retain only the top 1000 ranked features for ev-
ery word. Finally, we construct the DT network as
follows: each word is a node in the network and
the edge weight between two nodes is defined as
the number of features that the two corresponding
words share in common.
4 Tracking sense changes
The basic idea of our algorithm for tracking sense
changes is as follows. If a word undergoes a
sense change, this can be detected by comparing
its senses obtained from two different time pe-
riods. Since we aim to detect this change au-
tomatically, we require distributional representa-
tions corresponding to word senses for different
time periods. We, therefore, utilize the basic hy-
pothesis of unsupervised sense induction to in-
duce the sense clusters over various time periods
and then compare these clusters to detect sense
change. The basic premises of the ?unsupervised
sense induction? are briefly described below.
4.1 Unsupervised sense induction
We use the co-occurrence based graph clustering
framework introduced in (Biemann, 2006). The
algorithm proceeds in three basic steps. Firstly,
a co-occurrence graph is created for every target
word found in DT. Next, the neighbourhood/ego
graph is clustered using the Chinese Whispers
(CW) algorithm (see (McAuley and Leskovec,
2012) for similar approaches). The algorithm, in
particular, produces a set of clusters for each target
word by decomposing its open neighborhood. We
hypothesize that each different cluster corresponds
to a particular sense of the target word. For a de-
tailed description, the reader is referred to (Bie-
mann, 2011).
If a word undergoes sense change, this can be
detected by comparing the sense clusters obtained
from two different time periods by the algorithm
outlined above. For this purpose, we use statis-
tics from the DT corresponding to two different
time intervals, say tv
i
and tv
j
. We then run the
sense induction algorithm over these two different
datasets. Now, for a given word w that appears
in both the datasets, we get two different set of
clusters, say C
i
and C
j
. Without loss of gener-
ality, let us assume that our algorithm detects m
sense clusters for the word w in tv
i
and n sense
clusters in tv
j
. Let C
i
= {s
i1
, s
i2
, . . . , s
im
} and
C
j
= {s
j1
, s
j2
, . . . , s
jn
}, where s
kz
denotes z
th
sense cluster for word w during time interval tv
k
.
1022
We next describe our algorithm for detecting sense
change from these sets of sense clusters.
4.2 Split, join, birth and death
We hypothesize that word w can undergo sense
change from one time interval (tv
i
) to another
(tv
j
) as per one of the following scenarios:
Split A sense cluster s
iz
in tv
i
splits into two (or
more) sense clusters, s
jp
1
and s
jp
2
in tv
j
Join Two sense clusters s
iz
1
and s
iz
2
in tv
i
join to
make a single cluster s
jp
in tv
j
Birth A new sense cluster s
jp
appears in tv
j
,
which was absent in tv
i
Death A sense cluster s
iz
in tv
i
dies out and does
not appear in tv
j
To detect split, join, birth or death, we build an
(m+1)? (n+1) matrix I to capture the intersec-
tion between sense clusters of two different time
periods. The first m rows and n columns corre-
spond to the sense clusters in tv
i
and tv
j
espec-
tively. We append an additional row and column to
capture the fraction of words, which did not show
up in any of the sense clusters in another time in-
terval. So, an element I
kl
of the matrix
? 1 ? k ? m, 1 ? l ? n: denotes the frac-
tion of words in a newer sense cluster s
jl
,
that were also present in an older sense clus-
ter s
ik
.
? k = m + 1, 1 ? l ? n: denotes the fraction
of words in the sense cluster s
jl
, that were not
present in any of the m clusters in tv
i
.
? 1 ? k ? m, l = n + 1: denotes the fraction
of words in the sense cluster s
ik
, that did not
show up in any of the n clusters in tv
j
.
Thus, the matrix I captures all the four possible
scenarios for sense change. Since we can not
expect a perfect split, birth etc., we used certain
threshold values to detect if a candidate word is
undergoing sense change via one of these four
cases. In Figure 1, as an example, we illustrate
the birth of a new sense for the word ?compiler?.
4.3 Multi-stage filtering
To make sure that the candidate words obtained
via our algorithm are meaningful, we applied
multi-stage filtering to prune the candidate word
list. The following criterion were used for the fil-
tering:
Stage 1 We utilize the fact that the CW algorithm
is non-deterministic in nature. We apply CW
three times over the source and target time inter-
vals. We obtain the candidate word lists using our
algorithm for the three runs, then take the inter-
section to output those words, which came up in
all the three runs.
Stage 2 From the above list, we retain only those
candidate words, which have a part-of-speech tag
?NN? or ?NNS?, as we focus on nouns for this
work.
Stage 3 We sort the candidate list obtained in
Stage 2 as per their occurrence in the first time
period. Then, we remove the top 20% and the
bottom 20% words from this list. Therefore, we
consider the torso of the frequency distribution
which is the most informative part for this type
of an analysis.
5 Experimental framework
For our experiments, we utilized DTs created for
8 different time periods: 1520-1908, 1909-1953,
1954-1972, 1973-1986, 1987-1995, 1996-2001,
2002-2005 and 2006-2008 (Riedl et al, 2014).
The time periods were set such that the amount
of data in each time period is roughly the same.
We will also use T
1
to T
8
to denote these time pe-
riods. The parameters for CW clustering were set
as follows. The size of the neighbourhood (N )
to be clustered was set to 200. The parameter n
regulating the edge density in this neighbourhood
was set to 200 as well. The parameter a was set to
lin, which corresponds to favouring smaller clus-
ters by hub downweighing
2
. The threshold values
used to detect the sense changes were as follows.
For birth, at least 80% words of the target cluster
should be novel. For split, each split cluster should
have at least 30% words of the source cluster and
the total intersection of all the split clusters should
be > 80%. The same parameters were used for the
join and death case with the interchange of source
and target clusters.
5.1 Signals of sense change
Making comparisons between all the pairs of time
periods gave us 28 candidate words lists. For
2
data available at http://sf.net/p/jobimtext/
wiki/LREC2014_Google_DT/
1023
Figure 1: Example of the birth of a new sense for the word ?compiler?
each of these comparison, we applied the multi-
stage filtering to obtain the pruned list of candidate
words. Table 1 provides some statistics about the
number of candidate words obtained correspond-
ing to the birth case. The rows correspond to the
source time-period and the columns correspond to
the target time periods. An element of the table
shows the number of candidate words obtained
by comparing the corresponding source and target
time periods.
Table 1: Number of candidate birth senses be-
tween all time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
8
T
1
2498 3319 3901 4220 4238 4092 3578
T
2
1451 2330 2789 2834 2789 2468
T
3
917 1460 1660 1827 1815
T
4
517 769 1099 1416
T
5
401 818 1243
T
6
682 1107
T
7
609
The table clearly shows a trend. For most of
the cases, the number of candidate birth senses
tends to increase as we go from left to right. Sim-
ilarly, this number decreases as we go down in
the table. This is quite intuitive since going from
left to right corresponds to increasing the gap be-
tween two time periods while going down cor-
responds to decreasing this gap. As the gap in-
creases (decreases), one would expect more (less)
new senses coming in. Even while moving diago-
nally, the candidate words tend to decrease as we
move downwards. This corresponds to the fact
that the number of years in the time periods de-
creases as we move downwards, and therefore, the
gap also decreases.
5.2 Stability analysis & sense change location
Formally, we consider a sense change from tv
i
to tv
j
stable if it was also detected while com-
paring tv
i
with the following time periods tv
k
s.
This number of subsequent time periods, where
the same sense change is detected, helps us to de-
termine the age of a new sense. Similarly, for a
candidate sense change from tv
i
to tv
j
, we say that
the location of the sense change is tv
j
if and only
if that sense change does not get detected by com-
paring tv
i
with any time interval tv
k
, intermediate
between tv
i
and tv
j
.
Table 1 gives a lot of candidate words for sense
change. However, not all the candidate words
were stable. Thus, it was important to prune these
results using stability analysis. Also, it is to be
noted that these results do not pin-point to the ex-
act time-period, when the sense change might have
taken place. For instance, among the 4238 candi-
date birth sense detected by comparing T
1
and T
6
,
many of these new senses might have come up in
between T
2
to T
5
as well. We prune these lists fur-
ther based on the stability of the sense, as well as
to locate the approximate time interval, in which
the sense change might have occurred.
Table 2 shows the number of stable (at least
twice) senses as well as the number of stable
sense changes located in that particular time pe-
riod. While this decreases recall, we found this to
be beneficial for the accuracy of the method.
Once we were able to locate the senses as well
as to find the age of the senses, we attempted to
1024
Table 2: Number of candidate birth senses ob-
tained for different time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
1
2498 3319 3901 4220 4238 4092
stable 537 989 1368 1627 1540 1299
located 537 754 772 686 420 300
T
2
1451 2330 2789 2834 2789
stable 343 718 938 963 810
located 343 561 517 357 227
select some representative words and plotted them
on a timeline as per the birth period and their age
in Figure 2. The source time period here is 1909-
1953.
6 Evaluation framework
During evaluation, we considered the clusters ob-
tained using the 1909-1953 time-slice as our refer-
ence and attempted to track sense change by com-
paring these with the clusters obtained for 2002-
2005. The sense change detected was categorized
as to whether it was a new sense (birth), a single
sense got split into two or more senses (split) or
two or more senses got merged (join) or a particu-
lar sense died (death). We present a few instances
of the resulting clusters in the paper and refer the
reader to the supplementary material
3
for the rest
of the results.
6.1 Manual evaluation
The algorithm detected a lot of candidate words
for the cases of birth, split/join as well as death.
Since it was difficult to go through all the candi-
date sense changes for all the comparisons man-
ually, we decided to randomly select some can-
didate words, which were flagged by our algo-
rithm as undergoing sense change, while compar-
ing 1909-1953 and 2002-2005 DT. We selected 48
random samples of candidate words for birth cases
and 21 random samples for split/join cases. One
of the authors annotated each of the birth cases
identifying whether or not the algorithm signalled
a true sense change while another author did the
same task for the split/join cases. The accuracy as
per manual evaluation was found to be 60.4% for
the birth cases and 57% for the split/join cases.
Table 3 shows the evaluation results for a few
candidate words, flagged due to birth. Columns
3
http://cse.iitkgp.ac.in/resgrp/cnerg/
acl2014_wordsense/
correspond to the candidate words, words obtained
in the cluster of each candidate word (we will use
the term ?birth cluster? for these words, hence-
forth), which indicated a new sense, the results
of manual evaluation as well as the possible sense
this birth cluster denotes.
Table 4 shows the corresponding evaluation re-
sults for a few candidate words, flagged due to
split or join.
A further analysis of the words marked due
to birth in the random samples indicates that
there are 22 technology-related words, 2 slangs,
3 economics related words and 2 general words.
For the split-join case we found that there are
3 technology-related words while the rest of the
words are general. Therefore one of the key ob-
servations is that most of the technology related
words (where the neighborhood is completely
new) could be extracted from our birth results. In
contrast, for the split-join instances most of the re-
sults are from the general category since the neigh-
borhood did not change much here; it either got
split or merged from what it was earlier.
6.2 Automated evaluation with WordNet
In addition to manual evaluation, we also per-
formed automated evaluation for the candidate
words. We chose WordNet for automated evalua-
tion because not only does it have a wide coverage
of word senses but also it is being maintained and
updated regularly to incorporate new senses. We
did this evaluation for the candidate birth, join and
split sense clusters obtained by comparing 1909-
1953 time period with respect to 2002-2005. For
our evaluation, we developed an aligner to align
the word clusters obtained with WordNet senses.
The aligner constructs a WordNet dictionary for
the purpose of synset algnment. The CW clus-
ter is then aligned to WordNet synsets by compar-
ing the clusters with WordNet graph and the synset
with the maximum alignment score is returned as
the output. In summary, the aligner tool takes as
input the CW cluster and returns a WordNet synset
id that corresponds to the cluster words. The eval-
uation settings were as follows:
Birth: For a candidate word flagged as birth, we
first find out the set of all WordNet synset ids for
its CW clusters in the source time period (1909-
1953 in this case). Let S
init
denote the union of
these synset ids. We then find WordNet synset id
for its birth-cluster, say s
new
. Then, if s
new
/?
1025
Figure 2: Examples of birth senses placed on a timeline as per their location as well as age
Table 3: Manual evaluation for seven randomly chosen candidate birth clusters between time periods
1909-1953 and 2002-2005
Sl Candidate birth cluster Evaluation judgement,
No. Word comments
1 implant gel, fibre, coatings, cement, materials, metal, filler No, New set of words but
silicone, composite, titanium, polymer, coating similar sense already existed
2 passwords browsers, server, functionality, clients, workstation Yes, New sense related
printers, software, protocols, hosts, settings, utilities to ?a computer sense?
3 giants multinationals, conglomerates, manufacturers Yes, New sense as ?an
corporations, competitors, enterprises, companies organization with very great
businesses, brands, firms size or force?
4 donation transplantation, donation, fertilization, transfusions Yes, The new usage of donation
transplant, transplants, insemination, donors, donor ... associated with body organs etc.
5 novice negro, fellow, emigre, yankee, realist, quaker, teen No, this looks like a false
male, zen, lady, admiring, celebrity, thai, millionaire ... positive
6 partitions server, printers, workstation, platforms, arrays Yes, New usage related to
modules, computers, workstations, kernel ... the ?computing? domain
7 yankees athletics, cubs, tigers, sox, bears, braves, pirates Yes, related to the ?New
cardinals, dodgers, yankees, giants, cardinals ... York Yankees? team
S
init
, it implies that this is a new sense that was
not present in the source clusters and we call it a
?success? as per WordNet.
Join: For the join case, we find WordNet synset
ids s
1
and s
2
for the clusters obtained in the
source time period and s
new
for the join cluster
in the target time period. If s
1
6= s
2
and s
new
is
either s
1
or s
2
, we call it a ?success?.
Split: For the split case, we find WordNet synset
id s
old
for the source cluster and synset ids s
1
and s
2
for the target split clusters. If s
1
6= s
2
and either s
1
, or s
2
retains the id s
old
, we call it a
?success?.
Table 5 show the results of WordNet based eval-
uation. In case of birth we observe a success of
Table 5: Results of the automatic evaluation using
WordNet
Category No. of Candidate Words Success Cases
Birth 810 44%
Split 24 46%
Join 28 43%
44% while for split and join we observe a success
of 46% and 43% respectively. We then manually
verified some of the words that were deemed as
successes, as well as investigated WordNet sense
they were mapped to. Table 6 shows some of the
words for which the evaluation detected success
along with WordNet senses. Clearly, the cluster
words correspond to a newer sense for these words
1026
Table 4: Manual evaluation for five randomly chosen candidate split/join clusters between time periods
1909-1953 and 2002-2005
Sl Candidate Source and target clusters
No. Word
1 intonation S: whisper, glance, idioms, gesture, chant, sob, inflection, diction, sneer, rhythm, accents ...
(split) T
1
: nod, tone, grimace, finality, gestures, twang, shake, shrug, irony, scowl, twinkle ...
T
2
: accents, phrase, rhythm, style, phonology, diction, utterance, cadence, harmonies ...
Yes, T
1
corresponds to intonation in normal conversations while T
2
corresponds to the use of accents in
formal and research literature
2 diagonal S: coast, edge, shoreline, coastline, border, surface, crease, edges, slope, sides, seaboard ...
(split) T
1
: circumference, center, slant, vertex, grid, clavicle, margin, perimeter, row, boundary ..
T
2
: border, coast, seaboard, seashore, shoreline, waterfront, shore, shores, coastline, coasts
Yes, the split T
1
is based on mathematics where as T
2
is based on geography
3 mantra S
1
: sutra, stanza, chanting, chants, commandments, monologue, litany, verse, verses ...
(join) S
2
: praise, imprecation, benediction, praises, curse, salutation, benedictions, eulogy ...
T : blessings, spell, curses, spells, rosary, prayers, blessing, prayer, benediction ...
Yes, the two seemingly distinct senses of mantra - a contextual usage for chanting and prayer (S
1
)
and another usage in its effect - salutations, benedictions (S
2
) have now merged in T .
4 continuum S: circumference, ordinate, abscissa, coasts, axis, path, perimeter, arc, plane axis ...
(split) T
1
: roadsides, corridors, frontier, trajectories, coast, shore, trail, escarpment, highways ...
T
2
: arc, ellipse, meridians, equator, axis, axis, plane, abscissa, ordinate, axis, meridian ....
Yes, the split S
1
denotes the usage of ?continuum? with physical objects while the
the split S
2
corresponds to its usages in mathematics domain.
5 headmaster S
1
: master, overseer, councillor, chancellor, tutors, captain, general, principal ...
(join) S
2
: mentor, confessor, tutor, founder, rector, vicar, graduate, counselor, lawyer ...
T : chaplain, commander, surveyor, coordinator, consultant, lecturer, inspector ...
No, it seems a false positive
and the mapped WordNet synset matches the birth
cluster to a very high degree.
6.3 Evaluation with a slang list
Slangs are words and phrases that are regarded as
very informal, and are typically restricted to a par-
ticular context. New slang words come up every
now and then, and this plays an integral part in the
phenomena of sense change. We therefore decided
to perform an evaluation as to how many slang
words were being detected by our candidate birth
clusters. We used a list of slangs available from
the slangcity website
4
. We collected slangs for the
years 2002-2005 and found the intersection with
our candidate birth words. Note that the website
had a large number of multi-word expressions that
we did not consider in our study. Further, some
of the words appeared as either erroneous or very
transient (not existing more than a few months) en-
tires, which had to be removed from the list. All
these removal left us with a very little space for
comparison; however, despite this we found 25
slangs from the website that were present in our
birth results, e.g. ?bum?, ?sissy?, ?thug?, ?dude? etc.
4
http://slangcity.com/email_archive/
index_2003.htm
6.4 Evaluation of candidate death clusters
Much of our evaluation was focussed on the birth
sense clusters, mainly because these are more in-
teresting from a lexicographic perspective. Addi-
tionally, the main theme of this work was to de-
tect new senses for a given word. To detect a
true death of a sense, persistence analysis was re-
quired, that is, to verify if the sense was persist-
ing earlier and vanished after a certain time period.
While such an analysis goes beyond the scope of
this paper, we selected some interesting candidate
?death? senses. Table 7 shows some of these inter-
esting candidate words, their death cluster along
with the possible vanished meaning, identified by
the authors. While these words are still used in a
related sense, the original meaning does not exist
in the modern usage.
7 Conclusions
In this paper, we presented a completely unsu-
pervised method to detect word sense changes
by analyzing millions of digitized books archived
spanning several centuries. In particular, we con-
structed DT networks over eight different time
windows, clustered these networks and compared
these clusters to identify the emergence of novel
1027
Table 6: Example of randomly chosen candidate birth clusters mapped to WordNet
Sl Candidate birth cluster Synset Id,
No. Word WordNet sense
1 macro code, query, handler, program, procedure, subroutine 6582403, a set sequence of steps,
module, script part of larger computer program
2 caller browser, compiler, sender, routers, workstation, cpu 4175147, a computer that
host, modem, router, server provides client stations with access to files
3 searching coding, processing, learning, computing, scheduling 1144355, programming: setting an
planning, retrieval, routing, networking, navigation order and time for planned events
4 hooker bitch, whore, stripper, woman slut, prostitute 10485440, a woman who
girl, dancer ... engages in sexual intercourse for money
5 drones helicopters, fighters, rockets, flights, planes 4264914, a craft capable of
vehicles, bomber, missions, submarines ... traveling in outer space
6 amps inverters, capacitor, oscillators, switches, mixer 2955247, electrical device characterized
transformer, windings, capacitors, circuits ... by its capacity to store an electric charge
7 compilers interfaces, algorithms, programming, software 6566077, written programs pertaining
modules, libraries, routines, tools, utilities ... to the operation of a computer system
Table 7: Some representative examples for candidate death sense clusters
Sl Candidate death cluster Vanished meaning
No. Word
1 slop jeans, velveteen, tweed, woollen, rubber, sealskin, wear clothes and bedding supplied to
oilskin, sheepskin, velvet, calico, deerskin, goatskin, cloth ... sailors by the navy
2 blackmail subsidy, rent, presents, tributes, money, fine, bribes Origin: denoting protection money
dues, tolls, contributions, contribution, customs, duties ... levied by Scottish chiefs
3 repertory dictionary, study, compendium, bibliography, lore, directory Origin: denoting an index
catalogues, science, catalog, annals, digest, literature ... or catalog: from late Latin repertorium
4 phrasing contour, outline, construction, handling, grouping, arrangement in the sense ?style or manner of
structure, modelling, selection, form ... expression?: via late Latin Greek phrasis
senses. The performance of our method has been
evaluated manually as well as by comparison with
WordNet and a list of slang words. Through man-
ual evaluation we found that the algorithm could
correctly identify 60.4% birth cases from a set of
48 random samples and 57% split/join cases from
a set of 21 randomly picked samples. Quite strik-
ingly, we observe that (i) in 44% cases the birth of
a novel sense is attested by WordNet, (ii) in 46%
cases the split of an older sense is signalled on
comparison with WordNet and (iii) in 43% cases
the join of two senses is attested by WordNet.
These results might have strong lexicographic im-
plications ? even if one goes by very moderate es-
timates almost half of the words would be candi-
date entries in WordNet if they were not already
part of it. This method can be extremely useful
in the construction of lexico-semantic networks
for low-resource languages, as well as for keeping
lexico-semantic resources up to date in general.
Future research directions based on this work
are manifold. On one hand, our method can be
used by lexicographers in designing new dictio-
naries where candidate new senses can be semi-
automatically detected and included, thus greatly
reducing the otherwise required manual effort.
On the other hand, this method can be directly
used for various NLP/IR applications like seman-
tic search, automatic word sense discovery as well
as disambiguation. For semantic search, taking
into account the newer senses of the word can in-
crease the relevance of the query result. Similarly,
a disambiguation engine informed with the newer
senses of a word can increase the efficiency of
disambiguation, and recognize senses uncovered
by the inventory that would otherwise have to be
wrongly assigned to covered senses. In addition,
this method can be also extended to the ?NNP?
part-of-speech (i.e., named entities) to identify
changes in role of a person/place. Furthermore,
it would be interesting to apply this method to lan-
guages other than English and to try to align new
senses of cognates across languages.
Acknowledgements
AM would like to thank DAAD for supporting the
faculty exchange programme to TU Darmstadt.
PG would like to thank Google India Private Ltd.
for extending travel support to attend the confer-
ence. MR and CB have been supported by an IBM
SUR award and by LOEWE as part of the research
center Digital Humanities.
1028
References
J. Allan, R. Papka and V. Lavrenko. 1998. On-line
new event detection and tracking. In proceedings of
SIGIR, 37?45, Melbourne, Australia.
D. Bamman and G. Crane. 2011. Measuring Historical
Word Sense Variation. In proceedings of JCDL, 1?
10, New York, NY, USA.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, 73?80, New York, USA.
C. Biemann. 2011. Structure Discovery in Natural
Language. Springer Heidelberg Dordrecht London
New York. ISBN 978-3-642-25922-7.
D. Blei and J. Lafferty. 2006. Dynamic topic mod-
els. In proceedings of ICML, 113?120, Pittsburgh,
Pennsylvania.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayash and K. Kanzaki. 2009. Enhancing the
Japanese WordNet. In proceedings of workshop on
Asian Language Resources, 1?8, Suntec, Singapore.
P. Cook, J. H. Lau, M. Rundell, D. McCarthy, T. Bald-
win. 2013. A lexicographic appraisal of an auto-
matic approach for detecting new word senses. In
proceedings of eLex, 49-65, Tallinn, Estonia.
Y. Goldberg and J. Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In proceedings of the Joint
Conference on Lexical and Computational Seman-
tics (*SEM), 241?247, Atlanta, GA, USA.
G. Heyer, F. Holz and S. Teresniak. 2009. Change of
topics over time ? tracking topics by their change of
meaning. In proceedings of KDIR, Madeira, Portu-
gal.
N. Ide and J. Veronis. 1998. Introduction to the special
issue on word sense disambiguation: The state of the
art. Computational Linguistics, 24(1):1?40.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The sketch engine. In Proceedings of EU-
RALEX, 105?116, Lorient, France.
A. Kilgarriff and D. Tugwell. 2001. Word sketch: Ex-
traction and display of significant collocations for
lexicography. In proceedings of COLLOCATION:
Computational Extraction, Analysis and Exploita-
tion, 32?38, Toulouse, France.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In pro-
ceedings of ACL/EACL, 64?71, Madrid, Spain.
V. Loreto, A. Mukherjee and F. Tria. 2012. On the ori-
gin of the hierarchy of color names. PNAS, 109(18),
6819?6824.
S. K. Maity, T. M. Venkat and A. Mukherjee. 2012.
Opinion formation in time-varying social networks:
The case of the naming game. Phys. Rev. E, 86,
036110.
J. McAuley and J. Leskovec. 2012. Learning to dis-
cover social circles in ego networks. In proceedings
of NIPS, 548?556, Nevada, USA.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak and E. L. Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science, 331(6014):176?182.
R. Mihalcea and V. Nastase. 2012. Word epoch disam-
biguation: finding how words change over time. In
proceedings of ACL, 259?263, Jeju Island, Korea.
A. Mukherjee, F. Tria, A. Baronchelli, A. Puglisi and V.
Loreto. 2011. Aging in language dynamics. PLoS
ONE, 6(2): e16677.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
P. P?a?akk?o and K. Lind?en. 2012. Finding a location
for a new word in WordNet. In proceedings of the
Global WordNet Conference, Matsue, Japan.
M. Riedl and C. Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In proceedings of EMNLP,
884?890, Seattle, Washington, USA.
M. Riedl, R. Steuer and C. Biemann. 2014. Distributed
distributional similarities of Google books over the
centuries. In proceedings of LREC, Reykjavik, Ice-
land.
P. Rychl?y and A. Kilgarriff. 2007. An efficient al-
gorithm for building a distributional thesaurus (and
other sketch engine developments). In proceedings
of ACL, poster and demo sessions, 41?44, Prague,
Czech Republic.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Sp?ark-Jones. 1986. Synonymy and Semantic Clas-
sification. Edinburgh University Press. ISBN 0-
85224-517-3.
N. Tahmasebi, T. Risse and S. Dietze. 2011. Towards
automatic language evolution tracking: a study on
word sense tracking. In proceedings of EvoDyn, vol.
784, Bonn, Germany.
X. Wang and A. McCallum. 2006. Topics over time:
a non-Markov continuous-time model of topical
trends. In proceedings of KDD, 424?433, Philadel-
phia, PA, USA.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In proceed-
ings of the workshop on Detecting and Exploiting
Cultural Diversity on the Social Web, 35?40, Glas-
gow, Scotland, UK.
1029
