Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Indirect-HMM-based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems 
 
Xiaodong He?, Mei Yang? *, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore? 
 
?Microsoft Research ?Dept. of Electrical Engineering 
One Microsoft Way University of Washington 
Redmond, WA 98052 USA Seattle, WA 98195, USA 
{xiaohe,jfgao, panguyen, 
bobmoore}@microsoft.com 
yangmei@u.washington.edu 
 
 
Abstract 
This paper presents a new hypothesis alignment method 
for combining outputs of multiple machine translation 
(MT) systems. An indirect hidden Markov model 
(IHMM) is proposed to address the synonym matching 
and word ordering issues in hypothesis alignment.  
Unlike traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly from a 
variety of sources including word semantic similarity, 
word surface similarity, and a distance-based distortion 
penalty. The IHMM-based method significantly 
outperforms the state-of-the-art TER-based alignment 
model in our experiments on NIST benchmark 
datasets.  Our combined SMT system using the 
proposed method achieved the best Chinese-to-English 
translation result in the constrained training track of the 
2008 NIST Open MT Evaluation. 
1 Introduction* 
System combination has been applied successfully 
to various machine translation tasks. Recently, 
confusion-network-based system combination 
algorithms have been developed to combine 
outputs of multiple machine translation (MT) 
systems to form a consensus output (Bangalore, et 
al. 2001, Matusov et al, 2006, Rosti et al, 2007, 
Sim et al, 2007). A confusion network comprises a 
sequence of sets of alternative words, possibly 
including null?s, with associated scores. The 
consensus output is then derived by selecting one 
word from each set of alternatives, to produce the 
sequence with the best overall score, which could 
be assigned in various ways such as by voting, by 
                                                          
* Mei Yang performed this work when she was an intern with 
Microsoft Research. 
using posterior probability estimates, or by using a 
combination of these measures and other features. 
Constructing a confusion network requires 
choosing one of the hypotheses as the backbone 
(also called ?skeleton? in the literature), and other 
hypotheses are aligned to it at the word level. High 
quality hypothesis alignment is crucial to the 
performance of the resulting system combination. 
However, there are two challenging issues that 
make MT hypothesis alignment difficult. First, 
different hypotheses may use different 
synonymous words to express the same meaning, 
and these synonyms need to be aligned to each 
other. Second, correct translations may have 
different word orderings in different hypotheses 
and these words need to be properly reordered in 
hypothesis alignment.  
In this paper, we propose an indirect hidden 
Markov model (IHMM) for MT hypothesis 
alignment. The HMM provides a way to model 
both synonym matching and word ordering. Unlike 
traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly 
from a variety of sources including word semantic 
similarity, word surface similarity, and a distance-
based distortion penalty, without using large 
amount of training data. Our combined SMT 
system using the proposed method gave the best 
result on the Chinese-to-English test in the 
constrained training track of the 2008 NIST Open 
MT Evaluation (MT08). 
2 Confusion-network-based MT system 
combination 
The current state-of-the-art is confusion-network-
based MT system combination as described by 
98
 Rosti and colleagues (Rosti et al, 2007a, Rosti et 
al., 2007b). The major steps are illustrated in 
Figure 1. In Fig. 1 (a), hypotheses from different 
MT systems are first collected. Then in Fig. 1 (b), 
one of the hypotheses is selected as the backbone 
for hypothesis alignment. This is usually done by a 
sentence-level minimum Bayes risk (MBR) 
method which selects a hypothesis that has the 
minimum average distance compared to all 
hypotheses. The backbone determines the word 
order of the combined output. Then as illustrated in 
Fig. 1 (c), all other hypotheses are aligned to the 
backbone. Note that in Fig. 1 (c) the symbol ? 
denotes a null word, which is inserted by the 
alignment normalization algorithm described in 
section 3.4. Fig. 1 (c) also illustrates the handling 
of synonym alignment (e.g., aligning ?car? to 
?sedan?), and word re-ordering of the hypothesis. 
Then in Fig. 1 (d), a confusion network is 
constructed based on the aligned hypotheses, 
which consists of a sequence of sets in which each 
word is aligned to a list of alternative words 
(including null) in the same set. Then, a set of 
global and local features are used to decode the 
confusion network.  
  
E1 he have good car 
argmin ( , )B E EE TER E E?? ? ?? ?E E
 
E2 he has nice sedan 
E3 it a nice car        e.g., EB = E1 E4 a sedan he has 
(a)  hypothesis set                    (b) backbone selection 
 
EB he have ? good car      he  have   ?   good   car 
       he   has    ?   nice    sedan 
       it     ?       a   nice    car   
E4 a  ?  sedan  he   has      he   has    a     ?       sedan 
(c)  hypothesis alignment        (d) confusion network 
 
Figure 1: Confusion-network-based MT system 
combination.  
3 Indirect-HMM-based Hypothesis 
Alignment  
In confusion-network-based system combination 
for SMT, a major difficulty is aligning hypotheses 
to the backbone. One possible statistical model for 
word alignment is the HMM, which has been 
widely used for bilingual word alignment (Vogel et 
al., 1996, Och and Ney, 2003). In this paper, we 
propose an indirect-HMM method for monolingual 
hypothesis alignment. 
 
3.1 IHMM for hypothesis alignment  
 
Let 
1 1( ,..., )I Ie e e? denote the backbone, 
1 1( ,..., )J Je e e? ? ??  a hypothesis to be aligned to 1Ie , 
and 
1 1( ,..., )J Ja a a?  the alignment that specifies 
the position of the backbone word aligned to each 
hypothesis word. We treat each word in the 
backbone as an HMM state and the words in the 
hypothesis as the observation sequence. We use a 
first-order HMM, assuming that the emission 
probability 
( | )jj ap e e?
 depends only on the 
backbone word, and the transition probability 
1( | , )j jp a a I?
 depends only on the position of the 
last state and the length of the backbone. Treating 
the alignment as hidden variable, the conditional 
probability that the hypothesis is generated by the 
backbone is given by  
 
 
1
1 1 1
1
( | ) ( | , ) ( | )jJ
JJ I
j j j a
ja
p e e p a a I p e e?
?
? ?? ?? ? ???
 (1) 
  
As in HMM-based bilingual word alignment 
(Och and Ney, 2003), we also associate a null with 
each backbone word to allow generating 
hypothesis words that do not align to any backbone 
word.  
In HMM-based hypothesis alignment, emission 
probabilities model the similarity between a 
backbone word and a hypothesis word, and will be 
referred to as the similarity model. The transition 
probabilities model word reordering, and will be 
called the distortion model. 
 
3.2 Estimation of the similarity model 
 
The similarity model, which specifies the emission 
probabilities of the HMM, models the similarity 
between a backbone word and a hypothesis word. 
Since both words are in the same language, the 
similarity model can be derived based on both 
semantic similarity and surface similarity, and the 
overall similarity model is a linear interpolation of 
the two: 
 
( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e? ?? ? ?? ? ? ? ?  (2) 
 
99
 where ( | )sem j ip e e?
 and ( | )sur j ip e e?
 reflect the 
semantic and surface similarity between 
je?
 and  
ie , respectively, and ? is the interpolation factor. 
Since the semantic similarity between two 
target words is source-dependent, the semantic 
similarity model is derived by using the source 
word sequence as a hidden layer: 
 
0
( | )
( | ) ( | , )
sem j i
K
k i j k i
k
p e e
p f e p e f e
?
?
???
 
0
( | ) ( | )K k i j k
k
p f e p e f
?
???     (3) 
 
where 
1 1( ,..., )K Kf f f?  is the source sentence. 
Moreover, in order to handle the case that two 
target words are synonyms but neither of them has 
counter-part in the source sentence, a null is 
introduced on the source side, which is represented 
by f0. The last step in (3) assumes that first ei 
generates all source words including null. Then ej? 
is generated by all source words including null.  
In the common SMT scenario where a large 
amount of bilingual parallel data is available, we 
can estimate the translation probabilities from a 
source word to a target word and vice versa via 
conventional bilingual word alignment. Then both 
( | )k ip f e  and ( | )j kp e f?
 in (3) can be derived:  
 
2( | ) ( | )j k s t j kp e f p e f? ??
 
 
where 
2 ( | )s t j kp e f?
 is the translation model from 
the source-to-target word alignment model, and 
( | )k ip f e  , which enforces the sum-to-1 constraint 
over all words in the source sentence, takes the 
following form, 
 
2
2
0
( | )( | )
( | )
t s k i
k i K
t s k i
k
p f ep f e
p f e
?
?
?
 
 
where 
2 ( | )t s k ip f e  is the translation model from 
the  target-to-source word alignment model. In our 
method, 
2 ( | )t s ip null e  for all target words is 
simply a constant pnull, whose value is optimized 
on held-out data 1.  
The surface similarity model can be estimated 
in several ways. A very simple model could be 
based on exact match: the surface similarity model, 
( | )sur j ip e e?
, would take the value 1.0 if e?= e, and 
0 otherwise 2 . However, a smoothed surface 
similarity model is used in our method. If the target 
language uses alphabetic orthography, as English 
does, we treat words as letter sequences and the 
similarity measure can be the length of the longest 
matched prefix (LMP) or the length of the longest 
common subsequence (LCS) between them. Then, 
this raw similarity measure is transformed to a 
surface similarity score between 0 and 1 through 
an exponential mapping,  
 
? ?( | ) exp ( , ) 1sur j i j ip e e s e e?? ?? ?? ? ?? ?    (4) 
 
where ( , )j is e e?
 is computed as 
 
( , )( , ) max(| |,| |)
j i
j i
j i
M e es e e e e
?? ? ?
 
 
and ( , )j iM e e?
 is the raw similarity measure of ej? 
ei, which is the length of the LMP or LCS of ej? 
and ei. and ? is a smoothing factor that 
characterizes the mapping, Thus as ? approaches 
infinity, ( | )sur j ip e e?
 backs off to the exact match 
model. We found the smoothed similarity model of 
(4) yields slightly better results than the exact 
match model. Both LMP- and LCS- based methods 
achieve similar performance but the computation 
of LMP is faster. Therefore, we only report results 
of the LMP-based smoothed similarity model.  
 
3.3 Estimation of the distortion model 
 
The distortion model, which specifies the transition 
probabilities of the HMM, models the first-order 
dependencies of word ordering. In bilingual 
HMM-based word alignment, it is commonly 
assumed that transition probabilities 
                                                          
1  The other direction, 
2 ( | )s t ip e null? , is available from the 
source-to-target translation model. 
2 Usually a small back-off value is assigned instead of 0.  
100
 1( | , )? ?? ?j jp a i a i I
 depend only on the jump 
distance (i - i')  (Vogel et al, 1996):  
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
?
??? ?
???
             (5) 
 
As suggested by Liang et al (2006), we can 
group the distortion parameters {c(d)}, d= i - i', 
into a few buckets. In our implementation, 11 
buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5), 
c(?6). The probability mass for transitions with 
jump distance larger than 6 and less than -4 is 
uniformly divided. By doing this, only a handful of 
c(d) parameters need to be estimated. Although it 
is possible to estimate them using the EM 
algorithm on a small development set, we found 
that a particularly simple model, described below, 
works surprisingly well in our experiments.  
Since both the backbone and the hypothesis are 
in the same language, It seems intuitive that the 
distortion model should favor monotonic 
alignment and only allow non-monotonic 
alignment with a certain penalty. This leads us to 
use a distortion model of the following form, 
where K is a tuning factor optimized on held-out 
data. 
 
? ? ? ?1 1c d d ??? ? ?, d= ?4, ?, 6   (6) 
 
As shown in Fig. 2, the value of distortion score 
peaks at d=1, i.e., the monotonic alignment, and 
decays for non-monotonic alignments depending 
on how far it diverges from the monotonic 
alignment. 
 
Figure 2, the distance-based distortion parameters 
computed according to (6), where K=2. 
 
Following Och and Ney (2003), we use a fixed 
value p0 for the probability of jumping to a null 
state, which can be optimized on held-out data, and 
the overall distortion model becomes 
 
0
0
              if     state( | , ) (1 ) ( | , )  otherwise
p i nullp i i I p p i i I
??? ? ? ?? ???
 
 
3.4 Alignment normalization 
 
Given an HMM, the Viterbi alignment algorithm 
can be applied to find the best alignment between 
the backbone and the hypothesis, 
 
1
1 1
1
? argmax ( | , ) ( | )jJ
JJ
j j j aa j
a p a a I p e e?
?
? ??? ? ??
  (7) 
 
However, the alignment produced by the 
algorithm cannot be used directly to build a 
confusion network. There are two reasons for this. 
First, the alignment produced may contain 1-N 
mappings between the backbone and the 
hypothesis whereas 1-1 mappings are required in 
order to build a confusion network. Second, if 
hypothesis words are aligned to a null in the 
backbone or vice versa, we need to insert actual 
nulls into the right places in the hypothesis and the 
backbone, respectively. Therefore, we need to 
normalize the alignment produced by Viterbi 
search. 
 
EB ? e2  ?2   ?   
   ?    ?      e2        ?     ?      ? 
           e1'    e2'    e3'   e4'    
Eh e1'    e2'    e3'   e4'  
(a) hypothesis words are aligned to the backbone null  
 
EB e1  ?1  e2  ?2  e3  ?3    
   ?    e1     e2        e3      ? 
           e2'    ?      e1'   
Eh e1'    e2'    ?  
(b) a backbone word is aligned to no hypothesis word 
 
Figure 3: illustration of alignment normalization 
 
First, whenever more than one hypothesis 
words are aligned to one backbone word, we keep 
the link which gives the highest occupation 
probability computed via the forward-backward 
algorithm. The other hypothesis words originally 
 -4                     1                      6  
 1.0 
 0.0 
   c(d) 
  d 
101
 aligned to the backbone word will be aligned to the 
null associated with that backbone word. 
Second, for the hypothesis words that are 
aligned to a particular null on the backbone side, a 
set of nulls are inserted around that backbone word 
associated with the null such that no links cross 
each other. As illustrated in Fig. 3 (a), if a 
hypothesis word e2? is aligned to the backbone 
word e2, a null is inserted in front of the backbone 
word e2 linked to the hypothesis word e1? that 
comes before e2?. Nulls are also inserted for other 
hypothesis words such as e3? and e4? after the 
backbone word e2. If there is no hypothesis word 
aligned to that backbone word, all nulls are 
inserted after that backbone word .3 
For a backbone word that is aligned to no 
hypothesis word, a null is inserted on the 
hypothesis side, right after the hypothesis word 
which is aligned to the immediately preceding 
backbone word. An example is shown in Fig. 3 (b). 
4 Related work 
The two main hypothesis alignment methods for 
system combination in the previous literature are 
GIZA++ and TER-based methods. Matusov et al 
(2006) proposed using GIZA++ to align words 
between different MT hypotheses, where all 
hypotheses of the test corpus are collected to create 
hypothesis pairs for GIZA++ training. This 
approach uses the conventional HMM model 
bootstrapped from IBM Model-1 as implemented 
in GIZA++, and heuristically combines results 
from aligning in both directions. System 
combination based on this approach gives an 
improvement over the best single system. 
However, the number of hypothesis pairs for 
training is limited by the size of the test corpus. 
Also, MT hypotheses from the same source 
sentence are correlated with each other and these 
hypothesis pairs are not i.i.d. data samples. 
Therefore, GIZA++ training on such a data set may 
be unreliable.  
Bangalore et al (2001) used a multiple string-
matching algorithm based on Levenshtein edit 
distance, and later Sim et al (2007) and Rosti et al 
(2007) extended it to a TER-based method for 
hypothesis alignment. TER (Snover et al, 2006) 
                                                          
3  This only happens if no hypothesis word is aligned to a 
backbone word but some hypothesis words are aligned to the 
null associated with that backbone word. 
measures the minimum number of edits, including 
substitution, insertion, deletion, and shift of blocks 
of words, that are needed to modify a hypothesis so 
that it exactly matches the other hypothesis. The 
best alignment is the one that gives the minimum 
number of translation edits. TER-based confusion 
network construction and system combination has 
demonstrated superior performance on various 
large-scale MT tasks (Rosti. et al 2007). However, 
when searching for the optimal alignment, the 
TER-based method uses a strict surface hard match 
for counting edits. Therefore, it is not able to 
handle synonym matching well. Moreover, 
although TER-based alignment allows phrase 
shifts to accommodate the non-monotonic word 
ordering, all non-monotonic shifts are penalized 
equally no matter how short or how long the move 
is, and this penalty is set to be the same as that for 
substitution, deletion, and insertion edits. 
Therefore, its modeling of non-monotonic word 
ordering is very coarse-grained.  
In contrast to the GIZA++-based method, our 
IHMM-based method has a similarity model 
estimated using bilingual word alignment HMMs 
that are trained on a large amount of bi-text data. 
Moreover, the surface similarity information is 
explicitly incorporated in our model, while it is 
only used implicitly via parameter initialization for 
IBM Model-1 training by Matusov et al (2006). 
On the other hand, the TER-based alignment 
model is similar to a coarse-grained, non-
normalized version of our IHMM, in which the 
similarity model assigns no penalty to an exact 
surface match and a fixed penalty to all 
substitutions, insertions, and deletions, and the 
distortion model simply assigns no penalty to a 
monotonic jump, and a fixed penalty to all other 
jumps, equal to the non-exact-match penalty in the 
similarity model. 
There have been other hypothesis alignment 
methods. Karakos, et al (2008) proposed an ITG-
based method for hypothesis alignment, Rosti et al 
(2008) proposed an incremental alignment method, 
and a heuristic-based matching algorithm was 
proposed by Jayaraman and Lavie (2005).  
5 Evaluation 
In this section, we evaluate our IHMM-based 
hypothesis alignment method on the Chinese-to-
English (C2E) test in the constrained training track 
102
 of the 2008 NIST Open MT Evaluation (NIST, 
2008). We compare to the TER-based method used 
by Rosti et al (2007). In the following 
experiments, the NIST BLEU score is used as the 
evaluation metric (Papineni et al, 2002), which is 
reported as a percentage in the following sections.  
 
5.1 Implementation details 
 
In our implementation, the backbone is selected 
with MBR. Only the top hypothesis from each 
single system is considered as a backbone. A 
uniform posteriori probability is assigned to all 
hypotheses. TER is used as loss function in the 
MBR computation.  
Similar to (Rosti et al, 2007), each word in the 
confusion network is associated with a word 
posterior probability. Given a system S, each of its 
hypotheses is assigned with a rank-based score of 
1/(1+r)?, where r is the rank of the hypothesis, and 
? is a rank smoothing parameter. The system 
specific rank-based score of a word w for a given 
system S is the sum of all the rank-based scores of 
the hypotheses in system S that contain the word w 
at the given position (after hypothesis alignment). 
This score is then normalized by the sum of the 
scores of all the alternative words at the same 
position and from the same system S to generate 
the system specific word posterior. Then, the total 
word posterior of w over all systems is a sum of 
these system specific posteriors weighted by 
system weights. 
Beside the word posteriors, we use language 
model scores and a word count as features for 
confusion network decoding. 
Therefore, for an M-way system combination 
that uses N LMs, a total of M+N+1 decoding 
parameters, including M-1 system weights, one 
rank smoothing factor, N language model weights, 
and one weight for the word count feature, are 
optimized using Powell?s method (Brent, 1973) to 
maximize BLEU score on a development set4 . 
Two language models are used in our 
experiments. One is a trigram model estimated 
from the English side of the parallel training data, 
and the other is a 5-gram model trained on the 
English GigaWord corpus from LDC using the 
MSRLM toolkit (Nguyen et al 2007). 
                                                          
4 The parameters of IHMM are not tuned by maximum-BLEU 
training. 
In order to reduce the fluctuation of BLEU 
scores caused by the inconsistent translation output 
length, an unsupervised length adaptation method 
has been devised. We compute an expected length 
ratio between the MT output and the source 
sentences on the development set after maximum- 
BLEU training. Then during test, we adapt the 
length of the translation output by adjusting the 
weight of the word count feature such that the 
expected output/source length ratio is met. In our 
experiments, we apply length adaptation to the 
system combination output at the level of the 
whole test corpus. 
 
5.2  Development and test data  
 
The development (dev) set used for system 
combination parameter training contains 1002 
sentences sampled from the previous NIST MT 
Chinese-to-English test sets: 35% from MT04, 
55% from MT05, and 10% from MT06-newswire. 
The test set is the MT08 Chinese-to-English 
?current? test set, which includes 1357 sentences 
from both newswire and web-data genres. Both 
dev and test sets have four references per sentence. 
As inputs to the system combination, 10-best 
hypotheses for each source sentence in the dev and 
test sets are collected from each of the eight single 
systems. All outputs on the MT08 test set were 
true-cased before scoring using a log-linear 
conditional Markov model proposed by Toutanova 
et al (2008). However, to save computation effort, 
the results on the dev set are reported in case 
insensitive BLEU (ciBLEU) score instead. 
 
5.3  Experimental results 
 
In our main experiments, outputs from a total of 
eight single MT systems were combined. As listed 
in Table 1, Sys-1 is a tree-to-string system 
proposed by Quirk et al, (2005); Sys-2 is a phrase-
based system with fast pruning proposed by Moore 
and Quirk (2008); Sys-3 is a phrase-based system 
with syntactic source reordering proposed by 
Wang et al (2007a); Sys-4 is a syntax-based pre-
ordering system proposed by Li et. al. (2007); Sys-
5 is a hierarchical system proposed by Chiang 
(2007); Sys-6 is a lexicalized re-ordering system 
proposed by Xiong et al (2006); Sys-7 is a two-
pass phrase-based system with adapted LM 
proposed by Foster and Kuhn (2007); and  Sys-8 is 
103
 a hierarchical system with two-pass rescoring 
using a parser-based LM proposed by Wang et al, 
(2007b). All systems were trained within the 
confines of the constrained training condition of 
NIST MT08 evaluation. These single systems are 
optimized with maximum-BLEU training on 
different subsets of the previous NIST MT test 
data. The bilingual translation models used to 
compute the semantic similarity are from the word-
dependent HMMs proposed by He (2007), which 
are trained on two million parallel sentence-pairs 
selected from the training corpus allowed by the 
constrained training condition of MT08.  
 
5.3.1 Comparison with TER alignment 
In the IHMM-based method, the smoothing 
factor for surface similarity model is set to ? = 3, 
the interpolation factor of the overall similarity 
model is set to ? = 0.3, and the controlling factor of 
the distance-based distortion parameters is set to 
K=2. These settings are optimized on the dev set. 
Individual system results and system combination 
results using both IHMM and TER alignment, on 
both the dev and test sets, are presented in Table 1. 
The TER-based hypothesis alignment tool used in 
our experiments is the publicly available TER Java 
program, TERCOM (Snover et al, 2006). Default 
settings of TERCOM are used in the following 
experiments. 
On the dev set, the case insensitive BLEU score 
of the IHMM-based 8-way system combination 
output is about 5.8 points higher than that of the 
best single system. Compared to the TER-based 
method, the IHMM-based method is about 1.5 
BLEU points better. On the MT08 test set, the 
IHMM-based system combination gave a case 
sensitive BLEU score of 30.89%. It outperformed 
the best single system by 4.7 BLEU points and the 
TER-based system combination by 1.0 BLEU 
points. Note that the best single system on the dev 
set and the test set are different. The different 
single systems are optimized on different tuning 
sets, so this discrepancy between dev set and test 
set results is presumably due to differing degrees 
of mismatch between the dev and test sets and the 
various tuning sets. 
 
 
 
 
 
Table 1. Results of single and combined systems 
on the dev set and the MT08 test set  
System Dev 
ciBLEU% 
MT08 
BLEU% 
System 1 34.08 21.75 
System 2 33.78 20.42 
System 3 34.75 21.69 
System 4 37.85 25.52 
System 5 37.80 24.57 
System 6 37.28 24.40 
System 7 32.37 25.51 
System 8 34.98 26.24 
TER 42.11 29.89 
IHMM 43.62 30.89 
 
In order to evaluate how well our method 
performs when we combine more systems, we 
collected MT outputs on MT08 from seven 
additional single systems as summarized in Table 
2. These systems belong to two groups. Sys-9 to 
Sys-12 are in the first group. They are syntax-
augmented hierarchical systems similar to those 
described by Shen et al (2008) using different 
Chinese word segmentation and language models. 
The second group has Sys-13 to Sys-15. Sys-13 is 
a phrasal system proposed by Koehn et al (2003), 
Sys-14 is a hierarchical system proposed by 
Chiang (2007), and Sys-15 is a syntax-based 
system proposed by Galley et al (2006). All seven 
systems were trained within the confines of the 
constrained training condition of NIST MT08 
evaluation.  
We collected 10-best MT outputs only on the 
MT08 test set from these seven extra systems. No 
MT outputs on our dev set are available from them 
at present. Therefore, we directly adopt system 
combination parameters trained for the previous 8-
way system combination, except the system 
weights, which are re-set by the following 
heuristics: First, the total system weight mass 1.0 is 
evenly divided among the three groups of single 
systems: {Sys-1~8}, {Sys-9~12}, and {Sys-
13~15}. Each group receives a total system weight 
mass of 1/3. Then the weight mass is further 
divided in each group: in the first group, the 
original weights of systems 1~8 are multiplied by 
1/3; in the second and third groups, the weight 
mass is evenly distributed within the group, i.e., 
1/12 for each system in group 2, and 1/9 for each 
104
 system in group 35.  Length adaptation is applied to 
control the final output length, where the same 
expected length ratio of the previous 8-way system 
combination is adopted. 
The results of the 15-way system combination 
are presented in Table 3. It shows that the IHMM-
based method is still about 1 BLEU point better 
than the TER-based method. Moreover, combining 
15 single systems gives an output that has a NIST 
BLEU score of 34.82%, which is 3.9 points better 
than the best submission to the NIST MT08 
constrained training track (NIST, 2008). To our 
knowledge, this is the best result reported on this 
task. 
 
Table 2. Results of seven additional single systems 
on the NIST MT08 test set 
System MT08 
BLEU% 
System 9 29.59 
System 10 29.57 
System 11 29.64 
System 12 29.85 
System 13 25.53 
System 14 26.04 
System 15 29.70 
 
Table 3. Results of the 15-way system combination 
on the NIST MT08 C2E test set 
Sys. Comb.  MT08 
BLEU% 
TER 33.81 
IHMM 34.82 
 
5.3.2 Effect of the similarity model  
In this section, we evaluate the effect of the 
semantic similarity model and the surface 
similarity model by varying the interpolation 
weight ? of (2). The results on both the dev and 
test sets are reported in Table 4. In one extreme 
case, ? = 1, the overall similarity model is based 
only on semantic similarity. This gives a case 
insensitive BLEU score of 41.70% and a case 
sensitive BLEU score of 28.92% on the dev and 
test set, respectively. The accuracy is significantly 
improved to 43.62% on the dev set and 30.89% on 
test set when ? = 0.3. In another extreme case, ? = 
                                                          
5 This is just a rough guess because no dev set is available. We 
believe a better set of system weights could be obtained if MT 
outputs on a common dev set were available. 
0, in which only the surface similarity model is 
used for the overall similarity model, the 
performance degrades by about 0.2 point. 
Therefore, the surface similarity information seems 
more important for monolingual hypothesis 
alignment, but both sub-models are useful.  
 
Table 4. Effect of the similarity model 
 Dev 
ciBLEU% 
Test 
BLEU% 
? = 1.0 41.70 28.92 
? = 0.7 42.86 30.50 
? = 0.5 43.11 30.94 
? = 0.3 43.62 30.89 
? = 0.0 43.35 30.73 
 
5.3.3 Effect of the distortion model  
We investigate the effect of the distance-based 
distortion model by varying the controlling factor 
K in (6). For example, setting K=1.0 gives a linear-
decay distortion model, and setting K=2.0 gives a 
quadratic smoothed distance-based distortion 
model. As shown in Table 5, the optimal result can 
be achieved using a properly smoothed distance-
based distortion model. 
 
Table 5. Effect of the distortion model 
 Dev 
ciBLEU% 
Test 
BLEU% 
K=1.0 42.94 30.44 
K=2.0 43.62 30.89 
K=4.0 43.17 30.30 
K=8.0 43.09 30.01 
6 Conclusion 
Synonym matching and word ordering are two 
central issues for hypothesis alignment in 
confusion-network-based MT system combination. 
In this paper, an IHMM-based method is proposed 
for hypothesis alignment. It uses a similarity model 
for synonym matching and a distortion model for 
word ordering. In contrast to previous methods, the 
similarity model explicitly incorporates both 
semantic and surface word similarity, which is 
critical to monolingual word alignment, and a 
smoothed distance-based distortion model is used 
to model the first-order dependency of word 
ordering, which is shown to be better than simpler 
approaches. 
105
 Our experimental results show that the IHMM-
based hypothesis alignment method gave superior 
results on the NIST MT08 C2E test set compared 
to the TER-based method. Moreover, we show that 
our system combination method can scale up to 
combining more systems and produce a better 
output that has a case sensitive BLEU score of 
34.82, which is 3.9 BLEU points better than the 
best official submission of MT08.  
Acknowledgement 
The authors are grateful to Chris Quirk, Arul 
Menezes, Kristina Toutanova, William Dolan, Mu 
Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, 
Ming Zhou, George Foster, Roland Kuhn, Jing 
Zheng, Wen Wang, Necip Fazil Ayan, Dimitra 
Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin 
Knight, Jens-Soenke Voeckler, Spyros Matsoukas, 
and Antti-Veikko Rosti for assistance with the MT 
systems and/or for the valuable suggestions and 
discussions.  
 
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In Proc. 
of IEEE ASRU, pp. 351?354. 
Richard Brent, 1973. Algorithms for Minimization 
without Derivatives. Prentice-Hall, Chapter 7. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
George Foster and Roland Kuhn. 2007. Mixture-Model 
Adaptation for SMT. In Proc. of the Second ACL 
Workshop on Statistical Machine Translation. pp. 
128 ? 136. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Proc. 
of COLING-ACL, pp. 961?968. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. In Proc. of the 
Second ACL Workshop on Statistical Machine 
Translation. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word 
matching. In Proc. of EAMT. pp. 143 ? 152. 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proc. of ACL-HLT, pp. 81?84. 
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming 
Zhou, Yi Guan. 2007. A Probabilistic Approach to 
Syntax-based Reordering for Statistical Machine 
Translation. In Proc. of ACL. pp. 720 ? 727. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by Agreement. In Proc. of NAACL. pp 
104 ? 111.  
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing consensus translation from 
multiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL, pp. 33?40. 
Robert Moore and Chris Quirk. 2007. Faster Beam-
Search Decoding for Phrasal Statistical Machine 
Translation. In Proc. of MT Summit XI. 
Patrick Nguyen, Jianfeng Gao and Milind Mahajan. 
2007. MSRLM: a scalable language modeling 
toolkit. Microsoft Research Technical Report MSR-
TR-2007-144. 
NIST. 2008. The 2008 NIST Open Machine Translation 
Evaluation. www.nist.gov/speech/tests/mt/2008/doc/  
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of ACL, 
pp. 311?318. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase based translation. In Proc. of 
NAACL. pp. 48 ? 54. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL. pp. 271?
279. 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proc. of NAACL-
HLT, pp. 228?235. 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007b. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL, pp. 312?319. 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental Hypothesis 
Alignment for Building Confusion Networks with 
Application to Machine Translation System 
Combination, In Proc. of the Third ACL Workshop 
on Statistical Machine Translation, pp. 183?186. 
Libin Shen, Jinxi Xu, Ralph Weischedel. 2008. A New 
String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. In Proc. of ACL-HLT, pp. 577?585. 
106
 Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. 
Consensus network decoding for statistical machine 
translation system combination. In Proc. of ICASSP, 
vol. 4. pp. 105?108. 
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of 
translation edit rate with targeted human annotation. 
In Proc. of AMTA. 
Kristina Toutanova, Hisami Suzuki and Achim Ruopp. 
2008. Applying Morphology Generation Models to 
Machine Translation. In Proc. of ACL. pp. 514 ? 522. 
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based Word Alignment In Statistical 
Translation. In Proc. of COLING. pp. 836-841. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007a. Chinese Syntactic Reordering for Statistical 
Machine Translation.  In Proc. of EMNLP-CoNLL. 
pp. 737-745. 
Wen Wang, Andreas Stolcke, Jing Zheng. 2007b. 
Reranking Machine Translation Hypotheses With 
Structured and Web-based Language Models. In 
Proc. of IEEE ASRU. pp. 159 ? 164. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL. 
pp. 521 ? 528. 
107
NAACL HLT Demonstration Program, pages 31?32,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Voice-Rate: A Dialog System for Consumer Ratings 
 
Geoffrey Zweig, Y.C. Ju, Patrick Nguyen, Dong Yu, 
Ye-Yi Wang and Alex Acero 
Speech Research Group 
Microsoft Corp. 
Redmond, WA 98052 
{gzweig,yuncj,panguyen,dongyu, yeyi-
wang,alexac}@microsoft.com 
 
  
Abstract 
Voice-Rate is an automated dialog system 
which provides access to over one million 
ratings of products and businesses. By 
calling a toll-free number, consumers can 
access ratings for products, national busi-
nesses such as airlines, and local busi-
nesses such as restaurants. Voice-Rate 
also has a facility for recording and ana-
lyzing ratings that are given over the 
phone. The service has been primed with 
ratings taken from a variety of web 
sources, and we are augmenting these 
with user ratings. Voice-Rate can be ac-
cessed by dialing 1-877-456-DATA. 
1 Overview 
 Voice-Rate is an automated dialog system de-
signed to help consumers while they are shopping. 
The target user is a consumer who is considering 
making an impulse purchase and would like to get 
more information. He or she can take out a cell-
phone, call Voice-Rate, and get rating information 
to help decide whether to buy the item. Here are 
three sample scenarios: 
 
 Sally has gone to Home Depot to buy 
some paint to touch-up scratches on the 
wall at home. She?ll use exactly the same 
color and brand as when she first painted 
the wall, so she knows what she wants. 
While at Home Depot, however, Sally sees 
some hand-held vacuum cleaners and de-
cides it might be nice to have one. But, she 
is unsure whether which of the available 
models is better: The ?Black & Decker 
CHV1400 Cyclonic DustBuster,? the 
?Shark SV736? or the ?Eureka 71A.? Sally 
calls Voice-Rate and gets the ratings and 
makes an informed purchase. 
 John is on vacation with his family in Seat-
tle. After going up in the Space Needle, 
they walk by ?Abbondanza Pizzeria? and 
are considering lunch there. While it looks 
good, there are almost no diners inside, 
and John is suspicious. He calls Voice-
Rate and discovers that in fact the restau-
rant is highly rated, and decides to go 
there. 
 Returning from his vacation, John drops 
his rental car off at the airport. The rental 
company incorrectly asserts that he has 
scratched the car, and causes a big hassle, 
until they finally realize that they already 
charged the last customer for the same 
scratch. Unhappy with the surly service, 
John calls Voice-Rate and leaves a warn-
ing for others.  
 
Currently, Voice-Rate can deliver ratings for over 
one million products, two hundred thousand res-
taurants in over sixteen hundred cities; and about 
three thousand national businesses.  
2 Technical Challenges 
To make Voice-Rate operational, it was necessary 
to solve the key challenges of name resolution and 
disambiguation. Users rarely make an exactly cor-
rect specification of a product or business, and it is 
necessary both to utilize a ?fuzzy-match? for name 
lookup, and to deploy a carefully designed disam-
biguation strategy.  
31
Voice-Rate solves the fuzzy-matching process by 
treating spoken queries as well as business and 
product names as documents, and then performing 
TF-IDF based lookup. For a review of name 
matching methods, see e.g. Cohen et al, 2003. In 
the ideal case, after a user asks for a particular 
product or business, the best-matching item as 
measured by TF-IDF would be the one intended by 
the user. In reality, of course, this is often not the 
case, and further dialog is necessary to determine 
the user?s intent. For concreteness, we will illu-
strate the disambiguation process in the context of 
product identification. 
 
When a user calls Voice-Rate and asks for a prod-
uct review, the system solicits the user for the 
product name, does TF-IDF lookup, and presents 
the highest-scoring match for user confirmation. If 
the user does not accept the retrieved item, Voice-
Rate initiates a disambiguation dialog.  
 
Aside from inadequate product coverage, which 
cannot be fixed at runtime, there are two possible 
sources for error: automatic speech recognition 
(ASR) errors, and TF-IDF lookup errors.  The dis-
ambiguation process begins by eliminating the 
first. To do this, it asks the user if his or her exact 
words were the recognized text, and if not to repeat 
the request. This loop iterates twice, and if the us-
er?s exact words still have not been identified, 
Voice-Rate apologizes and hangs up. 
 
Once the user?s exact words have been validated, 
Voice-Rate gets a positive identification on the 
product category. From the set of high-scoring TF-
IDF items, a list of possible categories is compiled. 
For example, for ?The Lord of the Rings The Two 
Towers,? there are items in Video Games, DVDs, 
Music, VHS, Software, Books, Websites, and Toys 
and Games. These categories are read to the user, 
who is asked to select one. All the close-matching 
product names in the selected category are then 
read to the user, until one is selected or the list is 
exhausted.  
3 Related Work 
To our knowledge, Voice-Rate is the first large 
scale ratings dialog system. However, the technol-
ogy behind it is closely related to previous dialog 
systems, especially directory assistance or ?411? 
systems (e.g. Kamm et al, 1994, Natarajan et al, 
2002, Levin et al, 2005, Jan et al, 2003).  A gen-
eral discussion of name-matching techniques such 
as TF-IDF can be found in (Cohen et al, 2003, 
Bilenko et al, 2003). 
 
The second area of related research has to do with 
web rating systems. Interesting work on extracting 
information from such ratings can be found in, e.g. 
(Linden et al, 2003, Hu et al, 2004, Gammon et 
al., 2005). Work has also been done using text-
based input to determine relevant products (Chai et 
al., 2002).  Our own work differs from this in that 
it focuses on spoken input, and in its breadth ? 
covering both products and businesses. 
References  
M. Bilenko, R. Mooney, W. W. Cohen, P. Ravikumar and S. 
Fienberg. 2003. Adaptive Name-Matching in Information 
Integration. IEEE Intelligent Systems 18(5): 16-23 (2003).  
J. Chai, V. Horvath, N. Nicolov, M. Stys, N. Kambhatla, W. 
Zadrozny and P. Melville.  2002. Natural Language Assis-
tant- A Dialog System for Online Product Recommenda-
tion. AI Magazine (23), 2002 
 
W. W. Cohen, P Ravikumar and S. E. Fienberg . 2003. A 
comparison of string distance metrics for name-matching 
tasks. Proceedings of the IJCAI-2003 Workshop on Infor-
mation, 2003 
M.  Gamon, A. Aue, S. Corston-Oliver and E. Ringger. 2005. 
Pulse: Mining Customer Opinions from Free Text. In Lec-
ture Notes in Computer Science. Vol. 3646. Springer Ver-
lag. (IDA 2005)., pages 121-132. 
M. Hu and B. Liu. 2004. Mining and summarizing customer 
reviews. Proceedings of the 2004 ACM SIGKDD interna-
tional conference. 
 
E. E. Jan, B. Maison, L. Mangu and G. Zweig. 2003. Auto-
matic construction of Unique Signatures and Confusable 
sets for Natural Language Directory Assistance Applica-
tion.  Eurospeech 2003 
C. A. Kamm, K. M. Yang, C. R. Shamieh and S. Singhal. 
1994. Speech recognition issues for directory assistance 
applications. Second IEEE Workshop on Interactive Voice 
Technology for Telecommunications Applications. 
 
E. Levin and A. M. Man?. 2005. Voice User Interface Design 
for Automated Directory Assistance Eurospeech 2005. 
G. Linden, B. Smith and J. York. Amazon.com recommenda-
tions: item-to-item collaborative filtering. 2003.  Internet 
Computing, IEEE , vol.7, no.1pp. 76- 80. 
 
P. Natarajan, R. Prasad, R. Schwartz and J. Makhoul. 2002. A 
Scalable Architecture for Directory Assistance Automation, 
ICASSP 2002, Orlando, Florida. 
32
Proceedings of NAACL HLT 2009: Short Papers, pages 101?104,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multi-scale Personalization for Voice Search Applications
Daniel Bolan?os
Center for Spoken Language Research
University of Colorado at Boulder, USA
bolanos@cslr.colorado.edu
Geoffrey Zweig
Microsoft Research
One Microsoft Way, Redmond, WA 98052
gzweig@microsoft.com
Patrick Nguyen
Microsoft Research
One Microsoft Way, Redmond, WA 98052
panguyen@microsoft.com
Abstract
Voice Search applications provide a very con-
venient and direct access to a broad variety
of services and information. However, due to
the vast amount of information available and
the open nature of the spoken queries, these
applications still suffer from recognition er-
rors. This paper explores the utilization of per-
sonalization features for the post-processing
of recognition results in the form of n-best
lists. Personalization is carried out from three
different angles: short-term, long-term and
Web-based, and a large variety of features are
proposed for use in a log-linear classification
framework.
Experimental results on data obtained from a
commercially deployed Voice Search system
show that the combination of the proposed
features leads to a substantial sentence error
rate reduction. In addition, it is shown that
personalization features which are very dif-
ferent in nature can successfully complement
each other.
1 Introduction
Search engines are a powerful mechanism to find
specific content through the use of queries. In recent
years, due to the vast amount of information avail-
able, there has been significant research on the use of
recommender algorithms to select what information
will be presented to the user. These systems try to
predict what content a user may want based not only
on the user?s query but on the user?s past queries,
history of clicked results, and preferences. In (Tee-
van et al, 1996) it was observed that a significant
percent of the queries made by a user in a search
engine are associated to a repeated search. Recom-
mender systems like (Das et al, 2007) and (Dou et
al., 2007) take advantage of this fact to refine the
search results and improve the search experience.
In this paper, we explore the use of personaliza-
tion in the context of voice searches rather than web
queries. Specifically, we focus on data from a multi-
modal cellphone-based business search application
(Acero et al, 2008). In such an application, repeated
queries can be a powerful tool for personalization.
These can be classified into short and long-term rep-
etitions. Short-term repetitions are typically caused
by a speech recognition error, which produces an in-
correct search result and makes the user repeat or
reformulate the query. On the other hand, long-term
repetitions, as in text-based search applications, oc-
cur when the user needs to access some information
that was accessed previously, for example, the exact
location of a pet clinic.
This paper proposes several different user per-
sonalization methods for increasing the recognition
accuracy in Voice Search applications. The pro-
posed personalization methods are based on extract-
ing short-term, long-term and Web-based features
from the user?s history. In recent years, other user
personalization methods like deriving personalized
pronunciations have proven successful in the context
of mobile applications (Deligne et al, 2002).
The rest of this paper is organized as follows: Sec-
tion 2 describes the classification method used for
rescoring the recognition hypotheses. Section 3 de-
scribes the proposed personalization methods. Sec-
tion 4 describes the experiments carried out. Finally,
101
conclusions from this work are drawn in section 5.
2 Rescoring procedure
2.1 Log linear classification
Our work will proceed by using a log-linear clas-
sifier similar to the maximum entropy approach of
(Berger and Della Pietra, 1996) to predict which
word sequence W appearing on an n-best list N is
most likely to be correct. This is estimated as
P (W |N) = exp(
?
i ?ifi(W,N))?
W ??N exp(?i ?ifi(W ?, N)) . (1)
The feature functions fi(W,N) can represent ar-
bitrary attributes of W and N . This can be seen
to be the same as a maximum entropy formulation
where the class is defined as the word sequence (thus
allowing potentially infinite values) but with sums
restricted as a computational convenience to only
those class values (word strings) appearing on the n-
best list. The models were estimated with a widely
available toolkit (Mahajan, 2007).
2.2 Feature extraction
Given the use of a log-linear classifier, the crux of
our work lies in the specific features used. As a base-
line, we take the hypothesis rank, which results in
the 1-best accuracy of the decoder. Additional fea-
tures were obtained from the personalization meth-
ods described in the following section.
3 Personalization methods
3.1 Short-term personalization
Short-term personalization aims at modeling the re-
pair/repetition behavior of the user. Short-term fea-
tures are a mechanism suitable for representing neg-
ative evidence: if the user repeats a utterance it nor-
mally means that the hypotheses in the previous n-
best lists are not correct. For this reason, if a hy-
pothesis is contained in a preceding n-best list, that
hypothesis should be weighted negatively during the
rescoring.
A straightforward method for identifying likely
repetitions consists of using a fixed size time win-
dow and considering all the user queries within that
window as part of the same repetition round. Once
an appropriate window size has been determined,
the proposed short-term features can be extracted for
each hypothesis using a binary tree like the one de-
picted in figure 1, where feature values are in the
leaves of the tree.
Does a recent (60s) n-best
list contain the hypothesis
we are scoring?
seen = 1
seen & clicked = 0
seen & clicked = 0
No
Did the user click
on that hypothesis?
Yes
seen = 0
seen & clicked = 1
seen & clicked = 0
No
seen = 0
seen & clicked = 0
seen & clicked = 1
Yes
Figure 1: Short-term feature extraction (note that over-
lines mean ?do not?).
Given these features, we expect ?seen and not
clicked? to have a negative weight while ?seen and
clicked? should have a positive weight.
3.2 Long-term personalization
Long-term personalization consists of using the user
history (i.e. recognition hypotheses that were con-
firmed by the user in the past) to predict which
recognition results are more likely. The assumption
here is that recognition hypotheses in the n-best list
that match or ?resemble? those in the user history are
more likely to be correct. The following list enumer-
ates the long-term features proposed in this work:
? User history (occurrences): number of times
the hypothesis appears in the user history.
? User history (alone): 1 if the hypothesis ap-
pears in the user history and no other compet-
ing hypothesis does, otherwise 0.
? User history (most clicked): 1 if the hypothe-
sis appears in the user history and was clicked
more times than any other competing hypothe-
sis.
? User history (most recent): 1 if the hypothe-
sis appears in the user history and was clicked
102
more recently than any other competing hy-
pothesis.
? User history (edit distance): minimum edit dis-
tance between the hypothesis and the closest
query in the user history, normalized by the
number of words.
? User history (words in common): maximum
number of words in common between the hy-
pothesis and each of the queries in the user his-
tory, normalized by the number of words in the
hypothesis.
? User history (plural/singular): 1 if either the
plural or singular version of the hypothesis ap-
pears in the user history, otherwise 0.
? Global history: 1 if the hypothesis has ever
been clicked by any user, otherwise 0.
? Global history (alone): 1 if the hypothesis is the
only one in the n-best that has ever been clicked
by any user, otherwise 0.
Note that the last two features proposed make
use of the ?global history? which comprises all the
queries made by any user.
3.3 LiveSearch-based features
Typically, users ask for businesses that exist, and if
a business exists it probably appears in a Web docu-
ment indexed by Live Search (Live Search, 2006). It
is reasonable to assume that the relevance of a given
business is connected to the number of times it ap-
pears in the indexed Web documents, and in this sec-
tion we derive such features.
For the scoring process, an application has been
built that makes automated queries to Live Search,
and for each hypothesis in the n-best list obtains the
number of Web documents in which it appears. De-
noting by x the number of Web documents in which
the hypothesis (the exact sequence of words, e.g.
?tandoor indian restaurant?) appears, the following
features are proposed:
? Logarithm of the absolute count: log(x).
? Search results rank: sort the hypotheses in the
n-best list by their relative value of x and use
the rank as a feature.
? Relative relevance (I): 1 if the hypothesis was
not found and there is another hypothesis in the
n-best list that was found more than 100 times,
otherwise 0.
? Relative relevance (II): 1 if the the hypothesis
appears fewer than 10 times and there is an-
other hypothesis in the n-best list that appears
more than 100 times, otherwise 0.
4 Experiments
4.1 Data
The data used for the experiments comprises 22473
orthographically transcribed business utterances ex-
tracted from a commercially deployed large vocabu-
lary directory assistance system.
For each of the transcribed utterances two n-best
lists were produced, one from the commercially de-
ployed system and other from an enhanced decoder
with a lower sentence error rate (SER). In the exper-
iments, due to their lower oracle error rate, n-bests
from the enhanced decoder were used for doing the
rescoring. However, these n-bests do not correspond
to the listings shown in the user?s device screen (i.e.
do not match the user interaction) so are not suit-
able for identifying repetitions. For this reason, the
short term features were computed by comparing a
hypothesis from the enhanced decoder with the orig-
inal n-best list from the immediate past. Note that all
other features were computed solely with reference
to the n-bests from the enhanced decoder.
A rescoring subset was made from the original
dataset using only those utterances in which the n-
best lists contain the correct hypothesis (in any po-
sition) and have more than one hypothesis. For all
other utterances, rescoring cannot have any effect.
The size of the rescoring subset is 43.86% the size
of the original dataset for a total of 9858 utterances.
These utterances were chronologically partitioned
into a training set containing two thirds and a test
set with the rest.
4.2 Results
The baseline system for the evaluation of the pro-
posed features consist of a ME classifier trained on
only one feature, the hypothesis rank. The resulting
sentence error rate (SER) of this classifier is that of
the best single path, and it is 24.73%. To evaluate
103
the contribution of each of the features proposed in
section 3, a different ME classifier was trained us-
ing that feature in addition to the baseline feature.
Finally, another ME classifier was trained on all the
features together.
Table 1 summarizes the Sentence Error Rate
(SER) for each of the proposed features in isolation
and all together respect to the baseline. ?UH? stands
for user history.
Features SER
Hypothesis rank (baseline) 24.73%
base + repet. (seen) 24.48%
base + repet. (seen & clicked) 24.32%
base + repet. (seen & clicked) 24.73%
base + UH (occurrences) 23.76%
base + UH (alone) 23.79%
base + UH (most clicked) 23.73%
base + UH (most recent) 23.88%
base + UH (edit distance) 23.76%
base + UH (words in common) 24.60%
base + UH (plural/singular) 24.76%
base + GH 24.63%
base + GH (alone) 24.66%
base + Live Search (absolute count) 24.35%
base + Live Search (rank) 24.85%
base + Live Search (relative I) 23.51%
base + Live Search (relative II) 23.69%
base + all 21.54%
Table 1: Sentence Error Rate (SER) for each of the fea-
tures in isolation and for the combination of all of them.
5 Conclusions
The proposed features reduce the SER of the base-
line system by 3.19% absolute on the rescoring set,
and by 1.40% absolute on the whole set of tran-
scribed utterances.
Repetition based features are moderately useful;
by incorporating them into the rescoring it is possi-
ble to reduce the SER from 24.73% to 24.32%. Al-
though repetitions cover a large percentage of the
data, it is believed that inconsistencies in the user
interaction (the right listing is displayed but not con-
firmed by the user) prevented further improvement.
As expected, long-term personalization based fea-
tures contribute to improve the classification accu-
racy. The UH (occurrences) feature by itself is able
to reduce the SER in about a 1%.
Live Search has shown a very good potential for
feature extraction. In this respect it is interesting to
note that a right design of the features seems critical
to take full advantage of it. The relative number of
counts of one hypothesis respect to other hypotheses
in the n-best list is more informative than an absolute
or ranked count. A simple feature using this kind of
information, like Live Search (relative I), can reduce
the SER in more than 1% respect to the baseline.
Finally, it has been shown that personalization
based features can complement each other very well.
References
Alex Acero, Neal Bernstein, Rob Chambers, Yun-Cheng
Ju, Xiao Li, Julian Odell, Patrick Nguyen, Oliver
Scholtz and Geoffrey Zweig. 2008. Live Search
for Mobile: Web Services by Voice on the Cellphone.
ICASSP 2008, March 31 2008-April 4 2008. Las Ve-
gas, NV, USA.
Adam L. Berger; Vincent J. Della Pietra; Stephen A.
Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 1996. 22(1): p. 39-72.
Abhinandan Das, Mayur Datar and Ashutosh Garg.
2007. Google News Personalization: Scalable Online
Collaborative Filtering. WWW 2007 / Track: Indus-
trial Practice and Experience May 8-12, 2007. Banff,
Alberta, Canada.
Sabine Deligne, Satya Dharanipragada, Ramesh
Gopinath, Benoit Maison, Peder Olsen and Harry
Printz. 2002. A robust high accuracy speech recog-
nition system for mobile applications. Speech and
Audio Processing, IEEE Transactions on, Nov 2002,
Volume: 10, Issue: 8, On page(s): 551- 561.
Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007.
A large-scale evaluation and analysis of personalized
search strategies. In WWW ?07: Proceedings of the
16th international conference on World Wide Web,
pages 581 - 590, New York, NY, USA, 2007. ACM
Press.
Live Search. ?http://www.live.com,?.
Milind Mahajan. 2007. Conditional Maximum-Entropy
Training Tool http://research.microsoft.com/en-
us/downloads/9f199826-49d5-48b6-ba1b-
f623ecf36432/.
Jaime Teevan, Eytan Adar, Rosie Jones and Michael A.
S. Potts. 2007. Information Re-Retrieval: Repeat
Queries in Yahoos Logs. SIGIR, 2007.
104
Proceedings of the Second Workshop on Statistical Machine Translation, pages 72?79,
Prague, June 2007. c?2007 Association for Computational Linguistics
Training Non-Parametric Features for Statistical Machine Translation
Patrick Nguyen, Milind Mahajan and Xiaodong He
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052
{panguyen,milindm,xiaohe}@microsoft.com
Abstract
Modern statistical machine translation sys-
tems may be seen as using two components:
feature extraction, that summarizes informa-
tion about the translation, and a log-linear
framework to combine features. In this pa-
per, we propose to relax the linearity con-
straints on the combination, and hence relax-
ing constraints of monotonicity and indepen-
dence of feature functions. We expand fea-
tures into a non-parametric, non-linear, and
high-dimensional space. We extend empir-
ical Bayes reward training of model param-
eters to meta parameters of feature genera-
tion. In effect, this allows us to trade away
some human expert feature design for data.
Preliminary results on a standard task show
an encouraging improvement.
1 Introduction
In recent years, statistical machine translation have
experienced a quantum leap in quality thanks to au-
tomatic evaluation (Papineni et al, 2002) and error-
based optimization (Och, 2003). The conditional
log-linear feature combination framework (Berger,
Della Pietra and Della Pietra, 1996) is remarkably
simple and effective in practice. Therefore, re-
cent efforts (Och et al, 2004) have concentrated on
feature design ? wherein more intelligent features
may be added. Because of their simplicity, how-
ever, log-linear models impose some constraints on
how new information may be inserted into the sys-
tem to achieve the best results. In other words,
new information needs to be parameterized care-
fully into one or more real valued feature functions.
Therefore, that requires some human knowledge and
understanding. When not readily available, this
is typically replaced with painstaking experimenta-
tion. We propose to replace that step with automatic
training of non-parametric agnostic features instead,
hopefully relieving the burden of finding the optimal
parameterization.
First, we define the model and the objective func-
tion training framework, then we describe our new
non-parametric features.
2 Model
In this section, we describe the general log-linear
model used for statistical machine translation, as
well as a training objective function and algorithm.
The goal is to translate a French (source) sentence
indexed by t, with surface string ft. Among a set of
Kt outcomes, we denote an English (target) a hy-
pothesis with surface string e(t)k indexed by k.
2.1 Log-linear Model
The prevalent translation model in modern systems
is a conditional log-linear model (Och and Ney,
2002). From a hypothesis e(t)k , we extract features
h(t)k , abbreviated hk, as a function of e
(t)
k and ft. The
conditional probability of a hypothesis e(t)k given a
source sentence ft is:
pk , p(e(t)k |ft) ,
exp[? ? hk]
Zft;?
,
72
where the partition function Zft;? is given by:
Zft;? =
?
j
exp[? ? hj ].
The vector of parameters of the model ?, gives a
relative importance to each feature function compo-
nent.
2.2 Training Criteria
In this section, we quickly review how to adjust ?
to get better translation results. First, let us define
the figure of merit used for evaluation of translation
quality.
2.2.1 BLEU Evaluation
The BLEU score (Papineni et al, 2002) was de-
fined to measure overlap between a hypothesized
translation and a set of human references. n-gram
overlap counts {cn}4n=1 are computed over the test
set sentences, and compared to the total counts of
n-grams in the hypothesis:
cn,(t)k , max. # of matching n-grams for hyp. e(t)k ,
an,(t)k , # of n-grams in hypothesis e(t)k .
Those quantities are abbreviated ck and ak to sim-
plify the notation. The precision ratio Pn for an n-
gram order n is:
Pn ,
?
t c
n,(t)
k
?
t a
n,(t)
k
.
A brevity penalty BP is also taken into account, to
avoid favoring overly short sentences:
BP , min{1; exp(1 ? ra)},
where r is the average length of the shortest sen-
tence1, and a is the average length of hypotheses.
The BLEU score the set of hypotheses {e(t)k } is:
B({e(t)k }) , BP ? exp
( 4
?
n=1
1
4 logPn
)
.
1As implemented by NIST mteval-v11b.pl.
Oracle BLEU hypothesis: There is no easy way
to pick the set hypotheses from an n-best list that
will maximize the overall BLEU score. Instead, to
compute oracle BLEU hypotheses, we chose, for
each sentence independently, the hypothesis with the
highest BLEU score computed for a sentence itself.
We believe that it is a relatively tight lower bound
and equal for practical purposes to the true oracle
BLEU.
2.2.2 Maximum Likelihood
Used in earlier models (Och and Ney, 2002), the
likelihood criterion is defined as the likelihood of an
oracle hypothesis e(t)k? , typically a single reference
translation, or alternatively the closest match which
was decoded. When the model is correct and infi-
nite amounts of data are available, this method will
converge to the Bayes error (minimum achievable
error), where we define a classification task of se-
lecting k? against all others.
2.2.3 Regularization Schemes
One can convert a maximum likelihood problem
into maximum a posteriori using Bayes? rule:
argmax
?
?
t
p(?|{e(t)k , ft}) = argmax?
?
t
pkp0(?),
where p0(?) is the prior distribution of ?. The
most frequently used prior in practice is the normal
prior (Chen and Rosenfeld, 2000):
log p0(?) , ?||?||
2
2?2 ? log |?|,
where ?2 > 0 is the variance. It can be thought of
as the inverse of a Lagrange multiplier when work-
ing with constrained optimization on the Euclidean
norm of ?. When not interpolated with the likeli-
hood, the prior can be thought of as a penalty term.
The entropy penalty may also be used:
H , ? 1T
T
?
t=1
Kt
?
k=1
pk log pk.
Unlike the Gaussian prior, the entropy is indepen-
dent of parameterization (i.e., it does not depend on
how features are expressed).
73
2.2.4 Minimum Error Rate Training
A good way of training ? is to minimize empirical
top-1 error on training data (Och, 2003). Compared
to maximum-likelihood, we now give partial credit
for sentences which are only partially correct. The
criterion is:
argmax
?
?
t
B({e(t)
k?
}) : e(t)
k?
= argmax
e(t)j
pj.
We optimize the ? so that the BLEU score of the
most likely hypotheses is improved. For that reason,
we call this criterion BLEU max. This function is
not convex and there is no known exact efficient op-
timization for it. However, there exists a linear-time
algorithm for exact line search against that objec-
tive. The method is often used in conjunction with
coordinate projection to great success.
2.2.5 Maximum Empirical Bayes Reward
The algorithm may be improved by giving partial
credit for confidence pk of the model to partially cor-
rect hypotheses outside of the most likely hypothe-
sis (Smith and Eisner, 2006):
1
T
T
?
t=1
Kt
?
k=1
pk logB({ek(t)}).
Instead of the BLEU score, we use its logrithm, be-
cause we think it is exponentially hard to improve
BLEU. This model is equivalent to the previous
model when pk give all the probability mass to the
top-1. That can be reached, for instance, when ?
has a very large norm. There is no known method
to train against this objective directly, however, ef-
ficient approximations have been developed. Again,
it is not convex.
It is hoped that this criterion is better suited for
high-dimensional feature spaces. That is our main
motivation for using this objective function through-
out this paper. With baseline features and on our
data set, this criterion also seemed to lead to results
similar to Minimum Error Rate Training.
We can normalize B to a probability measure
b({e(t)k }). The empirical Bayes reward also coin-
cides with a divergence D(p||b).
2.3 Training Algorithm
We train our model using a gradient ascent method
over an approximation of the empirical Bayes re-
ward function.
2.3.1 Approximation
Because the empirical Bayes reward is defined
over a set of sentences, it may not be decomposed
sentence by sentence. This is computationally bur-
densome. Its sufficient statistics are r, ?t ck and
?
t ak. The function may be reconstructed in a first-
order approximation with respect to each of these
statistics. In practice this has the effect of commut-
ing the expectation inside of the functional, and for
that reason we call this criterion BLEU soft. This ap-
proximation is called linearization (Smith and Eis-
ner, 2006). We used a first-order approximation for
speed, and ease of interpretation of the derivations.
The new objective function is:
J , log B?P +
4
?
n=1
1
4 log
?
t Ec
n,(t)
k
?
t Ea
n,(t)
k
,
where the average bleu penalty is:
log B?P , min{0; 1 ? r
Ek,ta1,(t)k
}.
The expectation is understood to be under the cur-
rent estimate of our log-linear model. Because B?P is
not differentiable, we replace the hard min function
with a sigmoid, yielding:
log B?P ? u(r ? Ek,ta1,(t)k )
(
1? r
Ek,ta1,(t)k
)
,
with the sigmoid function u(x) defines a soft step
function:
u(x) , 11 + e??x ,
with a parameter ? ? 1.
2.3.2 Gradients and Sufficient Statistics
We can obtain the gradients of the objective func-
tion using the chain rule by first differentiating with
respect to the probability. First, let us decompose
the log-precision of the expected counts:
log P?n = log Ecn,(t)k ? log Ea
n,(t)
k .
74
Each n-gram precision may be treated separately.
For each n-gram order, let us define sufficient statis-
tics ? for the precision:
?c? ,
?
t,k
(??pk)ck; ?a? ,
?
t,k
(??pk)ak,
where the gradient of the probabilities is given by:
??pk = pk(hk ? h?),
with:
h? ,
Kt
?
j=1
pjhj .
The derivative of the precision P?n is:
??log P?n =
1
T
[ ?c?
Eck
? ?
a
?
Eak
]
For the length, the derivative of log B?P is:
u(r?Ea)
[
(ra ? 1)[1 ? u(r ? Ea)]? +
r
(Ea)2
]
?a1? ,
where ?a1? is the 1-gram component of ?a?. Finally,
the derivative of the entropy is:
??H =
?
k,t
(1 + log pk)??pk.
2.3.3 RProp
For all our experiments, we chose RProp (Ried-
miller and Braun, 1992) as the gradient ascent al-
gorithm. Unlike other gradient algorithms, it is only
based on the sign of the gradient components at each
iteration. It is relatively robust to the objective func-
tion, requires little memory, does not require meta
parameters to be tuned, and is simple to implement.
On the other hand, it typically requires more iter-
ations than stochastic gradient (Kushner and Yin,
1997) or L-BFGS (Nocedal and Wright, 1999).
Using fairly conservative stopping criteria, we ob-
served that RProp was about 6 times faster than Min-
imum Error Rate Training.
3 Adding Features
The log-linear model is relatively simple, and is usu-
ally found to yield good performance in practice.
With these considerations in mind, feature engineer-
ing is an active area of research (Och et al, 2004).
Because the model is fairly simple, some of the in-
telligence must be shifted to feature design. After
having decided what new information should go in
the overall score, there is an extra effort involved
in expressing or parameterizing features in a way
which will be easiest for the model learn. Experi-
mentation is usually required to find the best config-
uration.
By adding non-parametric features, we propose
to mitigate the parameterization problem. We will
not add new information, but rather, propose a way
to insulate research from the parameterization. The
system should perform equivalently invariant of any
continuous invertible transformation of the original
input.
3.1 Existing Features
The baseline system is a syntax based machine
translation system as described in (Quirk, Menezes
and Cherry, 2005). Our existing feature set includes
11 features, among which the following:
? Target hypothesis word count.
? Treelet count used to construct the candidate.
? Target language models, based on the Giga-
word corpus (5-gram) and target side of parallel
training data (3-gram).
? Order models, which assign a probability to the
position of each target node relative to its head.
? Treelet translation model.
? Dependency-based bigram language models.
3.2 Re-ranking Framework
Our algorithm works in a re-ranking framework.
In particular, we are adding features which are not
causal or additive. Features for a hypothesis may
not be accumulating by looking at the English (tar-
get) surface string words from the left to the right
and adding a contribution per word. Word count,
for instance, is causal and additive. This property
is typically required for efficient first-pass decod-
ing. Instead, we look at a hypothesis sentence as a
whole. Furthermore, we assume that the Kt-best list
provided to us contains the entire probability space.
75
In particular, the computation of the partition func-
tion is performed over all Kt-best hypotheses. This
is clearly not correct, and is the subject of further
study. We use the n-best generation scheme inter-
leaved with ? optimization as described in (Och,
2003).
3.3 Issues with Parameterization
As alluded to earlier, when designing a new feature
in the log-linear model, one has to be careful to find
the best embodiment. In general, a set of features
must satisfy the following properties, ranked from
strict to lax:
? Linearity (warping)
? Monotonicity
? Independence (conjunction)
Firstly, a feature should be linearly correlated with
performance. There should be no region were it
matters less than other regions. For instance, in-
stead of a word count, one might consider adding
its logarithm instead. Secondly, the ?goodness? of a
hypothesis associated with a feature must be mono-
tonic. For instance, using the signed difference be-
tween word count in the French (source) and En-
glish (target) does not satisfy this. (In that case, one
would use the absolute value instead.) Lastly, there
should be no inter-dependence between features. As
an example, we can consider adding multiple lan-
guage model scores. Whether we should consider
ratios those of, globally linearly or log-linearly in-
terpolating them, is open to debate. When features
interact across dimensions, it becomes unclear what
the best embodiment should be.
3.4 Non-parametric Features
A generic solution may be sought in non-parametric
processing. Our method can be derived from a quan-
tized Parzen estimate of the feature density function.
3.4.1 Parzen Window
The Parzen window is an early empirical kernel
method (Duda and Hart, 1973). For an observation
hm, we extrapolate probability mass around it with
a smoothing window ?(?). The density function is:
p(h) = 1M
K
?
m=1
?(h? hm),
assuming ?(?) is a density function. Parzen win-
dows converge to the true density estimate, albeit
slowly, under weak assumptions.
3.4.2 Bin Features
One popular way of using continuous features in
log-linear models is to convert a single continuous
feature into multiple ?bin? features. Each bin feature
is defined as the indicator function of whether the
original continuous feature was in a certain range.
The bins were selected so that each bin collects an
equal share of the probability mass. This is equiva-
lent to the maximum likelihood estimate of the den-
sity function subject to a fixed number of rectangular
density kernels. Since that mapping is not differen-
tiable with respect to the original features, one may
use sigmoids to soften the boundaries.
Bin features are useful to relax the requirements
of linearity and monotonicity. However, because
they work on each feature individually, they do not
address the problem of inter-dependence between
features.
3.4.3 Gaussian Mixture Model Features
Bin features may be generalized to multi-
dimensional kernels by using a Gaussian smoothing
window instead of a rectangular window. The direct
analogy is vector quantization. The idea is to weight
specific regions of the feature space differently. As-
suming that we have M Gaussians each with mean
vector ?m and diagonal covariance matrix Cm, and
prior weight wm. We will add m new features, each
defined as the posterior in the mixture model:
hm , wmN (h;?m, Cm)?
r wrN (h;?r, Cr)
.
It is believed that any reasonable choice of kernels
will yield roughly equivalent results (Povey et al,
2004), if the amount of training data and the number
of kernels are both sufficiently large. We show two
methods for obtaining clusters. In contrast with bins,
lossless representation becomes rapidly impossible.
ML kernels: The canonical way of obtaining clus-
ter is to use the standard Gaussian mixture training.
First, a single Gaussian is trained on the whole data
set. Then, the Gaussian is split into two Gaussians,
with each mean vector perturbed, and the Gaus-
sians are retrained using maximum-likelihood in an
76
expectation-maximization framework (Rabiner and
Huang, 1993). The number of Gaussians is typically
increased exponentially.
Perceptron kernels: We also experimented with
another quicker way of obtaining kernels. We
chose an equal prior and a global covariance matrix.
Means were obtained as follows: for each sentence
in the training set, if the top-1 candidate was differ-
ent from the approximate maximum oracle BLEU
hypothesis, both were inserted. It is a quick way
to bootstrap and may reach the oracle BLEU score
quickly.
In the limit, GMMs will converge to the oracle
BLEU. In the next section, we show how to re-
estimate these kernels if needed.
3.5 Re-estimation Formul?
Features may also be trained using the same empir-
ical maximum Bayes reward. Let ? be the hyper-
parameter vector used to generate features. In the
case of language models, for instance, this could be
backoff weights. Let us further assume that the fea-
ture values are differentiable with respect to ?. Gra-
dient ascent may be applied again but this time with
respect to ?. Using the chain rule:
??J = (??h)(?hpk)(?pkJ),
with ?hpk = pk(1 ? pk)?. Let us take the example
of re-estimating the mean of a Gaussian kernel ?m:
??mhm = ?wmhm(1 ? hm)C?1m (?m ? h),
for its own feature, and for other posteriors r 6= m:
??mhr = ?wrhrhmC?1m (?m ? h),
which is typically close to zero if no two Gaussians
fire simultaneously.
4 Experimental Results
For our experiments, we used the standard NIST
MT-02 data set to evaluate our system.
4.1 NIST System
A relatively simple baseline was used for our exper-
iments. The system is syntactically-driven (Quirk,
Menezes and Cherry, 2005). The system was trained
on 175k sentences which were selected from the
NIST training data (NIST, 2006) to cover words in
source language sentences of the MT02 develop-
ment and evaluation sets. The 5-gram target lan-
guage model was trained on the Gigaword mono-
lingual data using absolute discounting smoothing.
In a single decoding, the system generated 1000 hy-
potheses per sentence whenever possible.
4.2 Leave-one-out Training
In order to have enough data for training, we gen-
erated our n-best lists using 10-fold leave-one-out
training: base feature extraction models were trained
on 9/10th of the data, then used for decoding the
held-out set. The process was repeated for all 10
parts. A single ? was then optimized on the com-
bined lists of all systems. That ? was used for an-
other round of 10 decodings. The process was re-
peated until it reached convergence after 7 iterations.
Each decoding generated about 100 hypotheses, and
there was relatively little overlap across decodings.
Therefore, there were about 1M hypotheses in total.
The combined list of all iterations was used for all
subsequent experiments of feature expansion.
4.3 BLEU Training Results
We tried training systems under the empirical Bayes
reward criterion, and appending either bin or GMM
features. We will find that bin features are es-
sentially ineffective while GMM features show a
modest improvement. We did not retrain hyper-
parameters.
4.3.1 Convexity of the Empirical Bayes Reward
The first question to ask is how many local op-
tima does the cost surface have using the standard
features. A complex cost surface indicates that some
gain may be had with non-linear features, but it also
shows that special care should be taken during op-
timization. Non-convexity is revealed by sensitivity
to initialization points. Thus, we decided to initial-
ize from all vertices of the unit hypercube, and since
we had 11 features, we ran 211 experiments. The
histogram of BLEU scores on dev data after conver-
gence is shown on Figure 1. We also plotted the his-
togram of an example dimension in Figure 2. The
range of BLEU scores and lambdas is reasonably
narrow. Even though ? seems to be bimodal, we see
77
that this does not seriously affect the BLEU score.
This is not definitive evidence but we provisionally
pretend that the cost surface is almost convex for
practical purposes.
24.8 24.9 25 25.1 25.2 25.3 25.4
0
200
400
600
800
BLEU score
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 1: Histogram of BLEU scores after training
from 211 initializations.
?60 ?40 ?20 0
0
100
200
300
400
500
600
700
? value
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 2: Histogram of one ? parameter after train-
ing from 211 initializations.
4.3.2 Bin Features
A log-linear model can be converted into a bin
feature model nearly exactly by setting ? values
in such a way that scores will be equal. Equiva-
lent weights (marked as ?original? in Figure 3) have
the shape of an error function (erf): this is because
the input feature is a cummulative random variable,
which quickly converges to a Gaussian (by the cen-
tral limit theorem). After training the ? weights for
the log-linear model, weights may be converted into
bins and re-trained. On Figure 3, we show that relax-
ing the monotonicity constraint leads to rough val-
ues for ?. Surprisingly, the BLEU score and ob-
jective on the training set only increases marginally.
Starting from ? = 0, we obtained nearly exactly the
same training objective value. By varying the num-
ber of bins (20-50), we observed similar behavior as
well.
0 10 20 30 40 50
?1.5
?1
?0.5
0
0.5
1
bin id
va
lu
e
 
 
original weights
trained weights
Figure 3: Values before and after training bin fea-
tures. Monotonicity constraint has been relaxed.
BLEU score is virtually unchanged.
4.3.3 GMM Features
Experiments were carried out with GMM fea-
tures. The summary is shown on Table 1. The
baseline was the log-linear model trained with the
baseline features. The baseline features are included
in all systems. We trained GMM models using the
iterative mixture splitting interleaved with EM re-
estimation, split up to 1024 and 16384 Gaussians,
which we call GMM-ML-1k and GMM-ML-16k re-
spectively. We also used the ?perceptron? selec-
tion features on the training set to bootstrap quickly
to 300k Gaussians (GMM-PCP-300k), and ran the
same algorithm on the development set (GMM-
PCP-2k). Therefore, GMM-PCP-300k had 300k
features, and was trained on 175k sentences (each
with about 700 hypotheses). For all experiments but
?unreg? (unregularized), we chose a prior Gaussian
prior with variance empirically by looking at the de-
velopment set. For all but GMM-PCP-300k, regu-
larization did not seem to have a noticeably positive
effect on development BLEU scores. All systems
were seeded with the baseline log-linear model, and
78
all additional weights set to zero, and then trained
with about 50 iterations, but convergence in BLEU
score, empirical reward, and development BLEU
score occurred after about 30 iterations. In that set-
ting, we found that regularized empirical Bayes re-
ward, BLEU score on training data, and BLEU score
on development and evaluation to be well corre-
lated. Cursory experiments revealed that using mul-
tiple initializations did not significantly alter the fi-
nal BLEU score.
System Train Dev Eval
Oracle 14.10 N/A N/A
Baseline 10.95 35.15 25.95
GMM-ML-1k 10.95 35.15 25.95
GMM-ML-16k 11.09 35.25 25.89
GMM-PCP-2k 10.95 35.15 25.95
GMM-PCP-300k-unreg 13.00 N/A N/A
GMM-PCP-300k 12.11 35.74 26.42
Table 1: BLEU scores for GMM features vs the lin-
ear baseline, using different selection methods and
number of kernels.
Perceptron kernels based on the training set im-
proved the baseline by 0.5 BLEU points. We mea-
sured significance with the Wilcoxon signed rank
test, by batching 10 sentences at a time to produce
an observation. The difference was found to be sig-
nificant at a 0.9-confidence level. The improvement
may be limited due to local optima or the fact that
original feature are well-suited for log-linear mod-
els.
5 Conclusion
In this paper, we have introduced a non-parametric
feature expansion, which guarantees invariance to
the specific embodiment of the original features.
Feature generation models, including feature ex-
pansion, may be trained using maximum regular-
ized empirical Bayes reward. This may be used as
an end-to-end framework to train all parameters of
the machine translation system. Experimentally, we
found that Gaussian mixture model (GMM) features
yielded a 0.5 BLEU improvement.
Although this is an encouraging result, further
study is required on hyper-parameter re-estimation,
presence of local optima, use of complex original
features to test the effectiveness of the parameteri-
zation invariance, and evaluation on a more compet-
itive baseline.
References
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. ACL?02.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, vol 22:1, pp.
39?71.
S. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and
Audio Processing, vol 8:2, pp. 37?50.
R. O. Duda and P. E. Hart. 1973. Pattern Classification
and Scene Analysis. Wiley & Sons, 1973.
H. J. Kushner and G. G. Yin. 1997. Stochastic Approxi-
mation Algorithms and Applications. Springer-Verlag,
1997.
National Institute of Standards and Technology. 2006.
The 2006 Machine Translation Evaluation Plan.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer-Verlag, 1999.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. ACL?03.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
HLT/NAACL?04.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. ACL?02.
D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau
and G. Zweig. 2004. fMPE: Discriminatively trained
features for speech recognition. RT?04 Meeting.
C. Quirk, A. Menezes and C. Cherry. 2005. De-
pendency Tree Translation: Syntactically Informed
Phrasal SMT. ACL?05.
L. R. Rabiner and B.-H. Huang. 1993. Fundamentals of
Speech Recognition. Prentice Hall.
M. Riedmiller and H. Braun. 1992. RPROP: A Fast
Adaptive Learning Algorithm. Proc. of ISCIS VII.
D. A. Smith and J. Eisner. 2006. Minimum-Risk
Annealing for Training Log-Linear Models. ACL-
COLING?06.
79
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21?28,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Learning N-Best Correction Models from Implicit User Feedback  
in a Multi-Modal Local Search Application 
 
 
Dan Bohus, Xiao Li, Patrick Nguyen, Geoffrey Zweig 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
{dbohus, xiaol, panguyen, gzweig}@microsoft.com 
 
 
 
 
 
 
Abstract 
We describe a novel n-best correction model 
that can leverage implicit user feedback (in 
the form of clicks) to improve performance in 
a multi-modal speech-search application. The 
proposed model works in two stages. First, the 
n-best list generated by the speech recognizer 
is expanded with additional candidates, based 
on confusability information captured via user 
click statistics. In the second stage, this ex-
panded list is rescored and pruned to produce 
a more accurate and compact n-best list. Re-
sults indicate that the proposed n-best correc-
tion model leads to significant improvements 
over the existing baseline, as well as other tra-
ditional n-best rescoring approaches.  
1 Introduction 
Supported by years of research in speech recogni-
tion and related technologies, as well as advances 
in mobile devices, speech-enabled mobile applica-
tions are finally transitioning into day-to-day use. 
One example is Live Search for Windows Mobile 
(2008), a speech-enabled application that allows 
users to get access to local information by speaking 
a query into their device. Several other systems 
operating in similar domains have recently become 
available (TellMeByMobile, 2008; Nuance Mobile 
Search, 2008; V-Lingo Mobile, 2008; VoiceSignal 
Search, 2008.) 
Traditionally, multi-modal systems leverage the 
additional input channels such as text or buttons to 
compensate for the current shortcomings of speech 
recognition technology. For instance, after the user 
speaks a query, the Live Search for Windows Mo-
bile application displays a confirmation screen that 
contains the n-best recognition results. The user 
selects the correct hypothesis using the buttons on 
the device, and only then the system displays the 
corresponding search results (see Figure 1.) 
We argue that ideally multi-modal systems 
could use the additional, more accurate input chan-
nels not only for confirmation or immediate cor-
rection, but also to learn from the interaction and 
improve their performance over time, without ex-
plicit human supervision. For example, in the inte-
raction paradigm described above, apart from 
providing the means for selecting the correct rec-
ognition result from an n-best list, the user click on 
a hypothesis can provide valuable information 
about the errors made by system, which could be 
exploited to further improve performance.  
Consider for instance the following numbers 
from an analysis of logged click data in the Live 
Search for Windows Mobile system. Over a certain 
period of time, the results Beer and Gear were dis-
played together in an n-best list 122 times. Out of 
these cases, Beer was clicked 67% of the time, and 
Gear was never clicked. In 25% of the cases when 
Beer was selected, Gear was incorrectly presented 
above (i.e. higher than) Beer in the n-best list. 
More importantly, there are also 82 cases in which 
Gear appears in an n-best list, but Beer does not. A 
manual inspection reveals that, in 22% of these 
cases, the actual spoken utterance was indeed Beer. 
The clicks therefore indicate that the engine often 
misrecognizes Gear instead of Beer.  
21
Ideally, the system should be able to take advan-
tage of this information and use the clicks to create 
an automatic positive feedback loop. We can envi-
sion several ways in which this could be accom-
plished. A possible approach would be to use all 
the clicked results to adapt the existing language or 
acoustic models. Another, higher-level approach is 
to treat the recognition process as a black-box, and 
use the click feedback (perhaps also in conjunction 
with other high-level information) to post-process 
the results recognition results. 
While both approaches have their merits, in this 
work we concentrate on the latter paradigm. We 
introduce a novel n-best correction model that le-
verages the click data to improve performance in a 
speech-enabled multi-modal application. The pro-
posed model works in two stages. First, the n-best 
list generated by the speech recognizer is expanded 
with additional candidates, based on results confu-
sability information captured by the click statistics. 
For instance, in the 82 cases we mentioned above 
when Gear was present in the n-best list but Beer 
was not, Beer (as well as potentially other results) 
would also be added to form an expanded n-best 
list. The expanded list is then rescored and pruned 
to construct a corrected, more accurate n-best list.  
The proposed approach, described in detail in 
Section 3, draws inspiration from earlier work in 
post-recognition error-correction models (Ringger 
and Allen, 1996; Ringger and Allen, 1997) and n-
best rescoring (Chotimongkol and Rudnicky, 2001; 
Birkenes et al, 2007). The novelty of our approach 
lies in: (1) the use of user click data in a deployed 
multi-modal system for creating a positive feed-
back loop, and (2) the development of an n-best 
correction model based on implicit feedback that 
outperforms traditional rescoring-only approaches. 
Later on, in Section 5, we will discuss in more de-
tail the relationship of the proposed approach to 
these and other works previously reported in the 
literature.  
Before moving on to describe the n-best correc-
tion model in more detail, we give a high-level 
overview of Live Search for Windows Mobile, the 
multi-modal, mobile local search application that 
provided the test-bed for evaluating this work.  
2 Live Search for Windows Mobile  
Live Search for Windows Mobile is an application 
that enables local web-search on mobile devices. In 
its current version, it allows users to find informa-
tion about local businesses and restaurants, to ob-
tain driving directions, explore maps, view current 
traffic, get movie show-times, etc. A number of 
screen-shots are illustrated in Figure 1. 
Recently, Live Search for Windows Mobile has 
been extended with a speech interface (notice the 
Speak button assigned to the left soft-key in Figure 
1.a.) The speech-based interaction with the system 
proceeds as follows: the user clicks the Speak but-
ton and speaks the name of a local business, for 
instance A-B-C Hauling, or a general category such 
as Vietnamese Restaurants. The application end-
points the audio and forwards it over the data 
channel to a server (Figure 1.b.) Recognition is 
performed on the server side, and the resulting n-
best list is sent back to the client application, where 
it is displayed to the user (Figure 1.c.) The user can 
select the correct item from the n-best list, re-speak 
the request, or abandon the interaction altogether 
by pressing Cancel. Once the user selects an item in 
the n-best list, the corresponding search results are 
displayed (Figure 1.d.) 
(a) (b) (c) (d) 
Figure 1. Windows Live Search for Mobile. (a) initial screen; (b) user is speaking a request; (c) n-best list 
is presented; (d) final search results are displayed 
22
Apart from business names, the system also 
handles speech input for addresses, as well as 
compound requests, such as Shamiana Restaurant 
in Kirkland, Washington. For the latter cases, a 
two-tier recognition and confirmation process is 
used. In the first stage a location n-best list is gen-
erated and sent to the client for confirmation. After 
the user selects the location, a second recognition 
stage uses a grammar tailored to that specific loca-
tion to re-recognize the utterance. The client then 
displays the final n-best list from which the user 
can select the correct result. 
Several details about the system architecture and 
the structure of the recognition process have been 
omitted here due to space considerations. For the 
interested reader, a more in-depth description of 
this system is available in (Acero et al, 2008).  
3 Approach 
We now turn our attention to the proposed n-best 
correction model 
3.1 Overview 
The model works in two stages, illustrated in Fig-
ure 2. In the first stage the n-best list produced by 
the speech recognizer is expanded with several 
alternative hypotheses. In the second stage, the 
expanded n-best list is rescored to construct the 
final, corrected n-best list.  
The n-best expansion step relies on a result con-
fusion matrix, constructed from click information. 
The matrix, which we will describe in more detail 
in the following subsection, contains information 
about which result was selected (clicked) by the 
user when a certain result was displayed. For in-
stance, in the example from Figure 2, the matrix 
indicates that when Burlington appeared in the n-
best list, Bar was clicked once, Bowling was 
clicked 13 times, Burger King was clicked twice, 
and Burlington was clicked 15 times (see hashed 
row in matrix.) The last element in the row indi-
cates that there were 7 cases in which Burlington 
was decoded, but nothing (?) was clicked. Essen-
tially, the matrix captures information about the 
confusability of different recognition results.  
The expansion step adds to an n-best list gener-
ated by the recognizer all the results that were pre-
viously clicked in conjunction with any one of the 
items in the given n-best list. For instance, in the 
example from Figure 2, the n-best list contains 
Sterling, Stirling, Burlington and Cooling. Based 
on the confusion matrix, this list will be expanded 
to also include Bar, Bowling, Burger King, Tow-
ing, and Turley. In this particular case, the correct 
recognition result, Bowling, is added in the ex-
panded n-best list.  
In the final step, the expanded list is rescored. In 
the previous example, for simplicity of explana-
tion, a simple heuristic for re-scoring was used: 
add all the counts on the columns corresponding to 
each expanded result. As a consequence, the cor-
Burlington 
Cooling 
Sterling 
Stirling 
0  ?   7    0     0    ?     0    0  ?   1   0    9 
0  ?   4    0     0    ?   10    1  ?   2   2    5 
0  ?   4    0     0    ?     4    1  ?   0   0    9 
B
u
rl
in
g
to
n
 
B
o
w
li
n
g
 
B
u
rg
er
 K
in
g
 
T
o
w
in
g
 
T
u
rl
ey
 
S
ti
rl
in
g
 
B
ar
 
S
te
rl
in
g
 
Sterling 
Stirling 
Burlington 
Cooling + ? 
Bowling  28 
Burlington  15 
Sterling  14 
Towing  3 
Burger King  2 
Stirling  2 
Turley  2 
Bar  1 
 
Bar 
Bowling 
Burger King 
Burlington 
Sterling  
Stirling 
Towing  
Turley 
 
? 
Result Confusion Matrix 
Initial  
N-Best 
Expanded 
N-Best 
Corrected 
(expanded & 
rescored) 
N-Best 
Figure 2. A confusion-based n-best correction model 
1  ? 13    2   15    ?     0    0  ?   0   0    7 
?
 
Stage 1: Expansion Stage 2: Rescoring 
23
rect recognition result, Bowling, was pushed to the 
top of the n-best list.  
We begin by formally describing the construc-
tion of the results confusability matrix and the ex-
pansion process in the next two sub-sections. Then, 
we describe three rescoring approaches. The first 
one is based on an error-correction model con-
structed from the confusion matrix. The other two, 
are more traditional rescoring approaches, based 
on language model adaptation.  
3.2 The Result Confusion Matrix 
The result confusion matrix is computed in a sim-
ple traversal of the click logs. The rows in the ma-
trix correspond to decoded results, i.e. results that 
have appeared in an n-best list. The columns in the 
matrix correspond to clicked (or intended) results, 
i.e. results that the user has clicked on in the n-best 
list. The entries at the intersection of row ? and 
column ? correspond to the number of times result 
? was clicked when result ? was decoded: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
In addition, the last column in the matrix, de-
noted ? contains the number of times no result was 
clicked when result ? was displayed: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
The rows in the matrix can therefore be used to 
compute the maximum likelihood estimate for the 
conditional probability distribution: 
 
???(?|?) =
?? ,?
 ?? ,??
 . 
 
The full dimensions of the result confusion ma-
trix can grow very large since the matrix is con-
structed at the result level (the average number of 
words per displayed result is 2.01). The number of 
rows equals the number of previously decoded re-
sults, and the number of columns equals the num-
ber of previously clicked results. However, the 
matrix is very sparse and can be stored efficiently 
using a sparse matrix representation. 
3.3 N-Best Expansion 
The first step in the proposed n-best correction 
model is to expand the initial n-best list with all 
results that have been previously clicked in con-
junction with the items in the current n-best list. 
Let?s denote by ? = {??}?=1..?  the initial n-best 
list produced by the speech recognizer. Then, the 
expanded n-best list ?? will contain all ?? , as well 
as all previously clicked results ? such that there 
exists ? with ??? ,? > 0. 
3.4 Confusion Matrix Based Rescoring  
Ideally, we would like to rank the hypotheses in 
the expanded list ?? according to ?(?|?), where ? 
represents the intended result and ? represents the 
acoustics of the spoken utterance. This can be re-
written as follows: 
 
                       ? ? ? =  ?(?|?) ? ?(?|?)? .             [1] 
 
The first component in this model is an error-
correction model ?(?|?). This model describes the 
conditional probability that the correct (or in-
tended) result is ? given that result ? has been de-
coded. While this conditional model cannot be 
constructed directly, we can replace it by a proxy - 
?(?|?), which models the probability that the re-
sult ? will be clicked, given that result ? was de-
coded. As mentioned earlier in subsection 3.2, this 
conditional probability distribution can be com-
puted from the result confusion matrix. In replac-
ing ? ? ?  with ?(?|?), we are making the 
assumption that the clicks correspond indeed to the 
correct, intended results, and to nothing else1. 
Notice that the result confusion matrix is gener-
ally very sparse. The maximum likelihood estima-
tor ???(?|?) will therefore often be inappropriate. 
To address this data sparsity issue, we linearly in-
terpolate the maximum likelihood estimator with 
an overall model ??(?|?): 
 
? ? ? =  ???? ? ? + (1? ?)?? ? ? . 
 
The overall model is defined in terms of two 
constants, ? and ?, as follows: 
 
?? ? ? =  
?, ?? ? = ?
?, ?? ? ? ?
  
 
where ? is the overall probability in the whole 
dataset of clicking on a given decoded result, and 
? is computed such that ?? ? ?  normalizes to 1. 
                                                          
1 While this assumption generally holds, we have also ob-
served cases where it is violated: sometimes users (perhaps 
accidentally) click on an incorrect result; other times the cor-
rect result is in the list but nothing is clicked (perhaps the user 
was simply testing out the recognition capabilities of the sys-
tem, without having an actual information need) 
24
Finally, the ? interpolation parameter is determined 
empirically on the development set.  
The second component in the confusion based 
rescoring model from equation [1] is ?(?|?). This 
is the recognition score for hypothesis ?. The n-
best rescoring model from [1] becomes: 
 
? ? ? =   ???? ? ?? + (1? ?)?? ? ??  ? ?(?? |?)
????
 
3.5 Language Model Based Rescoring 
A more traditional alternative for n-best rescoring 
is to adapt the bigram language model used by the 
system in light of the user click data, and re-rank 
the decoded results by: 
 
? ? ? ? ? ??  ? ? ? ? ?? ?(??) 
 
Here ? ? ??  is the acoustic score assigned by 
the recognizer to hypothesis ?? , and ?(??) is the 
adapted language model score for this hypothesis.  
A simple approach for adapting the system?s 
language model is to add the word sequences of 
the user-clicked results to the original training sen-
tences and to re-estimate the language model ?(?). 
We will refer to this method as maximum likelih-
ood (ML) estimation. A second approach, referred 
to as conditional maximum likelihood (CML) es-
timation, is to adapt the language model such as to 
directly maximize the conditional likelihood of the 
correct result given acoustics, i.e., 
 
? ? ? =
? ? ? ?(?)
 ? ? ?? ?(??)????
 
 
Note that this is the same objective function as 
the one used in Section 3.4, except that here the 
click data is used to estimate the language model 
instead of the error correction model. Again, in 
practice we assume that users click on correct re-
sults, i.e. ? = ?. 
4 Experiments  
We now discuss a number of experiments and the 
results obtained using the proposed n-best correc-
tion approach.  
4.1 Data 
For the purposes of the experiments described be-
low we extracted just over 800,000 queries from 
the server logs in which the recognizer had gener-
ated a simple n-best list2. For each recognition 
event, we collected from the system logs the n-best 
list, and the result clicked by the user (if the user 
clicked on any result).  
In addition, for testing purposes, we also make 
use of 11529 orthographically transcribed user re-
quests. The transcribed set was further divided into 
a development set containing 5680 utterances and 
a test set containing 5849 utterances.  
4.2 Initial N-Best Rescoring 
To tease apart the effects of expansion and rescor-
ing in the proposed n-best correction model, we 
began by using the rescoring techniques on the 
initial n-best lists, without first expanding them. 
Since the actual recognition confidence scores 
?(?? |?) were not available in the system logs, we 
replaced them with an exponential probability den-
sity function based on the rank of the hypothesis:  
 
? ??  ? = 2
??  
 
We then rescored the n-best lists from the test 
set according to the three rescoring models de-
scribed earlier: confusion matrix, maximum like-
lihood (ML), and conditional maximum likelihood 
(CML). We computed the sentence level accuracy 
for the rescored n-best list, at different cutoffs. The 
accuracy was measured by comparing the rescored 
hypotheses against the available transcripts. 
Note that the maximum depth of the n-best lists 
generated by the recognizer is 10; this is the max-
imum number of hypotheses that can be displayed 
on the mobile device. However, the system may 
generate fewer than 10 hypotheses. The observed 
average n-best list size in the test set was 4.2.  
The rescoring results are illustrated in Figure 3 
and reported in Table 1. The X axis in Figure 3 
shows the cutoff at which the n-best accuracy was 
computed. For instance in the baseline system, the 
correct hypothesis was contained in the top result 
in 46.2% of cases, in the top-2 results in 50.5% of 
the cases and in the top-3 results in 51.5% of the 
cases. The results indicate that all the rescoring 
models improve performance relative to the base-
                                                          
2 We did not consider cases where a false-recognition event 
was fired (e.g. if no speech was detected in the audio signal) ? 
in these cases no n-best list is generated. We also did not con-
sider cases where a compound n-best was generated (e.g. for 
compound requests like Shamiana in Kirkland, Washington) 
25
line. The improvement is smallest for the maxi-
mum likelihood (ML) language model rescoring 
approach, but is still statistically significant 
(? = 0.008 in a Wilcoxon sign-rank test.) The con-
fusion-matrix based rescoring and the CML rescor-
ing models perform similarly well, leading to a 1% 
absolute improvement in 1-best and 2-best sen-
tence-level accuracy from the baseline (? < 10?5). 
No statistically significant difference can be de-
tected between these two models. At the same 
time, they both outperform the maximum likelih-
ood rescoring model (? < 0.03). 
4.3 N-Best Correction 
Next, we evaluated the end-to-end n-best correc-
tion approach. The n-best lists were first expanded, 
as described in section 3.3, and the expanded lists 
were ranked using the confusion matrix based res-
coring model described in Section 3.4.  
The expansion process enlarges the original n-
best lists. Immediately after expansion, the average 
n-best size grows from 4.2 to 96.9. The oracle per-
formance for the expanded n-best lists increases to 
59.8% (versus 53.5% in the initial n-best lists.) 
After rescoring, we trimmed the expanded n-best 
lists to a maximum of 10 hypotheses: we still want 
to obey the mobile device display constraint. The 
resulting average n-best size was 7.09 (this is low-
er than 10 since there are cases when the system 
cannot generate enough expansion hypotheses.) 
The sentence-level accuracy of the corrected n-
best lists is displayed in line 4 from Table 1. A di-
rect comparison with the rescoring-only models or 
with the baseline is however unfair, due to the 
larger average size of the corrected n-best lists. To 
create a fair comparison and to better understand 
the performance of the n-best correction process, 
we pruned the corrected n-best lists by eliminating 
all hypotheses with a score below a certain thre-
shold. By varying this rejection threshold, we can 
therefore control the average depth of the resulting 
corrected n-best lists. At a rejection threshold of 
0.004, the average corrected n-best size is 4.15, 
comparable to the baseline of 4.2 .  
The performance for the corresponding cor-
rected (and pruned) n-best lists is shown in line 5 
from Table 1 and illustrated in Figure 4. In contrast 
to a rescoring-only approach, the expansion pro-
cess allows for improved performance at higher 
depths in the n-best list. The maximum n-best per-
formance (while keeping the average n-best size at 
4.15), is 56.5%, a 3% absolute improvement over 
the baseline (? < 10?5).  
Figure 5 provides more insight into the relation-
ship between the sentence-level accuracy of the 
corrected (and pruned) n-best lists and the average 
n-best size (the plot was generated by varying the 
rejection threshold.) The result we discussed above 
can also be observed here: at the same average n-
best size, the n-best correction model significantly 
outperforms the baseline. Furthermore, we can see 
that we can attain the same level of accuracy as the 
baseline system while cutting the average n-best 
size by more than 50%, from 4.22 to 2. In the op-
posite direction, if we are less sensitive to the 
number of items displayed in the n-best list (except 
for the 10-maximum constraint we already obey), 
we can further increase the overall performance by 
another 0.8% absolute to 57.3%; this overall accu-
racy is attained at an average n-best size of 7.09.  
Figure 3. Initial n-best rescoring (test-set) 
Table 1. Test-set sentence-level n-best accuracy; 
(0) baseline; (1)-(3) initial n-best rescoring;  
(4)-(5) expansion + rescoring 
 Model 1-
Best 
2-
Best 
3-
Best 
10-
Best 
0 Baseline 46.2 50.5 51.5 53.5 
1 ML Rescoring  46.8 50.9 52.1 53.5 
2 CML Rescoring 47.4 51.4 52.6 53.5 
3 Confusion Matrix Resc. 47.3 51.5 52.5 53.5 
4 Expansion + Rescoring 
(size=7.09) 
46.8 52.3 54.5 57.3 
5 Expansion + Rescoring 
(size=4.15) 
46.8 52.3 54.4 56.5 
 
26
Finally, we also investigated rescoring the ex-
panded n-best lists using the CML approach. To 
apply CML, an initial ranking of the expanded n-
best lists is however needed. If we use the ranking 
produced by the confusion-matrix based model 
discussed above, no further performance improve-
ments can be observed.  
5 Related work 
The n-best correction model we have described in 
this paper draws inspiration from earlier works on 
post-recognition error correction models, n-best 
rescoring and implicitly supervised learning. In 
this section we discuss some of the similarities and 
differences between the proposed approach and 
previous work. 
The idea of correcting speech recognition errors 
in a post-processing step has been proposed earlier 
by (Ringger and Allen, 1996; Ringger and Allen, 
1997). The authors showed that, in the presence of 
transcribed data, a translation-based post-processor 
can be trained to correct the results of a speech 
recognizer, leading to a 15% relative WER im-
provement in a corpus of TRAINS-95 dialogues.  
The n-best correction approach described here is 
different in two important aspects. First, instead of 
making use of transcripts, the proposed error-
correction model is trained using implicit user 
feedback obtained in a multi-modal interface (in 
this case user clicks in the n-best list.) This is a less 
costly endeavor, as the system automatically ob-
tains the supervision signal directly from the inte-
raction; no transcripts are necessary. Second, the 
approach operates on the entire n-best list, rather 
than only on the top hypothesis; as such, it has ad-
ditional information that can be helpful in making 
corrections. At Figure 2 illustrates, there is a poten-
tial for multiple incorrect hypotheses to point to-
wards and reinforce the same correction 
hypothesis, leading to improved performance (in 
this example, Burlington, Cooling, Sterling and 
Stirling were all highly confusable with Bowling, 
which was the correct hypothesis). 
The n-best correction model we have described 
includes a rescoring step. N-best rescoring ap-
proaches have been investigated extensively in the 
speech recognition community. In the dialog 
community, n-best rescoring techniques that use 
higher-level, dialog features have also been pro-
posed and evaluated (Chotimongkol and Rudnicky, 
2001). Apart from using the click feedback, the 
novelty in our approach lies in the added expansion 
step and in the use of an error-correction model for 
rescoring. We have seen that the confusability-
based n-best expansion process leads to signifi-
cantly improved performance, even if we force the 
model to keep the same average n-best size. 
Finally, the work discussed in this paper has 
commonalities with previous works on lightly su-
pervised learning in the speech community, e.g. 
(Lamel and Gauvain, 2002) and leveraging implicit 
feedback for learning from interaction, e.g. (Baner-
jee and Rudnicky, 2007; Bohus and Rudnicky, 
2007). In all these cases, the goal is to minimize 
the need for manually-labeled data, and learn di-
Figure 5. Overall n-best accuracy as a function of 
the average n-best size  
53.5% 
56.5% 
57.3% 
Figure 4. N-Best correction (test-set) 
27
rectly from the interaction. We believe that in the 
long term this family of learning techniques will 
play a key role towards building autonomous, self-
improving systems. 
6 Conclusion and future work 
We have proposed and evaluated a novel n-best 
correction model that leverages implicit user feed-
back in a multi-modal interface to create a positive 
feedback loop. While the experiments reported 
here were conducted in the context of a local 
search application, the approach is applicable in 
any multi-modal interface that elicits selection in 
an n-best list from the user.  
The proposed n-best correction model works in 
two stages. First, the n-best list generated by the 
speech recognizer is expanded with additional hy-
potheses based on confusability information cap-
tured from previous user clicks. This expanded list 
is then rescored and pruned to create a more accu-
rate and more compact n-best list. Our experiments 
show that the proposed n-best correction approach 
significantly outperforms both the baseline and 
other traditional n-best rescoring approaches, with-
out increasing the average length of the n-best lists.  
Several issues remain to be investigated. The 
models discussed in this paper focus on post-
recognition processing. Other ways of using the 
click data can also be envisioned. For instance, one 
approach would be to add all the clicked results to 
the existing language model training data and 
create an updated recognition language model. In 
the future, we plan to investigate the relationship 
between these two approaches, and to whether they 
can be used in conjunction. Earlier related work 
(Ringger and Allen, 1997) suggests that this should 
indeed be the case. 
Second, the click-based error-correction model 
we have described in section 3.4 operates at the 
result level. The proposed model is essentially a 
sentence level, memory-based translation model. 
In the future, we also plan to investigate word-
level error-correction models, using machine trans-
lation techniques like the ones discussed in (Ring-
ger and Allen, 1997; Li et al, 2008). 
Finally, we plan to investigate how this process 
of learning from implicit feedback in a multi-
modal interface can be streamlined, such that the 
system continuously learns online, with a minimal 
amount of human intervention.  
Acknowledgments 
This work would have not been possible without 
the help of a number of other people. We would 
like to especially thank Oliver Scholz, Julian 
Odell, Christopher Dac, Tim Paek, Y.C. Ju, Paul 
Bennett, Eric Horvitz and Alex Acero for their 
help and for useful conversations and feedback. 
References  
Acero, A., N. Bernstein, et al (2008). "Live Search for 
Mobile: Web Services by Voice on the Cellphone". 
ICASSP'08. Las Vegas, NV. 
Banerjee, S. and A. Rudnicky (2007). "Segmenting 
Meetings into Agenda Items by Extracting Implicit 
Supervision from Human Note-Taking". IUI'2007. 
Honolulu, Hawaii. 
Birkenes, O., T. Matsui, et al (2007). "N-Best Rescor-
ing for Speech Recognition using Penalized Logis-
tic Regression Machines with Garbage Class". 
ICASSP'2007, Honolulu, Hawaii. 
Bohus, D. and A. Rudnicky (2007). "Implicitly-
supervised learning in spoken language interfaces: 
an application to the confidence annotation prob-
lem". SIGdial 2007, Antwerp, Belgium. 
Chotimongkol, A. and A. Rudnicky (2001). "N-best 
Speech Hypotheses Reordering Using Linear Re-
gression". Eurospeech'2001, Aalborg, Denmark. 
Lamel, L. and J.-L. Gauvain (2002). "Lightly Super-
vised and Unsupervised Acoustic Model Training." 
Computer Speech and Language 16: 115-129. 
Li, X., Y.-C. Ju, et al (2008). "Language Modeling for 
Voice Search: a Machine Translation Approach". 
ICASSP'08, Las Vegas, NV. 
Live Search for Windows Mobile (2008): 
 http://mobile.search.live.com 
Nuance Mobile Search (2008): 
http://www.nuance.com/mobilesearch. 
Ringger, E. and J. Allen (1996). "Error Correction via 
Post-Processor for Continuous Speech Recogni-
tion". ICASSP'96, Atlanta, GA. 
Ringger, E. and J. Allen (1997). "Robust Error Correc-
tion of Continuous Speech Recognition". ESCA-
NATO Workshop on Robust Speech Recognition 
for Unknown Communication Channels, Pont-a-
Mousson, France. 
TellMeByMobile (2008): 
http://www.tellme.com/products/tellmebymobile. 
V-Lingo Mobile. (2008): 
http://www.vlingomobile.com/downloads.html. 
VoiceSignal Search. (2008): 
http://www.voicesignal.com/solutions/vsearch.php. 
 
28
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 104?111,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Optimal Dialog in Consumer-Rating Systems using a POMDP Framework
Zhifei Li
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
zhifei.work@gmail.com
Patrick Nguyen, Geoffrey Zweig
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052, USA
{panguyen,gzweig}@microsoft.com
Abstract
Voice-Rate is an experimental dialog system
through which a user can call to get prod-
uct information. In this paper, we describe
an optimal dialog management algorithm for
Voice-Rate. Our algorithm uses a POMDP
framework, which is probabilistic and cap-
tures uncertainty in speech recognition and
user knowledge. We propose a novel method
to learn a user knowledge model from a review
database. Simulation results show that the
POMDP system performs significantly better
than a deterministic baseline system in terms
of both dialog failure rate and dialog interac-
tion time. To the best of our knowledge, our
work is the first to show that a POMDP can
be successfully used for disambiguation in a
complex voice search domain like Voice-Rate.
1 Introduction
In recent years, web-based shopping and rating sys-
tems have provided a valuable service to consumers
by allowing them to shop products and share their
assessments of products online. The use of these
systems, however, requires access to a web interface,
typically through a laptop or desktop computer, and
this restricts their usefulness. While mobile phones
also provide some web access, their small screens
make them inconvenient to use. Therefore, there
arises great interests in having a spoken dialog in-
terface through which a user can call to get product
information (e.g., price, rating, review, etc.) on the
fly. Voice-Rate (Zweig et al, 2007) is such a sys-
tem. Here is a typical scenario under which shows
the usefulness of the Voice-Rate system. A user en-
ters a store and finds that a digital camera he has
not planned to buy is on sale. Before he decides
to buy the camera, he takes out his cell phone and
calls Voice-Rate to see whether the price is really
a bargain and what other people have said about
the camera. This helps him to make a wise deci-
sion. The Voice-Rate system (Zweig et al, 2007) in-
volves many techniques, e.g., information retrieval,
review summarization, speech recognition, speech
synthesis, dialog management, etc. In this paper, we
mainly focus on the dialog management component.
When a user calls Voice-Rate for the information
of a specific product, the system needs to identify,
from a database containing millions of products, the
exact product the user intends. To achieve this, the
system first solicits the user for the product name.
Using the product name as a query, the system then
retrieves from its database a list of products related
to the query. Ideally, the highest-ranked product
should be the one intended by the user. In reality,
this is often not the case due to various reasons. For
example, there might be a speech recognition error
or an information retrieval ranking error. Moreover,
the product name is usually very ambiguous in iden-
tifying an exact product. The product name that the
user says may not be exactly the same as the name
in the product database. For example, while the user
says ?Canon Powershot SD750?, the exact name
in the product database may be ?Canon Powershot
SD750 Digital Camera?. Even the user says the ex-
act name, it is possible that the same name may be
corresponding to different products in different cat-
egories, for instance books and movies.
Due to the above reasons, whenever the Voice-
Rate system finds multiple products matching the
user?s initial speech query, it initiates a dialog proce-
dure to identify the intended product by asking ques-
tions about the products. In the product database,
104
many attributes can be used to identify a product.
For example, a digital camera has the product name,
category, brand, resolution, zoom, etc. Given a list
of products, different attributes may have different
ability to distinguish the products. For example, if
the products belong to many categories, the category
attribute is very useful to distinguish the products. In
contrast, if all the products belong to a single cate-
gory, it makes no sense to ask a question on the cat-
egory. In addition to the variability in distinguishing
products, different attributes may require different
knowledge from the user in order for them to an-
swer questions about these attributes. For example,
while most users can easily answer a question on
category, they may not be able to answer a question
on the part number of a product, though the part
number is unique and perfect to distinguish prod-
ucts. Other variabilities are in the difficulty that the
attributes impose on speech recognition and speech
synthesis. Clearly, given a list of products and a set
of attributes, what questions and in what order to ask
is essential to make the dialog successful. Our goal
is to dynamically find such important attributes at
each stage/turn.
The baseline system (Zweig et al, 2007) asks
questions only on product name and category. The
order of questions is fixed: first ask questions on
product category, and then on name. Moreover, it
is deterministic and does not model uncertainly in
speech recognition and user knowledge. Partially
observable Markov decision process (POMDP) has
been shown to be a general framework to capture the
uncertainty in spoken dialog systems. In this paper,
we present a POMDP-based probabilistic system,
which utilizes rich product information and captures
uncertainty in speech recognition and user knowl-
edge. We propose a novel method to learn a user
knowledge model from a review database. Our sim-
ulation results show that the POMDP-based system
improves the baseline significantly.
To the best of our knowledge, our work is the first
to show that a POMDP can be successfully used for
disambiguation in a complex voice search domain
like Voice-Rate.
2 Voice-Rate Dialog System Overview
Figure 1 shows the main flow in the Voice-Rate sys-
tem with simplification. Specifically, when a user
calls Voice-Rate for the information of a specific
 
 
Yes 
Begin 
Information Retrieval 
Dialog Manager 
End 
Initial Speech Query 
List of Products 
Corrupted User Action 
Human 
Speech recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
? Intended product  
Figure 1: Flow Chart of Voice-Rate System
Step-1: remove products that do not match
the user action
Step-2: any category question to ask?
yes: ask the question and return
no: go to step-3
Step-3: ask a product name question
Table 1: Baseline Dialog Manager Algorithm
product, the system first solicits the user for the
product name. Treating the user input as a query
and the product names in the product database as
documents, the system retrieves a list of products
that match the user input based on TF-IDF mea-
sure. Then, the dialog manager dynamically gener-
ates questions to identify the specific intended prod-
uct. Once the product is found, the system plays
back its rating information. In this paper, we mainly
focus on the dialog manager component.
Baseline Dialog Manager: Table 1 shows the
baseline dialog manager. In Step-1, it removes all
the products that are not consistent with the user re-
sponse. For example, if the user answers ?camera?
when given a question on category, the system re-
moves all the products that do not belong to category
?camera?. In Step-2 and Step-3, the baseline system
asks questions about product name and product cat-
egory, and product category has a higher priority.
3 Overview of POMDP
3.1 Basic Definitions
A Partially Observable Markov Decision Process
(POMDP) is a general framework to handle uncer-
tainty in a spoken dialog system. Following nota-
105
tions in Williams and Young (2007), a POMDP is
defined as a tuple {S,A, T,R,O,Z, ?,~b0} where S
is a set of states s describing the environment; A is
a set of machine actions a operating on the environ-
ment; T defines a transition probability P (s? |s, a);
R defines a reward function r(s, a); O is a set of ob-
servations o, and an observation can be thought as
a corrupted version of a user action; Z defines an
observation probability P (o? |s? , a); ? is a geometric
discount factor; and~b0 is an initial belief vector.
The POMDP operates as follows. At each time-
step (a.k.a. stage), the environment is in some unob-
served state s. Since s is not known exactly, a distri-
bution (called a belief vector ~b) over possible states
is maintained where~b(s) indicates the probability of
being in a particular state s. Based on the current be-
lief vector ~b, an optimal action selection algorithm
selects a machine action a, receives a reward r, and
the environment transits to a new unobserved state
s? . The environment then generates an observation
o? (i.e., a user action), after which the system update
the belief vector ~b. We call the process of adjusting
the belief vector~b at each stage ?belief update?.
3.2 Applying POMDP in Practice
As mentioned in Williams and Young (2007), it is
not trivial to apply the POMDP framework to a
specific application. To achieve this, one normally
needs to design the following three components:
? State Diagram Modeling
? Belief Update
? Optimal Action Selection
The state diagram defines the topology of the
graph, which contains three kinds of elements: sys-
tem state, machine action, and user action. To drive
the transitions, one also needs to define a set of
models (e.g., user goal model, user action model,
etc.). The modeling assumptions are application-
dependent. The state diagram, together with the
models, determines the dynamics of the system.
In general, the belief update depends on the ob-
servation probability and the transition probability,
while the transition probability itself depends on the
modeling assumptions the system makes. Thus, the
exact belief update formula is application-specific.
Optimal action selection is essentially an opti-
mization algorithm, which can be defined as,
a? = argmax
a?A
G(P (a)), (1)
where A refers to a set of machine actions a.
Clearly, the optimal action selection requires three
sub-components: a goodness measure function G, a
prediction algorithm P , and a search algorithm (i.e.,
the argmax operator). The prediction algorithm is
used to predict the behavior of the system in the
future if a given machine action a was taken. The
search algorithm can use an exhaustive linear search
or an approximated greedy search depending on the
size of A (Murphy, 2000; Spaan and Vlassis, 2005).
4 POMDP Framework in Voice-Rate
In this section, we present our instantiation of
POMDP in the Voice-Rate system.
4.1 State Diagram Modeling
4.1.1 State Diagram Design
Table 2 summarizes the main design choices in
the state diagram for our application, i.e., identifying
the intended product from a large list of products.
As in Williams and Young (2007), we incorporate
both the user goal (i.e., the intended product) and
the user action in the system state. Moreover, to ef-
ficiently update belief vector and compute optimal
action, the state space is dynamically generated and
pruned. In particular, instead of listing all the possi-
ble combinations between the products and the user
actions, at each stage, we only generate states con-
taining the products and the user actions that are rel-
evant to the last machine action. Moreover, at each
stage, if the belief probability of a product is smaller
than a threshold, we prune out this product and all
its associated system states. Note that the intended
product may be pruned away due to an overly large
threshold. In the simulation, we will use a develop-
ment set to tune this threshold.
As shown in Table 2, five kinds of machine ac-
tions are defined. The questions on product names
are usually long, imposing difficulty in speech syn-
thesis/recgonition and user input. Thus, short ques-
tions (e.g., questions on category or simple at-
tributes) are preferable. This partly motivate us to
exploit rich product information to help the dialog.
Seven kinds of user actions are defined as shown
in Table 2. Among them, the user actions ?others?,
?not related?, and ?not known? are special. Specif-
ically, to limit the question length and to ensure the
106
Component Design Comments
System State (Product, User action) e.g., (HP Computer, Category: computer)
Machine Action Question on Category e.g., choose category: Electronics, Movie, Book
Question on Product name e.g., choose product name: Canon SD750 digital cam-
era, Canon Powershot A40 digital camera, Canon
SD950 digital camera, Others
Question on Attribute e.g., choose memory size: 64M, 128M, 256M
Confirmation question e.g., you want Canon SD750 camera, yes or no?
Play Rating e.g., I think you want Canon SD750 digital camera,
here is the rating!
User Action Category e.g., Movie
Product name e.g., Canon SD750 digital camera
Attribute value e.g., memory size: 64M
Others used when a question has too many possible options
Yes/No used for a confirmation question
Not related used if the intended product is unrelated to the question
Not known used if the user does not have required knowledge to
answer the question
Table 2: State Diagram Design in Voice-Rate
human is able to memorize all the options, we re-
strict the number of options in a single question to a
threshold N (e.g., 5). Clearly, given a list of prod-
ucts and a question, there might be more than N pos-
sible options. In such a case, we need to merge some
options into the ?others? class. The third example in
Table 2 shows an example with the ?others? option.
One may exploit a clustering algorithm (e.g., an it-
erative greedy search algorithm) to find an optimal
merge. In our system, we simply take the top-(N -1)
options (ranked by the belief probabilities) and treat
all the remaining options as ?others?.
The ?not related? option is required when some
candidate products are irrelevant to the question. For
example, when the system asks a question regarding
the attribute ?cpu speed? while the products contain
both books and computers, the ?not related? option
is required in case the intended product is a book.
Lastly, while some attributes are very useful to
distinguish the products, a user may not have enough
knowledge to answer a question on these attributes.
For example, while there is a unique part number for
each product, however, the user may not know the
exact part number for the intended product. Thus,
?not known? option is required whenever the system
expects the user is unable to answer the question.
4.1.2 Models
We assume that the user does not change his goal
(i.e., the intended product) along the dialog. We
also assume that the user rationally answers the
question to achieve his goal. Additionally, we as-
sume that the speech synthesis is good enough such
that the user always gets the right information that
the system intends to convey. The two main mod-
els that we consider include an observation model
that captures speech recognition uncertainty, and a
user knowledge model that captures the variability
of user knowledge required for answering questions
on different attributes.
Observation Model: Since the speech recogni-
tion engine we are using returns only a one-best and
its confidence value C ? [0, 1]. We define the obser-
vation function as follows,
P (a?u|au) =
{
C if a?u = au,
1?C
|Au|?1 otherwise.
(2)
where au is the true user action, a?u is the speech
recognition output (i.e., corrupted user action), and
Au is the set of user actions related to the last ma-
chine action.
User Knowledge Model: In most of the appli-
cations (Roy et al, 2000; Williams, 2007) where
107
the POMDP framework got applied, it is normally
assumed that the user needs only common sense to
answer the questions asked by the dialog system.
Our application is more complex as the product in-
formation is very rich. A user may have different
difficulty in answering different questions. For ex-
ample, while a user can easily answer a question on
category, he may not be able to answer a question
on the part number. Thus, we define a user knowl-
edge model to capture such uncertainty. Specifically,
given a question (say am) and an intended product
(say gu) in the user?s mind, we want to know how
likely the user has required knowledge to answer the
question. Formally, the user knowledge model is,
P (au|gu, am) =
?
??
??
P (unk|gu, am) if au=unk,
1? P (unk|gu, am) if au=truth,
0 otherwise.
(3)
where unk represents the user action ?not known?.
Clearly, given a specific product gu and a specific
question am, there is exactly one correct user ac-
tion (represented by truth in Equation 3), and its
probability is 1 ? P (unk|gu, am). Now, to obtain
a user knowledge model, we only need to obtain
P (unk|gu, am). As shown in Table 2, there are four
kinds of question-type machine actions am. We as-
sume that the user always has knowledge to answer
a question regarding the category and product name,
and thus P (unk|gu, am) for these types of machine
actions are zero regardless of what the specific prod-
uct gu is. Therefore, we only need to consider
P (unk|gu, am) when am is a question about an at-
tribute (say attr). Moreover, since there are millions
of products, to deal with the data sparsity issue, we
assume P (unk|gu, am) does not depends on a spe-
cific product gu, instead it depends on only the cate-
gory (say cat) of the product gu. Therefore,
P (unk|gu, am) ? P (unk|cat,attr). (4)
Now, we only need to get the probability
P (unk|cat,attr) for each attribute attr in each cate-
gory cat. To learn P (unk|cat,attr), one may collect
data from human, which is very expensive. Instead,
we learn this model from a database of online re-
views for the products. Our method is based on the
following intuition: if a user cares/knows about an
attribute of a product, he will mention either the at-
tribute name, or the attribute value, or both in his
review of this product. With this intuition, the occur-
rence frequency of a given attr in a given category
cat is collected from the review database, followed
by proper weighting, scaling and normalization, and
thus P (unk|cat,attr) is obtained.
4.2 Belief Update
Based on the model assumptions in Section 4.1.2,
the belief update formula for the state (gu, a?u) is,
~b(gu, a?u) = (5)
k ? P (a??u|a
?
u)P (a
?
u|gu, am)
?
au?A(gu)
~b(gu, au)
where k is a normalization constant. The P (a??u|a
?
u)
is the observation function as defined in Equation 2,
while P (a?u|gu, am) is the user knowledge model as
defined in Equation 3. The A(gu) represents the set
of user actions au related to the system states for
which the intended product is gu.
In our state representation, a single product gu
is associated with several states which differ in the
user action au, and the belief probability of gu is the
sum of the probabilities of these states. Therefore,
even there is a speech recognition error or an un-
intentional user mistake, the true product still gets
a non-zero belief probability (though the true/ideal
user action au gets a zero probability). Moreover,
the probability of the true product will get promoted
through later iterations. Therefore, our system has
error-handling capability, which is one of the major
advantages over the deterministic baseline system.
4.3 Optimal Action Selection
As mentioned in Section 3.2, the optimal action se-
lection involves three sub-components: a prediction
algorithm, a goodness measure, and a search algo-
rithm. Ideally, in our application, we should mini-
mize the time required to successfully identify the
intended product. Clearly, this is too difficult as
it needs to predict the infinite future and needs to
encode the time into a reward function. Therefore,
for simplicity, we predict only one-step forward, and
use the entropy as a goodness measure1. Formally,
1Due to this approximation, one may argue that our model
is more like the greedy information theoretic model in Paek and
Chickering (2005), instead of a POMDP model. However, we
believe that our model follows the POMDP modeling frame-
work in general, though it does not involve reinforcement learn-
ing currently.
108
the optimization function is as follows:
a? = argmin
a?A
H(Products | a), (6)
where H(Products | a) is the entropy over the belief
probabilities of the products if the machine action
a was taken. When predicting the belief vector us-
ing Equation 5, we consider only the user knowledge
model and ignore the observation function2.
In the above, we consider only the question-type
machine actions. We also need to decide when
to take the play rating action such that the dialog
will terminate. Specifically, we take the play rating
action whenever the belief probability of the most
probable product is greater than a threshold. More-
over, the threshold should depend on the number of
surviving products. For example, if there are fifty
surviving products and the most probable product
has a belief probability greater than 0.3, it is reason-
able to take the play rating action. This is not true
if there are only four surviving products. Also note
that if we set the thresholds to too small values, the
system may play the rating for a wrong product. We
will use a development set to tune these thresholds.
4.3.1 Machine Action Filtering during Search
We use an exhaustive linear search for the opera-
tor argmin in Equation 6. However, additional filter-
ing during the search is required.
Repeated Question: Since the speech response
from the user to a question is probabilistic, it is quite
possible that the system will choose the same ques-
tion that has been asked in previous stages3. Since
our product information is very rich, many differ-
ent questions have the similar capability to reduce
entropy. Therefore, during the search, we simply ig-
nore all the questions asked in previous stages.
?Not Related? Option: While reducing entropy
helps to reduce the confusion at the machine side, it
does not measure the ?weirdness? of a question to
the human. For example, when the intended product
is a book and the candidate products contain both
books and computers, it is quite possible that the
optimal action, based solely on entropy reduction,
2Note that we ignore the observation function only in the
prediction, not in real belief update.
3In a regular decision tree, the answer to a question is deter-
ministic. It never asks the same question as that does not lead to
any additional reduction of entropy. This problem is also due to
the fact we do not have an explicit reward function.
is a question on the attribute ?cpu speed?. Clearly,
such a question is very weird to the human as he is
looking for a book that has nothing related to ?cpu
speed?. Though the user may be able to choose the
?not related? option correctly after thinking for a
while, it degrades the dialog quality. Therefore, for
a given question, whenever the system predicts that
the user will have to choose the ?not related? option
with a probability greater than a threshold, we sim-
ply ignore such questions in the search. Clearly, if
we set the threshold as zero, we essentially elimi-
nates the ?not related? option. That is, at each stage,
we generate questions only on attributes that apply
to all the candidate products. Since we dynamically
remove products whose probability is smaller than
a threshold at each stage, the valid question set dy-
namically expands. Specifically, at the beginning,
only very general questions (e.g., questions on cate-
gory) are valid, then more refined questions become
valid (e.g., questions on product brand), and finally
very specific questions are valid (e.g, questions on
product model). This leads to very natural behav-
ior in identifying a product, i.e., coarse to fine4. It
also makes the system adapt to the user knowledge.
Specifically, as the user demonstrates deeper knowl-
edge of the products by answering the questions cor-
rectly, it makes sense to ask more refined questions
about the products.
5 Simulation Results
To evaluate system performance, ideally one should
ask people to call the system, and manually collect
the performance data. This is very expensive. Al-
ternatively, we develop a simulation method, which
is automatic and thus allow fast evaluation of the
system during development5. In fact, many design
choices in Section 4 are inspired by the simulation.
5.1 Simulation Model
Figure 2 illustrates the general framework for the
simulation. The process is very similar to that in
Figure 1 except that the human user and the speech
4While the baseline dialog manager achieves the similar be-
havior by manually enforcing the order of questions, the sys-
tem here automatically discovers the order of questions and the
question set is much more richer than that in the baseline.
5However, we agree that simulation is not without its limi-
tations and the results may not precisely reflect real scenarios.
109
  
Yes 
Begin 
Information Retrieval 
Dialog Manager 
? Baseline 
? POMDP 
End 
Initial Query 
List of Products 
Corrupted User Action 
Simulated User 
? Intended product  
? User knowledge model 
Simulated  
Speech Recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
Figure 2: Flow Chart in Simulation
recognizer are replaced with a simulated compo-
nent, and that the simulated user has access to a user
knowledge model. In particular, we generate the
user action and its corrupted version using random
number generators by following the models defined
in Equations 3 and 2, respectively. We use a fixed
value (e.g., 0.9) for C in Equation 2.
Clearly, our goal here is not to evaluate the good-
ness of the user knowledge model or the speech rec-
ognizer. Instead, we want to see how the probabilis-
tic dialog manger (i.e., POMDP) performs compared
with the deterministic baseline dialog manager, and
to see whether the richer attribute information helps
to reduce the dialog interaction time.
5.2 Data Resources
In the system, we use three data resources: a prod-
uct database, a review database, and a query-click
database. The product database contains detailed in-
formation for 0.2 million electronics and computer
related products. The review database is used for
learning the user knowledge model. The query-
click database contains 2289 pairs in the format (text
query, product clicked). One example pair is (Canon
Powershot A700, Canon Powershot A700 6.2MP
digital camera). We divide it into a development set
(1308 pairs) and a test set (981 pairs).
5.3 Results on Information Retrieval
For each initial query, the information retrieval
(IR) engine returns a list of top-ranked products.
Whether the intended product is in the returned list
depends on the size of the list. If the intended prod-
uct is in the list, the IR successfully recalled the
product. Table 3 shows the correlation between the
recall rate and the size of the returned list. Clearly,
the larger the list size is, the larger the recall rate is.
One may notice that the IR recall rate is low. This
is because the query-click data set is very noisy, that
is, the clicked product may be nothing to do with
the query. For example, (msn shopping, Handspring
Treo 270) is one of the pairs in our data set.
List Size Recall Rate (%)
50 38.36
100 41.46
150 43.5
Table 3: Information Retrieval Recall Rates on Test set
5.4 Dialog System Configuration and Tuning
As mentioned in Section 4, several parameters in the
system are configurable and tunable. Specifically,
we set the max number of options in a question as
5, and the threshold for ?not related? option as zero.
We use the development set to tune the following pa-
rameters: the threshold of the belief probability be-
low which the product is pruned, and the thresholds
above which the most probable product is played.
The parameters are tuned in a way such that no dia-
log error is made on the development set.
5.5 Results on Error Handling
Even the IR succeeds, the dialog system may not
find the intended product successfully. In particu-
lar, the baseline system does not have error handling
capability. Whenever the system makes a speech
recognition error or the user mistakenly answers a
question, the dialog system fails (either plays the rat-
ing for a wrong product or fails to find any product).
On the contrary, our POMDP framework has error
handling functionality due to its probabilistic na-
ture. Table 5 compares the dialog error rate between
the baseline and the POMDP systems. Clearly,
the POMDP system performs much better to han-
dle errors. Note that the POMDP system does not
eliminate dialog failures on the test set because the
thresholds are not perfect for the test set6. This is
due to two reasons: the system may prune the in-
tended product (reason-1), and the system may play
the rating for a wrong product (reason-2).
6Note that the POMDP system does not have dialog failures
on the development set as we tune the system in this way.
110
System Size Average MaxStages Characters Words Stages Characters Words
Baseline
50 2.44 524.0 82.3 11 2927 546
100 3.37 765.4 120.4 25 7762 1369
150 3.90 906.4 143.0 30 9345 1668
POMDP
50 1.57 342.8 54.3 4 2659 466
100 2.36 487.9 76.6 18 3575 597
150 2.59 541.3 85.0 19 4898 767
Table 4: Interaction Time Results on Test Set
Size Baseline POMDP (%)(%) Total Reason-1 Reason-2
50 13.8 8.2 4.2 4.0
100 17.7 2.7 1.2 1.5
150 19.3 4.7 0.7 4.0
Table 5: Dialog Failure Rate on Test Set
5.6 Results on Interaction Time
It is quite difficult to measure the exact interaction
time, so instead we measure it through the number of
stages/characters/words required during the dialog
process. Clearly, the number of characters is the one
that matches most closely to the true time. Table 4
reports the average and maximum numbers. In gen-
eral, the POMDP system performs much better than
the baseline system. One may notice the difference
in the number of stages between the baseline and
the POMDP systems is not as significant as in the
number of characters. This is because the POMDP
system is able to exploit very short questions while
the baseline system mainly uses the product name
question, which is normally very long. The long
question on product name also imposes difficulty in
speech synthesis, user input, and speech recognition,
though this is not reflected in the simulation.
6 Conclusions
In this paper, we have applied the POMDP frame-
work into Voice-Rate, a system through which a
user can call to get product information (e.g., price,
rating, review, etc.). We have proposed a novel
method to learn a user knowledge model from a re-
view database. Compared with a deterministic base-
line system (Zweig et al, 2007), the POMDP system
is probabilistic and is able to handle speech recogni-
tion errors and user mistakes, in which case the de-
terministic baseline system is doomed to fail. More-
over, the POMDP system exploits richer product in-
formation to reduce the interaction time required to
complete a dialog. We have developed a simulation
model, and shown that the POMDP system improves
the baseline system significantly in terms of both di-
alog failure rate and dialog interaction time. We also
implement our POMDP system into a speech demo
and plan to carry out tests through humans.
Acknowledgement
This work was conducted during the first author?s
internship at Microsoft Research; thanks to Dan Bo-
hus, Ghinwa Choueiter, Yun-Cheng Ju, Xiao Li,
Milind Mahajan, Tim Paek, Yeyi Wang, and Dong
Yu for helpful discussions.
References
K. Murphy. 2000. A survey of POMDP solution tech-
niques. Technical Report, U. C. Berkeley.
T. Paek and D. Chickering. 2005. The Markov assump-
tion in spoken dialogue management. In Proc of SIG-
dial 2005.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc of ACL 2000.
M. Spaan and N. Vlassis. 2005. Perseus: randomized
point-based value iteration for POMDPs. Journal of
Artificial Intelligence Research, 24:195-220.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technology.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
G. Zweig, P. Nguyen, Y.C. Ju, Y.Y. Wang, D. Yu, and
A. Acero. 2007. The Voice-Rate Dialog System for
Consumer Ratings. In Proc of Interspeech 2007.
111
