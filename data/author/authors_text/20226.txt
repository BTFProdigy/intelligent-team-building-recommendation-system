Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 294?299,
Dublin, Ireland, August 23-24, 2014.
GPLSI: Supervised Sentiment Analysis in Twitter using Skipgrams
Javi Fern
?
andez, Yoan Guti
?
errez, Jos
?
e M. G
?
omez, Patricio Mart??nez-Barco
Department of Software and Computing Systems
University of Alicante
{javifm,ygutierrez,jmgomez,patricio}@dlsi.ua.es
Abstract
In this paper we describe the system sub-
mitted for the SemEval 2014 Task 9 (Sen-
timent Analysis in Twitter) Subtask B. Our
contribution consists of a supervised ap-
proach using machine learning techniques,
which uses the terms in the dataset as fea-
tures. In this work we do not employ any
external knowledge and resources. The
novelty of our approach lies in the use
of words, ngrams and skipgrams (not-
adjacent ngrams) as features, and how they
are weighted.
1 Introduction
The Web 2.0 has become one of the most im-
portant sources of data to extract useful and het-
erogeneous knowledge from. Texts can provide
factual information, such as descriptions and lists
of features, and opinion-based information, which
would include reviews, emotions, or feelings. This
subjective information can be expressed through
different textual genres, such as blogs, forums, so-
cial networks and microblogs.
An example of microblogging social network is
Twitter
1
, which has gained much popularity in the
last years. This website enables its users to send
and read text-based messages of up to 140 char-
acters, known as tweets. This site can be a vast
source of subjective information in real time; mil-
lions of users share opinions on different aspects
of their everyday life. Extracting this subjective
information has a great value for both general and
expert users. However, it is difficult to exploit it
accordingly, mainly because of the short length of
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
creativecommons.org/licenses/by/4.0/
1
http://twitter.com
the tweets, the informality, and the lack of context.
Sentiment Analysis (SA) systems must be adapted
to face the challenges of this new textual genre.
International competitions related to the assess-
ment of SA systems in Twitter have taken place.
Some of them include the TASS workshop in the
SEPLN conference (Villena-Rom?an et al., 2013),
the RepLab workshop in the CLEF conference
(Amig?o et al., 2012), and the Sentiment Analysis
in Twitter task (Task 2) in the last SemEval work-
shop (Nakov et al., 2013).
In this paper we describe the system submit-
ted for the SemEval 2014 Sentiment Analysis in
Twitter task (Task 9 Subtask B)
2
(Rosenthal et al.,
2014). This task consists of performing an au-
tomatic sentiment analysis to determine whether
a message expresses a positive, negative, or neu-
tral sentiment. The organisers of this task provide
three datasets: training, development training, and
development test. The participants can use the
training and development training datasets to train
and validate their models, but the development test
dataset can only be used for validation. The size
and distribution of polarities of these datasets is
shown in Table 1. Once their systems are ready,
the participants must classify each text in the offi-
cial test corpus and send the results to the organis-
ers, who will perform the official evaluation.
Polarity Train Dev Train Dev Test
Positive 2,148 362 1,572
Neutral 2,915 448 1,640
Negative 836 187 601
Total 5,899 997 3,813
Table 1: Dataset distribution in number of tweets.
The goal of the present work is to create a re-
liable polarity classifier, built only from a training
set without any external knowledge and resources.
2
http://alt.qcri.org/semeval2014/
294
Our contribution consists of a supervised approach
using machine learning techniques, which uses the
terms in the dataset as features. The novelty of our
approach lies in the feature generation and weight-
ing, using not only single words and ngrams as
features but also skipgrams. This approach is de-
scribed in detail in Section 3. Subsequently, in
Section 4 we show the assessment of our model
in the competition. Finally, the conclusions and
future work are presented in Section 5. The fol-
lowing Section 2 shows some relevant background
related to this work.
2 Related Work
The goal of Sentiment analysis (SA) is to identify
the opinions expressed in text and classify texts
accordingly (Dadvar et al., 2011). Two main ap-
proaches can be followed (Annett and Kondrak,
2008; Liu, 2010; Taboada et al., 2011): lexical ap-
proaches (unsupervised SA) and machine learning
approaches (supervised SA). Lexical approaches
focus on building dictionaries and lexicons of la-
belled words. This labeling gives a score for each
word, that indicates how strong is the relation be-
tween that word and each polarity. The most com-
mon way to classify a text using these scores is
by adding the positive values and subtracting the
negative values of the terms in that text. If the to-
tal score is positive, that text is classified as pos-
itive, otherwise it is classified as negative. These
dictionaries can be created manually (Stone et al.,
1966) or automatically (Turney, 2002). Examples
of lexicons are WordNet Affect (Strapparava and
Valitutti, 2004), SentiWordNet (Esuli and Sebas-
tiani, 2006), MicroWNOP (Cerini et al., 2007) or
JRC Tonality (Balahur et al., 2009). However, it
is very difficult to collect and maintain a univer-
sal sentiment lexicon because different words may
be used in different domains (Qiu et al., 2009) and
some words are domain dependent (Turney, 2002).
The second approach uses machine learning
techniques. These techniques require the previous
creation of a corpus containing a set of classified
texts to train a classifier, which can then be applied
to classify a set of unclassified texts. The majority
of the researches employ Support Vector Machines
(Mullen and Collier, 2004; Prabowo et al., 2009;
Wilson et al., 2005) or Na??ve Bayes (Pang and Lee,
2004; Wiebe et al., 2005; Tan et al., 2009) classi-
fiers because they usually obtain the best results.
In this approach, texts are represented as vectors
of features, and depending on the features used
the system can reach better results (bag-of-words
and lexeme-based features are the more commonly
used (Pang and Lee, 2008)). These classifiers per-
form very well in the domain that they are trained
on, but their performance drops when the same
classifier is used in a different domain (Pang and
Lee, 2008; Tan et al., 2009).
The problem of the domain dependence is com-
mon to both approaches. When the lexicons and
classifiers generated are used in a domain different
from the one they were built for, they use to per-
form worse (Turney, 2002; Pang and Lee, 2008;
Qiu et al., 2009; Tan et al., 2009). Creating a
domain-specific lexicon or classifier means mak-
ing a manual effort. Although some studies try
to overcome this problem by generating the lexi-
cons automatically (Turney, 2002), learning from
unannotated texts (Wiebe et al., 2005) or using hy-
brid approaches (Andreevskaia and Bergler, 2008;
Bollen et al., 2011; Zhang and Ye, 2008), a min-
imal intervention from experts in the domain is
needed. In this study we use the machine learning
approach due to the promising results obtained in
previous works (Boldrini et al., 2009; Fern?andez
et al., 2011; Fern?andez et al., 2013).
3 Methodology
Our contribution consists of a supervised approach
using machine learning techniques, which uses the
terms in the dataset as features. In summary, our
approach starts making a basic normalisation of
each tweet in the dataset (see Section 3.1). Next,
these texts are tokenised to extract their terms, and
these terms are combined to create skipgrams (see
Section 3.2). Finally, these skipgrams are em-
ployed as features for a supervised machine learn-
ing algorithm (see Section 3.3).
3.1 Basic normalisation
We perform a very basic normalisation, as we
do not want to lose the potential subjective infor-
mation given by the not normalised original text.
Each tweet in the dataset is normalised following
these steps:
1. Lower case conversion. All the characters in
the tweet text are converted to lower case.
2. Character repetition removal. If the same
character is repeated more than 3 times, the
rest of repetitions are removed, so we can
295
still recognize if a word had repeated char-
acters. For example, the words gooood and
gooooood would be normalised to goood, but
the word good would remain the same. We
assume the ambiguity of some words like the
one in the example, which can refer to the
words good and god.
3. Usernames and hashtags substitution. We
do not consider usernames and hashtags as
they are not usually the words that represent
a subjective sentence, they use to be the topic
of the tweet. They are not removed com-
pletely but they are replaced by the strings
USERNAME and HASHTAG.
So excited to go to #Alicante tomorrow
with the best friend everrrrr @John!!!!
?
so excited to go to #alicante tomorrow
with the best friend everrrrr @john!!!!
?
so excited to go to #alicante tomorrow
with the best friend everrr @john!!!
?
so excited to go to HASHTAG tomorrow
with the best friend everrr USERNAME!!!
Figure 1: Example of normalisation process.
3.2 Tokenisation
Once we have normalised the texts, we extract all
their terms. In this work, we consider a term as a
group of adjacent characters of the same type (let-
ters, numbers or punctuation symbols). For exam-
ple, the text want2go!! would be tokenised to the
terms want, 2, go, and !!. Note that we employ all
the terms extracted, not filtering any of them.
Finally, we obtain the skipgrams of the terms in
the text. Skipgrams are a technique largely used in
the field of speech processing, whereby n-grams
are formed (bigrams, trigrams, etc.) but in addi-
tion to allowing adjacent sequences of words, it
also allows tokens to be skipped (Guthrie et al.,
2006). More specifically, in a k-skip-n-gram, n de-
termines the maximum number of terms, and k the
maximum number of skips allowed. In this way
skipgrams are new terms that retain part of the se-
quentiality of the terms, but in a more flexible way
than ngrams. Note that a ngram can be described
as a skipgram where k = 0. An example is shown
in Figure 2.
Normalised tweet
so excited to go to HASHTAG tomorrow
with the best friend everrr USERNAME!!!
?
Single terms
(so) (excited) (to) (go) (to) (HASHTAG) (with)
(the) (best) (friend) (everrr) (USERNAME) (!!!)
?
Skipgrams (n = 2, k = 1)
(so) (so excited) (so to) (excited) (excited to)
(excited go)
(to) (to go) (to to) (go) (go to) (go HASHTAG) (to)
(to HASHTAG) (to with) (HASHTAG) (HASHTAG
with) (HASHTAG the) (with) (with the) (with best)
(the) (the best) (the friend) (best) (best friend)
(best everrr) (friend) (friend everrr) (friend
USERNAME) (everrr) (everrr USERNAME)
(everrr !!!) (USERNAME) (USERNAME !!!)
Figure 2: Example of tokenisation process.
3.3 Supervised Learning
To build our model we employed Support Vector
Machines (SVM) as the supervised machine learn-
ing algorithm, as it has been proved to be effective
on text categorisation tasks and robust on large
feature spaces (Sebastiani, 2002; Mohammad et
al., 2013). More specifically, we used the Weka
3
(Hall et al., 2009) LibSVM (Chang and Lin, 2011)
implementation with the default parameters (lin-
ear kernel, C = 1,  = 0.1).
The skipgrams extracted in the previous step are
employed as features for the SVM. The weight of
each feature in each text will be calculated depend-
ing on the skipgram it represents, using the for-
mula in Equation 1.
w(s, t) =
terms(s)
terms(s) + skips(s, t)
(1)
Wherew(s, t) represents the weight of the skip-
gram s in the text t, terms is a function that
returns the number of terms in skipgram s, and
skips is a function that returns the number of skips
of the skipgram s in the text t. This formula gives
more importance to the skipgrams with a lower
number of skips. In the example of the Figure 2,
the skipgram best friend would have a weight of
2/(2 + 0) = 1, while skipgram best everrr would
have a weight of 2/(2 + 1) = 0.66.
3
http://www.cs.waikato.ac.nz/ml/weka/
296
Parameters P R F1 Score
Baseline 0.630 0.604 0.580 0.447
Words n = 1 0.611 0.612 0.604 0.530
Ngrams n = 2 0.617 0.620 0.618 0.557
n = 3 0.620 0.621 0.621 0.564
n = 4 0.620 0.621 0.620 0.565
n = max 0.621 0.622 0.621 0.566
Skipgrams n = 2, k = 1 0.623 0.625 0.624 0.571
n = 2, k = 2 0.626 0.624 0.626 0.572
n = 2, k = max 0.627 0.624 0.625 0.575
n = 3, k = 1 0.620 0.616 0.617 0.566
n = 3, k = 2 0.625 0.614 0.618 0.564
n = 3, k = max 0.636 0.588 0.599 0.544
Table 2: Experiments performed and scores obtained.
4 Evaluation
We performed a series of experiments employ-
ing both the training corpus and the development
training corpus to create our model, and the devel-
opment test corpus to validate it. We used as base-
line the system presented to the workshop TASS
2012 (Fern?andez et al., 2013), which also uses
skipgrams and scores them depending on their
density but, instead of using the skipgrams as fea-
tures of a machine learning model, the polarity of
each text is determined by a combination of the
scores of its skipgrams.
The results of our experiments are shown in Ta-
ble 2. In this table we show the weighted precision
(P), the weighted recall (R), the weighted F-score
(F1) and the score obtained using the scorer tool
provided by the workshop organisers (Score). The
notation n = max indicates there was no limit
with the number of terms, and k = max indi-
cates there was no restriction with the number of
skips. As we can see, the presented approach out-
performs the baseline proposed and the best results
are obtained using skipgrams, specifically when
n = 2 and k = max. These are the parameters
of the system submitted to the competition.
Our main observation is that incrementing the
number of terms increases the precision of the sys-
tem. A possible explanation for this might be that
ngrams/skipgrams with a greater number of words
are more specific and representative of a given
polarity. In addition, using skipgrams insted of
ngrams also improves the precision. However, no
significant differences were found between exper-
iments with a different number of skips.
In Table 3 we can see the official results ob-
tained in the SemEval 2014 competition. The
best rank was obtained in the experiments with the
Twitter 2014 Sarcasm dataset.
Dataset Rank Score
Live Journal 34 0.573
SMS 2013 35 0.466
Twitter 2013 28 0.575
Twitter 2014 30 0.561
Twitter 2014 Sarcasm 8 0.539
Table 3: Official SemEval 2014 Subtask B results.
5 Conclusions
In this paper we described the system submitted
for the SemEval 2014 Task 9 (Sentiment Analysis
in Twitter). It consists of a supervised approach
using machine learning techniques, without em-
ploying any external knowledge and resources.
The novelty of our approach lies in the feature gen-
eration and weighting, using not only single words
and ngrams as features but also skipgrams. In the
experiments performed we showed that employ-
ing skipgrams instead of single words or ngrams
improves the results for these datasets. This fact
suggests that our approach is promising and en-
courages us to continue with our research.
As future work, we plan to find new methods
to combine the weights of the skipgrams, evaluate
our approaches on different corpora and different
domains (in order to check their robustness), and
start adding external knowledge and resources.
297
Acknowledgements
This research work has been partially funded by
the University of Alicante, Generalitat Valenciana,
Spanish Government and the European Com-
mission through the projects, ?Tratamiento in-
teligente de la informaci?on para la ayuda a la toma
de decisiones? (GRE12-44), ATTOS (TIN2012-
38536-C03-03), LEGOLANG (TIN2012-31224),
SAM (FP7-611312), FIRST (FP7-287607) and
ACOMP/2013/067.
References
Enrique Amig?o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview of
RepLab 2012: Evaluating Online Reputation Man-
agement Systems. In Conference and Labs of the
Evaluation Forum (CLEF 2012).
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies (ACL HLT 2008), pages
290?298.
Michelle Annett and Grzegorz Kondrak. 2008. A
Comparison of Sentiment Analysis Techniques: Po-
larizing Movie Blogs. In Proceedings of the
21st Canadian Conference on Artificial Intelligence
(CCAI 2008), pages 25?35.
Alexandra Balahur, Ralf Steinberger, Erik Van Der
Goot, Bruno Pouliquen, and Mijail Kabadjov. 2009.
Opinion Mining on Newspaper Quotations. In 2009
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 523?526.
Ester Boldrini, Javier Fern?andez Mart??nez,
Jos?e Manuel G?omez Soriano, Patricio
Mart??nez Barco, et al. 2009. Machine learn-
ing techniques for automatic opinion detection in
non-traditional textual genres.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011.
Modeling Public Mood and Emotion: Twitter Senti-
ment and Socio-Economic Phenomena. In Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM 2011).
Sabrina Cerini, Valentina Compagnoni, Alice Demon-
tis, Maicol Formentelli, and G Gandini. 2007.
Micro-WNOp: A Gold Standard for the Evaluation
of Automatically Compiled Lexical Resources for
Opinion Mining. Language resources and linguis-
tic theory: Typology, second language acquisition,
English linguistics, pages 200?210.
Chih-chung Chang and Chih-jen Lin. 2011. LIBSVM
: A Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2:1?39.
Maral Dadvar, Claudia Hauff, and FMG De Jong.
2011. Scope of Negation Detection in Sentiment
Analysis. In Proceedings of the Dutch-Belgian In-
formation Retrieval Workshop (DIR 2011), pages
16?20.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of LREC, vol-
ume 6, pages 417?422.
Javi Fern?andez, Ester Boldrini, Jos?e M. G?omez,
and Patricio Mart??nez-Barco. 2011. Evaluat-
ing EmotiBlog Robustness for Sentiment Analysis
Tasks. In Natural Language Processing and Infor-
mation Systems, pages 290?294.
Javi Fern?andez, Yoan Guti?errez, Jos?e M. G?omez, Patri-
cio Mart??nez-Barco, Andr?es Montoyo, and Rafael
Mu?noz. 2013. Sentiment Analysis of Spanish
Tweets Using a Ranking Algorithm and Skipgrams.
In XXIX Congreso de la Sociedad Espa?nola de
Procesamiento de Lenguaje Natural (SEPLN 2013),
pages 133?142.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and Yorick Wilks. 2006. A Closer Look at Skip-
gram Modelling. In 5th international Conference on
Language Resources and Evaluation (LREC 2006),
pages 1?4.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Bing Liu. 2010. Sentiment Analysis and Subjectiv-
ity. In Handbook of Natural Language Processing,
pages 1?38.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation (SemEval-2013).
Tony Mullen and Nigel Collier. 2004. Sentiment Anal-
ysis using Support Vector Machines with Diverse In-
formation Sources. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2004), pages 412?418.
Preslav Nakov, Sara Rosenthal, Alan Ritter, and
Theresa Wilson. 2013. SemEval-2013 Task 2:
Sentiment Analysis in Twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2013), volume 2, pages 312?320.
298
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts. In Proceed-
ings of the 42nd annual meeting on Association for
Computational Linguistics (ACL 2004), page 271.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Rudy Prabowo, Mike Thelwall, and Wulfruna Street.
2009. Sentiment Analysis: A Combined Approach.
Journal of Informetrics, 3:143?157.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding Domain Sentiment Lexicon
through Double Propagation. In Proceedings of the
21st international Joint Conference on Artificial In-
telligence (IJCAI 2009), pages 1199?1204.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014).
Fabrizio Sebastiani. 2002. Machine Learning in Au-
tomated Text Categorization. ACM Computing Sur-
veys (CSUR), 34(1):1?47, March.
Philip J. Stone, Dexter C. Dunphy, and Marshall S.
Smith. 1966. The General Inquirer: A Computer
Approach to Content Analysis.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet Affect: an Affective Extension of Word-
Net. In LREC, volume 4, pages 1083?1086.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Songbo Tan, Xueqi Cheng, Yuefen Wang, and Hongbo
Xu. 2009. Adapting Naive Bayes to Domain Adap-
tation for Sentiment Analysis. Advances in Informa-
tion Retrieval, pages 337?349.
Peter D. Turney. 2002. Thumbs Up or Thumbs
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 417?424.
Julio Villena-Rom?an, Eugenio Mart??nez-C?amara, Sara
Lana-Serrano, and Jos?e Carlos Gonz?alez-Crist?obal.
2013. TASS - Workshop on Sentiment Analysis
at SEPLN. Procesamiento del Lenguaje Natural,
50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language resources and
evaluation, 39(2-3):165?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35.
Min Zhang and Xingyao Ye. 2008. A Generation
Model to Unify Topic Relevance and Lexicon-based
Sentiment for Opinion Retrieval. In Proceedings of
the 31st annual international ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval (SIGIR 2008), pages 411?418, New York,
New York, USA. ACM Press.
299
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 716?721,
Dublin, Ireland, August 23-24, 2014.
UMCC_DLSI_SemSim: Multilingual System for Measuring  
Semantic Textual Similarity 
 
 
Alexander Ch?vez 
H?ctor D?vila 
DI, University of Matanzas, Cuba.  
{alexander.chavez,  
hector.davila}@umcc.cu 
Yoan Guti?rrez 
Antonio Fern?ndez-Orqu?n 
Andr?s Montoyo 
Rafael Mu?oz 
DLSI, University of Alicante, Spain. 
{ygutierrez 
montoyo,rafael}@dlsi.ua.es, 
antonybr@yahoo.com 
 
Abstract 
In this paper we describe the 
specifications and results of 
UMCC_DLSI system, which was 
involved in Semeval-2014 addressing two 
subtasks of Semantic Textual Similarity 
(STS, Task 10, for English and Spanish), 
and one subtask of Cross-Level Semantic 
Similarity (Task 3). As a supervised 
system, it was provided by different types 
of lexical and semantic features to train a 
classifier which was used to decide the 
correct answers for distinct subtasks. 
These features were obtained applying the 
Hungarian algorithm over a semantic 
network to create semantic alignments 
among words. Regarding the Spanish 
subtask of Task 10 two runs were 
submitted, where our Run2 was the best 
ranked with a general correlation of 0.807. 
However, for English subtask our best run 
(Run1 of our 3 runs) reached 16th place of 
38 of the official ranking, obtaining a 
general correlation of 0.682. In terms of 
Task 3, only addressing Paragraph to 
Sentence subtask, our best run (Run1 of 2 
runs) obtained a correlation value of 0.760 
reaching 3rd place of 34. 
1 Introduction 
Many applications of language processing rely on 
measures of proximity or remoteness of various 
kinds of linguistic units (words, meanings, 
sentences, documents). Thus, issues such as 
disambiguation of meanings, detection of lexical 
chains, establishing relationships between 
documents, clustering, etc., require accurate 
similarity measures. 
The problem of formalizing and quantifying an 
intuitive notion of similarity has a long history in 
philosophy, psychology, artificial intelligence, 
and through the years has followed many different 
perspectives (Hirst, 2001). Recent research in the 
field of Computational Linguistics has 
emphasized the perspective of semantic relations 
between two lexemes in a lexical resource, or its 
inverse, semantic distance. The similarity of 
sentences is a confidence score that reflects the 
relationship between the meanings of two 
sentences. This similarity has been addressed in 
the literature with terminologies such as affinity, 
proximity, distance, difference and divergence 
(Jenhani, et al., 2007). The different applications 
of text similarity have been separated into a group 
of similarity tasks: between two long texts, for 
document classification; between a short text with 
a long text, for Web search; and between two short 
texts, for paraphrase recognition, automatic 
machine translation, etc. (Han, et al., 2013). 
At present, the calculation of the similarity 
between texts has been tackled from different 
points of views. Some have opted for a single 
measure to capture all the features of texts and 
other models have been trained with various 
measures to take text features separately. In this 
work, we addressed the combination of several 
measures using a Supervised Machine Learning 
(SVM) approach. Moreover, we intend to 
introduce a new approach to calculate textual 
similarities using a knowledge-based system, 
which is based on a set of cases composed by a 
vector with values of several measures. We also 
combined both approaches. 
This work is licensed under a Creative Commons 
Attribution 4.0 International Licence. Page numbers and 
proceedings footer are added by the organisers. Licence 
details: http://creativecommons.org/licenses/by/4.0/ 
716
After this introduction, the rest of the paper is 
organized as follows. Section 2 shows the Pre-
processing stage. Subsequently, in Section 3 we 
show the different features used in our system. In 
Section 4 we describe our knowledge-based 
system. Tasks and runs are provided in Section 5. 
Finally, the conclusions and further work can be 
found in Section 6. 
2 Pre-processing 
Below are listed the pre-processing steps 
performed by our system. In bold we emphasize 
some cases which were used in different tasks. 
? All brackets were removed.  
? The abbreviations were expanded to their 
respective meanings. It was applied using a 
list of the most common abbreviations in 
English, with 819 and Spanish with 473. 
Phrases like ?The G8? and ?The Group of 
Eight? are detected as identical. 
? Deletion of hyphen to identify words 
forms. For example, ?well-studied? was 
replaced by ?well studied?. Example taken 
from line 13 of MSRpar corpus in test set 
of Semeval STS 2012 (Agirre, et al., 2012). 
? The sentences were tokenized and POS-
tagged using Freeling 3.0 (Padr? and 
Stanilovsky, 2012). 
? All contractions were expanded. For 
example: n't, 'mand 's. In the case of 's was 
replaced with ?is? or ?of?, ?Tom's bad? to 
?Tom is bad? and ?Tom's child? by "Child 
of Tom". (Only for English tasks). 
? Punctuation marks were removed from the 
tokens except for the decimal point in 
numbers. 
? Stop words were removed. We used a list 
of the most common stop words. (28 for 
English and 48 for Spanish). 
? The words were mapped to the most 
common sense of WordNet 3.0. (Only for 
Spanish task). 
? A syntactic tree was built for every 
sentence using Freeling 3.0. 
                                                 
1 The windows is the number of intermediate words 
between two words. 
2 Dataset of high quality English paragraphs containing over 
three billion words and it is available in 
http://ebiquity.umbc.edu/resource/html/id/351 
3 Features Extraction 
Measures of semantic similarity have been 
traditionally used between words or concepts, and 
much less between text segments, (i.e. two or 
more words). The emphasis on word to word 
similarity is probably due to the availability of 
resources that specifically encode relations 
between words or concepts (e.g. WordNet) 
(Mihalcea, et al., 2006). Following we describe 
the similarity measures used in this approach. 
3.1 Semantic Similarity of Words 
A relatively large number of word to word 
similarity metrics have previously been proposed 
in the literature, ranging from distance-oriented 
measures computed on semantic networks, to 
metrics based on models of distributional 
similarity learned from large text collections 
(Mihalcea, et al., 2006). 
3.2 Corpus-based Measures 
Corpus-based measures of word semantic 
similarity try to identify the degree of similarity 
between words using information exclusively 
derived from large corpora (Mihalcea, et al., 
2006). We considered one metric named Latent 
Semantic Analysis (LSA) (Landauer, et al., 1998). 
Latent Semantic Analysis: The Latent 
semantic analysis (LSA) is a corpus/document 
based measure proposed by Landauer in 1998. In 
LSA term co-occurrences in a corpus are captured 
by means of a dimensionality reduction operated 
by singular value decomposition (SVD) on the 
term-by-document matrix ?  representing the 
corpus (Mihalcea, et al., 2006). There is a 
variation of LSA called HAL (Hyperspace 
Analog to Language) (Burgess, et al., 1998) that 
is based on the co-occurrence of words in a 
common context. The variation consists of 
counting the number of occurrences in that two 
words appear at n1 distance (called windows). 
For the co-occurrence matrix of words we took 
as core the UMBC WebBase corpus2 (Han, et al., 
2013), which is derived from the Stanford 
WebBase project3 . For the calculation of HAL 
measure we used the Cosine Similarity between 
the vectors for each pair of words. 
3 Stanford WebBase 2001. http://bit.ly/WebBase.  
 
717
3.3 Knowledge-based Measures 
There are many measures developed to quantify 
the degree of semantic relation between two 
words senses using semantic network 
information. For example: 
Leacock & Chodorow Similarity: The 
Leacock & Chodorow (LC) similarity is 
determined as follows: 
????? = ? log (
??????
2??
)        (1) 
Where length is the length of the shortest path 
between senses using node-counting and D is the 
maximum depth of the taxonomy (Leacock and 
Chodorow, 1998) 
Wu and Palmer: The Wu and Palmer 
similarity metric (Wup) measures the depth of two 
given senses in the WordNet taxonomy, and the 
depth of the least common subsumer (LCS), and 
combine them into a similarity score (Wu and 
Palmer, 1994): 
?????? =
2??????(???)
?????(?????1)+?????(?????2)
      (2) 
 
Resnik: The Resnik similarity (Res) returns 
the information content (IC) of the LCS of two 
senses: 
?????? = ??(???)      (3) 
 
Where IC is defined as: 
 
??(?) = ? log?(?)       (4) 
 
And P(c) is the probability of encountering an 
instance of sense c in a large corpus (Resnik, 
1995) (Mihalcea, et al., 2006). 
Lin: The Lin similarity builds on Resnik?s 
measure and adds a normalization factor 
consisting of the information content of the two 
inputs senses (Lin, 1998): 
 
?????? =
2???(???)
??(?????)+??(?????2)
   (5) 
 
Jiang & Conrath: The Jiang and Conrath 
similarity (JC) is defined as follows (Jiang and 
Conrath, 1997): 
????? =
1
??(?????1)+??(?????2)?2???(???)
   (6) 
 
PathLen: The PathLen similarity (Len) 
involves the path lengths between two senses in 
the taxonomy (Pedersen, et al., 2004). 
                                                 
4 Copyright (c) 2006 by Chris Parkinson, available in 
http://sourceforge.net/projects/simmetrics/ 
 
??????? = ? log ???????(?????1, ?????1)(7) 
 
Where ???????(?????1, ?????1)  is the 
number of edges in the shortest path between 
?????1and ?????2. 
Word Similarity: In order to calculate the 
similarity between two words (WS) we used the 
following expression: 
 
??(?1,?2) =????1 ? ??????(?1)
?2 ? ??????(?2)
???(?1, ?2) 
(8) 
 
Where  ???(?1, ?2)  is one of the similarity 
metrics at sense level previously described. 
3.4 Lexical Features 
We used a well-known lexical attributes similarity 
measures based on distances between strings. 
Dice-Similarity, Euclidean-Distance, Jaccard-
Similarity, Jaro-Winkler, Levenstein Distance, 
Overlap-Coefficient, QGrams Distance, Smith-
Waterman, Smith-Waterman-Gotoh, Smith-
Waterman-Gotoh-Windowed-Affine. 
These metrics have been obtained from an API 
(Application Program Interface) SimMetrics 
library v1.5 for.NET4 2.0. 
3.5 Word Similarity Models 
With the purpose of calculating the similarity 
between two words, we developed two models 
involving the previous word similarity metrics. 
These were defined as: 
Max Word Similarity: The Max Word 
Similarity (MaxSim) is defined as follows: 
 
??????(?1,?2) =         
                
{
1              ????????????(?1,?2) = 1
??? (??????(?1,?2), ??????(?1,?2))
 
(9) 
Where ??????????(?1,?2) is the QGram-
Distance between w1 and w2. 
Statistics and Weight Ratio: For calculating 
the weight ratio in this measure of similarity was 
used WordNet 3.0 and it was defined in (10): 
????????? (?1,?2) =   
(??????(?1,?2) + (
1
??????(?1,?2)
))
2
 
 
(10) 
718
 
Where ??????(?1,?2) takes a value based 
on the type of relationship between w1 and w2. 
The possible values are defined in Table 1. 
Value Relation between ?1 and ?2  
10 Antonym. 
1 Synonym. 
2 Direct Hypernym, Similar_To or 
Derivationally Related Form. 
3 Two-links indirect Hypernym, Similar_To 
or Derivationally Related Form. 
3 One word is often found in the gloss of the 
other. 
9 Otherwise. 
Table 1: Values of Weight Ratio. 
3.6 Sentence Alignment 
In the recognition of texts? similarities, several 
methods of lexical alignment have been used and 
can be appreciated by different point of views 
(Brockett, 2007) (Dagan, et al., 2005). Glickman 
(2006) used the measurement of the overleap 
grade between bags of words as a form of 
sentence alignment. Rada et al. (2006) made 
reference to an all-for-all alignment, leaving open 
the possibility when the same word of a sentence 
is aligned with several sentences. For this task, we 
used the Hungarian assignment algorithm as a 
way to align two sentences (Kuhn, 1955). Using 
that, the alignment cost between the sentences was 
reduced. To increase the semantic possibilities we 
used all word similarity metrics (including the two 
word similarity models) as a function cost. 
3.7 N-Grams Alignment 
Using the Max Word Similarity model, we 
calculated three features based on 2-gram, 3-gram 
and 4-gram alignment with the Hungarian 
algorithm. 
4 Knowledge-based System 
For similarity calculation between two phrases, 
we developed a knowledge-based system using 
SemEval-2012, SemEval-2013 and SemEval-
2014 training corpus (Task 10 and Task 1 for the 
last one). For each training pair of phrases we 
obtained a vector with all measures explained 
above. Having it, we estimated the similarity 
value between two new phrases by applying the 
Euclidian distance between the new vector (made 
with the sentence pair we want to estimate the 
similarity value) and each vector in the training 
corpus. Then, the value of the instance with minor 
Euclidian Distance was assigned to the new pair 
of phrases. 
5 Tasks and runs 
Our system participated in Sentence to Phrase 
subtask of Task 3: ?Cross-Level Semantic 
Similarity? (Jurgens, et al., 2014) and in two 
subtasks of Task 10: ?Multilingual Semantic 
Textual Similarity? of SemEval-2014. It is 
important to remark that our system, using SVM 
approach, did not participate in Task 1: 
?Evaluation of compositional distributional 
semantic models on full sentences through 
semantic relatedness and textual entailment?, due 
to deadline issues. We compared our system 
results with the final ranking of Task 1 and we 
could have reached the 6th place of the ranking for 
Relatedness Subtask with a 0.781 of correlation 
coefficient, and the 9th place for Entailment 
Subtask with an accuracy of 77.41%. 
 
 
 
 
Task 
10 
Sp 
Task 10 
En 
Task 3 
Sentence 
to 
Phrase 
Features/Runs 1 2 1 2 3 1 2 
PathLenAlign x  x x  x x 
ResAlign x  x x  x x 
LcAlign x  x x  x x 
WupAlign x  x x  x x 
Res x  x x  x x 
Lc x  x x  x x 
DiceSimilarity x x x x  x x 
EuclideanDistance x x x x  x x 
JaccardSimilarity x x x x  x x 
JaroWinkler x x x x  x x 
Levenstein x x x x  x x 
Overlap- 
Coefficient 
x x x x  x x 
QGramsDistance x x x x  x x 
SmithWaterman x x x x  x x 
SmithWatermanGotoh x x x x  x x 
SmithWatermanGotoh- 
WindowedAffine 
x x x x  x x 
BiGramAlingHungMax x  x x  x x 
TriGramAlingHungMax x  x x  x x 
TetraGramAlingHungMax x  x x  x x 
WordAlingHungStatWeigthRatio x  x x  x x 
SentenceLengthPhrase1 x  x x  x x 
SentenceLengthPhrase2 x  x x  x x 
Table 2: Features and runs. Spanish (Sp) and 
English (En). 
In Table 2 is important to remark the 
following situations: 
? In Task 10 Spanish (two runs), we used the 
training corpus of Task 10 English. 
719
? In Run2 of Task 10 English, the similarity 
score was replaced for the knowledge-
based system value if Euclidean Distance 
of the most similar case was less than 0.30.  
? Run3 of Task 10 English was a knowledge-
based system. 
? In Run1 of Sentence to Phrase of Task 3, 
we trained the SVM model using only the 
training corpus of this task. 
? In Run2 of Sentence to Phrase of Task 3, 
we trained the SVM model using the 
training corpus of this task and the training 
corpus of Task 10 English. 
6 Conclusion 
In this paper we introduced a new framework for 
recognizing Semantic Textual Similarity, 
involving feature extraction for SVM model and a 
knowledge-based system. We analyzed different 
ways to estimate textual similarities applying this 
framework. We can see in Table 3 that all runs 
obtained encouraging results. Our best run was 
first position of the ranking for task 10 (Spanish) 
and other important positions were reached in the 
others subtasks. According to our participation, 
we used a SVM which works with features 
extracted from six different strategies: String-
Based Similarity Measures, Semantic Similarity 
Measures, Lexical-Semantic Alignment, 
Statistical Similarity Measures and Semantic 
Alignment. Finally, we can conclude that our 
system achieved important results and it is able to 
be applied on different scenarios, such as task 10, 
task 3.1 and task 1. See Table 3 and the beginning 
of Section 5. 
Subtask Run 
SemEval-
2014 
Position 
Task 10-
Spanish 
Run1 4 
Run2 1 
Task 10-
English 
Run1 16 
Run2 18 
Run3 33 
Task-3 
Run1 3 
Run2 16 
Table 3: SemEval-2014 results. 
As further work, we plan to analyze the main 
differences between task 10 for Spanish and 
English in order to homogenise both system?s 
results. 
Acknowledgments 
This research work has been partially funded by 
the University of Alicante, Generalitat 
Valenciana, Spanish Government and the 
European Commission through the projects, 
"Tratamiento inteligente de la informaci?n para la 
ayuda a la toma de decisiones" (GRE12-44), 
ATTOS (TIN2012-38536-C03-03), LEGOLANG 
(TIN2012-31224), SAM (FP7-611312), FIRST 
(FP7-287607) and ACOMP/2013/067. 
Reference 
Eneko Agirre, Mona Diab, Daniel Cer and Aitor 
Gonzalez-Agirre, 2012. SemEval 2012 Task 6: A 
Pilot on Semantic Textual Similarity.. s.l., First 
Join Conference on Lexical and Computational 
Semantic (*SEM), Montr?al, Canada. 2012., pp. 
385-393. 
Chris Brockett, 2007. Aligning the RTE 2006 Corpus. 
Microsoft Research, p. 14. 
Curt Burgess, Kay Livesay and Kevin Lund, 1998. 
Explorations in Context Space: Words, 
Sentences, Discourse. Discourse Processes, Issue 
25, pp. 211 - 257. 
Ido Dagan, Oren Glickman and Bernardo Magnini, 
2005. The PASCAL Recognising Textual 
Entailment Challenge. En: Proceedings of the 
PASCAL Challenges Workshop on Recognising 
Textual Entailment. 
Oren Glickman, Ido Dagan and Moshe Koppel, 2006. 
A Lexical Alignment Model for Probabilistic 
Textual Entailment. In: Proceedings of the First 
International Conference on Machine Learning 
Challenges: Evaluating Predictive Uncertainty 
Visual Object Classification, and Recognizing 
Textual Entailment. Southampton, UK: Springer-
Verlag, pp. 287--298. 
Lushan Han et al., 2013. UMBC_EBIQUITY-CORE: 
Semantic Textual Similarity Systems. s.l., s.n. 
Alexander B. Hirst and Graeme, 2001. Semantic 
distance in WordNet: An experimental, 
application-oriented evaluation of five measures. 
Ilyes Jenhani, Nahla Ben Amor and Zi Elouedi, 2007. 
Information Affinity: A New Similarity Measure 
for Possibilistic Uncertain Information. En: 
Symbolic and Quantitative Approaches to 
Reasoning with Uncertainty. s.l.:Springer Berlin 
Heidelberg, pp. 840-852. 
Jay Jiang and David Conrath, 1997. Semantic 
similarity based on corpus statistics and lexical 
taxonomy. s.l., Proceedings of the International 
Conference on Research in Computational 
Linguistics. 
David Jurgens, Mohammad Taher and Roberto 
Navigli, 2014. SemEval-2014 Task 3: Cross-
720
Level Semantic Similarity. Dublin, Ireland, In 
Proceedings of the 8th International Workshop on 
Semantic Evaluation., pp. 23-24. 
Harold W. Kuhn, 1955. The Hungarian Method for the 
assignment problem. Naval Research Logistics 
Quarterly. 
Thomas K. Landauer, Peter W. Foltz and Darrell 
Laham, 1998. Introduction to latent semantic 
analysis. Discourse Processes, Issue 25, pp. 259-
284. 
Claudia Leacock and Martin Chodorow, 1998. 
Combining local context and WordNet sense 
similarity for word sense identification. s.l.:s.n. 
Lin Dekang, 1998. An information-theoretic definition 
of similarity. s.l., Proceedings of the International 
Conf. on Machine Learning. 
Rada Mihalcea, Courtney Corley and Carlo 
Strapparava, 2006. Corpus-based and 
knowledge-based measures of text semantic 
similarity. In: IN AAAI?06. s.l.:21st National 
Conference on Artificial Intelligence, pp. 775--
780. 
Lu?s Padr? and Evgeny Stanilovsky, 2012. FreeLing 
3.0: Towards Wider Multilinguality. Istanbul, 
Turkey, Proceedings of the Language Resources 
and Evaluation Conference (LREC 2012) ELRA. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi, 2004. WordNet::Similarity - 
Measuring the Relatedness of Concepts. 
American Association for Artificial Intelligence. 
Philip Resnik, 1995. Using information content to 
evaluate semantic similarity. s.l., Proceedings of 
the 14th International Joint Conference on 
Artificial Intelligence. 
Zhibiao Wu and Martha Palmer, 1994. Verb semantics 
and lexical selection. 
 
 
721
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 727?731,
Dublin, Ireland, August 23-24, 2014.
UMCC DLSI: Sentiment Analysis in Twitter using Polirity Lexicons and
Tweet Similarity
Pedro Aniel S
?
anchez-Mirabal,
Yarelis Ruano Torres,
Suilen Hern
?
andez Alvarado
University of Matanzas / Cuba
pedroasm@umcc.cu
yara@umcc.cu
suilen.alvarado@umcc.cu
Yoan Guti
?
errez,
Andr
?
es Montoyo,
Rafael Mu
?
noz
University of Alicante/Spain
ygutierrez@dlsi.ua.es
montoyo@dlsi.ua.es
rafael@dlsi.ua.es
Abstract
This paper describes a system sub-
mitted to SemEval-2014 Task 4B:
Sentiment Analysis in Twitter, by the
team UMCC DLSI Sem integrated by
researchers of the University of Matanzas,
Cuba and the University of Alicante,
Spain. The system adopts a cascade
classification process that uses two classi-
fiers, K-NN using the lexical Levenshtein
metric and a Dagging model trained over
attributes extracted from annotated cor-
pora and sentiment lexicons. Phrases that
fit the distance thresholds were automat-
ically classified by the KNN model, the
others, were evaluated with the Dagging
model. This system achieved over 52.4%
of correctly classified instances in the
Twitter message-level subtask.
1 Introduction
Nowadays, one of the most important sources of
data to extract useful and heterogeneous knowl-
edge is Textual Information. Daily, millions
of Tweets, SMS and blog comments increase
the huge volume of information available for re-
searchers. Texts can provide factual information,
such as: descriptions, lists of characteristics, or
even instructions to opinion-based information,
which would include reviews, emotions, or feel-
ings (Guti?errez et al., 2013). These facts have
motivated that dealing with the identification and
extraction of opinions and sentiments in texts re-
quires special attention. Applications of Senti-
ment Analysis are now more common than ever
in fields like politics and business. More than 50
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
systems participating in this task, clearly indicate
the increase of interest in the scientific community.
Twitter messages can be found among of the
most used corpora nowadays for Sentiment Anal-
ysis (SA). This kind of messages involves an evi-
dent informality which has been addressed in dif-
ferent ways. For example, there are some works
like (Guti?errez et al., 2013) that apply normali-
sation textual tools to reduce the informality of
the twitter messages. Authors such as (Go et al.,
2009), (Guti?errez et al., 2013), (Fern?andez et al.,
2013) and others are focused on the application
of preprocessing processes and feature reduction
to be able to standardise twitter messages and re-
duce different types of elements like hashtags, user
nicks, urls, etc.
In terms of those techniques that can be used
for SA, we can cite (Pang et al., 2002) who built
a lexicon with associated polarity value, starting
with a set of classified seed adjectives and using
conjunctions (and) disjunctions (or, but) to deduce
the orientation of new words in a corpus. This re-
search was based on machine learning techniques
to address Sentiment Classification. Other inter-
esting research is (Turney, 2002), which classi-
fies words according to their polarity based on
the idea that terms with similar orientation tend
to co-occur in documents. There are a large quan-
tity of approaches to deal with SA, and basically
most of them are based on word bags and/or an-
notated corpora as knowledge base. Based on this
information the SA systems are able to apply dif-
ferent types of evaluation techniques such as ma-
chine learning or statistic formulas to predict the
correct classification. As part of machine learn-
ing approaches we would like to mention those
works such as (Go et al., 2009), (Mohammad et
al., 2013) and others that were based on feature
vectors and which cover a wide range settings of
SA. As a starting point, we based this work on
the (Mohammad et al., 2013) approach, adding
727
new features extracted from the sentiment repos-
itories Sentiment 140
1
and NRC-Hashtag Senti-
ment (Mohammad and Turney, 2013).
The remainder of this paper is structured as fol-
lows: section 2 describes in detail the approach
presented. In section 3 we explain the experiments
we carried out. Finally in section 4 conclusions
and future works are expounded.
2 System Description
In this section we present our system in detail
which is able to classify the polarity of tweets as
positive, negative, or neutral.
The system is structured in two main stages.
The first stage consists of classifying a given
tweet. For that, we first recovered all the tweets
from the training corpus that have a similarity
value greater than a fixed threshold T . The sec-
ond stage consists of classifying using the K-NN
rule (Coomans and Massart, 1982), considering as
K all tweets recovered. The process begins with
T = 0.9 decreasing it until T = 0.6. In section 3
we will explain how these values were determined.
As similarity metric we use the Levenshtein
(Levenshtein, 1966) lexical distance. In case that
we cannot find any tweet fulfilling the condition,
the tweet polarity is assigned using a second clas-
sifier trained using Dagging which combines sev-
eral Logistic classifiers set by WEKA as default.
2.1 Preprocessing
The first step in our system is to pre-process all
tweets. The following operations were applied in
the given order.
- Replacing emoticons: Each emoticon is
replaced by a word according to a
lexicon of emoticons. The mean-
ings of the emoticons were taken from
http://en.wikipedia.org/wiki/
List_of_emoticons.
- Replacing acronyms: Each acronym is re-
placed by its meaning. The meanings of the
acronyms were taken from http://www.
acronymfinder.com/.
- Cleaning text: Remove not alphanumeric char-
acters from the tweet.
- Replacing abbreviations: Each abbrevia-
tion is replaced by its respective words.
1
http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
The abbreviations were taken from
http://en.wikipedia.org/wiki/
Abbreviation.
- Lemmatising: Each word is replaced by its
lemma. We use Freeling 3.0 (Padr?o and
Stanilovsky, 2012) for this purpose. We only
retain lemmas corresponding to adjectives,
adverbs, interjections, nouns and verbs.
- Expanding contractions: Each contraction
is replaced by its respective word. The
contractions were taken from http://
www.softschools.com/language_
arts/grammar/contractions/
contractions_list/.
- Deleting punctuation marks.
- Deleting stop words. The stop words
were taken from http://www.ranks.
nl/stopwords.
2.2 Recovering tweets from similarity
As it was explained before, in a first step we tried
to classify tweets using the K-NN rule. To recover
the K similar tweets we used the Levenshtein met-
ric (Levenshtein, 1966). This measure allows to
compute the similarity of two strings of symbols
counting the minimum number of deletions, sub-
stitutions and insertions necessary to transform
one string into another. In our case, each word in
the string is considered as a symbol. In the future
we plan to improve this metric using Levenshtein
at word level and then at sentence level. This met-
ric is known as DLED (Double Levenshteins Edit
Distance) and will be taken from (Fern?andez et al.,
2012).
2.3 Features for Dagging classifier
We represented each tweet as a vector of features
based in (Mohammad et al., 2013) plus other new
ones. Also we used the lexicons Sentiment 140
and NRC-Hashtag Sentiment as it was defined
by Mohammad.
Also two new lexicons, named NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0 were
derived from the NRC Emotion Lexicon (Mo-
hammad and Turney, 2013). In the first case we
associated to each word just the values in the
columns positive and negative of NRC Emotion
Lexicon, thus, no sentiment score was computed.
728
For the second lexicon, the positive score was cal-
culated as the sum of the values for the classifica-
tions positive, anticipation, joy, surprise and trust.
On the other hand, the negative score was com-
puted as the sum of the values for the classifica-
tions negative, anger, disgust, fear, sadness and
trust.
In each case we computed the following at-
tributes:
- Pos: Sum of the positive scores of each token
in the tweet over the number of tokens in the
tweet.
- Neg: Sum of the negative scores of each token
in the tweet over the number of tokens in the
tweet.
- PercentPos:
100?Pos
Pos+Neg
- MissNGram: Percent of tokens in the tweet that
were not found in the lexicon.
For the Sentiment 140 and NRC-Hashtag Sen-
timent lexicons we also computed the feature:
- SSE: Sum of the sentiment score of each token
in the tweet over the number of tokens in the
tweet.
Based on the information involved into Senti-
ment 140 and NRC-Hashtag Sentiment lexicons,
unigrams, bigrams and pairs were tokenised in-
volving any non-contiguous combination of the
previous n-grams. With respect to the pairs extrac-
tion were considered the following possibilities:
unigram-unigram, unigram-bigram and bigram-
bigram. Similar to (Mohammad et al., 2013) dif-
ferent set of attributes were generated for each
type of token. As result an initial set of 50 at-
tributes were obtained.
In the case of the new lexicons (NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0), only
unigrams were considered. Moreover, the feature
SSE was not computed. So, another 8 features
were taken into account with respect to these lexi-
cons.
Finally we computed:
- NCL: Percent of tokens in capital letters.
- NoE: Number of emoticons in the tweet.
- NoA: Number of acronyms in the tweet.
In general the system works with a total of 61
attributes.
2.4 Classifier Design
As training set, we joined the preprocessed tweets
from both the train and development sets pro-
vided by the Task9B of Semeval-2014. The
Dagging classifier was trained using this set
with the following parameters -F 15 -S 1 -W
weka.classifiers.functions.Logistic ? -R 1.0E-8 -
M -1 using a 10 fold cross-validation as evaluation
method.
3 Experiments
The experiments were evaluated over the training
dataset provided by Task 9: Sentiment Analysis in
Twitter, subtask B. Based on the explanation pro-
vided in section 2 according to the initialisation of
the threshold T to ensure that the K similar tweets
are in fact similar enough, we carried out an exper-
iment for different values of T . These experiments
refer an analysis to know how the variation of T
affects the classification results.
T % CCI
0.9 86.7
0.8 83.3
0.7 74.1
0.6 67.2
0.5 61.1
0.4 55.0
0.3 56.0
Table 1: Results of the K-NN classifier using Lev-
enshtein metric.
T % CCI
0.9 81.2
0.8 83.3
0.7 74.1
0.6 66.7
0.5 63.1
0.4 60.6
0.3 54.2
Table 2: Results of the K-NN classifier using
Matching Coefficient metric.
The first stage of the system was applied to
compute the number of instances which have at
least one instance with a similarity value greater
than T . We computed the percent of instances
correctly classified (%CCI). Table 1 shows the
behaviour of the system when T changes. Table
2 shows the results of the K-NN classifier using
729
System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm
Best result 74.8 70.3 72.1 71.0 58.2
Average result 63.5 55.6 59.8 60.6 45.4
UMCC-DLSI-Sem 53.1 50.0 52.0 55.4 42.8
Worse result 29.3 24.6 34.2 33.0 29.0
Table 3: Results in the SemEval-2014 Task 4B.
Matching Coefficient metric (http://www.
coli.uni-saarland.de/courses/LT1/
2011/slides/stringmetrics.pdf).
This metric counts the quantity of matched
symbols (words in this case) between two
sentences.
Furthermore, we repeated this experiment using
the Matching Coefficient similarity metric to bet-
ter tunning the algorithm and to evaluate if the re-
sults behave in a similar way when T changes. In
both cases, we use the implementation provided in
the SimMetrics library.
As those results shows, when T decrease the ac-
curacy decrease too. In practice, for the values of
T lower than 0.6 the results are worse than 61.4%
using the Dagging classifier in the 10 fold cross-
validation. For that reason, as was mentioned in 2,
we only tried to apply the first stage for values of
T ? 0.6 .
We evaluated our system in the challenge Task
4B: Sentiment Analysis in Twitter, using the pro-
vided training and test data of this challenge.
Based on the classifier obtained in the training pro-
cess we tested our system over the test dataset
achieving values of %CCI up to 55.4. Table 3
show detailed results for each of the 5 different
sources.
4 Conclusions and Future Works
Our system was based on an approach that follows
two stages to classify the polarity of tweets. Re-
gardless the fact that our system behaves worse
than the average, we consider that the approach is
suitable to deal with SA, since our results are close
to the average. As future works we will study
other approaches in order to encourage further de-
velopments of this proposal. Several issues could
be adjusted, for example, other distances should be
tested and evaluated such as DLED (Double Lev-
enshteins Edit Distance) (Fern?andez et al., 2012).
Also, features that encode information about the
presence of negation and opposition words could
be very useful.
Acknowledgements
This research work has been partially funded
by the University of Alicante, Generalitat Va-
lenciana, Spanish Government and the European
Commission through the projects, ?Tratamiento
inteligente de la informacin para la ayuda a la toma
de decisiones? (GRE12-44), ATTOS (TIN2012-
38536-C03-03), LEGOLANG (TIN2012-31224),
SAM (FP7-611312), FIRST (FP7-287607) and
ACOMP/2013/067.
References
D. Coomans and D.L. Massart. 1982. Alternative k-
nearest neighbour rules in supervised pattern recog-
nition : Part 1. k-nearest neighbour classification by
using alternative voting rules. Analytica Chimica
Acta, 136(0):15?27.
Antonio Fern?andez, Yoan Guti?errez, H?ector D?avila,
Alexander Ch?avez, Andy Gonz?alez, Rainel Estrada,
Yenier Casta?neda, Sonia V?azquez, Andr?es Montoyo,
and Rafael Mu?noz. 2012. Umcc dlsi: Multidimen-
sional lexical-semantic textual similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
608?616, Montr?eal, Canada, 7-8 June. Association
for Computational Linguistics.
Javi Fern?andez, Yoan Guti?errez, Jos?e M G?omez, Patri-
cio Mart?nez-Barco, Andr?es Montoyo, and Rafael
Munoz. 2013. Sentiment analysis of spanish tweets
using a ranking algorithm and skipgrams. Proc. of
the TASS workshop at SEPLN 2013. IV Congreso
Espa?nol de Inform?atica, pages 17?20.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Yoan Guti?errez, Andy Gonz?alez, Roger P?erez, Jos?e I.
Abreu, Antonio Fern?andez Orqu??n, Alejandro Mos-
quera, Andr?es Montoyo, Rafael Mu?noz, and Franc
Camara. 2013. Umcc dlsi-(sa): Using a ranking
algorithm and informal features to solve sentiment
analysis in twitter. In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International
730
Workshop on Semantic Evaluation (SemEval 2013),
pages 443?449, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Vladimir Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and rever-
sals. Cybernetics and Control Theory, 10(8):707?
710. Original in Doklady Akademii Nauk SSSR
163(4): 845?848 (1965).
Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436?465.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
731
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 773?778,
Dublin, Ireland, August 23-24, 2014.
UO UA: Using Latent Semantic Analysis to Build a Domain-Dependent
Sentiment Resource
Reynier Ortega
Adrian Fonseca
Carlos Mu
?
niz
CERPAMID
Ave Patricio Lumumba, S/N
Santigo de Cuba, Cuba
reynier.ortega@cerpamid.co.cu
adrian@cerpamid.co.cu
Yoan Guti
?
errez
Andr
?
es Montoyo
DLSI, University of Alicante
Carretera de San Vicente, S/N
Alicante, Spain
ygutierrez@dlsi.ua.es
montoyo@dlsi.ua.es
Abstract
In this paper we present our contribution to
SemEval-2014 Task 4: Aspect Based Sen-
timent Analysis (Pontiki et al., 2014), Sub-
task 2: Aspect Term Polarity for Laptop
domain. The most outstanding feature in
this contribution is the automatic building
of a domain-depended sentiment resource
using Latent Semantic Analysis. We in-
duce, for each term, two real scores that in-
dicate its use in positive and negative con-
texts in the domain of interest. The aspect
term polarity classification is carried out
in two phases: opinion words extraction
and polarity classification. The opinion
words related with an aspect are obtained
using dependency relations. These rela-
tions are provided by the Stanford Parser
1
.
Finally, the polarity of the feature, in a
given review, is determined from the pos-
itive and negative scores of each word re-
lated to it. The results obtained by our ap-
proach are encouraging if we consider that
the construction of the polarity lexicon is
performed fully automatically.
1 Introduction
Hundreds of millions of people and thousands
of companies around the world, actively use So-
cial Media
2
. Every day are more amazing web-
sites and applications (Facebook, Twitter, MyS-
pace, Amazon, etc.) that allow the easy sharing
of information in near real time. For this rea-
son, at present, the Web is flooded with subjec-
tive, personal and affective data. Mining this huge
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://nlp.stanford.edu:8080/parser/
2
http://en.wikipedia.org/wiki/Social media
volume of information offer both interesting chal-
lenges and useful intelligent applications such as
recommendation systems (Dong et al., 2013; Sun
et al., 2009) and customer?s reviews summariza-
tion (Bafna and Toshniwal, 2013; Balahur and
Montoyo, 2008).
Nowadays, companies have redirected their
marketing strategies toward the Web. Each one
of them advertises that their products are the best,
amazing, easy to use, long lasting and cheap. But
are these advertisements really true? Obviously,
not everything is true. The companies usually ex-
aggerate the product?s quality and in many cases
tend not to advertise the limitations of their prod-
ucts. Therefore, taking a rational decision about
which product is the best among the variety of ex-
isting options can be very stressful.
To avoid this situation, frequently we trust in
the experiences gained by others who have pur-
chased the product of our interest, or one similar.
The existence of websites like Ciao
3
, Epinions
4
and Cnet
5
make possible to the customers to inter-
change their experiences about a specific product,
and to future clients avoid products advertising
However, the existence of a large volume of re-
views entails that it is impossible to conduct an
effective exploration before making a final deci-
sion. The most important benefit of having that
amount of user-generated content on hand, specif-
ically product?s reviews, is that, these data can be
explored by a computer system to obtain informa-
tion about products and their features.
The task of aspect-based sentiment analysis
(Liu, 2012) is a fine-grained level of Sentiment
Analysis (Pang and Lee, 2008). This aim to iden-
tify the aspects (e.g., battery, screen, food, ser-
vice, size, weight, time-life) of given target entities
3
www.ciao.com
4
www.epinions.com
5
www.cnet.com
773
(e.g., laptops, restaurants, camera) and the senti-
ment expressed towards each aspect (e.g., positive,
negative, neutral). This are composed by two ba-
sic phases: feature extraction and feature polarity
classification.
In this paper we present our contribution for
SemEval-2014 Task 4: Aspect Based Sentiment
Analysis (Pontiki et al., 2014), Subtask 2: Aspect
Term Polarity. In this approach we only focus on
the polarity classification problem. For this, we
induce a domain-dependent sentiment lexicon ap-
plying Latent Semantic Analysis (LSA) on prod-
uct reviews corpus, gathered from Ciao. The clas-
sification phase is carried out as follow: the opin-
ion words related with the product aspect are draw
out using the dependency relations provided by
Stanford Parser, then the polarity of the extracted
words are combined to obtain overall aspect polar-
ity.
The paper is organized as follows. Section 2 de-
scribes our approach. Further on, in Section 3, we
discuss the results obtained in the SemEval 2014
Task No. 4 subtask 2. Finally, section 4 provides
concluding remarks.
2 UO UA System
One of major challenge in sentiment analysis into
product reviews, is dealing with a quite domain de-
pendence. For instance, the word ?unpredictable?
can be considered as positive in Movie domain,
however it is very negative in Airplane domain.
For this reason, we propose to create a specific
sentiment lexicon for addressing aspect based sen-
timent analysis in reviews.
Our proposal is divided in two main phases. The
first one aims to build a domain-dependent senti-
ment resource for Laptop domain applying LSA.
The second phase obtains the words related by
means of some dependency relation with the as-
pect, and later, the polarity of these words are
obtained from induced polarity lexicon and com-
bined for computing overall aspect polarity.
2.1 Domain-Dependent Polarity Lexicon
The use of sentiment resource has been proven
to be useful to build, train, and evaluate systems
for sentiment analysis (Guti?errez et al., 2013; Bal-
ahur, 2011). In order to build sentiment resource,
several approach has been presented. In one of
the first works, presented by (Hatzivassiloglou and
McKeown, 1997), was proposed to take into ac-
count if adjectives are linked by adversative or
copulative conjunctions for detecting its polarity.
In (Turney and Littman, 2003) the authors exposed
a method for inferring the semantic orientation of
a word from its statistical association with a set
of positive and negative paradigm words, mea-
sured by point-wise mutual information (PMI). In
(2004), Hu and Liu suggested a technique to ex-
pand the lexicon using the relations of synonymy
and antonym provided by WordNet (Fellbaum,
1998). In (2009), Cruz et al., created a sentiment
resource based on a graph, constructed from con-
junctive expressions between pairs of adjectives,
observed in a review corpus. PageRank algorithm
(Page et al., 1999) was adapted to be used on
graphs with positive and negative edges, in order
to obtain the semantic orientation of words.
Despite the wide range of existing proposals
for resources construction, the results achieved
with them are far from expected. As we have
already seen, in aspect based sentiment analy-
sis, the polarity of a word is heavily dependent
on the domain; and general propose sentiment
resource such as General Inquirer (Stone et al.,
1966), WordNet-Affect(Strapparava and Valitutti,
2004), SentiWordNet(Baccianella et al., 2010) or
HowNet (Dong et al., 2010) do not capture this
dependency. On the other hand, the human anno-
tators can not create specific sentiment resources
for each new product launched to market. There-
fore, propose methods to create these resources is
a challenging task.
In this paper we address this task, presenting a
framework for building domain-dependent senti-
ment resource. Our proposal is compounded of
four phases. (See figure 1).
Firstly, review pages about the product of in-
terest can be retrieved from different websites, for
instance, Ciao, Epinions and Cnet (in this work
we only use reviews from Ciao). This reviews
are parsed and cleaned (this time we use Python
XML Parser
6
). For each page we extract: pros,
cons, title, full review and rating. In this work we
have only focus on the pros and cons attributes be-
cause they are usually very brief, consist of short
phrases or sentence segments and give a positive
and negative evaluation about the product aspects.
Each pros and cons in remainder paper will be
considered as positive and negative samples, re-
spectively.
6
https://docs.python.org/2/library/xml.html
774
Figure 1: Building domain-dependent sentiment
resource.
Subsequently, the samples are preprocessed,
applying a POS-Tagging tool (Padr?o and
Stanilovsky, 2012) to convert all words in lem-
mas. After that, the stopwords are removed
from text. Afterward each sample is represented
using the classic vector space model (Salton et
al., 1975). Intending to measure the association
between term and class we add a special term to
the vectors. In positive samples the term t
pos
is
added whereas in the negative samples the term
t
neg
is aggregated.
Later, we apply a Latent Semantic Analysis
(this time we use, Gensim python package) to cal-
culate the strength of the semantic association be-
tween words and classes. LSA uses the Singular
Value Decomposition (SVD) to analyze the statis-
tical relationships among words in a corpus.
The first step is construct a matrix M
n?m
, in
which the row vectors v
i
represent lemmas and the
column vectors s
i
the positive and negative sample
(pros and cons). In each cell t
ij
, we have the TF
score (Term Frequency) of the i
th
lemma in j
th
sample. The next step is apply Singular Value De-
composition to matrix M
n?m
to decompose it into
a product of three matrices U?V
T
, then, we select
the k largest singular values, and their correspond-
ing singular vectors from U and V , obtained an
approximation
?
M = U
k
?
k
V
T
k
of rank k to orig-
inal matrix M
n?m
. After LSA is performed, we
use the new matrix
?
M to measure the association
between lemmas l
i
and l
j
computing the cosine
measure between vectors v
i
and v
j
, with the equa-
tion 1.
LSA
score
(l
i
, l
j
) =
< v
i
, v
j
>
? v
i
? ? ? v
j
?
(1)
Finally, the polarity lexicon contains lemmas
l
i
and its positive and negative scores. This val-
ues are computed using LSA
score
(l
i
, t
pos
) and
LSA
score
(l
i
, t
neg
) respectively. The table 1 show
some top positive and negative words computed
with this strategy.
Positive Score Negative Score
sturdy 0.8249 prone 0.8322
superb 0.7293 weak 0.8189
durable 0.7074 disaster 0.8120
sexy 0.6893 erm 0.8118
powerfull 0.6700 ill 0.8107
robust 0.6686 uncomfortable 0.8084
affordable 0.6630 noisy 0.7917
suuupeerrr 0.6550 overwhelm 0.7514
lighweight 0.6550 unsturdy 0.7491
unbreakable 0.6542 lousy 0.7143
Table 1: Examples of positive and negative words.
With aim to do our contribution to SemEval-
2014, Task 4: Aspect Based Sentiment Analysis
(Pontiki et al., 2014), Subtask 2: Aspect Term Po-
larity, we gathered 3010 Laptop Reviews, from
Ciao and create a corpus with 6020 samples, 3010
positives (Pros) and 3010 negatives (Cons). This
corpus was used as input in the developed frame-
work (See figure 1). In this time we utilize Freel-
ing
7
as POS-Tagging tool and Gensim Python
Packages
8
to perform LSA (only the most 100
most significant eigenvalue are used). After that,
a domain-dependent sentiment resource (DLSR)
with 4482 term was created for Laptop reviews.
2.2 Aspect Polarity Classification
In order to exploit our domain-dependent senti-
ment resource building for Laptop domain, we de-
velop an unsupervised method based on language
rule to classify the product aspect. The basic rules
are used to find dependency relation between as-
pect and their attributes. The figure 2 show the
architecture of our proposal.
The proposed method receive as input a tuple
(P
feature
, R), where P
feature
represent the aspect
to evaluate, and R is the context (review) in it ap-
pears.
7
http://nlp.lsi.upc.edu/freeling/
8
https://pypi.python.org/pypi/gensim
775
Figure 2: Apect polarity classification.
The dependency parsed is applied to review R,
using Stanford Parser. Following that, we extract
a set of tuples W , each tuple is represented as a
pair (Att,Mod) where Att is a word related with
the aspect P
feature
through some dependency re-
lations shown in Table 2, and Mod is a integer
value indicating if Att is modified by a valence
shifter (Polanyi and Zaenen, 2004), (we only con-
sider negation words, e.g., never, no, not, don?t,
nothing, etc.) , and default value of 0 is assign, in
case that, the Att is modified by a valence shifter,
we assign value of -1.
Dependency relations
mod subj nsubj
amod csub csubpass
advmod obj dobj
vmod iobj pobj
rcmod npadvmod nn
subj xcomp advcl
Table 2: Stanford Parser dependency relations.
Once, the set of pairs W was obtained, the po-
larity of the feature P
feature
is determined from
the scores of the attributes (related words) that de-
scribe it. To sum up, for each pair (Att,Mod) ?
W , the positive Pos((Att,Mod)) and negative
Neg((Att,Mod)) scores are calculated as:
Neg((Att,Mod)) =
{
?N(Att) if Mod < 0
N(Att) otherwise
(2)
Pos((Att,Mod)) =
{
?P (Att) if Mod < 0
P (Att) otherwise
(3)
Where P (Att) and N(Att) are the positive and
negative score for Att in domain-dependent senti-
ment resource DLSR.
Finally, the global positive and negative scores
(SO
pos
, SO
neg
) are calculated as:
SO
pos
(P
feature
) =
?
w?W
Pos(w) (4)
SO
neg
(P
feature
) =
?
w?W
Neg(w) (5)
If SO
pos
is greater than SO
neg
then the aspect is
considered as positive. On the contrary, if SO
pos
is less than SO
neg
the aspect is negative. Finally,
if SO
pos
is equal to SO
neg
the aspect is considered
as neutral.
3 Results
In this section we present the evaluation of our
system in the context of SemEval-2014, Task 4:
Aspect Based Sentiment Analysis (Pontiki et al.,
2014), Subtask 2: Aspect Term Polarity. For
evaluating the participant?s system two unlabeled
domain-specific datasets for laptops and restau-
rants were distributed. For each dataset two runs
can be submitted, the first (constrained), the sys-
tem can only be used the provided training data
and other resources such as lexicons. In the sec-
ond (unconstrained), the system can use additional
data for training. We send one run for laptop
dataset and it only use external data retrieved from
Ciao website (the training data was not used) (un-
constrained).
The results achieve by our method are illustrate
in Table 3. As may be observed, the accuracy
Label Pr Rc F1
conflict 0.0 0.0 0.0
negative 0,5234 0,3764 0,4379
neutral 0,4556 0,4074 0,4302
positive 0,6364 0,7561 0,6911
Accuracy 0.55198777
Table 3: Results in aspect polarity classification
for laptop dataset.
achieve by UA OU was 0.55, and F1 measure for
negative, neutral and positive were 0,4379, 0,4302
and 0,6911 respectively. In case of conflict polar-
ity we reached a 0.0 F1 value because our system
not handle this situation. For this subtask (Laptop
domain) a total of 32 runs was submitted by all
776
systems participant?s and our run was ranked as
25
th
. The results despite not achieving expected,
are encouraging. These evidence the feasibility of
building resources from data available on the web,
for aspect-based sentiment analysis.
4 Conclusions
In this article, we presented and evaluated the
approach considered for our participation in
SemEval-2014 Task 4: Aspect Based Sentiment
Analysis (Pontiki et al., 2014), Subtask 2: Aspect
Term Polarity, specifically for Laptop Domain.
We present a framework for building domain-
dependent sentiment resources applying Latent
Semantic Analysis and build a special resource for
polarity classification in Laptop domain. This re-
source was combined into unsupervised method to
compute the polarity associated to different aspect
in reviews. The results obtained by our approach
are encouraging if we consider that the construc-
tion of the polarity lexicon is performed fully au-
tomatically.
Acknowledgements
This research work has been partially funded by
the University of Alicante, Generalitat Valenciana,
Spanish Government and the European Com-
mission through the projects, ?Tratamiento in-
teligente de la informaci?on para la ayuda a la toma
de decisiones? (GRE12-44), ATTOS (TIN2012-
38536-C03-03), LEGOLANG (TIN2012-31224),
SAM (FP7-611312), FIRST (FP7-287607) and
ACOMP/2013/067.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. Proceedings of the Seventh International
Conference on Language Resources and Evaluation,
LREC ?10, Valletta, Malta, May.
Kushal Bafna and Durga Toshniwal. 2013. Fea-
ture based summarization of customers? reviews
of online products. Procedia Computer Science,
22(0):142 ? 151. 17th International Conference in
Knowledge Based and Intelligent Information and
Engineering Systems - KES2013.
Alexandra Balahur and Andr?es Montoyo. 2008. Mul-
tilingual feature-driven opinion extraction and sum-
marization from customer reviews. In Epaminon-
das Kapetanios, Vijayan Sugumaran, and Myra
Spiliopoulou, editors, Natural Language and Infor-
mation Systems, volume 5039 of Lecture Notes in
Computer Science, pages 345?346. Springer Berlin
Heidelberg.
Alexandra Balahur. 2011. Methods and Resources
for Sentiment Analysis in Multilingual Documents
of Different Text Types. Ph.D. thesis, Department
of Software and Computing Systems. Alcalant, Al-
calant University.
Ferm??n Cruz, Jos?e Antonio Troyano, Francisco Javier
Ortega, and Carlos Garc??a Vallejo. 2009. In-
ducci?on de un lexic?on de opini?on orientado al do-
minio. Procesamiento del Lenguaje Natural, 43:5?
12.
Zhendong Dong, Qiang Dong, and Changling Hao.
2010. HowNet and its computation of meaning. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, COL-
ING?10, pages 53?56, Stroudsburg, PA, USA.
Ruihai Dong, Markus Schaal, Michael P. O?Mahony,
Kevin McCarthy, and Barry Smyth. 2013. Opinion-
ated product recommendation. In Sarah Jane Delany
and Santiago Onta?n?on, editors, Case-Based Rea-
soning Research and Development, volume 7969 of
Lecture Notes in Computer Science, pages 44?58.
Springer Berlin Heidelberg.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Yoan Guti?errez, Andy Gonz?alez, Roger P?erez, Jos?e
Abreu, Antonio Fern?andez Orqu??n, Alejandro Mos-
quera, Andr?es Montoyo, Rafael Mu?noz, and Franc
Camara. 2013. UMCC DLSI-(SA): Using a rank-
ing algorithm and informal features to solve senti-
ment analysis in Twitter. Atlanta, Georgia, USA,
page 443.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Joint ACL/EACL Con-
ference, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freel-
ing 3.0: Towards wider multilinguality. Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, May. European Language Resources
Association (ELRA).
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PagerRank Citation
Ranking: Bringing Order to the Web.
777
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Yan Qu, James Shana-
han, and Janyce Wiebe, editors, Proceedings of the
AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications. AAAI
Press. AAAI technical report SS-04-07.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect Based Sentiment Analysis. In International
Workshop on Semantic Evaluation (SemEval), 2014.
Gerard Salton, Andrew Wong, and Chung Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Philip James Stone, Dexter Colboyd Dunphy, Marshall
Smith, and Daniel Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of Word-
Net. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC, pages 1083?1086, Lisbon, Portugal, May
26-28. European Language Resources Association.
Jianshu Sun, Chong Long, Xiaoyan Zhu, and Minlie
Huang. 2009. Mining reviews for product compar-
ison and recommendation. Polibits, pages 33 ? 40,
06.
Peter Turney and Michael Lederman Littman. 2003.
Measuring praise and criticism: Inference of seman-
tic orientation from association. ACM Transactions
on Information Systems (TOIS), 21(4):315?346.
778
